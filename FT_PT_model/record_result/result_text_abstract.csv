,paper_id,gen_sum
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper studies the problem of privacy-preserving federated learning (FL) under local differential privacy constraints. The authors propose a gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) that addresses both concerns. The proposed algorithm is based on a novel privacy preserving quantization scheme that uses a constant number of bits per dimension per client. Then the authors improve the base algorithm in two ways: first, they apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, they utilize randomized rotation as a preprocessing step to reduce quantization error. Finally, the practicality of the proposed framework is demonstrated on benchmark datasets."
1,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper studies the problem of privacy-preserving federated learning (FL) under local differential privacy constraints. The authors propose a gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) that addresses both concerns. The proposed algorithm is based on a novel privacy preserving quantization scheme that uses a constant number of bits per dimension per client. Then the authors improve the base algorithm in two ways: first, they apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, they utilize randomized rotation as a preprocessing step to reduce quantization error. Finally, the practicality of the proposed framework is demonstrated on benchmark datasets."
2,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper studies the problem of privacy-preserving federated learning (FL) under local differential privacy constraints. The authors propose a gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) that addresses both concerns. The proposed algorithm is based on a novel privacy preserving quantization scheme that uses a constant number of bits per dimension per client. Then the authors improve the base algorithm in two ways: first, they apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, they utilize randomized rotation as a preprocessing step to reduce quantization error. Finally, the practicality of the proposed framework is demonstrated on benchmark datasets."
3,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a self-attention network (SAN) method that leverages prior knowledge to improve the language representation. The method is based on the idea that prior knowledge is useful for language representation learning. The authors propose to use the word frequency knowledge for monolingual data and other prior translation lexicon knowledge for the bilingual data, respectively. The proposed method is evaluated on WMT14 English-to-German and WMT17 Chinese-toEnglish translation tasks."
4,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a self-attention network (SAN) method that leverages prior knowledge to improve the language representation. The method is based on the idea that prior knowledge is useful for language representation learning. The authors propose to use the word frequency knowledge for monolingual data and other prior translation lexicon knowledge for the bilingual data, respectively. The proposed method is evaluated on WMT14 English-to-German and WMT17 Chinese-toEnglish translation tasks."
5,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a self-attention network (SAN) method that leverages prior knowledge to improve the language representation. The method is based on the idea that prior knowledge is useful for language representation learning. The authors propose to use the word frequency knowledge for monolingual data and other prior translation lexicon knowledge for the bilingual data, respectively. The proposed method is evaluated on WMT14 English-to-German and WMT17 Chinese-toEnglish translation tasks."
6,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov Games (BSMG) model that can model uncertainty over attacker types and the nuances of an MTD system. The model is trained using Bayesian reinforcement learning (BRL). The authors show that the learned model converges to an SSE of a BSMG and then demonstrate that it improves the state-of-the-art in MTD for web-application security.
7,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov Games (BSMG) model that can model uncertainty over attacker types and the nuances of an MTD system. The model is trained using Bayesian reinforcement learning (BRL). The authors show that the learned model converges to an SSE of a BSMG and then demonstrate that it improves the state-of-the-art in MTD for web-application security.
8,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov Games (BSMG) model that can model uncertainty over attacker types and the nuances of an MTD system. The model is trained using Bayesian reinforcement learning (BRL). The authors show that the learned model converges to an SSE of a BSMG and then demonstrate that it improves the state-of-the-art in MTD for web-application security.
9,SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper proposes a VAE ensemble framework for unsupervised disentangled representation learning. The proposed method is based on the assumption that the latent representations are unique in their own ways, and the disentanglement representations are “alike” (similar up to a signed permutation transformation). The authors propose a pair-wise linear transformation between the latent reprere14 sentations. The authors show both theoretically and experimentally that the proposed method encourages the linear transformations connecting the VAEs to be trivial transformations, aligning the latent representation of different models to be  alike."
10,SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper proposes a VAE ensemble framework for unsupervised disentangled representation learning. The proposed method is based on the assumption that the latent representations are unique in their own ways, and the disentanglement representations are “alike” (similar up to a signed permutation transformation). The authors propose a pair-wise linear transformation between the latent reprere14 sentations. The authors show both theoretically and experimentally that the proposed method encourages the linear transformations connecting the VAEs to be trivial transformations, aligning the latent representation of different models to be  alike."
11,SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper proposes a VAE ensemble framework for unsupervised disentangled representation learning. The proposed method is based on the assumption that the latent representations are unique in their own ways, and the disentanglement representations are “alike” (similar up to a signed permutation transformation). The authors propose a pair-wise linear transformation between the latent reprere14 sentations. The authors show both theoretically and experimentally that the proposed method encourages the linear transformations connecting the VAEs to be trivial transformations, aligning the latent representation of different models to be  alike."
12,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The proposed method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The graph neural network in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. The paper leverages the progress of unsupervised representation learning in natural language processing to provide a significant boost to AutoML.
13,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The proposed method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The graph neural network in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. The paper leverages the progress of unsupervised representation learning in natural language processing to provide a significant boost to AutoML.
14,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The proposed method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The graph neural network in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. The paper leverages the progress of unsupervised representation learning in natural language processing to provide a significant boost to AutoML.
15,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural networks. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. Based on this finding, they suggest that compositional learning approaches considering only model architecture design are unlikely to achieve complete compositionality."
16,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural networks. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. Based on this finding, they suggest that compositional learning approaches considering only model architecture design are unlikely to achieve complete compositionality."
17,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural networks. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. Based on this finding, they suggest that compositional learning approaches considering only model architecture design are unlikely to achieve complete compositionality."
18,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper proposes a new approach for entity alignment based on knowledge graph embedding learning. The proposed approach is based on the observation that the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function. The authors argue that such a margin cannot guarantee to be tight enough for alignment learning. To mitigate this problem, the authors propose to learn KG-invariant and principled entity representations, meanwhile preserving the original infrastructure of existing methods. The experiments demonstrate consistent and significant improvement against the existing embedding-based entity alignment methods."
19,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper proposes a new approach for entity alignment based on knowledge graph embedding learning. The proposed approach is based on the observation that the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function. The authors argue that such a margin cannot guarantee to be tight enough for alignment learning. To mitigate this problem, the authors propose to learn KG-invariant and principled entity representations, meanwhile preserving the original infrastructure of existing methods. The experiments demonstrate consistent and significant improvement against the existing embedding-based entity alignment methods."
20,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper proposes a new approach for entity alignment based on knowledge graph embedding learning. The proposed approach is based on the observation that the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function. The authors argue that such a margin cannot guarantee to be tight enough for alignment learning. To mitigate this problem, the authors propose to learn KG-invariant and principled entity representations, meanwhile preserving the original infrastructure of existing methods. The experiments demonstrate consistent and significant improvement against the existing embedding-based entity alignment methods."
21,SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design framework to improve the compression and energy efficiency of the PhlatCam camera. The proposed SACoD framework jointly optimizes the mask coded in the camera sensor and the backend CNN model via differential neural architecture search. Extensive experiments including both simulation and physical measurement on the manufactured masks show that the proposed method achieves aggressive model compression while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art (SOTA)."
22,SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design framework to improve the compression and energy efficiency of the PhlatCam camera. The proposed SACoD framework jointly optimizes the mask coded in the camera sensor and the backend CNN model via differential neural architecture search. Extensive experiments including both simulation and physical measurement on the manufactured masks show that the proposed method achieves aggressive model compression while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art (SOTA)."
23,SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design framework to improve the compression and energy efficiency of the PhlatCam camera. The proposed SACoD framework jointly optimizes the mask coded in the camera sensor and the backend CNN model via differential neural architecture search. Extensive experiments including both simulation and physical measurement on the manufactured masks show that the proposed method achieves aggressive model compression while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art (SOTA)."
24,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a temporal matrix factorization model for understanding behavioral heterogeneity in complex social systems applicable in behavioral biology, social sciences, neuroscience, and information science. The authors use a unique dataset containing lifetime trajectories of all individuals over multiple generations in two honey bee colonies, and jointly learn the average developmental path and structured variations of individuals in the social network over their entire lives. The method yields inherently interpretable embeddings that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived."
25,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a temporal matrix factorization model for understanding behavioral heterogeneity in complex social systems applicable in behavioral biology, social sciences, neuroscience, and information science. The authors use a unique dataset containing lifetime trajectories of all individuals over multiple generations in two honey bee colonies, and jointly learn the average developmental path and structured variations of individuals in the social network over their entire lives. The method yields inherently interpretable embeddings that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived."
26,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a temporal matrix factorization model for understanding behavioral heterogeneity in complex social systems applicable in behavioral biology, social sciences, neuroscience, and information science. The authors use a unique dataset containing lifetime trajectories of all individuals over multiple generations in two honey bee colonies, and jointly learn the average developmental path and structured variations of individuals in the social network over their entire lives. The method yields inherently interpretable embeddings that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived."
27,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation pipeline for image reconstruction tasks arising in medical imaging and explore its effectiveness at reducing the required training data in a variety of settings. The authors focus on accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. The proposed DA pipeline is specifically designed to utilize the invariances of medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. The experimental results show that for some problem regimes, DA can achieve comparable performance to the state of the art while using significantly fewer training data."
28,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation pipeline for image reconstruction tasks arising in medical imaging and explore its effectiveness at reducing the required training data in a variety of settings. The authors focus on accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. The proposed DA pipeline is specifically designed to utilize the invariances of medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. The experimental results show that for some problem regimes, DA can achieve comparable performance to the state of the art while using significantly fewer training data."
29,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation pipeline for image reconstruction tasks arising in medical imaging and explore its effectiveness at reducing the required training data in a variety of settings. The authors focus on accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. The proposed DA pipeline is specifically designed to utilize the invariances of medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. The experimental results show that for some problem regimes, DA can achieve comparable performance to the state of the art while using significantly fewer training data."
30,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm of ordered data for effective order learning. First, the order-identity decomposition (ORID) network is developed to divide the information of an object instance into an order-related feature and an identity feature. Then, the group object instances into clusters according to their identity features using a repulsive term. Moreover, the rank of a test instance is estimated by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm can cluster ordered data effectively and also yield excellent rank estimation performance."
31,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm of ordered data for effective order learning. First, the order-identity decomposition (ORID) network is developed to divide the information of an object instance into an order-related feature and an identity feature. Then, the group object instances into clusters according to their identity features using a repulsive term. Moreover, the rank of a test instance is estimated by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm can cluster ordered data effectively and also yield excellent rank estimation performance."
32,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm of ordered data for effective order learning. First, the order-identity decomposition (ORID) network is developed to divide the information of an object instance into an order-related feature and an identity feature. Then, the group object instances into clusters according to their identity features using a repulsive term. Moreover, the rank of a test instance is estimated by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm can cluster ordered data effectively and also yield excellent rank estimation performance."
33,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for episodic exploration in model-free reinforcement learning. The proposed method is based on the observation that humans distinguish good exploration behaviors by looking into the entire episode. The authors propose to use an episode-level exploration score to evaluate the exploration behavior of each episode and store the high-scoring episodes in a small ranking buffer. The method is evaluated on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks."
34,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for episodic exploration in model-free reinforcement learning. The proposed method is based on the observation that humans distinguish good exploration behaviors by looking into the entire episode. The authors propose to use an episode-level exploration score to evaluate the exploration behavior of each episode and store the high-scoring episodes in a small ranking buffer. The method is evaluated on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks."
35,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for episodic exploration in model-free reinforcement learning. The proposed method is based on the observation that humans distinguish good exploration behaviors by looking into the entire episode. The authors propose to use an episode-level exploration score to evaluate the exploration behavior of each episode and store the high-scoring episodes in a small ranking buffer. The method is evaluated on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks."
36,SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) that learns a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. The key challenge is how to align the representations in the two spaces. To solve this problem, the authors propose Isometric Propagation Network (IPN), which learns to propagate the class representations on an auto-generated graph within each space. The authors regularize the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. The proposed method achieves state-of-the-art performance on three popular ZSL benchmarks."
37,SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) that learns a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. The key challenge is how to align the representations in the two spaces. To solve this problem, the authors propose Isometric Propagation Network (IPN), which learns to propagate the class representations on an auto-generated graph within each space. The authors regularize the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. The proposed method achieves state-of-the-art performance on three popular ZSL benchmarks."
38,SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) that learns a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. The key challenge is how to align the representations in the two spaces. To solve this problem, the authors propose Isometric Propagation Network (IPN), which learns to propagate the class representations on an auto-generated graph within each space. The authors regularize the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. The proposed method achieves state-of-the-art performance on three popular ZSL benchmarks."
39,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, the authors propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. The proposed method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state. Experiments on GLUE/SuperGLUE show the effectiveness of the proposed method."
40,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, the authors propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. The proposed method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state. Experiments on GLUE/SuperGLUE show the effectiveness of the proposed method."
41,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, the authors propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. The proposed method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state. Experiments on GLUE/SuperGLUE show the effectiveness of the proposed method."
42,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper studies the sensitivity of the learning algorithm with respect to varying quality in the image input for autonomous driving. Based on the results of sensitivity analysis, the authors propose an algorithm to improve the overall performance of the task of “learning to steer”. The results show that the proposed algorithm is able to enhance the learning outcomes up to 48%. A comparative study drawn between the approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of the proposed method."
43,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper studies the sensitivity of the learning algorithm with respect to varying quality in the image input for autonomous driving. Based on the results of sensitivity analysis, the authors propose an algorithm to improve the overall performance of the task of “learning to steer”. The results show that the proposed algorithm is able to enhance the learning outcomes up to 48%. A comparative study drawn between the approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of the proposed method."
44,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper studies the sensitivity of the learning algorithm with respect to varying quality in the image input for autonomous driving. Based on the results of sensitivity analysis, the authors propose an algorithm to improve the overall performance of the task of “learning to steer”. The results show that the proposed algorithm is able to enhance the learning outcomes up to 48%. A comparative study drawn between the approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of the proposed method."
45,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), an algorithm to enforce the hard constraints of optimization problems. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. The authors demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid."
46,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), an algorithm to enforce the hard constraints of optimization problems. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. The authors demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid."
47,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), an algorithm to enforce the hard constraints of optimization problems. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. The authors demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid."
48,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper proposes a new regularization scheme for neural network pruning. The proposed scheme is based on the L2 regularization, where the regularization grows large gradually to tackle two central problems of pruning: pruning schedule and weight importance scoring. The growing penalty scheme also brings an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning settings."
49,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper proposes a new regularization scheme for neural network pruning. The proposed scheme is based on the L2 regularization, where the regularization grows large gradually to tackle two central problems of pruning: pruning schedule and weight importance scoring. The growing penalty scheme also brings an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning settings."
50,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper proposes a new regularization scheme for neural network pruning. The proposed scheme is based on the L2 regularization, where the regularization grows large gradually to tackle two central problems of pruning: pruning schedule and weight importance scoring. The growing penalty scheme also brings an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning settings."
51,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper investigates the role of planning in model-based reinforcement learning (MBRL). The authors study MuZero, a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. The authors perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. The results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization."
52,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper investigates the role of planning in model-based reinforcement learning (MBRL). The authors study MuZero, a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. The authors perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. The results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization."
53,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper investigates the role of planning in model-based reinforcement learning (MBRL). The authors study MuZero, a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. The authors perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. The results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization."
54,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes a new value iteration network (VIN) method for implicit planning in reinforcement learning. The main idea is to use contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning to improve the performance of VIN-like models when the underlying MDP is discrete, fixed and known. Experiments show that the proposed method outperforms model-free baselines across three general MDP setups."
55,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes a new value iteration network (VIN) method for implicit planning in reinforcement learning. The main idea is to use contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning to improve the performance of VIN-like models when the underlying MDP is discrete, fixed and known. Experiments show that the proposed method outperforms model-free baselines across three general MDP setups."
56,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes a new value iteration network (VIN) method for implicit planning in reinforcement learning. The main idea is to use contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning to improve the performance of VIN-like models when the underlying MDP is discrete, fixed and known. Experiments show that the proposed method outperforms model-free baselines across three general MDP setups."
57,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the inductive bias of neural networks for learning read-once Boolean functions. The authors first observe empirically that the learned neurons are aligned with the terms of the DNF, despite the fact that there are many zero-error networks that do not have this property. They then provide theoretical insights on the learning process and the optimization to better understand why gradient descent “chooses” the compact representation. Finally, they empirically show that their results are validated in the empirical case for high dimensional DNFs, more general network architectures and tabular datasets."
58,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the inductive bias of neural networks for learning read-once Boolean functions. The authors first observe empirically that the learned neurons are aligned with the terms of the DNF, despite the fact that there are many zero-error networks that do not have this property. They then provide theoretical insights on the learning process and the optimization to better understand why gradient descent “chooses” the compact representation. Finally, they empirically show that their results are validated in the empirical case for high dimensional DNFs, more general network architectures and tabular datasets."
59,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the inductive bias of neural networks for learning read-once Boolean functions. The authors first observe empirically that the learned neurons are aligned with the terms of the DNF, despite the fact that there are many zero-error networks that do not have this property. They then provide theoretical insights on the learning process and the optimization to better understand why gradient descent “chooses” the compact representation. Finally, they empirically show that their results are validated in the empirical case for high dimensional DNFs, more general network architectures and tabular datasets."
60,SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for learning a graph-structured reinforcement learning model for sparse reward environments. The main idea is to build a dynamic graph on top of state transitions in the replay buffer based on historical trajectories and develop an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. Experiments show that the proposed method can outperform the state-of-the-art algorithms in terms of sample efficiency."
61,SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for learning a graph-structured reinforcement learning model for sparse reward environments. The main idea is to build a dynamic graph on top of state transitions in the replay buffer based on historical trajectories and develop an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. Experiments show that the proposed method can outperform the state-of-the-art algorithms in terms of sample efficiency."
62,SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for learning a graph-structured reinforcement learning model for sparse reward environments. The main idea is to build a dynamic graph on top of state transitions in the replay buffer based on historical trajectories and develop an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. Experiments show that the proposed method can outperform the state-of-the-art algorithms in terms of sample efficiency."
63,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method to sample from different training levels for reinforcement learning agents. The method is based on the idea that different levels provide different learning progress for an agent at specific times during training. The authors propose to use temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level’s future learning potential when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it."
64,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method to sample from different training levels for reinforcement learning agents. The method is based on the idea that different levels provide different learning progress for an agent at specific times during training. The authors propose to use temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level’s future learning potential when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it."
65,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method to sample from different training levels for reinforcement learning agents. The method is based on the idea that different levels provide different learning progress for an agent at specific times during training. The authors propose to use temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level’s future learning potential when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it."
66,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic method for out-of-distribution (OOD) data augmentation. The main idea is to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. The proposed method leverages automatic differentiation procedures and randomized singular value decomposition for scalability. The experimental results show that the proposed method outperforms strong and widely used baselines on Text and Image classification tasks."
67,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic method for out-of-distribution (OOD) data augmentation. The main idea is to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. The proposed method leverages automatic differentiation procedures and randomized singular value decomposition for scalability. The experimental results show that the proposed method outperforms strong and widely used baselines on Text and Image classification tasks."
68,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic method for out-of-distribution (OOD) data augmentation. The main idea is to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. The proposed method leverages automatic differentiation procedures and randomized singular value decomposition for scalability. The experimental results show that the proposed method outperforms strong and widely used baselines on Text and Image classification tasks."
69,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper studies the problem of one-shot word learning in a simulated 3D world. The authors propose a novel dual-coding external memory mechanism, which is able to encode both visual and language information, and then use this information to guide the agent to manipulate a novel object. The paper shows that the agent can manipulate the object as instructed (“Put the dax on the bed”), combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. The experiments show that the proposed method generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects."
70,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper studies the problem of one-shot word learning in a simulated 3D world. The authors propose a novel dual-coding external memory mechanism, which is able to encode both visual and language information, and then use this information to guide the agent to manipulate a novel object. The paper shows that the agent can manipulate the object as instructed (“Put the dax on the bed”), combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. The experiments show that the proposed method generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects."
71,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper studies the problem of one-shot word learning in a simulated 3D world. The authors propose a novel dual-coding external memory mechanism, which is able to encode both visual and language information, and then use this information to guide the agent to manipulate a novel object. The paper shows that the agent can manipulate the object as instructed (“Put the dax on the bed”), combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. The experiments show that the proposed method generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects."
72,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of class imbalance in few-shot learning. The authors study the effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. They extensively compare over 10 state-of-the-art few shot learning methods using backbones of different depths on multiple datasets. Their analysis reveals that compared to the balanced task, the performances of their class-imbalance counterparts always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric-based algorithms generally suffer less, and strategies used to mitigate imbalance in supervised learning can be adapted to the few shot case resulting in better performances."
73,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of class imbalance in few-shot learning. The authors study the effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. They extensively compare over 10 state-of-the-art few shot learning methods using backbones of different depths on multiple datasets. Their analysis reveals that compared to the balanced task, the performances of their class-imbalance counterparts always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric-based algorithms generally suffer less, and strategies used to mitigate imbalance in supervised learning can be adapted to the few shot case resulting in better performances."
74,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of class imbalance in few-shot learning. The authors study the effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. They extensively compare over 10 state-of-the-art few shot learning methods using backbones of different depths on multiple datasets. Their analysis reveals that compared to the balanced task, the performances of their class-imbalance counterparts always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric-based algorithms generally suffer less, and strategies used to mitigate imbalance in supervised learning can be adapted to the few shot case resulting in better performances."
75,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a novel graph convolutional layer for graph neural networks. The proposed method is based on the Polynomial Graph Convolution (PGC) layer, which is a non-linear convolution operator with a larger receptive field. The authors show that the proposed method can be more expressive than the most common convolution operators and their linear stacking. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach."
76,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a novel graph convolutional layer for graph neural networks. The proposed method is based on the Polynomial Graph Convolution (PGC) layer, which is a non-linear convolution operator with a larger receptive field. The authors show that the proposed method can be more expressive than the most common convolution operators and their linear stacking. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach."
77,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a novel graph convolutional layer for graph neural networks. The proposed method is based on the Polynomial Graph Convolution (PGC) layer, which is a non-linear convolution operator with a larger receptive field. The authors show that the proposed method can be more expressive than the most common convolution operators and their linear stacking. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach."
78,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes a method for sim-to-real transfer for scene graph generation. The authors propose to decompose the domain gap into appearance, label and prediction discrepancies between the two domains. To handle these discrepancies, pseudo statistic based self-learning and adversarial techniques are introduced. Experiments are conducted on toy simulators and real-world data."
79,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes a method for sim-to-real transfer for scene graph generation. The authors propose to decompose the domain gap into appearance, label and prediction discrepancies between the two domains. To handle these discrepancies, pseudo statistic based self-learning and adversarial techniques are introduced. Experiments are conducted on toy simulators and real-world data."
80,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes a method for sim-to-real transfer for scene graph generation. The authors propose to decompose the domain gap into appearance, label and prediction discrepancies between the two domains. To handle these discrepancies, pseudo statistic based self-learning and adversarial techniques are introduced. Experiments are conducted on toy simulators and real-world data."
81,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper proposes a model-free algorithm for continuous action reinforcement learning (DRL). The proposed algorithm, Randomized Ensembled Double Q-Learning (REDQ), is a combination of three components: (1) a high update-to-data (UTD) ratio, (2) an ensemble of Q functions, and (3) in-target minimization across a random subset of the Q functions from the ensemble. The proposed method is evaluated on the MuJoCo continuous action benchmark and compared with the state-of-the-art model-based algorithm."
82,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper proposes a model-free algorithm for continuous action reinforcement learning (DRL). The proposed algorithm, Randomized Ensembled Double Q-Learning (REDQ), is a combination of three components: (1) a high update-to-data (UTD) ratio, (2) an ensemble of Q functions, and (3) in-target minimization across a random subset of the Q functions from the ensemble. The proposed method is evaluated on the MuJoCo continuous action benchmark and compared with the state-of-the-art model-based algorithm."
83,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper proposes a model-free algorithm for continuous action reinforcement learning (DRL). The proposed algorithm, Randomized Ensembled Double Q-Learning (REDQ), is a combination of three components: (1) a high update-to-data (UTD) ratio, (2) an ensemble of Q functions, and (3) in-target minimization across a random subset of the Q functions from the ensemble. The proposed method is evaluated on the MuJoCo continuous action benchmark and compared with the state-of-the-art model-based algorithm."
84,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a new architecture for learning from probability distributions that is invariant to permutation of the features. The proposed architecture, called DIDA, inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. Experiments are conducted on two tasks: (1) predicting whether two dataset patches are extracted from the same initial dataset, and (2) whether the learning performance achieved by a hyper-parameter configuration under a fixed algorithm (ranging in k-NN, SVM, logistic regression and linear SGD) dominates that of another configuration, for a dataset extracted from OpenML benchmarking suite. On both tasks, DIDA outperforms the state-of-the-art: DSS and DATASET2VEC architectures, as well as the models based on hand-crafted meta-features of the literature."
85,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a new architecture for learning from probability distributions that is invariant to permutation of the features. The proposed architecture, called DIDA, inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. Experiments are conducted on two tasks: (1) predicting whether two dataset patches are extracted from the same initial dataset, and (2) whether the learning performance achieved by a hyper-parameter configuration under a fixed algorithm (ranging in k-NN, SVM, logistic regression and linear SGD) dominates that of another configuration, for a dataset extracted from OpenML benchmarking suite. On both tasks, DIDA outperforms the state-of-the-art: DSS and DATASET2VEC architectures, as well as the models based on hand-crafted meta-features of the literature."
86,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a new architecture for learning from probability distributions that is invariant to permutation of the features. The proposed architecture, called DIDA, inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. Experiments are conducted on two tasks: (1) predicting whether two dataset patches are extracted from the same initial dataset, and (2) whether the learning performance achieved by a hyper-parameter configuration under a fixed algorithm (ranging in k-NN, SVM, logistic regression and linear SGD) dominates that of another configuration, for a dataset extracted from OpenML benchmarking suite. On both tasks, DIDA outperforms the state-of-the-art: DSS and DATASET2VEC architectures, as well as the models based on hand-crafted meta-features of the literature."
87,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,This paper proposes a novel graph learning method based on graph Laplacian-like matrices. The main idea is to learn ultra-sparse undirected graphs from potentially high-dimensional input data. The proposed method is based on a combination of several existing spectral graph densification methods. The authors show that the spectral embedding (or approximate effective-resistance) distances on the graph will encode the similarities between the original input data points. The paper also shows that the most spectrally-critical edges can be added to the graph by identifying and including the most spectral critical edges into the graph.
88,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,This paper proposes a novel graph learning method based on graph Laplacian-like matrices. The main idea is to learn ultra-sparse undirected graphs from potentially high-dimensional input data. The proposed method is based on a combination of several existing spectral graph densification methods. The authors show that the spectral embedding (or approximate effective-resistance) distances on the graph will encode the similarities between the original input data points. The paper also shows that the most spectrally-critical edges can be added to the graph by identifying and including the most spectral critical edges into the graph.
89,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,This paper proposes a novel graph learning method based on graph Laplacian-like matrices. The main idea is to learn ultra-sparse undirected graphs from potentially high-dimensional input data. The proposed method is based on a combination of several existing spectral graph densification methods. The authors show that the spectral embedding (or approximate effective-resistance) distances on the graph will encode the similarities between the original input data points. The paper also shows that the most spectrally-critical edges can be added to the graph by identifying and including the most spectral critical edges into the graph.
90,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a novel unsupervised learning approach named goal-conditioned policy with intrinsic motivation (GPIM) that jointly learns both an abstract-level policy and a goal-conditional policy. The abstract level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal conditioned policy. In addition, the learned discriminator serves as an intrinsic reward function for the policy to imitate the trajectory induced by the abstract- level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed method."
91,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a novel unsupervised learning approach named goal-conditioned policy with intrinsic motivation (GPIM) that jointly learns both an abstract-level policy and a goal-conditional policy. The abstract level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal conditioned policy. In addition, the learned discriminator serves as an intrinsic reward function for the policy to imitate the trajectory induced by the abstract- level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed method."
92,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a novel unsupervised learning approach named goal-conditioned policy with intrinsic motivation (GPIM) that jointly learns both an abstract-level policy and a goal-conditional policy. The abstract level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal conditioned policy. In addition, the learned discriminator serves as an intrinsic reward function for the policy to imitate the trajectory induced by the abstract- level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed method."
93,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning. The authors propose an adaptive feature-switching criterion based on the feature distance between the deployed Q-network and the underlying learning Q-networks. Experiments are conducted on a medical treatment environment and a collection of Atari games.
94,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning. The authors propose an adaptive feature-switching criterion based on the feature distance between the deployed Q-network and the underlying learning Q-networks. Experiments are conducted on a medical treatment environment and a collection of Atari games.
95,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning. The authors propose an adaptive feature-switching criterion based on the feature distance between the deployed Q-network and the underlying learning Q-networks. Experiments are conducted on a medical treatment environment and a collection of Atari games.
96,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a homotopy-SGD algorithm for solving large-scale non-convex optimization problems. The main contribution of the paper is to provide a theoretical analysis of the proposed algorithm under some mild assumptions on the problem structure. The theoretical analysis shows that, with a specifically designed scheme for the homotope parameter, the proposed method enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations confirm the theoretical results."
97,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a homotopy-SGD algorithm for solving large-scale non-convex optimization problems. The main contribution of the paper is to provide a theoretical analysis of the proposed algorithm under some mild assumptions on the problem structure. The theoretical analysis shows that, with a specifically designed scheme for the homotope parameter, the proposed method enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations confirm the theoretical results."
98,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a homotopy-SGD algorithm for solving large-scale non-convex optimization problems. The main contribution of the paper is to provide a theoretical analysis of the proposed algorithm under some mild assumptions on the problem structure. The theoretical analysis shows that, with a specifically designed scheme for the homotope parameter, the proposed method enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations confirm the theoretical results."
99,SP:195d090d9df0bda33103edcbbaf300e43f4562be,This paper proposes a meta-learning method for estimating the 3D shape of objects from sparse point clouds. The proposed method is based on a Bayesian meta-learner that learns the posterior distribution of a latent representation conditioned on the sparse cloud. The method is evaluated on the standard ShapeNet and ICL-NUIM benchmarks.
100,SP:195d090d9df0bda33103edcbbaf300e43f4562be,This paper proposes a meta-learning method for estimating the 3D shape of objects from sparse point clouds. The proposed method is based on a Bayesian meta-learner that learns the posterior distribution of a latent representation conditioned on the sparse cloud. The method is evaluated on the standard ShapeNet and ICL-NUIM benchmarks.
101,SP:195d090d9df0bda33103edcbbaf300e43f4562be,This paper proposes a meta-learning method for estimating the 3D shape of objects from sparse point clouds. The proposed method is based on a Bayesian meta-learner that learns the posterior distribution of a latent representation conditioned on the sparse cloud. The method is evaluated on the standard ShapeNet and ICL-NUIM benchmarks.
102,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper proposes a channel-wise activation suppression (CAS) strategy to improve the robustness of deep neural networks against adversarial examples. The main idea is to train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness. The experimental results show that CAS can suppress redundant activation from being activated by adversarial perturbations."
103,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper proposes a channel-wise activation suppression (CAS) strategy to improve the robustness of deep neural networks against adversarial examples. The main idea is to train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness. The experimental results show that CAS can suppress redundant activation from being activated by adversarial perturbations."
104,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper proposes a channel-wise activation suppression (CAS) strategy to improve the robustness of deep neural networks against adversarial examples. The main idea is to train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness. The experimental results show that CAS can suppress redundant activation from being activated by adversarial perturbations."
105,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper studies the connection between initialization, optimization, and overparametrization in neural networks with random initialization. The authors show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. They show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn, minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, they derive a novel O(h-1/2) upper-bound on the operator norm distance between the trained network and the min norm solution."
106,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper studies the connection between initialization, optimization, and overparametrization in neural networks with random initialization. The authors show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. They show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn, minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, they derive a novel O(h-1/2) upper-bound on the operator norm distance between the trained network and the min norm solution."
107,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper studies the connection between initialization, optimization, and overparametrization in neural networks with random initialization. The authors show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. They show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn, minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, they derive a novel O(h-1/2) upper-bound on the operator norm distance between the trained network and the min norm solution."
108,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of deep neural networks (DNNs) for ReLU activations. The authors show that DNNs have essentially the same approximation properties as their “shallow” two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. "
109,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of deep neural networks (DNNs) for ReLU activations. The authors show that DNNs have essentially the same approximation properties as their “shallow” two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. "
110,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of deep neural networks (DNNs) for ReLU activations. The authors show that DNNs have essentially the same approximation properties as their “shallow” two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. "
111,SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper proposes a method to address the overestimation problem in deep reinforcement learning. The main idea is to add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which the authors provide theoretical and experimental guarantee for future policy improvement. The authors evaluate their method on a set of classical control tasks and show that the proposed algorithms are more computationally efficient and stable than several existing algorithms for continuous control."
112,SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper proposes a method to address the overestimation problem in deep reinforcement learning. The main idea is to add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which the authors provide theoretical and experimental guarantee for future policy improvement. The authors evaluate their method on a set of classical control tasks and show that the proposed algorithms are more computationally efficient and stable than several existing algorithms for continuous control."
113,SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper proposes a method to address the overestimation problem in deep reinforcement learning. The main idea is to add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which the authors provide theoretical and experimental guarantee for future policy improvement. The authors evaluate their method on a set of classical control tasks and show that the proposed algorithms are more computationally efficient and stable than several existing algorithms for continuous control."
114,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes an extension of the inverse reinforcement learning (IRL) problem to a well-posed expectation optimization problem, where the goal is to recover the probability distribution over reward functions from expert demonstrations. The authors propose to use the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the reward distribution as the first solution to the SIRL problem. The proposed method is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem. "
115,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes an extension of the inverse reinforcement learning (IRL) problem to a well-posed expectation optimization problem, where the goal is to recover the probability distribution over reward functions from expert demonstrations. The authors propose to use the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the reward distribution as the first solution to the SIRL problem. The proposed method is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem. "
116,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes an extension of the inverse reinforcement learning (IRL) problem to a well-posed expectation optimization problem, where the goal is to recover the probability distribution over reward functions from expert demonstrations. The authors propose to use the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the reward distribution as the first solution to the SIRL problem. The proposed method is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem. "
117,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervision learning. The main contribution is a simple but realistic “expansion” assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. The authors prove that under these assumptions, the minimizers of population objectives based on self training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, they immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness."
118,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervision learning. The main contribution is a simple but realistic “expansion” assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. The authors prove that under these assumptions, the minimizers of population objectives based on self training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, they immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness."
119,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervision learning. The main contribution is a simple but realistic “expansion” assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. The authors prove that under these assumptions, the minimizers of population objectives based on self training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, they immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness."
120,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for long-term video prediction. The main idea is to use a hierarchical model to predict the future frames, and then translate the predicted frames to pixels by videoto-video translation. The method is evaluated on three challenging datasets involving car driving and human dancing, and demonstrates that it can generate complicated scene structures and motions over a very long time horizon."
121,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for long-term video prediction. The main idea is to use a hierarchical model to predict the future frames, and then translate the predicted frames to pixels by videoto-video translation. The method is evaluated on three challenging datasets involving car driving and human dancing, and demonstrates that it can generate complicated scene structures and motions over a very long time horizon."
122,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for long-term video prediction. The main idea is to use a hierarchical model to predict the future frames, and then translate the predicted frames to pixels by videoto-video translation. The method is evaluated on three challenging datasets involving car driving and human dancing, and demonstrates that it can generate complicated scene structures and motions over a very long time horizon."
123,SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new method for graph neural networks (GNNs) based on the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. The paper also analyzes the geometric properties of IGNNS from the perspective of dynamical system."
124,SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new method for graph neural networks (GNNs) based on the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. The paper also analyzes the geometric properties of IGNNS from the perspective of dynamical system."
125,SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new method for graph neural networks (GNNs) based on the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. The paper also analyzes the geometric properties of IGNNS from the perspective of dynamical system."
126,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a geometric graph generative adversarial network (GG-GAN) for graph generation. The proposed method is based on the geometric graph (GG) model, where each node is associated with a position in space and the edges are connected based on a similarity function. The authors propose to use a Wasserstein GAN to model complex relations, isomorphic graphs consistently, and fully exploiting the latent distribution. The experimental results show that GG-GAN is permutation equivariant and can scale to tens of thousands of nodes."
127,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a geometric graph generative adversarial network (GG-GAN) for graph generation. The proposed method is based on the geometric graph (GG) model, where each node is associated with a position in space and the edges are connected based on a similarity function. The authors propose to use a Wasserstein GAN to model complex relations, isomorphic graphs consistently, and fully exploiting the latent distribution. The experimental results show that GG-GAN is permutation equivariant and can scale to tens of thousands of nodes."
128,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a geometric graph generative adversarial network (GG-GAN) for graph generation. The proposed method is based on the geometric graph (GG) model, where each node is associated with a position in space and the edges are connected based on a similarity function. The authors propose to use a Wasserstein GAN to model complex relations, isomorphic graphs consistently, and fully exploiting the latent distribution. The experimental results show that GG-GAN is permutation equivariant and can scale to tens of thousands of nodes."
129,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a method for exploring how neurons within a neural network interact. Specifically, they consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they formalize the problem in terms of the Minimum Description Length principle, by which they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world."
130,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a method for exploring how neurons within a neural network interact. Specifically, they consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they formalize the problem in terms of the Minimum Description Length principle, by which they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world."
131,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a method for exploring how neurons within a neural network interact. Specifically, they consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they formalize the problem in terms of the Minimum Description Length principle, by which they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world."
132,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of policy optimization in the worst-case setting. The authors provide a unified conceptual and mathematical framework for the study of algorithms in this regime. In particular, they show that for naı̈ve approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the best possible world. They derive families of algorithms which follow this principle."
133,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of policy optimization in the worst-case setting. The authors provide a unified conceptual and mathematical framework for the study of algorithms in this regime. In particular, they show that for naı̈ve approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the best possible world. They derive families of algorithms which follow this principle."
134,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of policy optimization in the worst-case setting. The authors provide a unified conceptual and mathematical framework for the study of algorithms in this regime. In particular, they show that for naı̈ve approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the best possible world. They derive families of algorithms which follow this principle."
135,SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient neural ODE solver based on asynchronous leapfrog (ALF) solver. The proposed method has a constant memory cost w.r.t. the number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). The authors validate MALI in various tasks: on image recognition tasks, to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, the proposed method significantly outperforms the existing methods."
136,SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient neural ODE solver based on asynchronous leapfrog (ALF) solver. The proposed method has a constant memory cost w.r.t. the number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). The authors validate MALI in various tasks: on image recognition tasks, to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, the proposed method significantly outperforms the existing methods."
137,SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient neural ODE solver based on asynchronous leapfrog (ALF) solver. The proposed method has a constant memory cost w.r.t. the number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). The authors validate MALI in various tasks: on image recognition tasks, to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, the proposed method significantly outperforms the existing methods."
138,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provide an in-depth analysis that assesses the ability of each model to fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) generalize robustly to unseen conditions composed of unseen object combinations. The authors also identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionsings, and using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process."
139,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provide an in-depth analysis that assesses the ability of each model to fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) generalize robustly to unseen conditions composed of unseen object combinations. The authors also identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionsings, and using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process."
140,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provide an in-depth analysis that assesses the ability of each model to fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) generalize robustly to unseen conditions composed of unseen object combinations. The authors also identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionsings, and using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process."
141,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper proposes Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), a variant of Graph Neural Networks (GNNs) that first augments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. The authors show theoretically and numerically that GA-MLP with suitable operators can distinguish almost all non-isomorphic graphs, just like the WeisfeilerLehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, the authors prove a separation in expressive power between GA -MLPs and G-NNs that grows exponentially in depth. "
142,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper proposes Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), a variant of Graph Neural Networks (GNNs) that first augments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. The authors show theoretically and numerically that GA-MLP with suitable operators can distinguish almost all non-isomorphic graphs, just like the WeisfeilerLehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, the authors prove a separation in expressive power between GA -MLPs and G-NNs that grows exponentially in depth. "
143,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper proposes Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), a variant of Graph Neural Networks (GNNs) that first augments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. The authors show theoretically and numerically that GA-MLP with suitable operators can distinguish almost all non-isomorphic graphs, just like the WeisfeilerLehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, the authors prove a separation in expressive power between GA -MLPs and G-NNs that grows exponentially in depth. "
144,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,This paper proposes an actor-learner distillation (ALD) method that transfers learning progress from a large capacity learner model to a small capacity actor model. The main idea is to use a continual form of distillation that leverages a continual update of the learner network and the actor network. The proposed method is evaluated on a series of partially-observable tasks. The results show that the proposed method can recover the sample-efficiency gains of the transformer model while maintaining the fast inference and reduced total training time of the LSTM actor model with the same computational cost.
145,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,This paper proposes an actor-learner distillation (ALD) method that transfers learning progress from a large capacity learner model to a small capacity actor model. The main idea is to use a continual form of distillation that leverages a continual update of the learner network and the actor network. The proposed method is evaluated on a series of partially-observable tasks. The results show that the proposed method can recover the sample-efficiency gains of the transformer model while maintaining the fast inference and reduced total training time of the LSTM actor model with the same computational cost.
146,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,This paper proposes an actor-learner distillation (ALD) method that transfers learning progress from a large capacity learner model to a small capacity actor model. The main idea is to use a continual form of distillation that leverages a continual update of the learner network and the actor network. The proposed method is evaluated on a series of partially-observable tasks. The results show that the proposed method can recover the sample-efficiency gains of the transformer model while maintaining the fast inference and reduced total training time of the LSTM actor model with the same computational cost.
147,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for multi-domain few-shot image classification. The main idea is to dynamically re-weighting and composing the most appropriate domain-specific representations. The experiments show that URT sets a new state-of-the-art result on Meta-Dataset.
148,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for multi-domain few-shot image classification. The main idea is to dynamically re-weighting and composing the most appropriate domain-specific representations. The experiments show that URT sets a new state-of-the-art result on Meta-Dataset.
149,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for multi-domain few-shot image classification. The main idea is to dynamically re-weighting and composing the most appropriate domain-specific representations. The experiments show that URT sets a new state-of-the-art result on Meta-Dataset.
150,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a self-taught associative memory (STAM) architecture for unsupervised Progressive Learning (UPL). The proposed architecture is based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The proposed STAM architecture is evaluated on a set of UPL benchmarks."
151,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a self-taught associative memory (STAM) architecture for unsupervised Progressive Learning (UPL). The proposed architecture is based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The proposed STAM architecture is evaluated on a set of UPL benchmarks."
152,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a self-taught associative memory (STAM) architecture for unsupervised Progressive Learning (UPL). The proposed architecture is based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The proposed STAM architecture is evaluated on a set of UPL benchmarks."
153,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized training of deep learning models. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They highlight the intimate interplay between network topology and learning rate at the different training phases and discuss the implications for communication efficient training schemes."
154,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized training of deep learning models. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They highlight the intimate interplay between network topology and learning rate at the different training phases and discuss the implications for communication efficient training schemes."
155,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized training of deep learning models. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They highlight the intimate interplay between network topology and learning rate at the different training phases and discuss the implications for communication efficient training schemes."
156,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,This paper proposes a method for learning a similarity metric between two sequences in a siamese recurrent neural network (RNN). The similarity metric is defined as the distance between two identical sub-networks of the RNN. The authors show that the similarity metric can be learned in a weakly supervised way. The proposed method is evaluated on an activity recognition task.
157,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,This paper proposes a method for learning a similarity metric between two sequences in a siamese recurrent neural network (RNN). The similarity metric is defined as the distance between two identical sub-networks of the RNN. The authors show that the similarity metric can be learned in a weakly supervised way. The proposed method is evaluated on an activity recognition task.
158,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,This paper proposes a method for learning a similarity metric between two sequences in a siamese recurrent neural network (RNN). The similarity metric is defined as the distance between two identical sub-networks of the RNN. The authors show that the similarity metric can be learned in a weakly supervised way. The proposed method is evaluated on an activity recognition task.
159,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation on the performance of distributed SGD methods. The authors consider the case where there are multiple models trained on the same dataset, and each of the models is trained with different batch sizes and learning rate schedules. In this case, the authors propose to use a weaker synchronization mechanism (codistillation) to encourage the models to represent the same function through an auxiliary loss."
160,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation on the performance of distributed SGD methods. The authors consider the case where there are multiple models trained on the same dataset, and each of the models is trained with different batch sizes and learning rate schedules. In this case, the authors propose to use a weaker synchronization mechanism (codistillation) to encourage the models to represent the same function through an auxiliary loss."
161,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation on the performance of distributed SGD methods. The authors consider the case where there are multiple models trained on the same dataset, and each of the models is trained with different batch sizes and learning rate schedules. In this case, the authors propose to use a weaker synchronization mechanism (codistillation) to encourage the models to represent the same function through an auxiliary loss."
162,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The authors argue that the flatness of the local minimum found by SGD is related to the eigenvalues of the Hessian, which essentially controls the magnitude of the stochastic gradient noise, and the ‘tail-index’, which measures the heaviness of the tails of the network weights at convergence. The paper proves that depending on the structure of the loss at the minimum, the choices of the algorithm parameters $\� and b, the SGD iterates will converge to a heavy-tailed stationary distribution. "
163,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The authors argue that the flatness of the local minimum found by SGD is related to the eigenvalues of the Hessian, which essentially controls the magnitude of the stochastic gradient noise, and the ‘tail-index’, which measures the heaviness of the tails of the network weights at convergence. The paper proves that depending on the structure of the loss at the minimum, the choices of the algorithm parameters $\� and b, the SGD iterates will converge to a heavy-tailed stationary distribution. "
164,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The authors argue that the flatness of the local minimum found by SGD is related to the eigenvalues of the Hessian, which essentially controls the magnitude of the stochastic gradient noise, and the ‘tail-index’, which measures the heaviness of the tails of the network weights at convergence. The paper proves that depending on the structure of the loss at the minimum, the choices of the algorithm parameters $\� and b, the SGD iterates will converge to a heavy-tailed stationary distribution. "
165,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the effect of bandpass filtering on the performance of community detection models. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection. In particular, it is possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies."
166,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the effect of bandpass filtering on the performance of community detection models. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection. In particular, it is possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies."
167,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the effect of bandpass filtering on the performance of community detection models. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection. In particular, it is possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies."
168,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a method for learning a task-specific graph structure through self-supervised learning. The proposed method, SLAPS, learns both the graph structure and the GNN parameters simultaneously. The authors show that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task specific graph structure on established benchmarks."
169,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a method for learning a task-specific graph structure through self-supervised learning. The proposed method, SLAPS, learns both the graph structure and the GNN parameters simultaneously. The authors show that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task specific graph structure on established benchmarks."
170,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a method for learning a task-specific graph structure through self-supervised learning. The proposed method, SLAPS, learns both the graph structure and the GNN parameters simultaneously. The authors show that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task specific graph structure on established benchmarks."
171,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel detection method for continual learning. The novelty detection method is based on network confusion caused by training incoming data as a new class. The proposed method is evaluated on MNIST, SVHN, Cifar-10, CIFAR-100, and CRIB."
172,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel detection method for continual learning. The novelty detection method is based on network confusion caused by training incoming data as a new class. The proposed method is evaluated on MNIST, SVHN, Cifar-10, CIFAR-100, and CRIB."
173,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel detection method for continual learning. The novelty detection method is based on network confusion caused by training incoming data as a new class. The proposed method is evaluated on MNIST, SVHN, Cifar-10, CIFAR-100, and CRIB."
174,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for learning to interpret natural language constraints for safe reinforcement learning. Specifically, the authors propose a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. The proposed method is evaluated on a new multi-task benchmark, called HAZARDWORLD, where the agent must optimize reward while not violating constraints specified in free-form text. The authors show that their method achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) compared to existing approaches."
175,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for learning to interpret natural language constraints for safe reinforcement learning. Specifically, the authors propose a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. The proposed method is evaluated on a new multi-task benchmark, called HAZARDWORLD, where the agent must optimize reward while not violating constraints specified in free-form text. The authors show that their method achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) compared to existing approaches."
176,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for learning to interpret natural language constraints for safe reinforcement learning. Specifically, the authors propose a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. The proposed method is evaluated on a new multi-task benchmark, called HAZARDWORLD, where the agent must optimize reward while not violating constraints specified in free-form text. The authors show that their method achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) compared to existing approaches."
177,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection network (CAFENet) based on meta-learning strategy. The proposed method uses a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching. Extensive simulation results confirm that the proposed CAFEnet achieves better performance compared to the baseline methods using fine-tuning or few shot segmentation."
178,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection network (CAFENet) based on meta-learning strategy. The proposed method uses a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching. Extensive simulation results confirm that the proposed CAFEnet achieves better performance compared to the baseline methods using fine-tuning or few shot segmentation."
179,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection network (CAFENet) based on meta-learning strategy. The proposed method uses a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching. Extensive simulation results confirm that the proposed CAFEnet achieves better performance compared to the baseline methods using fine-tuning or few shot segmentation."
180,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method for generating explainable explanations for graph neural networks (GNNs). The proposed method, Causal Screening, incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, the proposed method can be used to generate faithful and concise explanations for any GNN model."
181,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method for generating explainable explanations for graph neural networks (GNNs). The proposed method, Causal Screening, incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, the proposed method can be used to generate faithful and concise explanations for any GNN model."
182,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method for generating explainable explanations for graph neural networks (GNNs). The proposed method, Causal Screening, incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, the proposed method can be used to generate faithful and concise explanations for any GNN model."
183,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,This paper proposes a new measure of network simplicity based on the smallest fraction of the network’s parameters that can be kept while pruning without adversely affecting its training loss. The authors show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10.
184,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,This paper proposes a new measure of network simplicity based on the smallest fraction of the network’s parameters that can be kept while pruning without adversely affecting its training loss. The authors show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10.
185,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,This paper proposes a new measure of network simplicity based on the smallest fraction of the network’s parameters that can be kept while pruning without adversely affecting its training loss. The authors show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10.
186,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a method for learning discrete representations for long-horizon tasks. The method is based on the idea that discrete representations can be used to learn a sequence of abstract states for a low-level model-predictive controller to follow. The proposed method is evaluated on a set of long-term tasks.
187,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a method for learning discrete representations for long-horizon tasks. The method is based on the idea that discrete representations can be used to learn a sequence of abstract states for a low-level model-predictive controller to follow. The proposed method is evaluated on a set of long-term tasks.
188,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a method for learning discrete representations for long-horizon tasks. The method is based on the idea that discrete representations can be used to learn a sequence of abstract states for a low-level model-predictive controller to follow. The proposed method is evaluated on a set of long-term tasks.
189,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization problem. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed precision quantization scheme of the original model."
190,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization problem. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed precision quantization scheme of the original model."
191,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization problem. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed precision quantization scheme of the original model."
192,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of quantized networks against gradient based adversarial attacks. The authors claim that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. To mitigate this issue, the authors introduce a simple temperature scaling approach to mitigate the issue while preserving the decision boundary. The experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate that the temperature scaled attacks obtain near-perfect success rate on quantized network while outperforming original attacks on adversarially trained models as well as floating-point networks."
193,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of quantized networks against gradient based adversarial attacks. The authors claim that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. To mitigate this issue, the authors introduce a simple temperature scaling approach to mitigate the issue while preserving the decision boundary. The experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate that the temperature scaled attacks obtain near-perfect success rate on quantized network while outperforming original attacks on adversarially trained models as well as floating-point networks."
194,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of quantized networks against gradient based adversarial attacks. The authors claim that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. To mitigate this issue, the authors introduce a simple temperature scaling approach to mitigate the issue while preserving the decision boundary. The experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate that the temperature scaled attacks obtain near-perfect success rate on quantized network while outperforming original attacks on adversarially trained models as well as floating-point networks."
195,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel interpretable recurrent neural network (RNN) model, called ProtoryNet, in which a new concept of prototype trajectories is introduced. The prototype trajectory enables intuitive, fine-grained interpretation of how the model reached to the final prediction, resembling the process of how humans analyze paragraphs. Experiments conducted on multiple public data sets demonstrate that the proposed method not only is more interpretable but also is more accurate than the current state-of-the-art prototype-based method."
196,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel interpretable recurrent neural network (RNN) model, called ProtoryNet, in which a new concept of prototype trajectories is introduced. The prototype trajectory enables intuitive, fine-grained interpretation of how the model reached to the final prediction, resembling the process of how humans analyze paragraphs. Experiments conducted on multiple public data sets demonstrate that the proposed method not only is more interpretable but also is more accurate than the current state-of-the-art prototype-based method."
197,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel interpretable recurrent neural network (RNN) model, called ProtoryNet, in which a new concept of prototype trajectories is introduced. The prototype trajectory enables intuitive, fine-grained interpretation of how the model reached to the final prediction, resembling the process of how humans analyze paragraphs. Experiments conducted on multiple public data sets demonstrate that the proposed method not only is more interpretable but also is more accurate than the current state-of-the-art prototype-based method."
198,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper studies the problem of parameter estimation for hidden Markov models (HMMs) when the true patient health state is not fully known. The authors propose a special case of recurrent neural networks (RNNs), which they name hidden MarkOV recurrent neural network (HMRNNs). They prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. They show that HMRN parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. They then demonstrate how the proposed method can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
199,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper studies the problem of parameter estimation for hidden Markov models (HMMs) when the true patient health state is not fully known. The authors propose a special case of recurrent neural networks (RNNs), which they name hidden MarkOV recurrent neural network (HMRNNs). They prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. They show that HMRN parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. They then demonstrate how the proposed method can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
200,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper studies the problem of parameter estimation for hidden Markov models (HMMs) when the true patient health state is not fully known. The authors propose a special case of recurrent neural networks (RNNs), which they name hidden MarkOV recurrent neural network (HMRNNs). They prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. They show that HMRN parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. They then demonstrate how the proposed method can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
201,SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method for analyzing single-cell RNA-Seq dataset of Drosophila T4/T5 neurons, focusing on their dendritic and axonal phenotypes. The method is based on factorized linear discriminant analysis (FLDA), which seeks a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. The authors further leverage their approach with a sparsity-based regularization algorithm, which selects a few genes important to a specific phenotypical feature or feature combination. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
202,SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method for analyzing single-cell RNA-Seq dataset of Drosophila T4/T5 neurons, focusing on their dendritic and axonal phenotypes. The method is based on factorized linear discriminant analysis (FLDA), which seeks a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. The authors further leverage their approach with a sparsity-based regularization algorithm, which selects a few genes important to a specific phenotypical feature or feature combination. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
203,SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method for analyzing single-cell RNA-Seq dataset of Drosophila T4/T5 neurons, focusing on their dendritic and axonal phenotypes. The method is based on factorized linear discriminant analysis (FLDA), which seeks a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. The authors further leverage their approach with a sparsity-based regularization algorithm, which selects a few genes important to a specific phenotypical feature or feature combination. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
204,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,This paper proposes a variational approximation of the posterior distribution over the saliency map of an image classifier. The posterior distribution is estimated using the likelihood function of the classifier's predictive probability of the image and that of the perturbed image. The authors show that the approximate posterior is effective in explaining the classifiers' behavior. 
205,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,This paper proposes a variational approximation of the posterior distribution over the saliency map of an image classifier. The posterior distribution is estimated using the likelihood function of the classifier's predictive probability of the image and that of the perturbed image. The authors show that the approximate posterior is effective in explaining the classifiers' behavior. 
206,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,This paper proposes a variational approximation of the posterior distribution over the saliency map of an image classifier. The posterior distribution is estimated using the likelihood function of the classifier's predictive probability of the image and that of the perturbed image. The authors show that the approximate posterior is effective in explaining the classifiers' behavior. 
207,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a novel neural debiasing method for pretrained sentence encoders, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, the proposed method effectively reduces the bias degree of pretrained text encoder, while continuously showing desirable performance on downstream tasks."
208,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a novel neural debiasing method for pretrained sentence encoders, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, the proposed method effectively reduces the bias degree of pretrained text encoder, while continuously showing desirable performance on downstream tasks."
209,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a novel neural debiasing method for pretrained sentence encoders, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, the proposed method effectively reduces the bias degree of pretrained text encoder, while continuously showing desirable performance on downstream tasks."
210,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. The proposed method is based on sample-wise randomized smoothing, which assigns different noise levels to different samples. For certification, the authors carefully allocate specific robust regions for each test sample. The experimental results demonstrate that the proposed method achieves better accuracy-robustness trade-off in the transductive setting."
211,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. The proposed method is based on sample-wise randomized smoothing, which assigns different noise levels to different samples. For certification, the authors carefully allocate specific robust regions for each test sample. The experimental results demonstrate that the proposed method achieves better accuracy-robustness trade-off in the transductive setting."
212,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. The proposed method is based on sample-wise randomized smoothing, which assigns different noise levels to different samples. For certification, the authors carefully allocate specific robust regions for each test sample. The experimental results demonstrate that the proposed method achieves better accuracy-robustness trade-off in the transductive setting."
213,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The proposed method is based on a reduced entropy condition approximate inference method that results in rich posteriors. Given a large ensemble of degraded samples, the method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The experimental results on a data recovery task under the common setting of missing values and noise demonstrate superior performance to existing variational methods for imputation and de-noising with different real data sets."
214,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The proposed method is based on a reduced entropy condition approximate inference method that results in rich posteriors. Given a large ensemble of degraded samples, the method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The experimental results on a data recovery task under the common setting of missing values and noise demonstrate superior performance to existing variational methods for imputation and de-noising with different real data sets."
215,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The proposed method is based on a reduced entropy condition approximate inference method that results in rich posteriors. Given a large ensemble of degraded samples, the method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The experimental results on a data recovery task under the common setting of missing values and noise demonstrate superior performance to existing variational methods for imputation and de-noising with different real data sets."
216,SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a method to incorporate multi-hop context information into attention computation in graph neural networks (GNNs). Specifically, the authors propose to compute attention between nodes that are not directly connected, and diffuses the attention scores across the network, which increases the “receptive field” for every layer of the GNN. Experiments on node classification and knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results."
217,SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a method to incorporate multi-hop context information into attention computation in graph neural networks (GNNs). Specifically, the authors propose to compute attention between nodes that are not directly connected, and diffuses the attention scores across the network, which increases the “receptive field” for every layer of the GNN. Experiments on node classification and knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results."
218,SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a method to incorporate multi-hop context information into attention computation in graph neural networks (GNNs). Specifically, the authors propose to compute attention between nodes that are not directly connected, and diffuses the attention scores across the network, which increases the “receptive field” for every layer of the GNN. Experiments on node classification and knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results."
219,SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new multi-task test to evaluate the ability of text models to multitask. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. The authors find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong."
220,SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new multi-task test to evaluate the ability of text models to multitask. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. The authors find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong."
221,SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new multi-task test to evaluate the ability of text models to multitask. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. The authors find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong."
222,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"This paper proposes a pre-training approach for table semantic parsing. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). Then, they pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in Table semantic parsing into the pretraining language model. To maintain the model’s ability to represent real-world data, they also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre- training process. The proposed pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt."
223,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"This paper proposes a pre-training approach for table semantic parsing. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). Then, they pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in Table semantic parsing into the pretraining language model. To maintain the model’s ability to represent real-world data, they also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre- training process. The proposed pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt."
224,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"This paper proposes a pre-training approach for table semantic parsing. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). Then, they pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in Table semantic parsing into the pretraining language model. To maintain the model’s ability to represent real-world data, they also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre- training process. The proposed pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt."
225,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. The authors prove that the standard MTL LS-SVMs algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-VMs may outperform the MTL approach, even for quite resembling tasks). The analysis provides a simple method to correct these biases, and reveals (ii) the sufficient statistics at play in the method, which can be efficiently estimated. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure."
226,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. The authors prove that the standard MTL LS-SVMs algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-VMs may outperform the MTL approach, even for quite resembling tasks). The analysis provides a simple method to correct these biases, and reveals (ii) the sufficient statistics at play in the method, which can be efficiently estimated. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure."
227,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. The authors prove that the standard MTL LS-SVMs algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-VMs may outperform the MTL approach, even for quite resembling tasks). The analysis provides a simple method to correct these biases, and reveals (ii) the sufficient statistics at play in the method, which can be efficiently estimated. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure."
228,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper proposes a conditional neural process (CNP) that is permutation invariant and group-equivariant. The authors provide a decomposition theorem for permutation-invariant CNPs and show that it is possible to construct an infinite-dimensional latent space to handle group symmetries. In addition, the authors propose an architecture using Lie group convolutional layers for practical implementation. Experiments are conducted on a 1D regression task and an image completion task."
229,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper proposes a conditional neural process (CNP) that is permutation invariant and group-equivariant. The authors provide a decomposition theorem for permutation-invariant CNPs and show that it is possible to construct an infinite-dimensional latent space to handle group symmetries. In addition, the authors propose an architecture using Lie group convolutional layers for practical implementation. Experiments are conducted on a 1D regression task and an image completion task."
230,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper proposes a conditional neural process (CNP) that is permutation invariant and group-equivariant. The authors provide a decomposition theorem for permutation-invariant CNPs and show that it is possible to construct an infinite-dimensional latent space to handle group symmetries. In addition, the authors propose an architecture using Lie group convolutional layers for practical implementation. Experiments are conducted on a 1D regression task and an image completion task."
231,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes a method for offline text generation based on off-policy learning from demonstrations. The method is based on importance weighting, where the importance of each token in the reference is weighted by its importance in the context of the demonstration. The authors argue that this is an effective way to mitigate the problem of exposure bias due to mismatched history distributions (gold vs. model-generated). The proposed method is evaluated on summarization, question generation, and machine translation tasks."
232,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes a method for offline text generation based on off-policy learning from demonstrations. The method is based on importance weighting, where the importance of each token in the reference is weighted by its importance in the context of the demonstration. The authors argue that this is an effective way to mitigate the problem of exposure bias due to mismatched history distributions (gold vs. model-generated). The proposed method is evaluated on summarization, question generation, and machine translation tasks."
233,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes a method for offline text generation based on off-policy learning from demonstrations. The method is based on importance weighting, where the importance of each token in the reference is weighted by its importance in the context of the demonstration. The authors argue that this is an effective way to mitigate the problem of exposure bias due to mismatched history distributions (gold vs. model-generated). The proposed method is evaluated on summarization, question generation, and machine translation tasks."
234,SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes InfoPro, a local supervised learning method for end-to-end (E2E) training of deep neural networks. InfoPro is based on an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The InfoPro loss is difficult to compute in its original form, but the authors derive a feasible upper bound as a surrogate optimization objective, and derive a simple but effective algorithm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate the effectiveness of InfoPro."
235,SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes InfoPro, a local supervised learning method for end-to-end (E2E) training of deep neural networks. InfoPro is based on an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The InfoPro loss is difficult to compute in its original form, but the authors derive a feasible upper bound as a surrogate optimization objective, and derive a simple but effective algorithm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate the effectiveness of InfoPro."
236,SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes InfoPro, a local supervised learning method for end-to-end (E2E) training of deep neural networks. InfoPro is based on an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The InfoPro loss is difficult to compute in its original form, but the authors derive a feasible upper bound as a surrogate optimization objective, and derive a simple but effective algorithm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate the effectiveness of InfoPro."
237,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a novel sampling method for improving the expressive power of GNNs. The proposed sampling method is based on the idea that diverse sampling offers it diverse neighborhoods, i.e., rooted sub-graphs, and the representation of target node is finally obtained via aggregating the representations of diverse neighborhoods obtained using any GNN model. Experiments are conducted on three benchmark datasets and multi-label node classification task."
238,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a novel sampling method for improving the expressive power of GNNs. The proposed sampling method is based on the idea that diverse sampling offers it diverse neighborhoods, i.e., rooted sub-graphs, and the representation of target node is finally obtained via aggregating the representations of diverse neighborhoods obtained using any GNN model. Experiments are conducted on three benchmark datasets and multi-label node classification task."
239,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a novel sampling method for improving the expressive power of GNNs. The proposed sampling method is based on the idea that diverse sampling offers it diverse neighborhoods, i.e., rooted sub-graphs, and the representation of target node is finally obtained via aggregating the representations of diverse neighborhoods obtained using any GNN model. Experiments are conducted on three benchmark datasets and multi-label node classification task."
240,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the robustness of video machine learning models to bit-level network and file corruptions, which can arise from network transmission failures or hardware errors, and explore defenses against such corruptions. The authors explore two types of defenses: corruption-agnostic and corruption-aware defenses. In particular, the authors propose Bit-corruption Augmented Training (BAT), a corruptionaware baseline that exploits knowledge of bit- level corruptions to enforce model invariance to such corruptionions."
241,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the robustness of video machine learning models to bit-level network and file corruptions, which can arise from network transmission failures or hardware errors, and explore defenses against such corruptions. The authors explore two types of defenses: corruption-agnostic and corruption-aware defenses. In particular, the authors propose Bit-corruption Augmented Training (BAT), a corruptionaware baseline that exploits knowledge of bit- level corruptions to enforce model invariance to such corruptionions."
242,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the robustness of video machine learning models to bit-level network and file corruptions, which can arise from network transmission failures or hardware errors, and explore defenses against such corruptions. The authors explore two types of defenses: corruption-agnostic and corruption-aware defenses. In particular, the authors propose Bit-corruption Augmented Training (BAT), a corruptionaware baseline that exploits knowledge of bit- level corruptions to enforce model invariance to such corruptionions."
243,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a new representation learning method for time-varying graphs. The proposed method is based on skip-gram embedding and negative sampling. The authors show that the proposed method can disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. They empirically evaluate their method using time-resolved face-to-face proximity data, showing that the learned representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction."
244,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a new representation learning method for time-varying graphs. The proposed method is based on skip-gram embedding and negative sampling. The authors show that the proposed method can disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. They empirically evaluate their method using time-resolved face-to-face proximity data, showing that the learned representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction."
245,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a new representation learning method for time-varying graphs. The proposed method is based on skip-gram embedding and negative sampling. The authors show that the proposed method can disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. They empirically evaluate their method using time-resolved face-to-face proximity data, showing that the learned representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction."
246,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a skip-tree task to evaluate the logical reasoning abilities of language models for formal mathematics. The authors evaluate the models on several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions, and completing equalities. They also analyze the models’ ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs."
247,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a skip-tree task to evaluate the logical reasoning abilities of language models for formal mathematics. The authors evaluate the models on several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions, and completing equalities. They also analyze the models’ ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs."
248,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a skip-tree task to evaluate the logical reasoning abilities of language models for formal mathematics. The authors evaluate the models on several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions, and completing equalities. They also analyze the models’ ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs."
249,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of imbalanced gradients in adversarial robustness. The authors propose a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. The MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23%. For 6 out of the 12 defenses, our attack can reduce their PGI robustness by at least 9%. The results suggest that imbalanced gradient need to be carefully addressed for more reliable adversarial adversarial attacks."
250,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of imbalanced gradients in adversarial robustness. The authors propose a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. The MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23%. For 6 out of the 12 defenses, our attack can reduce their PGI robustness by at least 9%. The results suggest that imbalanced gradient need to be carefully addressed for more reliable adversarial adversarial attacks."
251,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of imbalanced gradients in adversarial robustness. The authors propose a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. The MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23%. For 6 out of the 12 defenses, our attack can reduce their PGI robustness by at least 9%. The results suggest that imbalanced gradient need to be carefully addressed for more reliable adversarial adversarial attacks."
252,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a method for proving semantic equivalence between two complex expressions represented as typed trees. The system is based on incremental graph-to-sequence networks, and is trained to generate axiomatic proofs of equivalence. The paper is well-written and easy to follow. The experimental results show that the proposed method outperforms the state-of-the-art."
253,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a method for proving semantic equivalence between two complex expressions represented as typed trees. The system is based on incremental graph-to-sequence networks, and is trained to generate axiomatic proofs of equivalence. The paper is well-written and easy to follow. The experimental results show that the proposed method outperforms the state-of-the-art."
254,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a method for proving semantic equivalence between two complex expressions represented as typed trees. The system is based on incremental graph-to-sequence networks, and is trained to generate axiomatic proofs of equivalence. The paper is well-written and easy to follow. The experimental results show that the proposed method outperforms the state-of-the-art."
255,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a method to reduce the parameters of multimodal Transformers in the context of audio-visual video representation learning. Specifically, they decompose the Transformer into modality-specific and modality shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. They show that their approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch."
256,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a method to reduce the parameters of multimodal Transformers in the context of audio-visual video representation learning. Specifically, they decompose the Transformer into modality-specific and modality shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. They show that their approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch."
257,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a method to reduce the parameters of multimodal Transformers in the context of audio-visual video representation learning. Specifically, they decompose the Transformer into modality-specific and modality shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. They show that their approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch."
258,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes a method for online few-shot learning. The method is based on the observation that in the online setting, the object classes are correlated within a context and inferring the correct context can lead to better performance. The authors propose a new dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. They also propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
259,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes a method for online few-shot learning. The method is based on the observation that in the online setting, the object classes are correlated within a context and inferring the correct context can lead to better performance. The authors propose a new dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. They also propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
260,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes a method for online few-shot learning. The method is based on the observation that in the online setting, the object classes are correlated within a context and inferring the correct context can lead to better performance. The authors propose a new dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. They also propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
261,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) in the face of distribution shift and ever gbv rowing and changing training data, when temporal graphs evolve over time. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs. They show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph. In most cases, i.e., 14 out 18 experiments, a temporal window of size 1 is sufficient to maintain at least 90%."
262,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) in the face of distribution shift and ever gbv rowing and changing training data, when temporal graphs evolve over time. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs. They show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph. In most cases, i.e., 14 out 18 experiments, a temporal window of size 1 is sufficient to maintain at least 90%."
263,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) in the face of distribution shift and ever gbv rowing and changing training data, when temporal graphs evolve over time. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs. They show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph. In most cases, i.e., 14 out 18 experiments, a temporal window of size 1 is sufficient to maintain at least 90%."
264,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,This paper proposes a method to regularize the representation feature space of deep reinforcement learning (RL) agents by using cross-state self-constraint (CSSC) to compare representation similarity across different pairs of states. The idea is that the implicit feedback between state and action from the agent’s experience helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. The proposed method is evaluated on the OpenAI ProcGen benchmark and shows significant improvement on generalization performance across most of Procgen games.
265,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,This paper proposes a method to regularize the representation feature space of deep reinforcement learning (RL) agents by using cross-state self-constraint (CSSC) to compare representation similarity across different pairs of states. The idea is that the implicit feedback between state and action from the agent’s experience helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. The proposed method is evaluated on the OpenAI ProcGen benchmark and shows significant improvement on generalization performance across most of Procgen games.
266,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,This paper proposes a method to regularize the representation feature space of deep reinforcement learning (RL) agents by using cross-state self-constraint (CSSC) to compare representation similarity across different pairs of states. The idea is that the implicit feedback between state and action from the agent’s experience helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. The proposed method is evaluated on the OpenAI ProcGen benchmark and shows significant improvement on generalization performance across most of Procgen games.
267,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting. The authors draw a connection between the adversarial attack and the influence maximization problem on the graph, and propose a group of attack strategies. The experiments verify that the proposed strategies significantly degrade the performance of three popular GNN models."
268,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting. The authors draw a connection between the adversarial attack and the influence maximization problem on the graph, and propose a group of attack strategies. The experiments verify that the proposed strategies significantly degrade the performance of three popular GNN models."
269,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting. The authors draw a connection between the adversarial attack and the influence maximization problem on the graph, and propose a group of attack strategies. The experiments verify that the proposed strategies significantly degrade the performance of three popular GNN models."
270,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper proposes a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to mitigate the problem of sparse graph learning. The authors demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low-rank assumption. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
271,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper proposes a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to mitigate the problem of sparse graph learning. The authors demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low-rank assumption. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
272,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper proposes a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to mitigate the problem of sparse graph learning. The authors demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low-rank assumption. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
273,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML) that jointly optimizes automated data augmentation (DA), neural architecture search (NAS) and hyper-parameter optimization (HPO). The proposed method performs co-optimization of the neural architectures, training hyperparameters, and data augmentations policies in an end to end fashion without the need of model retraining. Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet compared with multi-stage AutoML algorithms with higher computational efficiency."
274,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML) that jointly optimizes automated data augmentation (DA), neural architecture search (NAS) and hyper-parameter optimization (HPO). The proposed method performs co-optimization of the neural architectures, training hyperparameters, and data augmentations policies in an end to end fashion without the need of model retraining. Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet compared with multi-stage AutoML algorithms with higher computational efficiency."
275,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML) that jointly optimizes automated data augmentation (DA), neural architecture search (NAS) and hyper-parameter optimization (HPO). The proposed method performs co-optimization of the neural architectures, training hyperparameters, and data augmentations policies in an end to end fashion without the need of model retraining. Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet compared with multi-stage AutoML algorithms with higher computational efficiency."
276,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends prior networks to regression tasks by considering the Normal-Wishart distribution. Prior Networks are a class of models that yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via Ensemble Distribution Distillation (EnD), such that its accuracy, calibration, and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks."
277,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends prior networks to regression tasks by considering the Normal-Wishart distribution. Prior Networks are a class of models that yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via Ensemble Distribution Distillation (EnD), such that its accuracy, calibration, and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks."
278,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends prior networks to regression tasks by considering the Normal-Wishart distribution. Prior Networks are a class of models that yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via Ensemble Distribution Distillation (EnD), such that its accuracy, calibration, and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks."
279,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,"This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The proposed method is based on the popular gradient-based model-agnostic meta learning algorithm (MAML), which is a typical algorithm that suffers from these limitations. The authors propose to incorporate MAML into a bayesian online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed method can effectively prevent catastrophic forgetting."
280,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,"This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The proposed method is based on the popular gradient-based model-agnostic meta learning algorithm (MAML), which is a typical algorithm that suffers from these limitations. The authors propose to incorporate MAML into a bayesian online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed method can effectively prevent catastrophic forgetting."
281,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,"This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The proposed method is based on the popular gradient-based model-agnostic meta learning algorithm (MAML), which is a typical algorithm that suffers from these limitations. The authors propose to incorporate MAML into a bayesian online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed method can effectively prevent catastrophic forgetting."
282,SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. The authors show that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, the authors prove that, in several cases, RNP-GNNs can greatly reduce computational complexity compared to the existing higher-order kGNN and Local Relational Pooling (LRP) networks."
283,SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. The authors show that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, the authors prove that, in several cases, RNP-GNNs can greatly reduce computational complexity compared to the existing higher-order kGNN and Local Relational Pooling (LRP) networks."
284,SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. The authors show that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, the authors prove that, in several cases, RNP-GNNs can greatly reduce computational complexity compared to the existing higher-order kGNN and Local Relational Pooling (LRP) networks."
285,SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the behavior of multi-agent learning algorithms in normal-form games. The authors extend the volume-expansion argument of Cheung & Piliouras via the canonical game decomposition into zero-sum and coordination components. They characterize bimatrix games where MWU, its optimistic variant (OMWU) or Follow-the-Regularized-Leader (FTRL) algorithms are Lyapunov chaotic almost everywhere in the cumulative payoff space. For multi-player games, they present a local equivalence of volume change between general games and graphical games."
286,SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the behavior of multi-agent learning algorithms in normal-form games. The authors extend the volume-expansion argument of Cheung & Piliouras via the canonical game decomposition into zero-sum and coordination components. They characterize bimatrix games where MWU, its optimistic variant (OMWU) or Follow-the-Regularized-Leader (FTRL) algorithms are Lyapunov chaotic almost everywhere in the cumulative payoff space. For multi-player games, they present a local equivalence of volume change between general games and graphical games."
287,SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the behavior of multi-agent learning algorithms in normal-form games. The authors extend the volume-expansion argument of Cheung & Piliouras via the canonical game decomposition into zero-sum and coordination components. They characterize bimatrix games where MWU, its optimistic variant (OMWU) or Follow-the-Regularized-Leader (FTRL) algorithms are Lyapunov chaotic almost everywhere in the cumulative payoff space. For multi-player games, they present a local equivalence of volume change between general games and graphical games."
288,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,This paper proposes a new adaptive algorithm for deep learning based on marginal regret bound minimization. The main idea is to design the proximal function of adaptive algorithms as a function of the marginal regret minimization objective. The authors show that the proposed algorithm achieves marginal optimality and can converge faster than existing adaptive algorithms in the long term. Experiments on MNIST and CIFAR-10 show the superiority of the proposed method.
289,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,This paper proposes a new adaptive algorithm for deep learning based on marginal regret bound minimization. The main idea is to design the proximal function of adaptive algorithms as a function of the marginal regret minimization objective. The authors show that the proposed algorithm achieves marginal optimality and can converge faster than existing adaptive algorithms in the long term. Experiments on MNIST and CIFAR-10 show the superiority of the proposed method.
290,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,This paper proposes a new adaptive algorithm for deep learning based on marginal regret bound minimization. The main idea is to design the proximal function of adaptive algorithms as a function of the marginal regret minimization objective. The authors show that the proposed algorithm achieves marginal optimality and can converge faster than existing adaptive algorithms in the long term. Experiments on MNIST and CIFAR-10 show the superiority of the proposed method.
291,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes expected quadratic utility maximization (EQUM) as a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The proposed EQUM has several interpretations, such as reward-constrained variance minimization and regularization, as well as agent utility maximisation. In addition, the computation of the EQUM is easier than that of existing mean variance RL methods, which require double sampling. In experiments, the proposed method shows the effectiveness of the proposed algorithm in benchmark setting of RL and financial data."
292,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes expected quadratic utility maximization (EQUM) as a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The proposed EQUM has several interpretations, such as reward-constrained variance minimization and regularization, as well as agent utility maximisation. In addition, the computation of the EQUM is easier than that of existing mean variance RL methods, which require double sampling. In experiments, the proposed method shows the effectiveness of the proposed algorithm in benchmark setting of RL and financial data."
293,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes expected quadratic utility maximization (EQUM) as a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The proposed EQUM has several interpretations, such as reward-constrained variance minimization and regularization, as well as agent utility maximisation. In addition, the computation of the EQUM is easier than that of existing mean variance RL methods, which require double sampling. In experiments, the proposed method shows the effectiveness of the proposed algorithm in benchmark setting of RL and financial data."
294,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes a method for multi-task learning with auxiliary tasks. The proposed method is based on implicit differentiation. The authors propose to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. Then, when no useful auxiliary task is known, the authors propose how to generate a meaningful, novel auxiliary task. Experiments on image segmentation and learning with attributes in the low data regime show that the proposed method consistently outperforms competing methods."
295,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes a method for multi-task learning with auxiliary tasks. The proposed method is based on implicit differentiation. The authors propose to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. Then, when no useful auxiliary task is known, the authors propose how to generate a meaningful, novel auxiliary task. Experiments on image segmentation and learning with attributes in the low data regime show that the proposed method consistently outperforms competing methods."
296,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes a method for multi-task learning with auxiliary tasks. The proposed method is based on implicit differentiation. The authors propose to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. Then, when no useful auxiliary task is known, the authors propose how to generate a meaningful, novel auxiliary task. Experiments on image segmentation and learning with attributes in the low data regime show that the proposed method consistently outperforms competing methods."
297,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,This paper proposes a new measure of uncertainty for neural machine translation (NMT) models. The proposed measure is based on the dropout uncertainty of the model. The authors evaluate the proposed measure on the task of German-English translation using WMT13 and Europarl.
298,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,This paper proposes a new measure of uncertainty for neural machine translation (NMT) models. The proposed measure is based on the dropout uncertainty of the model. The authors evaluate the proposed measure on the task of German-English translation using WMT13 and Europarl.
299,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,This paper proposes a new measure of uncertainty for neural machine translation (NMT) models. The proposed measure is based on the dropout uncertainty of the model. The authors evaluate the proposed measure on the task of German-English translation using WMT13 and Europarl.
300,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the problem of pruning neural networks at initialization. The authors compare the performance of SNIP, GraSP, SynFlow, and magnitude pruning after training. They show that random shuffling the weights within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune."
301,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the problem of pruning neural networks at initialization. The authors compare the performance of SNIP, GraSP, SynFlow, and magnitude pruning after training. They show that random shuffling the weights within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune."
302,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the problem of pruning neural networks at initialization. The authors compare the performance of SNIP, GraSP, SynFlow, and magnitude pruning after training. They show that random shuffling the weights within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune."
303,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper proposes a federated learning protocol that simultaneously protects against both a semi-honest server and Byzantine malicious clients. The authors propose to split the clients into shards, securely aggregate each shard’s updates and run FilterL2 on the updates from different shards. Besides, the authors also propose to use secure aggregation to protect the clients from Byzantine malicious server. The evaluation shows that FED-LEARNING consistently achieves optimal or close-to-optimal performance under three attacks."
304,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper proposes a federated learning protocol that simultaneously protects against both a semi-honest server and Byzantine malicious clients. The authors propose to split the clients into shards, securely aggregate each shard’s updates and run FilterL2 on the updates from different shards. Besides, the authors also propose to use secure aggregation to protect the clients from Byzantine malicious server. The evaluation shows that FED-LEARNING consistently achieves optimal or close-to-optimal performance under three attacks."
305,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper proposes a federated learning protocol that simultaneously protects against both a semi-honest server and Byzantine malicious clients. The authors propose to split the clients into shards, securely aggregate each shard’s updates and run FilterL2 on the updates from different shards. Besides, the authors also propose to use secure aggregation to protect the clients from Byzantine malicious server. The evaluation shows that FED-LEARNING consistently achieves optimal or close-to-optimal performance under three attacks."
306,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper presents a benchmark suite of offline MBO tasks with a unified evaluation protocol and reference implementations of recent methods. The benchmark suite includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics. "
307,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper presents a benchmark suite of offline MBO tasks with a unified evaluation protocol and reference implementations of recent methods. The benchmark suite includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics. "
308,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper presents a benchmark suite of offline MBO tasks with a unified evaluation protocol and reference implementations of recent methods. The benchmark suite includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics. "
309,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"This paper proposes a generative model for multi-modal data. The proposed model is based on a generalized ELBO formulation, which combines two previous methods as special cases and combines their benefits without compromises. The experimental results show the effectiveness of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks."
310,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"This paper proposes a generative model for multi-modal data. The proposed model is based on a generalized ELBO formulation, which combines two previous methods as special cases and combines their benefits without compromises. The experimental results show the effectiveness of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks."
311,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"This paper proposes a generative model for multi-modal data. The proposed model is based on a generalized ELBO formulation, which combines two previous methods as special cases and combines their benefits without compromises. The experimental results show the effectiveness of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks."
312,SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method for optimizing black-box functions. The idea is to use user priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). PrBO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. Experiments show that PrBO is around 12x faster than state-of-the-art methods without user prior and 10,000x faster that random search on a common suite of benchmarks. The paper also shows that the proposed method is robust to misleading priors."
313,SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method for optimizing black-box functions. The idea is to use user priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). PrBO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. Experiments show that PrBO is around 12x faster than state-of-the-art methods without user prior and 10,000x faster that random search on a common suite of benchmarks. The paper also shows that the proposed method is robust to misleading priors."
314,SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method for optimizing black-box functions. The idea is to use user priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). PrBO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. Experiments show that PrBO is around 12x faster than state-of-the-art methods without user prior and 10,000x faster that random search on a common suite of benchmarks. The paper also shows that the proposed method is robust to misleading priors."
315,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes to use binary neural networks to reduce the computational cost of deep generative models. Specifically, the authors propose a new class of binary weight normalization, and provide insights for architecture designs of these binarized models. The authors demonstrate that two state-of-the-art deep models, the ResNet VAE and Flow++ models, can be binarised effectively using these techniques. They train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time."
316,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes to use binary neural networks to reduce the computational cost of deep generative models. Specifically, the authors propose a new class of binary weight normalization, and provide insights for architecture designs of these binarized models. The authors demonstrate that two state-of-the-art deep models, the ResNet VAE and Flow++ models, can be binarised effectively using these techniques. They train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time."
317,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes to use binary neural networks to reduce the computational cost of deep generative models. Specifically, the authors propose a new class of binary weight normalization, and provide insights for architecture designs of these binarized models. The authors demonstrate that two state-of-the-art deep models, the ResNet VAE and Flow++ models, can be binarised effectively using these techniques. They train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time."
318,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning to address the problem of out-of-distribution shifts in the data distribution in a general context. Specifically, the authors propose three novel model based robust training algorithms that improve the robustness of DL with respect to natural variation. The main contribution of this paper is to obtain models of natural variation, which vary data over a range of natural conditions. Then by exploiting these models, the proposed three novel models can be used to improve the model robustness. The experiments show that the proposed algorithms outperform classifiers trained via ERM, adversarial training, and domain adaptation techniques."
319,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning to address the problem of out-of-distribution shifts in the data distribution in a general context. Specifically, the authors propose three novel model based robust training algorithms that improve the robustness of DL with respect to natural variation. The main contribution of this paper is to obtain models of natural variation, which vary data over a range of natural conditions. Then by exploiting these models, the proposed three novel models can be used to improve the model robustness. The experiments show that the proposed algorithms outperform classifiers trained via ERM, adversarial training, and domain adaptation techniques."
320,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning to address the problem of out-of-distribution shifts in the data distribution in a general context. Specifically, the authors propose three novel model based robust training algorithms that improve the robustness of DL with respect to natural variation. The main contribution of this paper is to obtain models of natural variation, which vary data over a range of natural conditions. Then by exploiting these models, the proposed three novel models can be used to improve the model robustness. The experiments show that the proposed algorithms outperform classifiers trained via ERM, adversarial training, and domain adaptation techniques."
321,SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the optimization of two-layer convolutional neural networks with ReLU activations. The authors propose a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two-and three-layer CNN architectures. The main result is that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an `1 regularized convex program that encourages sparsity in the spectral domain. Furthermore, the authors extend their approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers."
322,SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the optimization of two-layer convolutional neural networks with ReLU activations. The authors propose a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two-and three-layer CNN architectures. The main result is that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an `1 regularized convex program that encourages sparsity in the spectral domain. Furthermore, the authors extend their approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers."
323,SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the optimization of two-layer convolutional neural networks with ReLU activations. The authors propose a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two-and three-layer CNN architectures. The main result is that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an `1 regularized convex program that encourages sparsity in the spectral domain. Furthermore, the authors extend their approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers."
324,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a method for interpretable learning from demonstration data. The method is based on a probabilistic generative model that is trained on a set of demonstrations. The model is trained with the help of a latent variable that is aligned with high-level notions and concepts that are manifested in the demonstrations. This is achieved by using labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. The proposed method is evaluated in the context of table-top robot manipulation tasks performed by a PR2 robot."
325,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a method for interpretable learning from demonstration data. The method is based on a probabilistic generative model that is trained on a set of demonstrations. The model is trained with the help of a latent variable that is aligned with high-level notions and concepts that are manifested in the demonstrations. This is achieved by using labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. The proposed method is evaluated in the context of table-top robot manipulation tasks performed by a PR2 robot."
326,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a method for interpretable learning from demonstration data. The method is based on a probabilistic generative model that is trained on a set of demonstrations. The model is trained with the help of a latent variable that is aligned with high-level notions and concepts that are manifested in the demonstrations. This is achieved by using labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. The proposed method is evaluated in the context of table-top robot manipulation tasks performed by a PR2 robot."
327,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes a method for reducing overfitting in pretrained language models when fine-tuning on low-resource target tasks. Specifically, the authors propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features in the pretrained model during the fine tuning phase to reduce overfitting. The proposed method is evaluated on seven different tasks in different tasks and shows that the proposed method significantly improves transfer learning in low resource scenarios, surpassing prior work. "
328,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes a method for reducing overfitting in pretrained language models when fine-tuning on low-resource target tasks. Specifically, the authors propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features in the pretrained model during the fine tuning phase to reduce overfitting. The proposed method is evaluated on seven different tasks in different tasks and shows that the proposed method significantly improves transfer learning in low resource scenarios, surpassing prior work. "
329,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes a method for reducing overfitting in pretrained language models when fine-tuning on low-resource target tasks. Specifically, the authors propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features in the pretrained model during the fine tuning phase to reduce overfitting. The proposed method is evaluated on seven different tasks in different tasks and shows that the proposed method significantly improves transfer learning in low resource scenarios, surpassing prior work. "
330,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover 3D shape from a single 2D image in an unsupervised manner. The key idea is to use an off-the-shelf 2D GAN that is trained on RGB images only. The core of the method is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The proposed method does not require 2D keypoint or 3D annotations, and it successfully recovers 3D shapes with high precision for human faces, cars, buildings, etc."
331,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover 3D shape from a single 2D image in an unsupervised manner. The key idea is to use an off-the-shelf 2D GAN that is trained on RGB images only. The core of the method is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The proposed method does not require 2D keypoint or 3D annotations, and it successfully recovers 3D shapes with high precision for human faces, cars, buildings, etc."
332,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover 3D shape from a single 2D image in an unsupervised manner. The key idea is to use an off-the-shelf 2D GAN that is trained on RGB images only. The core of the method is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The proposed method does not require 2D keypoint or 3D annotations, and it successfully recovers 3D shapes with high precision for human faces, cars, buildings, etc."
333,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tailed classifier called RoutIng Diverse Experts (RIDE). It reduces model variance with multiple experts, reduces model bias with a distribution-aware diversity loss, reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks."
334,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tailed classifier called RoutIng Diverse Experts (RIDE). It reduces model variance with multiple experts, reduces model bias with a distribution-aware diversity loss, reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks."
335,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tailed classifier called RoutIng Diverse Experts (RIDE). It reduces model variance with multiple experts, reduces model bias with a distribution-aware diversity loss, reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks."
336,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper analyzes the effectiveness of different pruning criteria for channel pruning in CNNs. The authors point out two issues: (1) Similarity and (2) Applicability: The filters’ importance scores are too close to distinguish the network redundancy well. They also break some stereotypes, such as that the results of `1 and `2 pruning are not always similar. Based on the empirical experiments, the authors make the assumption that the well-trained convolutional filters in each layer approximately follow a Gaussian distribution."
337,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper analyzes the effectiveness of different pruning criteria for channel pruning in CNNs. The authors point out two issues: (1) Similarity and (2) Applicability: The filters’ importance scores are too close to distinguish the network redundancy well. They also break some stereotypes, such as that the results of `1 and `2 pruning are not always similar. Based on the empirical experiments, the authors make the assumption that the well-trained convolutional filters in each layer approximately follow a Gaussian distribution."
338,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper analyzes the effectiveness of different pruning criteria for channel pruning in CNNs. The authors point out two issues: (1) Similarity and (2) Applicability: The filters’ importance scores are too close to distinguish the network redundancy well. They also break some stereotypes, such as that the results of `1 and `2 pruning are not always similar. Based on the empirical experiments, the authors make the assumption that the well-trained convolutional filters in each layer approximately follow a Gaussian distribution."
339,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for code search, code completion, code summarization, and code refinement. The main contribution of the paper is to use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of “wherethe-value-comes-from” between variables. Instead of taking syntactic-level structures of code like abstract syntax tree (AST), the paper proposes to take data flow as a semantic representation of code. The paper also introduces two structure-aware pretraining tasks: one is to predict code structure edges, and the other is to align representations between source code and code structure. The experimental results show that the proposed model outperforms the state-of-the-art on four downstream tasks."
340,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for code search, code completion, code summarization, and code refinement. The main contribution of the paper is to use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of “wherethe-value-comes-from” between variables. Instead of taking syntactic-level structures of code like abstract syntax tree (AST), the paper proposes to take data flow as a semantic representation of code. The paper also introduces two structure-aware pretraining tasks: one is to predict code structure edges, and the other is to align representations between source code and code structure. The experimental results show that the proposed model outperforms the state-of-the-art on four downstream tasks."
341,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for code search, code completion, code summarization, and code refinement. The main contribution of the paper is to use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of “wherethe-value-comes-from” between variables. Instead of taking syntactic-level structures of code like abstract syntax tree (AST), the paper proposes to take data flow as a semantic representation of code. The paper also introduces two structure-aware pretraining tasks: one is to predict code structure edges, and the other is to align representations between source code and code structure. The experimental results show that the proposed model outperforms the state-of-the-art on four downstream tasks."
342,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The authors assume that the existence of enough unlabeled data that follow the true distribution, and that the estimated true distribution can be roughly estimated from domain knowledge or a few samples. During training neural networks to generate a regression model, an adversarial network is used to force the distribution of predicted values to follow the estimated ‘true’ distribution. The forcing algorithm regularizes the regression results while keeping the information of the training data."
343,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The authors assume that the existence of enough unlabeled data that follow the true distribution, and that the estimated true distribution can be roughly estimated from domain knowledge or a few samples. During training neural networks to generate a regression model, an adversarial network is used to force the distribution of predicted values to follow the estimated ‘true’ distribution. The forcing algorithm regularizes the regression results while keeping the information of the training data."
344,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The authors assume that the existence of enough unlabeled data that follow the true distribution, and that the estimated true distribution can be roughly estimated from domain knowledge or a few samples. During training neural networks to generate a regression model, an adversarial network is used to force the distribution of predicted values to follow the estimated ‘true’ distribution. The forcing algorithm regularizes the regression results while keeping the information of the training data."
345,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of compositional generalization. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, the authors propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than 20% absolute increase in various experiments compared with baselines."
346,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of compositional generalization. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, the authors propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than 20% absolute increase in various experiments compared with baselines."
347,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of compositional generalization. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, the authors propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than 20% absolute increase in various experiments compared with baselines."
348,SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper proposes a new poisoning method for online reinforcement learning (RL) based on the idea of stability radius in RL. The authors propose a new metric, stability radius, to measure the vulnerability of RL algorithms. They also propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. Experiments show that the proposed method successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget."
349,SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper proposes a new poisoning method for online reinforcement learning (RL) based on the idea of stability radius in RL. The authors propose a new metric, stability radius, to measure the vulnerability of RL algorithms. They also propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. Experiments show that the proposed method successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget."
350,SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper proposes a new poisoning method for online reinforcement learning (RL) based on the idea of stability radius in RL. The authors propose a new metric, stability radius, to measure the vulnerability of RL algorithms. They also propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. Experiments show that the proposed method successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget."
351,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper proposes Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only O(N) tensor operations. The paper shows that DTR closely matches the performance of optimal static checkpointing in simulated experiments."
352,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper proposes Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only O(N) tensor operations. The paper shows that DTR closely matches the performance of optimal static checkpointing in simulated experiments."
353,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper proposes Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only O(N) tensor operations. The paper shows that DTR closely matches the performance of optimal static checkpointing in simulated experiments."
354,SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task to detect whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. The authors also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach."
355,SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task to detect whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. The authors also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach."
356,SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task to detect whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. The authors also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach."
357,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a new method for conditional generative adversarial networks (cGAN) that uses NAS to find a different architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data. The proposed method follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability."
358,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a new method for conditional generative adversarial networks (cGAN) that uses NAS to find a different architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data. The proposed method follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability."
359,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a new method for conditional generative adversarial networks (cGAN) that uses NAS to find a different architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data. The proposed method follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability."
360,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a novel regularization framework for causal inference. The authors formalize unconfoundedness as an orthogonality constraint, and provide theoretical guarantees that this yields an asymptotically normal estimator for the average causal effect. Based on the theoretical guarantees, the authors develop deep orthogonal networks for un-confounded treatments (DONUT) which learn outcomes that are orthongonal to the treatment assignment. The experimental results show that DONUT outperforms state-of-the-art substantially."
361,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a novel regularization framework for causal inference. The authors formalize unconfoundedness as an orthogonality constraint, and provide theoretical guarantees that this yields an asymptotically normal estimator for the average causal effect. Based on the theoretical guarantees, the authors develop deep orthogonal networks for un-confounded treatments (DONUT) which learn outcomes that are orthongonal to the treatment assignment. The experimental results show that DONUT outperforms state-of-the-art substantially."
362,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a novel regularization framework for causal inference. The authors formalize unconfoundedness as an orthogonality constraint, and provide theoretical guarantees that this yields an asymptotically normal estimator for the average causal effect. Based on the theoretical guarantees, the authors develop deep orthogonal networks for un-confounded treatments (DONUT) which learn outcomes that are orthongonal to the treatment assignment. The experimental results show that DONUT outperforms state-of-the-art substantially."
363,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the expressive power of affine parameters used to transform features in deep learning. Specifically, the authors investigate the performance achieved when training only these parameters in BatchNorm and freezing all weights at their random initializations. The authors show that BatchNets reach 82% (CIFAR-10 and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. "
364,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the expressive power of affine parameters used to transform features in deep learning. Specifically, the authors investigate the performance achieved when training only these parameters in BatchNorm and freezing all weights at their random initializations. The authors show that BatchNets reach 82% (CIFAR-10 and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. "
365,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the expressive power of affine parameters used to transform features in deep learning. Specifically, the authors investigate the performance achieved when training only these parameters in BatchNorm and freezing all weights at their random initializations. The authors show that BatchNets reach 82% (CIFAR-10 and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. "
366,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a test entropy minimization (tent) method for fully test-time adaptation. Tent estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark."
367,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a test entropy minimization (tent) method for fully test-time adaptation. Tent estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark."
368,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a test entropy minimization (tent) method for fully test-time adaptation. Tent estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark."
369,SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. The proposed method can be used to learn deterministic and probabilistic automata from data, learn well-calibrated models on real-world classification tasks, improve the performance of out-of-distribution detection, and control the explorationexploitation trade-off in reinforcement learning."
370,SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. The proposed method can be used to learn deterministic and probabilistic automata from data, learn well-calibrated models on real-world classification tasks, improve the performance of out-of-distribution detection, and control the explorationexploitation trade-off in reinforcement learning."
371,SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. The proposed method can be used to learn deterministic and probabilistic automata from data, learn well-calibrated models on real-world classification tasks, improve the performance of out-of-distribution detection, and control the explorationexploitation trade-off in reinforcement learning."
372,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper proposes a new method for privacy-preserving deep learning. The method is based on residual perturbation, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms the state-of-the-art DP stochastic gradient descent (DPSGD) in both membership privacy protection and utility."
373,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper proposes a new method for privacy-preserving deep learning. The method is based on residual perturbation, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms the state-of-the-art DP stochastic gradient descent (DPSGD) in both membership privacy protection and utility."
374,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper proposes a new method for privacy-preserving deep learning. The method is based on residual perturbation, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms the state-of-the-art DP stochastic gradient descent (DPSGD) in both membership privacy protection and utility."
375,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of the PoWER-BERT model to address the issue of inefficiency and redundancy. The main idea is to train a large-scale transformer, called Length-Adaptive Transformer, once and use it for various inference scenarios without re-training it. To do so, the authors train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. They then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. Additionally, they extend the applicability of the proposed extension beyond sequence-level classification into token-level recognition such as span-based question-answering."
376,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of the PoWER-BERT model to address the issue of inefficiency and redundancy. The main idea is to train a large-scale transformer, called Length-Adaptive Transformer, once and use it for various inference scenarios without re-training it. To do so, the authors train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. They then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. Additionally, they extend the applicability of the proposed extension beyond sequence-level classification into token-level recognition such as span-based question-answering."
377,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of the PoWER-BERT model to address the issue of inefficiency and redundancy. The main idea is to train a large-scale transformer, called Length-Adaptive Transformer, once and use it for various inference scenarios without re-training it. To do so, the authors train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. They then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. Additionally, they extend the applicability of the proposed extension beyond sequence-level classification into token-level recognition such as span-based question-answering."
378,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressiveness of neighborhood aggregation GNNs. The authors reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficient matrices for building more powerful aggregators and even injective aggregators. Based on the theoretical analysis, the authors develop two GNN layers, ExpandingConv and CombConv. Experimental results show that the proposed models significantly boost performance, especially for large and densely connected graphs."
379,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressiveness of neighborhood aggregation GNNs. The authors reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficient matrices for building more powerful aggregators and even injective aggregators. Based on the theoretical analysis, the authors develop two GNN layers, ExpandingConv and CombConv. Experimental results show that the proposed models significantly boost performance, especially for large and densely connected graphs."
380,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressiveness of neighborhood aggregation GNNs. The authors reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficient matrices for building more powerful aggregators and even injective aggregators. Based on the theoretical analysis, the authors develop two GNN layers, ExpandingConv and CombConv. Experimental results show that the proposed models significantly boost performance, especially for large and densely connected graphs."
381,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"This paper proposes a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. Experiments are conducted on several state-of-the-art models across multiple datasets."
382,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"This paper proposes a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. Experiments are conducted on several state-of-the-art models across multiple datasets."
383,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"This paper proposes a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. Experiments are conducted on several state-of-the-art models across multiple datasets."
384,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training examples unlearnable for deep learning models. The proposed method is based on the idea of error-minimizing noise, which is intentionally generated to reduce the error of one or more of the training examples close to zero, which can trick the model into believing there is “nothing” to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. The paper empirically verify the effectiveness of the proposed method in both sample-wise and class-wise forms."
385,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training examples unlearnable for deep learning models. The proposed method is based on the idea of error-minimizing noise, which is intentionally generated to reduce the error of one or more of the training examples close to zero, which can trick the model into believing there is “nothing” to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. The paper empirically verify the effectiveness of the proposed method in both sample-wise and class-wise forms."
386,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training examples unlearnable for deep learning models. The proposed method is based on the idea of error-minimizing noise, which is intentionally generated to reduce the error of one or more of the training examples close to zero, which can trick the model into believing there is “nothing” to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. The paper empirically verify the effectiveness of the proposed method in both sample-wise and class-wise forms."
387,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper proposes a novel extension of the MuZero algorithm, Nondeterministic MuZero (NDMZ), to the case of non-zero-sum games of perfect information. NDMZ formalizes chance as a player in the game and incorporates the chance player into MuZero network architecture and tree search. Experiments are conducted on the Atari suite and show the effectiveness of the proposed algorithm."
388,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper proposes a novel extension of the MuZero algorithm, Nondeterministic MuZero (NDMZ), to the case of non-zero-sum games of perfect information. NDMZ formalizes chance as a player in the game and incorporates the chance player into MuZero network architecture and tree search. Experiments are conducted on the Atari suite and show the effectiveness of the proposed algorithm."
389,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper proposes a novel extension of the MuZero algorithm, Nondeterministic MuZero (NDMZ), to the case of non-zero-sum games of perfect information. NDMZ formalizes chance as a player in the game and incorporates the chance player into MuZero network architecture and tree search. Experiments are conducted on the Atari suite and show the effectiveness of the proposed algorithm."
390,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes a method for option learning in reinforcement learning. The method is based on the Hindsight Off-policy Options (HO2) algorithm. The main idea of HO2 is to use a dynamic programming inference procedure to train the policy components for every time-step. The authors show that HO2 outperforms existing option learning methods and that both action and temporal abstraction provide strong benefits, particularly in more demanding simulated robot manipulation tasks."
391,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes a method for option learning in reinforcement learning. The method is based on the Hindsight Off-policy Options (HO2) algorithm. The main idea of HO2 is to use a dynamic programming inference procedure to train the policy components for every time-step. The authors show that HO2 outperforms existing option learning methods and that both action and temporal abstraction provide strong benefits, particularly in more demanding simulated robot manipulation tasks."
392,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes a method for option learning in reinforcement learning. The method is based on the Hindsight Off-policy Options (HO2) algorithm. The main idea of HO2 is to use a dynamic programming inference procedure to train the policy components for every time-step. The authors show that HO2 outperforms existing option learning methods and that both action and temporal abstraction provide strong benefits, particularly in more demanding simulated robot manipulation tasks."
393,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel functional form of the Bellman equation, which is motivated by the fact that it is not necessary to optimize for the expected cumulative return (ECR) in reinforcement learning (RL) algorithms. The authors propose a new objective function to maximize the expected maximum reward along a trajectory, and introduce the corresponding Bellman operators, and provide a proof of convergence. The proposed objective function is evaluated on the synthesizable molecule generation task, and achieves state-of-the-art results."
394,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel functional form of the Bellman equation, which is motivated by the fact that it is not necessary to optimize for the expected cumulative return (ECR) in reinforcement learning (RL) algorithms. The authors propose a new objective function to maximize the expected maximum reward along a trajectory, and introduce the corresponding Bellman operators, and provide a proof of convergence. The proposed objective function is evaluated on the synthesizable molecule generation task, and achieves state-of-the-art results."
395,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel functional form of the Bellman equation, which is motivated by the fact that it is not necessary to optimize for the expected cumulative return (ECR) in reinforcement learning (RL) algorithms. The authors propose a new objective function to maximize the expected maximum reward along a trajectory, and introduce the corresponding Bellman operators, and provide a proof of convergence. The proposed objective function is evaluated on the synthesizable molecule generation task, and achieves state-of-the-art results."
396,SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model which generates parameters for extending the base model to a new feature, by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. To assess the performance of CHN, the authors use a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data. The authors show that this system obtains improved few-shot learning performance for novel features over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks."
397,SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model which generates parameters for extending the base model to a new feature, by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. To assess the performance of CHN, the authors use a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data. The authors show that this system obtains improved few-shot learning performance for novel features over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks."
398,SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model which generates parameters for extending the base model to a new feature, by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. To assess the performance of CHN, the authors use a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data. The authors show that this system obtains improved few-shot learning performance for novel features over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks."
399,SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method for Bayesian deep learning that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The main idea is to perform inference over only a small subset of the model parameters while keeping all others as point estimates. This enables us to use expressive posterior approximations that would otherwise be intractable for the full model. The method is evaluated on MNIST and CIFAR-10 datasets."
400,SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method for Bayesian deep learning that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The main idea is to perform inference over only a small subset of the model parameters while keeping all others as point estimates. This enables us to use expressive posterior approximations that would otherwise be intractable for the full model. The method is evaluated on MNIST and CIFAR-10 datasets."
401,SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method for Bayesian deep learning that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The main idea is to perform inference over only a small subset of the model parameters while keeping all others as point estimates. This enables us to use expressive posterior approximations that would otherwise be intractable for the full model. The method is evaluated on MNIST and CIFAR-10 datasets."
402,SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a method for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the proposed method uses a model of the environment to obtain an embedding for state and actions and presents a generic architecture that uses these to learn a policy. The proposed method is evaluated on several games, robotic control, and recommender systems."
403,SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a method for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the proposed method uses a model of the environment to obtain an embedding for state and actions and presents a generic architecture that uses these to learn a policy. The proposed method is evaluated on several games, robotic control, and recommender systems."
404,SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a method for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the proposed method uses a model of the environment to obtain an embedding for state and actions and presents a generic architecture that uses these to learn a policy. The proposed method is evaluated on several games, robotic control, and recommender systems."
405,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a viewmaker network for unsupervised representation learning. Viewmaker is a generative model that generates views by generating and then adding an `p-bounded perturbation to the input, and is trained adversarially with respect to the main encoder network. Experiments on CIFAR-10 show that the learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations—despite not including transformations like cropping or color jitter. Furthermore, the proposed viewmaker networks outperform baseline augmentations on speech recordings (+9 points on average) and wearable sensor data (+17 points)."
406,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a viewmaker network for unsupervised representation learning. Viewmaker is a generative model that generates views by generating and then adding an `p-bounded perturbation to the input, and is trained adversarially with respect to the main encoder network. Experiments on CIFAR-10 show that the learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations—despite not including transformations like cropping or color jitter. Furthermore, the proposed viewmaker networks outperform baseline augmentations on speech recordings (+9 points on average) and wearable sensor data (+17 points)."
407,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a viewmaker network for unsupervised representation learning. Viewmaker is a generative model that generates views by generating and then adding an `p-bounded perturbation to the input, and is trained adversarially with respect to the main encoder network. Experiments on CIFAR-10 show that the learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations—despite not including transformations like cropping or color jitter. Furthermore, the proposed viewmaker networks outperform baseline augmentations on speech recordings (+9 points on average) and wearable sensor data (+17 points)."
408,SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. It then proposes a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. The experiments on CIFAR10, SVHN and MNIST demonstrate the effectiveness of the proposed method."
409,SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. It then proposes a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. The experiments on CIFAR10, SVHN and MNIST demonstrate the effectiveness of the proposed method."
410,SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. It then proposes a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. The experiments on CIFAR10, SVHN and MNIST demonstrate the effectiveness of the proposed method."
411,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a hierarchical VAE that can be used to generate samples quickly and outperform the PixelCNN in log-likelihood on all natural image benchmarks. The authors argue that VAEs can represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. They test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. "
412,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a hierarchical VAE that can be used to generate samples quickly and outperform the PixelCNN in log-likelihood on all natural image benchmarks. The authors argue that VAEs can represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. They test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. "
413,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a hierarchical VAE that can be used to generate samples quickly and outperform the PixelCNN in log-likelihood on all natural image benchmarks. The authors argue that VAEs can represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. They test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. "
414,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a new method for learning contrastive representations. The authors propose to sample negative examples conditionally in a ""ring"" around each positive. They prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. They show that choosing semi-hard negatives can yield stronger contrastive representation."
415,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a new method for learning contrastive representations. The authors propose to sample negative examples conditionally in a ""ring"" around each positive. They prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. They show that choosing semi-hard negatives can yield stronger contrastive representation."
416,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a new method for learning contrastive representations. The authors propose to sample negative examples conditionally in a ""ring"" around each positive. They prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. They show that choosing semi-hard negatives can yield stronger contrastive representation."
417,SP:613a0e2d8cbe703f37c182553801be7537333f64,This paper proposes a new method for recovering batch data from the shared aggregated gradients in federated learning (FL). The proposed method is called catastrophic data leakage in FL (CAFE). CAFE can recover large-batch data with high data recovery quality. Experimental results on vertical and horizontal FL settings validate the effectiveness of CAFE.
418,SP:613a0e2d8cbe703f37c182553801be7537333f64,This paper proposes a new method for recovering batch data from the shared aggregated gradients in federated learning (FL). The proposed method is called catastrophic data leakage in FL (CAFE). CAFE can recover large-batch data with high data recovery quality. Experimental results on vertical and horizontal FL settings validate the effectiveness of CAFE.
419,SP:613a0e2d8cbe703f37c182553801be7537333f64,This paper proposes a new method for recovering batch data from the shared aggregated gradients in federated learning (FL). The proposed method is called catastrophic data leakage in FL (CAFE). CAFE can recover large-batch data with high data recovery quality. Experimental results on vertical and horizontal FL settings validate the effectiveness of CAFE.
420,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for multi-agent relational inference. The model is based on the DYnamic multi-Agent Relational Inference (DYARI) framework. The main contribution of the paper is the introduction of dynamic relations, which is an important topic in the field. The paper is well-written and easy to follow."
421,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for multi-agent relational inference. The model is based on the DYnamic multi-Agent Relational Inference (DYARI) framework. The main contribution of the paper is the introduction of dynamic relations, which is an important topic in the field. The paper is well-written and easy to follow."
422,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for multi-agent relational inference. The model is based on the DYnamic multi-Agent Relational Inference (DYARI) framework. The main contribution of the paper is the introduction of dynamic relations, which is an important topic in the field. The paper is well-written and easy to follow."
423,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes a method for inductive learning of user-item rating matrices for collaborative filtering. The proposed method is based on the idea of learning a hidden relational graph among users from the rating matrix. The authors propose a relation inference model that estimates their underlying relations (as dense weighted graphs) to other users with respect to historical rating patterns. The relational graphs enable attentive message passing from users to users in the latent space and are updated in end-to-end manner. The key advantage of this method is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on several matrix completion benchmarks."
424,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes a method for inductive learning of user-item rating matrices for collaborative filtering. The proposed method is based on the idea of learning a hidden relational graph among users from the rating matrix. The authors propose a relation inference model that estimates their underlying relations (as dense weighted graphs) to other users with respect to historical rating patterns. The relational graphs enable attentive message passing from users to users in the latent space and are updated in end-to-end manner. The key advantage of this method is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on several matrix completion benchmarks."
425,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes a method for inductive learning of user-item rating matrices for collaborative filtering. The proposed method is based on the idea of learning a hidden relational graph among users from the rating matrix. The authors propose a relation inference model that estimates their underlying relations (as dense weighted graphs) to other users with respect to historical rating patterns. The relational graphs enable attentive message passing from users to users in the latent space and are updated in end-to-end manner. The key advantage of this method is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on several matrix completion benchmarks."
426,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage model for disentangled representation learning. First, a pre-trained autoencoder model is used to learn disentanglement, and then a deep generative model is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. The proposed method is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit model such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. The experimental results show that the proposed method has much higher reconstruction quality than current state-of-the-art methods with equivalent disentangle performance across multiple standard benchmarks."
427,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage model for disentangled representation learning. First, a pre-trained autoencoder model is used to learn disentanglement, and then a deep generative model is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. The proposed method is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit model such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. The experimental results show that the proposed method has much higher reconstruction quality than current state-of-the-art methods with equivalent disentangle performance across multiple standard benchmarks."
428,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage model for disentangled representation learning. First, a pre-trained autoencoder model is used to learn disentanglement, and then a deep generative model is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. The proposed method is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit model such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. The experimental results show that the proposed method has much higher reconstruction quality than current state-of-the-art methods with equivalent disentangle performance across multiple standard benchmarks."
429,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations for learning and representing the optimal policy in reinforcement learning (RL). The authors study several popular MI based objectives through this lens, and find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. They corroborate their theoretical results with empirical experiments on a simulated game environment with visual observations."
430,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations for learning and representing the optimal policy in reinforcement learning (RL). The authors study several popular MI based objectives through this lens, and find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. They corroborate their theoretical results with empirical experiments on a simulated game environment with visual observations."
431,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations for learning and representing the optimal policy in reinforcement learning (RL). The authors study several popular MI based objectives through this lens, and find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. They corroborate their theoretical results with empirical experiments on a simulated game environment with visual observations."
432,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. The authors show that the non-convex neural network learning problem is equivalent to a finite-dimensional convex copositive program. In particular, the authors provide the first algorithms for provably finding the global minimum of the vector output Neural Network training problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. "
433,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. The authors show that the non-convex neural network learning problem is equivalent to a finite-dimensional convex copositive program. In particular, the authors provide the first algorithms for provably finding the global minimum of the vector output Neural Network training problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. "
434,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. The authors show that the non-convex neural network learning problem is equivalent to a finite-dimensional convex copositive program. In particular, the authors provide the first algorithms for provably finding the global minimum of the vector output Neural Network training problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. "
435,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The key idea is to learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. The proposed method can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language."
436,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The key idea is to learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. The proposed method can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language."
437,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The key idea is to learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. The proposed method can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language."
438,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a method for link prediction based on rule-based reasoning. The proposed method is based on a combination of two existing approaches: (1) embedding and (2) reasoning based on rules. The main contribution of the paper is to combine the two approaches. The experimental results on FB15k, WN18, and FB15K-R show the effectiveness of the proposed method."
439,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a method for link prediction based on rule-based reasoning. The proposed method is based on a combination of two existing approaches: (1) embedding and (2) reasoning based on rules. The main contribution of the paper is to combine the two approaches. The experimental results on FB15k, WN18, and FB15K-R show the effectiveness of the proposed method."
440,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a method for link prediction based on rule-based reasoning. The proposed method is based on a combination of two existing approaches: (1) embedding and (2) reasoning based on rules. The main contribution of the paper is to combine the two approaches. The experimental results on FB15k, WN18, and FB15K-R show the effectiveness of the proposed method."
441,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a new architecture that combines declarative and procedural knowledge for video games. The main idea is to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources. The proposed architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
442,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a new architecture that combines declarative and procedural knowledge for video games. The main idea is to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources. The proposed architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
443,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a new architecture that combines declarative and procedural knowledge for video games. The main idea is to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources. The proposed architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
444,SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a neural module network architecture for video-grounded language tasks. The proposed architecture is based on the Visio-Linguistic Neural Module Network (VilNMN), which decomposes all language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Experiments show that VilNMN can achieve promising performance on video QA and video dialogues."
445,SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a neural module network architecture for video-grounded language tasks. The proposed architecture is based on the Visio-Linguistic Neural Module Network (VilNMN), which decomposes all language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Experiments show that VilNMN can achieve promising performance on video QA and video dialogues."
446,SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a neural module network architecture for video-grounded language tasks. The proposed architecture is based on the Visio-Linguistic Neural Module Network (VilNMN), which decomposes all language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Experiments show that VilNMN can achieve promising performance on video QA and video dialogues."
447,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variations of Policy-Space Response Oracles (PSRO), a framework for learning policies in multi-agent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). The first method, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy. The second method is Mixed-Opponents, which constructs a pure-strategy opponent by mixing existing strategy’‘s action-value estimates, instead of their policies. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy."
448,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variations of Policy-Space Response Oracles (PSRO), a framework for learning policies in multi-agent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). The first method, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy. The second method is Mixed-Opponents, which constructs a pure-strategy opponent by mixing existing strategy’‘s action-value estimates, instead of their policies. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy."
449,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variations of Policy-Space Response Oracles (PSRO), a framework for learning policies in multi-agent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). The first method, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy. The second method is Mixed-Opponents, which constructs a pure-strategy opponent by mixing existing strategy’‘s action-value estimates, instead of their policies. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy."
450,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper proposes a non-attentive text-to-speech model, which replaces the attention mechanism with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The paper also proposes a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
451,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper proposes a non-attentive text-to-speech model, which replaces the attention mechanism with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The paper also proposes a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
452,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper proposes a non-attentive text-to-speech model, which replaces the attention mechanism with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The paper also proposes a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
453,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for estimating bird’s eye view layout from a pair of stereo images. The main idea is to use inverse perspective mapping (IPM) to map the input images and their features to the bird's eye view. The proposed method is evaluated on two datasets: KITTI (Geiger et al., 2013) and a synthetically generated dataset from the CARLA (Dosovitskiy et al, 2017)."
454,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for estimating bird’s eye view layout from a pair of stereo images. The main idea is to use inverse perspective mapping (IPM) to map the input images and their features to the bird's eye view. The proposed method is evaluated on two datasets: KITTI (Geiger et al., 2013) and a synthetically generated dataset from the CARLA (Dosovitskiy et al, 2017)."
455,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for estimating bird’s eye view layout from a pair of stereo images. The main idea is to use inverse perspective mapping (IPM) to map the input images and their features to the bird's eye view. The proposed method is evaluated on two datasets: KITTI (Geiger et al., 2013) and a synthetically generated dataset from the CARLA (Dosovitskiy et al, 2017)."
456,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup.
457,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup.
458,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup.
459,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for learning minimal regions in an input that are most relevant for a neural network’s prediction. The method uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. After solving for the minimal masks, the method scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains “where a model is looking” when making a prediction."
460,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for learning minimal regions in an input that are most relevant for a neural network’s prediction. The method uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. After solving for the minimal masks, the method scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains “where a model is looking” when making a prediction."
461,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for learning minimal regions in an input that are most relevant for a neural network’s prediction. The method uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. After solving for the minimal masks, the method scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains “where a model is looking” when making a prediction."
462,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a method for 3D pose estimation that combines deep neural networks with 3D generative representations of objects into a unified neural architecture that is called NeMo. The main idea is to learn a generative model of neural feature activations at each vertex on a dense 3D mesh. To avoid local optima in the reconstruction loss, the feature extractor is trained to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on PASCAL3D+, occluded-PASCAL-3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion and unseen pose compared to standard deep networks, while retaining competitive performance on regular data. Interestingly, NeMo performs reasonably well even when the mesh representation only crudely approximates the true object geometry with a cuboid."
463,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a method for 3D pose estimation that combines deep neural networks with 3D generative representations of objects into a unified neural architecture that is called NeMo. The main idea is to learn a generative model of neural feature activations at each vertex on a dense 3D mesh. To avoid local optima in the reconstruction loss, the feature extractor is trained to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on PASCAL3D+, occluded-PASCAL-3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion and unseen pose compared to standard deep networks, while retaining competitive performance on regular data. Interestingly, NeMo performs reasonably well even when the mesh representation only crudely approximates the true object geometry with a cuboid."
464,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a method for 3D pose estimation that combines deep neural networks with 3D generative representations of objects into a unified neural architecture that is called NeMo. The main idea is to learn a generative model of neural feature activations at each vertex on a dense 3D mesh. To avoid local optima in the reconstruction loss, the feature extractor is trained to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on PASCAL3D+, occluded-PASCAL-3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion and unseen pose compared to standard deep networks, while retaining competitive performance on regular data. Interestingly, NeMo performs reasonably well even when the mesh representation only crudely approximates the true object geometry with a cuboid."
465,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes a method for feature compatible learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning (NIST). NIST requires only features extracted by the old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for NIST, and extend it to handle the case where the old models is a black-box. Specifically, they learn a simple pseudo classifier in lieu of the old, and further enhance it with a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
466,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes a method for feature compatible learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning (NIST). NIST requires only features extracted by the old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for NIST, and extend it to handle the case where the old models is a black-box. Specifically, they learn a simple pseudo classifier in lieu of the old, and further enhance it with a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
467,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes a method for feature compatible learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning (NIST). NIST requires only features extracted by the old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for NIST, and extend it to handle the case where the old models is a black-box. Specifically, they learn a simple pseudo classifier in lieu of the old, and further enhance it with a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
468,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. The authors propose to use an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200∼20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyperparameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). "
469,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. The authors propose to use an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200∼20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyperparameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). "
470,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. The authors propose to use an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200∼20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyperparameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). "
471,SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes an autoregressive method for entity retrieval. The key idea is to generate unique entity names, left to right, token-by-token in an auto-regressive fashion and conditioned on the context. This enables the model to directly capture relations between context and entity name, effectively cross encoding both; the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; and the exact softmax loss can be efficiently computed without the need to subsample negative data. Experiments on entity disambiguation, end-to-end entity linking and document retrieval tasks demonstrate the effectiveness of the proposed method."
472,SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes an autoregressive method for entity retrieval. The key idea is to generate unique entity names, left to right, token-by-token in an auto-regressive fashion and conditioned on the context. This enables the model to directly capture relations between context and entity name, effectively cross encoding both; the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; and the exact softmax loss can be efficiently computed without the need to subsample negative data. Experiments on entity disambiguation, end-to-end entity linking and document retrieval tasks demonstrate the effectiveness of the proposed method."
473,SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes an autoregressive method for entity retrieval. The key idea is to generate unique entity names, left to right, token-by-token in an auto-regressive fashion and conditioned on the context. This enables the model to directly capture relations between context and entity name, effectively cross encoding both; the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; and the exact softmax loss can be efficiently computed without the need to subsample negative data. Experiments on entity disambiguation, end-to-end entity linking and document retrieval tasks demonstrate the effectiveness of the proposed method."
474,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The routing requests are supplied adversarially. On each time step t, the algorithm receives a routing request and must select a valid path and incur a cost ce = fe(x t e) + η t e, where x t e is the flow on edge e at time t, fe is the congestion function, and η e is a noise sample drawn from an unknown distribution. The algorithm observes ce, and can use this observation in future routing decisions."
475,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The routing requests are supplied adversarially. On each time step t, the algorithm receives a routing request and must select a valid path and incur a cost ce = fe(x t e) + η t e, where x t e is the flow on edge e at time t, fe is the congestion function, and η e is a noise sample drawn from an unknown distribution. The algorithm observes ce, and can use this observation in future routing decisions."
476,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The routing requests are supplied adversarially. On each time step t, the algorithm receives a routing request and must select a valid path and incur a cost ce = fe(x t e) + η t e, where x t e is the flow on edge e at time t, fe is the congestion function, and η e is a noise sample drawn from an unknown distribution. The algorithm observes ce, and can use this observation in future routing decisions."
477,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a novel masking strategy based on Pointwise mutual information (PMI) that jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking such as whole-word masking, entity/phrase masking and random-span masking. The authors show experimentally that the proposed method achieves the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training."
478,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a novel masking strategy based on Pointwise mutual information (PMI) that jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking such as whole-word masking, entity/phrase masking and random-span masking. The authors show experimentally that the proposed method achieves the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training."
479,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a novel masking strategy based on Pointwise mutual information (PMI) that jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking such as whole-word masking, entity/phrase masking and random-span masking. The authors show experimentally that the proposed method achieves the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training."
480,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the problem of amortized inference for sequential latent variable models (LVMs) with the evidence lower bound (ELBO). The authors show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. This mimics the Bayesian filter. The authors demonstrate the theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics."
481,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the problem of amortized inference for sequential latent variable models (LVMs) with the evidence lower bound (ELBO). The authors show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. This mimics the Bayesian filter. The authors demonstrate the theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics."
482,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the problem of amortized inference for sequential latent variable models (LVMs) with the evidence lower bound (ELBO). The authors show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. This mimics the Bayesian filter. The authors demonstrate the theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics."
483,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the generalization properties of distributed kernel ridge regression together with random features (DKRR-RF), and provides optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the authors first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(|D|) memory and O(\sqrt{D}) time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, they derive generalization bound in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance."
484,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the generalization properties of distributed kernel ridge regression together with random features (DKRR-RF), and provides optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the authors first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(|D|) memory and O(\sqrt{D}) time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, they derive generalization bound in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance."
485,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the generalization properties of distributed kernel ridge regression together with random features (DKRR-RF), and provides optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the authors first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(|D|) memory and O(\sqrt{D}) time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, they derive generalization bound in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance."
486,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address the problem of finding an effective proxy search space (PS) that is only a small subset of the global search space. Specifically, the authors observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20% according to the ground truth) in the GS, such a correlation drops dramatically. This raises the question of whether we can find an effective Proxy Search space that only consists of a small portion of the GS to improve the search efficiency while at the same time keeping a good correlation for the top performing architectures. The authors empirically show that EPS can achieve near optimal NAS performance and surpass all existing state-of-the-art."
487,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address the problem of finding an effective proxy search space (PS) that is only a small subset of the global search space. Specifically, the authors observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20% according to the ground truth) in the GS, such a correlation drops dramatically. This raises the question of whether we can find an effective Proxy Search space that only consists of a small portion of the GS to improve the search efficiency while at the same time keeping a good correlation for the top performing architectures. The authors empirically show that EPS can achieve near optimal NAS performance and surpass all existing state-of-the-art."
488,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address the problem of finding an effective proxy search space (PS) that is only a small subset of the global search space. Specifically, the authors observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20% according to the ground truth) in the GS, such a correlation drops dramatically. This raises the question of whether we can find an effective Proxy Search space that only consists of a small portion of the GS to improve the search efficiency while at the same time keeping a good correlation for the top performing architectures. The authors empirically show that EPS can achieve near optimal NAS performance and surpass all existing state-of-the-art."
489,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). The authors propose to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. The authors show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards."
490,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). The authors propose to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. The authors show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards."
491,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). The authors propose to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. The authors show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards."
492,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of overparameterized convolutional neural networks (CNNs) for image classification. In particular, the authors study the phenomenon of ""Pattern Statistics Inductive Bias"" (PSI), where the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. The authors prove that in this setting, if a learning algorithm satisfies PSI then its sample complexity is O(d log(d)) where d is the filter dimension. In contrast, the VC dimension lower bound is exponential in d."
493,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of overparameterized convolutional neural networks (CNNs) for image classification. In particular, the authors study the phenomenon of ""Pattern Statistics Inductive Bias"" (PSI), where the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. The authors prove that in this setting, if a learning algorithm satisfies PSI then its sample complexity is O(d log(d)) where d is the filter dimension. In contrast, the VC dimension lower bound is exponential in d."
494,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of overparameterized convolutional neural networks (CNNs) for image classification. In particular, the authors study the phenomenon of ""Pattern Statistics Inductive Bias"" (PSI), where the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. The authors prove that in this setting, if a learning algorithm satisfies PSI then its sample complexity is O(d log(d)) where d is the filter dimension. In contrast, the VC dimension lower bound is exponential in d."
495,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper proposes a contrastive learning method for document classification under topic modeling assumptions. The proposed method is based on the idea that similar and dissimilar pairs of data points can be used to learn a representation of documents that reveals their underlying topic posterior information to linear models. Experiments are conducted on a semi-supervised dataset and show that the proposed method outperforms the baseline methods.
496,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper proposes a contrastive learning method for document classification under topic modeling assumptions. The proposed method is based on the idea that similar and dissimilar pairs of data points can be used to learn a representation of documents that reveals their underlying topic posterior information to linear models. Experiments are conducted on a semi-supervised dataset and show that the proposed method outperforms the baseline methods.
497,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper proposes a contrastive learning method for document classification under topic modeling assumptions. The proposed method is based on the idea that similar and dissimilar pairs of data points can be used to learn a representation of documents that reveals their underlying topic posterior information to linear models. Experiments are conducted on a semi-supervised dataset and show that the proposed method outperforms the baseline methods.
498,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multimodal generative models. The main idea is to train the generative model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multi-modal data. Experiments show that the proposed method enables data-efficient multi-odal learning on challenging datasets."
499,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multimodal generative models. The main idea is to train the generative model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multi-modal data. Experiments show that the proposed method enables data-efficient multi-odal learning on challenging datasets."
500,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multimodal generative models. The main idea is to train the generative model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multi-modal data. Experiments show that the proposed method enables data-efficient multi-odal learning on challenging datasets."
501,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes a method to improve the generative quality of VAEs by adding a reweighting factor to the prior distribution. The reweighted prior is based on the energy-based prior, which is the product of a base prior distribution and a re-weighted factor, designed to bring the base closer to the aggregate posterior. The proposed method is trained using noise contrastive estimation, and generalizes it to hierarchical VAEs with many latent variable groups. Experiments are conducted on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets."
502,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes a method to improve the generative quality of VAEs by adding a reweighting factor to the prior distribution. The reweighted prior is based on the energy-based prior, which is the product of a base prior distribution and a re-weighted factor, designed to bring the base closer to the aggregate posterior. The proposed method is trained using noise contrastive estimation, and generalizes it to hierarchical VAEs with many latent variable groups. Experiments are conducted on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets."
503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes a method to improve the generative quality of VAEs by adding a reweighting factor to the prior distribution. The reweighted prior is based on the energy-based prior, which is the product of a base prior distribution and a re-weighted factor, designed to bring the base closer to the aggregate posterior. The proposed method is trained using noise contrastive estimation, and generalizes it to hierarchical VAEs with many latent variable groups. Experiments are conducted on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets."
504,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper proposes a regularized inverse reinforcement learning (IRL) method that uses strongly convex regularizers to the learner’s policy in order to avoid degenerate solutions. The authors propose tractable solutions, and practical methods to obtain them, for regularized IRL. Theoretical analysis is provided to support the theoretical claims. Experiments are conducted on both discrete and continuous control tasks."
505,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper proposes a regularized inverse reinforcement learning (IRL) method that uses strongly convex regularizers to the learner’s policy in order to avoid degenerate solutions. The authors propose tractable solutions, and practical methods to obtain them, for regularized IRL. Theoretical analysis is provided to support the theoretical claims. Experiments are conducted on both discrete and continuous control tasks."
506,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper proposes a regularized inverse reinforcement learning (IRL) method that uses strongly convex regularizers to the learner’s policy in order to avoid degenerate solutions. The authors propose tractable solutions, and practical methods to obtain them, for regularized IRL. Theoretical analysis is provided to support the theoretical claims. Experiments are conducted on both discrete and continuous control tasks."
507,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. CLSR can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many to one translation, particularly with unbalanced training data."
508,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. CLSR can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many to one translation, particularly with unbalanced training data."
509,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. CLSR can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many to one translation, particularly with unbalanced training data."
510,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The authors split the data into uncertain and certain samples based on small loss criteria. To impose geometric constraints on the uncertain samples, the authors normalize them into the wasserstein ball centered on certain samples. Experimental results demonstrate that the proposed WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets, which have diverse noisy labels."
511,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The authors split the data into uncertain and certain samples based on small loss criteria. To impose geometric constraints on the uncertain samples, the authors normalize them into the wasserstein ball centered on certain samples. Experimental results demonstrate that the proposed WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets, which have diverse noisy labels."
512,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The authors split the data into uncertain and certain samples based on small loss criteria. To impose geometric constraints on the uncertain samples, the authors normalize them into the wasserstein ball centered on certain samples. Experimental results demonstrate that the proposed WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets, which have diverse noisy labels."
513,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of image classifiers. The method is based on Platt scaling, which is a popular method for estimating the probability of a given classifier outputting a set of labels. The authors propose to regularize the scores of unlikely classes that are added to the output of the classifier to ensure that the set of predicted labels contains the true label with a user-specified probability, such as 90%. The authors provide a finite-sample coverage guarantee for every model and dataset, and show that their method outperforms existing approaches."
514,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of image classifiers. The method is based on Platt scaling, which is a popular method for estimating the probability of a given classifier outputting a set of labels. The authors propose to regularize the scores of unlikely classes that are added to the output of the classifier to ensure that the set of predicted labels contains the true label with a user-specified probability, such as 90%. The authors provide a finite-sample coverage guarantee for every model and dataset, and show that their method outperforms existing approaches."
515,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of image classifiers. The method is based on Platt scaling, which is a popular method for estimating the probability of a given classifier outputting a set of labels. The authors propose to regularize the scores of unlikely classes that are added to the output of the classifier to ensure that the set of predicted labels contains the true label with a user-specified probability, such as 90%. The authors provide a finite-sample coverage guarantee for every model and dataset, and show that their method outperforms existing approaches."
516,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper proposes a scalable algorithm to compute Wasserstein-2 barycenters given sample access to the input measures, which are not restricted to being discrete. The authors employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. The theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach is provided."
517,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper proposes a scalable algorithm to compute Wasserstein-2 barycenters given sample access to the input measures, which are not restricted to being discrete. The authors employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. The theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach is provided."
518,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper proposes a scalable algorithm to compute Wasserstein-2 barycenters given sample access to the input measures, which are not restricted to being discrete. The authors employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. The theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach is provided."
519,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of learning to separate two low-dimensional submanifolds of the unit sphere. The main contribution of the paper is to prove that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L, and the number of i.i.d. samples from the manifolds is polynomials in L. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. The analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem."
520,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of learning to separate two low-dimensional submanifolds of the unit sphere. The main contribution of the paper is to prove that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L, and the number of i.i.d. samples from the manifolds is polynomials in L. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. The analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem."
521,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of learning to separate two low-dimensional submanifolds of the unit sphere. The main contribution of the paper is to prove that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L, and the number of i.i.d. samples from the manifolds is polynomials in L. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. The analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem."
522,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised methods. The proposed method, called advantage-weighted regression (AWR), consists of two standard supervised training steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The paper provides a theoretical motivation for AWR and analyzes its properties when incorporating the data from experience replay. Experimental results on a suite of standard OpenAI Gym benchmark tasks show that AWR achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms."
523,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised methods. The proposed method, called advantage-weighted regression (AWR), consists of two standard supervised training steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The paper provides a theoretical motivation for AWR and analyzes its properties when incorporating the data from experience replay. Experimental results on a suite of standard OpenAI Gym benchmark tasks show that AWR achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms."
524,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised methods. The proposed method, called advantage-weighted regression (AWR), consists of two standard supervised training steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The paper provides a theoretical motivation for AWR and analyzes its properties when incorporating the data from experience replay. Experimental results on a suite of standard OpenAI Gym benchmark tasks show that AWR achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms."
525,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method to learn the bitwidth of the quantized weights and the bit width of the layers by making the period of the sinusoidal regularizer a trainable parameter. The method is a gradient-based mechanism that jointly learns the quantised weights as well as the heterogeneous bitwidths. The authors show that WaveQ balances compute efficiency and accuracy, and provides a heterogenous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR10, MobileNet, ResNet-18, ResNets-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is also applicable to quantizing transformers and yields significant benefits."
526,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method to learn the bitwidth of the quantized weights and the bit width of the layers by making the period of the sinusoidal regularizer a trainable parameter. The method is a gradient-based mechanism that jointly learns the quantised weights as well as the heterogeneous bitwidths. The authors show that WaveQ balances compute efficiency and accuracy, and provides a heterogenous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR10, MobileNet, ResNet-18, ResNets-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is also applicable to quantizing transformers and yields significant benefits."
527,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method to learn the bitwidth of the quantized weights and the bit width of the layers by making the period of the sinusoidal regularizer a trainable parameter. The method is a gradient-based mechanism that jointly learns the quantised weights as well as the heterogeneous bitwidths. The authors show that WaveQ balances compute efficiency and accuracy, and provides a heterogenous bitwidth assignment for quantization of a large variety of deep networks (AlexNet, CIFAR10, MobileNet, ResNet-18, ResNets-20, SVHN, and VGG-11) that virtually preserves the accuracy. WaveQ is also applicable to quantizing transformers and yields significant benefits."
528,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper proposes a data augmentation method for neural machine translation by randomly replacing words or mixup with their aligned alternatives in another language. The proposed method is based on the idea that aligned word pairs appear in the same position of each other during training, which is helpful to form bilingual embeddings which are proved useful to provide a performance boost. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
529,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper proposes a data augmentation method for neural machine translation by randomly replacing words or mixup with their aligned alternatives in another language. The proposed method is based on the idea that aligned word pairs appear in the same position of each other during training, which is helpful to form bilingual embeddings which are proved useful to provide a performance boost. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
530,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper proposes a data augmentation method for neural machine translation by randomly replacing words or mixup with their aligned alternatives in another language. The proposed method is based on the idea that aligned word pairs appear in the same position of each other during training, which is helpful to form bilingual embeddings which are proved useful to provide a performance boost. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
531,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,This paper proposes a real-time contribution measurement method for federated learning. The contribution rate of each agent is estimated by considering the current round and the previous round to obtain the contribution rate. The authors compare the Shapley Value in game theory and the proposed method is more sensitive to both data quantity and data quality under the premise of maintaining real time. Experiments are conducted on Penn Treebank dataset to verify the effectiveness of the proposed algorithm.
532,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,This paper proposes a real-time contribution measurement method for federated learning. The contribution rate of each agent is estimated by considering the current round and the previous round to obtain the contribution rate. The authors compare the Shapley Value in game theory and the proposed method is more sensitive to both data quantity and data quality under the premise of maintaining real time. Experiments are conducted on Penn Treebank dataset to verify the effectiveness of the proposed algorithm.
533,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,This paper proposes a real-time contribution measurement method for federated learning. The contribution rate of each agent is estimated by considering the current round and the previous round to obtain the contribution rate. The authors compare the Shapley Value in game theory and the proposed method is more sensitive to both data quantity and data quality under the premise of maintaining real time. Experiments are conducted on Penn Treebank dataset to verify the effectiveness of the proposed algorithm.
534,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"This paper studies the problem of adversarially corrupted Bayesian networks. The authors propose a nearly linear time algorithm for this problem with a dimension-independent error guarantee. The algorithm and analysis are considerably simpler than those in previous work. The main contribution of this paper is to establish a direct connection between robust learning of Bayesian network and robust mean estimation. As a subroutine in the algorithm, the authors develop an algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples."
535,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"This paper studies the problem of adversarially corrupted Bayesian networks. The authors propose a nearly linear time algorithm for this problem with a dimension-independent error guarantee. The algorithm and analysis are considerably simpler than those in previous work. The main contribution of this paper is to establish a direct connection between robust learning of Bayesian network and robust mean estimation. As a subroutine in the algorithm, the authors develop an algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples."
536,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"This paper studies the problem of adversarially corrupted Bayesian networks. The authors propose a nearly linear time algorithm for this problem with a dimension-independent error guarantee. The algorithm and analysis are considerably simpler than those in previous work. The main contribution of this paper is to establish a direct connection between robust learning of Bayesian network and robust mean estimation. As a subroutine in the algorithm, the authors develop an algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples."
537,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-term visual planning. The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Empirically, the proposed method outperforms prior model-based approaches on challenging visual control tasks with sparse rewards and long term goals."
538,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-term visual planning. The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Empirically, the proposed method outperforms prior model-based approaches on challenging visual control tasks with sparse rewards and long term goals."
539,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-term visual planning. The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Empirically, the proposed method outperforms prior model-based approaches on challenging visual control tasks with sparse rewards and long term goals."
540,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model describing curation for image classification. The authors argue that the likelihood under the generative models closely matches the tempered likelihoods used in past work. This is extremely concerning, because if the model is accurate, Bayesian inference/decision theory is optimal, and any artificial changes to the posterior should harm performance."
541,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model describing curation for image classification. The authors argue that the likelihood under the generative models closely matches the tempered likelihoods used in past work. This is extremely concerning, because if the model is accurate, Bayesian inference/decision theory is optimal, and any artificial changes to the posterior should harm performance."
542,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model describing curation for image classification. The authors argue that the likelihood under the generative models closely matches the tempered likelihoods used in past work. This is extremely concerning, because if the model is accurate, Bayesian inference/decision theory is optimal, and any artificial changes to the posterior should harm performance."
543,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the tradeoff between autoregressive and non-autoregressive neural machine translation (NMT) models. The authors argue that the speed disadvantage of NMT is overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. To address these issues, the authors propose to use a single-layer auto-regressive decoder for NMT models and show that it can outperform strong NMT methods with comparable inference speed."
544,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the tradeoff between autoregressive and non-autoregressive neural machine translation (NMT) models. The authors argue that the speed disadvantage of NMT is overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. To address these issues, the authors propose to use a single-layer auto-regressive decoder for NMT models and show that it can outperform strong NMT methods with comparable inference speed."
545,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the tradeoff between autoregressive and non-autoregressive neural machine translation (NMT) models. The authors argue that the speed disadvantage of NMT is overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. To address these issues, the authors propose to use a single-layer auto-regressive decoder for NMT models and show that it can outperform strong NMT methods with comparable inference speed."
546,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper studies the phenomenon of epoch-wise double descent, i.e., the test error of a deep neural network also shows double descent as the number of training epoches increases. The authors extend the bias-variance decomposition to epoch-wide double descent and reveal that the variance also contributes the most to the zero-one loss, as in model-wise Double descent. Inspired by this result, the authors propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error. It can be used to predict the generalization ability of a DNN when the zero one loss is used in test, and hence early stopping may be achieved without using a validation set."
547,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper studies the phenomenon of epoch-wise double descent, i.e., the test error of a deep neural network also shows double descent as the number of training epoches increases. The authors extend the bias-variance decomposition to epoch-wide double descent and reveal that the variance also contributes the most to the zero-one loss, as in model-wise Double descent. Inspired by this result, the authors propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error. It can be used to predict the generalization ability of a DNN when the zero one loss is used in test, and hence early stopping may be achieved without using a validation set."
548,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper studies the phenomenon of epoch-wise double descent, i.e., the test error of a deep neural network also shows double descent as the number of training epoches increases. The authors extend the bias-variance decomposition to epoch-wide double descent and reveal that the variance also contributes the most to the zero-one loss, as in model-wise Double descent. Inspired by this result, the authors propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error. It can be used to predict the generalization ability of a DNN when the zero one loss is used in test, and hence early stopping may be achieved without using a validation set."
549,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies the robustness of model-agnostic meta-learning (MAML) in few-shot learning. The authors propose a robustness-promoting regularization for MAML to improve robustness. They show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. Furthermore, they investigate how robust regularization can efficiently be designed to efficiently optimize the robust regularizer. Finally, extensive experiments are conducted to demonstrate the effectiveness of the proposed methods."
550,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies the robustness of model-agnostic meta-learning (MAML) in few-shot learning. The authors propose a robustness-promoting regularization for MAML to improve robustness. They show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. Furthermore, they investigate how robust regularization can efficiently be designed to efficiently optimize the robust regularizer. Finally, extensive experiments are conducted to demonstrate the effectiveness of the proposed methods."
551,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies the robustness of model-agnostic meta-learning (MAML) in few-shot learning. The authors propose a robustness-promoting regularization for MAML to improve robustness. They show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. Furthermore, they investigate how robust regularization can efficiently be designed to efficiently optimize the robust regularizer. Finally, extensive experiments are conducted to demonstrate the effectiveness of the proposed methods."
552,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the problem of meta-learning for quadratic loss optimization. The authors show that the meta-gradient computed directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. They also characterize when it is necessary to compute the meta gradient on a separate validation set instead of the original training set. Finally, they verify their results empirically and show that a similar phenomenon appears even for learned optimizers parametrized by neural networks."
553,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the problem of meta-learning for quadratic loss optimization. The authors show that the meta-gradient computed directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. They also characterize when it is necessary to compute the meta gradient on a separate validation set instead of the original training set. Finally, they verify their results empirically and show that a similar phenomenon appears even for learned optimizers parametrized by neural networks."
554,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the problem of meta-learning for quadratic loss optimization. The authors show that the meta-gradient computed directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. They also characterize when it is necessary to compute the meta gradient on a separate validation set instead of the original training set. Finally, they verify their results empirically and show that a similar phenomenon appears even for learned optimizers parametrized by neural networks."
555,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a method for semisupervised learning of graph features. The main idea is to use the neighborhood aggregation capability of GVCLN and use dual views to obtain different representations. For view-consistent representations between two views, two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while a view consistent loss is applied to the two views. The pseudo-label loss is designed by using the common high-confidence predictions. The experimental results show that the proposed method achieves state-of-the-art performance on several node classification tasks."
556,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a method for semisupervised learning of graph features. The main idea is to use the neighborhood aggregation capability of GVCLN and use dual views to obtain different representations. For view-consistent representations between two views, two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while a view consistent loss is applied to the two views. The pseudo-label loss is designed by using the common high-confidence predictions. The experimental results show that the proposed method achieves state-of-the-art performance on several node classification tasks."
557,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a method for semisupervised learning of graph features. The main idea is to use the neighborhood aggregation capability of GVCLN and use dual views to obtain different representations. For view-consistent representations between two views, two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while a view consistent loss is applied to the two views. The pseudo-label loss is designed by using the common high-confidence predictions. The experimental results show that the proposed method achieves state-of-the-art performance on several node classification tasks."
558,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,This paper proposes a neural network architecture for predicting the dynamics of systems when the underlying dynamics shall be inferred from the data directly. The authors show that such coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian dynamics. The proposed network identifies the conserved quantities in an unsupervised way and finds improved performance compared to networks biasing just to the Hamiltonian.
559,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,This paper proposes a neural network architecture for predicting the dynamics of systems when the underlying dynamics shall be inferred from the data directly. The authors show that such coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian dynamics. The proposed network identifies the conserved quantities in an unsupervised way and finds improved performance compared to networks biasing just to the Hamiltonian.
560,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,This paper proposes a neural network architecture for predicting the dynamics of systems when the underlying dynamics shall be inferred from the data directly. The authors show that such coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian dynamics. The proposed network identifies the conserved quantities in an unsupervised way and finds improved performance compared to networks biasing just to the Hamiltonian.
561,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes a novel architecture that combines gradient boosted decision trees (GBDT) and graph neural networks (GNN) for heterogeneous graph representation learning. The main idea is to train GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. The proposed model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. Experiments are conducted on a variety of graphs with tabular features."
562,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes a novel architecture that combines gradient boosted decision trees (GBDT) and graph neural networks (GNN) for heterogeneous graph representation learning. The main idea is to train GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. The proposed model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. Experiments are conducted on a variety of graphs with tabular features."
563,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes a novel architecture that combines gradient boosted decision trees (GBDT) and graph neural networks (GNN) for heterogeneous graph representation learning. The main idea is to train GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. The proposed model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. Experiments are conducted on a variety of graphs with tabular features."
564,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of data splitting in meta-learning in the asymptotic setting where the number of tasks goes to infinity. The authors show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. In contrast, if the data are generated from linear models (the realizable regime), both the splitting and nonsplitting methods converge to optimal prior. Further, perhaps surprisingly, the authors demonstrate that the non splitting method achieves a strictly better excess risk under this data distribution, even when the regularization parameter and split ratio are optimally tuned."
565,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of data splitting in meta-learning in the asymptotic setting where the number of tasks goes to infinity. The authors show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. In contrast, if the data are generated from linear models (the realizable regime), both the splitting and nonsplitting methods converge to optimal prior. Further, perhaps surprisingly, the authors demonstrate that the non splitting method achieves a strictly better excess risk under this data distribution, even when the regularization parameter and split ratio are optimally tuned."
566,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of data splitting in meta-learning in the asymptotic setting where the number of tasks goes to infinity. The authors show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. In contrast, if the data are generated from linear models (the realizable regime), both the splitting and nonsplitting methods converge to optimal prior. Further, perhaps surprisingly, the authors demonstrate that the non splitting method achieves a strictly better excess risk under this data distribution, even when the regularization parameter and split ratio are optimally tuned."
567,SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard confident examples from the noisy training data. The method is based on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. To extract hard examples that contain non-simple patterns and are entangled with the inaccurately labeled examples, the authors borrow the idea of momentum from physics. Specifically, they alternately update the confident examples and refine the classifier. The experiments on benchmark-simulated and real-world label-noise data illustrate the effectiveness of the proposed method."
568,SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard confident examples from the noisy training data. The method is based on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. To extract hard examples that contain non-simple patterns and are entangled with the inaccurately labeled examples, the authors borrow the idea of momentum from physics. Specifically, they alternately update the confident examples and refine the classifier. The experiments on benchmark-simulated and real-world label-noise data illustrate the effectiveness of the proposed method."
569,SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard confident examples from the noisy training data. The method is based on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. To extract hard examples that contain non-simple patterns and are entangled with the inaccurately labeled examples, the authors borrow the idea of momentum from physics. Specifically, they alternately update the confident examples and refine the classifier. The experiments on benchmark-simulated and real-world label-noise data illustrate the effectiveness of the proposed method."
570,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certified robustness against data poisoning attacks. The authors propose intrinsic majority vote mechanisms in kNN and radius nearest neighbors (rNN) to improve the robustness of the certified defenses against general data poisoning attack. They show that kNNs and rNNs have intrinsic majority voting mechanisms, which are more robust than state-of-the-art certified defenses. They also provide empirical evaluation results on MNIST and CIFAR10."
571,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certified robustness against data poisoning attacks. The authors propose intrinsic majority vote mechanisms in kNN and radius nearest neighbors (rNN) to improve the robustness of the certified defenses against general data poisoning attack. They show that kNNs and rNNs have intrinsic majority voting mechanisms, which are more robust than state-of-the-art certified defenses. They also provide empirical evaluation results on MNIST and CIFAR10."
572,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certified robustness against data poisoning attacks. The authors propose intrinsic majority vote mechanisms in kNN and radius nearest neighbors (rNN) to improve the robustness of the certified defenses against general data poisoning attack. They show that kNNs and rNNs have intrinsic majority voting mechanisms, which are more robust than state-of-the-art certified defenses. They also provide empirical evaluation results on MNIST and CIFAR10."
573,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. The authors theoretically analyze how batch-size influence such a metric and propose the formula to evaluate some rough range of optimal batch size. To address the dependency, the authors analyze an estimator for gradients that considers the randomness arising from two consecutive layers in GNN, and suggest a guideline for picking the appropriate scale of the batch. The experimental results show that in contrast to conventional deep learning models, GNNs benefit from large batch sizes."
574,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. The authors theoretically analyze how batch-size influence such a metric and propose the formula to evaluate some rough range of optimal batch size. To address the dependency, the authors analyze an estimator for gradients that considers the randomness arising from two consecutive layers in GNN, and suggest a guideline for picking the appropriate scale of the batch. The experimental results show that in contrast to conventional deep learning models, GNNs benefit from large batch sizes."
575,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. The authors theoretically analyze how batch-size influence such a metric and propose the formula to evaluate some rough range of optimal batch size. To address the dependency, the authors analyze an estimator for gradients that considers the randomness arising from two consecutive layers in GNN, and suggest a guideline for picking the appropriate scale of the batch. The experimental results show that in contrast to conventional deep learning models, GNNs benefit from large batch sizes."
576,SP:30d97322709cd292a49f936c767099f11b0e2913,"This paper proposes a method to calibrate the confidence scores of neural network classifiers for detecting misclassification errors. The method is based on a Gaussian Processes (GP) model, which is used to estimate the uncertainty of the calibrated confidence scores. The proposed method is compared with other confidence estimation methods on 125 UCI datasets, and compared with out-of-distribution and adversarial samples on a vision task with a large deep learning architecture."
577,SP:30d97322709cd292a49f936c767099f11b0e2913,"This paper proposes a method to calibrate the confidence scores of neural network classifiers for detecting misclassification errors. The method is based on a Gaussian Processes (GP) model, which is used to estimate the uncertainty of the calibrated confidence scores. The proposed method is compared with other confidence estimation methods on 125 UCI datasets, and compared with out-of-distribution and adversarial samples on a vision task with a large deep learning architecture."
578,SP:30d97322709cd292a49f936c767099f11b0e2913,"This paper proposes a method to calibrate the confidence scores of neural network classifiers for detecting misclassification errors. The method is based on a Gaussian Processes (GP) model, which is used to estimate the uncertainty of the calibrated confidence scores. The proposed method is compared with other confidence estimation methods on 125 UCI datasets, and compared with out-of-distribution and adversarial samples on a vision task with a large deep learning architecture."
579,SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. The approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. The paper evaluates the method on question answering, obtaining state-of-the-art results."
580,SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. The approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. The paper evaluates the method on question answering, obtaining state-of-the-art results."
581,SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. The approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. The paper evaluates the method on question answering, obtaining state-of-the-art results."
582,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper presents a method for learning safe controllers for reinforcement learning. The method is based on the idea that the constraints of a constrained MDP can be expressed as finite automata, which can then be used to learn a dense cost function. The authors show that the learned cost function can be used as a surrogate for the cost function of the MDP, which is then used to train the controller. The proposed method is evaluated on Safety Gym, MuJoCo, and Atari environments."
583,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper presents a method for learning safe controllers for reinforcement learning. The method is based on the idea that the constraints of a constrained MDP can be expressed as finite automata, which can then be used to learn a dense cost function. The authors show that the learned cost function can be used as a surrogate for the cost function of the MDP, which is then used to train the controller. The proposed method is evaluated on Safety Gym, MuJoCo, and Atari environments."
584,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper presents a method for learning safe controllers for reinforcement learning. The method is based on the idea that the constraints of a constrained MDP can be expressed as finite automata, which can then be used to learn a dense cost function. The authors show that the learned cost function can be used as a surrogate for the cost function of the MDP, which is then used to train the controller. The proposed method is evaluated on Safety Gym, MuJoCo, and Atari environments."
585,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model that aims to improve the comprehensibility of classifications. The main idea is to separate the notion of a decision path and an explanation path. To do so, instead of having one monolithic decision tree, they build several smaller decision subtrees and cascade them in sequence. This way each subtree identifies the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples."
586,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model that aims to improve the comprehensibility of classifications. The main idea is to separate the notion of a decision path and an explanation path. To do so, instead of having one monolithic decision tree, they build several smaller decision subtrees and cascade them in sequence. This way each subtree identifies the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples."
587,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model that aims to improve the comprehensibility of classifications. The main idea is to separate the notion of a decision path and an explanation path. To do so, instead of having one monolithic decision tree, they build several smaller decision subtrees and cascade them in sequence. This way each subtree identifies the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples."
588,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of network width on the performance of deep neural networks. The authors compare different ways of increasing model width while keeping the number of parameters constant. They show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the weight is secondary, as long as the model achieves high training accuarcy. They analyze these models in the framework of Gaussian Process kernels. They find that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance."
589,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of network width on the performance of deep neural networks. The authors compare different ways of increasing model width while keeping the number of parameters constant. They show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the weight is secondary, as long as the model achieves high training accuarcy. They analyze these models in the framework of Gaussian Process kernels. They find that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance."
590,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of network width on the performance of deep neural networks. The authors compare different ways of increasing model width while keeping the number of parameters constant. They show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the weight is secondary, as long as the model achieves high training accuarcy. They analyze these models in the framework of Gaussian Process kernels. They find that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance."
591,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embedding for entities and relations in the graph. Experimental results on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
592,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embedding for entities and relations in the graph. Experimental results on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
593,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embedding for entities and relations in the graph. Experimental results on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
594,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes an unsupervised learning algorithm for autoregressive sequence modeling. The algorithm is based on a variational inference approach, where the latent variable of the learner is the order by which the sequence was generated. The authors propose an end-to-end optimization algorithm, which uses policy gradients to optimize the variational lower bound. Experiments on sequence modeling tasks show that the proposed algorithm is competitive with or even better than fixed orders."
595,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes an unsupervised learning algorithm for autoregressive sequence modeling. The algorithm is based on a variational inference approach, where the latent variable of the learner is the order by which the sequence was generated. The authors propose an end-to-end optimization algorithm, which uses policy gradients to optimize the variational lower bound. Experiments on sequence modeling tasks show that the proposed algorithm is competitive with or even better than fixed orders."
596,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes an unsupervised learning algorithm for autoregressive sequence modeling. The algorithm is based on a variational inference approach, where the latent variable of the learner is the order by which the sequence was generated. The authors propose an end-to-end optimization algorithm, which uses policy gradients to optimize the variational lower bound. Experiments on sequence modeling tasks show that the proposed algorithm is competitive with or even better than fixed orders."
597,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction schema that can accelerate any sampling method under the memory budget. The motivation for the proposed schema is a careful analysis for the variance of sampling methods where it is shown that the induced variance can be decomposed into node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance during backward propagation. Theoretical convergence of the proposed algorithm is theoretically analyzed. Experiments are conducted on different sampling methods and applied them to different real-world graphs.
598,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction schema that can accelerate any sampling method under the memory budget. The motivation for the proposed schema is a careful analysis for the variance of sampling methods where it is shown that the induced variance can be decomposed into node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance during backward propagation. Theoretical convergence of the proposed algorithm is theoretically analyzed. Experiments are conducted on different sampling methods and applied them to different real-world graphs.
599,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction schema that can accelerate any sampling method under the memory budget. The motivation for the proposed schema is a careful analysis for the variance of sampling methods where it is shown that the induced variance can be decomposed into node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance during backward propagation. Theoretical convergence of the proposed algorithm is theoretically analyzed. Experiments are conducted on different sampling methods and applied them to different real-world graphs.
600,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a conditional adversarial generator for image manipulation. The generator learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. The proposed method is evaluated on a variety of image manipulation tasks."
601,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a conditional adversarial generator for image manipulation. The generator learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. The proposed method is evaluated on a variety of image manipulation tasks."
602,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a conditional adversarial generator for image manipulation. The generator learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. The proposed method is evaluated on a variety of image manipulation tasks."
603,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method for maximum common subgraph (MCS) detection. The main idea is to use a branch and bound algorithm as the backbone search algorithm to extract subgraphs by selecting one node pair at a time. In order to make better node selection decision at each step, the authors replace the node selection heuristics with a novel task-specific Deep Q-Network (DQN). The authors leverage the search process to provide supervision in a pre-training stage and guide our agent during an imitation learning stage. Experiments on synthetic and real-world large graph pairs demonstrate that the proposed method outperforms state-of-the-art MCS solvers and neural graph matching network models."
604,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method for maximum common subgraph (MCS) detection. The main idea is to use a branch and bound algorithm as the backbone search algorithm to extract subgraphs by selecting one node pair at a time. In order to make better node selection decision at each step, the authors replace the node selection heuristics with a novel task-specific Deep Q-Network (DQN). The authors leverage the search process to provide supervision in a pre-training stage and guide our agent during an imitation learning stage. Experiments on synthetic and real-world large graph pairs demonstrate that the proposed method outperforms state-of-the-art MCS solvers and neural graph matching network models."
605,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method for maximum common subgraph (MCS) detection. The main idea is to use a branch and bound algorithm as the backbone search algorithm to extract subgraphs by selecting one node pair at a time. In order to make better node selection decision at each step, the authors replace the node selection heuristics with a novel task-specific Deep Q-Network (DQN). The authors leverage the search process to provide supervision in a pre-training stage and guide our agent during an imitation learning stage. Experiments on synthetic and real-world large graph pairs demonstrate that the proposed method outperforms state-of-the-art MCS solvers and neural graph matching network models."
606,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes a method for reconstructing a wireframe from a 3D point cloud. The method consists of two steps: (1) a feature encoder, which takes as input a set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments, and (2) a pruned set of candidate vertices, which are then linked with an exhaustive set of edges to obtain the final wireframe. The network is trainable, and errors can be backpropagated through the entire sequence. The authors validate the proposed method on a publicly available synthetic dataset, for which the ground truth wireframes are accessible, and on a new real-world dataset."
607,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes a method for reconstructing a wireframe from a 3D point cloud. The method consists of two steps: (1) a feature encoder, which takes as input a set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments, and (2) a pruned set of candidate vertices, which are then linked with an exhaustive set of edges to obtain the final wireframe. The network is trainable, and errors can be backpropagated through the entire sequence. The authors validate the proposed method on a publicly available synthetic dataset, for which the ground truth wireframes are accessible, and on a new real-world dataset."
608,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes a method for reconstructing a wireframe from a 3D point cloud. The method consists of two steps: (1) a feature encoder, which takes as input a set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments, and (2) a pruned set of candidate vertices, which are then linked with an exhaustive set of edges to obtain the final wireframe. The network is trainable, and errors can be backpropagated through the entire sequence. The authors validate the proposed method on a publicly available synthetic dataset, for which the ground truth wireframes are accessible, and on a new real-world dataset."
609,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies stochastic optimization under weaker assumptions on the distribution of noise than those used in usual analysis. In particular, the authors consider the case when the noise variation is known, and show that it is always beneficial to adapt the step-size and exploit the noise variability. The authors also provide an online estimator of the noise level, thereby recovering close variants of RMSProp."
610,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies stochastic optimization under weaker assumptions on the distribution of noise than those used in usual analysis. In particular, the authors consider the case when the noise variation is known, and show that it is always beneficial to adapt the step-size and exploit the noise variability. The authors also provide an online estimator of the noise level, thereby recovering close variants of RMSProp."
611,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies stochastic optimization under weaker assumptions on the distribution of noise than those used in usual analysis. In particular, the authors consider the case when the noise variation is known, and show that it is always beneficial to adapt the step-size and exploit the noise variability. The authors also provide an online estimator of the noise level, thereby recovering close variants of RMSProp."
612,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method to improve the performance of neural machine translation (NMT) models by incorporating prior word alignment information into the decoding process. Specifically, the authors propose an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and efficient way. Experiments show that the proposed method achieves BLEU improvements over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
613,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method to improve the performance of neural machine translation (NMT) models by incorporating prior word alignment information into the decoding process. Specifically, the authors propose an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and efficient way. Experiments show that the proposed method achieves BLEU improvements over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
614,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method to improve the performance of neural machine translation (NMT) models by incorporating prior word alignment information into the decoding process. Specifically, the authors propose an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and efficient way. Experiments show that the proposed method achieves BLEU improvements over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
615,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper proposes a new benchmark for reinforcement learning, MDP Playground, which is a parameterized collection of fast-to-run toy benchmarks in OpenAI Gym that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. The authors consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. Furthermore, since we can generate environments with a desired value for each of the dimensions, in addition to having fine-grained control over the environments’ hardness, we also have the ground truth available for evaluating algorithms."
616,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper proposes a new benchmark for reinforcement learning, MDP Playground, which is a parameterized collection of fast-to-run toy benchmarks in OpenAI Gym that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. The authors consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. Furthermore, since we can generate environments with a desired value for each of the dimensions, in addition to having fine-grained control over the environments’ hardness, we also have the ground truth available for evaluating algorithms."
617,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper proposes a new benchmark for reinforcement learning, MDP Playground, which is a parameterized collection of fast-to-run toy benchmarks in OpenAI Gym that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. The authors consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. Furthermore, since we can generate environments with a desired value for each of the dimensions, in addition to having fine-grained control over the environments’ hardness, we also have the ground truth available for evaluating algorithms."
618,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes quantile calibration for regression models. The proposed method is based on quantile regularizer, which is a black box method that can be used as a blackbox to calibrate any probabilistic regression model. The authors provide a detailed formal analysis of the side-effects of Isotonic Regression when used for regression calibration. The method is trainable in an end-to-end fashion, without requiring an additional dataset. Empirical results demonstrate that the proposed method improves calibration for various models trained on diverse architectures that provide uncertainty estimates."
619,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes quantile calibration for regression models. The proposed method is based on quantile regularizer, which is a black box method that can be used as a blackbox to calibrate any probabilistic regression model. The authors provide a detailed formal analysis of the side-effects of Isotonic Regression when used for regression calibration. The method is trainable in an end-to-end fashion, without requiring an additional dataset. Empirical results demonstrate that the proposed method improves calibration for various models trained on diverse architectures that provide uncertainty estimates."
620,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes quantile calibration for regression models. The proposed method is based on quantile regularizer, which is a black box method that can be used as a blackbox to calibrate any probabilistic regression model. The authors provide a detailed formal analysis of the side-effects of Isotonic Regression when used for regression calibration. The method is trainable in an end-to-end fashion, without requiring an additional dataset. Empirical results demonstrate that the proposed method improves calibration for various models trained on diverse architectures that provide uncertainty estimates."
621,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a method for 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. The method leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. The combination of variational inference, neural networks and a differentiable raycaster ensures that the model is amenable to end-to-end gradient-based optimisation."
622,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a method for 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. The method leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. The combination of variational inference, neural networks and a differentiable raycaster ensures that the model is amenable to end-to-end gradient-based optimisation."
623,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a method for 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. The method leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. The combination of variational inference, neural networks and a differentiable raycaster ensures that the model is amenable to end-to-end gradient-based optimisation."
624,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for using textual descriptions to improve the generalization of control policies to new scenarios. The authors propose to use a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. The model is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. The grounding acquired by EMMA is also robust to noisy descriptions and linguistic variation. Experiments on 1320 games show that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines."
625,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for using textual descriptions to improve the generalization of control policies to new scenarios. The authors propose to use a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. The model is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. The grounding acquired by EMMA is also robust to noisy descriptions and linguistic variation. Experiments on 1320 games show that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines."
626,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for using textual descriptions to improve the generalization of control policies to new scenarios. The authors propose to use a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. The model is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. The grounding acquired by EMMA is also robust to noisy descriptions and linguistic variation. Experiments on 1320 games show that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines."
627,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes a new policy gradient estimator for actor-critic. The proposed estimator is based on a new state-value function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value. The authors prove the theoretical consistency of the estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms.
628,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes a new policy gradient estimator for actor-critic. The proposed estimator is based on a new state-value function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value. The authors prove the theoretical consistency of the estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms.
629,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes a new policy gradient estimator for actor-critic. The proposed estimator is based on a new state-value function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value. The authors prove the theoretical consistency of the estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms.
630,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper investigates the effect of increasing the depth within an overparameterized regime. The authors introduce local and global labels as abstract but simple classification rules. The experimental results suggest that deeper is better for local labels, whereas shallower are better for global labels. They also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. The NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning."
631,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper investigates the effect of increasing the depth within an overparameterized regime. The authors introduce local and global labels as abstract but simple classification rules. The experimental results suggest that deeper is better for local labels, whereas shallower are better for global labels. They also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. The NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning."
632,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper investigates the effect of increasing the depth within an overparameterized regime. The authors introduce local and global labels as abstract but simple classification rules. The experimental results suggest that deeper is better for local labels, whereas shallower are better for global labels. They also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. The NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning."
633,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies few-shot learning via representation learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2( n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. First, they study the case where this common representation is low-dimensional and provide a risk bound of $\epsilon$(dk n1T + k n2)$ for the linear representation class; here d is the ambient input dimension and k(d) is the dimension of the representation. They further extend this result to handle a general representation function class and obtain a similar result. Next, they consider the case when the common representation may be high-dimensional but is capacity-constrained (say in norm). "
634,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies few-shot learning via representation learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2( n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. First, they study the case where this common representation is low-dimensional and provide a risk bound of $\epsilon$(dk n1T + k n2)$ for the linear representation class; here d is the ambient input dimension and k(d) is the dimension of the representation. They further extend this result to handle a general representation function class and obtain a similar result. Next, they consider the case when the common representation may be high-dimensional but is capacity-constrained (say in norm). "
635,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies few-shot learning via representation learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2( n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. First, they study the case where this common representation is low-dimensional and provide a risk bound of $\epsilon$(dk n1T + k n2)$ for the linear representation class; here d is the ambient input dimension and k(d) is the dimension of the representation. They further extend this result to handle a general representation function class and obtain a similar result. Next, they consider the case when the common representation may be high-dimensional but is capacity-constrained (say in norm). "
636,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"This paper proposes a method to evaluate the robustness of neural networks to the inputs' semantic features. The proposed method is based on a black-box approach to determine features for which a network is robust or weak. Specifically, the authors define provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features."
637,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"This paper proposes a method to evaluate the robustness of neural networks to the inputs' semantic features. The proposed method is based on a black-box approach to determine features for which a network is robust or weak. Specifically, the authors define provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features."
638,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"This paper proposes a method to evaluate the robustness of neural networks to the inputs' semantic features. The proposed method is based on a black-box approach to determine features for which a network is robust or weak. Specifically, the authors define provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features."
639,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task reinforcement learning. The main idea is to use the expectation-maximization algorithm to find clusters of related tasks and use these to improve sample complexity. The proposed method is intuitive, simple to implement and orthogonal to other multi task learning algorithms. The experimental results show the effectiveness of the proposed method."
640,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task reinforcement learning. The main idea is to use the expectation-maximization algorithm to find clusters of related tasks and use these to improve sample complexity. The proposed method is intuitive, simple to implement and orthogonal to other multi task learning algorithms. The experimental results show the effectiveness of the proposed method."
641,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task reinforcement learning. The main idea is to use the expectation-maximization algorithm to find clusters of related tasks and use these to improve sample complexity. The proposed method is intuitive, simple to implement and orthogonal to other multi task learning algorithms. The experimental results show the effectiveness of the proposed method."
642,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning method for non-stationary time series. The proposed method, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. TNC learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from distribution of non-neighboring signals."
643,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning method for non-stationary time series. The proposed method, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. TNC learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from distribution of non-neighboring signals."
644,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning method for non-stationary time series. The proposed method, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. TNC learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from distribution of non-neighboring signals."
645,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a method for learning modular networks for multi-task learning, transfer learning, and domain adaptation. The proposed method is based on the idea that the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in case of multi-tasks learning and transfer learning. They also show how their approach can be used to increase accuracy of existing architectures for image classification tasks such as IMAGENET, without any parameter increase."
646,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a method for learning modular networks for multi-task learning, transfer learning, and domain adaptation. The proposed method is based on the idea that the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in case of multi-tasks learning and transfer learning. They also show how their approach can be used to increase accuracy of existing architectures for image classification tasks such as IMAGENET, without any parameter increase."
647,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a method for learning modular networks for multi-task learning, transfer learning, and domain adaptation. The proposed method is based on the idea that the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in case of multi-tasks learning and transfer learning. They also show how their approach can be used to increase accuracy of existing architectures for image classification tasks such as IMAGENET, without any parameter increase."
648,SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is a phenomenon that the learned latent space becomes uninformative. This is related to local optima of the objective function that are often introduced by a fixed hyperparameter resembling the data variance. The authors suggest that this variance parameter regularizes the VAE and affects its smoothness, which is the magnitude of its gradient. This paper proposes AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter, and thus avoids oversmoothing the model. Experiments on MNIST and CelebA datasets show that the proposed method improves the FID of images generated from MNIST."
649,SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is a phenomenon that the learned latent space becomes uninformative. This is related to local optima of the objective function that are often introduced by a fixed hyperparameter resembling the data variance. The authors suggest that this variance parameter regularizes the VAE and affects its smoothness, which is the magnitude of its gradient. This paper proposes AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter, and thus avoids oversmoothing the model. Experiments on MNIST and CelebA datasets show that the proposed method improves the FID of images generated from MNIST."
650,SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is a phenomenon that the learned latent space becomes uninformative. This is related to local optima of the objective function that are often introduced by a fixed hyperparameter resembling the data variance. The authors suggest that this variance parameter regularizes the VAE and affects its smoothness, which is the magnitude of its gradient. This paper proposes AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter, and thus avoids oversmoothing the model. Experiments on MNIST and CelebA datasets show that the proposed method improves the FID of images generated from MNIST."
651,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a generative model based on variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. The proposed model combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead to three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). Second, the model performs domain alignment to find correlations and interpolate between different databases. Finally, the authors study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures."
652,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a generative model based on variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. The proposed model combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead to three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). Second, the model performs domain alignment to find correlations and interpolate between different databases. Finally, the authors study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures."
653,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a generative model based on variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. The proposed model combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead to three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). Second, the model performs domain alignment to find correlations and interpolate between different databases. Finally, the authors study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures."
654,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,This paper proposes a self-supervised representation learning method that uses human interaction and attention cues to learn better representations compared to visualonly representations. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. They use human’s interactions with their visual surrounding as a training signal for representation learning. The experiments show that the proposed method outperforms a visual-only state-of-the-art method MoCo.
655,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,This paper proposes a self-supervised representation learning method that uses human interaction and attention cues to learn better representations compared to visualonly representations. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. They use human’s interactions with their visual surrounding as a training signal for representation learning. The experiments show that the proposed method outperforms a visual-only state-of-the-art method MoCo.
656,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,This paper proposes a self-supervised representation learning method that uses human interaction and attention cues to learn better representations compared to visualonly representations. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. They use human’s interactions with their visual surrounding as a training signal for representation learning. The experiments show that the proposed method outperforms a visual-only state-of-the-art method MoCo.
657,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the problem of negative pretraining, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch when either are trained on a target task. The authors propose three interventions to remove and fix it. First, acting on the learning process, altering the learning rate after pretraining can yield even better results than training directly on the target task, and second, increasing the discretization of data distribution changes from start to target task instead of “jumping” to a target. Finally, at the model-level, resetting network biases to larger values can also remove negative preraining effects, albeit to a smaller degree."
658,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the problem of negative pretraining, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch when either are trained on a target task. The authors propose three interventions to remove and fix it. First, acting on the learning process, altering the learning rate after pretraining can yield even better results than training directly on the target task, and second, increasing the discretization of data distribution changes from start to target task instead of “jumping” to a target. Finally, at the model-level, resetting network biases to larger values can also remove negative preraining effects, albeit to a smaller degree."
659,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the problem of negative pretraining, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch when either are trained on a target task. The authors propose three interventions to remove and fix it. First, acting on the learning process, altering the learning rate after pretraining can yield even better results than training directly on the target task, and second, increasing the discretization of data distribution changes from start to target task instead of “jumping” to a target. Finally, at the model-level, resetting network biases to larger values can also remove negative preraining effects, albeit to a smaller degree."
660,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of finding safe spots in images that are robust to adversarial attacks. The authors propose a bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets. The experiments also show that they can be used to improve both the empirical and certified robustness on smoothed classifiers.
661,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of finding safe spots in images that are robust to adversarial attacks. The authors propose a bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets. The experiments also show that they can be used to improve both the empirical and certified robustness on smoothed classifiers.
662,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of finding safe spots in images that are robust to adversarial attacks. The authors propose a bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets. The experiments also show that they can be used to improve both the empirical and certified robustness on smoothed classifiers.
663,SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to disentangle space and time in point cloud sequences. The spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution was used to model the dynamics of the spatial regions along the time dimension. Furthermore, the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequence in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet."
664,SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to disentangle space and time in point cloud sequences. The spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution was used to model the dynamics of the spatial regions along the time dimension. Furthermore, the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequence in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet."
665,SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to disentangle space and time in point cloud sequences. The spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution was used to model the dynamics of the spatial regions along the time dimension. Furthermore, the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequence in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet."
666,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. The main idea is to model the acoustic information in both utterance and phoneme level, and use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning. To better trade off the adaptation parameters and voice quality, the authors introduce conditional layer normalization in the mel-spectrogram decoder. The experimental results show that the proposed method achieves better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice."
667,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. The main idea is to model the acoustic information in both utterance and phoneme level, and use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning. To better trade off the adaptation parameters and voice quality, the authors introduce conditional layer normalization in the mel-spectrogram decoder. The experimental results show that the proposed method achieves better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice."
668,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. The main idea is to model the acoustic information in both utterance and phoneme level, and use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning. To better trade off the adaptation parameters and voice quality, the authors introduce conditional layer normalization in the mel-spectrogram decoder. The experimental results show that the proposed method achieves better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice."
669,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors show that initialization is only one piece of the puzzle and a wider view of tailoring optimization to sparse networks yields promising results. They show that the default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, they show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime."
670,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors show that initialization is only one piece of the puzzle and a wider view of tailoring optimization to sparse networks yields promising results. They show that the default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, they show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime."
671,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors show that initialization is only one piece of the puzzle and a wider view of tailoring optimization to sparse networks yields promising results. They show that the default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, they show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime."
672,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"This paper studies the relationship between label propagation (LPA) and graph convolutional neural networks (GCN) for node classification. Specifically, the authors study the relation between LPA and GCN in terms of two aspects: (1) feature/label smoothing where they analyze how the features/label of one node is spread over its neighbors; and (2) feature and label influence of how much the initial feature/labels of the same node influences the final feature of another node. Based on the theoretical analysis, they propose an end-to-end model that unifies GCN and LPA. In particular, the edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. The proposed model can be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models."
673,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"This paper studies the relationship between label propagation (LPA) and graph convolutional neural networks (GCN) for node classification. Specifically, the authors study the relation between LPA and GCN in terms of two aspects: (1) feature/label smoothing where they analyze how the features/label of one node is spread over its neighbors; and (2) feature and label influence of how much the initial feature/labels of the same node influences the final feature of another node. Based on the theoretical analysis, they propose an end-to-end model that unifies GCN and LPA. In particular, the edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. The proposed model can be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models."
674,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"This paper studies the relationship between label propagation (LPA) and graph convolutional neural networks (GCN) for node classification. Specifically, the authors study the relation between LPA and GCN in terms of two aspects: (1) feature/label smoothing where they analyze how the features/label of one node is spread over its neighbors; and (2) feature and label influence of how much the initial feature/labels of the same node influences the final feature of another node. Based on the theoretical analysis, they propose an end-to-end model that unifies GCN and LPA. In particular, the edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. The proposed model can be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models."
675,SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of a classifier trained on the ground truth label generating function (LGF) generated by a function within another function space, which the authors call the generator space. The authors propose a joint entropy-like measure of complexity between function spaces (classifier and generator) called co-complexity, which leads to tighter bounds on the generalisation error in this setting. The paper also shows that reducing the invariance and dissociation coefficients of the classifier, while maintaining its dissociation coefficient, improves the training error and generalization gap. The theoretical results are supported by empirical validation on the CNN architecture and its transformation-equivariant extensions."
676,SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of a classifier trained on the ground truth label generating function (LGF) generated by a function within another function space, which the authors call the generator space. The authors propose a joint entropy-like measure of complexity between function spaces (classifier and generator) called co-complexity, which leads to tighter bounds on the generalisation error in this setting. The paper also shows that reducing the invariance and dissociation coefficients of the classifier, while maintaining its dissociation coefficient, improves the training error and generalization gap. The theoretical results are supported by empirical validation on the CNN architecture and its transformation-equivariant extensions."
677,SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of a classifier trained on the ground truth label generating function (LGF) generated by a function within another function space, which the authors call the generator space. The authors propose a joint entropy-like measure of complexity between function spaces (classifier and generator) called co-complexity, which leads to tighter bounds on the generalisation error in this setting. The paper also shows that reducing the invariance and dissociation coefficients of the classifier, while maintaining its dissociation coefficient, improves the training error and generalization gap. The theoretical results are supported by empirical validation on the CNN architecture and its transformation-equivariant extensions."
678,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a novel post-training quantization (PTQ) method, called BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. The main idea is to reconstruct the basic building blocks in neural networks and reconstructs them one-by-one. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks."
679,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a novel post-training quantization (PTQ) method, called BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. The main idea is to reconstruct the basic building blocks in neural networks and reconstructs them one-by-one. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks."
680,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a novel post-training quantization (PTQ) method, called BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. The main idea is to reconstruct the basic building blocks in neural networks and reconstructs them one-by-one. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks."
681,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of dataset properties on the calibration of deep neural networks. The authors show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. They also study the effects of label quality, showing how label noise dramatically increases calibration error. Finally, the authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation."
682,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of dataset properties on the calibration of deep neural networks. The authors show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. They also study the effects of label quality, showing how label noise dramatically increases calibration error. Finally, the authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation."
683,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of dataset properties on the calibration of deep neural networks. The authors show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. They also study the effects of label quality, showing how label noise dramatically increases calibration error. Finally, the authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation."
684,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication in multi-agent settings. The authors propose a method to train agents to communicate via actuating their joints in a 3D environment. They show that under realistic assumptions, a non-uniform distribution of intents and a common knowledge energy cost, these agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice. They propose and evaluate initial training improvements to address these challenges."
685,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication in multi-agent settings. The authors propose a method to train agents to communicate via actuating their joints in a 3D environment. They show that under realistic assumptions, a non-uniform distribution of intents and a common knowledge energy cost, these agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice. They propose and evaluate initial training improvements to address these challenges."
686,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication in multi-agent settings. The authors propose a method to train agents to communicate via actuating their joints in a 3D environment. They show that under realistic assumptions, a non-uniform distribution of intents and a common knowledge energy cost, these agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice. They propose and evaluate initial training improvements to address these challenges."
687,SP:5ba686e2eef369fa49b10ba3f41f102740836859,This paper proposes a new method for estimating the uncertainty of sequential regression. The method is based on the idea that the uncertainty can be expressed in terms of symmetric and asymmetric terms. The authors show that the proposed method can be applied to both drift and non-drift settings.
688,SP:5ba686e2eef369fa49b10ba3f41f102740836859,This paper proposes a new method for estimating the uncertainty of sequential regression. The method is based on the idea that the uncertainty can be expressed in terms of symmetric and asymmetric terms. The authors show that the proposed method can be applied to both drift and non-drift settings.
689,SP:5ba686e2eef369fa49b10ba3f41f102740836859,This paper proposes a new method for estimating the uncertainty of sequential regression. The method is based on the idea that the uncertainty can be expressed in terms of symmetric and asymmetric terms. The authors show that the proposed method can be applied to both drift and non-drift settings.
690,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,This paper proposes two unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. The second formulation is an unbalanced distance between mm-spaces up to isometries. The authors show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme.
691,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,This paper proposes two unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. The second formulation is an unbalanced distance between mm-spaces up to isometries. The authors show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme.
692,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,This paper proposes two unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. The second formulation is an unbalanced distance between mm-spaces up to isometries. The authors show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme.
693,SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a new pruning method, called all-alive pruning (AAP), to find sparse networks with only trainable weights. The authors argue that dead connections do not contribute to model capacity, and observe that existing pruning methods (i.e., iterative pruning, one-shot pruning and dynamic pruning) consistently improve the accuracy of original methods at 128×–4096× compression ratios on three benchmark datasets. The paper is well-written and easy to follow."
694,SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a new pruning method, called all-alive pruning (AAP), to find sparse networks with only trainable weights. The authors argue that dead connections do not contribute to model capacity, and observe that existing pruning methods (i.e., iterative pruning, one-shot pruning and dynamic pruning) consistently improve the accuracy of original methods at 128×–4096× compression ratios on three benchmark datasets. The paper is well-written and easy to follow."
695,SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a new pruning method, called all-alive pruning (AAP), to find sparse networks with only trainable weights. The authors argue that dead connections do not contribute to model capacity, and observe that existing pruning methods (i.e., iterative pruning, one-shot pruning and dynamic pruning) consistently improve the accuracy of original methods at 128×–4096× compression ratios on three benchmark datasets. The paper is well-written and easy to follow."
696,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method is based on the idea of learning multiple attribute transformations simultaneously, integrating attribute regression into the training of transformation functions, and applying a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. Experiments show that the proposed method achieves state-of-the-art performance for targeted image manipulation."
697,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method is based on the idea of learning multiple attribute transformations simultaneously, integrating attribute regression into the training of transformation functions, and applying a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. Experiments show that the proposed method achieves state-of-the-art performance for targeted image manipulation."
698,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method is based on the idea of learning multiple attribute transformations simultaneously, integrating attribute regression into the training of transformation functions, and applying a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. Experiments show that the proposed method achieves state-of-the-art performance for targeted image manipulation."
699,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes a feature alignment regularization method for GANs for discrete sequence generation. The proposed method is based on the Feature Statistics Alignment (FSA) paradigm, which forces the mean statistics of the fake data distribution to approach that of real data as close as possible in a finite-dimensional feature space. The authors claim that this regularization is the first to employ feature alignment in the Gumbel-Softmax based GAN framework for sequence generation and demonstrate the effectiveness of the proposed method on synthetic and real-world datasets."
700,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes a feature alignment regularization method for GANs for discrete sequence generation. The proposed method is based on the Feature Statistics Alignment (FSA) paradigm, which forces the mean statistics of the fake data distribution to approach that of real data as close as possible in a finite-dimensional feature space. The authors claim that this regularization is the first to employ feature alignment in the Gumbel-Softmax based GAN framework for sequence generation and demonstrate the effectiveness of the proposed method on synthetic and real-world datasets."
701,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes a feature alignment regularization method for GANs for discrete sequence generation. The proposed method is based on the Feature Statistics Alignment (FSA) paradigm, which forces the mean statistics of the fake data distribution to approach that of real data as close as possible in a finite-dimensional feature space. The authors claim that this regularization is the first to employ feature alignment in the Gumbel-Softmax based GAN framework for sequence generation and demonstrate the effectiveness of the proposed method on synthetic and real-world datasets."
702,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes a method to address the problem of reward progressivity in reinforcement learning, where the rewards tend to increase in magnitude over time. The authors propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. Experiments are conducted on two domains with extreme reward progressiveness, where standard value-based methods struggle significantly, and the proposed method is able to make much farther progress."
703,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes a method to address the problem of reward progressivity in reinforcement learning, where the rewards tend to increase in magnitude over time. The authors propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. Experiments are conducted on two domains with extreme reward progressiveness, where standard value-based methods struggle significantly, and the proposed method is able to make much farther progress."
704,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes a method to address the problem of reward progressivity in reinforcement learning, where the rewards tend to increase in magnitude over time. The authors propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. Experiments are conducted on two domains with extreme reward progressiveness, where standard value-based methods struggle significantly, and the proposed method is able to make much farther progress."
705,SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper studies the representation learned by deep neural networks during training. The authors propose a notion of usable information contained in the representations learned by a deep network, and use it to study how optimal representations for the task emerge during the training. They show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations. In the process of arriving at a minimal sufficient representation, they find that the content of the representation changes dynamically during training, and that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded."
706,SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper studies the representation learned by deep neural networks during training. The authors propose a notion of usable information contained in the representations learned by a deep network, and use it to study how optimal representations for the task emerge during the training. They show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations. In the process of arriving at a minimal sufficient representation, they find that the content of the representation changes dynamically during training, and that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded."
707,SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper studies the representation learned by deep neural networks during training. The authors propose a notion of usable information contained in the representations learned by a deep network, and use it to study how optimal representations for the task emerge during the training. They show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations. In the process of arriving at a minimal sufficient representation, they find that the content of the representation changes dynamically during training, and that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded."
708,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper proposes a new variance reduction algorithm for nonconvex-strongly-concave min-max optimization. The proposed algorithm, SREDA-Boost, is a variant of the SREDa algorithm, which has less restrictive initialization requirement and an accuracy-independent (and much bigger) stepsize. The authors provide a theoretical analysis of the convergence of the proposed algorithm and show that it can achieve the optimal complexity dependence on the required accuracy level. In addition, the authors propose a zeroth-order variant, ZO-SREDA, which uses only the information about function values not gradients, and shows that it outperforms the best known complexity dependence dependence on."
709,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper proposes a new variance reduction algorithm for nonconvex-strongly-concave min-max optimization. The proposed algorithm, SREDA-Boost, is a variant of the SREDa algorithm, which has less restrictive initialization requirement and an accuracy-independent (and much bigger) stepsize. The authors provide a theoretical analysis of the convergence of the proposed algorithm and show that it can achieve the optimal complexity dependence on the required accuracy level. In addition, the authors propose a zeroth-order variant, ZO-SREDA, which uses only the information about function values not gradients, and shows that it outperforms the best known complexity dependence dependence on."
710,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper proposes a new variance reduction algorithm for nonconvex-strongly-concave min-max optimization. The proposed algorithm, SREDA-Boost, is a variant of the SREDa algorithm, which has less restrictive initialization requirement and an accuracy-independent (and much bigger) stepsize. The authors provide a theoretical analysis of the convergence of the proposed algorithm and show that it can achieve the optimal complexity dependence on the required accuracy level. In addition, the authors propose a zeroth-order variant, ZO-SREDA, which uses only the information about function values not gradients, and shows that it outperforms the best known complexity dependence dependence on."
711,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of few-shot object detection. The authors show that the generalization gap is caused by the number of categories used during training, and that it holds for different models, backbones and datasets. To address this problem, the authors propose to increase number of object categories during training and show that it can improve generalization from seen to unseen classes from 45% to 89%."
712,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of few-shot object detection. The authors show that the generalization gap is caused by the number of categories used during training, and that it holds for different models, backbones and datasets. To address this problem, the authors propose to increase number of object categories during training and show that it can improve generalization from seen to unseen classes from 45% to 89%."
713,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of few-shot object detection. The authors show that the generalization gap is caused by the number of categories used during training, and that it holds for different models, backbones and datasets. To address this problem, the authors propose to increase number of object categories during training and show that it can improve generalization from seen to unseen classes from 45% to 89%."
714,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of few-shot object detection. The authors show that the generalization gap is caused by the number of categories used during training, and that it holds for different models, backbones and datasets. To address this problem, the authors propose to increase number of object categories during training and show that it can improve generalization from seen to unseen classes from 45% to 89%."
715,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a new method for single-view implicit surface reconstruction. The proposed method is based on a closed-form Differentiable Gradient Sampling (DGS) solution that enables backpropagation of the loss on spatial gradients to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. The experimental results on ShapeNet and ScannetV2 demonstrate the effectiveness of the proposed method."
716,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a new method for single-view implicit surface reconstruction. The proposed method is based on a closed-form Differentiable Gradient Sampling (DGS) solution that enables backpropagation of the loss on spatial gradients to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. The experimental results on ShapeNet and ScannetV2 demonstrate the effectiveness of the proposed method."
717,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a new method for single-view implicit surface reconstruction. The proposed method is based on a closed-form Differentiable Gradient Sampling (DGS) solution that enables backpropagation of the loss on spatial gradients to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. The experimental results on ShapeNet and ScannetV2 demonstrate the effectiveness of the proposed method."
718,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a new method for single-view implicit surface reconstruction. The proposed method is based on a closed-form Differentiable Gradient Sampling (DGS) solution that enables backpropagation of the loss on spatial gradients to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. The experimental results on ShapeNet and ScannetV2 demonstrate the effectiveness of the proposed method."
719,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a simple training strategy called “Pseudo-to-Real” for high-memoryfootprint-required large models. It is compatible with large models with architecture of sequential layers. The authors demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides, the authors also provide a technique, Granular CPU offloading, to manage CPU memory for training large model."
720,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a simple training strategy called “Pseudo-to-Real” for high-memoryfootprint-required large models. It is compatible with large models with architecture of sequential layers. The authors demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides, the authors also provide a technique, Granular CPU offloading, to manage CPU memory for training large model."
721,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a simple training strategy called “Pseudo-to-Real” for high-memoryfootprint-required large models. It is compatible with large models with architecture of sequential layers. The authors demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides, the authors also provide a technique, Granular CPU offloading, to manage CPU memory for training large model."
722,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a simple training strategy called “Pseudo-to-Real” for high-memoryfootprint-required large models. It is compatible with large models with architecture of sequential layers. The authors demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides, the authors also provide a technique, Granular CPU offloading, to manage CPU memory for training large model."
723,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper studies the problem of energy-based generative models (EBMs) with shallow overparametrized neural network energies. The authors derive variational principles dual to maximum likelihood EBMs for both the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. They also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
724,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper studies the problem of energy-based generative models (EBMs) with shallow overparametrized neural network energies. The authors derive variational principles dual to maximum likelihood EBMs for both the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. They also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
725,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper studies the problem of energy-based generative models (EBMs) with shallow overparametrized neural network energies. The authors derive variational principles dual to maximum likelihood EBMs for both the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. They also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
726,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper studies the problem of energy-based generative models (EBMs) with shallow overparametrized neural network energies. The authors derive variational principles dual to maximum likelihood EBMs for both the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. They also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
727,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,This paper studies the lower bounds of differentially private ERM for general convex functions. The main contribution of this paper is to provide tight $\Omega(\sqrt{n})$ lower bounds for both constrained and unconstrained cases. This is achieved by introducing a novel biased mean property for fingerprinting codes. The authors also introduce an auxiliary dimension to simplify the computation brought by `2 loss.
728,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,This paper studies the lower bounds of differentially private ERM for general convex functions. The main contribution of this paper is to provide tight $\Omega(\sqrt{n})$ lower bounds for both constrained and unconstrained cases. This is achieved by introducing a novel biased mean property for fingerprinting codes. The authors also introduce an auxiliary dimension to simplify the computation brought by `2 loss.
729,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,This paper studies the lower bounds of differentially private ERM for general convex functions. The main contribution of this paper is to provide tight $\Omega(\sqrt{n})$ lower bounds for both constrained and unconstrained cases. This is achieved by introducing a novel biased mean property for fingerprinting codes. The authors also introduce an auxiliary dimension to simplify the computation brought by `2 loss.
730,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,This paper studies the lower bounds of differentially private ERM for general convex functions. The main contribution of this paper is to provide tight $\Omega(\sqrt{n})$ lower bounds for both constrained and unconstrained cases. This is achieved by introducing a novel biased mean property for fingerprinting codes. The authors also introduce an auxiliary dimension to simplify the computation brought by `2 loss.
731,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key idea is to use a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The proposed method covers all the classical Wasserstein gradient flows including the heat equation and the porous medium equation. The experimental results demonstrate the performance and scalability of the proposed algorithm."
732,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key idea is to use a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The proposed method covers all the classical Wasserstein gradient flows including the heat equation and the porous medium equation. The experimental results demonstrate the performance and scalability of the proposed algorithm."
733,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key idea is to use a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The proposed method covers all the classical Wasserstein gradient flows including the heat equation and the porous medium equation. The experimental results demonstrate the performance and scalability of the proposed algorithm."
734,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key idea is to use a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The proposed method covers all the classical Wasserstein gradient flows including the heat equation and the porous medium equation. The experimental results demonstrate the performance and scalability of the proposed algorithm."
735,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper proposes a meta-feature learning approach to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed Meta-features with the space of distributions on the hyperparameter configurations. The authors claim that the learned meta-meta-features induce a topology on the set of datasets that is exploited to define a distribution of promising hyperparameters configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that the proposed method outperforms the state-of-the-art AutoML systems, AutoSkLearn and Probabilistic Matrix Factorization. "
736,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper proposes a meta-feature learning approach to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed Meta-features with the space of distributions on the hyperparameter configurations. The authors claim that the learned meta-meta-features induce a topology on the set of datasets that is exploited to define a distribution of promising hyperparameters configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that the proposed method outperforms the state-of-the-art AutoML systems, AutoSkLearn and Probabilistic Matrix Factorization. "
737,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper proposes a meta-feature learning approach to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed Meta-features with the space of distributions on the hyperparameter configurations. The authors claim that the learned meta-meta-features induce a topology on the set of datasets that is exploited to define a distribution of promising hyperparameters configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that the proposed method outperforms the state-of-the-art AutoML systems, AutoSkLearn and Probabilistic Matrix Factorization. "
738,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper proposes a meta-feature learning approach to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed Meta-features with the space of distributions on the hyperparameter configurations. The authors claim that the learned meta-meta-features induce a topology on the set of datasets that is exploited to define a distribution of promising hyperparameters configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that the proposed method outperforms the state-of-the-art AutoML systems, AutoSkLearn and Probabilistic Matrix Factorization. "
739,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, the authors learn a set of base sub-networks of different sizes, and aggregated on-demand according to inference requirements. Extensive experiments demonstrate the effectiveness of the proposed method."
740,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, the authors learn a set of base sub-networks of different sizes, and aggregated on-demand according to inference requirements. Extensive experiments demonstrate the effectiveness of the proposed method."
741,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, the authors learn a set of base sub-networks of different sizes, and aggregated on-demand according to inference requirements. Extensive experiments demonstrate the effectiveness of the proposed method."
742,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, the authors learn a set of base sub-networks of different sizes, and aggregated on-demand according to inference requirements. Extensive experiments demonstrate the effectiveness of the proposed method."
743,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes an extragradient-type algorithm for nonconvex-nonconcave minimax problems. The main contribution of this paper is to extend the work on variational inequalities to the case where the weak Minty variational inequality (MVI) holds. In particular, the authors show that the proposed algorithm converges globally even in settings where the underlying operator exhibits limit cycles. The proposed algorithm is applicable to constrained and regularized problems and involves an adaptive stepsize allowing for potentially larger stepsizes. Moreover, a variant with stochastic oracles is proposed, which is directly relevant for training of generative adversarial networks."
744,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes an extragradient-type algorithm for nonconvex-nonconcave minimax problems. The main contribution of this paper is to extend the work on variational inequalities to the case where the weak Minty variational inequality (MVI) holds. In particular, the authors show that the proposed algorithm converges globally even in settings where the underlying operator exhibits limit cycles. The proposed algorithm is applicable to constrained and regularized problems and involves an adaptive stepsize allowing for potentially larger stepsizes. Moreover, a variant with stochastic oracles is proposed, which is directly relevant for training of generative adversarial networks."
745,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes an extragradient-type algorithm for nonconvex-nonconcave minimax problems. The main contribution of this paper is to extend the work on variational inequalities to the case where the weak Minty variational inequality (MVI) holds. In particular, the authors show that the proposed algorithm converges globally even in settings where the underlying operator exhibits limit cycles. The proposed algorithm is applicable to constrained and regularized problems and involves an adaptive stepsize allowing for potentially larger stepsizes. Moreover, a variant with stochastic oracles is proposed, which is directly relevant for training of generative adversarial networks."
746,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes an extragradient-type algorithm for nonconvex-nonconcave minimax problems. The main contribution of this paper is to extend the work on variational inequalities to the case where the weak Minty variational inequality (MVI) holds. In particular, the authors show that the proposed algorithm converges globally even in settings where the underlying operator exhibits limit cycles. The proposed algorithm is applicable to constrained and regularized problems and involves an adaptive stepsize allowing for potentially larger stepsizes. Moreover, a variant with stochastic oracles is proposed, which is directly relevant for training of generative adversarial networks."
747,SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies the problem of neural contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves O(\sqrt{T}) finitetime regret, where T is the learning time horizon. Compared with existing neural contextual bandit algorithms, their approach is computationally much more efficient since it only needs to explore the last layer of the deep neural network."
748,SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies the problem of neural contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves O(\sqrt{T}) finitetime regret, where T is the learning time horizon. Compared with existing neural contextual bandit algorithms, their approach is computationally much more efficient since it only needs to explore the last layer of the deep neural network."
749,SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies the problem of neural contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves O(\sqrt{T}) finitetime regret, where T is the learning time horizon. Compared with existing neural contextual bandit algorithms, their approach is computationally much more efficient since it only needs to explore the last layer of the deep neural network."
750,SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies the problem of neural contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves O(\sqrt{T}) finitetime regret, where T is the learning time horizon. Compared with existing neural contextual bandit algorithms, their approach is computationally much more efficient since it only needs to explore the last layer of the deep neural network."
751,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking. To reduce exponential-time shapley computations, the methodology includes a Monte Carlo simulation to avoid unnecessary explorations. It reduces the search space by 80% and categorizes the training action sets into dispensable and indispensable groups. Additionally, it ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study."
752,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking. To reduce exponential-time shapley computations, the methodology includes a Monte Carlo simulation to avoid unnecessary explorations. It reduces the search space by 80% and categorizes the training action sets into dispensable and indispensable groups. Additionally, it ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study."
753,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking. To reduce exponential-time shapley computations, the methodology includes a Monte Carlo simulation to avoid unnecessary explorations. It reduces the search space by 80% and categorizes the training action sets into dispensable and indispensable groups. Additionally, it ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study."
754,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking. To reduce exponential-time shapley computations, the methodology includes a Monte Carlo simulation to avoid unnecessary explorations. It reduces the search space by 80% and categorizes the training action sets into dispensable and indispensable groups. Additionally, it ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study."
755,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method for estimating the uncertainty of model predictions when there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). The method assumes given importance weights that encode how the probabilities of the training examples change under the covariate shifts. The authors extend their algorithm to the setting where we are given confidence intervals for the importance weights. They demonstrate the effectiveness of their method on DomainNet and ImageNet.
756,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method for estimating the uncertainty of model predictions when there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). The method assumes given importance weights that encode how the probabilities of the training examples change under the covariate shifts. The authors extend their algorithm to the setting where we are given confidence intervals for the importance weights. They demonstrate the effectiveness of their method on DomainNet and ImageNet.
757,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method for estimating the uncertainty of model predictions when there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). The method assumes given importance weights that encode how the probabilities of the training examples change under the covariate shifts. The authors extend their algorithm to the setting where we are given confidence intervals for the importance weights. They demonstrate the effectiveness of their method on DomainNet and ImageNet.
758,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method for estimating the uncertainty of model predictions when there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). The method assumes given importance weights that encode how the probabilities of the training examples change under the covariate shifts. The authors extend their algorithm to the setting where we are given confidence intervals for the importance weights. They demonstrate the effectiveness of their method on DomainNet and ImageNet.
759,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount unlabelled data to progressively refine the model parameters. In particular, the authors consider the binary Gaussian mixture model and derive bounds that are amenable to numerical evaluation. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR."
760,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount unlabelled data to progressively refine the model parameters. In particular, the authors consider the binary Gaussian mixture model and derive bounds that are amenable to numerical evaluation. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR."
761,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount unlabelled data to progressively refine the model parameters. In particular, the authors consider the binary Gaussian mixture model and derive bounds that are amenable to numerical evaluation. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR."
762,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount unlabelled data to progressively refine the model parameters. In particular, the authors consider the binary Gaussian mixture model and derive bounds that are amenable to numerical evaluation. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR."
763,SP:570149eb8fb97928f94312e40bdc48dfe9885848,This paper proposes a generative planning method for model-free reinforcement learning. The main idea is to generate multi-step action sequences for reaching high-value regions. The proposed method is evaluated on several benchmark environments and compared with several baseline methods.
764,SP:570149eb8fb97928f94312e40bdc48dfe9885848,This paper proposes a generative planning method for model-free reinforcement learning. The main idea is to generate multi-step action sequences for reaching high-value regions. The proposed method is evaluated on several benchmark environments and compared with several baseline methods.
765,SP:570149eb8fb97928f94312e40bdc48dfe9885848,This paper proposes a generative planning method for model-free reinforcement learning. The main idea is to generate multi-step action sequences for reaching high-value regions. The proposed method is evaluated on several benchmark environments and compared with several baseline methods.
766,SP:570149eb8fb97928f94312e40bdc48dfe9885848,This paper proposes a generative planning method for model-free reinforcement learning. The main idea is to generate multi-step action sequences for reaching high-value regions. The proposed method is evaluated on several benchmark environments and compared with several baseline methods.
767,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorization to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix and the tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix or tensor decomposition methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines.
768,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorization to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix and the tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix or tensor decomposition methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines.
769,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorization to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix and the tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix or tensor decomposition methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines.
770,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorization to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix and the tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix or tensor decomposition methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines.
771,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes an interpretation technique to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. The goal is to train a function as an interpreter for the target output variable. The proposed method is based on an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. Experiments on simulated and real data sets demonstrate the effectiveness of the proposed method."
772,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes an interpretation technique to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. The goal is to train a function as an interpreter for the target output variable. The proposed method is based on an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. Experiments on simulated and real data sets demonstrate the effectiveness of the proposed method."
773,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes an interpretation technique to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. The goal is to train a function as an interpreter for the target output variable. The proposed method is based on an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. Experiments on simulated and real data sets demonstrate the effectiveness of the proposed method."
774,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes an interpretation technique to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. The goal is to train a function as an interpreter for the target output variable. The proposed method is based on an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. Experiments on simulated and real data sets demonstrate the effectiveness of the proposed method."
775,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a method for learning a policy that maximizes a function of the cumulative distribution function (CDF) of the full-episode outcomes. The CDF function is defined as the sum of the expected returns of all possible actions in the full episode. The authors show how to estimate the policy gradient for a broad class of CDF-based objectives via sampling, and how to incorporate variance reduction measures to facilitate effective on-policy learning. They show that the proposed method can be used to train agents with different “risk profiles” in penalty-based formulations of six OpenAI Safety Gym environments, and that moderate emphasis on improvement in training scenarios where the agent performs poorly both increases the accumulation of positive rewards and decreases the frequency of incurred penalties."
776,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a method for learning a policy that maximizes a function of the cumulative distribution function (CDF) of the full-episode outcomes. The CDF function is defined as the sum of the expected returns of all possible actions in the full episode. The authors show how to estimate the policy gradient for a broad class of CDF-based objectives via sampling, and how to incorporate variance reduction measures to facilitate effective on-policy learning. They show that the proposed method can be used to train agents with different “risk profiles” in penalty-based formulations of six OpenAI Safety Gym environments, and that moderate emphasis on improvement in training scenarios where the agent performs poorly both increases the accumulation of positive rewards and decreases the frequency of incurred penalties."
777,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a method for learning a policy that maximizes a function of the cumulative distribution function (CDF) of the full-episode outcomes. The CDF function is defined as the sum of the expected returns of all possible actions in the full episode. The authors show how to estimate the policy gradient for a broad class of CDF-based objectives via sampling, and how to incorporate variance reduction measures to facilitate effective on-policy learning. They show that the proposed method can be used to train agents with different “risk profiles” in penalty-based formulations of six OpenAI Safety Gym environments, and that moderate emphasis on improvement in training scenarios where the agent performs poorly both increases the accumulation of positive rewards and decreases the frequency of incurred penalties."
778,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a method for learning a policy that maximizes a function of the cumulative distribution function (CDF) of the full-episode outcomes. The CDF function is defined as the sum of the expected returns of all possible actions in the full episode. The authors show how to estimate the policy gradient for a broad class of CDF-based objectives via sampling, and how to incorporate variance reduction measures to facilitate effective on-policy learning. They show that the proposed method can be used to train agents with different “risk profiles” in penalty-based formulations of six OpenAI Safety Gym environments, and that moderate emphasis on improvement in training scenarios where the agent performs poorly both increases the accumulation of positive rewards and decreases the frequency of incurred penalties."
779,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease. The method is based on a neural process model, which is used to learn a surrogate model for the simulator dynamics. The surrogate model is trained by iteratively querying the simulator, gather more data, and continuously improving the model. The authors provide theoretical analysis and demonstrate that their approach reduces sample complexity compared with random sampling in high dimension. Empirically, the authors demonstrate that the proposed method can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples."
780,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease. The method is based on a neural process model, which is used to learn a surrogate model for the simulator dynamics. The surrogate model is trained by iteratively querying the simulator, gather more data, and continuously improving the model. The authors provide theoretical analysis and demonstrate that their approach reduces sample complexity compared with random sampling in high dimension. Empirically, the authors demonstrate that the proposed method can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples."
781,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease. The method is based on a neural process model, which is used to learn a surrogate model for the simulator dynamics. The surrogate model is trained by iteratively querying the simulator, gather more data, and continuously improving the model. The authors provide theoretical analysis and demonstrate that their approach reduces sample complexity compared with random sampling in high dimension. Empirically, the authors demonstrate that the proposed method can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples."
782,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease. The method is based on a neural process model, which is used to learn a surrogate model for the simulator dynamics. The surrogate model is trained by iteratively querying the simulator, gather more data, and continuously improving the model. The authors provide theoretical analysis and demonstrate that their approach reduces sample complexity compared with random sampling in high dimension. Empirically, the authors demonstrate that the proposed method can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples."
783,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper studies the problem of differentially private (DP) training of large deep learning models for NLP tasks. The authors show that DP-SGD can be used to improve the performance of large pretrained models with DP optimization on moderately-sized corpora. They show that the performance drop can be mitigated with (1) the use of large prerained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. They also propose a memory saving technique that allows clipping to run without instantiating per-example gradients for any linear layer in the model.
784,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper studies the problem of differentially private (DP) training of large deep learning models for NLP tasks. The authors show that DP-SGD can be used to improve the performance of large pretrained models with DP optimization on moderately-sized corpora. They show that the performance drop can be mitigated with (1) the use of large prerained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. They also propose a memory saving technique that allows clipping to run without instantiating per-example gradients for any linear layer in the model.
785,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper studies the problem of differentially private (DP) training of large deep learning models for NLP tasks. The authors show that DP-SGD can be used to improve the performance of large pretrained models with DP optimization on moderately-sized corpora. They show that the performance drop can be mitigated with (1) the use of large prerained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. They also propose a memory saving technique that allows clipping to run without instantiating per-example gradients for any linear layer in the model.
786,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper studies the problem of differentially private (DP) training of large deep learning models for NLP tasks. The authors show that DP-SGD can be used to improve the performance of large pretrained models with DP optimization on moderately-sized corpora. They show that the performance drop can be mitigated with (1) the use of large prerained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. They also propose a memory saving technique that allows clipping to run without instantiating per-example gradients for any linear layer in the model.
787,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent’s skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods significantly in terms of convergence speed and final performance."
788,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent’s skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods significantly in terms of convergence speed and final performance."
789,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent’s skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods significantly in terms of convergence speed and final performance."
790,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent’s skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods significantly in terms of convergence speed and final performance."
791,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a new split MLP architecture, CoordX, to accelerate inference and training of coordinate-based MLPs for implicit neural representations. The initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
792,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a new split MLP architecture, CoordX, to accelerate inference and training of coordinate-based MLPs for implicit neural representations. The initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
793,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a new split MLP architecture, CoordX, to accelerate inference and training of coordinate-based MLPs for implicit neural representations. The initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
794,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a new split MLP architecture, CoordX, to accelerate inference and training of coordinate-based MLPs for implicit neural representations. The initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
795,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to learn object-centric representations of visual scenes without relying on annotations. The proposed method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The paper empirically shows that the learned representations can discover objects in a scene without supervision. "
796,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to learn object-centric representations of visual scenes without relying on annotations. The proposed method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The paper empirically shows that the learned representations can discover objects in a scene without supervision. "
797,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to learn object-centric representations of visual scenes without relying on annotations. The proposed method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The paper empirically shows that the learned representations can discover objects in a scene without supervision. "
798,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to learn object-centric representations of visual scenes without relying on annotations. The proposed method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The paper empirically shows that the learned representations can discover objects in a scene without supervision. "
799,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the explanation of graph neural networks (GNNs). The authors argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. To address this problem, the authors propose to use the front-door adjustment and introduce a surrogate variable of the explanatory subgraphs. Specifically, they devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of the proposed method."
800,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the explanation of graph neural networks (GNNs). The authors argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. To address this problem, the authors propose to use the front-door adjustment and introduce a surrogate variable of the explanatory subgraphs. Specifically, they devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of the proposed method."
801,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the explanation of graph neural networks (GNNs). The authors argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. To address this problem, the authors propose to use the front-door adjustment and introduce a surrogate variable of the explanatory subgraphs. Specifically, they devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of the proposed method."
802,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the explanation of graph neural networks (GNNs). The authors argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. To address this problem, the authors propose to use the front-door adjustment and introduce a surrogate variable of the explanatory subgraphs. Specifically, they devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of the proposed method."
803,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper investigates whether pretrained models are better active learners, capable of asking for example labels that disambiguate between the possible tasks a user may be trying to specify. Specifically, the authors study a few-shot setting where the provided training data leaves the model unsure of the task: is it to predict the shape or the color of the object? Pretraining enables models to disentangle and weigh various competing features, making them good active learners that can choose disambiguous examples (e.g. the blue square), resolving this task ambiguity. "
804,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper investigates whether pretrained models are better active learners, capable of asking for example labels that disambiguate between the possible tasks a user may be trying to specify. Specifically, the authors study a few-shot setting where the provided training data leaves the model unsure of the task: is it to predict the shape or the color of the object? Pretraining enables models to disentangle and weigh various competing features, making them good active learners that can choose disambiguous examples (e.g. the blue square), resolving this task ambiguity. "
805,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper investigates whether pretrained models are better active learners, capable of asking for example labels that disambiguate between the possible tasks a user may be trying to specify. Specifically, the authors study a few-shot setting where the provided training data leaves the model unsure of the task: is it to predict the shape or the color of the object? Pretraining enables models to disentangle and weigh various competing features, making them good active learners that can choose disambiguous examples (e.g. the blue square), resolving this task ambiguity. "
806,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper investigates whether pretrained models are better active learners, capable of asking for example labels that disambiguate between the possible tasks a user may be trying to specify. Specifically, the authors study a few-shot setting where the provided training data leaves the model unsure of the task: is it to predict the shape or the color of the object? Pretraining enables models to disentangle and weigh various competing features, making them good active learners that can choose disambiguous examples (e.g. the blue square), resolving this task ambiguity. "
807,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder, which is trained to perform graph edit actions for automated program repair. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. Experimental results show that GRAPHIX significantly outperforms CodeBERT and BART and is as competitive as state-of-the-art pretrained Transformer models despite using fewer parameters. "
808,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder, which is trained to perform graph edit actions for automated program repair. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. Experimental results show that GRAPHIX significantly outperforms CodeBERT and BART and is as competitive as state-of-the-art pretrained Transformer models despite using fewer parameters. "
809,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder, which is trained to perform graph edit actions for automated program repair. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. Experimental results show that GRAPHIX significantly outperforms CodeBERT and BART and is as competitive as state-of-the-art pretrained Transformer models despite using fewer parameters. "
810,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder, which is trained to perform graph edit actions for automated program repair. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. Experimental results show that GRAPHIX significantly outperforms CodeBERT and BART and is as competitive as state-of-the-art pretrained Transformer models despite using fewer parameters. "
811,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes a new method for federated adversarial training (FAT) that relaxes the inner-maximization of Adversarial Training into a lower bound friendly to Federated Learning. The authors provide theoretical analysis about this α-weighted mechanism and its effect on the convergence of FAT. Empirically, the extensive experiments are conducted to comprehensively understand the characteristics of α-WFAT."
812,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes a new method for federated adversarial training (FAT) that relaxes the inner-maximization of Adversarial Training into a lower bound friendly to Federated Learning. The authors provide theoretical analysis about this α-weighted mechanism and its effect on the convergence of FAT. Empirically, the extensive experiments are conducted to comprehensively understand the characteristics of α-WFAT."
813,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes a new method for federated adversarial training (FAT) that relaxes the inner-maximization of Adversarial Training into a lower bound friendly to Federated Learning. The authors provide theoretical analysis about this α-weighted mechanism and its effect on the convergence of FAT. Empirically, the extensive experiments are conducted to comprehensively understand the characteristics of α-WFAT."
814,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes a new method for federated adversarial training (FAT) that relaxes the inner-maximization of Adversarial Training into a lower bound friendly to Federated Learning. The authors provide theoretical analysis about this α-weighted mechanism and its effect on the convergence of FAT. Empirically, the extensive experiments are conducted to comprehensively understand the characteristics of α-WFAT."
815,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a method for continual learning with unlabeled data. The main idea is to add a data-programming layer to existing continual learning methods, such as CNNL, ORDisCo, DistillMatch, and DistillNet. The idea is that the data-processing layer can be applied to any existing supervised LML framework. The authors show that the proposed method can achieve 97% performance of supervised learning on fully labeled data in terms of accuracy and catastrophic forgetting prevention."
816,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a method for continual learning with unlabeled data. The main idea is to add a data-programming layer to existing continual learning methods, such as CNNL, ORDisCo, DistillMatch, and DistillNet. The idea is that the data-processing layer can be applied to any existing supervised LML framework. The authors show that the proposed method can achieve 97% performance of supervised learning on fully labeled data in terms of accuracy and catastrophic forgetting prevention."
817,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a method for continual learning with unlabeled data. The main idea is to add a data-programming layer to existing continual learning methods, such as CNNL, ORDisCo, DistillMatch, and DistillNet. The idea is that the data-processing layer can be applied to any existing supervised LML framework. The authors show that the proposed method can achieve 97% performance of supervised learning on fully labeled data in terms of accuracy and catastrophic forgetting prevention."
818,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a method for continual learning with unlabeled data. The main idea is to add a data-programming layer to existing continual learning methods, such as CNNL, ORDisCo, DistillMatch, and DistillNet. The idea is that the data-processing layer can be applied to any existing supervised LML framework. The authors show that the proposed method can achieve 97% performance of supervised learning on fully labeled data in terms of accuracy and catastrophic forgetting prevention."
819,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two methods for generating adversarial examples that can simultaneously be misclassified by the model and detected as non-adversarial. The first method, Selective Projected Gradient Descent (SPGD), is a gradient-based attack that tries to generate adversarial samples that are orthogonal to the original gradients. The second method, Orthogonal projected gradient descent (OGD), is an adversarial example generation method that is based on the idea that the gradients of the gradient of the original gradient should be orthogonally different. The authors show that the proposed method is able to achieve 0% detection rate on four state-of-the-art detection defenses while maintaining a 0% accuracy."
820,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two methods for generating adversarial examples that can simultaneously be misclassified by the model and detected as non-adversarial. The first method, Selective Projected Gradient Descent (SPGD), is a gradient-based attack that tries to generate adversarial samples that are orthogonal to the original gradients. The second method, Orthogonal projected gradient descent (OGD), is an adversarial example generation method that is based on the idea that the gradients of the gradient of the original gradient should be orthogonally different. The authors show that the proposed method is able to achieve 0% detection rate on four state-of-the-art detection defenses while maintaining a 0% accuracy."
821,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two methods for generating adversarial examples that can simultaneously be misclassified by the model and detected as non-adversarial. The first method, Selective Projected Gradient Descent (SPGD), is a gradient-based attack that tries to generate adversarial samples that are orthogonal to the original gradients. The second method, Orthogonal projected gradient descent (OGD), is an adversarial example generation method that is based on the idea that the gradients of the gradient of the original gradient should be orthogonally different. The authors show that the proposed method is able to achieve 0% detection rate on four state-of-the-art detection defenses while maintaining a 0% accuracy."
822,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two methods for generating adversarial examples that can simultaneously be misclassified by the model and detected as non-adversarial. The first method, Selective Projected Gradient Descent (SPGD), is a gradient-based attack that tries to generate adversarial samples that are orthogonal to the original gradients. The second method, Orthogonal projected gradient descent (OGD), is an adversarial example generation method that is based on the idea that the gradients of the gradient of the original gradient should be orthogonally different. The authors show that the proposed method is able to achieve 0% detection rate on four state-of-the-art detection defenses while maintaining a 0% accuracy."
823,SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes a variational approach to evaluate the value of an ensemble of players in a cooperative game. The approach is based on the maximum entropy principle, and the authors show that under certain conditions, the variational index can be used to derive a set of game-theoretic axioms. The authors also show that the proposed approach can achieve better decoupling error than existing methods."
824,SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes a variational approach to evaluate the value of an ensemble of players in a cooperative game. The approach is based on the maximum entropy principle, and the authors show that under certain conditions, the variational index can be used to derive a set of game-theoretic axioms. The authors also show that the proposed approach can achieve better decoupling error than existing methods."
825,SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes a variational approach to evaluate the value of an ensemble of players in a cooperative game. The approach is based on the maximum entropy principle, and the authors show that under certain conditions, the variational index can be used to derive a set of game-theoretic axioms. The authors also show that the proposed approach can achieve better decoupling error than existing methods."
826,SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes a variational approach to evaluate the value of an ensemble of players in a cooperative game. The approach is based on the maximum entropy principle, and the authors show that under certain conditions, the variational index can be used to derive a set of game-theoretic axioms. The authors also show that the proposed approach can achieve better decoupling error than existing methods."
827,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator is useful in interactive learning environments arising in active learning or reinforcement learning. Experiments on downstream tasks including sequential model optimization and reinforcement learning demonstrate its advantage against existing methods."
828,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator is useful in interactive learning environments arising in active learning or reinforcement learning. Experiments on downstream tasks including sequential model optimization and reinforcement learning demonstrate its advantage against existing methods."
829,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator is useful in interactive learning environments arising in active learning or reinforcement learning. Experiments on downstream tasks including sequential model optimization and reinforcement learning demonstrate its advantage against existing methods."
830,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator is useful in interactive learning environments arising in active learning or reinforcement learning. Experiments on downstream tasks including sequential model optimization and reinforcement learning demonstrate its advantage against existing methods."
831,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. In particular, the authors use geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n), to learn the coordinate descent algorithm. Compared to the state-of-the-art SVD method, the proposed algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs. They further improve upon vanilla product quantization significantly in an end-to-end training scenario."
832,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. In particular, the authors use geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n), to learn the coordinate descent algorithm. Compared to the state-of-the-art SVD method, the proposed algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs. They further improve upon vanilla product quantization significantly in an end-to-end training scenario."
833,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. In particular, the authors use geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n), to learn the coordinate descent algorithm. Compared to the state-of-the-art SVD method, the proposed algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs. They further improve upon vanilla product quantization significantly in an end-to-end training scenario."
834,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. In particular, the authors use geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n), to learn the coordinate descent algorithm. Compared to the state-of-the-art SVD method, the proposed algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs. They further improve upon vanilla product quantization significantly in an end-to-end training scenario."
835,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The first stage consists of a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The second stage uses a neural network to infer the relation between the source and the target. The proposed method is evaluated on the Raven Progressive Matrix (RPM) dataset."
836,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The first stage consists of a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The second stage uses a neural network to infer the relation between the source and the target. The proposed method is evaluated on the Raven Progressive Matrix (RPM) dataset."
837,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The first stage consists of a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The second stage uses a neural network to infer the relation between the source and the target. The proposed method is evaluated on the Raven Progressive Matrix (RPM) dataset."
838,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The first stage consists of a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The second stage uses a neural network to infer the relation between the source and the target. The proposed method is evaluated on the Raven Progressive Matrix (RPM) dataset."
839,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the learned representations generalize well and can be transferred to new datasets."
840,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the learned representations generalize well and can be transferred to new datasets."
841,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the learned representations generalize well and can be transferred to new datasets."
842,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the learned representations generalize well and can be transferred to new datasets."
843,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how their model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
844,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how their model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
845,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how their model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
846,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how their model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
847,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the continual learning (CL) problem in the context of pre-trained language models (PLMs). The authors compare the performance of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. The authors also conduct representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL methods on each layer. The experiments and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques."
848,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the continual learning (CL) problem in the context of pre-trained language models (PLMs). The authors compare the performance of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. The authors also conduct representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL methods on each layer. The experiments and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques."
849,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the continual learning (CL) problem in the context of pre-trained language models (PLMs). The authors compare the performance of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. The authors also conduct representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL methods on each layer. The experiments and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques."
850,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the continual learning (CL) problem in the context of pre-trained language models (PLMs). The authors compare the performance of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. The authors also conduct representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL methods on each layer. The experiments and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques."
851,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper proposes a defense against directed deviation attack, a state-of-the-art model poisoning attack. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. The proposed TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients."
852,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper proposes a defense against directed deviation attack, a state-of-the-art model poisoning attack. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. The proposed TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients."
853,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper proposes a defense against directed deviation attack, a state-of-the-art model poisoning attack. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. The proposed TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients."
854,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper proposes a defense against directed deviation attack, a state-of-the-art model poisoning attack. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. The proposed TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients."
855,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a debiasing method for estimating the average marginal effects of linear functionals of high-dimensional regression functions. The authors propose to learn the Riesz representation of the linear functional using Neural Nets and Random Forests. The main contribution of the paper is to propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined neural network and regression loss, while sharing representation layers for the two functions. They also propose a Random Forest method which learns a locally linear representation of linear functions. Experiments show that the proposed method outperforms Shi et al. (2019) for the average treatment effect functional."
856,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a debiasing method for estimating the average marginal effects of linear functionals of high-dimensional regression functions. The authors propose to learn the Riesz representation of the linear functional using Neural Nets and Random Forests. The main contribution of the paper is to propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined neural network and regression loss, while sharing representation layers for the two functions. They also propose a Random Forest method which learns a locally linear representation of linear functions. Experiments show that the proposed method outperforms Shi et al. (2019) for the average treatment effect functional."
857,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a debiasing method for estimating the average marginal effects of linear functionals of high-dimensional regression functions. The authors propose to learn the Riesz representation of the linear functional using Neural Nets and Random Forests. The main contribution of the paper is to propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined neural network and regression loss, while sharing representation layers for the two functions. They also propose a Random Forest method which learns a locally linear representation of linear functions. Experiments show that the proposed method outperforms Shi et al. (2019) for the average treatment effect functional."
858,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a debiasing method for estimating the average marginal effects of linear functionals of high-dimensional regression functions. The authors propose to learn the Riesz representation of the linear functional using Neural Nets and Random Forests. The main contribution of the paper is to propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined neural network and regression loss, while sharing representation layers for the two functions. They also propose a Random Forest method which learns a locally linear representation of linear functions. Experiments show that the proposed method outperforms Shi et al. (2019) for the average treatment effect functional."
859,SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. The authors consider a practical MARL setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. They propose a mini-batch Markovian sampled fully decentralized actor-Critic algorithm and analyze its finite-time convergence and sample complexity. They show that the sample complexity of this algorithm is O(N/\epsilon^2/\ell_2) and matches that of the state-of-the-art single-agent actorcritic algorithms for reinforcement learning."
860,SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. The authors consider a practical MARL setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. They propose a mini-batch Markovian sampled fully decentralized actor-Critic algorithm and analyze its finite-time convergence and sample complexity. They show that the sample complexity of this algorithm is O(N/\epsilon^2/\ell_2) and matches that of the state-of-the-art single-agent actorcritic algorithms for reinforcement learning."
861,SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. The authors consider a practical MARL setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. They propose a mini-batch Markovian sampled fully decentralized actor-Critic algorithm and analyze its finite-time convergence and sample complexity. They show that the sample complexity of this algorithm is O(N/\epsilon^2/\ell_2) and matches that of the state-of-the-art single-agent actorcritic algorithms for reinforcement learning."
862,SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. The authors consider a practical MARL setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. They propose a mini-batch Markovian sampled fully decentralized actor-Critic algorithm and analyze its finite-time convergence and sample complexity. They show that the sample complexity of this algorithm is O(N/\epsilon^2/\ell_2) and matches that of the state-of-the-art single-agent actorcritic algorithms for reinforcement learning."
863,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper proposes a new theoretical understanding of contrastive learning. The main idea is that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make intra-class samples cluster together. The paper suggests an alternative understanding of the role of aligning positive samples is more like a surrogate task than an ultimate goal, and it is the overlapping augmented views (i.e., the chaos) that create a ladder to gradually learn class-separated representations."
864,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper proposes a new theoretical understanding of contrastive learning. The main idea is that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make intra-class samples cluster together. The paper suggests an alternative understanding of the role of aligning positive samples is more like a surrogate task than an ultimate goal, and it is the overlapping augmented views (i.e., the chaos) that create a ladder to gradually learn class-separated representations."
865,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper proposes a new theoretical understanding of contrastive learning. The main idea is that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make intra-class samples cluster together. The paper suggests an alternative understanding of the role of aligning positive samples is more like a surrogate task than an ultimate goal, and it is the overlapping augmented views (i.e., the chaos) that create a ladder to gradually learn class-separated representations."
866,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper proposes a new theoretical understanding of contrastive learning. The main idea is that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make intra-class samples cluster together. The paper suggests an alternative understanding of the role of aligning positive samples is more like a surrogate task than an ultimate goal, and it is the overlapping augmented views (i.e., the chaos) that create a ladder to gradually learn class-separated representations."
867,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The proposed method achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model. Moreover, the proposed method generalizes well to severely degraded real speech recordings."
868,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The proposed method achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model. Moreover, the proposed method generalizes well to severely degraded real speech recordings."
869,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The proposed method achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model. Moreover, the proposed method generalizes well to severely degraded real speech recordings."
870,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The proposed method achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model. Moreover, the proposed method generalizes well to severely degraded real speech recordings."
871,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a method for modeling multivariate time series with residual tensor networks (MVSRTN). The main idea is to derive the mathematical representation of the variable space and then use a tensor network based on the idea of low-rank approximation. The tensor components are shared to ensure the translation invariance of the network. In order to improve the ability to model long-term sequences, the authors propose an N-order residual connection approach and couple it to the space-approximated tensor neural network. Moreover, the seriesvariable encoder is designed to improve quality of variable space, and use the skip-connection layer to achieve the dissemination of information such as scale. Experimental results verify the effectiveness of the proposed method."
872,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a method for modeling multivariate time series with residual tensor networks (MVSRTN). The main idea is to derive the mathematical representation of the variable space and then use a tensor network based on the idea of low-rank approximation. The tensor components are shared to ensure the translation invariance of the network. In order to improve the ability to model long-term sequences, the authors propose an N-order residual connection approach and couple it to the space-approximated tensor neural network. Moreover, the seriesvariable encoder is designed to improve quality of variable space, and use the skip-connection layer to achieve the dissemination of information such as scale. Experimental results verify the effectiveness of the proposed method."
873,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a method for modeling multivariate time series with residual tensor networks (MVSRTN). The main idea is to derive the mathematical representation of the variable space and then use a tensor network based on the idea of low-rank approximation. The tensor components are shared to ensure the translation invariance of the network. In order to improve the ability to model long-term sequences, the authors propose an N-order residual connection approach and couple it to the space-approximated tensor neural network. Moreover, the seriesvariable encoder is designed to improve quality of variable space, and use the skip-connection layer to achieve the dissemination of information such as scale. Experimental results verify the effectiveness of the proposed method."
874,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a method for modeling multivariate time series with residual tensor networks (MVSRTN). The main idea is to derive the mathematical representation of the variable space and then use a tensor network based on the idea of low-rank approximation. The tensor components are shared to ensure the translation invariance of the network. In order to improve the ability to model long-term sequences, the authors propose an N-order residual connection approach and couple it to the space-approximated tensor neural network. Moreover, the seriesvariable encoder is designed to improve quality of variable space, and use the skip-connection layer to achieve the dissemination of information such as scale. Experimental results verify the effectiveness of the proposed method."
875,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper proposes a variant of Stochastic Compositional Optimization (SCO) for training GNNs on large graphs. The main idea is to store the moving averages of the aggregated features of all nodes in the graph in the most recent iteration. Theoretical analysis shows that the proposed algorithm preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. The experiments validate the theoretical results and show that the algorithm outperforms the traditional Adam SGD for GNN training with a small memory overhead.
876,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper proposes a variant of Stochastic Compositional Optimization (SCO) for training GNNs on large graphs. The main idea is to store the moving averages of the aggregated features of all nodes in the graph in the most recent iteration. Theoretical analysis shows that the proposed algorithm preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. The experiments validate the theoretical results and show that the algorithm outperforms the traditional Adam SGD for GNN training with a small memory overhead.
877,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper proposes a variant of Stochastic Compositional Optimization (SCO) for training GNNs on large graphs. The main idea is to store the moving averages of the aggregated features of all nodes in the graph in the most recent iteration. Theoretical analysis shows that the proposed algorithm preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. The experiments validate the theoretical results and show that the algorithm outperforms the traditional Adam SGD for GNN training with a small memory overhead.
878,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper proposes a variant of Stochastic Compositional Optimization (SCO) for training GNNs on large graphs. The main idea is to store the moving averages of the aggregated features of all nodes in the graph in the most recent iteration. Theoretical analysis shows that the proposed algorithm preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. The experiments validate the theoretical results and show that the algorithm outperforms the traditional Adam SGD for GNN training with a small memory overhead.
879,SP:72e0cac289dce803582053614ec9ee93e783c838,This paper proposes Circulant MinHash (C-MinHash) which uses only two independent random permutations in a circulant manner to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data. The paper provides theoretical results that the proposed method leads to uniformly smaller variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed algorithm.
880,SP:72e0cac289dce803582053614ec9ee93e783c838,This paper proposes Circulant MinHash (C-MinHash) which uses only two independent random permutations in a circulant manner to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data. The paper provides theoretical results that the proposed method leads to uniformly smaller variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed algorithm.
881,SP:72e0cac289dce803582053614ec9ee93e783c838,This paper proposes Circulant MinHash (C-MinHash) which uses only two independent random permutations in a circulant manner to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data. The paper provides theoretical results that the proposed method leads to uniformly smaller variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed algorithm.
882,SP:72e0cac289dce803582053614ec9ee93e783c838,This paper proposes Circulant MinHash (C-MinHash) which uses only two independent random permutations in a circulant manner to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data. The paper provides theoretical results that the proposed method leads to uniformly smaller variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed algorithm.
883,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training scheme to achieve multiple-norm robustness against the union of lp-threat models. The proposed method is based on geometric considerations of the different lp balls and costs as much as normal adversarial train against a single lp threat model. Moreover, the authors show that using their E-AT scheme one can fine-tune with just 3 epochs any lp robust model (for p \in {1, 2,∞}) and achieve multiple norm adversarial robustness. In this way, they boost the state-of-the-art for multiple norm robustness to more than 51%."
884,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training scheme to achieve multiple-norm robustness against the union of lp-threat models. The proposed method is based on geometric considerations of the different lp balls and costs as much as normal adversarial train against a single lp threat model. Moreover, the authors show that using their E-AT scheme one can fine-tune with just 3 epochs any lp robust model (for p \in {1, 2,∞}) and achieve multiple norm adversarial robustness. In this way, they boost the state-of-the-art for multiple norm robustness to more than 51%."
885,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training scheme to achieve multiple-norm robustness against the union of lp-threat models. The proposed method is based on geometric considerations of the different lp balls and costs as much as normal adversarial train against a single lp threat model. Moreover, the authors show that using their E-AT scheme one can fine-tune with just 3 epochs any lp robust model (for p \in {1, 2,∞}) and achieve multiple norm adversarial robustness. In this way, they boost the state-of-the-art for multiple norm robustness to more than 51%."
886,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training scheme to achieve multiple-norm robustness against the union of lp-threat models. The proposed method is based on geometric considerations of the different lp balls and costs as much as normal adversarial train against a single lp threat model. Moreover, the authors show that using their E-AT scheme one can fine-tune with just 3 epochs any lp robust model (for p \in {1, 2,∞}) and achieve multiple norm adversarial robustness. In this way, they boost the state-of-the-art for multiple norm robustness to more than 51%."
887,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed, which allows the gate to choose either the public encoding or the corresponding private encoding. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
888,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed, which allows the gate to choose either the public encoding or the corresponding private encoding. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
889,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed, which allows the gate to choose either the public encoding or the corresponding private encoding. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
890,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed, which allows the gate to choose either the public encoding or the corresponding private encoding. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
891,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the problem of partitioning energy-based sequence models. The authors argue that there are no good deterministic or randomized estimates of partition functions, which makes model selection and learning model parameters not only difficult, but generally undecidable. In particular, they show that under common assumptions, no useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, they consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness."
892,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the problem of partitioning energy-based sequence models. The authors argue that there are no good deterministic or randomized estimates of partition functions, which makes model selection and learning model parameters not only difficult, but generally undecidable. In particular, they show that under common assumptions, no useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, they consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness."
893,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the problem of partitioning energy-based sequence models. The authors argue that there are no good deterministic or randomized estimates of partition functions, which makes model selection and learning model parameters not only difficult, but generally undecidable. In particular, they show that under common assumptions, no useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, they consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness."
894,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the problem of partitioning energy-based sequence models. The authors argue that there are no good deterministic or randomized estimates of partition functions, which makes model selection and learning model parameters not only difficult, but generally undecidable. In particular, they show that under common assumptions, no useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, they consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness."
895,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), a new family of distance metrics, constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. ASWD is derived from a key observation that (random) linear projections of samples residing on these hypersuranfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors provide the condition under which the ASWD metric is a valid metric and show that this can be obtained by an injective neural network architecture. The ASWD can be optimized by gradient ascent efficiently."
896,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), a new family of distance metrics, constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. ASWD is derived from a key observation that (random) linear projections of samples residing on these hypersuranfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors provide the condition under which the ASWD metric is a valid metric and show that this can be obtained by an injective neural network architecture. The ASWD can be optimized by gradient ascent efficiently."
897,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), a new family of distance metrics, constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. ASWD is derived from a key observation that (random) linear projections of samples residing on these hypersuranfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors provide the condition under which the ASWD metric is a valid metric and show that this can be obtained by an injective neural network architecture. The ASWD can be optimized by gradient ascent efficiently."
898,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), a new family of distance metrics, constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. ASWD is derived from a key observation that (random) linear projections of samples residing on these hypersuranfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors provide the condition under which the ASWD metric is a valid metric and show that this can be obtained by an injective neural network architecture. The ASWD can be optimized by gradient ascent efficiently."
899,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a method for improving coordination and performance of multi-agent reinforcement learners (MARL). The proposed method, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS can subdivide complex tasks making them easier to solve and enables systems of RL agents to quickly solve environments with sparse rewards."
900,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a method for improving coordination and performance of multi-agent reinforcement learners (MARL). The proposed method, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS can subdivide complex tasks making them easier to solve and enables systems of RL agents to quickly solve environments with sparse rewards."
901,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a method for improving coordination and performance of multi-agent reinforcement learners (MARL). The proposed method, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS can subdivide complex tasks making them easier to solve and enables systems of RL agents to quickly solve environments with sparse rewards."
902,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a method for improving coordination and performance of multi-agent reinforcement learners (MARL). The proposed method, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS can subdivide complex tasks making them easier to solve and enables systems of RL agents to quickly solve environments with sparse rewards."
903,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a method for multi-hop question answering (QA) that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The proposed method, called DOCHOPPER, is able to “retrieve” either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. To enable this novel behavior, the proposed method does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact representation of a hierarchical part of a document, which can potentially be quite large."
904,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a method for multi-hop question answering (QA) that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The proposed method, called DOCHOPPER, is able to “retrieve” either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. To enable this novel behavior, the proposed method does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact representation of a hierarchical part of a document, which can potentially be quite large."
905,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a method for multi-hop question answering (QA) that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The proposed method, called DOCHOPPER, is able to “retrieve” either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. To enable this novel behavior, the proposed method does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact representation of a hierarchical part of a document, which can potentially be quite large."
906,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a method for multi-hop question answering (QA) that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The proposed method, called DOCHOPPER, is able to “retrieve” either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. To enable this novel behavior, the proposed method does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact representation of a hierarchical part of a document, which can potentially be quite large."
907,SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a method for extracting refined labels (e.g. vocal characteristics) from known initial labels. The proposed method first uses a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm. The method is validated on recordings from the MassEffect 3 video game. Experiments show that the proposed method can bring out interesting voice characteristics without any priori knowledge."
908,SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a method for extracting refined labels (e.g. vocal characteristics) from known initial labels. The proposed method first uses a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm. The method is validated on recordings from the MassEffect 3 video game. Experiments show that the proposed method can bring out interesting voice characteristics without any priori knowledge."
909,SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a method for extracting refined labels (e.g. vocal characteristics) from known initial labels. The proposed method first uses a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm. The method is validated on recordings from the MassEffect 3 video game. Experiments show that the proposed method can bring out interesting voice characteristics without any priori knowledge."
910,SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a method for extracting refined labels (e.g. vocal characteristics) from known initial labels. The proposed method first uses a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm. The method is validated on recordings from the MassEffect 3 video game. Experiments show that the proposed method can bring out interesting voice characteristics without any priori knowledge."
911,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks. The key idea is to learn a disentangled representation of local and non-local features using a task-agnostic Vision Transformer and a task specific head/tail. To enable decomposition between the task-specific and common representation, the authors propose an alternating training strategy in which task- specific learning for the heads and tails is run on the clients by fixing the Transformer, and alternates with task-aggressively learning on the server. The experimental results on various low-level and high-level computer vision datasets show that the proposed method synergistically improves the performance of task specific network of each client while maintaining privacy."
912,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks. The key idea is to learn a disentangled representation of local and non-local features using a task-agnostic Vision Transformer and a task specific head/tail. To enable decomposition between the task-specific and common representation, the authors propose an alternating training strategy in which task- specific learning for the heads and tails is run on the clients by fixing the Transformer, and alternates with task-aggressively learning on the server. The experimental results on various low-level and high-level computer vision datasets show that the proposed method synergistically improves the performance of task specific network of each client while maintaining privacy."
913,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks. The key idea is to learn a disentangled representation of local and non-local features using a task-agnostic Vision Transformer and a task specific head/tail. To enable decomposition between the task-specific and common representation, the authors propose an alternating training strategy in which task- specific learning for the heads and tails is run on the clients by fixing the Transformer, and alternates with task-aggressively learning on the server. The experimental results on various low-level and high-level computer vision datasets show that the proposed method synergistically improves the performance of task specific network of each client while maintaining privacy."
914,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks. The key idea is to learn a disentangled representation of local and non-local features using a task-agnostic Vision Transformer and a task specific head/tail. To enable decomposition between the task-specific and common representation, the authors propose an alternating training strategy in which task- specific learning for the heads and tails is run on the clients by fixing the Transformer, and alternates with task-aggressively learning on the server. The experimental results on various low-level and high-level computer vision datasets show that the proposed method synergistically improves the performance of task specific network of each client while maintaining privacy."
915,SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking in reinforcement learning (RL). The authors construct four environments with misspecified rewards and investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They find that more capable agents exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. To address this, they propose an anomaly detection task for aberrant policies and offer several baseline detectors."
916,SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking in reinforcement learning (RL). The authors construct four environments with misspecified rewards and investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They find that more capable agents exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. To address this, they propose an anomaly detection task for aberrant policies and offer several baseline detectors."
917,SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking in reinforcement learning (RL). The authors construct four environments with misspecified rewards and investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They find that more capable agents exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. To address this, they propose an anomaly detection task for aberrant policies and offer several baseline detectors."
918,SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking in reinforcement learning (RL). The authors construct four environments with misspecified rewards and investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They find that more capable agents exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. To address this, they propose an anomaly detection task for aberrant policies and offer several baseline detectors."
919,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective (f-TVO) that generalizes the TVO by replacing the Kullback-Leibler (KL) divergence with arbitary differeitiable f divergence. The f -TVO is derived from a deformed χ-geometry perspective, which is the deformed geodesic between variational posterior distribution and true posterior distribution. The authors propose to use reparameterization trick and Monte Carlo approximation. Experiments on VAE and Bayesian neural network show that the proposed f-TVOs performs better than the cooresponding baseline f-Divergence."
920,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective (f-TVO) that generalizes the TVO by replacing the Kullback-Leibler (KL) divergence with arbitary differeitiable f divergence. The f -TVO is derived from a deformed χ-geometry perspective, which is the deformed geodesic between variational posterior distribution and true posterior distribution. The authors propose to use reparameterization trick and Monte Carlo approximation. Experiments on VAE and Bayesian neural network show that the proposed f-TVOs performs better than the cooresponding baseline f-Divergence."
921,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective (f-TVO) that generalizes the TVO by replacing the Kullback-Leibler (KL) divergence with arbitary differeitiable f divergence. The f -TVO is derived from a deformed χ-geometry perspective, which is the deformed geodesic between variational posterior distribution and true posterior distribution. The authors propose to use reparameterization trick and Monte Carlo approximation. Experiments on VAE and Bayesian neural network show that the proposed f-TVOs performs better than the cooresponding baseline f-Divergence."
922,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective (f-TVO) that generalizes the TVO by replacing the Kullback-Leibler (KL) divergence with arbitary differeitiable f divergence. The f -TVO is derived from a deformed χ-geometry perspective, which is the deformed geodesic between variational posterior distribution and true posterior distribution. The authors propose to use reparameterization trick and Monte Carlo approximation. Experiments on VAE and Bayesian neural network show that the proposed f-TVOs performs better than the cooresponding baseline f-Divergence."
923,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of existing deep reinforcement learning (RL) methods in the continuous control setting. The authors show that the performance of policies trained using existing methods varies significantly across training runs, epochs of training, and evaluation runs; the critics’ initialization plays the major role in ensemble-based actor-critic exploration, while the training is mostly invariant to the actor’s initialization; a strategy based on posterior sampling explores better than the approximated UCB combined with the weighted Bellman backup; the weighted bellman backup alone cannot replace the clipped double Q-Learning; and the proposed ensemble deep deterministic policy gradient (ED2) method can achieve state-of-the-art results on continuous control tasks from OpenAI Gym MuJoCo."
924,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of existing deep reinforcement learning (RL) methods in the continuous control setting. The authors show that the performance of policies trained using existing methods varies significantly across training runs, epochs of training, and evaluation runs; the critics’ initialization plays the major role in ensemble-based actor-critic exploration, while the training is mostly invariant to the actor’s initialization; a strategy based on posterior sampling explores better than the approximated UCB combined with the weighted Bellman backup; the weighted bellman backup alone cannot replace the clipped double Q-Learning; and the proposed ensemble deep deterministic policy gradient (ED2) method can achieve state-of-the-art results on continuous control tasks from OpenAI Gym MuJoCo."
925,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of existing deep reinforcement learning (RL) methods in the continuous control setting. The authors show that the performance of policies trained using existing methods varies significantly across training runs, epochs of training, and evaluation runs; the critics’ initialization plays the major role in ensemble-based actor-critic exploration, while the training is mostly invariant to the actor’s initialization; a strategy based on posterior sampling explores better than the approximated UCB combined with the weighted Bellman backup; the weighted bellman backup alone cannot replace the clipped double Q-Learning; and the proposed ensemble deep deterministic policy gradient (ED2) method can achieve state-of-the-art results on continuous control tasks from OpenAI Gym MuJoCo."
926,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of existing deep reinforcement learning (RL) methods in the continuous control setting. The authors show that the performance of policies trained using existing methods varies significantly across training runs, epochs of training, and evaluation runs; the critics’ initialization plays the major role in ensemble-based actor-critic exploration, while the training is mostly invariant to the actor’s initialization; a strategy based on posterior sampling explores better than the approximated UCB combined with the weighted Bellman backup; the weighted bellman backup alone cannot replace the clipped double Q-Learning; and the proposed ensemble deep deterministic policy gradient (ED2) method can achieve state-of-the-art results on continuous control tasks from OpenAI Gym MuJoCo."
927,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for supervised learning. The authors propose two algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of this non-convex problem. The first algorithm, ELminimizer, finds the optimal EL fair predictor using a sequence of convex constrained optimizations. The second algorithm is a simple algorithm that is computationally more efficient compared to ELMinimizer. Experiments on real-world data show the effectiveness of the proposed algorithms."
928,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for supervised learning. The authors propose two algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of this non-convex problem. The first algorithm, ELminimizer, finds the optimal EL fair predictor using a sequence of convex constrained optimizations. The second algorithm is a simple algorithm that is computationally more efficient compared to ELMinimizer. Experiments on real-world data show the effectiveness of the proposed algorithms."
929,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for supervised learning. The authors propose two algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of this non-convex problem. The first algorithm, ELminimizer, finds the optimal EL fair predictor using a sequence of convex constrained optimizations. The second algorithm is a simple algorithm that is computationally more efficient compared to ELMinimizer. Experiments on real-world data show the effectiveness of the proposed algorithms."
930,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for supervised learning. The authors propose two algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of this non-convex problem. The first algorithm, ELminimizer, finds the optimal EL fair predictor using a sequence of convex constrained optimizations. The second algorithm is a simple algorithm that is computationally more efficient compared to ELMinimizer. Experiments on real-world data show the effectiveness of the proposed algorithms."
931,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks. The authors propose to evaluate models’ compositional skills conditioned on the semantic connections between new and old concepts. To this end, they augment a training dataset in either an inductive or deductive manner to exposure such semantic links to models. The experiments on SCAN, as well as two real-world datasets on semantic parsing, suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking."
932,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks. The authors propose to evaluate models’ compositional skills conditioned on the semantic connections between new and old concepts. To this end, they augment a training dataset in either an inductive or deductive manner to exposure such semantic links to models. The experiments on SCAN, as well as two real-world datasets on semantic parsing, suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking."
933,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks. The authors propose to evaluate models’ compositional skills conditioned on the semantic connections between new and old concepts. To this end, they augment a training dataset in either an inductive or deductive manner to exposure such semantic links to models. The experiments on SCAN, as well as two real-world datasets on semantic parsing, suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking."
934,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks. The authors propose to evaluate models’ compositional skills conditioned on the semantic connections between new and old concepts. To this end, they augment a training dataset in either an inductive or deductive manner to exposure such semantic links to models. The experiments on SCAN, as well as two real-world datasets on semantic parsing, suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking."
935,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a method for 3D shape representation learning using multi-scale wavelet decomposition. The proposed method firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
936,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a method for 3D shape representation learning using multi-scale wavelet decomposition. The proposed method firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
937,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a method for 3D shape representation learning using multi-scale wavelet decomposition. The proposed method firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
938,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a method for 3D shape representation learning using multi-scale wavelet decomposition. The proposed method firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
939,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes a combination of full and lightweight finetuning to improve the performance of a pretrained language model for natural language generation tasks. The main idea is to combine the benefits of both in-distribution (ID) and out-of-domain (OOD) fine-tuning. The authors show that an ensemble of the lightweight and full finetuned models achieves the best of both worlds: performance matching the better of both ID and OOD. They also show that a single model instead of two can achieve similar improvements with their proposed cocktail finetune. Finally, they provide some explanatory theory in a multiclass logistic regression setting with a large number of classes, describing how distillation on ID data can transfer the OOD behavior of one model to another."
940,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes a combination of full and lightweight finetuning to improve the performance of a pretrained language model for natural language generation tasks. The main idea is to combine the benefits of both in-distribution (ID) and out-of-domain (OOD) fine-tuning. The authors show that an ensemble of the lightweight and full finetuned models achieves the best of both worlds: performance matching the better of both ID and OOD. They also show that a single model instead of two can achieve similar improvements with their proposed cocktail finetune. Finally, they provide some explanatory theory in a multiclass logistic regression setting with a large number of classes, describing how distillation on ID data can transfer the OOD behavior of one model to another."
941,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes a combination of full and lightweight finetuning to improve the performance of a pretrained language model for natural language generation tasks. The main idea is to combine the benefits of both in-distribution (ID) and out-of-domain (OOD) fine-tuning. The authors show that an ensemble of the lightweight and full finetuned models achieves the best of both worlds: performance matching the better of both ID and OOD. They also show that a single model instead of two can achieve similar improvements with their proposed cocktail finetune. Finally, they provide some explanatory theory in a multiclass logistic regression setting with a large number of classes, describing how distillation on ID data can transfer the OOD behavior of one model to another."
942,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes a combination of full and lightweight finetuning to improve the performance of a pretrained language model for natural language generation tasks. The main idea is to combine the benefits of both in-distribution (ID) and out-of-domain (OOD) fine-tuning. The authors show that an ensemble of the lightweight and full finetuned models achieves the best of both worlds: performance matching the better of both ID and OOD. They also show that a single model instead of two can achieve similar improvements with their proposed cocktail finetune. Finally, they provide some explanatory theory in a multiclass logistic regression setting with a large number of classes, describing how distillation on ID data can transfer the OOD behavior of one model to another."
943,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models, a principled approach to iterative and interactive improvement of weakly supervised models via active learning. Specifically, WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets demonstrate that WARM can substantially improve the accuracy of probabilistically labels used to train downstream classifiers, with as few as 30 queries to experts. "
944,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models, a principled approach to iterative and interactive improvement of weakly supervised models via active learning. Specifically, WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets demonstrate that WARM can substantially improve the accuracy of probabilistically labels used to train downstream classifiers, with as few as 30 queries to experts. "
945,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models, a principled approach to iterative and interactive improvement of weakly supervised models via active learning. Specifically, WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets demonstrate that WARM can substantially improve the accuracy of probabilistically labels used to train downstream classifiers, with as few as 30 queries to experts. "
946,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models, a principled approach to iterative and interactive improvement of weakly supervised models via active learning. Specifically, WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets demonstrate that WARM can substantially improve the accuracy of probabilistically labels used to train downstream classifiers, with as few as 30 queries to experts. "
947,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper proposes a new algorithm for group distributionally robust optimization (Group-DRO) based on domain generalization. The key insight of the paper is that while ERM focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group DRO. Theoretically, the authors show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, they show that their proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM."
948,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper proposes a new algorithm for group distributionally robust optimization (Group-DRO) based on domain generalization. The key insight of the paper is that while ERM focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group DRO. Theoretically, the authors show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, they show that their proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM."
949,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper proposes a new algorithm for group distributionally robust optimization (Group-DRO) based on domain generalization. The key insight of the paper is that while ERM focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group DRO. Theoretically, the authors show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, they show that their proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM."
950,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper proposes a new algorithm for group distributionally robust optimization (Group-DRO) based on domain generalization. The key insight of the paper is that while ERM focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group DRO. Theoretically, the authors show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, they show that their proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM."
951,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for black-box models. The proposed method is based on the notion of directionality, which allows to identify the most influential features for prediction per instance. The authors apply their bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions."
952,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for black-box models. The proposed method is based on the notion of directionality, which allows to identify the most influential features for prediction per instance. The authors apply their bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions."
953,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for black-box models. The proposed method is based on the notion of directionality, which allows to identify the most influential features for prediction per instance. The authors apply their bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions."
954,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for black-box models. The proposed method is based on the notion of directionality, which allows to identify the most influential features for prediction per instance. The authors apply their bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions."
955,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The method is based on the idea of learning a probabilistic tree-based representation of patient history through recurrence, which is then used to learn a decision tree policy that adapts over time with patient information. The proposed method is evaluated on both synthetic and real-world datasets and compared to a number of state-of-the-art methods."
956,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The method is based on the idea of learning a probabilistic tree-based representation of patient history through recurrence, which is then used to learn a decision tree policy that adapts over time with patient information. The proposed method is evaluated on both synthetic and real-world datasets and compared to a number of state-of-the-art methods."
957,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The method is based on the idea of learning a probabilistic tree-based representation of patient history through recurrence, which is then used to learn a decision tree policy that adapts over time with patient information. The proposed method is evaluated on both synthetic and real-world datasets and compared to a number of state-of-the-art methods."
958,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The method is based on the idea of learning a probabilistic tree-based representation of patient history through recurrence, which is then used to learn a decision tree policy that adapts over time with patient information. The proposed method is evaluated on both synthetic and real-world datasets and compared to a number of state-of-the-art methods."
959,SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a multi-agent reinforcement learning (MARL) algorithm for data augmentation. The algorithm is based on the idea that each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. The experimental results show that the proposed method outperforms the state-of-the-art DA methods while requiring fewer computational resources.
960,SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a multi-agent reinforcement learning (MARL) algorithm for data augmentation. The algorithm is based on the idea that each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. The experimental results show that the proposed method outperforms the state-of-the-art DA methods while requiring fewer computational resources.
961,SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a multi-agent reinforcement learning (MARL) algorithm for data augmentation. The algorithm is based on the idea that each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. The experimental results show that the proposed method outperforms the state-of-the-art DA methods while requiring fewer computational resources.
962,SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a multi-agent reinforcement learning (MARL) algorithm for data augmentation. The algorithm is based on the idea that each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. The experimental results show that the proposed method outperforms the state-of-the-art DA methods while requiring fewer computational resources.
963,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes a causal framework for understanding adversarial vulnerability of deep neural networks. The authors construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of the intuition. Based on the causal framework, the authors propose a method to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method."
964,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes a causal framework for understanding adversarial vulnerability of deep neural networks. The authors construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of the intuition. Based on the causal framework, the authors propose a method to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method."
965,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes a causal framework for understanding adversarial vulnerability of deep neural networks. The authors construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of the intuition. Based on the causal framework, the authors propose a method to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method."
966,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes a causal framework for understanding adversarial vulnerability of deep neural networks. The authors construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of the intuition. Based on the causal framework, the authors propose a method to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method."
967,SP:9f09449a47464efb5458d0732df7664865558e6f,"This paper proposes a method for continual learning of convolutional neural networks. The proposed method is based on atom swapping, where each layer of the network is decomposed into a small set of low-rank filter atoms, and then a new filter subspace is learned for each task. The authors show that atom swapping can be applied to a wide range of optimization schemes, and the proposed method outperforms the state-of-the-art methods in both accuracy and scalability. The effectiveness of atom swapping is demonstrated both empirically and theoretically."
968,SP:9f09449a47464efb5458d0732df7664865558e6f,"This paper proposes a method for continual learning of convolutional neural networks. The proposed method is based on atom swapping, where each layer of the network is decomposed into a small set of low-rank filter atoms, and then a new filter subspace is learned for each task. The authors show that atom swapping can be applied to a wide range of optimization schemes, and the proposed method outperforms the state-of-the-art methods in both accuracy and scalability. The effectiveness of atom swapping is demonstrated both empirically and theoretically."
969,SP:9f09449a47464efb5458d0732df7664865558e6f,"This paper proposes a method for continual learning of convolutional neural networks. The proposed method is based on atom swapping, where each layer of the network is decomposed into a small set of low-rank filter atoms, and then a new filter subspace is learned for each task. The authors show that atom swapping can be applied to a wide range of optimization schemes, and the proposed method outperforms the state-of-the-art methods in both accuracy and scalability. The effectiveness of atom swapping is demonstrated both empirically and theoretically."
970,SP:9f09449a47464efb5458d0732df7664865558e6f,"This paper proposes a method for continual learning of convolutional neural networks. The proposed method is based on atom swapping, where each layer of the network is decomposed into a small set of low-rank filter atoms, and then a new filter subspace is learned for each task. The authors show that atom swapping can be applied to a wide range of optimization schemes, and the proposed method outperforms the state-of-the-art methods in both accuracy and scalability. The effectiveness of atom swapping is demonstrated both empirically and theoretically."
971,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse of Stein variational gradient descent (SVGD) in high-dimensional settings. The authors show that SVGD suffers from the curse of dimensionality, and propose to remove the bias from deterministic updates present in the “driving force” of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the qualitative side, the authors compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective, and show that the variance collapses phenomenon relates to the bias of deterministic update present in SVGD."
972,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse of Stein variational gradient descent (SVGD) in high-dimensional settings. The authors show that SVGD suffers from the curse of dimensionality, and propose to remove the bias from deterministic updates present in the “driving force” of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the qualitative side, the authors compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective, and show that the variance collapses phenomenon relates to the bias of deterministic update present in SVGD."
973,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse of Stein variational gradient descent (SVGD) in high-dimensional settings. The authors show that SVGD suffers from the curse of dimensionality, and propose to remove the bias from deterministic updates present in the “driving force” of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the qualitative side, the authors compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective, and show that the variance collapses phenomenon relates to the bias of deterministic update present in SVGD."
974,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse of Stein variational gradient descent (SVGD) in high-dimensional settings. The authors show that SVGD suffers from the curse of dimensionality, and propose to remove the bias from deterministic updates present in the “driving force” of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the qualitative side, the authors compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective, and show that the variance collapses phenomenon relates to the bias of deterministic update present in SVGD."
975,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the relationship between adversarial training (AT) and noisy labels (NL). The authors show that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Based on this, the authors propose to adopt PGD as a new criterion for sample selection to correct NL. The authors also show that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training, which suggests that AT itself is an NL correction."
976,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the relationship between adversarial training (AT) and noisy labels (NL). The authors show that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Based on this, the authors propose to adopt PGD as a new criterion for sample selection to correct NL. The authors also show that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training, which suggests that AT itself is an NL correction."
977,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the relationship between adversarial training (AT) and noisy labels (NL). The authors show that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Based on this, the authors propose to adopt PGD as a new criterion for sample selection to correct NL. The authors also show that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training, which suggests that AT itself is an NL correction."
978,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the relationship between adversarial training (AT) and noisy labels (NL). The authors show that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Based on this, the authors propose to adopt PGD as a new criterion for sample selection to correct NL. The authors also show that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training, which suggests that AT itself is an NL correction."
979,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a new statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods. "
980,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a new statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods. "
981,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a new statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods. "
982,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a new statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods. "
983,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM learns representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM, and demonstrate that the learned representations are meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. Furthermore, the interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
984,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM learns representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM, and demonstrate that the learned representations are meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. Furthermore, the interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
985,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM learns representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM, and demonstrate that the learned representations are meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. Furthermore, the interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
986,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM learns representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM, and demonstrate that the learned representations are meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. Furthermore, the interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
987,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems. The authors propose to reparametrize the optimization variables as the output of a neural network. They show that to obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization."
988,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems. The authors propose to reparametrize the optimization variables as the output of a neural network. They show that to obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization."
989,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems. The authors propose to reparametrize the optimization variables as the output of a neural network. They show that to obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization."
990,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems. The authors propose to reparametrize the optimization variables as the output of a neural network. They show that to obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization."
991,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method for non-parametric image classification. The method is based on a layered structure of Support Vector Machine (SVM) ensembles. The authors claim that the proposed method can reach higher accuracy than DCNNs when the training set is small. The experimental results show that while conventional DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples are small."
992,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method for non-parametric image classification. The method is based on a layered structure of Support Vector Machine (SVM) ensembles. The authors claim that the proposed method can reach higher accuracy than DCNNs when the training set is small. The experimental results show that while conventional DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples are small."
993,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method for non-parametric image classification. The method is based on a layered structure of Support Vector Machine (SVM) ensembles. The authors claim that the proposed method can reach higher accuracy than DCNNs when the training set is small. The experimental results show that while conventional DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples are small."
994,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method for non-parametric image classification. The method is based on a layered structure of Support Vector Machine (SVM) ensembles. The authors claim that the proposed method can reach higher accuracy than DCNNs when the training set is small. The experimental results show that while conventional DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples are small."
995,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
996,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
997,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
998,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
999,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for anytime pixel-level recognition. The architecture consists of a cascade of “exits” which are attached to the model to make multiple predictions and direct further computation. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Experiments on Cityscapes semantic segmentation and MPII human pose estimation demonstrate the effectiveness of the proposed architecture."
1000,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for anytime pixel-level recognition. The architecture consists of a cascade of “exits” which are attached to the model to make multiple predictions and direct further computation. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Experiments on Cityscapes semantic segmentation and MPII human pose estimation demonstrate the effectiveness of the proposed architecture."
1001,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for anytime pixel-level recognition. The architecture consists of a cascade of “exits” which are attached to the model to make multiple predictions and direct further computation. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Experiments on Cityscapes semantic segmentation and MPII human pose estimation demonstrate the effectiveness of the proposed architecture."
1002,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for anytime pixel-level recognition. The architecture consists of a cascade of “exits” which are attached to the model to make multiple predictions and direct further computation. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Experiments on Cityscapes semantic segmentation and MPII human pose estimation demonstrate the effectiveness of the proposed architecture."
1003,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a method for bootstrapping neural processes. The proposed method is based on the idea that the functional uncertainty can replace the latent variable in (Attentive) Neural Processes ((A)NP). The authors argue that the B(A)NPS method is non-parallelizable and memory-inefficient and fail to capture diverse patterns in the stochastic processes. To solve this problem, the authors propose a neural bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. Experiments on Bayesian optimization and contextual multi-armed bandit show that the proposed method achieves the best performance in the sequential decision-making tasks among NP methods."
1004,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a method for bootstrapping neural processes. The proposed method is based on the idea that the functional uncertainty can replace the latent variable in (Attentive) Neural Processes ((A)NP). The authors argue that the B(A)NPS method is non-parallelizable and memory-inefficient and fail to capture diverse patterns in the stochastic processes. To solve this problem, the authors propose a neural bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. Experiments on Bayesian optimization and contextual multi-armed bandit show that the proposed method achieves the best performance in the sequential decision-making tasks among NP methods."
1005,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a method for bootstrapping neural processes. The proposed method is based on the idea that the functional uncertainty can replace the latent variable in (Attentive) Neural Processes ((A)NP). The authors argue that the B(A)NPS method is non-parallelizable and memory-inefficient and fail to capture diverse patterns in the stochastic processes. To solve this problem, the authors propose a neural bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. Experiments on Bayesian optimization and contextual multi-armed bandit show that the proposed method achieves the best performance in the sequential decision-making tasks among NP methods."
1006,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a method for bootstrapping neural processes. The proposed method is based on the idea that the functional uncertainty can replace the latent variable in (Attentive) Neural Processes ((A)NP). The authors argue that the B(A)NPS method is non-parallelizable and memory-inefficient and fail to capture diverse patterns in the stochastic processes. To solve this problem, the authors propose a neural bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. Experiments on Bayesian optimization and contextual multi-armed bandit show that the proposed method achieves the best performance in the sequential decision-making tasks among NP methods."
1007,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal and self-supervised approach for pre-training genome data for regulatory downstream tasks. Specifically, they simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input, where three pre-testing tasks are proposed to improve the robustness and generalizability of the model. They pre-train their model on the ATAC-seq dataset with 17 million genome sequences."
1008,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal and self-supervised approach for pre-training genome data for regulatory downstream tasks. Specifically, they simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input, where three pre-testing tasks are proposed to improve the robustness and generalizability of the model. They pre-train their model on the ATAC-seq dataset with 17 million genome sequences."
1009,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal and self-supervised approach for pre-training genome data for regulatory downstream tasks. Specifically, they simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input, where three pre-testing tasks are proposed to improve the robustness and generalizability of the model. They pre-train their model on the ATAC-seq dataset with 17 million genome sequences."
1010,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal and self-supervised approach for pre-training genome data for regulatory downstream tasks. Specifically, they simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input, where three pre-testing tasks are proposed to improve the robustness and generalizability of the model. They pre-train their model on the ATAC-seq dataset with 17 million genome sequences."
1011,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a discriminator for out-of-distribution (OOD) detection. The discriminator is based on the geodesic distance between the underlying data distributions, and is able to combine confidence scores from the logits outputs and the learned features of a deep neural network. The proposed discriminator can be applied to any pre-trained neural network, works under different degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD sample. Empirically, the authors show that the proposed method outperforms competing state-of the-art methods on a variety of network architectures and datasets."
1012,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a discriminator for out-of-distribution (OOD) detection. The discriminator is based on the geodesic distance between the underlying data distributions, and is able to combine confidence scores from the logits outputs and the learned features of a deep neural network. The proposed discriminator can be applied to any pre-trained neural network, works under different degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD sample. Empirically, the authors show that the proposed method outperforms competing state-of the-art methods on a variety of network architectures and datasets."
1013,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a discriminator for out-of-distribution (OOD) detection. The discriminator is based on the geodesic distance between the underlying data distributions, and is able to combine confidence scores from the logits outputs and the learned features of a deep neural network. The proposed discriminator can be applied to any pre-trained neural network, works under different degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD sample. Empirically, the authors show that the proposed method outperforms competing state-of the-art methods on a variety of network architectures and datasets."
1014,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a discriminator for out-of-distribution (OOD) detection. The discriminator is based on the geodesic distance between the underlying data distributions, and is able to combine confidence scores from the logits outputs and the learned features of a deep neural network. The proposed discriminator can be applied to any pre-trained neural network, works under different degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD sample. Empirically, the authors show that the proposed method outperforms competing state-of the-art methods on a variety of network architectures and datasets."
1015,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper proposes a generalization of Cover’s Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity preserving transformations that constitute a group, such as translations and rotations. The authors show that the fraction of separable dichotomie is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. They test their theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
1016,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper proposes a generalization of Cover’s Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity preserving transformations that constitute a group, such as translations and rotations. The authors show that the fraction of separable dichotomie is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. They test their theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
1017,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper proposes a generalization of Cover’s Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity preserving transformations that constitute a group, such as translations and rotations. The authors show that the fraction of separable dichotomie is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. They test their theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
1018,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper proposes a generalization of Cover’s Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity preserving transformations that constitute a group, such as translations and rotations. The authors show that the fraction of separable dichotomie is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. They test their theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
1019,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a systematic approach to explain why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The authors base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate their approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
1020,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a systematic approach to explain why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The authors base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate their approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
1021,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a systematic approach to explain why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The authors base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate their approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
1022,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a systematic approach to explain why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The authors base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate their approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
1023,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a new architecture for kernel point convolution (KPConv) for 3D point cloud applications. The main idea is to use a depthwise kernel to reduce resource consumption, and re-calibrate the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. The paper also proposes a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on KPConv. Experimental evaluations show that the NAS-crafted MAKP-Conv network uses 96% fewer parameters on 3D Point cloud classification and segmentation benchmarks with better performance."
1024,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a new architecture for kernel point convolution (KPConv) for 3D point cloud applications. The main idea is to use a depthwise kernel to reduce resource consumption, and re-calibrate the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. The paper also proposes a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on KPConv. Experimental evaluations show that the NAS-crafted MAKP-Conv network uses 96% fewer parameters on 3D Point cloud classification and segmentation benchmarks with better performance."
1025,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a new architecture for kernel point convolution (KPConv) for 3D point cloud applications. The main idea is to use a depthwise kernel to reduce resource consumption, and re-calibrate the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. The paper also proposes a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on KPConv. Experimental evaluations show that the NAS-crafted MAKP-Conv network uses 96% fewer parameters on 3D Point cloud classification and segmentation benchmarks with better performance."
1026,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a new architecture for kernel point convolution (KPConv) for 3D point cloud applications. The main idea is to use a depthwise kernel to reduce resource consumption, and re-calibrate the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. The paper also proposes a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on KPConv. Experimental evaluations show that the NAS-crafted MAKP-Conv network uses 96% fewer parameters on 3D Point cloud classification and segmentation benchmarks with better performance."
1027,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a method to measure the data quality based on the learning behaviors of the data during adversarial training and find that low quality data may not be useful and even detrimental to the robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarial learning. They find that when low-Quality data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robust-accuracy trade-off becomes less significant."
1028,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a method to measure the data quality based on the learning behaviors of the data during adversarial training and find that low quality data may not be useful and even detrimental to the robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarial learning. They find that when low-Quality data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robust-accuracy trade-off becomes less significant."
1029,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a method to measure the data quality based on the learning behaviors of the data during adversarial training and find that low quality data may not be useful and even detrimental to the robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarial learning. They find that when low-Quality data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robust-accuracy trade-off becomes less significant."
1030,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a method to measure the data quality based on the learning behaviors of the data during adversarial training and find that low quality data may not be useful and even detrimental to the robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarial learning. They find that when low-Quality data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robust-accuracy trade-off becomes less significant."
1031,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives — Korobov functions. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU, and show that neural networks are near-optimal function approximators."
1032,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives — Korobov functions. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU, and show that neural networks are near-optimal function approximators."
1033,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives — Korobov functions. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU, and show that neural networks are near-optimal function approximators."
1034,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives — Korobov functions. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU, and show that neural networks are near-optimal function approximators."
1035,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on the speaker-listener dynamics of the Lewis Game. The authors first investigate how speaker-learner asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. They find that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. From then, they leverage this observation to control population heterogeneity without introducing confounding factors. They finally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages."
1036,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on the speaker-listener dynamics of the Lewis Game. The authors first investigate how speaker-learner asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. They find that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. From then, they leverage this observation to control population heterogeneity without introducing confounding factors. They finally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages."
1037,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on the speaker-listener dynamics of the Lewis Game. The authors first investigate how speaker-learner asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. They find that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. From then, they leverage this observation to control population heterogeneity without introducing confounding factors. They finally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages."
1038,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on the speaker-listener dynamics of the Lewis Game. The authors first investigate how speaker-learner asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. They find that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. From then, they leverage this observation to control population heterogeneity without introducing confounding factors. They finally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages."
1039,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a method for learning adaptive polynomial filters for graph neural networks (GNNs). The proposed method is based on an eigendecomposition of the graph and proposes to learn multiple adaptive filters acting on different subsets of the spectrum. Theoretical and empirical results show that the proposed method learns a better filter, thereby improving classification accuracy. Experiments are conducted on small and large graphs."
1040,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a method for learning adaptive polynomial filters for graph neural networks (GNNs). The proposed method is based on an eigendecomposition of the graph and proposes to learn multiple adaptive filters acting on different subsets of the spectrum. Theoretical and empirical results show that the proposed method learns a better filter, thereby improving classification accuracy. Experiments are conducted on small and large graphs."
1041,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a method for learning adaptive polynomial filters for graph neural networks (GNNs). The proposed method is based on an eigendecomposition of the graph and proposes to learn multiple adaptive filters acting on different subsets of the spectrum. Theoretical and empirical results show that the proposed method learns a better filter, thereby improving classification accuracy. Experiments are conducted on small and large graphs."
1042,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a method for learning adaptive polynomial filters for graph neural networks (GNNs). The proposed method is based on an eigendecomposition of the graph and proposes to learn multiple adaptive filters acting on different subsets of the spectrum. Theoretical and empirical results show that the proposed method learns a better filter, thereby improving classification accuracy. Experiments are conducted on small and large graphs."
1043,SP:903545b1b340ec5c13070e0f25f550c444de4124,This paper proposes a novel method for shortest distance embedding based on random walk. The proposed method is based on the idea that betweenness centrality (BC)-based random walk can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure. The authors also propose Distance Resampling (DR) from original walk paths before maximum likelihood optimization instead of the PMI-based optimization and prove that this strategy preserves distance relation with respect to any calibrated node via steering optimization objective to reconstruct a global distance matrix.
1044,SP:903545b1b340ec5c13070e0f25f550c444de4124,This paper proposes a novel method for shortest distance embedding based on random walk. The proposed method is based on the idea that betweenness centrality (BC)-based random walk can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure. The authors also propose Distance Resampling (DR) from original walk paths before maximum likelihood optimization instead of the PMI-based optimization and prove that this strategy preserves distance relation with respect to any calibrated node via steering optimization objective to reconstruct a global distance matrix.
1045,SP:903545b1b340ec5c13070e0f25f550c444de4124,This paper proposes a novel method for shortest distance embedding based on random walk. The proposed method is based on the idea that betweenness centrality (BC)-based random walk can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure. The authors also propose Distance Resampling (DR) from original walk paths before maximum likelihood optimization instead of the PMI-based optimization and prove that this strategy preserves distance relation with respect to any calibrated node via steering optimization objective to reconstruct a global distance matrix.
1046,SP:903545b1b340ec5c13070e0f25f550c444de4124,This paper proposes a novel method for shortest distance embedding based on random walk. The proposed method is based on the idea that betweenness centrality (BC)-based random walk can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure. The authors also propose Distance Resampling (DR) from original walk paths before maximum likelihood optimization instead of the PMI-based optimization and prove that this strategy preserves distance relation with respect to any calibrated node via steering optimization objective to reconstruct a global distance matrix.
1047,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL). The authors construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. The authors adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, the authors show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, CKL is a challenging and important problem that helps us better understand and train ever-changing LMs."
1048,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL). The authors construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. The authors adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, the authors show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, CKL is a challenging and important problem that helps us better understand and train ever-changing LMs."
1049,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL). The authors construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. The authors adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, the authors show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, CKL is a challenging and important problem that helps us better understand and train ever-changing LMs."
1050,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL). The authors construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. The authors adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, the authors show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, CKL is a challenging and important problem that helps us better understand and train ever-changing LMs."
1051,SP:639fd88482330389019fb5be7446a909b99a8609,This paper proposes a stochastic approach for the criterion minimization in decision trees. The proposed algorithm is based on the idea of greedily splitting the leaf nodes into a split and two leaf nodes until a certain stopping criterion is reached. The algorithm is shown to be faster than conventional exhaustive search by several orders of magnitude. It is also shown that the proposed algorithm minimizes an upper bound of the criterion.
1052,SP:639fd88482330389019fb5be7446a909b99a8609,This paper proposes a stochastic approach for the criterion minimization in decision trees. The proposed algorithm is based on the idea of greedily splitting the leaf nodes into a split and two leaf nodes until a certain stopping criterion is reached. The algorithm is shown to be faster than conventional exhaustive search by several orders of magnitude. It is also shown that the proposed algorithm minimizes an upper bound of the criterion.
1053,SP:639fd88482330389019fb5be7446a909b99a8609,This paper proposes a stochastic approach for the criterion minimization in decision trees. The proposed algorithm is based on the idea of greedily splitting the leaf nodes into a split and two leaf nodes until a certain stopping criterion is reached. The algorithm is shown to be faster than conventional exhaustive search by several orders of magnitude. It is also shown that the proposed algorithm minimizes an upper bound of the criterion.
1054,SP:639fd88482330389019fb5be7446a909b99a8609,This paper proposes a stochastic approach for the criterion minimization in decision trees. The proposed algorithm is based on the idea of greedily splitting the leaf nodes into a split and two leaf nodes until a certain stopping criterion is reached. The algorithm is shown to be faster than conventional exhaustive search by several orders of magnitude. It is also shown that the proposed algorithm minimizes an upper bound of the criterion.
1055,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the problem of learning rate schedulers for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The authors propose Eigencurve, a learning rate schedule that can achieve minimax optimal convergence rates (up to a constant) when the Hessian distribution is skewed, which is quite common in practice. In particular, the authors show that the optimal shape of the proposed schedule resembles that of cosine decay, which sheds light to the success of Cosine decay for such situations. Experimental results on image classification tasks on CIFAR-10 show that EigenCurve can significantly outperform step decay in the case of small epochs, especially when the number of epochs is small."
1056,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the problem of learning rate schedulers for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The authors propose Eigencurve, a learning rate schedule that can achieve minimax optimal convergence rates (up to a constant) when the Hessian distribution is skewed, which is quite common in practice. In particular, the authors show that the optimal shape of the proposed schedule resembles that of cosine decay, which sheds light to the success of Cosine decay for such situations. Experimental results on image classification tasks on CIFAR-10 show that EigenCurve can significantly outperform step decay in the case of small epochs, especially when the number of epochs is small."
1057,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the problem of learning rate schedulers for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The authors propose Eigencurve, a learning rate schedule that can achieve minimax optimal convergence rates (up to a constant) when the Hessian distribution is skewed, which is quite common in practice. In particular, the authors show that the optimal shape of the proposed schedule resembles that of cosine decay, which sheds light to the success of Cosine decay for such situations. Experimental results on image classification tasks on CIFAR-10 show that EigenCurve can significantly outperform step decay in the case of small epochs, especially when the number of epochs is small."
1058,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the problem of learning rate schedulers for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The authors propose Eigencurve, a learning rate schedule that can achieve minimax optimal convergence rates (up to a constant) when the Hessian distribution is skewed, which is quite common in practice. In particular, the authors show that the optimal shape of the proposed schedule resembles that of cosine decay, which sheds light to the success of Cosine decay for such situations. Experimental results on image classification tasks on CIFAR-10 show that EigenCurve can significantly outperform step decay in the case of small epochs, especially when the number of epochs is small."
1059,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning, where the goal is to learn a policy that maximizes the expected return of a learned dynamics model. This is done by penalizing rewards where there is insufficient data, and using a pessimistic MDP that lower bounds the true MDP. The paper compares a variety of uncertainty heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. They show that Bayesian Optimization produces superior configurations that are vastly different from those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance."
1060,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning, where the goal is to learn a policy that maximizes the expected return of a learned dynamics model. This is done by penalizing rewards where there is insufficient data, and using a pessimistic MDP that lower bounds the true MDP. The paper compares a variety of uncertainty heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. They show that Bayesian Optimization produces superior configurations that are vastly different from those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance."
1061,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning, where the goal is to learn a policy that maximizes the expected return of a learned dynamics model. This is done by penalizing rewards where there is insufficient data, and using a pessimistic MDP that lower bounds the true MDP. The paper compares a variety of uncertainty heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. They show that Bayesian Optimization produces superior configurations that are vastly different from those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance."
1062,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning, where the goal is to learn a policy that maximizes the expected return of a learned dynamics model. This is done by penalizing rewards where there is insufficient data, and using a pessimistic MDP that lower bounds the true MDP. The paper compares a variety of uncertainty heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. They show that Bayesian Optimization produces superior configurations that are vastly different from those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance."
1063,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a new experience replay method for off-policy model-free reinforcement learning (MfRL) based on model-augmented priority experience replay (MaPER). The main idea of MaPER is to use the TD-error of the critic network to improve the performance of the policy. The proposed method is based on the idea of curriculum learning, where the teacher network is trained to predict the Q-value of an experience based on experience replay. The experiments show that the proposed method outperforms the baselines."
1064,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a new experience replay method for off-policy model-free reinforcement learning (MfRL) based on model-augmented priority experience replay (MaPER). The main idea of MaPER is to use the TD-error of the critic network to improve the performance of the policy. The proposed method is based on the idea of curriculum learning, where the teacher network is trained to predict the Q-value of an experience based on experience replay. The experiments show that the proposed method outperforms the baselines."
1065,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a new experience replay method for off-policy model-free reinforcement learning (MfRL) based on model-augmented priority experience replay (MaPER). The main idea of MaPER is to use the TD-error of the critic network to improve the performance of the policy. The proposed method is based on the idea of curriculum learning, where the teacher network is trained to predict the Q-value of an experience based on experience replay. The experiments show that the proposed method outperforms the baselines."
1066,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a new experience replay method for off-policy model-free reinforcement learning (MfRL) based on model-augmented priority experience replay (MaPER). The main idea of MaPER is to use the TD-error of the critic network to improve the performance of the policy. The proposed method is based on the idea of curriculum learning, where the teacher network is trained to predict the Q-value of an experience based on experience replay. The experiments show that the proposed method outperforms the baselines."
1067,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a human-in-the-loop reinforcement learning method called Human-AI Copilot Optimization (HACO). HACO leverages proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that the proposed method achieves a substantially high sample efficiency in the safe driving benchmark and achieves high safety and generalizability.
1068,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a human-in-the-loop reinforcement learning method called Human-AI Copilot Optimization (HACO). HACO leverages proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that the proposed method achieves a substantially high sample efficiency in the safe driving benchmark and achieves high safety and generalizability.
1069,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a human-in-the-loop reinforcement learning method called Human-AI Copilot Optimization (HACO). HACO leverages proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that the proposed method achieves a substantially high sample efficiency in the safe driving benchmark and achieves high safety and generalizability.
1070,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a human-in-the-loop reinforcement learning method called Human-AI Copilot Optimization (HACO). HACO leverages proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that the proposed method achieves a substantially high sample efficiency in the safe driving benchmark and achieves high safety and generalizability.
1071,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. The authors theoretically prove the convergence of the iterative training process of DMIL and establish the connection between DMIL with the Expectation-Maximization algorithm. Empirically, DMIL achieves state-of-the-art few-shot imitation learning performance on the meta-world (Yu et al., 2019b) benchmark and comparable results on the Kitchen environment."
1072,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. The authors theoretically prove the convergence of the iterative training process of DMIL and establish the connection between DMIL with the Expectation-Maximization algorithm. Empirically, DMIL achieves state-of-the-art few-shot imitation learning performance on the meta-world (Yu et al., 2019b) benchmark and comparable results on the Kitchen environment."
1073,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. The authors theoretically prove the convergence of the iterative training process of DMIL and establish the connection between DMIL with the Expectation-Maximization algorithm. Empirically, DMIL achieves state-of-the-art few-shot imitation learning performance on the meta-world (Yu et al., 2019b) benchmark and comparable results on the Kitchen environment."
1074,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. The authors theoretically prove the convergence of the iterative training process of DMIL and establish the connection between DMIL with the Expectation-Maximization algorithm. Empirically, DMIL achieves state-of-the-art few-shot imitation learning performance on the meta-world (Yu et al., 2019b) benchmark and comparable results on the Kitchen environment."
1075,SP:fb0efa670729796471a7a562b231172103bb8749,This paper proposes a node embedding compression method for graph neural networks (GNNs) that uses bit vectors instead of float-point vectors to compress the input node embeddings. The authors argue that the bit vectors are more compact than the float-points and thus can be used in the training of GNNs. The proposed method is evaluated on a synthetic dataset and compared to several baselines.
1076,SP:fb0efa670729796471a7a562b231172103bb8749,This paper proposes a node embedding compression method for graph neural networks (GNNs) that uses bit vectors instead of float-point vectors to compress the input node embeddings. The authors argue that the bit vectors are more compact than the float-points and thus can be used in the training of GNNs. The proposed method is evaluated on a synthetic dataset and compared to several baselines.
1077,SP:fb0efa670729796471a7a562b231172103bb8749,This paper proposes a node embedding compression method for graph neural networks (GNNs) that uses bit vectors instead of float-point vectors to compress the input node embeddings. The authors argue that the bit vectors are more compact than the float-points and thus can be used in the training of GNNs. The proposed method is evaluated on a synthetic dataset and compared to several baselines.
1078,SP:fb0efa670729796471a7a562b231172103bb8749,This paper proposes a node embedding compression method for graph neural networks (GNNs) that uses bit vectors instead of float-point vectors to compress the input node embeddings. The authors argue that the bit vectors are more compact than the float-points and thus can be used in the training of GNNs. The proposed method is evaluated on a synthetic dataset and compared to several baselines.
1079,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper studies the problem of learning invariant representations for domain adaptation using domain-adversarial training. The authors propose to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta solvers) and derive asymptotic convergence guarantees for this family of optimizers. They show that these optimizers are significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers when used in conjunction with state-of-the-art domain adaptation methods."
1080,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper studies the problem of learning invariant representations for domain adaptation using domain-adversarial training. The authors propose to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta solvers) and derive asymptotic convergence guarantees for this family of optimizers. They show that these optimizers are significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers when used in conjunction with state-of-the-art domain adaptation methods."
1081,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper studies the problem of learning invariant representations for domain adaptation using domain-adversarial training. The authors propose to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta solvers) and derive asymptotic convergence guarantees for this family of optimizers. They show that these optimizers are significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers when used in conjunction with state-of-the-art domain adaptation methods."
1082,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper studies the problem of learning invariant representations for domain adaptation using domain-adversarial training. The authors propose to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta solvers) and derive asymptotic convergence guarantees for this family of optimizers. They show that these optimizers are significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers when used in conjunction with state-of-the-art domain adaptation methods."
1083,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes an instance-level regularizer called individual Flood (iFlood) to prevent overfitting of machine learning models. Specifically, iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The authors theoretically show that the design of the loss function can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. The experimental results on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability, and behave consistently at instance level."
1084,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes an instance-level regularizer called individual Flood (iFlood) to prevent overfitting of machine learning models. Specifically, iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The authors theoretically show that the design of the loss function can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. The experimental results on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability, and behave consistently at instance level."
1085,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes an instance-level regularizer called individual Flood (iFlood) to prevent overfitting of machine learning models. Specifically, iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The authors theoretically show that the design of the loss function can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. The experimental results on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability, and behave consistently at instance level."
1086,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes an instance-level regularizer called individual Flood (iFlood) to prevent overfitting of machine learning models. Specifically, iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The authors theoretically show that the design of the loss function can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. The experimental results on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability, and behave consistently at instance level."
1087,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a Hierarchical Value Function Spaces (HFS) method for long-horizon reinforcement learning. The main idea is to use the value functions corresponding to each lower-level skill. The value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations on maze-solving and robotic manipulation tasks demonstrate the effectiveness of the proposed method."
1088,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a Hierarchical Value Function Spaces (HFS) method for long-horizon reinforcement learning. The main idea is to use the value functions corresponding to each lower-level skill. The value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations on maze-solving and robotic manipulation tasks demonstrate the effectiveness of the proposed method."
1089,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a Hierarchical Value Function Spaces (HFS) method for long-horizon reinforcement learning. The main idea is to use the value functions corresponding to each lower-level skill. The value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations on maze-solving and robotic manipulation tasks demonstrate the effectiveness of the proposed method."
1090,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a Hierarchical Value Function Spaces (HFS) method for long-horizon reinforcement learning. The main idea is to use the value functions corresponding to each lower-level skill. The value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations on maze-solving and robotic manipulation tasks demonstrate the effectiveness of the proposed method."
1091,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. The authors propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. Top-N can replace i.i.d. generation in any VAE or GAN – it is easier to train and better captures complex dependencies in the data."
1092,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. The authors propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. Top-N can replace i.i.d. generation in any VAE or GAN – it is easier to train and better captures complex dependencies in the data."
1093,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. The authors propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. Top-N can replace i.i.d. generation in any VAE or GAN – it is easier to train and better captures complex dependencies in the data."
1094,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. The authors propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. Top-N can replace i.i.d. generation in any VAE or GAN – it is easier to train and better captures complex dependencies in the data."
1095,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). The authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. They also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, they show that the deep model accuracy will improve with growing training sets according to a power law."
1096,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). The authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. They also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, they show that the deep model accuracy will improve with growing training sets according to a power law."
1097,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). The authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. They also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, they show that the deep model accuracy will improve with growing training sets according to a power law."
1098,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). The authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. They also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, they show that the deep model accuracy will improve with growing training sets according to a power law."
1099,SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper studies the relationship between robustness and generalization in deep learning. The authors provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data augmentations (DA). In particular, they propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of domain generalization. They then discuss in details about different properties of DA and prove that under certain conditions, DA can be viewed as regularization and therefore improve generalization, and conduct extensive experiments to verify their theoretical findings."
1100,SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper studies the relationship between robustness and generalization in deep learning. The authors provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data augmentations (DA). In particular, they propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of domain generalization. They then discuss in details about different properties of DA and prove that under certain conditions, DA can be viewed as regularization and therefore improve generalization, and conduct extensive experiments to verify their theoretical findings."
1101,SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper studies the relationship between robustness and generalization in deep learning. The authors provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data augmentations (DA). In particular, they propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of domain generalization. They then discuss in details about different properties of DA and prove that under certain conditions, DA can be viewed as regularization and therefore improve generalization, and conduct extensive experiments to verify their theoretical findings."
1102,SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper studies the relationship between robustness and generalization in deep learning. The authors provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data augmentations (DA). In particular, they propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of domain generalization. They then discuss in details about different properties of DA and prove that under certain conditions, DA can be viewed as regularization and therefore improve generalization, and conduct extensive experiments to verify their theoretical findings."
1103,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of memorization in meta-learning. The authors argue that the universal label space is a confounder to be the cause of the memorization and frame the two lines of prevailing methods as different deconfounder approaches. Based on the causal inference principle of front-door adjustment, the authors propose two easy but effective deconfounder algorithms, i.e., sampling multiple versions of the meta-knowledge via Dropout and grouping the meta knowledge into multiple bins. Experiments on four benchmark datasets show the effectiveness of the proposed methods."
1104,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of memorization in meta-learning. The authors argue that the universal label space is a confounder to be the cause of the memorization and frame the two lines of prevailing methods as different deconfounder approaches. Based on the causal inference principle of front-door adjustment, the authors propose two easy but effective deconfounder algorithms, i.e., sampling multiple versions of the meta-knowledge via Dropout and grouping the meta knowledge into multiple bins. Experiments on four benchmark datasets show the effectiveness of the proposed methods."
1105,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of memorization in meta-learning. The authors argue that the universal label space is a confounder to be the cause of the memorization and frame the two lines of prevailing methods as different deconfounder approaches. Based on the causal inference principle of front-door adjustment, the authors propose two easy but effective deconfounder algorithms, i.e., sampling multiple versions of the meta-knowledge via Dropout and grouping the meta knowledge into multiple bins. Experiments on four benchmark datasets show the effectiveness of the proposed methods."
1106,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of memorization in meta-learning. The authors argue that the universal label space is a confounder to be the cause of the memorization and frame the two lines of prevailing methods as different deconfounder approaches. Based on the causal inference principle of front-door adjustment, the authors propose two easy but effective deconfounder algorithms, i.e., sampling multiple versions of the meta-knowledge via Dropout and grouping the meta knowledge into multiple bins. Experiments on four benchmark datasets show the effectiveness of the proposed methods."
1107,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the authors introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that the proposed method significantly outperforms various baselines in widely used Ad hoc teamwork tasks."
1108,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the authors introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that the proposed method significantly outperforms various baselines in widely used Ad hoc teamwork tasks."
1109,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the authors introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that the proposed method significantly outperforms various baselines in widely used Ad hoc teamwork tasks."
1110,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the authors introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that the proposed method significantly outperforms various baselines in widely used Ad hoc teamwork tasks."
1111,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The proposed method is based on an online version of Expectation-Maximization (EM) algorithm. The authors propose to use a normalizing flow (NF) model to map the data space to a latent space. The EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results are presented to illustrate the superior performance of the EMFlow compared to a couple of recently available methods."
1112,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The proposed method is based on an online version of Expectation-Maximization (EM) algorithm. The authors propose to use a normalizing flow (NF) model to map the data space to a latent space. The EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results are presented to illustrate the superior performance of the EMFlow compared to a couple of recently available methods."
1113,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The proposed method is based on an online version of Expectation-Maximization (EM) algorithm. The authors propose to use a normalizing flow (NF) model to map the data space to a latent space. The EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results are presented to illustrate the superior performance of the EMFlow compared to a couple of recently available methods."
1114,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The proposed method is based on an online version of Expectation-Maximization (EM) algorithm. The authors propose to use a normalizing flow (NF) model to map the data space to a latent space. The EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results are presented to illustrate the superior performance of the EMFlow compared to a couple of recently available methods."
1115,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a novel interpretable version of DNNs with rectified linear units (ReLUs) called deep linearly gated networks (DLGN). The authors extend the dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates are characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN disentangles the computations into two ‘mathematically’ interpretable linearities (i) the primal linearity between the input and the preactivations in the gating network and (ii) the ‘dual’ linearity in the path space in the weights network characterised by the neural network."
1116,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a novel interpretable version of DNNs with rectified linear units (ReLUs) called deep linearly gated networks (DLGN). The authors extend the dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates are characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN disentangles the computations into two ‘mathematically’ interpretable linearities (i) the primal linearity between the input and the preactivations in the gating network and (ii) the ‘dual’ linearity in the path space in the weights network characterised by the neural network."
1117,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a novel interpretable version of DNNs with rectified linear units (ReLUs) called deep linearly gated networks (DLGN). The authors extend the dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates are characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN disentangles the computations into two ‘mathematically’ interpretable linearities (i) the primal linearity between the input and the preactivations in the gating network and (ii) the ‘dual’ linearity in the path space in the weights network characterised by the neural network."
1118,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a novel interpretable version of DNNs with rectified linear units (ReLUs) called deep linearly gated networks (DLGN). The authors extend the dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates are characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN disentangles the computations into two ‘mathematically’ interpretable linearities (i) the primal linearity between the input and the preactivations in the gating network and (ii) the ‘dual’ linearity in the path space in the weights network characterised by the neural network."
1119,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalizer for vision transformers. The idea is to normalize tokens in both intra-token and inter-token manners to capture both the global contextual information and the local positional context. Experiments are conducted on ImageNet, COCO, and Long-Range Arena to show the effectiveness of the proposed method."
1120,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalizer for vision transformers. The idea is to normalize tokens in both intra-token and inter-token manners to capture both the global contextual information and the local positional context. Experiments are conducted on ImageNet, COCO, and Long-Range Arena to show the effectiveness of the proposed method."
1121,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalizer for vision transformers. The idea is to normalize tokens in both intra-token and inter-token manners to capture both the global contextual information and the local positional context. Experiments are conducted on ImageNet, COCO, and Long-Range Arena to show the effectiveness of the proposed method."
1122,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalizer for vision transformers. The idea is to normalize tokens in both intra-token and inter-token manners to capture both the global contextual information and the local positional context. Experiments are conducted on ImageNet, COCO, and Long-Range Arena to show the effectiveness of the proposed method."
1123,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes a method to measure the spectral bias of neural networks trained with SGD. The authors show that these networks indeed exhibit spectral bias, and that interventions that improve generalization sometimes increase and sometimes decrease the frequencies of the learned function. They also explore the connections between function frequency and image frequency and find that spectral bias is sensitive to the low frequencies prevalent in natural images."
1124,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes a method to measure the spectral bias of neural networks trained with SGD. The authors show that these networks indeed exhibit spectral bias, and that interventions that improve generalization sometimes increase and sometimes decrease the frequencies of the learned function. They also explore the connections between function frequency and image frequency and find that spectral bias is sensitive to the low frequencies prevalent in natural images."
1125,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes a method to measure the spectral bias of neural networks trained with SGD. The authors show that these networks indeed exhibit spectral bias, and that interventions that improve generalization sometimes increase and sometimes decrease the frequencies of the learned function. They also explore the connections between function frequency and image frequency and find that spectral bias is sensitive to the low frequencies prevalent in natural images."
1126,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes a method to measure the spectral bias of neural networks trained with SGD. The authors show that these networks indeed exhibit spectral bias, and that interventions that improve generalization sometimes increase and sometimes decrease the frequencies of the learned function. They also explore the connections between function frequency and image frequency and find that spectral bias is sensitive to the low frequencies prevalent in natural images."
1127,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper proposes a method for switching between modes of exploration in reinforcement learning (RL). The method is based on the observation that animals and humans exhibit a rich diversity in their exploration behaviors. The authors propose to use two modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. They also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, they report a promising and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time scales."
1128,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper proposes a method for switching between modes of exploration in reinforcement learning (RL). The method is based on the observation that animals and humans exhibit a rich diversity in their exploration behaviors. The authors propose to use two modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. They also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, they report a promising and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time scales."
1129,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper proposes a method for switching between modes of exploration in reinforcement learning (RL). The method is based on the observation that animals and humans exhibit a rich diversity in their exploration behaviors. The authors propose to use two modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. They also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, they report a promising and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time scales."
1130,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper proposes a method for switching between modes of exploration in reinforcement learning (RL). The method is based on the observation that animals and humans exhibit a rich diversity in their exploration behaviors. The authors propose to use two modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. They also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, they report a promising and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time scales."
1131,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. From the tree, the authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The proposed method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Empirically, experiments are conducted to demonstrate the effectiveness of the proposed method."
1132,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. From the tree, the authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The proposed method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Empirically, experiments are conducted to demonstrate the effectiveness of the proposed method."
1133,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. From the tree, the authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The proposed method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Empirically, experiments are conducted to demonstrate the effectiveness of the proposed method."
1134,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. From the tree, the authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The proposed method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Empirically, experiments are conducted to demonstrate the effectiveness of the proposed method."
1135,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a new architecture for video prediction, FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors argue that the inefficient use of parameters in the current video models is the main reason for underfitting. They analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and can be mitigated using existing image augmentation techniques."
1136,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a new architecture for video prediction, FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors argue that the inefficient use of parameters in the current video models is the main reason for underfitting. They analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and can be mitigated using existing image augmentation techniques."
1137,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a new architecture for video prediction, FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors argue that the inefficient use of parameters in the current video models is the main reason for underfitting. They analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and can be mitigated using existing image augmentation techniques."
1138,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a new architecture for video prediction, FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors argue that the inefficient use of parameters in the current video models is the main reason for underfitting. They analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and can be mitigated using existing image augmentation techniques."
1139,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the test loss of stochastic gradient descent (SGD) when training on features with arbitrary covariance structure. The authors propose an exact solveable model of SGD for both Gaussian features and arbitrary features and show that the simpler Gaussian model accurately predicts test loss for nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. They show that optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure. Finally, they extend their theory to the more usual setting of fixed subsampled training set, showing that both training and test error can be accurately predicted in their framework on real data."
1140,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the test loss of stochastic gradient descent (SGD) when training on features with arbitrary covariance structure. The authors propose an exact solveable model of SGD for both Gaussian features and arbitrary features and show that the simpler Gaussian model accurately predicts test loss for nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. They show that optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure. Finally, they extend their theory to the more usual setting of fixed subsampled training set, showing that both training and test error can be accurately predicted in their framework on real data."
1141,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the test loss of stochastic gradient descent (SGD) when training on features with arbitrary covariance structure. The authors propose an exact solveable model of SGD for both Gaussian features and arbitrary features and show that the simpler Gaussian model accurately predicts test loss for nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. They show that optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure. Finally, they extend their theory to the more usual setting of fixed subsampled training set, showing that both training and test error can be accurately predicted in their framework on real data."
1142,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the test loss of stochastic gradient descent (SGD) when training on features with arbitrary covariance structure. The authors propose an exact solveable model of SGD for both Gaussian features and arbitrary features and show that the simpler Gaussian model accurately predicts test loss for nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. They show that optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure. Finally, they extend their theory to the more usual setting of fixed subsampled training set, showing that both training and test error can be accurately predicted in their framework on real data."
1143,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the nonlinear, nonconvex optimization problem of training deep neural networks. In particular, the authors construct example optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange behaviors that are undesirable. The authors show that, with specific choices of learning rate and specific model (1) SGD may converge to local maxima, (2)SGD might only escape saddle points arbitrarily slowly, (3) sharp minima are preferred over flat ones, and (4) AMSGrad may converge faster than SGD. One crucial message is that the learning rate needs to be chosen and scheduled carefully to guarantee convergence."
1144,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the nonlinear, nonconvex optimization problem of training deep neural networks. In particular, the authors construct example optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange behaviors that are undesirable. The authors show that, with specific choices of learning rate and specific model (1) SGD may converge to local maxima, (2)SGD might only escape saddle points arbitrarily slowly, (3) sharp minima are preferred over flat ones, and (4) AMSGrad may converge faster than SGD. One crucial message is that the learning rate needs to be chosen and scheduled carefully to guarantee convergence."
1145,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the nonlinear, nonconvex optimization problem of training deep neural networks. In particular, the authors construct example optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange behaviors that are undesirable. The authors show that, with specific choices of learning rate and specific model (1) SGD may converge to local maxima, (2)SGD might only escape saddle points arbitrarily slowly, (3) sharp minima are preferred over flat ones, and (4) AMSGrad may converge faster than SGD. One crucial message is that the learning rate needs to be chosen and scheduled carefully to guarantee convergence."
1146,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the nonlinear, nonconvex optimization problem of training deep neural networks. In particular, the authors construct example optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange behaviors that are undesirable. The authors show that, with specific choices of learning rate and specific model (1) SGD may converge to local maxima, (2)SGD might only escape saddle points arbitrarily slowly, (3) sharp minima are preferred over flat ones, and (4) AMSGrad may converge faster than SGD. One crucial message is that the learning rate needs to be chosen and scheduled carefully to guarantee convergence."
1147,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper proposes a method for hyperparameter optimization in meta-learning. The method is based on knowledge distillation, where the second-order term is approximated by a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The proposed method allows online optimization and also is scalable to the hyper parametereter dimension and the horizon length. The experimental results show that the proposed method outperforms the state-of-the-art."
1148,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper proposes a method for hyperparameter optimization in meta-learning. The method is based on knowledge distillation, where the second-order term is approximated by a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The proposed method allows online optimization and also is scalable to the hyper parametereter dimension and the horizon length. The experimental results show that the proposed method outperforms the state-of-the-art."
1149,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper proposes a method for hyperparameter optimization in meta-learning. The method is based on knowledge distillation, where the second-order term is approximated by a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The proposed method allows online optimization and also is scalable to the hyper parametereter dimension and the horizon length. The experimental results show that the proposed method outperforms the state-of-the-art."
1150,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper proposes a method for hyperparameter optimization in meta-learning. The method is based on knowledge distillation, where the second-order term is approximated by a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The proposed method allows online optimization and also is scalable to the hyper parametereter dimension and the horizon length. The experimental results show that the proposed method outperforms the state-of-the-art."
1151,SP:a64b26faef315c3ece590322291bab198932c604,This paper proposes a method for few-shot meta-learning based on clustering and modulation. The proposed method is based on the common initialization of the meta-learner. The method is evaluated on two tasks: few shot image classification and cold start recommendation.
1152,SP:a64b26faef315c3ece590322291bab198932c604,This paper proposes a method for few-shot meta-learning based on clustering and modulation. The proposed method is based on the common initialization of the meta-learner. The method is evaluated on two tasks: few shot image classification and cold start recommendation.
1153,SP:a64b26faef315c3ece590322291bab198932c604,This paper proposes a method for few-shot meta-learning based on clustering and modulation. The proposed method is based on the common initialization of the meta-learner. The method is evaluated on two tasks: few shot image classification and cold start recommendation.
1154,SP:a64b26faef315c3ece590322291bab198932c604,This paper proposes a method for few-shot meta-learning based on clustering and modulation. The proposed method is based on the common initialization of the meta-learner. The method is evaluated on two tasks: few shot image classification and cold start recommendation.
1155,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. The proposed method relies on regularization to promote diversity on the OOD and ID data while preserving agreement on ID data. Extensive comparisons of the proposed method with state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical image datasets show significant gains.
1156,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. The proposed method relies on regularization to promote diversity on the OOD and ID data while preserving agreement on ID data. Extensive comparisons of the proposed method with state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical image datasets show significant gains.
1157,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. The proposed method relies on regularization to promote diversity on the OOD and ID data while preserving agreement on ID data. Extensive comparisons of the proposed method with state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical image datasets show significant gains.
1158,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. The proposed method relies on regularization to promote diversity on the OOD and ID data while preserving agreement on ID data. Extensive comparisons of the proposed method with state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical image datasets show significant gains.
1159,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a model for multi-agent trajectory prediction. The model consists of an encoder and a decoder. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal, social, and temporal dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model achieves state-of-the-art results on the nuScenes vehicle motion prediction leaderboard and achieves strong results on Argoverse vehicle prediction challenge."
1160,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a model for multi-agent trajectory prediction. The model consists of an encoder and a decoder. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal, social, and temporal dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model achieves state-of-the-art results on the nuScenes vehicle motion prediction leaderboard and achieves strong results on Argoverse vehicle prediction challenge."
1161,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a model for multi-agent trajectory prediction. The model consists of an encoder and a decoder. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal, social, and temporal dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model achieves state-of-the-art results on the nuScenes vehicle motion prediction leaderboard and achieves strong results on Argoverse vehicle prediction challenge."
1162,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a model for multi-agent trajectory prediction. The model consists of an encoder and a decoder. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal, social, and temporal dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model achieves state-of-the-art results on the nuScenes vehicle motion prediction leaderboard and achieves strong results on Argoverse vehicle prediction challenge."
1163,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper presents a user study on how well users can identify the relevant set of attributes compared to the ground-truth. To this end, the authors propose a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. The authors evaluate the baseline explanation technique against concept-based and counterfactual explanations. The results show that the baseline outperformed concept based explanations. Counterfactual explanation from an invertible neural network performed similarly as the baseline. However, they allowed users to identify some attributes more accurately."
1164,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper presents a user study on how well users can identify the relevant set of attributes compared to the ground-truth. To this end, the authors propose a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. The authors evaluate the baseline explanation technique against concept-based and counterfactual explanations. The results show that the baseline outperformed concept based explanations. Counterfactual explanation from an invertible neural network performed similarly as the baseline. However, they allowed users to identify some attributes more accurately."
1165,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper presents a user study on how well users can identify the relevant set of attributes compared to the ground-truth. To this end, the authors propose a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. The authors evaluate the baseline explanation technique against concept-based and counterfactual explanations. The results show that the baseline outperformed concept based explanations. Counterfactual explanation from an invertible neural network performed similarly as the baseline. However, they allowed users to identify some attributes more accurately."
1166,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper presents a user study on how well users can identify the relevant set of attributes compared to the ground-truth. To this end, the authors propose a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. The authors evaluate the baseline explanation technique against concept-based and counterfactual explanations. The results show that the baseline outperformed concept based explanations. Counterfactual explanation from an invertible neural network performed similarly as the baseline. However, they allowed users to identify some attributes more accurately."
1167,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an iterative training procedure for removing poisoned data from the training set. The method is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. The authors first train an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
1168,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an iterative training procedure for removing poisoned data from the training set. The method is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. The authors first train an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
1169,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an iterative training procedure for removing poisoned data from the training set. The method is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. The authors first train an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
1170,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an iterative training procedure for removing poisoned data from the training set. The method is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. The authors first train an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
1171,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively. The proposed method is conceptually simple but quite effective. The experiments demonstrate that its effectiveness lies in label-correlation utilization rather than document representation."
1172,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively. The proposed method is conceptually simple but quite effective. The experiments demonstrate that its effectiveness lies in label-correlation utilization rather than document representation."
1173,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively. The proposed method is conceptually simple but quite effective. The experiments demonstrate that its effectiveness lies in label-correlation utilization rather than document representation."
1174,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively. The proposed method is conceptually simple but quite effective. The experiments demonstrate that its effectiveness lies in label-correlation utilization rather than document representation."
1175,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of deep convolutional neural networks (DNNs) in the context of kernel methods. In particular, the authors study the RKHS of a simple hierarchical kernel with two or three convolution and pooling layers. The authors show that the kernel consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
1176,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of deep convolutional neural networks (DNNs) in the context of kernel methods. In particular, the authors study the RKHS of a simple hierarchical kernel with two or three convolution and pooling layers. The authors show that the kernel consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
1177,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of deep convolutional neural networks (DNNs) in the context of kernel methods. In particular, the authors study the RKHS of a simple hierarchical kernel with two or three convolution and pooling layers. The authors show that the kernel consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
1178,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of deep convolutional neural networks (DNNs) in the context of kernel methods. In particular, the authors study the RKHS of a simple hierarchical kernel with two or three convolution and pooling layers. The authors show that the kernel consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
1179,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK), which is the outer product of the neural network (NN) Jacobians. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. To address this issue, the authors propose two novel algorithms that change the exponent of the compute and memory requirements of the finite-width NTK, dramatically improving efficiency. The proposed algorithms are general-purpose JAX function transformations that apply to any differentiable computation."
1180,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK), which is the outer product of the neural network (NN) Jacobians. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. To address this issue, the authors propose two novel algorithms that change the exponent of the compute and memory requirements of the finite-width NTK, dramatically improving efficiency. The proposed algorithms are general-purpose JAX function transformations that apply to any differentiable computation."
1181,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK), which is the outer product of the neural network (NN) Jacobians. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. To address this issue, the authors propose two novel algorithms that change the exponent of the compute and memory requirements of the finite-width NTK, dramatically improving efficiency. The proposed algorithms are general-purpose JAX function transformations that apply to any differentiable computation."
1182,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK), which is the outer product of the neural network (NN) Jacobians. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. To address this issue, the authors propose two novel algorithms that change the exponent of the compute and memory requirements of the finite-width NTK, dramatically improving efficiency. The proposed algorithms are general-purpose JAX function transformations that apply to any differentiable computation."
1183,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes a method for offline constrained reinforcement learning (RL) in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute the policy that guarantees satisfying the cost constraints in the offline RL setting, since the offpolicy evaluation inherently has an estimation error. This paper proposes an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. The algorithm, COptiDICE, directly estimates the stationary distributions corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction."
1184,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes a method for offline constrained reinforcement learning (RL) in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute the policy that guarantees satisfying the cost constraints in the offline RL setting, since the offpolicy evaluation inherently has an estimation error. This paper proposes an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. The algorithm, COptiDICE, directly estimates the stationary distributions corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction."
1185,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes a method for offline constrained reinforcement learning (RL) in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute the policy that guarantees satisfying the cost constraints in the offline RL setting, since the offpolicy evaluation inherently has an estimation error. This paper proposes an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. The algorithm, COptiDICE, directly estimates the stationary distributions corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction."
1186,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes a method for offline constrained reinforcement learning (RL) in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute the policy that guarantees satisfying the cost constraints in the offline RL setting, since the offpolicy evaluation inherently has an estimation error. This paper proposes an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. The algorithm, COptiDICE, directly estimates the stationary distributions corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction."
1187,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a new parallel training scheme for Gated Recurrent Unit (GRU) networks. The proposed method is based on a multigrid reduction in time (MGRIT) solver. The key idea is to use a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach.
1188,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a new parallel training scheme for Gated Recurrent Unit (GRU) networks. The proposed method is based on a multigrid reduction in time (MGRIT) solver. The key idea is to use a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach.
1189,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a new parallel training scheme for Gated Recurrent Unit (GRU) networks. The proposed method is based on a multigrid reduction in time (MGRIT) solver. The key idea is to use a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach.
1190,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a new parallel training scheme for Gated Recurrent Unit (GRU) networks. The proposed method is based on a multigrid reduction in time (MGRIT) solver. The key idea is to use a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach.
1191,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method to learn a common embedding of fMRI data that denoises, and reveals its intrinsic structure. Specifically, the authors assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. The authors propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder) that learns a common shared embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. Experiments show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation."
1192,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method to learn a common embedding of fMRI data that denoises, and reveals its intrinsic structure. Specifically, the authors assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. The authors propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder) that learns a common shared embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. Experiments show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation."
1193,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method to learn a common embedding of fMRI data that denoises, and reveals its intrinsic structure. Specifically, the authors assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. The authors propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder) that learns a common shared embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. Experiments show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation."
1194,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method to learn a common embedding of fMRI data that denoises, and reveals its intrinsic structure. Specifically, the authors assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. The authors propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder) that learns a common shared embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. Experiments show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation."
1195,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution detection for multi-class, multi-label, and segmentation tasks. The new benchmark is ImageNet-21K, which is a large-scale dataset with high-resolution images and thousands of classes. To test ImageNet multiclass anomaly detectors, the authors introduce a new dataset of anomalous species. The authors also introduce a benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies."
1196,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution detection for multi-class, multi-label, and segmentation tasks. The new benchmark is ImageNet-21K, which is a large-scale dataset with high-resolution images and thousands of classes. To test ImageNet multiclass anomaly detectors, the authors introduce a new dataset of anomalous species. The authors also introduce a benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies."
1197,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution detection for multi-class, multi-label, and segmentation tasks. The new benchmark is ImageNet-21K, which is a large-scale dataset with high-resolution images and thousands of classes. To test ImageNet multiclass anomaly detectors, the authors introduce a new dataset of anomalous species. The authors also introduce a benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies."
1198,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution detection for multi-class, multi-label, and segmentation tasks. The new benchmark is ImageNet-21K, which is a large-scale dataset with high-resolution images and thousands of classes. To test ImageNet multiclass anomaly detectors, the authors introduce a new dataset of anomalous species. The authors also introduce a benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies."
1199,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of parametric tournament representations. The main contribution is to characterize the class of tournaments that arise out of d dimensional representations. Specifically, the authors show that these tournament classes have forbidden configurations which must necessarily be union of union of flip classes, a novel way to partition the set of all tournaments. For a general rank d tournament class, they show that the flip class associated with a coned-doubly regular tournament of size O(\sqrt{d}) must be a forbidden configuration. The authors further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows the authors to solve the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure."
1200,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of parametric tournament representations. The main contribution is to characterize the class of tournaments that arise out of d dimensional representations. Specifically, the authors show that these tournament classes have forbidden configurations which must necessarily be union of union of flip classes, a novel way to partition the set of all tournaments. For a general rank d tournament class, they show that the flip class associated with a coned-doubly regular tournament of size O(\sqrt{d}) must be a forbidden configuration. The authors further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows the authors to solve the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure."
1201,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of parametric tournament representations. The main contribution is to characterize the class of tournaments that arise out of d dimensional representations. Specifically, the authors show that these tournament classes have forbidden configurations which must necessarily be union of union of flip classes, a novel way to partition the set of all tournaments. For a general rank d tournament class, they show that the flip class associated with a coned-doubly regular tournament of size O(\sqrt{d}) must be a forbidden configuration. The authors further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows the authors to solve the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure."
1202,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of parametric tournament representations. The main contribution is to characterize the class of tournaments that arise out of d dimensional representations. Specifically, the authors show that these tournament classes have forbidden configurations which must necessarily be union of union of flip classes, a novel way to partition the set of all tournaments. For a general rank d tournament class, they show that the flip class associated with a coned-doubly regular tournament of size O(\sqrt{d}) must be a forbidden configuration. The authors further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows the authors to solve the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure."
1203,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a novel attention mechanism for neural processes (NPs) to capture appropriate context information. Specifically, the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in the target dataset and context embeddings independently. The proposed method is empirically evaluated on 1D regression, predator-prey model, and image completion."
1204,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a novel attention mechanism for neural processes (NPs) to capture appropriate context information. Specifically, the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in the target dataset and context embeddings independently. The proposed method is empirically evaluated on 1D regression, predator-prey model, and image completion."
1205,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a novel attention mechanism for neural processes (NPs) to capture appropriate context information. Specifically, the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in the target dataset and context embeddings independently. The proposed method is empirically evaluated on 1D regression, predator-prey model, and image completion."
1206,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a novel attention mechanism for neural processes (NPs) to capture appropriate context information. Specifically, the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in the target dataset and context embeddings independently. The proposed method is empirically evaluated on 1D regression, predator-prey model, and image completion."
1207,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a method for providing interpretability and explainability to black-box language models. The authors propose to use prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the network’s decisions. The proposed method is evaluated on a variety of tasks and shows that the proposed method outperforms the state-of-the-art.
1208,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a method for providing interpretability and explainability to black-box language models. The authors propose to use prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the network’s decisions. The proposed method is evaluated on a variety of tasks and shows that the proposed method outperforms the state-of-the-art.
1209,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a method for providing interpretability and explainability to black-box language models. The authors propose to use prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the network’s decisions. The proposed method is evaluated on a variety of tasks and shows that the proposed method outperforms the state-of-the-art.
1210,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a method for providing interpretability and explainability to black-box language models. The authors propose to use prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the network’s decisions. The proposed method is evaluated on a variety of tasks and shows that the proposed method outperforms the state-of-the-art.
1211,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. TRGP selects the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layerwise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that TRGP achieves significant improvement over related state-of-the-art methods."
1212,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. TRGP selects the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layerwise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that TRGP achieves significant improvement over related state-of-the-art methods."
1213,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. TRGP selects the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layerwise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that TRGP achieves significant improvement over related state-of-the-art methods."
1214,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. TRGP selects the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layerwise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that TRGP achieves significant improvement over related state-of-the-art methods."
1215,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a generalization bound based on the length of optimization trajectory under the gradient flow algorithm after convergence. The authors show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalisation bound, showing that short optimization paths after convergence indicate good generalization."
1216,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a generalization bound based on the length of optimization trajectory under the gradient flow algorithm after convergence. The authors show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalisation bound, showing that short optimization paths after convergence indicate good generalization."
1217,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a generalization bound based on the length of optimization trajectory under the gradient flow algorithm after convergence. The authors show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalisation bound, showing that short optimization paths after convergence indicate good generalization."
1218,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a generalization bound based on the length of optimization trajectory under the gradient flow algorithm after convergence. The authors show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalisation bound, showing that short optimization paths after convergence indicate good generalization."
1219,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, supported by theoretical and empirical findings. The authors argue that adversarial example are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. Utilizing this framework, they analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency based explanation for the commonly observed accuracy vs robustness trade-off."
1220,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, supported by theoretical and empirical findings. The authors argue that adversarial example are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. Utilizing this framework, they analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency based explanation for the commonly observed accuracy vs robustness trade-off."
1221,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, supported by theoretical and empirical findings. The authors argue that adversarial example are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. Utilizing this framework, they analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency based explanation for the commonly observed accuracy vs robustness trade-off."
1222,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, supported by theoretical and empirical findings. The authors argue that adversarial example are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. Utilizing this framework, they analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency based explanation for the commonly observed accuracy vs robustness trade-off."
1223,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper proposes a method to address the problem of heterophily in graph neural networks (GNNs). The authors first show that not all cases of homophily are harmful for GNNs with aggregation operation. Then, they propose a new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. Based on the proposed metrics and the observations, they find some cases of harmful heterophilies can be addressed by diversification operation. With this fact and knowledge of filterbanks, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer. The authors validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNN on most of the tasks without incurring significant computational burden."
1224,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper proposes a method to address the problem of heterophily in graph neural networks (GNNs). The authors first show that not all cases of homophily are harmful for GNNs with aggregation operation. Then, they propose a new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. Based on the proposed metrics and the observations, they find some cases of harmful heterophilies can be addressed by diversification operation. With this fact and knowledge of filterbanks, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer. The authors validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNN on most of the tasks without incurring significant computational burden."
1225,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper proposes a method to address the problem of heterophily in graph neural networks (GNNs). The authors first show that not all cases of homophily are harmful for GNNs with aggregation operation. Then, they propose a new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. Based on the proposed metrics and the observations, they find some cases of harmful heterophilies can be addressed by diversification operation. With this fact and knowledge of filterbanks, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer. The authors validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNN on most of the tasks without incurring significant computational burden."
1226,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper proposes a method to address the problem of heterophily in graph neural networks (GNNs). The authors first show that not all cases of homophily are harmful for GNNs with aggregation operation. Then, they propose a new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. Based on the proposed metrics and the observations, they find some cases of harmful heterophilies can be addressed by diversification operation. With this fact and knowledge of filterbanks, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer. The authors validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNN on most of the tasks without incurring significant computational burden."
1227,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a deep reinforcement learning (RL) method for solving small-sized instances of traveling salesman problems (TSP). The main idea is to leverage equivariance to facilitate training, and to interleave efficient local search heuristics with the usual RL training to smooth the value landscape. The proposed method is evaluated on random and realistic TSP problems against relevant state-of-the-art deep RL methods."
1228,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a deep reinforcement learning (RL) method for solving small-sized instances of traveling salesman problems (TSP). The main idea is to leverage equivariance to facilitate training, and to interleave efficient local search heuristics with the usual RL training to smooth the value landscape. The proposed method is evaluated on random and realistic TSP problems against relevant state-of-the-art deep RL methods."
1229,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a deep reinforcement learning (RL) method for solving small-sized instances of traveling salesman problems (TSP). The main idea is to leverage equivariance to facilitate training, and to interleave efficient local search heuristics with the usual RL training to smooth the value landscape. The proposed method is evaluated on random and realistic TSP problems against relevant state-of-the-art deep RL methods."
1230,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a deep reinforcement learning (RL) method for solving small-sized instances of traveling salesman problems (TSP). The main idea is to leverage equivariance to facilitate training, and to interleave efficient local search heuristics with the usual RL training to smooth the value landscape. The proposed method is evaluated on random and realistic TSP problems against relevant state-of-the-art deep RL methods."
1231,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a new federated learning framework to address the problem of non-IID data distribution among clients. The proposed method is based on the idea that each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. To ensure privacy, the local GANs are trained with differential privacy by adding artificial noise to the local model gradients before being upload to the PS. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation process. Extensive experiments demonstrate that the proposed framework outperforms the baseline methods by a large margin in several benchmark datasets under both the supervised and semi-supervised settings."
1232,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a new federated learning framework to address the problem of non-IID data distribution among clients. The proposed method is based on the idea that each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. To ensure privacy, the local GANs are trained with differential privacy by adding artificial noise to the local model gradients before being upload to the PS. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation process. Extensive experiments demonstrate that the proposed framework outperforms the baseline methods by a large margin in several benchmark datasets under both the supervised and semi-supervised settings."
1233,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a new federated learning framework to address the problem of non-IID data distribution among clients. The proposed method is based on the idea that each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. To ensure privacy, the local GANs are trained with differential privacy by adding artificial noise to the local model gradients before being upload to the PS. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation process. Extensive experiments demonstrate that the proposed framework outperforms the baseline methods by a large margin in several benchmark datasets under both the supervised and semi-supervised settings."
1234,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a new federated learning framework to address the problem of non-IID data distribution among clients. The proposed method is based on the idea that each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. To ensure privacy, the local GANs are trained with differential privacy by adding artificial noise to the local model gradients before being upload to the PS. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation process. Extensive experiments demonstrate that the proposed framework outperforms the baseline methods by a large margin in several benchmark datasets under both the supervised and semi-supervised settings."
1235,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a simple training method to improve certified robustness of smoothed classifiers. The main idea is to use the ""accuracy under Gaussian noise"" as an easy-to-compute proxy of adversarial robustness for each input. Then, the authors differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method outperforms existing state-of-the-art training methods."
1236,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a simple training method to improve certified robustness of smoothed classifiers. The main idea is to use the ""accuracy under Gaussian noise"" as an easy-to-compute proxy of adversarial robustness for each input. Then, the authors differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method outperforms existing state-of-the-art training methods."
1237,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a simple training method to improve certified robustness of smoothed classifiers. The main idea is to use the ""accuracy under Gaussian noise"" as an easy-to-compute proxy of adversarial robustness for each input. Then, the authors differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method outperforms existing state-of-the-art training methods."
1238,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a simple training method to improve certified robustness of smoothed classifiers. The main idea is to use the ""accuracy under Gaussian noise"" as an easy-to-compute proxy of adversarial robustness for each input. Then, the authors differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method outperforms existing state-of-the-art training methods."
1239,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,This paper proposes to remove all padding tokens from the BERT pretraining dataset. The authors propose two packing algorithms: shortest-pack-first histogram-packing (SPFHP) and non-negative least squares histogram packing (NNLSHP). The authors show that SPFHP is faster than NNLSHP and NLSHP is more depth efficient. The experiments show that the proposed method can achieve a 2x speed-up in terms of sequences/sec.
1240,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,This paper proposes to remove all padding tokens from the BERT pretraining dataset. The authors propose two packing algorithms: shortest-pack-first histogram-packing (SPFHP) and non-negative least squares histogram packing (NNLSHP). The authors show that SPFHP is faster than NNLSHP and NLSHP is more depth efficient. The experiments show that the proposed method can achieve a 2x speed-up in terms of sequences/sec.
1241,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,This paper proposes to remove all padding tokens from the BERT pretraining dataset. The authors propose two packing algorithms: shortest-pack-first histogram-packing (SPFHP) and non-negative least squares histogram packing (NNLSHP). The authors show that SPFHP is faster than NNLSHP and NLSHP is more depth efficient. The experiments show that the proposed method can achieve a 2x speed-up in terms of sequences/sec.
1242,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,This paper proposes to remove all padding tokens from the BERT pretraining dataset. The authors propose two packing algorithms: shortest-pack-first histogram-packing (SPFHP) and non-negative least squares histogram packing (NNLSHP). The authors show that SPFHP is faster than NNLSHP and NLSHP is more depth efficient. The experiments show that the proposed method can achieve a 2x speed-up in terms of sequences/sec.
1243,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm for translation models. The algorithm is a deterministic variant of Monte Carlo tree search, which can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. Empirically, the authors show that the adaptive tree-search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. "
1244,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm for translation models. The algorithm is a deterministic variant of Monte Carlo tree search, which can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. Empirically, the authors show that the adaptive tree-search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. "
1245,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm for translation models. The algorithm is a deterministic variant of Monte Carlo tree search, which can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. Empirically, the authors show that the adaptive tree-search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. "
1246,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm for translation models. The algorithm is a deterministic variant of Monte Carlo tree search, which can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. Empirically, the authors show that the adaptive tree-search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. "
1247,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method for anomaly detection that leverages Energy Based Model (EBM) to learn to associate low energies to correct values and higher energies to incorrect values. To avoid tedious data collection, the authors propose an adaptive sparse coding layer that can be used to quickly update what is normal during inference time. The authors also employ a meta learning scheme that simulates such a few shot setting during training."
1248,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method for anomaly detection that leverages Energy Based Model (EBM) to learn to associate low energies to correct values and higher energies to incorrect values. To avoid tedious data collection, the authors propose an adaptive sparse coding layer that can be used to quickly update what is normal during inference time. The authors also employ a meta learning scheme that simulates such a few shot setting during training."
1249,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method for anomaly detection that leverages Energy Based Model (EBM) to learn to associate low energies to correct values and higher energies to incorrect values. To avoid tedious data collection, the authors propose an adaptive sparse coding layer that can be used to quickly update what is normal during inference time. The authors also employ a meta learning scheme that simulates such a few shot setting during training."
1250,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method for anomaly detection that leverages Energy Based Model (EBM) to learn to associate low energies to correct values and higher energies to incorrect values. To avoid tedious data collection, the authors propose an adaptive sparse coding layer that can be used to quickly update what is normal during inference time. The authors also employ a meta learning scheme that simulates such a few shot setting during training."
1251,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in question answering (QA) systems. The authors create a large-scale dataset, CONTRAQA, which contains over 10K human-written and model-generated contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such threat, they build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
1252,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in question answering (QA) systems. The authors create a large-scale dataset, CONTRAQA, which contains over 10K human-written and model-generated contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such threat, they build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
1253,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in question answering (QA) systems. The authors create a large-scale dataset, CONTRAQA, which contains over 10K human-written and model-generated contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such threat, they build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
1254,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in question answering (QA) systems. The authors create a large-scale dataset, CONTRAQA, which contains over 10K human-written and model-generated contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such threat, they build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
1255,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes a method for cross-domain imitation learning that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Theoretical analysis is provided to characterise the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. Experiments on continuous control domains demonstrate the effectiveness of the proposed method."
1256,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes a method for cross-domain imitation learning that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Theoretical analysis is provided to characterise the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. Experiments on continuous control domains demonstrate the effectiveness of the proposed method."
1257,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes a method for cross-domain imitation learning that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Theoretical analysis is provided to characterise the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. Experiments on continuous control domains demonstrate the effectiveness of the proposed method."
1258,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes a method for cross-domain imitation learning that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Theoretical analysis is provided to characterise the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. Experiments on continuous control domains demonstrate the effectiveness of the proposed method."
1259,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a hierarchical cross contrastive learning (HCCL) method to further distill the information mismatched by the conventional contrastive loss. The HCCL uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. The proposed method not only regulates invariant on multiple hidden levels but also crosses different levels, improving the generalization ability of the learned visual representations. Extensive experimental results show that the proposed method outperforms most previous methods in various benchmark datasets."
1260,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a hierarchical cross contrastive learning (HCCL) method to further distill the information mismatched by the conventional contrastive loss. The HCCL uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. The proposed method not only regulates invariant on multiple hidden levels but also crosses different levels, improving the generalization ability of the learned visual representations. Extensive experimental results show that the proposed method outperforms most previous methods in various benchmark datasets."
1261,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a hierarchical cross contrastive learning (HCCL) method to further distill the information mismatched by the conventional contrastive loss. The HCCL uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. The proposed method not only regulates invariant on multiple hidden levels but also crosses different levels, improving the generalization ability of the learned visual representations. Extensive experimental results show that the proposed method outperforms most previous methods in various benchmark datasets."
1262,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a hierarchical cross contrastive learning (HCCL) method to further distill the information mismatched by the conventional contrastive loss. The HCCL uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. The proposed method not only regulates invariant on multiple hidden levels but also crosses different levels, improving the generalization ability of the learned visual representations. Extensive experimental results show that the proposed method outperforms most previous methods in various benchmark datasets."
1263,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a method for learning dynamic general equilibrium models for multi-agent meta-games. The method is based on reinforcement learning (RL) and is trained on a large number of agents. The authors show that RL can discover stable solutions that are -Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training. The proposed method is evaluated on real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes."
1264,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a method for learning dynamic general equilibrium models for multi-agent meta-games. The method is based on reinforcement learning (RL) and is trained on a large number of agents. The authors show that RL can discover stable solutions that are -Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training. The proposed method is evaluated on real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes."
1265,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a method for learning dynamic general equilibrium models for multi-agent meta-games. The method is based on reinforcement learning (RL) and is trained on a large number of agents. The authors show that RL can discover stable solutions that are -Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training. The proposed method is evaluated on real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes."
1266,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a method for learning dynamic general equilibrium models for multi-agent meta-games. The method is based on reinforcement learning (RL) and is trained on a large number of agents. The authors show that RL can discover stable solutions that are -Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training. The proposed method is evaluated on real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes."
1267,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method to prevent model stealing by requiring users to complete a proof-of-work before they can read the model’s predictions. The method calibrates the effort required to complete the proof of work to each query, which only introduces a slight overhead for regular users (up to 2x). To achieve this, the calibration applies tools from differential privacy to measure the information revealed by a query."
1268,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method to prevent model stealing by requiring users to complete a proof-of-work before they can read the model’s predictions. The method calibrates the effort required to complete the proof of work to each query, which only introduces a slight overhead for regular users (up to 2x). To achieve this, the calibration applies tools from differential privacy to measure the information revealed by a query."
1269,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method to prevent model stealing by requiring users to complete a proof-of-work before they can read the model’s predictions. The method calibrates the effort required to complete the proof of work to each query, which only introduces a slight overhead for regular users (up to 2x). To achieve this, the calibration applies tools from differential privacy to measure the information revealed by a query."
1270,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method to prevent model stealing by requiring users to complete a proof-of-work before they can read the model’s predictions. The method calibrates the effort required to complete the proof of work to each query, which only introduces a slight overhead for regular users (up to 2x). To achieve this, the calibration applies tools from differential privacy to measure the information revealed by a query."
1271,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a Multi-Resolution Continuous Normalizing Flows (MRCNF) model for image generation. The main contribution of the paper is to introduce a transformation between resolutions that allows for no change in the log likelihood. The authors show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time, using only one GPU."
1272,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a Multi-Resolution Continuous Normalizing Flows (MRCNF) model for image generation. The main contribution of the paper is to introduce a transformation between resolutions that allows for no change in the log likelihood. The authors show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time, using only one GPU."
1273,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a Multi-Resolution Continuous Normalizing Flows (MRCNF) model for image generation. The main contribution of the paper is to introduce a transformation between resolutions that allows for no change in the log likelihood. The authors show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time, using only one GPU."
1274,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a Multi-Resolution Continuous Normalizing Flows (MRCNF) model for image generation. The main contribution of the paper is to introduce a transformation between resolutions that allows for no change in the log likelihood. The authors show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time, using only one GPU."
1275,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect noisy labels. The method is based on the observation that good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. Based on the neighborhood information, the authors propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. Given good (but possibly imperfect) representations that are commonly available in practice, they theoretically analyze how they affect the local voting and provide guidelines for tuning neighborhood size."
1276,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect noisy labels. The method is based on the observation that good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. Based on the neighborhood information, the authors propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. Given good (but possibly imperfect) representations that are commonly available in practice, they theoretically analyze how they affect the local voting and provide guidelines for tuning neighborhood size."
1277,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect noisy labels. The method is based on the observation that good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. Based on the neighborhood information, the authors propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. Given good (but possibly imperfect) representations that are commonly available in practice, they theoretically analyze how they affect the local voting and provide guidelines for tuning neighborhood size."
1278,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect noisy labels. The method is based on the observation that good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. Based on the neighborhood information, the authors propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. Given good (but possibly imperfect) representations that are commonly available in practice, they theoretically analyze how they affect the local voting and provide guidelines for tuning neighborhood size."
1279,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a method for adversarial training of reinforcement learning agents under adversarial attacks. The proposed method is based on the actor-critic framework, where the actor is a policy-based learner and the critic is an RL-based adversary. The critic is trained to find the best policy perturbations for a given policy direction, and the actor learns to propose the best attack direction. The authors show that their method is theoretically optimal and significantly more efficient than prior work in environments with large state spaces. They also show that the proposed method achieves state-of-the-art empirical robustness in multiple tasks under strong adversaries."
1280,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a method for adversarial training of reinforcement learning agents under adversarial attacks. The proposed method is based on the actor-critic framework, where the actor is a policy-based learner and the critic is an RL-based adversary. The critic is trained to find the best policy perturbations for a given policy direction, and the actor learns to propose the best attack direction. The authors show that their method is theoretically optimal and significantly more efficient than prior work in environments with large state spaces. They also show that the proposed method achieves state-of-the-art empirical robustness in multiple tasks under strong adversaries."
1281,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a method for adversarial training of reinforcement learning agents under adversarial attacks. The proposed method is based on the actor-critic framework, where the actor is a policy-based learner and the critic is an RL-based adversary. The critic is trained to find the best policy perturbations for a given policy direction, and the actor learns to propose the best attack direction. The authors show that their method is theoretically optimal and significantly more efficient than prior work in environments with large state spaces. They also show that the proposed method achieves state-of-the-art empirical robustness in multiple tasks under strong adversaries."
1282,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a method for adversarial training of reinforcement learning agents under adversarial attacks. The proposed method is based on the actor-critic framework, where the actor is a policy-based learner and the critic is an RL-based adversary. The critic is trained to find the best policy perturbations for a given policy direction, and the actor learns to propose the best attack direction. The authors show that their method is theoretically optimal and significantly more efficient than prior work in environments with large state spaces. They also show that the proposed method achieves state-of-the-art empirical robustness in multiple tasks under strong adversaries."
1283,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy seeking algorithm, Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The novelty of the proposed algorithm is measured by a new metric, which measures the difference between policies. To address the dilemma between the task performance and the behavioral novelty in existing multi-objective optimization approaches, the authors propose to rethink the novelty-seeking problem through the lens of constrained optimization. Experimental results show that the proposed IPD can achieve a substantial improvement over previous novelty seeking methods in terms of the novelty of generated policies and their performances in the primal task."
1284,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy seeking algorithm, Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The novelty of the proposed algorithm is measured by a new metric, which measures the difference between policies. To address the dilemma between the task performance and the behavioral novelty in existing multi-objective optimization approaches, the authors propose to rethink the novelty-seeking problem through the lens of constrained optimization. Experimental results show that the proposed IPD can achieve a substantial improvement over previous novelty seeking methods in terms of the novelty of generated policies and their performances in the primal task."
1285,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy seeking algorithm, Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The novelty of the proposed algorithm is measured by a new metric, which measures the difference between policies. To address the dilemma between the task performance and the behavioral novelty in existing multi-objective optimization approaches, the authors propose to rethink the novelty-seeking problem through the lens of constrained optimization. Experimental results show that the proposed IPD can achieve a substantial improvement over previous novelty seeking methods in terms of the novelty of generated policies and their performances in the primal task."
1286,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy seeking algorithm, Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The novelty of the proposed algorithm is measured by a new metric, which measures the difference between policies. To address the dilemma between the task performance and the behavioral novelty in existing multi-objective optimization approaches, the authors propose to rethink the novelty-seeking problem through the lens of constrained optimization. Experimental results show that the proposed IPD can achieve a substantial improvement over previous novelty seeking methods in terms of the novelty of generated policies and their performances in the primal task."
1287,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes a method for removing reverberation in speech. The method is based on the observation that the reverberation of a speaker's speech can be influenced by the room geometry, materials, and speaker location. The paper proposes to learn to remove reverberation based on both the observed sounds and visual scene. The proposed method is evaluated on both simulated and real-world datasets and shows state-of-the-art performance."
1288,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes a method for removing reverberation in speech. The method is based on the observation that the reverberation of a speaker's speech can be influenced by the room geometry, materials, and speaker location. The paper proposes to learn to remove reverberation based on both the observed sounds and visual scene. The proposed method is evaluated on both simulated and real-world datasets and shows state-of-the-art performance."
1289,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes a method for removing reverberation in speech. The method is based on the observation that the reverberation of a speaker's speech can be influenced by the room geometry, materials, and speaker location. The paper proposes to learn to remove reverberation based on both the observed sounds and visual scene. The proposed method is evaluated on both simulated and real-world datasets and shows state-of-the-art performance."
1290,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes a method for removing reverberation in speech. The method is based on the observation that the reverberation of a speaker's speech can be influenced by the room geometry, materials, and speaker location. The paper proposes to learn to remove reverberation based on both the observed sounds and visual scene. The proposed method is evaluated on both simulated and real-world datasets and shows state-of-the-art performance."
1291,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a method for extrapolation to sequences that are longer than the ones seen during training. The proposed method, Attention with Linear Biases (ALiBi), biases query-key attention scores with a penalty that is proportional to their distance. The authors show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences with length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi outperforms several strong position methods on the WikiText-103 benchmark."
1292,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a method for extrapolation to sequences that are longer than the ones seen during training. The proposed method, Attention with Linear Biases (ALiBi), biases query-key attention scores with a penalty that is proportional to their distance. The authors show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences with length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi outperforms several strong position methods on the WikiText-103 benchmark."
1293,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a method for extrapolation to sequences that are longer than the ones seen during training. The proposed method, Attention with Linear Biases (ALiBi), biases query-key attention scores with a penalty that is proportional to their distance. The authors show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences with length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi outperforms several strong position methods on the WikiText-103 benchmark."
1294,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a method for extrapolation to sequences that are longer than the ones seen during training. The proposed method, Attention with Linear Biases (ALiBi), biases query-key attention scores with a penalty that is proportional to their distance. The authors show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences with length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi outperforms several strong position methods on the WikiText-103 benchmark."
1295,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the problem of multi-objective online convex optimization in the unconstrained max-min form. In particular, the authors propose a novel multi-Objective dynamic regret that is equivalent to the regret commonly used in the zero-order bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. The authors also propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min norm solver. The regret bounds of both variants are derived. Extensive experiments demonstrate the effectiveness of the proposed algorithm."
1296,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the problem of multi-objective online convex optimization in the unconstrained max-min form. In particular, the authors propose a novel multi-Objective dynamic regret that is equivalent to the regret commonly used in the zero-order bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. The authors also propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min norm solver. The regret bounds of both variants are derived. Extensive experiments demonstrate the effectiveness of the proposed algorithm."
1297,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the problem of multi-objective online convex optimization in the unconstrained max-min form. In particular, the authors propose a novel multi-Objective dynamic regret that is equivalent to the regret commonly used in the zero-order bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. The authors also propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min norm solver. The regret bounds of both variants are derived. Extensive experiments demonstrate the effectiveness of the proposed algorithm."
1298,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the problem of multi-objective online convex optimization in the unconstrained max-min form. In particular, the authors propose a novel multi-Objective dynamic regret that is equivalent to the regret commonly used in the zero-order bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. The authors also propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min norm solver. The regret bounds of both variants are derived. Extensive experiments demonstrate the effectiveness of the proposed algorithm."
1299,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a method for continual learning that uses negative examples as negative examples to learn the new classes. The idea is that the generated data are usually not able to improve the classification accuracy for the old classes, but they can be effective as negative example (or antagonists) to learn new classes, especially when the learning experiences are small and contain examples of just one or few classes. Experiments on CORe50 and ImageNet-1000 show that the proposed method outperforms existing generative replay approaches."
1300,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a method for continual learning that uses negative examples as negative examples to learn the new classes. The idea is that the generated data are usually not able to improve the classification accuracy for the old classes, but they can be effective as negative example (or antagonists) to learn new classes, especially when the learning experiences are small and contain examples of just one or few classes. Experiments on CORe50 and ImageNet-1000 show that the proposed method outperforms existing generative replay approaches."
1301,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a method for continual learning that uses negative examples as negative examples to learn the new classes. The idea is that the generated data are usually not able to improve the classification accuracy for the old classes, but they can be effective as negative example (or antagonists) to learn new classes, especially when the learning experiences are small and contain examples of just one or few classes. Experiments on CORe50 and ImageNet-1000 show that the proposed method outperforms existing generative replay approaches."
1302,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a method for continual learning that uses negative examples as negative examples to learn the new classes. The idea is that the generated data are usually not able to improve the classification accuracy for the old classes, but they can be effective as negative example (or antagonists) to learn new classes, especially when the learning experiences are small and contain examples of just one or few classes. Experiments on CORe50 and ImageNet-1000 show that the proposed method outperforms existing generative replay approaches."
1303,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework to alleviate the NP-hard challenge. The proposed IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. Then, the trained model is generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines."
1304,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework to alleviate the NP-hard challenge. The proposed IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. Then, the trained model is generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines."
1305,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework to alleviate the NP-hard challenge. The proposed IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. Then, the trained model is generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines."
1306,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework to alleviate the NP-hard challenge. The proposed IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. Then, the trained model is generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines."
1307,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graph generation. The proposed method is based on the idea of multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE uses higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. The model is end-to-end permutation-equivariant with respect to node ordering. Experiments are conducted on graph generation, molecular generation, unsupervised molecular representation learning, link prediction on citation graphs, and graph-based image generation."
1308,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graph generation. The proposed method is based on the idea of multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE uses higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. The model is end-to-end permutation-equivariant with respect to node ordering. Experiments are conducted on graph generation, molecular generation, unsupervised molecular representation learning, link prediction on citation graphs, and graph-based image generation."
1309,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graph generation. The proposed method is based on the idea of multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE uses higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. The model is end-to-end permutation-equivariant with respect to node ordering. Experiments are conducted on graph generation, molecular generation, unsupervised molecular representation learning, link prediction on citation graphs, and graph-based image generation."
1310,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graph generation. The proposed method is based on the idea of multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE uses higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. The model is end-to-end permutation-equivariant with respect to node ordering. Experiments are conducted on graph generation, molecular generation, unsupervised molecular representation learning, link prediction on citation graphs, and graph-based image generation."
1311,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper studies the problem of disentangling independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. The authors propose a general framework for nonlinear ICA, in which the mixing function can be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. They provide an insightful proof of the identifiability of the proposed framework, and verify their theory by experiments on artificial data and synthesized images."
1312,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper studies the problem of disentangling independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. The authors propose a general framework for nonlinear ICA, in which the mixing function can be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. They provide an insightful proof of the identifiability of the proposed framework, and verify their theory by experiments on artificial data and synthesized images."
1313,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper studies the problem of disentangling independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. The authors propose a general framework for nonlinear ICA, in which the mixing function can be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. They provide an insightful proof of the identifiability of the proposed framework, and verify their theory by experiments on artificial data and synthesized images."
1314,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper studies the problem of disentangling independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. The authors propose a general framework for nonlinear ICA, in which the mixing function can be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. They provide an insightful proof of the identifiability of the proposed framework, and verify their theory by experiments on artificial data and synthesized images."
1315,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional network architecture, called BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. The authors decompose multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture."
1316,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional network architecture, called BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. The authors decompose multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture."
1317,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional network architecture, called BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. The authors decompose multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture."
1318,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional network architecture, called BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. The authors decompose multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture."
1319,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that the proposed method outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
1320,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that the proposed method outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
1321,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that the proposed method outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
1322,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that the proposed method outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
1323,SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method for generating realistic images of facial expressions that are consistent with any desired target expression. The method is based on the idea of hallucinating facial geometric details consistent with the target expression, which are added to the smooth proxy geometry (marked as ‘Proxy Shading’, also extracted using FDS (Chen et al., 2019)), which is then used to render the final image. The proposed method is evaluated on a single image of a target expression (in this case ‘Happy’), and an initial detailed geometry extracted from FDS, which is shown in column ‘Reconstruction’ and row ‘Detailed Shading.’ The detailed geometry is then rendered using Neural Rendering to give a final image (first row of the column labelled “Happy”). A zoom-in of one of the predicted facial details and its render is also shown."
1324,SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method for generating realistic images of facial expressions that are consistent with any desired target expression. The method is based on the idea of hallucinating facial geometric details consistent with the target expression, which are added to the smooth proxy geometry (marked as ‘Proxy Shading’, also extracted using FDS (Chen et al., 2019)), which is then used to render the final image. The proposed method is evaluated on a single image of a target expression (in this case ‘Happy’), and an initial detailed geometry extracted from FDS, which is shown in column ‘Reconstruction’ and row ‘Detailed Shading.’ The detailed geometry is then rendered using Neural Rendering to give a final image (first row of the column labelled “Happy”). A zoom-in of one of the predicted facial details and its render is also shown."
1325,SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method for generating realistic images of facial expressions that are consistent with any desired target expression. The method is based on the idea of hallucinating facial geometric details consistent with the target expression, which are added to the smooth proxy geometry (marked as ‘Proxy Shading’, also extracted using FDS (Chen et al., 2019)), which is then used to render the final image. The proposed method is evaluated on a single image of a target expression (in this case ‘Happy’), and an initial detailed geometry extracted from FDS, which is shown in column ‘Reconstruction’ and row ‘Detailed Shading.’ The detailed geometry is then rendered using Neural Rendering to give a final image (first row of the column labelled “Happy”). A zoom-in of one of the predicted facial details and its render is also shown."
1326,SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method for generating realistic images of facial expressions that are consistent with any desired target expression. The method is based on the idea of hallucinating facial geometric details consistent with the target expression, which are added to the smooth proxy geometry (marked as ‘Proxy Shading’, also extracted using FDS (Chen et al., 2019)), which is then used to render the final image. The proposed method is evaluated on a single image of a target expression (in this case ‘Happy’), and an initial detailed geometry extracted from FDS, which is shown in column ‘Reconstruction’ and row ‘Detailed Shading.’ The detailed geometry is then rendered using Neural Rendering to give a final image (first row of the column labelled “Happy”). A zoom-in of one of the predicted facial details and its render is also shown."
1327,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The model is based on Transformer-based machine translation models, and the authors propose an evaluation protocol to measure disentanglement with regard to the realizations of syntactic roles. The experiments on raw English text from the SNLI dataset show that the proposed model can be induced without supervision, and ADVAE separates syntactic role better than classical sequence VAEs and Transformer VAEs, and can be separately modified in sentences by mere intervention on the associated latent variables."
1328,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The model is based on Transformer-based machine translation models, and the authors propose an evaluation protocol to measure disentanglement with regard to the realizations of syntactic roles. The experiments on raw English text from the SNLI dataset show that the proposed model can be induced without supervision, and ADVAE separates syntactic role better than classical sequence VAEs and Transformer VAEs, and can be separately modified in sentences by mere intervention on the associated latent variables."
1329,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The model is based on Transformer-based machine translation models, and the authors propose an evaluation protocol to measure disentanglement with regard to the realizations of syntactic roles. The experiments on raw English text from the SNLI dataset show that the proposed model can be induced without supervision, and ADVAE separates syntactic role better than classical sequence VAEs and Transformer VAEs, and can be separately modified in sentences by mere intervention on the associated latent variables."
1330,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The model is based on Transformer-based machine translation models, and the authors propose an evaluation protocol to measure disentanglement with regard to the realizations of syntactic roles. The experiments on raw English text from the SNLI dataset show that the proposed model can be induced without supervision, and ADVAE separates syntactic role better than classical sequence VAEs and Transformer VAEs, and can be separately modified in sentences by mere intervention on the associated latent variables."
1331,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a method for improving the exploration and coordination in multi-agent reinforcement learning (MARL). The proposed method is based on counterfactual rollouts of the agent’s policy and, through a sequence of computations, assesses the gap between other agents’ current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded. The method is evaluated on a set of challenging tasks with sparse rewards and partial observability that require learning complex cooperative strategies under a proper exploration scheme."
1332,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a method for improving the exploration and coordination in multi-agent reinforcement learning (MARL). The proposed method is based on counterfactual rollouts of the agent’s policy and, through a sequence of computations, assesses the gap between other agents’ current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded. The method is evaluated on a set of challenging tasks with sparse rewards and partial observability that require learning complex cooperative strategies under a proper exploration scheme."
1333,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a method for improving the exploration and coordination in multi-agent reinforcement learning (MARL). The proposed method is based on counterfactual rollouts of the agent’s policy and, through a sequence of computations, assesses the gap between other agents’ current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded. The method is evaluated on a set of challenging tasks with sparse rewards and partial observability that require learning complex cooperative strategies under a proper exploration scheme."
1334,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a method for improving the exploration and coordination in multi-agent reinforcement learning (MARL). The proposed method is based on counterfactual rollouts of the agent’s policy and, through a sequence of computations, assesses the gap between other agents’ current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded. The method is evaluated on a set of challenging tasks with sparse rewards and partial observability that require learning complex cooperative strategies under a proper exploration scheme."
1335,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of pre-trained models. The proposed method is based on a low-rank decomposition of the gradient of the input-output pair. The authors show that the proposed method can be trained on a single GPU in less than a day even for 10 billion+ parameter models. Experiments on T5, GPT, BERT, and BART show that MEND can effectively edit the behavior of models with more than 10 billion parameters."
1336,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of pre-trained models. The proposed method is based on a low-rank decomposition of the gradient of the input-output pair. The authors show that the proposed method can be trained on a single GPU in less than a day even for 10 billion+ parameter models. Experiments on T5, GPT, BERT, and BART show that MEND can effectively edit the behavior of models with more than 10 billion parameters."
1337,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of pre-trained models. The proposed method is based on a low-rank decomposition of the gradient of the input-output pair. The authors show that the proposed method can be trained on a single GPU in less than a day even for 10 billion+ parameter models. Experiments on T5, GPT, BERT, and BART show that MEND can effectively edit the behavior of models with more than 10 billion parameters."
1338,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of pre-trained models. The proposed method is based on a low-rank decomposition of the gradient of the input-output pair. The authors show that the proposed method can be trained on a single GPU in less than a day even for 10 billion+ parameter models. Experiments on T5, GPT, BERT, and BART show that MEND can effectively edit the behavior of models with more than 10 billion parameters."
1339,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in compositional manner. The experimental results on both one-and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
1340,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in compositional manner. The experimental results on both one-and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
1341,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in compositional manner. The experimental results on both one-and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
1342,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in compositional manner. The experimental results on both one-and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
1343,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents a study of the brain representations of Python code in the context of unsupervised machine learning models. The authors analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They find that brain representations encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties, and machine learned representations of code."
1344,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents a study of the brain representations of Python code in the context of unsupervised machine learning models. The authors analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They find that brain representations encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties, and machine learned representations of code."
1345,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents a study of the brain representations of Python code in the context of unsupervised machine learning models. The authors analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They find that brain representations encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties, and machine learned representations of code."
1346,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents a study of the brain representations of Python code in the context of unsupervised machine learning models. The authors analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They find that brain representations encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties, and machine learned representations of code."
1347,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model that can be trained end to end. The model consists of a speech encoder, phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The trained model is restricted to retain the source speaker’s voice, but unlike the original Translatron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. The paper also proposes a new method to retain each speaker's voices for input with speaker turns."
1348,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model that can be trained end to end. The model consists of a speech encoder, phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The trained model is restricted to retain the source speaker’s voice, but unlike the original Translatron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. The paper also proposes a new method to retain each speaker's voices for input with speaker turns."
1349,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model that can be trained end to end. The model consists of a speech encoder, phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The trained model is restricted to retain the source speaker’s voice, but unlike the original Translatron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. The paper also proposes a new method to retain each speaker's voices for input with speaker turns."
1350,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model that can be trained end to end. The model consists of a speech encoder, phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The trained model is restricted to retain the source speaker’s voice, but unlike the original Translatron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. The paper also proposes a new method to retain each speaker's voices for input with speaker turns."
1351,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of attribute-based few-shot learning of semantic classes. In particular, the authors focus on the fast learning of attributes that were not previously labeled. They show that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. They further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model’s generalization ability."
1352,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of attribute-based few-shot learning of semantic classes. In particular, the authors focus on the fast learning of attributes that were not previously labeled. They show that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. They further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model’s generalization ability."
1353,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of attribute-based few-shot learning of semantic classes. In particular, the authors focus on the fast learning of attributes that were not previously labeled. They show that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. They further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model’s generalization ability."
1354,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of attribute-based few-shot learning of semantic classes. In particular, the authors focus on the fast learning of attributes that were not previously labeled. They show that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. They further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model’s generalization ability."
1355,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,This paper proposes a method for sampling from unnormalized distributions. The method is based on the Wasserstein gradient flow of relative entropy. The authors propose a novel nonparametric approach to estimate the density ratios and simulate the ODE system with particle evolution. Experiments on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate the effectiveness of the proposed method.
1356,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,This paper proposes a method for sampling from unnormalized distributions. The method is based on the Wasserstein gradient flow of relative entropy. The authors propose a novel nonparametric approach to estimate the density ratios and simulate the ODE system with particle evolution. Experiments on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate the effectiveness of the proposed method.
1357,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,This paper proposes a method for sampling from unnormalized distributions. The method is based on the Wasserstein gradient flow of relative entropy. The authors propose a novel nonparametric approach to estimate the density ratios and simulate the ODE system with particle evolution. Experiments on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate the effectiveness of the proposed method.
1358,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,This paper proposes a method for sampling from unnormalized distributions. The method is based on the Wasserstein gradient flow of relative entropy. The authors propose a novel nonparametric approach to estimate the density ratios and simulate the ODE system with particle evolution. Experiments on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate the effectiveness of the proposed method.
1359,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum machine learning approach to classify larger, realistic images using quantum systems. The authors propose a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. They also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance. The proposed method is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop, and obtains accuracy comparable to classical neural networks with the same number of learnable parameters."
1360,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum machine learning approach to classify larger, realistic images using quantum systems. The authors propose a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. They also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance. The proposed method is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop, and obtains accuracy comparable to classical neural networks with the same number of learnable parameters."
1361,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum machine learning approach to classify larger, realistic images using quantum systems. The authors propose a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. They also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance. The proposed method is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop, and obtains accuracy comparable to classical neural networks with the same number of learnable parameters."
1362,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum machine learning approach to classify larger, realistic images using quantum systems. The authors propose a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. They also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance. The proposed method is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop, and obtains accuracy comparable to classical neural networks with the same number of learnable parameters."
1363,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning approach for face recognition. The main idea is to use differentially private local clustering (DPLC) to distill sanitized clusters from local class centers. Then, a consensus-aware recognition loss is introduced to encourage global consensuses among clients. The proposed framework is mathematically proved to be differentially privacy-utility. Experiments on a large-scale dataset show the effectiveness of the proposed method."
1364,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning approach for face recognition. The main idea is to use differentially private local clustering (DPLC) to distill sanitized clusters from local class centers. Then, a consensus-aware recognition loss is introduced to encourage global consensuses among clients. The proposed framework is mathematically proved to be differentially privacy-utility. Experiments on a large-scale dataset show the effectiveness of the proposed method."
1365,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning approach for face recognition. The main idea is to use differentially private local clustering (DPLC) to distill sanitized clusters from local class centers. Then, a consensus-aware recognition loss is introduced to encourage global consensuses among clients. The proposed framework is mathematically proved to be differentially privacy-utility. Experiments on a large-scale dataset show the effectiveness of the proposed method."
1366,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning approach for face recognition. The main idea is to use differentially private local clustering (DPLC) to distill sanitized clusters from local class centers. Then, a consensus-aware recognition loss is introduced to encourage global consensuses among clients. The proposed framework is mathematically proved to be differentially privacy-utility. Experiments on a large-scale dataset show the effectiveness of the proposed method."
1367,SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a new method for transfer learning in the context of image classification. The proposed method, Head-to-Toe probing (HEAD2TOE), selects features from all layers of the source model to train a classification head for the target-domain. The authors argue that fine-tuning all parameters of the model to the target domain may not be enough to leverage useful information from intermediate layers which are otherwise discarded by the later pretrained layers. To address this issue, the authors propose to select features from the source layer to train the classification head. The experimental results on the Visual Task Adaptation Benchmark (VTAB) show that Head2Toe outperforms the state-of-the-art in terms of transfer performance."
1368,SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a new method for transfer learning in the context of image classification. The proposed method, Head-to-Toe probing (HEAD2TOE), selects features from all layers of the source model to train a classification head for the target-domain. The authors argue that fine-tuning all parameters of the model to the target domain may not be enough to leverage useful information from intermediate layers which are otherwise discarded by the later pretrained layers. To address this issue, the authors propose to select features from the source layer to train the classification head. The experimental results on the Visual Task Adaptation Benchmark (VTAB) show that Head2Toe outperforms the state-of-the-art in terms of transfer performance."
1369,SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a new method for transfer learning in the context of image classification. The proposed method, Head-to-Toe probing (HEAD2TOE), selects features from all layers of the source model to train a classification head for the target-domain. The authors argue that fine-tuning all parameters of the model to the target domain may not be enough to leverage useful information from intermediate layers which are otherwise discarded by the later pretrained layers. To address this issue, the authors propose to select features from the source layer to train the classification head. The experimental results on the Visual Task Adaptation Benchmark (VTAB) show that Head2Toe outperforms the state-of-the-art in terms of transfer performance."
1370,SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a new method for transfer learning in the context of image classification. The proposed method, Head-to-Toe probing (HEAD2TOE), selects features from all layers of the source model to train a classification head for the target-domain. The authors argue that fine-tuning all parameters of the model to the target domain may not be enough to leverage useful information from intermediate layers which are otherwise discarded by the later pretrained layers. To address this issue, the authors propose to select features from the source layer to train the classification head. The experimental results on the Visual Task Adaptation Benchmark (VTAB) show that Head2Toe outperforms the state-of-the-art in terms of transfer performance."
1371,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes an object-oriented text dynamics (OOTD) model to solve decision-making problems in text-based games. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To improve the robustness of dynamics, the authors identify the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. Experiments show that the proposed model outperforms model-free baselines in terms of sample efficiency and running scores."
1372,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes an object-oriented text dynamics (OOTD) model to solve decision-making problems in text-based games. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To improve the robustness of dynamics, the authors identify the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. Experiments show that the proposed model outperforms model-free baselines in terms of sample efficiency and running scores."
1373,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes an object-oriented text dynamics (OOTD) model to solve decision-making problems in text-based games. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To improve the robustness of dynamics, the authors identify the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. Experiments show that the proposed model outperforms model-free baselines in terms of sample efficiency and running scores."
1374,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes an object-oriented text dynamics (OOTD) model to solve decision-making problems in text-based games. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To improve the robustness of dynamics, the authors identify the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. Experiments show that the proposed model outperforms model-free baselines in terms of sample efficiency and running scores."
1375,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification where a label taxonomy has a cost sensitive loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. This paper proposes a Layer-wise Abstaining Loss Minimization method (LAM), a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. The authors prove that there is a bijective mapping between the original hierarchical cost- sensitive loss and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learningto-absistain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems."
1376,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification where a label taxonomy has a cost sensitive loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. This paper proposes a Layer-wise Abstaining Loss Minimization method (LAM), a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. The authors prove that there is a bijective mapping between the original hierarchical cost- sensitive loss and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learningto-absistain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems."
1377,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification where a label taxonomy has a cost sensitive loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. This paper proposes a Layer-wise Abstaining Loss Minimization method (LAM), a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. The authors prove that there is a bijective mapping between the original hierarchical cost- sensitive loss and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learningto-absistain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems."
1378,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification where a label taxonomy has a cost sensitive loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. This paper proposes a Layer-wise Abstaining Loss Minimization method (LAM), a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. The authors prove that there is a bijective mapping between the original hierarchical cost- sensitive loss and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learningto-absistain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems."
1379,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks. By alternating the periodicity term, the authors can easily obtain a group where each bank differs in its temporal length and frequency resolution. Convolution of the proposed filterbanks with the raw waveform helps to achieve multi-scale perception in the time domain. Moreover, applying the proposed filters to recursively process a downsampled waveform enables to achieve multiple-scale representation in the frequency domain. Experiments on both direction of arrival estimation task and the physical location estimation task shows that the proposed method outperforms existing methods by a large margin."
1380,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks. By alternating the periodicity term, the authors can easily obtain a group where each bank differs in its temporal length and frequency resolution. Convolution of the proposed filterbanks with the raw waveform helps to achieve multi-scale perception in the time domain. Moreover, applying the proposed filters to recursively process a downsampled waveform enables to achieve multiple-scale representation in the frequency domain. Experiments on both direction of arrival estimation task and the physical location estimation task shows that the proposed method outperforms existing methods by a large margin."
1381,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks. By alternating the periodicity term, the authors can easily obtain a group where each bank differs in its temporal length and frequency resolution. Convolution of the proposed filterbanks with the raw waveform helps to achieve multi-scale perception in the time domain. Moreover, applying the proposed filters to recursively process a downsampled waveform enables to achieve multiple-scale representation in the frequency domain. Experiments on both direction of arrival estimation task and the physical location estimation task shows that the proposed method outperforms existing methods by a large margin."
1382,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks. By alternating the periodicity term, the authors can easily obtain a group where each bank differs in its temporal length and frequency resolution. Convolution of the proposed filterbanks with the raw waveform helps to achieve multi-scale perception in the time domain. Moreover, applying the proposed filters to recursively process a downsampled waveform enables to achieve multiple-scale representation in the frequency domain. Experiments on both direction of arrival estimation task and the physical location estimation task shows that the proposed method outperforms existing methods by a large margin."
1383,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of domain adaptation in the context of self-training, where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors show that under two assumptions: (i) access to samples from intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self training can be successfully applied on gradually shifted samples to adapt the model toward the target. They hypothesize that having (i), is enough to enable iterative self training to slowly adapt the models to the target distributions by making use of an implicit curriculum. In the case where (ii), more iterations hurt the performance of self training, they propose a method called GIFT (Gradual Interpolation of Features toward Target), which creates virtual samples by interpolating representations of examples from source and target domains."
1384,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of domain adaptation in the context of self-training, where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors show that under two assumptions: (i) access to samples from intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self training can be successfully applied on gradually shifted samples to adapt the model toward the target. They hypothesize that having (i), is enough to enable iterative self training to slowly adapt the models to the target distributions by making use of an implicit curriculum. In the case where (ii), more iterations hurt the performance of self training, they propose a method called GIFT (Gradual Interpolation of Features toward Target), which creates virtual samples by interpolating representations of examples from source and target domains."
1385,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of domain adaptation in the context of self-training, where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors show that under two assumptions: (i) access to samples from intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self training can be successfully applied on gradually shifted samples to adapt the model toward the target. They hypothesize that having (i), is enough to enable iterative self training to slowly adapt the models to the target distributions by making use of an implicit curriculum. In the case where (ii), more iterations hurt the performance of self training, they propose a method called GIFT (Gradual Interpolation of Features toward Target), which creates virtual samples by interpolating representations of examples from source and target domains."
1386,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of domain adaptation in the context of self-training, where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors show that under two assumptions: (i) access to samples from intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self training can be successfully applied on gradually shifted samples to adapt the model toward the target. They hypothesize that having (i), is enough to enable iterative self training to slowly adapt the models to the target distributions by making use of an implicit curriculum. In the case where (ii), more iterations hurt the performance of self training, they propose a method called GIFT (Gradual Interpolation of Features toward Target), which creates virtual samples by interpolating representations of examples from source and target domains."
1387,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on the observation that most of the videos on the internet contain user comments that are not relevant to the video. To deal with this issue, the authors propose an attention-based mechanism that allows the model to disregard text with irrelevant content. Experiments show that the proposed method can learn better, more contextualised, representations, while also achieving competitive results on standard video and text retrieval benchmarks."
1388,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on the observation that most of the videos on the internet contain user comments that are not relevant to the video. To deal with this issue, the authors propose an attention-based mechanism that allows the model to disregard text with irrelevant content. Experiments show that the proposed method can learn better, more contextualised, representations, while also achieving competitive results on standard video and text retrieval benchmarks."
1389,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on the observation that most of the videos on the internet contain user comments that are not relevant to the video. To deal with this issue, the authors propose an attention-based mechanism that allows the model to disregard text with irrelevant content. Experiments show that the proposed method can learn better, more contextualised, representations, while also achieving competitive results on standard video and text retrieval benchmarks."
1390,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on the observation that most of the videos on the internet contain user comments that are not relevant to the video. To deal with this issue, the authors propose an attention-based mechanism that allows the model to disregard text with irrelevant content. Experiments show that the proposed method can learn better, more contextualised, representations, while also achieving competitive results on standard video and text retrieval benchmarks."
1391,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes an exploration bonus discriminator disagreement intrinsic reward (DISDAIN) for unsupervised skill learning. DISDAIN is motivated by the observation that the discriminator will not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, the authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. The authors demonstrate empirically that the proposed method improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels)."
1392,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes an exploration bonus discriminator disagreement intrinsic reward (DISDAIN) for unsupervised skill learning. DISDAIN is motivated by the observation that the discriminator will not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, the authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. The authors demonstrate empirically that the proposed method improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels)."
1393,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes an exploration bonus discriminator disagreement intrinsic reward (DISDAIN) for unsupervised skill learning. DISDAIN is motivated by the observation that the discriminator will not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, the authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. The authors demonstrate empirically that the proposed method improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels)."
1394,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes an exploration bonus discriminator disagreement intrinsic reward (DISDAIN) for unsupervised skill learning. DISDAIN is motivated by the observation that the discriminator will not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, the authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. The authors demonstrate empirically that the proposed method improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels)."
1395,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning Tree and the residual edges. Based on the intermediate graph structure of the construction process, the authors can constrain its generation to molecular graphs that satisfy the chemical valence rules. The authors also design a Transformer architecture with tree based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework."
1396,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning Tree and the residual edges. Based on the intermediate graph structure of the construction process, the authors can constrain its generation to molecular graphs that satisfy the chemical valence rules. The authors also design a Transformer architecture with tree based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework."
1397,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning Tree and the residual edges. Based on the intermediate graph structure of the construction process, the authors can constrain its generation to molecular graphs that satisfy the chemical valence rules. The authors also design a Transformer architecture with tree based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework."
1398,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning Tree and the residual edges. Based on the intermediate graph structure of the construction process, the authors can constrain its generation to molecular graphs that satisfy the chemical valence rules. The authors also design a Transformer architecture with tree based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework."
1399,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of neural networks trained with polynomial neural networks (PNNs). The authors propose a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs based on a recently proposed parametrization of the PNN family. The authors show that the Π-Net family, i.e., a recent parametrized PNN, speeds up the learning of the higher frequencies. They verify the theoretical bias through extensive experiments. "
1400,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of neural networks trained with polynomial neural networks (PNNs). The authors propose a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs based on a recently proposed parametrization of the PNN family. The authors show that the Π-Net family, i.e., a recent parametrized PNN, speeds up the learning of the higher frequencies. They verify the theoretical bias through extensive experiments. "
1401,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of neural networks trained with polynomial neural networks (PNNs). The authors propose a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs based on a recently proposed parametrization of the PNN family. The authors show that the Π-Net family, i.e., a recent parametrized PNN, speeds up the learning of the higher frequencies. They verify the theoretical bias through extensive experiments. "
1402,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of neural networks trained with polynomial neural networks (PNNs). The authors propose a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs based on a recently proposed parametrization of the PNN family. The authors show that the Π-Net family, i.e., a recent parametrized PNN, speeds up the learning of the higher frequencies. They verify the theoretical bias through extensive experiments. "
1403,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method to identify hidden subnetworks in sparse neural networks. The idea is to use a Peek-a-Boo (PaB) game to identify the hidden subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork’s remaining weights. The authors argue that the unmasking process plays an important role in enlarging the capacity of the sparse subnetwork and thus grants two major benefits: (i) the disguised subnetwork easily outperform the hidden counterparts; (ii) the unmask process helps to relax the quality requirement on the sparse mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives. Extensive experiments with several large-scale models (ResNet-18, ResNet-50, and WideResNet) on CIFAR-10/100 datasets demonstrate the competency of the proposed method."
1404,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method to identify hidden subnetworks in sparse neural networks. The idea is to use a Peek-a-Boo (PaB) game to identify the hidden subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork’s remaining weights. The authors argue that the unmasking process plays an important role in enlarging the capacity of the sparse subnetwork and thus grants two major benefits: (i) the disguised subnetwork easily outperform the hidden counterparts; (ii) the unmask process helps to relax the quality requirement on the sparse mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives. Extensive experiments with several large-scale models (ResNet-18, ResNet-50, and WideResNet) on CIFAR-10/100 datasets demonstrate the competency of the proposed method."
1405,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method to identify hidden subnetworks in sparse neural networks. The idea is to use a Peek-a-Boo (PaB) game to identify the hidden subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork’s remaining weights. The authors argue that the unmasking process plays an important role in enlarging the capacity of the sparse subnetwork and thus grants two major benefits: (i) the disguised subnetwork easily outperform the hidden counterparts; (ii) the unmask process helps to relax the quality requirement on the sparse mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives. Extensive experiments with several large-scale models (ResNet-18, ResNet-50, and WideResNet) on CIFAR-10/100 datasets demonstrate the competency of the proposed method."
1406,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method to identify hidden subnetworks in sparse neural networks. The idea is to use a Peek-a-Boo (PaB) game to identify the hidden subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork’s remaining weights. The authors argue that the unmasking process plays an important role in enlarging the capacity of the sparse subnetwork and thus grants two major benefits: (i) the disguised subnetwork easily outperform the hidden counterparts; (ii) the unmask process helps to relax the quality requirement on the sparse mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives. Extensive experiments with several large-scale models (ResNet-18, ResNet-50, and WideResNet) on CIFAR-10/100 datasets demonstrate the competency of the proposed method."
1407,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a method to jointly clean the graph and denoise features of data. The main idea is to reconstruct the graph with its intrinsic properties, including similarity of two adjacent nodes’ features, sparsity of real-world graphs and many slight noises having small eigenvalues in perturbed graphs. Then, the convolution operation for features is proposed to find the optimal solution adopting the Laplacian smoothness and the prior knowledge that nodes with many neighbors are difficult to attack. The proposed method is evaluated on four datasets and compared with several baselines."
1408,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a method to jointly clean the graph and denoise features of data. The main idea is to reconstruct the graph with its intrinsic properties, including similarity of two adjacent nodes’ features, sparsity of real-world graphs and many slight noises having small eigenvalues in perturbed graphs. Then, the convolution operation for features is proposed to find the optimal solution adopting the Laplacian smoothness and the prior knowledge that nodes with many neighbors are difficult to attack. The proposed method is evaluated on four datasets and compared with several baselines."
1409,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a method to jointly clean the graph and denoise features of data. The main idea is to reconstruct the graph with its intrinsic properties, including similarity of two adjacent nodes’ features, sparsity of real-world graphs and many slight noises having small eigenvalues in perturbed graphs. Then, the convolution operation for features is proposed to find the optimal solution adopting the Laplacian smoothness and the prior knowledge that nodes with many neighbors are difficult to attack. The proposed method is evaluated on four datasets and compared with several baselines."
1410,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a method to jointly clean the graph and denoise features of data. The main idea is to reconstruct the graph with its intrinsic properties, including similarity of two adjacent nodes’ features, sparsity of real-world graphs and many slight noises having small eigenvalues in perturbed graphs. Then, the convolution operation for features is proposed to find the optimal solution adopting the Laplacian smoothness and the prior knowledge that nodes with many neighbors are difficult to attack. The proposed method is evaluated on four datasets and compared with several baselines."
1411,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns a continuous bijective mapping between 3D position and 2D texture-space coordinates. The method can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Experiments show that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
1412,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns a continuous bijective mapping between 3D position and 2D texture-space coordinates. The method can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Experiments show that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
1413,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns a continuous bijective mapping between 3D position and 2D texture-space coordinates. The method can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Experiments show that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
1414,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns a continuous bijective mapping between 3D position and 2D texture-space coordinates. The method can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Experiments show that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
1415,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a downsampling algorithm that makes the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation."
1416,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a downsampling algorithm that makes the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation."
1417,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a downsampling algorithm that makes the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation."
1418,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a downsampling algorithm that makes the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation."
1419,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper proposes a method for out-of-distribution (OOD) generalization of GNNs for node-level prediction on graphs. The main idea is to use multiple context explorers (specified as graph editers) that are adversarially trained to maximize the variance of risks from multiple virtual environments. The authors prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution."
1420,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper proposes a method for out-of-distribution (OOD) generalization of GNNs for node-level prediction on graphs. The main idea is to use multiple context explorers (specified as graph editers) that are adversarially trained to maximize the variance of risks from multiple virtual environments. The authors prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution."
1421,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper proposes a method for out-of-distribution (OOD) generalization of GNNs for node-level prediction on graphs. The main idea is to use multiple context explorers (specified as graph editers) that are adversarially trained to maximize the variance of risks from multiple virtual environments. The authors prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution."
1422,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper proposes a method for out-of-distribution (OOD) generalization of GNNs for node-level prediction on graphs. The main idea is to use multiple context explorers (specified as graph editers) that are adversarially trained to maximize the variance of risks from multiple virtual environments. The authors prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution."
1423,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for time series data augmentation for contrastive representation learning. The proposed method is based on information theory, where the authors provide a theoretical analysis for selecting feasible data augmentations. Then, InfoTS uses meta-learner and encoder to jointly optimize in an end-to-end manner to avoid sub-optimal solutions. Experiments on various datasets show the effectiveness of InfoTS."
1424,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for time series data augmentation for contrastive representation learning. The proposed method is based on information theory, where the authors provide a theoretical analysis for selecting feasible data augmentations. Then, InfoTS uses meta-learner and encoder to jointly optimize in an end-to-end manner to avoid sub-optimal solutions. Experiments on various datasets show the effectiveness of InfoTS."
1425,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for time series data augmentation for contrastive representation learning. The proposed method is based on information theory, where the authors provide a theoretical analysis for selecting feasible data augmentations. Then, InfoTS uses meta-learner and encoder to jointly optimize in an end-to-end manner to avoid sub-optimal solutions. Experiments on various datasets show the effectiveness of InfoTS."
1426,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for time series data augmentation for contrastive representation learning. The proposed method is based on information theory, where the authors provide a theoretical analysis for selecting feasible data augmentations. Then, InfoTS uses meta-learner and encoder to jointly optimize in an end-to-end manner to avoid sub-optimal solutions. Experiments on various datasets show the effectiveness of InfoTS."
1427,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the universality of winning tickets in the Lottery Ticket Hypothesis. In particular, the authors propose a renormalization group theory framework to explain why winning tickets can be transferred to similar tasks. They show that iterative magnitude pruning, the method used for discovering winning tickets, is a re-parameterization group scheme. They also show that winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitudes pruning has found generally in machine learning."
1428,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the universality of winning tickets in the Lottery Ticket Hypothesis. In particular, the authors propose a renormalization group theory framework to explain why winning tickets can be transferred to similar tasks. They show that iterative magnitude pruning, the method used for discovering winning tickets, is a re-parameterization group scheme. They also show that winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitudes pruning has found generally in machine learning."
1429,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the universality of winning tickets in the Lottery Ticket Hypothesis. In particular, the authors propose a renormalization group theory framework to explain why winning tickets can be transferred to similar tasks. They show that iterative magnitude pruning, the method used for discovering winning tickets, is a re-parameterization group scheme. They also show that winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitudes pruning has found generally in machine learning."
1430,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the universality of winning tickets in the Lottery Ticket Hypothesis. In particular, the authors propose a renormalization group theory framework to explain why winning tickets can be transferred to similar tasks. They show that iterative magnitude pruning, the method used for discovering winning tickets, is a re-parameterization group scheme. They also show that winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitudes pruning has found generally in machine learning."
1431,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models in the context of information retrieval. The authors prove theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. They show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, they propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural re-ranking datasets."
1432,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models in the context of information retrieval. The authors prove theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. They show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, they propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural re-ranking datasets."
1433,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models in the context of information retrieval. The authors prove theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. They show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, they propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural re-ranking datasets."
1434,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models in the context of information retrieval. The authors prove theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. They show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, they propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural re-ranking datasets."
1435,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the role of importance sampling (IS) in policy optimization (PO) algorithms. Importance sampling is typically employed as a what-if analysis tool, with the goal of estimating the performance of a target policy, given samples collected with a different behavioral policy. However, in Monte Carlo simulation, IS represents a variance minimization approach. In this paper, the authors analyze IS in these two guises, showing the connections between the two objectives. Theoretical findings are used to build a PO algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that employs variance minimisation as an inner loop. Empirical evaluations on continuous RL benchmarks demonstrate the robustness to small batch sizes."
1436,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the role of importance sampling (IS) in policy optimization (PO) algorithms. Importance sampling is typically employed as a what-if analysis tool, with the goal of estimating the performance of a target policy, given samples collected with a different behavioral policy. However, in Monte Carlo simulation, IS represents a variance minimization approach. In this paper, the authors analyze IS in these two guises, showing the connections between the two objectives. Theoretical findings are used to build a PO algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that employs variance minimisation as an inner loop. Empirical evaluations on continuous RL benchmarks demonstrate the robustness to small batch sizes."
1437,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the role of importance sampling (IS) in policy optimization (PO) algorithms. Importance sampling is typically employed as a what-if analysis tool, with the goal of estimating the performance of a target policy, given samples collected with a different behavioral policy. However, in Monte Carlo simulation, IS represents a variance minimization approach. In this paper, the authors analyze IS in these two guises, showing the connections between the two objectives. Theoretical findings are used to build a PO algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that employs variance minimisation as an inner loop. Empirical evaluations on continuous RL benchmarks demonstrate the robustness to small batch sizes."
1438,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the role of importance sampling (IS) in policy optimization (PO) algorithms. Importance sampling is typically employed as a what-if analysis tool, with the goal of estimating the performance of a target policy, given samples collected with a different behavioral policy. However, in Monte Carlo simulation, IS represents a variance minimization approach. In this paper, the authors analyze IS in these two guises, showing the connections between the two objectives. Theoretical findings are used to build a PO algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that employs variance minimisation as an inner loop. Empirical evaluations on continuous RL benchmarks demonstrate the robustness to small batch sizes."
1439,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate their method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 15% on the force MAE metric on the S2EF task and 21% on AFbT metric on IS2RS task, establishing new state-of-the-art results."
1440,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate their method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 15% on the force MAE metric on the S2EF task and 21% on AFbT metric on IS2RS task, establishing new state-of-the-art results."
1441,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate their method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 15% on the force MAE metric on the S2EF task and 21% on AFbT metric on IS2RS task, establishing new state-of-the-art results."
1442,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate their method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 15% on the force MAE metric on the S2EF task and 21% on AFbT metric on IS2RS task, establishing new state-of-the-art results."
1443,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a method for generating text that is consistent with the latent dynamics of a stochastic process of interest. The method is based on the idea that the dynamics of text changes in a document can be modeled as a latent process, which can then be used to generate text consistent with this latent process. The model is evaluated on a variety of text generation tasks, including text infilling, discourse coherence, and text length consistency. The results show that the proposed method outperforms the baseline GPT2 model on all tasks."
1444,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a method for generating text that is consistent with the latent dynamics of a stochastic process of interest. The method is based on the idea that the dynamics of text changes in a document can be modeled as a latent process, which can then be used to generate text consistent with this latent process. The model is evaluated on a variety of text generation tasks, including text infilling, discourse coherence, and text length consistency. The results show that the proposed method outperforms the baseline GPT2 model on all tasks."
1445,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a method for generating text that is consistent with the latent dynamics of a stochastic process of interest. The method is based on the idea that the dynamics of text changes in a document can be modeled as a latent process, which can then be used to generate text consistent with this latent process. The model is evaluated on a variety of text generation tasks, including text infilling, discourse coherence, and text length consistency. The results show that the proposed method outperforms the baseline GPT2 model on all tasks."
1446,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a method for generating text that is consistent with the latent dynamics of a stochastic process of interest. The method is based on the idea that the dynamics of text changes in a document can be modeled as a latent process, which can then be used to generate text consistent with this latent process. The model is evaluated on a variety of text generation tasks, including text infilling, discourse coherence, and text length consistency. The results show that the proposed method outperforms the baseline GPT2 model on all tasks."
1447,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for learning object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects. The proposed method learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The model requires no supervision or pre-training of any part of the network."
1448,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for learning object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects. The proposed method learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The model requires no supervision or pre-training of any part of the network."
1449,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for learning object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects. The proposed method learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The model requires no supervision or pre-training of any part of the network."
1450,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for learning object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects. The proposed method learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The model requires no supervision or pre-training of any part of the network."
1451,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning the disentangled representation from real spatio-temporal data for mobility forecasting. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. Experiments show that the proposed method can achieve state-of-the-art performance across multiple spatiotemporal datasets.
1452,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning the disentangled representation from real spatio-temporal data for mobility forecasting. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. Experiments show that the proposed method can achieve state-of-the-art performance across multiple spatiotemporal datasets.
1453,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning the disentangled representation from real spatio-temporal data for mobility forecasting. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. Experiments show that the proposed method can achieve state-of-the-art performance across multiple spatiotemporal datasets.
1454,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning the disentangled representation from real spatio-temporal data for mobility forecasting. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. Experiments show that the proposed method can achieve state-of-the-art performance across multiple spatiotemporal datasets.
1455,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a method for interpolating irregularly sampled time series. The proposed method is based on a temporal VAE architecture with a heteroscedastic output layer. The authors show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models.
1456,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a method for interpolating irregularly sampled time series. The proposed method is based on a temporal VAE architecture with a heteroscedastic output layer. The authors show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models.
1457,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a method for interpolating irregularly sampled time series. The proposed method is based on a temporal VAE architecture with a heteroscedastic output layer. The authors show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models.
1458,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a method for interpolating irregularly sampled time series. The proposed method is based on a temporal VAE architecture with a heteroscedastic output layer. The authors show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models.
1459,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two proxies for measuring the modularity of a neural network: importance and coherence. Specifically, the authors propose a set of statistical methods based on techniques conventionally used to interpret individual neurons. They apply the proxies to partitionings generated by spectrally clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent."
1460,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two proxies for measuring the modularity of a neural network: importance and coherence. Specifically, the authors propose a set of statistical methods based on techniques conventionally used to interpret individual neurons. They apply the proxies to partitionings generated by spectrally clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent."
1461,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two proxies for measuring the modularity of a neural network: importance and coherence. Specifically, the authors propose a set of statistical methods based on techniques conventionally used to interpret individual neurons. They apply the proxies to partitionings generated by spectrally clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent."
1462,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two proxies for measuring the modularity of a neural network: importance and coherence. Specifically, the authors propose a set of statistical methods based on techniques conventionally used to interpret individual neurons. They apply the proxies to partitionings generated by spectrally clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent."
1463,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the lottery ticket hypothesis to discover lightweight speech recognition models that are robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. The winning tickets can generalize to structured spersonality with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models."
1464,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the lottery ticket hypothesis to discover lightweight speech recognition models that are robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. The winning tickets can generalize to structured spersonality with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models."
1465,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the lottery ticket hypothesis to discover lightweight speech recognition models that are robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. The winning tickets can generalize to structured spersonality with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models."
1466,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the lottery ticket hypothesis to discover lightweight speech recognition models that are robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. The winning tickets can generalize to structured spersonality with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models."
1467,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and ones entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Surprisingly, the ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization 1."
1468,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and ones entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Surprisingly, the ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization 1."
1469,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and ones entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Surprisingly, the ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization 1."
1470,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and ones entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Surprisingly, the ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization 1."
1471,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for backdoor removal. The main idea is to use the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data.
1472,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for backdoor removal. The main idea is to use the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data.
1473,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for backdoor removal. The main idea is to use the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data.
1474,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for backdoor removal. The main idea is to use the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data.
1475,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD. The authors show that the convergence gap between optimal and random permutations can vary from exponential to nonexistent. They also show that for 1-dimensional strongly convex functions, there exist permutations that offer exponentially faster convergence compared to random. Finally, they show that there are easy-to-construct permutations for quadratic, strongly-convex functions."
1476,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD. The authors show that the convergence gap between optimal and random permutations can vary from exponential to nonexistent. They also show that for 1-dimensional strongly convex functions, there exist permutations that offer exponentially faster convergence compared to random. Finally, they show that there are easy-to-construct permutations for quadratic, strongly-convex functions."
1477,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD. The authors show that the convergence gap between optimal and random permutations can vary from exponential to nonexistent. They also show that for 1-dimensional strongly convex functions, there exist permutations that offer exponentially faster convergence compared to random. Finally, they show that there are easy-to-construct permutations for quadratic, strongly-convex functions."
1478,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD. The authors show that the convergence gap between optimal and random permutations can vary from exponential to nonexistent. They also show that for 1-dimensional strongly convex functions, there exist permutations that offer exponentially faster convergence compared to random. Finally, they show that there are easy-to-construct permutations for quadratic, strongly-convex functions."
1479,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes a method to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
1480,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes a method to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
1481,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes a method to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
1482,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes a method to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
1483,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes a new measure of expressiveness for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) of the dataset in representation space. The proposed measure, Cluster Learnability (CL), is defined in terms of the learning speed of a KNN classifier trained to predict Kmeans cluster labels for held-out representations. The paper also proposes modifying DeepCluster (Caron et al., 2018) to improve the learnability of the representations."
1484,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes a new measure of expressiveness for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) of the dataset in representation space. The proposed measure, Cluster Learnability (CL), is defined in terms of the learning speed of a KNN classifier trained to predict Kmeans cluster labels for held-out representations. The paper also proposes modifying DeepCluster (Caron et al., 2018) to improve the learnability of the representations."
1485,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes a new measure of expressiveness for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) of the dataset in representation space. The proposed measure, Cluster Learnability (CL), is defined in terms of the learning speed of a KNN classifier trained to predict Kmeans cluster labels for held-out representations. The paper also proposes modifying DeepCluster (Caron et al., 2018) to improve the learnability of the representations."
1486,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes a new measure of expressiveness for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) of the dataset in representation space. The proposed measure, Cluster Learnability (CL), is defined in terms of the learning speed of a KNN classifier trained to predict Kmeans cluster labels for held-out representations. The paper also proposes modifying DeepCluster (Caron et al., 2018) to improve the learnability of the representations."
1487,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box attacks such that the perturbations are limited in the salient region. The authors further propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptibility by refining perturbation in salient regions. Experimental results show that the proposed method is much more imperceptible than the ones generated by other attacks, and are interpretable to some extent. Furthermore, the approach is more robust to detection-based defense."
1488,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box attacks such that the perturbations are limited in the salient region. The authors further propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptibility by refining perturbation in salient regions. Experimental results show that the proposed method is much more imperceptible than the ones generated by other attacks, and are interpretable to some extent. Furthermore, the approach is more robust to detection-based defense."
1489,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box attacks such that the perturbations are limited in the salient region. The authors further propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptibility by refining perturbation in salient regions. Experimental results show that the proposed method is much more imperceptible than the ones generated by other attacks, and are interpretable to some extent. Furthermore, the approach is more robust to detection-based defense."
1490,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box attacks such that the perturbations are limited in the salient region. The authors further propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptibility by refining perturbation in salient regions. Experimental results show that the proposed method is much more imperceptible than the ones generated by other attacks, and are interpretable to some extent. Furthermore, the approach is more robust to detection-based defense."
1491,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a novel Graph2SMILES model that combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. The proposed model is an end-to-end architecture that can be used as a drop-in replacement for the Transformer in any task involving molecule(s)-to-molecule(s) transformations. In particular, the attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding."
1492,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a novel Graph2SMILES model that combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. The proposed model is an end-to-end architecture that can be used as a drop-in replacement for the Transformer in any task involving molecule(s)-to-molecule(s) transformations. In particular, the attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding."
1493,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a novel Graph2SMILES model that combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. The proposed model is an end-to-end architecture that can be used as a drop-in replacement for the Transformer in any task involving molecule(s)-to-molecule(s) transformations. In particular, the attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding."
1494,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a novel Graph2SMILES model that combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. The proposed model is an end-to-end architecture that can be used as a drop-in replacement for the Transformer in any task involving molecule(s)-to-molecule(s) transformations. In particular, the attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding."
1495,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a method for automatic disease diagnosis with reinforcement learning (RL) methods in task-oriented dialogues setting. The proposed method consists of a hierarchical policy of two levels. The high level policy is responsible for triggering a low level model, the low level policy consists of several symptom checkers and a disease classifier. Experimental results on both self-constructed real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
1496,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a method for automatic disease diagnosis with reinforcement learning (RL) methods in task-oriented dialogues setting. The proposed method consists of a hierarchical policy of two levels. The high level policy is responsible for triggering a low level model, the low level policy consists of several symptom checkers and a disease classifier. Experimental results on both self-constructed real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
1497,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a method for automatic disease diagnosis with reinforcement learning (RL) methods in task-oriented dialogues setting. The proposed method consists of a hierarchical policy of two levels. The high level policy is responsible for triggering a low level model, the low level policy consists of several symptom checkers and a disease classifier. Experimental results on both self-constructed real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
1498,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a method for automatic disease diagnosis with reinforcement learning (RL) methods in task-oriented dialogues setting. The proposed method consists of a hierarchical policy of two levels. The high level policy is responsible for triggering a low level model, the low level policy consists of several symptom checkers and a disease classifier. Experimental results on both self-constructed real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
1499,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing these challenges. First, under the SSFL framework, the authors analyze the compatibility of various centralized self supervised learning methods in FL setting and demonstrate that SimSiam networks performs best with the standard FedAvg algorithm. Moreover, to address the data heterogeneity at the edge devices in this framework, they have innovated several algorithms including perFedAvg, Ditto, and local fine-tuning, among others. To provide a comprehensive comparative analysis of all proposed algorithms, they also develop a distributed training system and related evaluation protocol for SSFL."
1500,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing these challenges. First, under the SSFL framework, the authors analyze the compatibility of various centralized self supervised learning methods in FL setting and demonstrate that SimSiam networks performs best with the standard FedAvg algorithm. Moreover, to address the data heterogeneity at the edge devices in this framework, they have innovated several algorithms including perFedAvg, Ditto, and local fine-tuning, among others. To provide a comprehensive comparative analysis of all proposed algorithms, they also develop a distributed training system and related evaluation protocol for SSFL."
1501,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing these challenges. First, under the SSFL framework, the authors analyze the compatibility of various centralized self supervised learning methods in FL setting and demonstrate that SimSiam networks performs best with the standard FedAvg algorithm. Moreover, to address the data heterogeneity at the edge devices in this framework, they have innovated several algorithms including perFedAvg, Ditto, and local fine-tuning, among others. To provide a comprehensive comparative analysis of all proposed algorithms, they also develop a distributed training system and related evaluation protocol for SSFL."
1502,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing these challenges. First, under the SSFL framework, the authors analyze the compatibility of various centralized self supervised learning methods in FL setting and demonstrate that SimSiam networks performs best with the standard FedAvg algorithm. Moreover, to address the data heterogeneity at the edge devices in this framework, they have innovated several algorithms including perFedAvg, Ditto, and local fine-tuning, among others. To provide a comprehensive comparative analysis of all proposed algorithms, they also develop a distributed training system and related evaluation protocol for SSFL."
1503,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper proposes a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show the adjustment operator for ResNet like DNN is the solution operator of the PDE. Theoretically, they prove that the robustness of DNN trained with their method is certifiable and their training method reduces the generalization gap for DNN. "
1504,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper proposes a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show the adjustment operator for ResNet like DNN is the solution operator of the PDE. Theoretically, they prove that the robustness of DNN trained with their method is certifiable and their training method reduces the generalization gap for DNN. "
1505,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper proposes a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show the adjustment operator for ResNet like DNN is the solution operator of the PDE. Theoretically, they prove that the robustness of DNN trained with their method is certifiable and their training method reduces the generalization gap for DNN. "
1506,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper proposes a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show the adjustment operator for ResNet like DNN is the solution operator of the PDE. Theoretically, they prove that the robustness of DNN trained with their method is certifiable and their training method reduces the generalization gap for DNN. "
1507,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the emergence of language in various language games, where agents interact and develop an emergent language to solve a task. The authors focus on the factors which determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. The expressivity is measured based on their generalisation performance across different games, and demonstrate that the language is a trade-off between the complexity and unpredictability of the context those languages exist in. Another novel contribution of this work is the discovery of message type collapse. "
1508,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the emergence of language in various language games, where agents interact and develop an emergent language to solve a task. The authors focus on the factors which determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. The expressivity is measured based on their generalisation performance across different games, and demonstrate that the language is a trade-off between the complexity and unpredictability of the context those languages exist in. Another novel contribution of this work is the discovery of message type collapse. "
1509,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the emergence of language in various language games, where agents interact and develop an emergent language to solve a task. The authors focus on the factors which determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. The expressivity is measured based on their generalisation performance across different games, and demonstrate that the language is a trade-off between the complexity and unpredictability of the context those languages exist in. Another novel contribution of this work is the discovery of message type collapse. "
1510,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the emergence of language in various language games, where agents interact and develop an emergent language to solve a task. The authors focus on the factors which determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. The expressivity is measured based on their generalisation performance across different games, and demonstrate that the language is a trade-off between the complexity and unpredictability of the context those languages exist in. Another novel contribution of this work is the discovery of message type collapse. "
1511,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a new exploration strategy for reinforcement learning. The exploration strategy is based on the Sample Average Uncertainty (SAU) method. The main difference with SAU is that it only depends on the value predictions, meaning that it does not need to rely on maintaining model posterior distributions. The authors propose to extend SAU from bandits to the general sequential Reinforcement Learning scenario."
1512,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a new exploration strategy for reinforcement learning. The exploration strategy is based on the Sample Average Uncertainty (SAU) method. The main difference with SAU is that it only depends on the value predictions, meaning that it does not need to rely on maintaining model posterior distributions. The authors propose to extend SAU from bandits to the general sequential Reinforcement Learning scenario."
1513,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a new exploration strategy for reinforcement learning. The exploration strategy is based on the Sample Average Uncertainty (SAU) method. The main difference with SAU is that it only depends on the value predictions, meaning that it does not need to rely on maintaining model posterior distributions. The authors propose to extend SAU from bandits to the general sequential Reinforcement Learning scenario."
1514,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a new exploration strategy for reinforcement learning. The exploration strategy is based on the Sample Average Uncertainty (SAU) method. The main difference with SAU is that it only depends on the value predictions, meaning that it does not need to rely on maintaining model posterior distributions. The authors propose to extend SAU from bandits to the general sequential Reinforcement Learning scenario."
1515,SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes a new method for long video generation based on implicit generative adversarial networks (DIGAN). The main idea is to use implicit neural representations (INRs) to model the dynamics of the video. Specifically, an INR-based video generator is used to improve the motion dynamics by manipulating the space and time coordinates differently and a motion discriminator is applied to identify the unnatural motions without observing the entire long frame sequences. Experiments on UCF-101 show that the proposed method outperforms the state-of-the-art."
1516,SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes a new method for long video generation based on implicit generative adversarial networks (DIGAN). The main idea is to use implicit neural representations (INRs) to model the dynamics of the video. Specifically, an INR-based video generator is used to improve the motion dynamics by manipulating the space and time coordinates differently and a motion discriminator is applied to identify the unnatural motions without observing the entire long frame sequences. Experiments on UCF-101 show that the proposed method outperforms the state-of-the-art."
1517,SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes a new method for long video generation based on implicit generative adversarial networks (DIGAN). The main idea is to use implicit neural representations (INRs) to model the dynamics of the video. Specifically, an INR-based video generator is used to improve the motion dynamics by manipulating the space and time coordinates differently and a motion discriminator is applied to identify the unnatural motions without observing the entire long frame sequences. Experiments on UCF-101 show that the proposed method outperforms the state-of-the-art."
1518,SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes a new method for long video generation based on implicit generative adversarial networks (DIGAN). The main idea is to use implicit neural representations (INRs) to model the dynamics of the video. Specifically, an INR-based video generator is used to improve the motion dynamics by manipulating the space and time coordinates differently and a motion discriminator is applied to identify the unnatural motions without observing the entire long frame sequences. Experiments on UCF-101 show that the proposed method outperforms the state-of-the-art."
1519,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of multi-label privacy-preserving multi-winner voting. The authors propose three different mechanisms: Binary, τ, and Powerset voting. Binary voting operates independently per label through composition. τ voting bounds votes optimally in their $\ell_2$ norm. Powerset Voting operates over the entire binary vector by viewing the possible outcomes as a power set. The paper theoretically analyzes tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting."
1520,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of multi-label privacy-preserving multi-winner voting. The authors propose three different mechanisms: Binary, τ, and Powerset voting. Binary voting operates independently per label through composition. τ voting bounds votes optimally in their $\ell_2$ norm. Powerset Voting operates over the entire binary vector by viewing the possible outcomes as a power set. The paper theoretically analyzes tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting."
1521,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of multi-label privacy-preserving multi-winner voting. The authors propose three different mechanisms: Binary, τ, and Powerset voting. Binary voting operates independently per label through composition. τ voting bounds votes optimally in their $\ell_2$ norm. Powerset Voting operates over the entire binary vector by viewing the possible outcomes as a power set. The paper theoretically analyzes tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting."
1522,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of multi-label privacy-preserving multi-winner voting. The authors propose three different mechanisms: Binary, τ, and Powerset voting. Binary voting operates independently per label through composition. τ voting bounds votes optimally in their $\ell_2$ norm. Powerset Voting operates over the entire binary vector by viewing the possible outcomes as a power set. The paper theoretically analyzes tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting."
1523,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a method to transfer the implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. The method is based on the idea that the learning rate schedule is a difficult to tune hyperparameter, which can depend on all other properties (architecture, optimizer/batch size, dataset, regularization,...) of the problem. The authors propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step length schedule from the optimizer. The proposed method can be used as a baseline for optimizer comparisons."
1524,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a method to transfer the implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. The method is based on the idea that the learning rate schedule is a difficult to tune hyperparameter, which can depend on all other properties (architecture, optimizer/batch size, dataset, regularization,...) of the problem. The authors propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step length schedule from the optimizer. The proposed method can be used as a baseline for optimizer comparisons."
1525,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a method to transfer the implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. The method is based on the idea that the learning rate schedule is a difficult to tune hyperparameter, which can depend on all other properties (architecture, optimizer/batch size, dataset, regularization,...) of the problem. The authors propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step length schedule from the optimizer. The proposed method can be used as a baseline for optimizer comparisons."
1526,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a method to transfer the implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. The method is based on the idea that the learning rate schedule is a difficult to tune hyperparameter, which can depend on all other properties (architecture, optimizer/batch size, dataset, regularization,...) of the problem. The authors propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step length schedule from the optimizer. The proposed method can be used as a baseline for optimizer comparisons."
1527,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning with sparse rewards. The main idea is to use the offline demonstration data generated by a sub-optimal behavior policy for faster and efficient online RL in such sparse reward settings. The proposed algorithm, called Learning Online with Guidance Offline (LOGO), merges a policy improvement step with an additional policy guidance step. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimality policy, while yet being able to learn beyond and approach optimality. The authors provide a theoretical analysis of the algorithm, and provide a lower bound on the performance improvement in each learning episode."
1528,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning with sparse rewards. The main idea is to use the offline demonstration data generated by a sub-optimal behavior policy for faster and efficient online RL in such sparse reward settings. The proposed algorithm, called Learning Online with Guidance Offline (LOGO), merges a policy improvement step with an additional policy guidance step. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimality policy, while yet being able to learn beyond and approach optimality. The authors provide a theoretical analysis of the algorithm, and provide a lower bound on the performance improvement in each learning episode."
1529,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning with sparse rewards. The main idea is to use the offline demonstration data generated by a sub-optimal behavior policy for faster and efficient online RL in such sparse reward settings. The proposed algorithm, called Learning Online with Guidance Offline (LOGO), merges a policy improvement step with an additional policy guidance step. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimality policy, while yet being able to learn beyond and approach optimality. The authors provide a theoretical analysis of the algorithm, and provide a lower bound on the performance improvement in each learning episode."
1530,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning with sparse rewards. The main idea is to use the offline demonstration data generated by a sub-optimal behavior policy for faster and efficient online RL in such sparse reward settings. The proposed algorithm, called Learning Online with Guidance Offline (LOGO), merges a policy improvement step with an additional policy guidance step. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimality policy, while yet being able to learn beyond and approach optimality. The authors provide a theoretical analysis of the algorithm, and provide a lower bound on the performance improvement in each learning episode."
1531,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,This paper proposes a two-stage Pareto framework for multi-task learning (MTL). The first stage is a neural network-based method to extract the weak Pareta front from the linear scalarization problem. The second stage is an FJC guided diffusive manifold that is used to bound the error between the true and the Stage-1 extracted weak pareto front. The proposed method is evaluated on a variety of benchmark datasets and compared to the state of the art.
1532,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,This paper proposes a two-stage Pareto framework for multi-task learning (MTL). The first stage is a neural network-based method to extract the weak Pareta front from the linear scalarization problem. The second stage is an FJC guided diffusive manifold that is used to bound the error between the true and the Stage-1 extracted weak pareto front. The proposed method is evaluated on a variety of benchmark datasets and compared to the state of the art.
1533,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,This paper proposes a two-stage Pareto framework for multi-task learning (MTL). The first stage is a neural network-based method to extract the weak Pareta front from the linear scalarization problem. The second stage is an FJC guided diffusive manifold that is used to bound the error between the true and the Stage-1 extracted weak pareto front. The proposed method is evaluated on a variety of benchmark datasets and compared to the state of the art.
1534,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,This paper proposes a two-stage Pareto framework for multi-task learning (MTL). The first stage is a neural network-based method to extract the weak Pareta front from the linear scalarization problem. The second stage is an FJC guided diffusive manifold that is used to bound the error between the true and the Stage-1 extracted weak pareto front. The proposed method is evaluated on a variety of benchmark datasets and compared to the state of the art.
1535,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of knowledge distillation, which aims to learn a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors show that a simple multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from task specific teacher(s). The proposed method outperforms the teacher (or best of all teachers) as well as the strong baseline of ImageNet pre-trained features."
1536,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of knowledge distillation, which aims to learn a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors show that a simple multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from task specific teacher(s). The proposed method outperforms the teacher (or best of all teachers) as well as the strong baseline of ImageNet pre-trained features."
1537,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of knowledge distillation, which aims to learn a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors show that a simple multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from task specific teacher(s). The proposed method outperforms the teacher (or best of all teachers) as well as the strong baseline of ImageNet pre-trained features."
1538,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of knowledge distillation, which aims to learn a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors show that a simple multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from task specific teacher(s). The proposed method outperforms the teacher (or best of all teachers) as well as the strong baseline of ImageNet pre-trained features."
1539,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. The authors propose a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications."
1540,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. The authors propose a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications."
1541,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. The authors propose a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications."
1542,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. The authors propose a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications."
1543,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. The proposed method learns a transferable reward signal formulated using the exemplary set by ordinal metric learning. Experiments on corrupted MNIST, the CU-Birds, and the COCO datasets demonstrate the effectiveness of the proposed method."
1544,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. The proposed method learns a transferable reward signal formulated using the exemplary set by ordinal metric learning. Experiments on corrupted MNIST, the CU-Birds, and the COCO datasets demonstrate the effectiveness of the proposed method."
1545,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. The proposed method learns a transferable reward signal formulated using the exemplary set by ordinal metric learning. Experiments on corrupted MNIST, the CU-Birds, and the COCO datasets demonstrate the effectiveness of the proposed method."
1546,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. The proposed method learns a transferable reward signal formulated using the exemplary set by ordinal metric learning. Experiments on corrupted MNIST, the CU-Birds, and the COCO datasets demonstrate the effectiveness of the proposed method."
1547,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a method for reducing the computational complexity of transformer-based attention in vision tasks. The proposed method is based on the idea that the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top k patches. The method is evaluated on image classification, feature matching, stereo matching, and object detection tasks and achieves state-of-the-art performance."
1548,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a method for reducing the computational complexity of transformer-based attention in vision tasks. The proposed method is based on the idea that the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top k patches. The method is evaluated on image classification, feature matching, stereo matching, and object detection tasks and achieves state-of-the-art performance."
1549,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a method for reducing the computational complexity of transformer-based attention in vision tasks. The proposed method is based on the idea that the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top k patches. The method is evaluated on image classification, feature matching, stereo matching, and object detection tasks and achieves state-of-the-art performance."
1550,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a method for reducing the computational complexity of transformer-based attention in vision tasks. The proposed method is based on the idea that the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top k patches. The method is evaluated on image classification, feature matching, stereo matching, and object detection tasks and achieves state-of-the-art performance."
1551,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes a method for learning options that can be transferred to new tasks without extrinsic rewards. The idea is to learn termination conditions of options by maximizing the mutual information between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, and derive the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options. Moreover, the reusability of options is tested by transferring options into various tasks."
1552,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes a method for learning options that can be transferred to new tasks without extrinsic rewards. The idea is to learn termination conditions of options by maximizing the mutual information between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, and derive the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options. Moreover, the reusability of options is tested by transferring options into various tasks."
1553,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes a method for learning options that can be transferred to new tasks without extrinsic rewards. The idea is to learn termination conditions of options by maximizing the mutual information between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, and derive the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options. Moreover, the reusability of options is tested by transferring options into various tasks."
1554,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes a method for learning options that can be transferred to new tasks without extrinsic rewards. The idea is to learn termination conditions of options by maximizing the mutual information between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, and derive the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options. Moreover, the reusability of options is tested by transferring options into various tasks."
1555,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a new method for open-world object detection, called semantic topology. The key idea is to assign all objects from the same category to their corresponding pre-defined node in the semantic graph. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed method can outperform the current state-of-the-art open world object detectors by a large margin."
1556,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a new method for open-world object detection, called semantic topology. The key idea is to assign all objects from the same category to their corresponding pre-defined node in the semantic graph. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed method can outperform the current state-of-the-art open world object detectors by a large margin."
1557,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a new method for open-world object detection, called semantic topology. The key idea is to assign all objects from the same category to their corresponding pre-defined node in the semantic graph. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed method can outperform the current state-of-the-art open world object detectors by a large margin."
1558,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a new method for open-world object detection, called semantic topology. The key idea is to assign all objects from the same category to their corresponding pre-defined node in the semantic graph. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed method can outperform the current state-of-the-art open world object detectors by a large margin."
1559,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a method to identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They show that balancing constraints on predicted class labels and decision boundaries are beneficial. They outperform competing baselines on standard classification datasets such as Cifar10, CIFAR-100, ImageNet, and long-tailed datasets."
1560,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a method to identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They show that balancing constraints on predicted class labels and decision boundaries are beneficial. They outperform competing baselines on standard classification datasets such as Cifar10, CIFAR-100, ImageNet, and long-tailed datasets."
1561,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a method to identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They show that balancing constraints on predicted class labels and decision boundaries are beneficial. They outperform competing baselines on standard classification datasets such as Cifar10, CIFAR-100, ImageNet, and long-tailed datasets."
1562,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a method to identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They show that balancing constraints on predicted class labels and decision boundaries are beneficial. They outperform competing baselines on standard classification datasets such as Cifar10, CIFAR-100, ImageNet, and long-tailed datasets."
1563,SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper proposes two methods for improving the generalization of semantic segmentation models. The first method is Instance-adaptive Batch Normalization (IaBN), which combines the feature statistics acquired at training time with those of the test sample. The second method is Seg-TTT, which adapts the model parameters to the test samples using a self-supervised loss. The experiments show that the proposed methods consistently and significantly outperform the baseline and attain a new state-of-the-art, substantially improving in accuracy over previous generalization methods."
1564,SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper proposes two methods for improving the generalization of semantic segmentation models. The first method is Instance-adaptive Batch Normalization (IaBN), which combines the feature statistics acquired at training time with those of the test sample. The second method is Seg-TTT, which adapts the model parameters to the test samples using a self-supervised loss. The experiments show that the proposed methods consistently and significantly outperform the baseline and attain a new state-of-the-art, substantially improving in accuracy over previous generalization methods."
1565,SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper proposes two methods for improving the generalization of semantic segmentation models. The first method is Instance-adaptive Batch Normalization (IaBN), which combines the feature statistics acquired at training time with those of the test sample. The second method is Seg-TTT, which adapts the model parameters to the test samples using a self-supervised loss. The experiments show that the proposed methods consistently and significantly outperform the baseline and attain a new state-of-the-art, substantially improving in accuracy over previous generalization methods."
1566,SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper proposes two methods for improving the generalization of semantic segmentation models. The first method is Instance-adaptive Batch Normalization (IaBN), which combines the feature statistics acquired at training time with those of the test sample. The second method is Seg-TTT, which adapts the model parameters to the test samples using a self-supervised loss. The experiments show that the proposed methods consistently and significantly outperform the baseline and attain a new state-of-the-art, substantially improving in accuracy over previous generalization methods."
1567,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a method for finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. The proposed method learns mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, the test sample can be scored by measuring whether the learned mappings lead to a small contrastive losses using the masked parts of this sample. The experiments show that the proposed method leads by a sizable accuracy gap in comparison to the literature and that the same default set rule of hyperparameters selection provides state of the art results across benchmarks."
1568,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a method for finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. The proposed method learns mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, the test sample can be scored by measuring whether the learned mappings lead to a small contrastive losses using the masked parts of this sample. The experiments show that the proposed method leads by a sizable accuracy gap in comparison to the literature and that the same default set rule of hyperparameters selection provides state of the art results across benchmarks."
1569,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a method for finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. The proposed method learns mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, the test sample can be scored by measuring whether the learned mappings lead to a small contrastive losses using the masked parts of this sample. The experiments show that the proposed method leads by a sizable accuracy gap in comparison to the literature and that the same default set rule of hyperparameters selection provides state of the art results across benchmarks."
1570,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a method for finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. The proposed method learns mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, the test sample can be scored by measuring whether the learned mappings lead to a small contrastive losses using the masked parts of this sample. The experiments show that the proposed method leads by a sizable accuracy gap in comparison to the literature and that the same default set rule of hyperparameters selection provides state of the art results across benchmarks."
1571,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,This paper proposes a method to learn a low-dimensional embedding space for neuropsychiatric neuroimaging data that preserves the diagnostic attributes of represented disorders. The authors propose a conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The proposed method is evaluated on two datasets and shows promising results.
1572,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,This paper proposes a method to learn a low-dimensional embedding space for neuropsychiatric neuroimaging data that preserves the diagnostic attributes of represented disorders. The authors propose a conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The proposed method is evaluated on two datasets and shows promising results.
1573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,This paper proposes a method to learn a low-dimensional embedding space for neuropsychiatric neuroimaging data that preserves the diagnostic attributes of represented disorders. The authors propose a conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The proposed method is evaluated on two datasets and shows promising results.
1574,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,This paper proposes a method to learn a low-dimensional embedding space for neuropsychiatric neuroimaging data that preserves the diagnostic attributes of represented disorders. The authors propose a conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The proposed method is evaluated on two datasets and shows promising results.
1575,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a parametric quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of parametric tensor-train network for feature extraction and a tensor product encoding for quantum encoding. The authors theoretically characterize the QTN by analyzing its representation power of input features and show that QTN enables an end-to-end parametric model pipeline, namely QTN-vQC. The experiments on the MNIST dataset demonstrate the advantages of the proposed method."
1576,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a parametric quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of parametric tensor-train network for feature extraction and a tensor product encoding for quantum encoding. The authors theoretically characterize the QTN by analyzing its representation power of input features and show that QTN enables an end-to-end parametric model pipeline, namely QTN-vQC. The experiments on the MNIST dataset demonstrate the advantages of the proposed method."
1577,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a parametric quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of parametric tensor-train network for feature extraction and a tensor product encoding for quantum encoding. The authors theoretically characterize the QTN by analyzing its representation power of input features and show that QTN enables an end-to-end parametric model pipeline, namely QTN-vQC. The experiments on the MNIST dataset demonstrate the advantages of the proposed method."
1578,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a parametric quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of parametric tensor-train network for feature extraction and a tensor product encoding for quantum encoding. The authors theoretically characterize the QTN by analyzing its representation power of input features and show that QTN enables an end-to-end parametric model pipeline, namely QTN-vQC. The experiments on the MNIST dataset demonstrate the advantages of the proposed method."
1579,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method for identifying commonalities between the underlying algorithms implemented by different neural networks trained for the same task. The method is based on the idea of constructing low-dimensional manifolds where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The authors show that the resulting model embedding spaces enable novel applications: clustering of neural networks on the basis of their high level computational processes in a manner that is less sensitive to reparameterization; model averaging of several networks trained on the same tasks to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model embeddings space."
1580,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method for identifying commonalities between the underlying algorithms implemented by different neural networks trained for the same task. The method is based on the idea of constructing low-dimensional manifolds where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The authors show that the resulting model embedding spaces enable novel applications: clustering of neural networks on the basis of their high level computational processes in a manner that is less sensitive to reparameterization; model averaging of several networks trained on the same tasks to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model embeddings space."
1581,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method for identifying commonalities between the underlying algorithms implemented by different neural networks trained for the same task. The method is based on the idea of constructing low-dimensional manifolds where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The authors show that the resulting model embedding spaces enable novel applications: clustering of neural networks on the basis of their high level computational processes in a manner that is less sensitive to reparameterization; model averaging of several networks trained on the same tasks to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model embeddings space."
1582,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method for identifying commonalities between the underlying algorithms implemented by different neural networks trained for the same task. The method is based on the idea of constructing low-dimensional manifolds where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The authors show that the resulting model embedding spaces enable novel applications: clustering of neural networks on the basis of their high level computational processes in a manner that is less sensitive to reparameterization; model averaging of several networks trained on the same tasks to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model embeddings space."
1583,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a method for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The method is implemented using a graph neural network as the constraint function and gradient descent as a constraint solver. The architecture can be trained by standard backpropagation. Experiments are conducted on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids."
1584,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a method for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The method is implemented using a graph neural network as the constraint function and gradient descent as a constraint solver. The architecture can be trained by standard backpropagation. Experiments are conducted on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids."
1585,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a method for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The method is implemented using a graph neural network as the constraint function and gradient descent as a constraint solver. The architecture can be trained by standard backpropagation. Experiments are conducted on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids."
1586,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a method for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The method is implemented using a graph neural network as the constraint function and gradient descent as a constraint solver. The architecture can be trained by standard backpropagation. Experiments are conducted on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids."
1587,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes a method for selecting high-quality policies for reinforcement learning (RL) based on evolutionary techniques. The proposed method is based on the idea of clustering-based selection, where each policy is divided into several clusters based on its behaviors, and a high quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on several continuous control tasks show that the proposed method can achieve better performance than previous methods."
1588,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes a method for selecting high-quality policies for reinforcement learning (RL) based on evolutionary techniques. The proposed method is based on the idea of clustering-based selection, where each policy is divided into several clusters based on its behaviors, and a high quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on several continuous control tasks show that the proposed method can achieve better performance than previous methods."
1589,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes a method for selecting high-quality policies for reinforcement learning (RL) based on evolutionary techniques. The proposed method is based on the idea of clustering-based selection, where each policy is divided into several clusters based on its behaviors, and a high quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on several continuous control tasks show that the proposed method can achieve better performance than previous methods."
1590,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes a method for selecting high-quality policies for reinforcement learning (RL) based on evolutionary techniques. The proposed method is based on the idea of clustering-based selection, where each policy is divided into several clusters based on its behaviors, and a high quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on several continuous control tasks show that the proposed method can achieve better performance than previous methods."
1591,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed linear stochastic approximation algorithm with Markovian noise and general consensus-type interaction. The authors consider the case where the interconnection matrix is doubly-stochastic and provide finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. In the case when the convex combination is required to be a straight average and interaction between any pair of neighboring agents may be uni-directional, the authors propose a push-type distributed stochastically approximation algorithm."
1592,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed linear stochastic approximation algorithm with Markovian noise and general consensus-type interaction. The authors consider the case where the interconnection matrix is doubly-stochastic and provide finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. In the case when the convex combination is required to be a straight average and interaction between any pair of neighboring agents may be uni-directional, the authors propose a push-type distributed stochastically approximation algorithm."
1593,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed linear stochastic approximation algorithm with Markovian noise and general consensus-type interaction. The authors consider the case where the interconnection matrix is doubly-stochastic and provide finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. In the case when the convex combination is required to be a straight average and interaction between any pair of neighboring agents may be uni-directional, the authors propose a push-type distributed stochastically approximation algorithm."
1594,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed linear stochastic approximation algorithm with Markovian noise and general consensus-type interaction. The authors consider the case where the interconnection matrix is doubly-stochastic and provide finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. In the case when the convex combination is required to be a straight average and interaction between any pair of neighboring agents may be uni-directional, the authors propose a push-type distributed stochastically approximation algorithm."
1595,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a graph neural network (GNN) that is combinatorially efficient, equivariant and constraint-aware. The core of the proposed GNN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Theoretically, the proposed GMN is proved to be universally expressive under certain conditions. Experiments are conducted on simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture."
1596,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a graph neural network (GNN) that is combinatorially efficient, equivariant and constraint-aware. The core of the proposed GNN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Theoretically, the proposed GMN is proved to be universally expressive under certain conditions. Experiments are conducted on simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture."
1597,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a graph neural network (GNN) that is combinatorially efficient, equivariant and constraint-aware. The core of the proposed GNN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Theoretically, the proposed GMN is proved to be universally expressive under certain conditions. Experiments are conducted on simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture."
1598,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a graph neural network (GNN) that is combinatorially efficient, equivariant and constraint-aware. The core of the proposed GNN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Theoretically, the proposed GMN is proved to be universally expressive under certain conditions. Experiments are conducted on simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture."
1599,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. The authors provide convergence analysis of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalization with a small fraction of personalized parameters."
1600,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. The authors provide convergence analysis of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalization with a small fraction of personalized parameters."
1601,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. The authors provide convergence analysis of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalization with a small fraction of personalized parameters."
1602,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. The authors provide convergence analysis of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalization with a small fraction of personalized parameters."
1603,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations that simulates viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in such unsegmented viewing sequences. The authors consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object."
1604,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations that simulates viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in such unsegmented viewing sequences. The authors consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object."
1605,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations that simulates viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in such unsegmented viewing sequences. The authors consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object."
1606,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations that simulates viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in such unsegmented viewing sequences. The authors consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object."
1607,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes an extension of the AlphaZero algorithm for goal-directed planning in a deterministic transition model. The main idea is to use hindsight experience replay (HVR) to guide the search of the tree in the MCTS algorithm. The authors show that HVR improves the performance of AlphaZero in a variety of domains, including a quantum compiling domain."
1608,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes an extension of the AlphaZero algorithm for goal-directed planning in a deterministic transition model. The main idea is to use hindsight experience replay (HVR) to guide the search of the tree in the MCTS algorithm. The authors show that HVR improves the performance of AlphaZero in a variety of domains, including a quantum compiling domain."
1609,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes an extension of the AlphaZero algorithm for goal-directed planning in a deterministic transition model. The main idea is to use hindsight experience replay (HVR) to guide the search of the tree in the MCTS algorithm. The authors show that HVR improves the performance of AlphaZero in a variety of domains, including a quantum compiling domain."
1610,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes an extension of the AlphaZero algorithm for goal-directed planning in a deterministic transition model. The main idea is to use hindsight experience replay (HVR) to guide the search of the tree in the MCTS algorithm. The authors show that HVR improves the performance of AlphaZero in a variety of domains, including a quantum compiling domain."
1611,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes a method for continual learning. The proposed method is based on the idea of iterative gradient optimization (RGO). The authors propose to use a virtual feature encoding layer (FEL) to represent different network structures with only task descriptors. Experiments are conducted on CIFAR-100, MiniImageNet, and ImageNet."
1612,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes a method for continual learning. The proposed method is based on the idea of iterative gradient optimization (RGO). The authors propose to use a virtual feature encoding layer (FEL) to represent different network structures with only task descriptors. Experiments are conducted on CIFAR-100, MiniImageNet, and ImageNet."
1613,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes a method for continual learning. The proposed method is based on the idea of iterative gradient optimization (RGO). The authors propose to use a virtual feature encoding layer (FEL) to represent different network structures with only task descriptors. Experiments are conducted on CIFAR-100, MiniImageNet, and ImageNet."
1614,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes a method for continual learning. The proposed method is based on the idea of iterative gradient optimization (RGO). The authors propose to use a virtual feature encoding layer (FEL) to represent different network structures with only task descriptors. Experiments are conducted on CIFAR-100, MiniImageNet, and ImageNet."
1615,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper studies the properties and applications of aligned generative models. Specifically, the authors empirically analyze aligned models and provide answers to important questions regarding their nature. They find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. They further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain."
1616,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper studies the properties and applications of aligned generative models. Specifically, the authors empirically analyze aligned models and provide answers to important questions regarding their nature. They find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. They further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain."
1617,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper studies the properties and applications of aligned generative models. Specifically, the authors empirically analyze aligned models and provide answers to important questions regarding their nature. They find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. They further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain."
1618,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper studies the properties and applications of aligned generative models. Specifically, the authors empirically analyze aligned models and provide answers to important questions regarding their nature. They find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. They further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain."
1619,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a relaxation of the Gromov-Wasserstein (GW) distance between two graphs based on Optimal Transport (OT). The authors argue that the GW distance imposes a coupling between all the nodes from the two considered graphs, which can be detrimental for tasks such as graph dictionary or partition learning. To alleviate this issue, the authors propose a new semi-relaxed version of the GW divergence, which relaxes the coupling between the nodes of the two graphs. The authors show that the proposed relaxation can lead to an efficient graph dictionary learning algorithm and demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion."
1620,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a relaxation of the Gromov-Wasserstein (GW) distance between two graphs based on Optimal Transport (OT). The authors argue that the GW distance imposes a coupling between all the nodes from the two considered graphs, which can be detrimental for tasks such as graph dictionary or partition learning. To alleviate this issue, the authors propose a new semi-relaxed version of the GW divergence, which relaxes the coupling between the nodes of the two graphs. The authors show that the proposed relaxation can lead to an efficient graph dictionary learning algorithm and demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion."
1621,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a relaxation of the Gromov-Wasserstein (GW) distance between two graphs based on Optimal Transport (OT). The authors argue that the GW distance imposes a coupling between all the nodes from the two considered graphs, which can be detrimental for tasks such as graph dictionary or partition learning. To alleviate this issue, the authors propose a new semi-relaxed version of the GW divergence, which relaxes the coupling between the nodes of the two graphs. The authors show that the proposed relaxation can lead to an efficient graph dictionary learning algorithm and demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion."
1622,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a relaxation of the Gromov-Wasserstein (GW) distance between two graphs based on Optimal Transport (OT). The authors argue that the GW distance imposes a coupling between all the nodes from the two considered graphs, which can be detrimental for tasks such as graph dictionary or partition learning. To alleviate this issue, the authors propose a new semi-relaxed version of the GW divergence, which relaxes the coupling between the nodes of the two graphs. The authors show that the proposed relaxation can lead to an efficient graph dictionary learning algorithm and demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion."
1623,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method for modeling systematic suboptimality in human behavior. The method is based on the Boltzmann policy distribution (BPD), which is a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
1624,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method for modeling systematic suboptimality in human behavior. The method is based on the Boltzmann policy distribution (BPD), which is a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
1625,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method for modeling systematic suboptimality in human behavior. The method is based on the Boltzmann policy distribution (BPD), which is a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
1626,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method for modeling systematic suboptimality in human behavior. The method is based on the Boltzmann policy distribution (BPD), which is a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
1627,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free post-training quantization method, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. The authors decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, the authors progressively compose sub-item and propose a novel data free optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short). The authors also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver."
1628,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free post-training quantization method, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. The authors decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, the authors progressively compose sub-item and propose a novel data free optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short). The authors also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver."
1629,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free post-training quantization method, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. The authors decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, the authors progressively compose sub-item and propose a novel data free optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short). The authors also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver."
1630,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free post-training quantization method, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. The authors decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, the authors progressively compose sub-item and propose a novel data free optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short). The authors also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver."
1631,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation based on a neural network architecture. The proposed method, SegTime, finds precise breakpoints, obviates sliding windows, handles long-term dependencies, and is insensitive to the label changing frequency. The method is evaluated on a variety of tasks, including stock market partitioning, sleep stage labelling, and human activity recognition."
1632,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation based on a neural network architecture. The proposed method, SegTime, finds precise breakpoints, obviates sliding windows, handles long-term dependencies, and is insensitive to the label changing frequency. The method is evaluated on a variety of tasks, including stock market partitioning, sleep stage labelling, and human activity recognition."
1633,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation based on a neural network architecture. The proposed method, SegTime, finds precise breakpoints, obviates sliding windows, handles long-term dependencies, and is insensitive to the label changing frequency. The method is evaluated on a variety of tasks, including stock market partitioning, sleep stage labelling, and human activity recognition."
1634,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation based on a neural network architecture. The proposed method, SegTime, finds precise breakpoints, obviates sliding windows, handles long-term dependencies, and is insensitive to the label changing frequency. The method is evaluated on a variety of tasks, including stock market partitioning, sleep stage labelling, and human activity recognition."
1635,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a decomposition based explanation for Graph Neural Networks (GNNs). The main idea is to decompose the information generation and aggregation mechanism of GNNs, which allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, the authors further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of the algorithm can be further improved by utilizing GNN characteristics. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
1636,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a decomposition based explanation for Graph Neural Networks (GNNs). The main idea is to decompose the information generation and aggregation mechanism of GNNs, which allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, the authors further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of the algorithm can be further improved by utilizing GNN characteristics. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
1637,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a decomposition based explanation for Graph Neural Networks (GNNs). The main idea is to decompose the information generation and aggregation mechanism of GNNs, which allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, the authors further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of the algorithm can be further improved by utilizing GNN characteristics. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
1638,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a decomposition based explanation for Graph Neural Networks (GNNs). The main idea is to decompose the information generation and aggregation mechanism of GNNs, which allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, the authors further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of the algorithm can be further improved by utilizing GNN characteristics. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
1639,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer, DiffStride, that learns the size of a cropping mask in the Fourier domain. The authors claim that this allows to find better configurations at a lower computational cost. Experiments on audio and image classification show the generality and effectiveness of the proposed method."
1640,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer, DiffStride, that learns the size of a cropping mask in the Fourier domain. The authors claim that this allows to find better configurations at a lower computational cost. Experiments on audio and image classification show the generality and effectiveness of the proposed method."
1641,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer, DiffStride, that learns the size of a cropping mask in the Fourier domain. The authors claim that this allows to find better configurations at a lower computational cost. Experiments on audio and image classification show the generality and effectiveness of the proposed method."
1642,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer, DiffStride, that learns the size of a cropping mask in the Fourier domain. The authors claim that this allows to find better configurations at a lower computational cost. Experiments on audio and image classification show the generality and effectiveness of the proposed method."
1643,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on a novel localized randomized smoothing approach, where the random perturbation strength for different input region is proportional to their importance for the outputs. The proposed method yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks."
1644,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on a novel localized randomized smoothing approach, where the random perturbation strength for different input region is proportional to their importance for the outputs. The proposed method yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks."
1645,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on a novel localized randomized smoothing approach, where the random perturbation strength for different input region is proportional to their importance for the outputs. The proposed method yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks."
1646,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on a novel localized randomized smoothing approach, where the random perturbation strength for different input region is proportional to their importance for the outputs. The proposed method yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks."
1647,SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases in normalizing flows. The method is based on embedding the embeddings of differentiable probabilistic models into equivalent bijective transformations. The proposed method is evaluated on a variety of structured inference problems, and compared with a number of baselines. The results show that the proposed method outperforms the baselines in most cases."
1648,SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases in normalizing flows. The method is based on embedding the embeddings of differentiable probabilistic models into equivalent bijective transformations. The proposed method is evaluated on a variety of structured inference problems, and compared with a number of baselines. The results show that the proposed method outperforms the baselines in most cases."
1649,SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases in normalizing flows. The method is based on embedding the embeddings of differentiable probabilistic models into equivalent bijective transformations. The proposed method is evaluated on a variety of structured inference problems, and compared with a number of baselines. The results show that the proposed method outperforms the baselines in most cases."
1650,SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases in normalizing flows. The method is based on embedding the embeddings of differentiable probabilistic models into equivalent bijective transformations. The proposed method is evaluated on a variety of structured inference problems, and compared with a number of baselines. The results show that the proposed method outperforms the baselines in most cases."
1651,SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of text-only language models (LMs) to learn to tie a word for which they have learned a representation, to its actual use in the world. The authors test whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, they show a model what the word “left” means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, “right”, in a similar grid world. They investigate a range of generative language models of varying sizes and show that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but also appears to generalize to several instances of unseen concepts as well."
1652,SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of text-only language models (LMs) to learn to tie a word for which they have learned a representation, to its actual use in the world. The authors test whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, they show a model what the word “left” means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, “right”, in a similar grid world. They investigate a range of generative language models of varying sizes and show that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but also appears to generalize to several instances of unseen concepts as well."
1653,SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of text-only language models (LMs) to learn to tie a word for which they have learned a representation, to its actual use in the world. The authors test whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, they show a model what the word “left” means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, “right”, in a similar grid world. They investigate a range of generative language models of varying sizes and show that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but also appears to generalize to several instances of unseen concepts as well."
1654,SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of text-only language models (LMs) to learn to tie a word for which they have learned a representation, to its actual use in the world. The authors test whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, they show a model what the word “left” means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, “right”, in a similar grid world. They investigate a range of generative language models of varying sizes and show that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but also appears to generalize to several instances of unseen concepts as well."
1655,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the problem of emergent language in the context of reinforcement learning. In particular, the authors argue that shaped rewards can bias the semantics of the learned language, significantly change the entropy of the language, and mask the potential effects of other environmental variables of interest. The authors use a simple sender-receiver navigation game to demonstrate how shaped rewards are intentionally designed conflicts with the basic premise that emergent phenomena arising from basic principles."
1656,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the problem of emergent language in the context of reinforcement learning. In particular, the authors argue that shaped rewards can bias the semantics of the learned language, significantly change the entropy of the language, and mask the potential effects of other environmental variables of interest. The authors use a simple sender-receiver navigation game to demonstrate how shaped rewards are intentionally designed conflicts with the basic premise that emergent phenomena arising from basic principles."
1657,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the problem of emergent language in the context of reinforcement learning. In particular, the authors argue that shaped rewards can bias the semantics of the learned language, significantly change the entropy of the language, and mask the potential effects of other environmental variables of interest. The authors use a simple sender-receiver navigation game to demonstrate how shaped rewards are intentionally designed conflicts with the basic premise that emergent phenomena arising from basic principles."
1658,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the problem of emergent language in the context of reinforcement learning. In particular, the authors argue that shaped rewards can bias the semantics of the learned language, significantly change the entropy of the language, and mask the potential effects of other environmental variables of interest. The authors use a simple sender-receiver navigation game to demonstrate how shaped rewards are intentionally designed conflicts with the basic premise that emergent phenomena arising from basic principles."
1659,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. The authors show that invariance of the feature representations strongly correlates with transfer performance, and distributional shift in class priors between data in the source and target languages negatively affects performance. To address this issue, the authors propose an importance weighted domain alignment method, IWDA, which is based on importance weighting. Experiments show that the proposed method outperforms the state-of-the-art in terms of zero-shot transfer performance."
1660,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. The authors show that invariance of the feature representations strongly correlates with transfer performance, and distributional shift in class priors between data in the source and target languages negatively affects performance. To address this issue, the authors propose an importance weighted domain alignment method, IWDA, which is based on importance weighting. Experiments show that the proposed method outperforms the state-of-the-art in terms of zero-shot transfer performance."
1661,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. The authors show that invariance of the feature representations strongly correlates with transfer performance, and distributional shift in class priors between data in the source and target languages negatively affects performance. To address this issue, the authors propose an importance weighted domain alignment method, IWDA, which is based on importance weighting. Experiments show that the proposed method outperforms the state-of-the-art in terms of zero-shot transfer performance."
1662,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. The authors show that invariance of the feature representations strongly correlates with transfer performance, and distributional shift in class priors between data in the source and target languages negatively affects performance. To address this issue, the authors propose an importance weighted domain alignment method, IWDA, which is based on importance weighting. Experiments show that the proposed method outperforms the state-of-the-art in terms of zero-shot transfer performance."
1663,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a bootstrapping-based meta-learning algorithm for model-free agents. The algorithm first bootstraps a target from the metalearner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. The bootstrapped mechanism can extend the effective meta learning horizon without requiring backpropagation through all updates."
1664,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a bootstrapping-based meta-learning algorithm for model-free agents. The algorithm first bootstraps a target from the metalearner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. The bootstrapped mechanism can extend the effective meta learning horizon without requiring backpropagation through all updates."
1665,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a bootstrapping-based meta-learning algorithm for model-free agents. The algorithm first bootstraps a target from the metalearner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. The bootstrapped mechanism can extend the effective meta learning horizon without requiring backpropagation through all updates."
1666,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a bootstrapping-based meta-learning algorithm for model-free agents. The algorithm first bootstraps a target from the metalearner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. The bootstrapped mechanism can extend the effective meta learning horizon without requiring backpropagation through all updates."
1667,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based reinforcement learning agents in comparison to their model-free counterparts. Specifically, the authors focus on MuZero [64], a powerful model based agent, and evaluate its performance on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, MuZero achieves state-of-the art generalization performance and data efficiency on Procgen [9]. However, these factors do not always provide the same benefits for task generalisation in Meta-World [81], indicating that transfer remains a challenge. Overall, this paper suggests that building generalizable agents requires moving beyond the single-task, model-Free paradigm and towards self- supervised model based agents that are trained in rich, procedural, multi-task environments."
1668,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based reinforcement learning agents in comparison to their model-free counterparts. Specifically, the authors focus on MuZero [64], a powerful model based agent, and evaluate its performance on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, MuZero achieves state-of-the art generalization performance and data efficiency on Procgen [9]. However, these factors do not always provide the same benefits for task generalisation in Meta-World [81], indicating that transfer remains a challenge. Overall, this paper suggests that building generalizable agents requires moving beyond the single-task, model-Free paradigm and towards self- supervised model based agents that are trained in rich, procedural, multi-task environments."
1669,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based reinforcement learning agents in comparison to their model-free counterparts. Specifically, the authors focus on MuZero [64], a powerful model based agent, and evaluate its performance on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, MuZero achieves state-of-the art generalization performance and data efficiency on Procgen [9]. However, these factors do not always provide the same benefits for task generalisation in Meta-World [81], indicating that transfer remains a challenge. Overall, this paper suggests that building generalizable agents requires moving beyond the single-task, model-Free paradigm and towards self- supervised model based agents that are trained in rich, procedural, multi-task environments."
1670,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based reinforcement learning agents in comparison to their model-free counterparts. Specifically, the authors focus on MuZero [64], a powerful model based agent, and evaluate its performance on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, MuZero achieves state-of-the art generalization performance and data efficiency on Procgen [9]. However, these factors do not always provide the same benefits for task generalisation in Meta-World [81], indicating that transfer remains a challenge. Overall, this paper suggests that building generalizable agents requires moving beyond the single-task, model-Free paradigm and towards self- supervised model based agents that are trained in rich, procedural, multi-task environments."
1671,SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph deconvolution network (GDN) to learn graph structure from data. The main idea is to unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that can learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. Since GDN directly operate on, combine, and refine graph objects (instead of node features), GDNs are inherently inductive and can generalize to larger-sized graphs after training. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the robustness and representation power of GDN."
1672,SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph deconvolution network (GDN) to learn graph structure from data. The main idea is to unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that can learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. Since GDN directly operate on, combine, and refine graph objects (instead of node features), GDNs are inherently inductive and can generalize to larger-sized graphs after training. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the robustness and representation power of GDN."
1673,SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph deconvolution network (GDN) to learn graph structure from data. The main idea is to unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that can learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. Since GDN directly operate on, combine, and refine graph objects (instead of node features), GDNs are inherently inductive and can generalize to larger-sized graphs after training. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the robustness and representation power of GDN."
1674,SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph deconvolution network (GDN) to learn graph structure from data. The main idea is to unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that can learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. Since GDN directly operate on, combine, and refine graph objects (instead of node features), GDNs are inherently inductive and can generalize to larger-sized graphs after training. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the robustness and representation power of GDN."
1675,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,This paper proposes a reinforcement learning algorithm for reward shaping that learns to construct a shaping-reward function that is tailored to the task. The algorithm is based on a Markov game between two agents. The first agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. Theoretical analysis is provided to show that the proposed algorithm is able to converge to the optimal reward function. Experiments are conducted on a variety of tasks and show the effectiveness of the proposed method.
1676,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,This paper proposes a reinforcement learning algorithm for reward shaping that learns to construct a shaping-reward function that is tailored to the task. The algorithm is based on a Markov game between two agents. The first agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. Theoretical analysis is provided to show that the proposed algorithm is able to converge to the optimal reward function. Experiments are conducted on a variety of tasks and show the effectiveness of the proposed method.
1677,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,This paper proposes a reinforcement learning algorithm for reward shaping that learns to construct a shaping-reward function that is tailored to the task. The algorithm is based on a Markov game between two agents. The first agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. Theoretical analysis is provided to show that the proposed algorithm is able to converge to the optimal reward function. Experiments are conducted on a variety of tasks and show the effectiveness of the proposed method.
1678,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,This paper proposes a reinforcement learning algorithm for reward shaping that learns to construct a shaping-reward function that is tailored to the task. The algorithm is based on a Markov game between two agents. The first agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. Theoretical analysis is provided to show that the proposed algorithm is able to converge to the optimal reward function. Experiments are conducted on a variety of tasks and show the effectiveness of the proposed method.
1679,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a method to improve the robustness of vertical federated learning (VFL) against backdoor attacks and inference-phase adversarial and missing feature attacks. The proposed method is based on the idea of recovering the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. In addition, the proposed method also defend against inference phase adversarial attacks and missing features attacks. Experiments on NUS-WIDE and CIFAR-10 datasets demonstrate the effectiveness of the method."
1680,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a method to improve the robustness of vertical federated learning (VFL) against backdoor attacks and inference-phase adversarial and missing feature attacks. The proposed method is based on the idea of recovering the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. In addition, the proposed method also defend against inference phase adversarial attacks and missing features attacks. Experiments on NUS-WIDE and CIFAR-10 datasets demonstrate the effectiveness of the method."
1681,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a method to improve the robustness of vertical federated learning (VFL) against backdoor attacks and inference-phase adversarial and missing feature attacks. The proposed method is based on the idea of recovering the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. In addition, the proposed method also defend against inference phase adversarial attacks and missing features attacks. Experiments on NUS-WIDE and CIFAR-10 datasets demonstrate the effectiveness of the method."
1682,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a method to improve the robustness of vertical federated learning (VFL) against backdoor attacks and inference-phase adversarial and missing feature attacks. The proposed method is based on the idea of recovering the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. In addition, the proposed method also defend against inference phase adversarial attacks and missing features attacks. Experiments on NUS-WIDE and CIFAR-10 datasets demonstrate the effectiveness of the method."
1683,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes a contrastive learning method for unsupervised dense retrieval. The method is based on the idea that contrastive loss can be used to improve the performance of dense retrieval methods. The authors show that the proposed method outperforms the existing methods on the BEIR benchmark and the MS MARCO dataset. They also show that fine-tuning the model on these datasets leads to improved performance.
1684,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes a contrastive learning method for unsupervised dense retrieval. The method is based on the idea that contrastive loss can be used to improve the performance of dense retrieval methods. The authors show that the proposed method outperforms the existing methods on the BEIR benchmark and the MS MARCO dataset. They also show that fine-tuning the model on these datasets leads to improved performance.
1685,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes a contrastive learning method for unsupervised dense retrieval. The method is based on the idea that contrastive loss can be used to improve the performance of dense retrieval methods. The authors show that the proposed method outperforms the existing methods on the BEIR benchmark and the MS MARCO dataset. They also show that fine-tuning the model on these datasets leads to improved performance.
1686,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes a contrastive learning method for unsupervised dense retrieval. The method is based on the idea that contrastive loss can be used to improve the performance of dense retrieval methods. The authors show that the proposed method outperforms the existing methods on the BEIR benchmark and the MS MARCO dataset. They also show that fine-tuning the model on these datasets leads to improved performance.
1687,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the tradeoff between fine-tuning and linear probing in the presence of distribution shift. The authors theoretically analyze the tradeoffs arising in overparameterized two-layer linear networks, characterizing how fine-tuneing can distort high-quality pretrained features which leads to low OOD accuracy. The analysis suggests that the simple two-step strategy of linear probing then full finetuning can achieve better ID and OOD performance."
1688,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the tradeoff between fine-tuning and linear probing in the presence of distribution shift. The authors theoretically analyze the tradeoffs arising in overparameterized two-layer linear networks, characterizing how fine-tuneing can distort high-quality pretrained features which leads to low OOD accuracy. The analysis suggests that the simple two-step strategy of linear probing then full finetuning can achieve better ID and OOD performance."
1689,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the tradeoff between fine-tuning and linear probing in the presence of distribution shift. The authors theoretically analyze the tradeoffs arising in overparameterized two-layer linear networks, characterizing how fine-tuneing can distort high-quality pretrained features which leads to low OOD accuracy. The analysis suggests that the simple two-step strategy of linear probing then full finetuning can achieve better ID and OOD performance."
1690,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the tradeoff between fine-tuning and linear probing in the presence of distribution shift. The authors theoretically analyze the tradeoffs arising in overparameterized two-layer linear networks, characterizing how fine-tuneing can distort high-quality pretrained features which leads to low OOD accuracy. The analysis suggests that the simple two-step strategy of linear probing then full finetuning can achieve better ID and OOD performance."
1691,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning novel classes (L2DNC) from unlabeled data. The authors propose a meta-learning approach to solve the problem. The main contribution of this paper is to study the implicit assumptions behind L2DN and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, the authors propose to use meta-learners to solve this problem. Empirical results show the effectiveness of the proposed method."
1692,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning novel classes (L2DNC) from unlabeled data. The authors propose a meta-learning approach to solve the problem. The main contribution of this paper is to study the implicit assumptions behind L2DN and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, the authors propose to use meta-learners to solve this problem. Empirical results show the effectiveness of the proposed method."
1693,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning novel classes (L2DNC) from unlabeled data. The authors propose a meta-learning approach to solve the problem. The main contribution of this paper is to study the implicit assumptions behind L2DN and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, the authors propose to use meta-learners to solve this problem. Empirical results show the effectiveness of the proposed method."
1694,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning novel classes (L2DNC) from unlabeled data. The authors propose a meta-learning approach to solve the problem. The main contribution of this paper is to study the implicit assumptions behind L2DN and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, the authors propose to use meta-learners to solve this problem. Empirical results show the effectiveness of the proposed method."
1695,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning a causal model of POMDPs, where the agent has access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). The authors propose a general yet simple methodology for safely leveraging offline data during learning. The proposed method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard pOMDP transition model via deconfounding using the recovered latent variable. The authors prove that their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and empirically on a series of synthetic toy problems."
1696,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning a causal model of POMDPs, where the agent has access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). The authors propose a general yet simple methodology for safely leveraging offline data during learning. The proposed method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard pOMDP transition model via deconfounding using the recovered latent variable. The authors prove that their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and empirically on a series of synthetic toy problems."
1697,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning a causal model of POMDPs, where the agent has access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). The authors propose a general yet simple methodology for safely leveraging offline data during learning. The proposed method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard pOMDP transition model via deconfounding using the recovered latent variable. The authors prove that their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and empirically on a series of synthetic toy problems."
1698,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning a causal model of POMDPs, where the agent has access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). The authors propose a general yet simple methodology for safely leveraging offline data during learning. The proposed method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard pOMDP transition model via deconfounding using the recovered latent variable. The authors prove that their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and empirically on a series of synthetic toy problems."
1699,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes to train a retriever to retrieve relevant passages from a textual knowledge corpus (e.g., Wikipedia) and provide these passages as additional context to the generator. The retriever is trained jointly with the generator and the generator by maximizing the evidence lower bound (ELBo) in expectation over the posterior distribution Q of passages given the input and the target output. The paper also proposes to use an additional guide retriever that is allowed to use the source output and “in hindsight” retrieve relevant passage during training. Experiments on the Wizard of Wikipedia dataset show that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage (19% relative improvements), and the end-to-end system produces better overall output."
1700,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes to train a retriever to retrieve relevant passages from a textual knowledge corpus (e.g., Wikipedia) and provide these passages as additional context to the generator. The retriever is trained jointly with the generator and the generator by maximizing the evidence lower bound (ELBo) in expectation over the posterior distribution Q of passages given the input and the target output. The paper also proposes to use an additional guide retriever that is allowed to use the source output and “in hindsight” retrieve relevant passage during training. Experiments on the Wizard of Wikipedia dataset show that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage (19% relative improvements), and the end-to-end system produces better overall output."
1701,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes to train a retriever to retrieve relevant passages from a textual knowledge corpus (e.g., Wikipedia) and provide these passages as additional context to the generator. The retriever is trained jointly with the generator and the generator by maximizing the evidence lower bound (ELBo) in expectation over the posterior distribution Q of passages given the input and the target output. The paper also proposes to use an additional guide retriever that is allowed to use the source output and “in hindsight” retrieve relevant passage during training. Experiments on the Wizard of Wikipedia dataset show that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage (19% relative improvements), and the end-to-end system produces better overall output."
1702,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes to train a retriever to retrieve relevant passages from a textual knowledge corpus (e.g., Wikipedia) and provide these passages as additional context to the generator. The retriever is trained jointly with the generator and the generator by maximizing the evidence lower bound (ELBo) in expectation over the posterior distribution Q of passages given the input and the target output. The paper also proposes to use an additional guide retriever that is allowed to use the source output and “in hindsight” retrieve relevant passage during training. Experiments on the Wizard of Wikipedia dataset show that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage (19% relative improvements), and the end-to-end system produces better overall output."
1703,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,This paper proposes a graph neural network (GNN) for influence estimation and influence maximization. The main contribution of the paper is the introduction of a GNN that parameterizes an upper bound of influence estimation on small simulated graphs and train it on large real graphs. Experiments show that the proposed GNN can provide accurate influence estimation for real graphs up to 10 times larger than the train set. The authors also develop a version of Cost Effective Lazy Forward optimization with GLIE instead of simulated influence estimation.
1704,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,This paper proposes a graph neural network (GNN) for influence estimation and influence maximization. The main contribution of the paper is the introduction of a GNN that parameterizes an upper bound of influence estimation on small simulated graphs and train it on large real graphs. Experiments show that the proposed GNN can provide accurate influence estimation for real graphs up to 10 times larger than the train set. The authors also develop a version of Cost Effective Lazy Forward optimization with GLIE instead of simulated influence estimation.
1705,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,This paper proposes a graph neural network (GNN) for influence estimation and influence maximization. The main contribution of the paper is the introduction of a GNN that parameterizes an upper bound of influence estimation on small simulated graphs and train it on large real graphs. Experiments show that the proposed GNN can provide accurate influence estimation for real graphs up to 10 times larger than the train set. The authors also develop a version of Cost Effective Lazy Forward optimization with GLIE instead of simulated influence estimation.
1706,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,This paper proposes a graph neural network (GNN) for influence estimation and influence maximization. The main contribution of the paper is the introduction of a GNN that parameterizes an upper bound of influence estimation on small simulated graphs and train it on large real graphs. Experiments show that the proposed GNN can provide accurate influence estimation for real graphs up to 10 times larger than the train set. The authors also develop a version of Cost Effective Lazy Forward optimization with GLIE instead of simulated influence estimation.
1707,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper studies active learning strategies for domain adaptation under the assumption of Lipschitz functions. The authors derive generalization error bounds for such strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds.
1708,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper studies active learning strategies for domain adaptation under the assumption of Lipschitz functions. The authors derive generalization error bounds for such strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds.
1709,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper studies active learning strategies for domain adaptation under the assumption of Lipschitz functions. The authors derive generalization error bounds for such strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds.
1710,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper studies active learning strategies for domain adaptation under the assumption of Lipschitz functions. The authors derive generalization error bounds for such strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds.
1711,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks based on the desingularization map. The authors show that the posterior distribution over the parameters of a singular model, following an algebraic-geometrical transformation known as a desingularity map, is asymptotically a mixture of standard forms. They then show that a generalized gamma mean-field variational family, following desingulumization, can recover the leading order term of the model evidence. The proposed method is a normalizing flow with the generalized gamma as the source distribution."
1712,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks based on the desingularization map. The authors show that the posterior distribution over the parameters of a singular model, following an algebraic-geometrical transformation known as a desingularity map, is asymptotically a mixture of standard forms. They then show that a generalized gamma mean-field variational family, following desingulumization, can recover the leading order term of the model evidence. The proposed method is a normalizing flow with the generalized gamma as the source distribution."
1713,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks based on the desingularization map. The authors show that the posterior distribution over the parameters of a singular model, following an algebraic-geometrical transformation known as a desingularity map, is asymptotically a mixture of standard forms. They then show that a generalized gamma mean-field variational family, following desingulumization, can recover the leading order term of the model evidence. The proposed method is a normalizing flow with the generalized gamma as the source distribution."
1714,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks based on the desingularization map. The authors show that the posterior distribution over the parameters of a singular model, following an algebraic-geometrical transformation known as a desingularity map, is asymptotically a mixture of standard forms. They then show that a generalized gamma mean-field variational family, following desingulumization, can recover the leading order term of the model evidence. The proposed method is a normalizing flow with the generalized gamma as the source distribution."
1715,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper studies the problem of domain generalization (DG). The authors propose a novel learning-theoretic generalization bound for DG that bounds novel domain generalisation performance in terms of the model’s Rademacher complexity. Based on this bound, the authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. Algorithmically, this analysis suggests that domain regularisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective. Empirical results on the DomainBed benchmark corroborate this theory."
1716,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper studies the problem of domain generalization (DG). The authors propose a novel learning-theoretic generalization bound for DG that bounds novel domain generalisation performance in terms of the model’s Rademacher complexity. Based on this bound, the authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. Algorithmically, this analysis suggests that domain regularisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective. Empirical results on the DomainBed benchmark corroborate this theory."
1717,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper studies the problem of domain generalization (DG). The authors propose a novel learning-theoretic generalization bound for DG that bounds novel domain generalisation performance in terms of the model’s Rademacher complexity. Based on this bound, the authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. Algorithmically, this analysis suggests that domain regularisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective. Empirical results on the DomainBed benchmark corroborate this theory."
1718,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper studies the problem of domain generalization (DG). The authors propose a novel learning-theoretic generalization bound for DG that bounds novel domain generalisation performance in terms of the model’s Rademacher complexity. Based on this bound, the authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. Algorithmically, this analysis suggests that domain regularisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective. Empirical results on the DomainBed benchmark corroborate this theory."
1719,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the maximum n-times coverage problem, where each element must be covered at least n times. This is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors introduce two practical solutions for n-time coverage based on integer linear programming and sequential greedy optimization. They show that the proposed method produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual’s HLA molecules."
1720,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the maximum n-times coverage problem, where each element must be covered at least n times. This is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors introduce two practical solutions for n-time coverage based on integer linear programming and sequential greedy optimization. They show that the proposed method produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual’s HLA molecules."
1721,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the maximum n-times coverage problem, where each element must be covered at least n times. This is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors introduce two practical solutions for n-time coverage based on integer linear programming and sequential greedy optimization. They show that the proposed method produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual’s HLA molecules."
1722,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the maximum n-times coverage problem, where each element must be covered at least n times. This is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors introduce two practical solutions for n-time coverage based on integer linear programming and sequential greedy optimization. They show that the proposed method produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual’s HLA molecules."
1723,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes an initialization method based on the slant asymptote to overcome gradient vanishing in SNN training. The proposed method is based on an approximation of the response curve of spiking neurons. The authors also propose a coding scheme to improve the training speed and the model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed method.
1724,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes an initialization method based on the slant asymptote to overcome gradient vanishing in SNN training. The proposed method is based on an approximation of the response curve of spiking neurons. The authors also propose a coding scheme to improve the training speed and the model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed method.
1725,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes an initialization method based on the slant asymptote to overcome gradient vanishing in SNN training. The proposed method is based on an approximation of the response curve of spiking neurons. The authors also propose a coding scheme to improve the training speed and the model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed method.
1726,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes an initialization method based on the slant asymptote to overcome gradient vanishing in SNN training. The proposed method is based on an approximation of the response curve of spiking neurons. The authors also propose a coding scheme to improve the training speed and the model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed method.
1727,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a method to mitigate the distribution shift in federated learning. The authors propose to use a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments on image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of distribution shift and significantly improve the final model performance."
1728,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a method to mitigate the distribution shift in federated learning. The authors propose to use a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments on image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of distribution shift and significantly improve the final model performance."
1729,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a method to mitigate the distribution shift in federated learning. The authors propose to use a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments on image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of distribution shift and significantly improve the final model performance."
1730,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a method to mitigate the distribution shift in federated learning. The authors propose to use a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments on image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of distribution shift and significantly improve the final model performance."
1731,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a method for pruning deep neural networks (DNs) based on a recently developed spline interpretation of DNs. The authors show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis. They leverage this new insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate the effectiveness of the proposed method."
1732,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a method for pruning deep neural networks (DNs) based on a recently developed spline interpretation of DNs. The authors show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis. They leverage this new insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate the effectiveness of the proposed method."
1733,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a method for pruning deep neural networks (DNs) based on a recently developed spline interpretation of DNs. The authors show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis. They leverage this new insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate the effectiveness of the proposed method."
1734,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a method for pruning deep neural networks (DNs) based on a recently developed spline interpretation of DNs. The authors show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis. They leverage this new insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate the effectiveness of the proposed method."
1735,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper proposes an implicit path alignment algorithm for fair representation learning. Specifically, the authors consider the problem as a bi-level optimization, where the representation is learned in the outer-level, and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost of differentiating, the proposed algorithm only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The authors further analyzed the error gap of the implicit approach and empirically validated the proposed method."
1736,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper proposes an implicit path alignment algorithm for fair representation learning. Specifically, the authors consider the problem as a bi-level optimization, where the representation is learned in the outer-level, and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost of differentiating, the proposed algorithm only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The authors further analyzed the error gap of the implicit approach and empirically validated the proposed method."
1737,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper proposes an implicit path alignment algorithm for fair representation learning. Specifically, the authors consider the problem as a bi-level optimization, where the representation is learned in the outer-level, and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost of differentiating, the proposed algorithm only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The authors further analyzed the error gap of the implicit approach and empirically validated the proposed method."
1738,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper proposes an implicit path alignment algorithm for fair representation learning. Specifically, the authors consider the problem as a bi-level optimization, where the representation is learned in the outer-level, and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost of differentiating, the proposed algorithm only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The authors further analyzed the error gap of the implicit approach and empirically validated the proposed method."
1739,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the importance of different design choices in reinforcement learning via supervised learning (RvS) methods, such as policy architectures and how the conditioning variable is constructed. The authors show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments find that more complex design choices such as large sequence models and value-based weighting schemes used in prior work are often not necessary."
1740,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the importance of different design choices in reinforcement learning via supervised learning (RvS) methods, such as policy architectures and how the conditioning variable is constructed. The authors show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments find that more complex design choices such as large sequence models and value-based weighting schemes used in prior work are often not necessary."
1741,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the importance of different design choices in reinforcement learning via supervised learning (RvS) methods, such as policy architectures and how the conditioning variable is constructed. The authors show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments find that more complex design choices such as large sequence models and value-based weighting schemes used in prior work are often not necessary."
1742,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the importance of different design choices in reinforcement learning via supervised learning (RvS) methods, such as policy architectures and how the conditioning variable is constructed. The authors show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments find that more complex design choices such as large sequence models and value-based weighting schemes used in prior work are often not necessary."
1743,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to model the exploration behavior of humans. The method is based on the idea that humans explore new environments by inferring the structure of unobserved spaces re-using spatial information collected from previously explored spaces. The authors propose a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as the MapInduction framework. They show that their computational framework better predicts human exploration behavior than non-inductive models. They also show that MapInduce, when used to augment state- of-the art approximate planning algorithms, improves their performance."
1744,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to model the exploration behavior of humans. The method is based on the idea that humans explore new environments by inferring the structure of unobserved spaces re-using spatial information collected from previously explored spaces. The authors propose a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as the MapInduction framework. They show that their computational framework better predicts human exploration behavior than non-inductive models. They also show that MapInduce, when used to augment state- of-the art approximate planning algorithms, improves their performance."
1745,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to model the exploration behavior of humans. The method is based on the idea that humans explore new environments by inferring the structure of unobserved spaces re-using spatial information collected from previously explored spaces. The authors propose a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as the MapInduction framework. They show that their computational framework better predicts human exploration behavior than non-inductive models. They also show that MapInduce, when used to augment state- of-the art approximate planning algorithms, improves their performance."
1746,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to model the exploration behavior of humans. The method is based on the idea that humans explore new environments by inferring the structure of unobserved spaces re-using spatial information collected from previously explored spaces. The authors propose a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as the MapInduction framework. They show that their computational framework better predicts human exploration behavior than non-inductive models. They also show that MapInduce, when used to augment state- of-the art approximate planning algorithms, improves their performance."
1747,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The key idea is to replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The proposed method is evaluated on a set of articulated pose tracking tasks and compare performance with learned baselines.
1748,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The key idea is to replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The proposed method is evaluated on a set of articulated pose tracking tasks and compare performance with learned baselines.
1749,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The key idea is to replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The proposed method is evaluated on a set of articulated pose tracking tasks and compare performance with learned baselines.
1750,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The key idea is to replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The proposed method is evaluated on a set of articulated pose tracking tasks and compare performance with learned baselines.
1751,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based model that supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The proposed method is evaluated on unconstrained molecular optimization tasks and outperforms the state-of-the-art methods on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. "
1752,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based model that supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The proposed method is evaluated on unconstrained molecular optimization tasks and outperforms the state-of-the-art methods on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. "
1753,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based model that supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The proposed method is evaluated on unconstrained molecular optimization tasks and outperforms the state-of-the-art methods on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. "
1754,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based model that supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The proposed method is evaluated on unconstrained molecular optimization tasks and outperforms the state-of-the-art methods on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. "
1755,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies imitation learning with deterministic experts. The authors show that imitation learning can be done by reduction to reinforcement learning with a stationary reward. Theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments on continuous control tasks confirm that the reduction works well in practice."
1756,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies imitation learning with deterministic experts. The authors show that imitation learning can be done by reduction to reinforcement learning with a stationary reward. Theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments on continuous control tasks confirm that the reduction works well in practice."
1757,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies imitation learning with deterministic experts. The authors show that imitation learning can be done by reduction to reinforcement learning with a stationary reward. Theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments on continuous control tasks confirm that the reduction works well in practice."
1758,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies imitation learning with deterministic experts. The authors show that imitation learning can be done by reduction to reinforcement learning with a stationary reward. Theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments on continuous control tasks confirm that the reduction works well in practice."
1759,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of over-parameterization of deep neural networks for fairness. The authors provide a theoretical analysis of the effect of reweighting algorithms on the worst-group performance. The main result is that the reweighted model always converges to the same ERM interpolator that fits all training samples, and consequently its worst group test performance will drop to same level as ERM in the long run. Then, the authors analyze whether adding regularization helps fix the issue, and prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
1760,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of over-parameterization of deep neural networks for fairness. The authors provide a theoretical analysis of the effect of reweighting algorithms on the worst-group performance. The main result is that the reweighted model always converges to the same ERM interpolator that fits all training samples, and consequently its worst group test performance will drop to same level as ERM in the long run. Then, the authors analyze whether adding regularization helps fix the issue, and prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
1761,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of over-parameterization of deep neural networks for fairness. The authors provide a theoretical analysis of the effect of reweighting algorithms on the worst-group performance. The main result is that the reweighted model always converges to the same ERM interpolator that fits all training samples, and consequently its worst group test performance will drop to same level as ERM in the long run. Then, the authors analyze whether adding regularization helps fix the issue, and prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
1762,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of over-parameterization of deep neural networks for fairness. The authors provide a theoretical analysis of the effect of reweighting algorithms on the worst-group performance. The main result is that the reweighted model always converges to the same ERM interpolator that fits all training samples, and consequently its worst group test performance will drop to same level as ERM in the long run. Then, the authors analyze whether adding regularization helps fix the issue, and prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
1763,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a new model of human-irrationality, Rational Inattention (RIRL), for multi-agent reinforcement learning (MARL). RIRL models the cost of cognitive information processing using mutual information. The authors show that using this model yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. Additionally, new strategies emerge compared to those under rationality assumptions."
1764,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a new model of human-irrationality, Rational Inattention (RIRL), for multi-agent reinforcement learning (MARL). RIRL models the cost of cognitive information processing using mutual information. The authors show that using this model yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. Additionally, new strategies emerge compared to those under rationality assumptions."
1765,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a new model of human-irrationality, Rational Inattention (RIRL), for multi-agent reinforcement learning (MARL). RIRL models the cost of cognitive information processing using mutual information. The authors show that using this model yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. Additionally, new strategies emerge compared to those under rationality assumptions."
1766,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a new model of human-irrationality, Rational Inattention (RIRL), for multi-agent reinforcement learning (MARL). RIRL models the cost of cognitive information processing using mutual information. The authors show that using this model yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. Additionally, new strategies emerge compared to those under rationality assumptions."
1767,SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a new method for generating imperceptible audio adversarial examples. The key idea is to leverage the phase consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, the authors propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate that the proposed method is able to generate full-sentence adversarial example and achieve a 6.64x generation speed-up over the current state-of-the-art."
1768,SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a new method for generating imperceptible audio adversarial examples. The key idea is to leverage the phase consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, the authors propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate that the proposed method is able to generate full-sentence adversarial example and achieve a 6.64x generation speed-up over the current state-of-the-art."
1769,SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a new method for generating imperceptible audio adversarial examples. The key idea is to leverage the phase consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, the authors propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate that the proposed method is able to generate full-sentence adversarial example and achieve a 6.64x generation speed-up over the current state-of-the-art."
1770,SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a new method for generating imperceptible audio adversarial examples. The key idea is to leverage the phase consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, the authors propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate that the proposed method is able to generate full-sentence adversarial example and achieve a 6.64x generation speed-up over the current state-of-the-art."
1771,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper analyzes the representation learned by BYOL and SimSiam in self-supervised learning. In particular, the authors analyze a generalized version of DirectPred, called DirectSet(α) and show that it provably learns a desirable projection matrix and also reduces the sample complexity on downstream tasks. The authors also propose a simpler and more computationally efficient algorithm, DirectCopy, which can achieve comparable performance with DirectPred."
1772,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper analyzes the representation learned by BYOL and SimSiam in self-supervised learning. In particular, the authors analyze a generalized version of DirectPred, called DirectSet(α) and show that it provably learns a desirable projection matrix and also reduces the sample complexity on downstream tasks. The authors also propose a simpler and more computationally efficient algorithm, DirectCopy, which can achieve comparable performance with DirectPred."
1773,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper analyzes the representation learned by BYOL and SimSiam in self-supervised learning. In particular, the authors analyze a generalized version of DirectPred, called DirectSet(α) and show that it provably learns a desirable projection matrix and also reduces the sample complexity on downstream tasks. The authors also propose a simpler and more computationally efficient algorithm, DirectCopy, which can achieve comparable performance with DirectPred."
1774,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper analyzes the representation learned by BYOL and SimSiam in self-supervised learning. In particular, the authors analyze a generalized version of DirectPred, called DirectSet(α) and show that it provably learns a desirable projection matrix and also reduces the sample complexity on downstream tasks. The authors also propose a simpler and more computationally efficient algorithm, DirectCopy, which can achieve comparable performance with DirectPred."
1775,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. The authors consider a system of multiscale ordinary differential equations, and derive a suitable time-discretization of this system. They also derive rigorous bounds to show the mitigation of exploding and vanishing gradients problem, a wellknown challenge for gradient-based recurrent sequential learning methods. The empirical results demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models."
1776,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. The authors consider a system of multiscale ordinary differential equations, and derive a suitable time-discretization of this system. They also derive rigorous bounds to show the mitigation of exploding and vanishing gradients problem, a wellknown challenge for gradient-based recurrent sequential learning methods. The empirical results demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models."
1777,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. The authors consider a system of multiscale ordinary differential equations, and derive a suitable time-discretization of this system. They also derive rigorous bounds to show the mitigation of exploding and vanishing gradients problem, a wellknown challenge for gradient-based recurrent sequential learning methods. The empirical results demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models."
1778,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. The authors consider a system of multiscale ordinary differential equations, and derive a suitable time-discretization of this system. They also derive rigorous bounds to show the mitigation of exploding and vanishing gradients problem, a wellknown challenge for gradient-based recurrent sequential learning methods. The empirical results demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models."
1779,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a method for learning rotation and permutation-equivariant deep neural networks on small point clouds. The proposed method is based on the geometric algebra, which is used to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of the proposed method on a variety of problems in physics, chemistry, and biology."
1780,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a method for learning rotation and permutation-equivariant deep neural networks on small point clouds. The proposed method is based on the geometric algebra, which is used to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of the proposed method on a variety of problems in physics, chemistry, and biology."
1781,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a method for learning rotation and permutation-equivariant deep neural networks on small point clouds. The proposed method is based on the geometric algebra, which is used to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of the proposed method on a variety of problems in physics, chemistry, and biology."
1782,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a method for learning rotation and permutation-equivariant deep neural networks on small point clouds. The proposed method is based on the geometric algebra, which is used to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of the proposed method on a variety of problems in physics, chemistry, and biology."
1783,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem in milliseconds by formulating a tripartite graph and learning the best assignment policy through the proposed edge-feature-embedded graph attention mechanism. The model is size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality. The online inference time is thousands of times faster than the exact mathematical programming methods."
1784,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem in milliseconds by formulating a tripartite graph and learning the best assignment policy through the proposed edge-feature-embedded graph attention mechanism. The model is size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality. The online inference time is thousands of times faster than the exact mathematical programming methods."
1785,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem in milliseconds by formulating a tripartite graph and learning the best assignment policy through the proposed edge-feature-embedded graph attention mechanism. The model is size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality. The online inference time is thousands of times faster than the exact mathematical programming methods."
1786,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem in milliseconds by formulating a tripartite graph and learning the best assignment policy through the proposed edge-feature-embedded graph attention mechanism. The model is size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality. The online inference time is thousands of times faster than the exact mathematical programming methods."
1787,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a method to infer concepts out of the dialogue context (CODC) in the dialogue summarization task. To evaluate the inference capability of different methods, the authors propose a new evaluation metric based on CODC. Experiments show that the proposed method can provide statistically significant improvements on both CIDEr and traditional automatic evaluation metrics."
1788,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a method to infer concepts out of the dialogue context (CODC) in the dialogue summarization task. To evaluate the inference capability of different methods, the authors propose a new evaluation metric based on CODC. Experiments show that the proposed method can provide statistically significant improvements on both CIDEr and traditional automatic evaluation metrics."
1789,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a method to infer concepts out of the dialogue context (CODC) in the dialogue summarization task. To evaluate the inference capability of different methods, the authors propose a new evaluation metric based on CODC. Experiments show that the proposed method can provide statistically significant improvements on both CIDEr and traditional automatic evaluation metrics."
1790,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of embedding entities into embeddings that capture the semantic dependencies between attributes. The authors consider the case where the embedding of an entity is obtained by pooling the embeds of its known attributes. In particular, they study the theoretical limitations of different embedding strategies, rather than their ability to effectively learn attribute dependencies in practice. They first show a number of negative results, revealing that some of the most popular embedding models are not able to capture even basic Horn rules. However, they also find that some embedding methods are capable, in principle, of modeling both monotonic and non-monotonic attribute dependencies."
1791,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of embedding entities into embeddings that capture the semantic dependencies between attributes. The authors consider the case where the embedding of an entity is obtained by pooling the embeds of its known attributes. In particular, they study the theoretical limitations of different embedding strategies, rather than their ability to effectively learn attribute dependencies in practice. They first show a number of negative results, revealing that some of the most popular embedding models are not able to capture even basic Horn rules. However, they also find that some embedding methods are capable, in principle, of modeling both monotonic and non-monotonic attribute dependencies."
1792,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of embedding entities into embeddings that capture the semantic dependencies between attributes. The authors consider the case where the embedding of an entity is obtained by pooling the embeds of its known attributes. In particular, they study the theoretical limitations of different embedding strategies, rather than their ability to effectively learn attribute dependencies in practice. They first show a number of negative results, revealing that some of the most popular embedding models are not able to capture even basic Horn rules. However, they also find that some embedding methods are capable, in principle, of modeling both monotonic and non-monotonic attribute dependencies."
1793,SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based models on reasoning tasks, including mathematical reasoning, commonsense reasoning, logical reasoning, and logical reasoning. The paper provides a comprehensive survey of the state-of-the-art transformer models on different reasoning tasks. The results show that the transformer models are able to perform well on mathematical reasoning tasks and reasoning tasks on logical reasoning tasks but not on natural language processing tasks. "
1794,SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based models on reasoning tasks, including mathematical reasoning, commonsense reasoning, logical reasoning, and logical reasoning. The paper provides a comprehensive survey of the state-of-the-art transformer models on different reasoning tasks. The results show that the transformer models are able to perform well on mathematical reasoning tasks and reasoning tasks on logical reasoning tasks but not on natural language processing tasks. "
1795,SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based models on reasoning tasks, including mathematical reasoning, commonsense reasoning, logical reasoning, and logical reasoning. The paper provides a comprehensive survey of the state-of-the-art transformer models on different reasoning tasks. The results show that the transformer models are able to perform well on mathematical reasoning tasks and reasoning tasks on logical reasoning tasks but not on natural language processing tasks. "
1796,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper proposes a compositional recursive learner (CRL) framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. The paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. The authors show on a symbolic and a high-dimensional domain that their compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not."
1797,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper proposes a compositional recursive learner (CRL) framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. The paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. The authors show on a symbolic and a high-dimensional domain that their compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not."
1798,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP), a method to reduce the number of activations and weights of neural networks. The idea is to learn the importance of neuron responses and connections, and then combine them with the weight pruning. Experiments are conducted on MNIST, CIFAR-10, and ImageNet to show the effectiveness of the proposed method."
1799,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP), a method to reduce the number of activations and weights of neural networks. The idea is to learn the importance of neuron responses and connections, and then combine them with the weight pruning. Experiments are conducted on MNIST, CIFAR-10, and ImageNet to show the effectiveness of the proposed method."
1800,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a new method for feature selection based on knockoff generation. The proposed method is based on the GAN framework. The key idea is to use a discriminator and a stability network to generate knockoffs, and then use a generator to generate the knockoffs. Experiments show that the proposed method performs as well as the original knockoff model in the Gaussian setting and outperforms the original model in non-Gaussian settings."
1801,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a new method for feature selection based on knockoff generation. The proposed method is based on the GAN framework. The key idea is to use a discriminator and a stability network to generate knockoffs, and then use a generator to generate the knockoffs. Experiments show that the proposed method performs as well as the original knockoff model in the Gaussian setting and outperforms the original model in non-Gaussian settings."
1802,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes a new method for sparse Winograd convolutional neural networks. The main idea is to perform spatial-Winograd pruning in a structured way, which efficiently transfers the spatial-domain sparsity into the winograd domain and avoids Winograddomain retraining. For the next step, the authors also perform pruning and retraining directly in the Winograg domain but propose to use an importance factor matrix to adjust weight importance and weight gradients. This adjustment makes it possible to effectively retrain the pruned network without changing the network structure."
1803,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes a new method for sparse Winograd convolutional neural networks. The main idea is to perform spatial-Winograd pruning in a structured way, which efficiently transfers the spatial-domain sparsity into the winograd domain and avoids Winograddomain retraining. For the next step, the authors also perform pruning and retraining directly in the Winograg domain but propose to use an importance factor matrix to adjust weight importance and weight gradients. This adjustment makes it possible to effectively retrain the pruned network without changing the network structure."
1804,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,"This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed model, Adversarially Learned Mixture Model (AMM), is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available."
1805,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,"This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed model, Adversarially Learned Mixture Model (AMM), is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available."
1806,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes a new estimator for variational inference and maximum likelihood estimation based on the augment-REINFORCE-merge (ARM) algorithm. The main idea is to augment the gradient estimator of the ELBO estimator with the gradient of the estimator in the augmented space, and then merge the two estimators via a common random number. The authors claim that the variance-reduction mechanism of the ARM estimator can be attributed to either antithetic sampling in an augmented space or the use of an optimal anti-symmetric “self-control” baseline function. The experimental results show that the proposed estimator achieves state-of-the-art performance in auto-encoding VAEs and MLE estimation for discrete latent variable models."
1807,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes a new estimator for variational inference and maximum likelihood estimation based on the augment-REINFORCE-merge (ARM) algorithm. The main idea is to augment the gradient estimator of the ELBO estimator with the gradient of the estimator in the augmented space, and then merge the two estimators via a common random number. The authors claim that the variance-reduction mechanism of the ARM estimator can be attributed to either antithetic sampling in an augmented space or the use of an optimal anti-symmetric “self-control” baseline function. The experimental results show that the proposed estimator achieves state-of-the-art performance in auto-encoding VAEs and MLE estimation for discrete latent variable models."
1808,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a modular probabilistic programming language, called MXFusion, which includes a new type of re-usable building blocks, called ""probabilistic modules"". The core idea is to provide a set of random variables with associated probabilistics distributions and dedicated inference methods, and to use the pre-specified inference methods of individual probabilism modules for inference of the whole model. The authors demonstrate the power and convenience of probabilistically modules in the proposed language with various examples of Gaussian process models, which are evaluated with experiments on real data."
1809,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a modular probabilistic programming language, called MXFusion, which includes a new type of re-usable building blocks, called ""probabilistic modules"". The core idea is to provide a set of random variables with associated probabilistics distributions and dedicated inference methods, and to use the pre-specified inference methods of individual probabilism modules for inference of the whole model. The authors demonstrate the power and convenience of probabilistically modules in the proposed language with various examples of Gaussian process models, which are evaluated with experiments on real data."
1810,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method to prune a given network once at initialization prior to training. The method uses a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. After pruning, the sparse network is trained in the standard way. The proposed method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks."
1811,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method to prune a given network once at initialization prior to training. The method uses a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. After pruning, the sparse network is trained in the standard way. The proposed method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks."
1812,SP:986b9781534ffec84619872cd269ad48d235f869,This paper studies the behavior of beam search in sequence synthesis. The authors find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. They propose two methods to constrain the search and show that constrained beam search effectively eliminates the problem and in some cases even leads to higher evaluation scores. The results generalize and improve upon previous observations on copies and training set predictions.
1813,SP:986b9781534ffec84619872cd269ad48d235f869,This paper studies the behavior of beam search in sequence synthesis. The authors find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. They propose two methods to constrain the search and show that constrained beam search effectively eliminates the problem and in some cases even leads to higher evaluation scores. The results generalize and improve upon previous observations on copies and training set predictions.
1814,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes a method to improve the sample efficiency when we have access to demonstrations. The proposed method, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment’s fixed initial state, the proposed method starts the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. The experiments show that Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game."
1815,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes a method to improve the sample efficiency when we have access to demonstrations. The proposed method, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment’s fixed initial state, the proposed method starts the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. The experiments show that Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game."
1816,SP:426c98718b2dbad640380ec4ccb2b656958389bc,This paper proposes a multi-layer pruning method (MLPrune) that can automatically decide appropriate compression ratios for all layers. The pruning criterion is based on a Kroneckerfactored Approximate Curvature method. The experiments show that the proposed method can compress AlexNet and VGG16 by 25x without loss in accuracy on ImageNet.
1817,SP:426c98718b2dbad640380ec4ccb2b656958389bc,This paper proposes a multi-layer pruning method (MLPrune) that can automatically decide appropriate compression ratios for all layers. The pruning criterion is based on a Kroneckerfactored Approximate Curvature method. The experiments show that the proposed method can compress AlexNet and VGG16 by 25x without loss in accuracy on ImageNet.
1818,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper proposes a method to visualize and understand GANs at the unit-, object-, and scene-level. The authors first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, the authors quantify the causal effect of these units by measuring the ability of interventions to control objects in the output. They examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. They provide open source interpretation tools to help researchers and practitioners better understand their GAN models."
1819,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper proposes a method to visualize and understand GANs at the unit-, object-, and scene-level. The authors first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, the authors quantify the causal effect of these units by measuring the ability of interventions to control objects in the output. They examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. They provide open source interpretation tools to help researchers and practitioners better understand their GAN models."
1820,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper studies the relationship between parameter and function distances in neural networks. The authors first show that the parameter-function distance is related to the function-L distance. Then, they propose a new learning rule that constrains the distance a network can travel through L-space in any one update. Finally, they demonstrate how the L distance could be applied directly to optimization."
1821,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper studies the relationship between parameter and function distances in neural networks. The authors first show that the parameter-function distance is related to the function-L distance. Then, they propose a new learning rule that constrains the distance a network can travel through L-space in any one update. Finally, they demonstrate how the L distance could be applied directly to optimization."
1822,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a deep neural network framework for modeling biological systems. The model is trained as a deep generative Markov model whose next state is a probability distribution based on the current state. This allows the model to be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. The proposed model is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems."
1823,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a deep neural network framework for modeling biological systems. The model is trained as a deep generative Markov model whose next state is a probability distribution based on the current state. This allows the model to be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. The proposed model is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems."
1824,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a generative adversarial network (GAN) for unsupervised classification. The proposed method is based on the graph Laplacian (GL) and the spectral clustering theory on the dimension reduced spaces. In particular, the proposed GAN consists of an approximate linear connector network C analogous to the cerebral cortex, between the discriminator D and the generator G. The network is used to estimate the unknown number of classes and the proposed method can also classify the images by using the estimated number."
1825,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a generative adversarial network (GAN) for unsupervised classification. The proposed method is based on the graph Laplacian (GL) and the spectral clustering theory on the dimension reduced spaces. In particular, the proposed GAN consists of an approximate linear connector network C analogous to the cerebral cortex, between the discriminator D and the generator G. The network is used to estimate the unknown number of classes and the proposed method can also classify the images by using the estimated number."
1826,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning permutation-invariant representations of sets. The method is based on the idea of permutation optimisation, which is to learn how to permute a set end-to-end. The permuted set can then be further processed to learn a permutation invariant representation of that set, avoiding a bottleneck in traditional set models. The proposed method is evaluated on four datasets and achieves state-of-the-art results."
1827,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning permutation-invariant representations of sets. The method is based on the idea of permutation optimisation, which is to learn how to permute a set end-to-end. The permuted set can then be further processed to learn a permutation invariant representation of that set, avoiding a bottleneck in traditional set models. The proposed method is evaluated on four datasets and achieves state-of-the-art results."
1828,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes a method for learning nonlinear embeddings for few-shot classification tasks. The method is based on the idea of projective subspace networks (PSNs), which are trained on a set of samples from a given class. The authors show that the proposed method outperforms baselines on supervised and semi-supervised few shot classification tasks, and that it is end-to-end learning."
1829,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes a method for learning nonlinear embeddings for few-shot classification tasks. The method is based on the idea of projective subspace networks (PSNs), which are trained on a set of samples from a given class. The authors show that the proposed method outperforms baselines on supervised and semi-supervised few shot classification tasks, and that it is end-to-end learning."
1830,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), this method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
1831,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), this method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
1832,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper proposes a new privacy-preserving representation learning framework where the utility provider and the user can decide what characteristics of the data they want to share (utility) and what they don't want to keep private (secret). The utility provider can use the same algorithm on original and sanitized data, a critical and novel attribute to help service providers accommodate varying privacy requirements with a single set of utility algorithms. The paper first analyzes the space of privacy preserving representations and derive natural information theoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of the original data X. Then, the paper proposes explicit learning architectures to learn privacy preserving representation that approach this bound in a data-driven fashion. The authors illustrate this framework through the implementation of three use cases; subject-with-insubject, where we tackle the problem of having a face identity detector that works only on a consenting subset of users, an important application, for mobile devices activated by face recognition; gender-and-subject, where privacy preserving facial verification is preserved while hiding the gender attribute for users who choose to do so; and emotion and gender, which is the case of hiding gender while preserving emotion detection."
1833,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper proposes a new privacy-preserving representation learning framework where the utility provider and the user can decide what characteristics of the data they want to share (utility) and what they don't want to keep private (secret). The utility provider can use the same algorithm on original and sanitized data, a critical and novel attribute to help service providers accommodate varying privacy requirements with a single set of utility algorithms. The paper first analyzes the space of privacy preserving representations and derive natural information theoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of the original data X. Then, the paper proposes explicit learning architectures to learn privacy preserving representation that approach this bound in a data-driven fashion. The authors illustrate this framework through the implementation of three use cases; subject-with-insubject, where we tackle the problem of having a face identity detector that works only on a consenting subset of users, an important application, for mobile devices activated by face recognition; gender-and-subject, where privacy preserving facial verification is preserved while hiding the gender attribute for users who choose to do so; and emotion and gender, which is the case of hiding gender while preserving emotion detection."
1834,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation method for GANs. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. Experiments on MNIST, Fashion-MNIST, CIFAR10, CELEBA demonstrate the effectiveness of the proposed method."
1835,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation method for GANs. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. Experiments on MNIST, Fashion-MNIST, CIFAR10, CELEBA demonstrate the effectiveness of the proposed method."
1836,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a method for studying the neural activity of primary visual cortex (V1) using a rotation-equivariant convolutional neural network (RNN). The RNN is trained to be rotation equivariant, which means that it is able to learn features at multiple different orientations. The authors show that the RNN outperforms a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many V1 neurons and are pooled sparsely to predict neural activity. "
1837,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a method for studying the neural activity of primary visual cortex (V1) using a rotation-equivariant convolutional neural network (RNN). The RNN is trained to be rotation equivariant, which means that it is able to learn features at multiple different orientations. The authors show that the RNN outperforms a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many V1 neurons and are pooled sparsely to predict neural activity. "
1838,SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper proposes a distributed training method for deep neural networks. The main idea is to use majority vote to aggregate the gradients of all workers in the network. The idea is that the majority vote is robust to adversarial attacks. The authors show that the proposed method converges in the large and mini-batch settings, establishing convergence for a parameter regime of ADAM as a byproduct. On the practical side, the authors built their distributed training system in Pytorch."
1839,SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper proposes a distributed training method for deep neural networks. The main idea is to use majority vote to aggregate the gradients of all workers in the network. The idea is that the majority vote is robust to adversarial attacks. The authors show that the proposed method converges in the large and mini-batch settings, establishing convergence for a parameter regime of ADAM as a byproduct. On the practical side, the authors built their distributed training system in Pytorch."
1840,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a GAN-based approach to generate realistic and high-fidelity stock market data based on generative adversarial networks. The authors model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. They test their approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data."
1841,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a GAN-based approach to generate realistic and high-fidelity stock market data based on generative adversarial networks. The authors model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. They test their approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data."
1842,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper proposes an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. The core ideas of the proposed method include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that the proposed algorithm can achieve higher expected rewards, and converge faster than existing baselines."
1843,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper proposes an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. The core ideas of the proposed method include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that the proposed algorithm can achieve higher expected rewards, and converge faster than existing baselines."
1844,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network structure (depth and width) on halting time and shows that larger models—wider models in particular—take fewer training steps to converge. The authors design simple experiments to quantitatively characterize the effects of overparametrization on weight space traversal. Results show that halting time improves when growing model’s width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a Power-law, and gradient vectors become more aligned with each other."
1845,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network structure (depth and width) on halting time and shows that larger models—wider models in particular—take fewer training steps to converge. The authors design simple experiments to quantitatively characterize the effects of overparametrization on weight space traversal. Results show that halting time improves when growing model’s width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a Power-law, and gradient vectors become more aligned with each other."
1846,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper proposes a method for learning optimal algorithms for online optimization problems. The method is based on the idea of adversarial distributions, which are distributions that encourage the learner to find algorithms that work well in the worst case. The proposed method is evaluated on the AdWords problem, the online knapsack problem, and the secretary problem. The results indicate that the proposed method has learned behaviors that are consistent with the optimal algorithms derived using the online primal-dual framework."
1847,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper proposes a method for learning optimal algorithms for online optimization problems. The method is based on the idea of adversarial distributions, which are distributions that encourage the learner to find algorithms that work well in the worst case. The proposed method is evaluated on the AdWords problem, the online knapsack problem, and the secretary problem. The results indicate that the proposed method has learned behaviors that are consistent with the optimal algorithms derived using the online primal-dual framework."
1848,SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper studies the effect of removing domain-specific components from deep reinforcement learning (RL) algorithms. Specifically, the authors propose to replace the fixed components with adaptive solutions from the literature. They show that the performance sometimes decreases with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive component performed better. They then investigate the main benefit of having fewer domain specific components, by comparing the learning performance of two systems on a different set of continuous control problems, without additional tuning of either system."
1849,SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper studies the effect of removing domain-specific components from deep reinforcement learning (RL) algorithms. Specifically, the authors propose to replace the fixed components with adaptive solutions from the literature. They show that the performance sometimes decreases with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive component performed better. They then investigate the main benefit of having fewer domain specific components, by comparing the learning performance of two systems on a different set of continuous control problems, without additional tuning of either system."
1850,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for vision-and-language navigation (VLN). The proposed method consists of two components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. Experiments are conducted on a standard benchmark and analyze the contributions of the primary components."
1851,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for vision-and-language navigation (VLN). The proposed method consists of two components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. Experiments are conducted on a standard benchmark and analyze the contributions of the primary components."
1852,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,This paper proposes two techniques to improve the performance of neural program synthesis from input-output examples. The first is execution-guided synthesis and synthesizer ensemble. The second is execution guided synthesis and synthesis ensemble. These techniques are general enough to be combined with any existing encoder-decoder-style neural program synthesizer. The experiments on the Karel dataset show that the proposed techniques can boost the accuracy from 77% to more than 90%.
1853,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,This paper proposes two techniques to improve the performance of neural program synthesis from input-output examples. The first is execution-guided synthesis and synthesizer ensemble. The second is execution guided synthesis and synthesis ensemble. These techniques are general enough to be combined with any existing encoder-decoder-style neural program synthesizer. The experiments on the Karel dataset show that the proposed techniques can boost the accuracy from 77% to more than 90%.
1854,SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the stability, convergence, and acceleration properties of batch normalization (BN) applied to a simplified model: ordinary least squares (OLS). The authors show that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, acceleration effects, and insensitivity to the choice of learning rates. The authors also demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems."
1855,SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the stability, convergence, and acceleration properties of batch normalization (BN) applied to a simplified model: ordinary least squares (OLS). The authors show that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, acceleration effects, and insensitivity to the choice of learning rates. The authors also demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems."
1856,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"This paper studies the problem of data noising in recurrent neural network language models. The authors show that each variant of data noise is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). Based on this insight, the authors propose a more principled method to apply at prediction time and propose natural extensions to data noise under the variational framework. In particular, they propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothhing method. They empirically verify their analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noise methods."
1857,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"This paper studies the problem of data noising in recurrent neural network language models. The authors show that each variant of data noise is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). Based on this insight, the authors propose a more principled method to apply at prediction time and propose natural extensions to data noise under the variational framework. In particular, they propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothhing method. They empirically verify their analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noise methods."
1858,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"This paper proposes a classifier-agnostic saliency map extraction method for weakly supervised localization. The proposed method is based on the idea that any classifier could use all parts of the image that it could use, not just one given in advance. The method is evaluated on the ImageNet dataset and shows that the proposed method outperforms existing weakly-supervised localization methods."
1859,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"This paper proposes a classifier-agnostic saliency map extraction method for weakly supervised localization. The proposed method is based on the idea that any classifier could use all parts of the image that it could use, not just one given in advance. The method is evaluated on the ImageNet dataset and shows that the proposed method outperforms existing weakly-supervised localization methods."
1860,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a knowledge flow method to transfer knowledge from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces. Upon training with knowledge flow the student is independent from the teachers. The method is evaluated on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other ‘knowledge exchange’ methods."
1861,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a knowledge flow method to transfer knowledge from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces. Upon training with knowledge flow the student is independent from the teachers. The method is evaluated on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other ‘knowledge exchange’ methods."
1862,SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes a method for learning a compact dynamical model for high-dimensional data. The proposed method is based on soft-clustering of the data and learn its dynamics to produce a compact model while still ensuring the original objectives of causal inference and accurate predictions. The main idea is to maximize the compression of the state variables such that the predictive ability and causal interdependence constraints between the original data streams and the compact model are closely bounded. The authors provide theoretical guarantees concerning the convergence of the proposed learning algorithm. They also provide an iterative scheme for updating the new model parameters. They demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems and apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity.
1863,SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes a method for learning a compact dynamical model for high-dimensional data. The proposed method is based on soft-clustering of the data and learn its dynamics to produce a compact model while still ensuring the original objectives of causal inference and accurate predictions. The main idea is to maximize the compression of the state variables such that the predictive ability and causal interdependence constraints between the original data streams and the compact model are closely bounded. The authors provide theoretical guarantees concerning the convergence of the proposed learning algorithm. They also provide an iterative scheme for updating the new model parameters. They demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems and apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity.
1864,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in “one shot”. The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of generated samples."
1865,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in “one shot”. The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of generated samples."
1866,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes an approximation strategy that significantly reduces a network’s memory footprint during training, but has negligible effect on training performance and computational expense. During the forward pass, the authors replace activations with lower-precision approximations immediately after they have been used by subsequent layers, thus freeing up memory. The approximate activations are then used during the backward pass. This approach limits the accumulation of errors across the forward and backward pass—because the forward computation across the network still happens at full precision, and the approximation has a limited effect when computing gradients to a layer's input."
1867,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes an approximation strategy that significantly reduces a network’s memory footprint during training, but has negligible effect on training performance and computational expense. During the forward pass, the authors replace activations with lower-precision approximations immediately after they have been used by subsequent layers, thus freeing up memory. The approximate activations are then used during the backward pass. This approach limits the accumulation of errors across the forward and backward pass—because the forward computation across the network still happens at full precision, and the approximation has a limited effect when computing gradients to a layer's input."
1868,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a novel method for face completion. The proposed method is based on the idea of learning a fully end-to-end framework that trains a generative adversarial network progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. The key idea is to encourage the network to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse to fine way. The experimental results show that the proposed method can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024×1024 resolution."
1869,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a novel method for face completion. The proposed method is based on the idea of learning a fully end-to-end framework that trains a generative adversarial network progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. The key idea is to encourage the network to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse to fine way. The experimental results show that the proposed method can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024×1024 resolution."
1870,SP:a300122021e93d695af85e158f2b402d21525bc8,"This paper studies the effect of reducing the precision of accumulators for partial sums in deep learning training. The authors derive a set of equations that relate the variance to the length of accumulation and the minimum number of bits needed for accumulation. They show that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums. They also show that reducing accumulation precision further degrades the quality of the trained network, proving that their equations produce tight bounds."
1871,SP:a300122021e93d695af85e158f2b402d21525bc8,"This paper studies the effect of reducing the precision of accumulators for partial sums in deep learning training. The authors derive a set of equations that relate the variance to the length of accumulation and the minimum number of bits needed for accumulation. They show that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums. They also show that reducing accumulation precision further degrades the quality of the trained network, proving that their equations produce tight bounds."
1872,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic weight matrix alignment properties of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. The authors show that for strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized ith weight matrix asymPTotically equals its rank-1 approximation uiv i ; (iii) these rank- 1 matrices are aligned across layers, meaning |v> i+1ui| → 1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network — the product of its weight matrices — converges in the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon."
1873,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic weight matrix alignment properties of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. The authors show that for strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized ith weight matrix asymPTotically equals its rank-1 approximation uiv i ; (iii) these rank- 1 matrices are aligned across layers, meaning |v> i+1ui| → 1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network — the product of its weight matrices — converges in the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon."
1874,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper proposes a method to extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN, consists of a persona based HRED generator (PHRED) and a conditional discriminator (PhredGANd). The proposed method is evaluated on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends."
1875,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper proposes a method to extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN, consists of a persona based HRED generator (PHRED) and a conditional discriminator (PhredGANd). The proposed method is evaluated on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends."
1876,SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a collaborative image-drawing game between two agents, called CoDraw. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art. The two players communicate via two-way communication using natural language. The authors collect the CoDraw dataset of ∼10K dialogs consisting of ∼138K messages exchanged between human agents. They define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition which pairs agents trained independently on disjoint subsets of the training data for evaluation."
1877,SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a collaborative image-drawing game between two agents, called CoDraw. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art. The two players communicate via two-way communication using natural language. The authors collect the CoDraw dataset of ∼10K dialogs consisting of ∼138K messages exchanged between human agents. They define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition which pairs agents trained independently on disjoint subsets of the training data for evaluation."
1878,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learn neural random fields (NRFs) with inclusive-divergence minimized auxiliary generator. The proposed approach is based on the theoretical analysis on exploiting gradient information in model sampling. The authors show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of their knowledge, represent the best-performed random fields in these tasks."
1879,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learn neural random fields (NRFs) with inclusive-divergence minimized auxiliary generator. The proposed approach is based on the theoretical analysis on exploiting gradient information in model sampling. The authors show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of their knowledge, represent the best-performed random fields in these tasks."
1880,SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper proposes a meta-gradient attack method for training time attacks on graph neural networks for node classification that perturb the discrete graph structure. The main idea is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. The experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings."
1881,SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper proposes a meta-gradient attack method for training time attacks on graph neural networks for node classification that perturb the discrete graph structure. The main idea is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. The experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings."
1882,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model based on the Cramer-Wold kernel. The proposed method is inspired by the Sliced-Wasserstein Autoencoders (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). The main contribution of this paper is to introduce a new cost function based on a characteristic kernel, called CramerWold Kernel, which has a simple closed-form in the case of normal prior. The authors show that the proposed method outperforms SWAE and MMD in terms of both quantitatively and qualitatively."
1883,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model based on the Cramer-Wold kernel. The proposed method is inspired by the Sliced-Wasserstein Autoencoders (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). The main contribution of this paper is to introduce a new cost function based on a characteristic kernel, called CramerWold Kernel, which has a simple closed-form in the case of normal prior. The authors show that the proposed method outperforms SWAE and MMD in terms of both quantitatively and qualitatively."
1884,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper studies the problem of many-class few-shot (MCFS) classification. The authors propose a memory-augmented hierarchical-classification network (MahiNet) for MCFS learning. The main idea is to explore the class hierarchy by exploring the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain. MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a CNN-based attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes."
1885,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper studies the problem of many-class few-shot (MCFS) classification. The authors propose a memory-augmented hierarchical-classification network (MahiNet) for MCFS learning. The main idea is to explore the class hierarchy by exploring the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain. MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a CNN-based attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes."
1886,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes a neural speed reading model that can skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and another capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word. The experimental evaluation shows that the proposed model achieves the best overall floating point operations (FLOP) reduction (hence is faster) while keeping the same accuracy or even improving it compared to a vanilla lSTM that reads the whole text."
1887,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes a neural speed reading model that can skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and another capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word. The experimental evaluation shows that the proposed model achieves the best overall floating point operations (FLOP) reduction (hence is faster) while keeping the same accuracy or even improving it compared to a vanilla lSTM that reads the whole text."
1888,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a nonlinear radial basis convolutional feature transformation by learning the Mahalanobis distance function. The proposed method is evaluated on three publicly available image classification and segmentation data-sets namely, MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14."
1889,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a nonlinear radial basis convolutional feature transformation by learning the Mahalanobis distance function. The proposed method is evaluated on three publicly available image classification and segmentation data-sets namely, MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14."
1890,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a novel policy exploration strategy called Neural Adaptive Dropout Policy Exploration (NADPEx) for deep reinforcement learning agents. NADPEx is based on the idea that dropout is a global random variable for conditional distribution, which can be incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee the policy’s stable improvement. Experiments are conducted on Mujoco tasks with sparse reward while naive exploration and parameter noise fail."
1891,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a novel policy exploration strategy called Neural Adaptive Dropout Policy Exploration (NADPEx) for deep reinforcement learning agents. NADPEx is based on the idea that dropout is a global random variable for conditional distribution, which can be incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee the policy’s stable improvement. Experiments are conducted on Mujoco tasks with sparse reward while naive exploration and parameter noise fail."
1892,SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes a new metric, the nonlinearity coefficient (NLC), which is computed in the network’s randomly initialized state. The authors argue that the NLC is an important tool for architecture search and design, as it can robustly predict poor training outcomes before training even begins. "
1893,SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes a new metric, the nonlinearity coefficient (NLC), which is computed in the network’s randomly initialized state. The authors argue that the NLC is an important tool for architecture search and design, as it can robustly predict poor training outcomes before training even begins. "
1894,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in classifiers. The authors show that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification, a previously unreported form of bias that can be traced back to the features of a trained model. The paper proposes two novel feature selection algorithms for mitigating bias amplification. The experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy."
1895,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in classifiers. The authors show that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification, a previously unreported form of bias that can be traced back to the features of a trained model. The paper proposes two novel feature selection algorithms for mitigating bias amplification. The experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy."
1896,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the effect of over-parametrization on the generalization of neural networks. The authors first show that for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, and as a result, increasing the overparameter improves the normalized margin and generalization error bounds for deep networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees. Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization."
1897,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the effect of over-parametrization on the generalization of neural networks. The authors first show that for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, and as a result, increasing the overparameter improves the normalized margin and generalization error bounds for deep networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees. Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization."
1898,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,This paper proposes a method to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. The experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations.
1899,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,This paper proposes a method to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. The experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations.
1900,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a method for learning representations that can be used to infer the dynamics of a system in the future. The main idea is to learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. The authors evaluate their approach on a suite of robotics tasks, including a manipulation task on a real Sawyer robotic arm directly from camera images."
1901,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a method for learning representations that can be used to infer the dynamics of a system in the future. The main idea is to learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. The authors evaluate their approach on a suite of robotics tasks, including a manipulation task on a real Sawyer robotic arm directly from camera images."
1902,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,This paper proposes a method for counterfactually guided policy search (CF-GPS) that leverages structural causal models for policy evaluation of arbitrary policies on individual off-policy episodes. The authors argue that the proposed method can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. The proposed method is evaluated on a grid-world task.
1903,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,This paper proposes a method for counterfactually guided policy search (CF-GPS) that leverages structural causal models for policy evaluation of arbitrary policies on individual off-policy episodes. The authors argue that the proposed method can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. The proposed method is evaluated on a grid-world task.
1904,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"This paper studies the relationship between adversarial robustness and the decision surface in parameter space. The authors show that the geometry of decision surfaces in input space correlates well with the robustness. Based on this observation, the authors propose a robustness indicator, which can evaluate a neural network’s intrinsic robustness property without testing its accuracy under adversarial attacks. The paper also proposes a robust training method."
1905,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"This paper studies the relationship between adversarial robustness and the decision surface in parameter space. The authors show that the geometry of decision surfaces in input space correlates well with the robustness. Based on this observation, the authors propose a robustness indicator, which can evaluate a neural network’s intrinsic robustness property without testing its accuracy under adversarial attacks. The paper also proposes a robust training method."
1906,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. The authors integrate the quantitative DNN energy estimation into the training process to assist the constrained optimization. They prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, their framework trains DNNs that provide higher accuracies under same or lower energy budgets."
1907,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. The authors integrate the quantitative DNN energy estimation into the training process to assist the constrained optimization. They prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, their framework trains DNNs that provide higher accuracies under same or lower energy budgets."
1908,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a Bayesian Policy Optimization algorithm to address model uncertainty in Markov Decision Processes (MDPs). The main idea is to learn a belief distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. To address challenges from discretizing the continuous latent parameter space, the authors propose a new policy network architecture that encodes the belief distribution independently from the observable state. The proposed method significantly outperforms existing algorithms that explicitly reason about belief distributions and is competitive with state-of-the-art Partially Observable MDPs."
1909,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a Bayesian Policy Optimization algorithm to address model uncertainty in Markov Decision Processes (MDPs). The main idea is to learn a belief distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. To address challenges from discretizing the continuous latent parameter space, the authors propose a new policy network architecture that encodes the belief distribution independently from the observable state. The proposed method significantly outperforms existing algorithms that explicitly reason about belief distributions and is competitive with state-of-the-art Partially Observable MDPs."
1910,SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for building unsupervised representations of entities and their compositions, by viewing each entity as a histogram (or distribution) over its contexts. The proposed method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. The key tools at the core of this framework are Wasserstein distances and Wasserstein barycenters, hence raising the question from our title. Empirical results show strong advantages gained through the proposed framework."
1911,SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for building unsupervised representations of entities and their compositions, by viewing each entity as a histogram (or distribution) over its contexts. The proposed method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. The key tools at the core of this framework are Wasserstein distances and Wasserstein barycenters, hence raising the question from our title. Empirical results show strong advantages gained through the proposed framework."
1912,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper investigates the relationship between model-based reinforcement learning and long-range planning in MuJoCo environments. The authors propose to use a dynamics model that directly predicts distant states, based on current state and a long sequence of actions, and thus is able to yield more accurate state estimates. These accurate predictions allow the authors to uncover the relation between model accuracy and performance, and translate to higher task reward that matches or exceeds current state-of-the-art model-free approaches."
1913,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper investigates the relationship between model-based reinforcement learning and long-range planning in MuJoCo environments. The authors propose to use a dynamics model that directly predicts distant states, based on current state and a long sequence of actions, and thus is able to yield more accurate state estimates. These accurate predictions allow the authors to uncover the relation between model accuracy and performance, and translate to higher task reward that matches or exceeds current state-of-the-art model-free approaches."
1914,SP:da14205470819495a3aad69d64de4033749d4d3e,This paper proposes a precision highway method for neural network quantization. The proposed method is based on the idea that the information flow between the input and output of the network can be reduced to a low-precision information flow. The precision highway is applied to both convolutional and recurrent neural networks. Experiments show that the proposed method outperforms the best existing quantization methods while offering no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss.
1915,SP:da14205470819495a3aad69d64de4033749d4d3e,This paper proposes a precision highway method for neural network quantization. The proposed method is based on the idea that the information flow between the input and output of the network can be reduced to a low-precision information flow. The precision highway is applied to both convolutional and recurrent neural networks. Experiments show that the proposed method outperforms the best existing quantization methods while offering no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss.
1916,SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a generative adversarial network (GAN) based method for image restoration. The main idea is to use a GAN to estimate the posterior probability of latent variables, and then to use the estimated posterior probability to generate an image that is the same as the original image. The method is evaluated on denoising, deblurring, super-resolution and inpainting."
1917,SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a generative adversarial network (GAN) based method for image restoration. The main idea is to use a GAN to estimate the posterior probability of latent variables, and then to use the estimated posterior probability to generate an image that is the same as the original image. The method is evaluated on denoising, deblurring, super-resolution and inpainting."
1918,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a novel method for knowledge distillation from few samples. The authors assume that both ""teacher"" and ""student"" have the same feature map sizes at each corresponding block, and add a 1×1 conv-layer at the end of each block in the student-net, and align the block-level outputs between ""teachers"" and student by estimating the parameters of the added layer with limited samples. Experiments verify that the proposed method is very efficient and effective."
1919,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a novel method for knowledge distillation from few samples. The authors assume that both ""teacher"" and ""student"" have the same feature map sizes at each corresponding block, and add a 1×1 conv-layer at the end of each block in the student-net, and align the block-level outputs between ""teachers"" and student by estimating the parameters of the added layer with limited samples. Experiments verify that the proposed method is very efficient and effective."
1920,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a new metric, H-score, to evaluate the performance of transferred representations from one task to another in classification problems. The proposed metric is based on a principled information theoretic approach, and it has a direct connection to the asymptotic error probability of the decision function based on the transferred feature. This formulation of transferability can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments on both synthetic and real image data show that the proposed metric can be meaningful in practice, and can generalize to inference problems beyond classification, such as recognition tasks for 3D indoor-scene understanding."
1921,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a new metric, H-score, to evaluate the performance of transferred representations from one task to another in classification problems. The proposed metric is based on a principled information theoretic approach, and it has a direct connection to the asymptotic error probability of the decision function based on the transferred feature. This formulation of transferability can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments on both synthetic and real image data show that the proposed metric can be meaningful in practice, and can generalize to inference problems beyond classification, such as recognition tasks for 3D indoor-scene understanding."
1922,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes a method to control the global sentence structure ahead of translation. The authors propose to learn discrete structural representations to encode syntactic information of target sentences. During translation, they can either let beam search to choose the structural codes automatically or specify the codes manually. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations."
1923,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes a method to control the global sentence structure ahead of translation. The authors propose to learn discrete structural representations to encode syntactic information of target sentences. During translation, they can either let beam search to choose the structural codes automatically or specify the codes manually. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations."
1924,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,This paper proposes a new generative adversarial network (GAN) loss function called Relativistic GANs (RGANs) that aims to decrease the probability that half of the data in the mini-batch is fake. The main idea is to use a discriminator that estimates the probability of the real data being more realistic than a randomly sampled fake data. The authors show that this property can be induced by using a “relativist discriminator” which estimate the probability for the given real data is more realistic. They also propose a variant of RGANs where the discriminator is trained to estimate the average probability that the real samples are more realistic compared to the fake samples. Experiments show that the proposed RGAN and RaGANs are more stable and generate higher quality data samples than their non-relativism counterparts.
1925,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,This paper proposes a new generative adversarial network (GAN) loss function called Relativistic GANs (RGANs) that aims to decrease the probability that half of the data in the mini-batch is fake. The main idea is to use a discriminator that estimates the probability of the real data being more realistic than a randomly sampled fake data. The authors show that this property can be induced by using a “relativist discriminator” which estimate the probability for the given real data is more realistic. They also propose a variant of RGANs where the discriminator is trained to estimate the average probability that the real samples are more realistic compared to the fake samples. Experiments show that the proposed RGAN and RaGANs are more stable and generate higher quality data samples than their non-relativism counterparts.
1926,SP:8df1599919dcb3329553e75ffb19059f192542ea,"This paper proposes Parameter Generation and Model Adaptation (PGMA) for continual learning. The proposed method learns to build a model, called the solver, with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the model to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach."
1927,SP:8df1599919dcb3329553e75ffb19059f192542ea,"This paper proposes Parameter Generation and Model Adaptation (PGMA) for continual learning. The proposed method learns to build a model, called the solver, with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the model to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach."
1928,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper proposes Relational Forward Models (RFM) for multi-agent learning. RFM is a model that learns to predict the future behavior of agents in a multi agent environment. The authors show that RFM can be used as an intermediate representation of the agent’s behavior, which is useful for understanding the dynamics of the environment. They also show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines."
1929,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper proposes Relational Forward Models (RFM) for multi-agent learning. RFM is a model that learns to predict the future behavior of agents in a multi agent environment. The authors show that RFM can be used as an intermediate representation of the agent’s behavior, which is useful for understanding the dynamics of the environment. They also show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines."
1930,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL) that uses demonstrations from other tasks to constrain the set of possible reward functions by learning a “prior” that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. The main idea is to learn a prior that can be used to learn the reward function from a limited set of demonstrations, and then use this prior as a reward function for RL. The authors demonstrate that their method can efficiently recover rewards from images for novel tasks and provide intuition as to how their approach is analogous to learning a prior."
1931,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL) that uses demonstrations from other tasks to constrain the set of possible reward functions by learning a “prior” that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. The main idea is to learn a prior that can be used to learn the reward function from a limited set of demonstrations, and then use this prior as a reward function for RL. The authors demonstrate that their method can efficiently recover rewards from images for novel tasks and provide intuition as to how their approach is analogous to learning a prior."
1932,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a general framework for meta-learning approximate Probabilistic Inference for Prediction (ML-PIP) that extends existing probabilistic interpretations of meta learning to cover a broad class of methods. Specifically, the authors propose an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. The proposed method replaces optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training."
1933,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a general framework for meta-learning approximate Probabilistic Inference for Prediction (ML-PIP) that extends existing probabilistic interpretations of meta learning to cover a broad class of methods. Specifically, the authors propose an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. The proposed method replaces optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training."
1934,SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper presents a novel deep learning architecture for classifying structured objects in ultrafine-grained datasets, where classes may not be clearly distinguishable by their appearance but rather by their context. The proposed method models sequences of images as linear-chain CRFs, and jointly learn the parameters from both localvisual features and neighboring class information. The visual features are learned by convolutional layers, whereas class-structure information is reparametrized by factorizing the CRF pairwise potential matrix. This form a context-based semantic similarity space, learned alongside the visual similarities, and dramatically increases the learning capacity of contextual information. This model overcomes the difficulties of existing CRF methods to learn the contextual relationships thoroughly when there is a large number of classes and the data is sparse."
1935,SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper presents a novel deep learning architecture for classifying structured objects in ultrafine-grained datasets, where classes may not be clearly distinguishable by their appearance but rather by their context. The proposed method models sequences of images as linear-chain CRFs, and jointly learn the parameters from both localvisual features and neighboring class information. The visual features are learned by convolutional layers, whereas class-structure information is reparametrized by factorizing the CRF pairwise potential matrix. This form a context-based semantic similarity space, learned alongside the visual similarities, and dramatically increases the learning capacity of contextual information. This model overcomes the difficulties of existing CRF methods to learn the contextual relationships thoroughly when there is a large number of classes and the data is sparse."
1936,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) to generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. The authors demonstrate that GANs can outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts."
1937,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) to generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. The authors demonstrate that GANs can outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts."
1938,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper proposes a method for verifying the robustness of neural networks to perturbations with bounded l_\infty norm. The authors formulate verification of piecewise-linear neural networks as a mixed integer program, which is two to three orders of magnitude quicker than the state-of-the-art. They achieve this by tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows them to verify properties on convolutional and residual networks with over 100,000 ReLUs."
1939,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper proposes a method for verifying the robustness of neural networks to perturbations with bounded l_\infty norm. The authors formulate verification of piecewise-linear neural networks as a mixed integer program, which is two to three orders of magnitude quicker than the state-of-the-art. They achieve this by tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows them to verify properties on convolutional and residual networks with over 100,000 ReLUs."
1940,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper proposes a method for learning a default policy that can be used to speed up and regularize learning. The method is based on the KL regularized expected reward objective and introduces an additional component, the default policy. The main idea is to restrict the amount of information the policy receives, forcing it to learn reusable behaviours that help the policy learn faster. The paper also discusses connections to information bottleneck approaches and to the variational EM algorithm. Experiments are conducted on both discrete and continuous action domains and demonstrate that the proposed method can significantly speed up learning."
1941,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper proposes a method for learning a default policy that can be used to speed up and regularize learning. The method is based on the KL regularized expected reward objective and introduces an additional component, the default policy. The main idea is to restrict the amount of information the policy receives, forcing it to learn reusable behaviours that help the policy learn faster. The paper also discusses connections to information bottleneck approaches and to the variational EM algorithm. Experiments are conducted on both discrete and continuous action domains and demonstrate that the proposed method can significantly speed up learning."
1942,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a new architecture for conversational machine comprehension, FLOWQA, which is based on an alternating parallel processing (AP) mechanism. The main idea is to incorporate intermediate representations generated during the process of answering previous questions, such as previous question/answer pairs, the document context, and the current question. The FLOW model is evaluated on CoQA and QuAC, where it outperforms the state-of-the-art models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy."
1943,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a new architecture for conversational machine comprehension, FLOWQA, which is based on an alternating parallel processing (AP) mechanism. The main idea is to incorporate intermediate representations generated during the process of answering previous questions, such as previous question/answer pairs, the document context, and the current question. The FLOW model is evaluated on CoQA and QuAC, where it outperforms the state-of-the-art models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy."
1944,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper proposes a generative model for Python code. The model is trained on a large dataset consisting of millions of fine-grained edits from thousands of Python developers. The authors propose a new composition of attentional and pointer network components to improve the performance and scalability of the model. The experimental results show that the proposed model is able to predict future edits.
1945,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper proposes a generative model for Python code. The model is trained on a large dataset consisting of millions of fine-grained edits from thousands of Python developers. The authors propose a new composition of attentional and pointer network components to improve the performance and scalability of the model. The experimental results show that the proposed model is able to predict future edits.
1946,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning method for multi-class classification that leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta-classification learning, optimizes a binary classifier for pairwise similarities prediction and through this process learns a multi class classifier as a submodule. The authors formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. Experiments are conducted on supervised, unsupervised cross-task, and semi-supervised settings."
1947,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning method for multi-class classification that leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta-classification learning, optimizes a binary classifier for pairwise similarities prediction and through this process learns a multi class classifier as a submodule. The authors formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. Experiments are conducted on supervised, unsupervised cross-task, and semi-supervised settings."
1948,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the problem of visual question answering in the context of psycholinguistic question answering. The authors compare the performance of FiLM, a deep neural network that is trained to answer visual question questions, with that of a human model trained on the same question. The main contribution of the paper is that the authors show that the FiLM model does not perform as well as the human model when the question is difficult to answer, as predicted by Weber’s law. They also show that this is due to confounding factors, such as spatial arrangement of the scene, which impede the effectiveness of the system."
1949,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the problem of visual question answering in the context of psycholinguistic question answering. The authors compare the performance of FiLM, a deep neural network that is trained to answer visual question questions, with that of a human model trained on the same question. The main contribution of the paper is that the authors show that the FiLM model does not perform as well as the human model when the question is difficult to answer, as predicted by Weber’s law. They also show that this is due to confounding factors, such as spatial arrangement of the scene, which impede the effectiveness of the system."
1950,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic extension of the DistMult and ComplEx embedding models for link prediction in relational knowledge graphs. The authors argue that knowledge graphs should be treated within a Bayesian framework because even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. To this end, the authors propose a variational inference to estimate a lower bound on the marginal likelihood of the data. They find that the main benefit of the Bayesian approach is that it allows for efficient, gradient based optimization over hyperparameters, which would lead to divergences in a non-Bayesian treatment."
1951,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic extension of the DistMult and ComplEx embedding models for link prediction in relational knowledge graphs. The authors argue that knowledge graphs should be treated within a Bayesian framework because even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. To this end, the authors propose a variational inference to estimate a lower bound on the marginal likelihood of the data. They find that the main benefit of the Bayesian approach is that it allows for efficient, gradient based optimization over hyperparameters, which would lead to divergences in a non-Bayesian treatment."
1952,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"This paper proposes two online learning algorithms for supervised dimension reduction. The first algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in. The second algorithm, IOSIR, uses an overlapping technique and develop an incremental overlapping SIR (IOSIR) algorithm. The authors verify the effectiveness and efficiency of both algorithms by simulations and real data applications."
1953,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"This paper proposes two online learning algorithms for supervised dimension reduction. The first algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in. The second algorithm, IOSIR, uses an overlapping technique and develop an incremental overlapping SIR (IOSIR) algorithm. The authors verify the effectiveness and efficiency of both algorithms by simulations and real data applications."
1954,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"This paper proposes a generative-discriminative model for multi-modal representation learning. The proposed model is based on a combination of two factors: multimodal discriminative and modality-specific generative factors. Multimodal features are shared across all modalities, while modality specific features are unique for each modality and contain the information required for generating data. The model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance."
1955,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"This paper proposes a generative-discriminative model for multi-modal representation learning. The proposed model is based on a combination of two factors: multimodal discriminative and modality-specific generative factors. Multimodal features are shared across all modalities, while modality specific features are unique for each modality and contain the information required for generating data. The model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance."
1956,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper presents a meta-learning approach for adaptive text-to-speech (TTS) with few data. The main idea is to learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the waveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker encoder with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-Speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity."
1957,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper presents a meta-learning approach for adaptive text-to-speech (TTS) with few data. The main idea is to learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the waveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker encoder with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-Speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity."
1958,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper studies the problem of robust estimation under Huber’s contamination model. In particular, the authors show that f-GANs and various depth functions can all be viewed as variational lower bounds of the total variation distance in the framework of f-Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. The authors show in both theory and experiments that some appropriate structures of discriminator networks with hidden layers in GAN leads to statistically optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist."
1959,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper studies the problem of robust estimation under Huber’s contamination model. In particular, the authors show that f-GANs and various depth functions can all be viewed as variational lower bounds of the total variation distance in the framework of f-Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. The authors show in both theory and experiments that some appropriate structures of discriminator networks with hidden layers in GAN leads to statistically optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist."
1960,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes a method to represent a scene via a symbolic program for its objects, attributes, and their relations. The authors propose a model that infers such scene programs by exploiting a hierarchical, object-based scene representation. Experiments demonstrate that the proposed method works well on synthetic data and transfers to real images with such compositional structure."
1961,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes a method to represent a scene via a symbolic program for its objects, attributes, and their relations. The authors propose a model that infers such scene programs by exploiting a hierarchical, object-based scene representation. Experiments demonstrate that the proposed method works well on synthetic data and transfers to real images with such compositional structure."
1962,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a time-gated LSTM RNN model, called the Gaussian-Gated L-LSTM. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. Because the time gate limits the updates of the neuron state, the number of computes needed for the network update is also reduced. By adding a computational budget term to the training loss, the authors obtain a network which further reduces the computational budget by at least 10x. Finally, a temporal curriculum learning schedule is proposed to speed up the convergence time of the equivalent L-STM on long sequences."
1963,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a time-gated LSTM RNN model, called the Gaussian-Gated L-LSTM. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. Because the time gate limits the updates of the neuron state, the number of computes needed for the network update is also reduced. By adding a computational budget term to the training loss, the authors obtain a network which further reduces the computational budget by at least 10x. Finally, a temporal curriculum learning schedule is proposed to speed up the convergence time of the equivalent L-STM on long sequences."
1964,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a plug-and-play method for out-of-distribution detection. The proposed method is based on the observation that the mean and standard deviation within feature maps differ greatly between in- and out of distribution samples. Based on this observation, the authors propose to use a simple ensembling of first and second order deep feature statistics to effectively differentiate in-sample and out-sample samples. Experiments are conducted on CIFAR-100 and TinyImageNet."
1965,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a plug-and-play method for out-of-distribution detection. The proposed method is based on the observation that the mean and standard deviation within feature maps differ greatly between in- and out of distribution samples. Based on this observation, the authors propose to use a simple ensembling of first and second order deep feature statistics to effectively differentiate in-sample and out-sample samples. Experiments are conducted on CIFAR-100 and TinyImageNet."
1966,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper studies the problem of model recovery for one-hidden-layer fully-connected neural networks with sigmoid activations. The authors prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point without requiring a fresh set of samples at each iteration."
1967,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper studies the problem of model recovery for one-hidden-layer fully-connected neural networks with sigmoid activations. The authors prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point without requiring a fresh set of samples at each iteration."
1968,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS is trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. Experiments show that FBS can provide 5x and 2x savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss."
1969,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS is trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. Experiments show that FBS can provide 5x and 2x savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss."
1970,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the training objective of GANs from the mixed Nash Equilibria (NE) perspective. In particular, the authors propose a novel algorithm for proximal optimization of the two-player game, where the goal is to maximize the mutual information between the two players in the game. The authors prove the convergence of the proposed algorithm to the mixed NE, and propose a principled procedure to reduce the prox methods to simple sampling routines, leading to practically efficient algorithms. The experimental results show that the proposed method outperforms SGD, Adam, and RMSProp in speed and quality."
1971,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the training objective of GANs from the mixed Nash Equilibria (NE) perspective. In particular, the authors propose a novel algorithm for proximal optimization of the two-player game, where the goal is to maximize the mutual information between the two players in the game. The authors prove the convergence of the proposed algorithm to the mixed NE, and propose a principled procedure to reduce the prox methods to simple sampling routines, leading to practically efficient algorithms. The experimental results show that the proposed method outperforms SGD, Adam, and RMSProp in speed and quality."
1972,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method for parameter-efficient transfer and multitask learning with deep neural networks. The basic idea is to learn a model patch a small set of parameters that will specialize to each task, instead of finetuning the last layer or the entire network. The authors show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly."
1973,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method for parameter-efficient transfer and multitask learning with deep neural networks. The basic idea is to learn a model patch a small set of parameters that will specialize to each task, instead of finetuning the last layer or the entire network. The authors show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly."
1974,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method for multi-modal data. The proposed method is based on batch normalization (BN) and extends the normalization to more than a single mean and variance. The main idea is to detect modes of data on-the-fly, jointly normalizing samples that share common features. The experimental results show that the proposed method outperforms BN and other widely used normalization techniques in several experiments."
1975,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method for multi-modal data. The proposed method is based on batch normalization (BN) and extends the normalization to more than a single mean and variance. The main idea is to detect modes of data on-the-fly, jointly normalizing samples that share common features. The experimental results show that the proposed method outperforms BN and other widely used normalization techniques in several experiments."
1976,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper studies the lottery ticket hypothesis, which claims that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets have won the initialization lottery: their connections have initial weights that make training particularly effective."
1977,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper studies the lottery ticket hypothesis, which claims that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets have won the initialization lottery: their connections have initial weights that make training particularly effective."
1978,SP:08c662296c7cf346f027e462d29184275fd6a102,This paper proposes a method for learning a state representation for Atari games. The state representation is learned using an attentive dynamics model (ADM). The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The method is evaluated on a set of Atari games and achieves state-of-the-art results.
1979,SP:08c662296c7cf346f027e462d29184275fd6a102,This paper proposes a method for learning a state representation for Atari games. The state representation is learned using an attentive dynamics model (ADM). The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The method is evaluated on a set of Atari games and achieves state-of-the-art results.
1980,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes a generative network that learns to generate all the weight parameters of deep neural networks. The method is inspired by generative adversarial networks (GANs) and is based on minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. The authors evaluate the likelihood of samples with a classification loss on MNIST and CIFAR-10 datasets and show that HyperGAN can learn to generate parameters which solve the MNIST/CIFAR datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. "
1981,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes a generative network that learns to generate all the weight parameters of deep neural networks. The method is inspired by generative adversarial networks (GANs) and is based on minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. The authors evaluate the likelihood of samples with a classification loss on MNIST and CIFAR-10 datasets and show that HyperGAN can learn to generate parameters which solve the MNIST/CIFAR datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. "
1982,SP:230b3e008e687e03a8b914084b93fc81609051c0,This paper proposes a differentiable estimator for the evidence lower bound (ELBO) of variational auto-encoders (VAEs). The main idea is to use importance sampling to estimate the ELBO. The authors evaluate the proposed estimator on Bernoulli and Categorically distributed latent representations on two datasets.
1983,SP:230b3e008e687e03a8b914084b93fc81609051c0,This paper proposes a differentiable estimator for the evidence lower bound (ELBO) of variational auto-encoders (VAEs). The main idea is to use importance sampling to estimate the ELBO. The authors evaluate the proposed estimator on Bernoulli and Categorically distributed latent representations on two datasets.
1984,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes a method to train a feed forward neural network with increased robustness against adversarial attacks compared to conventional training approaches. This is achieved using a novel pre-trained building block based on a mean field description of a Boltzmann machine. On the MNIST dataset the method achieves strong adversarial resistance without data augmentation or adversarial training.
1985,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes a method to train a feed forward neural network with increased robustness against adversarial attacks compared to conventional training approaches. This is achieved using a novel pre-trained building block based on a mean field description of a Boltzmann machine. On the MNIST dataset the method achieves strong adversarial resistance without data augmentation or adversarial training.
1986,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,This paper studies the effect of changes in the location and size of the visible region of the minimal image on human recognition accuracy. The authors show that such changes are much more prominent in DNNs and are independent from previous works that have reported lack of invariance to minor modifications in object location. They also show that this phenomenon is independent from human error and can also affect humans to a much lesser degree.
1987,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,This paper studies the effect of changes in the location and size of the visible region of the minimal image on human recognition accuracy. The authors show that such changes are much more prominent in DNNs and are independent from previous works that have reported lack of invariance to minor modifications in object location. They also show that this phenomenon is independent from human error and can also affect humans to a much lesser degree.
1988,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper proposes a method for multi-agent reinforcement learning (MARL) where each agent has its own preferences, intentions, skills, and can not be dictated to perform tasks they do not want to do. To achieve optimal coordination among these agents, the authors train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together."
1989,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper proposes a method for multi-agent reinforcement learning (MARL) where each agent has its own preferences, intentions, skills, and can not be dictated to perform tasks they do not want to do. To achieve optimal coordination among these agents, the authors train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together."
1990,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a new RNN architecture for time series. The proposed architecture is based on the idea of sparse and dense features, where sparse features are related to the frequency of the time series, while dense features relate to the time between specified events in the sequence and are used to modify the cell’s memory state. The authors also include two types of static (whole sequence level) features, one related to time and one not, which are combined with the encoder output. The experiments show that the proposed model does increase performance compared to standard cells."
1991,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a new RNN architecture for time series. The proposed architecture is based on the idea of sparse and dense features, where sparse features are related to the frequency of the time series, while dense features relate to the time between specified events in the sequence and are used to modify the cell’s memory state. The authors also include two types of static (whole sequence level) features, one related to time and one not, which are combined with the encoder output. The experiments show that the proposed model does increase performance compared to standard cells."
1992,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a neural model that tries to answer the question whether a propositional formula is always true. The model consists of a feedforward neural network and two recurrent neural networks. The feedforward network is a top-down neural network that takes a given formula and a property as input and outputs the answer to the question of whether the formula has the given property. The results of this network are then processed by two recurrent networks. One of the interesting aspects of this model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
1993,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a neural model that tries to answer the question whether a propositional formula is always true. The model consists of a feedforward neural network and two recurrent neural networks. The feedforward network is a top-down neural network that takes a given formula and a property as input and outputs the answer to the question of whether the formula has the given property. The results of this network are then processed by two recurrent networks. One of the interesting aspects of this model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
1994,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper proposes a curriculum learning-based method for sampling mini-batches for training deep neural networks. The method is based on the idea that the difficulty of a training image should be defined using transfer learning from some competitive “teacher” network trained on the Imagenet database. The authors then propose a bootstrap method to evaluate the difficulty using the same network without relying on a ”teacher.” The experiments show that the proposed method can improve the learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CifAR-100 datasets."
1995,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper proposes a curriculum learning-based method for sampling mini-batches for training deep neural networks. The method is based on the idea that the difficulty of a training image should be defined using transfer learning from some competitive “teacher” network trained on the Imagenet database. The authors then propose a bootstrap method to evaluate the difficulty using the same network without relying on a ”teacher.” The experiments show that the proposed method can improve the learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CifAR-100 datasets."
1996,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to provide generalization guarantees for deterministic and uncompressed neural networks. The main contribution of the paper is to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves generalize to the interaction between the matrices on test data, thereby implying a wide test loss minimum. The authors then apply their general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data)."
1997,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to provide generalization guarantees for deterministic and uncompressed neural networks. The main contribution of the paper is to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves generalize to the interaction between the matrices on test data, thereby implying a wide test loss minimum. The authors then apply their general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data)."
1998,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes a method for training discrete latent variable models for machine translation. In particular, the authors propose to use Expectation Maximization (EM) to train a discrete autoencoder (VQ-VAE) with a sequence-level knowledge distillation method. The authors show that the proposed method is able to outperform a greedy autoregressive baseline Transformer on CIFAR-10, while being 3.3 times faster at inference."
1999,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes a method for training discrete latent variable models for machine translation. In particular, the authors propose to use Expectation Maximization (EM) to train a discrete autoencoder (VQ-VAE) with a sequence-level knowledge distillation method. The authors show that the proposed method is able to outperform a greedy autoregressive baseline Transformer on CIFAR-10, while being 3.3 times faster at inference."
2000,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes two multi-view learning methods for learning sentence representations in an unsupervised fashion. The first method uses a generative objective and the other one uses a discriminative one. In both methods, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN) and the second view encode it with a simple linear model. The authors show that, after learning, the vectors produced by the learned representations provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view."
2001,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes two multi-view learning methods for learning sentence representations in an unsupervised fashion. The first method uses a generative objective and the other one uses a discriminative one. In both methods, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN) and the second view encode it with a simple linear model. The authors show that, after learning, the vectors produced by the learned representations provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view."
2002,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes an online distributed optimization method called Anytime Minibatch. In this method, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper presents a convergence analysis and analyzes the wall time performance. The numerical results show that the proposed method is up to 1.5 times faster in Amazon EC2 and it is more efficient when there is greater variability in compute node performance."
2003,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes an online distributed optimization method called Anytime Minibatch. In this method, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper presents a convergence analysis and analyzes the wall time performance. The numerical results show that the proposed method is up to 1.5 times faster in Amazon EC2 and it is more efficient when there is greater variability in compute node performance."
2004,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. The authors argue that such intrinsic reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. They test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.
2005,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. The authors argue that such intrinsic reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. They test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.
2006,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper studies the problem of underfitting in Neural Processes (NPs). The authors show that NPs suffer from underfitting at the inputs of the observed data they condition on. To address this issue, the authors propose to incorporate attention into NPs, allowing each input location to attend to the relevant context points for the prediction. They show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled."
2007,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper studies the problem of underfitting in Neural Processes (NPs). The authors show that NPs suffer from underfitting at the inputs of the observed data they condition on. To address this issue, the authors propose to incorporate attention into NPs, allowing each input location to attend to the relevant context points for the prediction. They show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled."
2008,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,This paper studies the problem of credit assignment in meta-reinforcement learning (meta-RL). The authors propose a new algorithm for meta-learning based on gradient-based meta-RL. The main contribution of the paper is a theoretical analysis of the credit assignment of meta-policy gradients to the pre-adaptation behavior. The authors then propose a method to control the statistical distance between the preadaptation and adapted policies. The proposed method is evaluated on a variety of tasks and compared to a number of baselines.
2009,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,This paper studies the problem of credit assignment in meta-reinforcement learning (meta-RL). The authors propose a new algorithm for meta-learning based on gradient-based meta-RL. The main contribution of the paper is a theoretical analysis of the credit assignment of meta-policy gradients to the pre-adaptation behavior. The authors then propose a method to control the statistical distance between the preadaptation and adapted policies. The proposed method is evaluated on a variety of tasks and compared to a number of baselines.
2010,SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes a message passing neural network (MNN) to solve SAT problems after only being trained as a classifier to predict satisfiability. The proposed method, NeuroSAT, generalizes to novel distributions, and at test time it can solve SAT tasks encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs."
2011,SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes a message passing neural network (MNN) to solve SAT problems after only being trained as a classifier to predict satisfiability. The proposed method, NeuroSAT, generalizes to novel distributions, and at test time it can solve SAT tasks encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs."
2012,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method for imitation learning for autonomous driving. The main idea is to augment the imitation loss with additional losses that penalize undesirable events and encourage progress. The authors show that the proposed method can handle complex situations in simulation, and demonstrate the model driving a car in the real world."
2013,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method for imitation learning for autonomous driving. The main idea is to augment the imitation loss with additional losses that penalize undesirable events and encourage progress. The authors show that the proposed method can handle complex situations in simulation, and demonstrate the model driving a car in the real world."
2014,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method to select a subset of training data to achieve faster training with no loss in model predictive performance. The idea is to first train a small proxy model quickly, then use to estimate the utility of individual training data points, and then select the most informative ones for training the large target model. Extensive experiments show that the proposed method leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data."
2015,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method to select a subset of training data to achieve faster training with no loss in model predictive performance. The idea is to first train a small proxy model quickly, then use to estimate the utility of individual training data points, and then select the most informative ones for training the large target model. Extensive experiments show that the proposed method leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data."
2016,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper proposes a new method for planning in continuous control problems. The main idea is to view the planning problem as a probabilistic inference problem over future optimal trajectories. The authors propose to use Sequential Monte Carlo and Bayesian smoothing in the context of control as inference. This allows them to use sampling methods, and thus, tackle planning in the continuous domains using a fixed computational budget. The proposed method can capture multimodal policies and can quickly learn continuous control tasks."
2017,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper proposes a new method for planning in continuous control problems. The main idea is to view the planning problem as a probabilistic inference problem over future optimal trajectories. The authors propose to use Sequential Monte Carlo and Bayesian smoothing in the context of control as inference. This allows them to use sampling methods, and thus, tackle planning in the continuous domains using a fixed computational budget. The proposed method can capture multimodal policies and can quickly learn continuous control tasks."
2018,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper studies the problem of adversarial robustness in the context of Bayesian classifiers. The authors show that the robustness of the Bayes classifier is sensitive to the input data distribution, and that the clean accuracy and robust accuracy of the adversarially trained model are different. They show empirically that standardly trained models achieve comparable clean accuracies on CIFAR-10 and MNIST datasets, but the robust accuracies of the models trained with adversarial perturbations are significantly different. This counter-intuitive phenomenon indicates that input data distributions alone can affect the adversarial performance of trained neural networks, not necessarily the tasks themselves."
2019,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper studies the problem of adversarial robustness in the context of Bayesian classifiers. The authors show that the robustness of the Bayes classifier is sensitive to the input data distribution, and that the clean accuracy and robust accuracy of the adversarially trained model are different. They show empirically that standardly trained models achieve comparable clean accuracies on CIFAR-10 and MNIST datasets, but the robust accuracies of the models trained with adversarial perturbations are significantly different. This counter-intuitive phenomenon indicates that input data distributions alone can affect the adversarial performance of trained neural networks, not necessarily the tasks themselves."
2020,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique for batch normalization. The proposed method is based on a transformation of layer weights instead of layer outputs. The authors claim that the proposed method keeps the contribution of positive and negative weights to the layer output in equilibrium. Experiments are conducted on CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet."
2021,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique for batch normalization. The proposed method is based on a transformation of layer weights instead of layer outputs. The authors claim that the proposed method keeps the contribution of positive and negative weights to the layer output in equilibrium. Experiments are conducted on CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet."
2022,SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper proposes a novel method to identify mislabeled examples on the fly and discard them during training. The idea is to use the implicit regularization effect of stochastic gradient descent with large learning rates. The proposed method is based on the loss statistics of SGD. The authors show that the proposed method can identify the examples that are most likely to be mislabelled. Then, they discard the examples on fly and continue training with the rest of the examples. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeling examples."
2023,SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper proposes a novel method to identify mislabeled examples on the fly and discard them during training. The idea is to use the implicit regularization effect of stochastic gradient descent with large learning rates. The proposed method is based on the loss statistics of SGD. The authors show that the proposed method can identify the examples that are most likely to be mislabelled. Then, they discard the examples on fly and continue training with the rest of the examples. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeling examples."
2024,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper provides theoretical analysis of latent variable generative models (HMs) for pretraining and downstream tasks. The generative model is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. The authors show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, prompt tuning obtains downstream guarantees with weaker nondegeneracies conditions, and the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long term memory. Experiments on synthetically generated data from HMMs corroborate the theoretical findings."
2025,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper provides theoretical analysis of latent variable generative models (HMs) for pretraining and downstream tasks. The generative model is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. The authors show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, prompt tuning obtains downstream guarantees with weaker nondegeneracies conditions, and the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long term memory. Experiments on synthetically generated data from HMMs corroborate the theoretical findings."
2026,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper provides theoretical analysis of latent variable generative models (HMs) for pretraining and downstream tasks. The generative model is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. The authors show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, prompt tuning obtains downstream guarantees with weaker nondegeneracies conditions, and the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long term memory. Experiments on synthetically generated data from HMMs corroborate the theoretical findings."
2027,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper provides theoretical analysis of latent variable generative models (HMs) for pretraining and downstream tasks. The generative model is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. The authors show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, prompt tuning obtains downstream guarantees with weaker nondegeneracies conditions, and the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long term memory. Experiments on synthetically generated data from HMMs corroborate the theoretical findings."
2028,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization in the domain generalization setting. In particular, the authors propose a new metric to measure the transferability between source and target domains. The metric is based on the difference between total variation and Wasserstein distance. The authors also propose a transferable feature embedding learning algorithm to evaluate the quality of the feature embeddings learned by existing algorithms. The experimental results show that the proposed algorithm achieves consistent improvement over many state-of the-art algorithms."
2029,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization in the domain generalization setting. In particular, the authors propose a new metric to measure the transferability between source and target domains. The metric is based on the difference between total variation and Wasserstein distance. The authors also propose a transferable feature embedding learning algorithm to evaluate the quality of the feature embeddings learned by existing algorithms. The experimental results show that the proposed algorithm achieves consistent improvement over many state-of the-art algorithms."
2030,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization in the domain generalization setting. In particular, the authors propose a new metric to measure the transferability between source and target domains. The metric is based on the difference between total variation and Wasserstein distance. The authors also propose a transferable feature embedding learning algorithm to evaluate the quality of the feature embeddings learned by existing algorithms. The experimental results show that the proposed algorithm achieves consistent improvement over many state-of the-art algorithms."
2031,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization in the domain generalization setting. In particular, the authors propose a new metric to measure the transferability between source and target domains. The metric is based on the difference between total variation and Wasserstein distance. The authors also propose a transferable feature embedding learning algorithm to evaluate the quality of the feature embeddings learned by existing algorithms. The experimental results show that the proposed algorithm achieves consistent improvement over many state-of the-art algorithms."
2032,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) an ordering over trajectories. They prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. They then provide a polynomial-time algorithms that construct a Markov-based reward function that can optimize tasks of each of these three types. They conclude with an empirical study that corroborates and illustrates their theoretical findings."
2033,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) an ordering over trajectories. They prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. They then provide a polynomial-time algorithms that construct a Markov-based reward function that can optimize tasks of each of these three types. They conclude with an empirical study that corroborates and illustrates their theoretical findings."
2034,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) an ordering over trajectories. They prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. They then provide a polynomial-time algorithms that construct a Markov-based reward function that can optimize tasks of each of these three types. They conclude with an empirical study that corroborates and illustrates their theoretical findings."
2035,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) an ordering over trajectories. They prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. They then provide a polynomial-time algorithms that construct a Markov-based reward function that can optimize tasks of each of these three types. They conclude with an empirical study that corroborates and illustrates their theoretical findings."
2036,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning. The authors argue that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. Specifically, the authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, they recast the generalization problem as solving the induced partially observed Markov decision process, which they call the epistemic pOMDP, and suggest a simple ensemble-based technique for approximately solving the partially observed problem."
2037,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning. The authors argue that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. Specifically, the authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, they recast the generalization problem as solving the induced partially observed Markov decision process, which they call the epistemic pOMDP, and suggest a simple ensemble-based technique for approximately solving the partially observed problem."
2038,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning. The authors argue that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. Specifically, the authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, they recast the generalization problem as solving the induced partially observed Markov decision process, which they call the epistemic pOMDP, and suggest a simple ensemble-based technique for approximately solving the partially observed problem."
2039,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning. The authors argue that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. Specifically, the authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, they recast the generalization problem as solving the induced partially observed Markov decision process, which they call the epistemic pOMDP, and suggest a simple ensemble-based technique for approximately solving the partially observed problem."
2040,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a method for estimating the Hessian matrix of value functions in meta-reinforcement learning. The main idea is to use off-policy evaluation to estimate the higher-order derivatives of the value functions. The paper provides a theoretical analysis of the bias and variance trade-off of Hessian estimates and proposes a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
2041,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a method for estimating the Hessian matrix of value functions in meta-reinforcement learning. The main idea is to use off-policy evaluation to estimate the higher-order derivatives of the value functions. The paper provides a theoretical analysis of the bias and variance trade-off of Hessian estimates and proposes a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
2042,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a method for estimating the Hessian matrix of value functions in meta-reinforcement learning. The main idea is to use off-policy evaluation to estimate the higher-order derivatives of the value functions. The paper provides a theoretical analysis of the bias and variance trade-off of Hessian estimates and proposes a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
2043,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a method for estimating the Hessian matrix of value functions in meta-reinforcement learning. The main idea is to use off-policy evaluation to estimate the higher-order derivatives of the value functions. The paper provides a theoretical analysis of the bias and variance trade-off of Hessian estimates and proposes a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
2044,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper proposes a new algorithm for distributed learning with a central server. The proposed algorithm is based on the idea of bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. The main difference is that the downlink compression only impacts local models, while the global model is preserved. To ensure it, MCM additionally combines model compression with a memory mechanism. "
2045,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper proposes a new algorithm for distributed learning with a central server. The proposed algorithm is based on the idea of bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. The main difference is that the downlink compression only impacts local models, while the global model is preserved. To ensure it, MCM additionally combines model compression with a memory mechanism. "
2046,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper proposes a new algorithm for distributed learning with a central server. The proposed algorithm is based on the idea of bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. The main difference is that the downlink compression only impacts local models, while the global model is preserved. To ensure it, MCM additionally combines model compression with a memory mechanism. "
2047,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper proposes a new algorithm for distributed learning with a central server. The proposed algorithm is based on the idea of bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. The main difference is that the downlink compression only impacts local models, while the global model is preserved. To ensure it, MCM additionally combines model compression with a memory mechanism. "
2048,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of “counterfactual invariance”, which is a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. In particular, the authors show that both the means and implications of counterfactually invariance depend fundamentally on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. The authors also provide practical schemes for learning (approximately) invariant predictors (without access to counterfactual examples)."
2049,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of “counterfactual invariance”, which is a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. In particular, the authors show that both the means and implications of counterfactually invariance depend fundamentally on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. The authors also provide practical schemes for learning (approximately) invariant predictors (without access to counterfactual examples)."
2050,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of “counterfactual invariance”, which is a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. In particular, the authors show that both the means and implications of counterfactually invariance depend fundamentally on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. The authors also provide practical schemes for learning (approximately) invariant predictors (without access to counterfactual examples)."
2051,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of “counterfactual invariance”, which is a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. In particular, the authors show that both the means and implications of counterfactually invariance depend fundamentally on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. The authors also provide practical schemes for learning (approximately) invariant predictors (without access to counterfactual examples)."
2052,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a new training strategy for GANs with limited data. The idea is to augment the real data distribution with generated images, which deceives the discriminator adaptively. The authors provide a theoretical analysis to examine the convergence and rationality of the proposed training strategy. The experimental results show that the proposed method is effective in improving the quality of the generated images."
2053,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a new training strategy for GANs with limited data. The idea is to augment the real data distribution with generated images, which deceives the discriminator adaptively. The authors provide a theoretical analysis to examine the convergence and rationality of the proposed training strategy. The experimental results show that the proposed method is effective in improving the quality of the generated images."
2054,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a new training strategy for GANs with limited data. The idea is to augment the real data distribution with generated images, which deceives the discriminator adaptively. The authors provide a theoretical analysis to examine the convergence and rationality of the proposed training strategy. The experimental results show that the proposed method is effective in improving the quality of the generated images."
2055,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a new training strategy for GANs with limited data. The idea is to augment the real data distribution with generated images, which deceives the discriminator adaptively. The authors provide a theoretical analysis to examine the convergence and rationality of the proposed training strategy. The experimental results show that the proposed method is effective in improving the quality of the generated images."
2056,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,This paper proposes a causal inference framework for multivariate point processes. The proposed framework is based on the idea of average treatment effect (ATE) and propensity scores. Theoretical analysis is provided to justify the proposed framework. Experiments are conducted on synthetic and real-world event datasets to demonstrate the effectiveness of the proposed method.
2057,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,This paper proposes a causal inference framework for multivariate point processes. The proposed framework is based on the idea of average treatment effect (ATE) and propensity scores. Theoretical analysis is provided to justify the proposed framework. Experiments are conducted on synthetic and real-world event datasets to demonstrate the effectiveness of the proposed method.
2058,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,This paper proposes a causal inference framework for multivariate point processes. The proposed framework is based on the idea of average treatment effect (ATE) and propensity scores. Theoretical analysis is provided to justify the proposed framework. Experiments are conducted on synthetic and real-world event datasets to demonstrate the effectiveness of the proposed method.
2059,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,This paper proposes a causal inference framework for multivariate point processes. The proposed framework is based on the idea of average treatment effect (ATE) and propensity scores. Theoretical analysis is provided to justify the proposed framework. Experiments are conducted on synthetic and real-world event datasets to demonstrate the effectiveness of the proposed method.
2060,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper studies the problem of adversarial attacks on GNNs. The authors propose a novel GNN with Adaptive residual, AirGNN, which aims to improve the robustness of the GNN against adversarial attack. The main contribution of the paper is to analyze the residual connections in the message passing of GNN and find that residual connections can amplify the vulnerability against abnormal node features. Based on this observation, the authors propose and derive a simple, efficient, interpretable, and adaptive message passing scheme, leading to a new GNN, called AIRGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm."
2061,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper studies the problem of adversarial attacks on GNNs. The authors propose a novel GNN with Adaptive residual, AirGNN, which aims to improve the robustness of the GNN against adversarial attack. The main contribution of the paper is to analyze the residual connections in the message passing of GNN and find that residual connections can amplify the vulnerability against abnormal node features. Based on this observation, the authors propose and derive a simple, efficient, interpretable, and adaptive message passing scheme, leading to a new GNN, called AIRGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm."
2062,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper studies the problem of adversarial attacks on GNNs. The authors propose a novel GNN with Adaptive residual, AirGNN, which aims to improve the robustness of the GNN against adversarial attack. The main contribution of the paper is to analyze the residual connections in the message passing of GNN and find that residual connections can amplify the vulnerability against abnormal node features. Based on this observation, the authors propose and derive a simple, efficient, interpretable, and adaptive message passing scheme, leading to a new GNN, called AIRGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm."
2063,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper studies the problem of adversarial attacks on GNNs. The authors propose a novel GNN with Adaptive residual, AirGNN, which aims to improve the robustness of the GNN against adversarial attack. The main contribution of the paper is to analyze the residual connections in the message passing of GNN and find that residual connections can amplify the vulnerability against abnormal node features. Based on this observation, the authors propose and derive a simple, efficient, interpretable, and adaptive message passing scheme, leading to a new GNN, called AIRGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm."
2064,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies online sequential decision problems where an agent must balance exploration and exploitation. The authors derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. They provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $O(\sqrt{AT})$ Bayesian regret for a problem with A actions after T rounds. They extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case, they show that Thompson sampling can produce policies outside of the optimistic sets and suffer linear regret in some instances. They call the resulting algorithm ‘variational Bayesian optimistic sampling’ (VBOS). The procedure works for any posteriors, i.e., it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness."
2065,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies online sequential decision problems where an agent must balance exploration and exploitation. The authors derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. They provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $O(\sqrt{AT})$ Bayesian regret for a problem with A actions after T rounds. They extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case, they show that Thompson sampling can produce policies outside of the optimistic sets and suffer linear regret in some instances. They call the resulting algorithm ‘variational Bayesian optimistic sampling’ (VBOS). The procedure works for any posteriors, i.e., it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness."
2066,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies online sequential decision problems where an agent must balance exploration and exploitation. The authors derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. They provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $O(\sqrt{AT})$ Bayesian regret for a problem with A actions after T rounds. They extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case, they show that Thompson sampling can produce policies outside of the optimistic sets and suffer linear regret in some instances. They call the resulting algorithm ‘variational Bayesian optimistic sampling’ (VBOS). The procedure works for any posteriors, i.e., it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness."
2067,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies online sequential decision problems where an agent must balance exploration and exploitation. The authors derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. They provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $O(\sqrt{AT})$ Bayesian regret for a problem with A actions after T rounds. They extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case, they show that Thompson sampling can produce policies outside of the optimistic sets and suffer linear regret in some instances. They call the resulting algorithm ‘variational Bayesian optimistic sampling’ (VBOS). The procedure works for any posteriors, i.e., it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness."
2068,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of choosing an order to draw samples in a stochastic gradient descent (SGD) algorithm for composite finite-sum minimization. The authors propose a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. The rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without-replacement sampling with variance reduction. They also propose a practical method to discover the optimal cyclic ordering."
2069,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of choosing an order to draw samples in a stochastic gradient descent (SGD) algorithm for composite finite-sum minimization. The authors propose a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. The rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without-replacement sampling with variance reduction. They also propose a practical method to discover the optimal cyclic ordering."
2070,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of choosing an order to draw samples in a stochastic gradient descent (SGD) algorithm for composite finite-sum minimization. The authors propose a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. The rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without-replacement sampling with variance reduction. They also propose a practical method to discover the optimal cyclic ordering."
2071,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of choosing an order to draw samples in a stochastic gradient descent (SGD) algorithm for composite finite-sum minimization. The authors propose a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. The rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without-replacement sampling with variance reduction. They also propose a practical method to discover the optimal cyclic ordering."
2072,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the performance of relative entropy policy search (REPS) in the context of stochastic gradient-based solvers. In particular, the authors provide guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. The authors first consider the setting in which we are given access to exact gradients and demonstrate how near-optimalality of the objective translates to near-optimability of the policy. Then, they introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy."
2073,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the performance of relative entropy policy search (REPS) in the context of stochastic gradient-based solvers. In particular, the authors provide guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. The authors first consider the setting in which we are given access to exact gradients and demonstrate how near-optimalality of the objective translates to near-optimability of the policy. Then, they introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy."
2074,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the performance of relative entropy policy search (REPS) in the context of stochastic gradient-based solvers. In particular, the authors provide guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. The authors first consider the setting in which we are given access to exact gradients and demonstrate how near-optimalality of the objective translates to near-optimability of the policy. Then, they introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy."
2075,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the performance of relative entropy policy search (REPS) in the context of stochastic gradient-based solvers. In particular, the authors provide guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. The authors first consider the setting in which we are given access to exact gradients and demonstrate how near-optimalality of the objective translates to near-optimability of the policy. Then, they introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy."
2076,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method to evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. Specifically, the authors propose to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, they also propose metrics to evaluate spatial smoothness of encoding 3D structure, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
2077,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method to evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. Specifically, the authors propose to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, they also propose metrics to evaluate spatial smoothness of encoding 3D structure, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
2078,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method to evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. Specifically, the authors propose to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, they also propose metrics to evaluate spatial smoothness of encoding 3D structure, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
2079,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method to evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. Specifically, the authors propose to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, they also propose metrics to evaluate spatial smoothness of encoding 3D structure, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
2080,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural network-based method for designing optimal auctions. The proposed method is based on the idea of preference networks, which encode constraints using (potentially human-provided) exemplars of desirable allocations. The paper also introduces a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that the proposed method can be competitive with existing state-of-the-art neural-network based auction designs."
2081,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural network-based method for designing optimal auctions. The proposed method is based on the idea of preference networks, which encode constraints using (potentially human-provided) exemplars of desirable allocations. The paper also introduces a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that the proposed method can be competitive with existing state-of-the-art neural-network based auction designs."
2082,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural network-based method for designing optimal auctions. The proposed method is based on the idea of preference networks, which encode constraints using (potentially human-provided) exemplars of desirable allocations. The paper also introduces a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that the proposed method can be competitive with existing state-of-the-art neural-network based auction designs."
2083,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural network-based method for designing optimal auctions. The proposed method is based on the idea of preference networks, which encode constraints using (potentially human-provided) exemplars of desirable allocations. The paper also introduces a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that the proposed method can be competitive with existing state-of-the-art neural-network based auction designs."
2084,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of user-level differential privacy in supervised learning. The authors consider the setting where each user has a training data set drawn from their own distribution Pi. In this setting, the goal is to learn the shared structure among the problems Pi, and solve their tasks better than they could individually. To this end, the authors formulate this question using joint, user- level differential privacy—that is, we control what is leaked about each user’s entire data set. They provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach."
2085,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of user-level differential privacy in supervised learning. The authors consider the setting where each user has a training data set drawn from their own distribution Pi. In this setting, the goal is to learn the shared structure among the problems Pi, and solve their tasks better than they could individually. To this end, the authors formulate this question using joint, user- level differential privacy—that is, we control what is leaked about each user’s entire data set. They provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach."
2086,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of user-level differential privacy in supervised learning. The authors consider the setting where each user has a training data set drawn from their own distribution Pi. In this setting, the goal is to learn the shared structure among the problems Pi, and solve their tasks better than they could individually. To this end, the authors formulate this question using joint, user- level differential privacy—that is, we control what is leaked about each user’s entire data set. They provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach."
2087,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of user-level differential privacy in supervised learning. The authors consider the setting where each user has a training data set drawn from their own distribution Pi. In this setting, the goal is to learn the shared structure among the problems Pi, and solve their tasks better than they could individually. To this end, the authors formulate this question using joint, user- level differential privacy—that is, we control what is leaked about each user’s entire data set. They provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach."
2088,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for evaluating VQA models against human-adversarial examples. The proposed AdVQA (Adversarial Question Answering Benchmark) is designed to evaluate the performance of state-of-the-art models on a set of adversarial examples generated by human subjects. The authors conduct an extensive analysis of the collected adversarial samples and provide guidance on future research directions.
2089,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for evaluating VQA models against human-adversarial examples. The proposed AdVQA (Adversarial Question Answering Benchmark) is designed to evaluate the performance of state-of-the-art models on a set of adversarial examples generated by human subjects. The authors conduct an extensive analysis of the collected adversarial samples and provide guidance on future research directions.
2090,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for evaluating VQA models against human-adversarial examples. The proposed AdVQA (Adversarial Question Answering Benchmark) is designed to evaluate the performance of state-of-the-art models on a set of adversarial examples generated by human subjects. The authors conduct an extensive analysis of the collected adversarial samples and provide guidance on future research directions.
2091,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for evaluating VQA models against human-adversarial examples. The proposed AdVQA (Adversarial Question Answering Benchmark) is designed to evaluate the performance of state-of-the-art models on a set of adversarial examples generated by human subjects. The authors conduct an extensive analysis of the collected adversarial samples and provide guidance on future research directions.
2092,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in the medial entorhinal cortex (MEC). The authors analyze the response patterns of different cell types in the MEC, and find that heterogeneous cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. The authors then evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. They find that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles — including those of grid cells themselves — despite not being explicitly trained for this purpose. Finally, the authors introduce a new MEC model that performs reward-modulated path integration. The experimental results show that this unified model matches neural recordings across all variable-reward conditions."
2093,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in the medial entorhinal cortex (MEC). The authors analyze the response patterns of different cell types in the MEC, and find that heterogeneous cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. The authors then evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. They find that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles — including those of grid cells themselves — despite not being explicitly trained for this purpose. Finally, the authors introduce a new MEC model that performs reward-modulated path integration. The experimental results show that this unified model matches neural recordings across all variable-reward conditions."
2094,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in the medial entorhinal cortex (MEC). The authors analyze the response patterns of different cell types in the MEC, and find that heterogeneous cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. The authors then evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. They find that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles — including those of grid cells themselves — despite not being explicitly trained for this purpose. Finally, the authors introduce a new MEC model that performs reward-modulated path integration. The experimental results show that this unified model matches neural recordings across all variable-reward conditions."
2095,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in the medial entorhinal cortex (MEC). The authors analyze the response patterns of different cell types in the MEC, and find that heterogeneous cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. The authors then evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. They find that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles — including those of grid cells themselves — despite not being explicitly trained for this purpose. Finally, the authors introduce a new MEC model that performs reward-modulated path integration. The experimental results show that this unified model matches neural recordings across all variable-reward conditions."
2096,SP:57f9812fa5e7d0c66d412beb035301684d760746,"This paper studies the problem of pathological training dynamics of KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. To remedy the pathology, the authors propose a non-parametric behavioral reference policy and show that it can outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks."
2097,SP:57f9812fa5e7d0c66d412beb035301684d760746,"This paper studies the problem of pathological training dynamics of KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. To remedy the pathology, the authors propose a non-parametric behavioral reference policy and show that it can outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks."
2098,SP:57f9812fa5e7d0c66d412beb035301684d760746,"This paper studies the problem of pathological training dynamics of KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. To remedy the pathology, the authors propose a non-parametric behavioral reference policy and show that it can outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks."
2099,SP:57f9812fa5e7d0c66d412beb035301684d760746,"This paper studies the problem of pathological training dynamics of KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. To remedy the pathology, the authors propose a non-parametric behavioral reference policy and show that it can outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks."
2100,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the role of local and translational invariance in the learning curve of kernel regression. Specifically, the authors consider the teacher-student setting, where the filter size of the teacher t is smaller than that of the student s. The authors show that local invariance is important for kernel regression, and that the exponent β is a function of s only and does not depend on the input dimension. They also show that performing kernel regression with a ridge that decreases with the size of training set leads to similar learning curve exponents to those obtained in the ridgeless case."
2101,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the role of local and translational invariance in the learning curve of kernel regression. Specifically, the authors consider the teacher-student setting, where the filter size of the teacher t is smaller than that of the student s. The authors show that local invariance is important for kernel regression, and that the exponent β is a function of s only and does not depend on the input dimension. They also show that performing kernel regression with a ridge that decreases with the size of training set leads to similar learning curve exponents to those obtained in the ridgeless case."
2102,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the role of local and translational invariance in the learning curve of kernel regression. Specifically, the authors consider the teacher-student setting, where the filter size of the teacher t is smaller than that of the student s. The authors show that local invariance is important for kernel regression, and that the exponent β is a function of s only and does not depend on the input dimension. They also show that performing kernel regression with a ridge that decreases with the size of training set leads to similar learning curve exponents to those obtained in the ridgeless case."
2103,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the role of local and translational invariance in the learning curve of kernel regression. Specifically, the authors consider the teacher-student setting, where the filter size of the teacher t is smaller than that of the student s. The authors show that local invariance is important for kernel regression, and that the exponent β is a function of s only and does not depend on the input dimension. They also show that performing kernel regression with a ridge that decreases with the size of training set leads to similar learning curve exponents to those obtained in the ridgeless case."
2104,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"This paper proposes a new method for training a deterministic autoencoder (DAE) based on variational autoencoders (VAEs). The authors argue that VAEs are limited by the prior assumption that latent representations learned by the model follow a simple uni-modal Gaussian distribution. To overcome this limitation, the authors propose an end-to-end training procedure, which efficiently shapes the latent space of the model during training and utilizes the capacity of expressive multi- modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. The experimental results show the expressiveness and sample quality of the proposed method."
2105,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"This paper proposes a new method for training a deterministic autoencoder (DAE) based on variational autoencoders (VAEs). The authors argue that VAEs are limited by the prior assumption that latent representations learned by the model follow a simple uni-modal Gaussian distribution. To overcome this limitation, the authors propose an end-to-end training procedure, which efficiently shapes the latent space of the model during training and utilizes the capacity of expressive multi- modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. The experimental results show the expressiveness and sample quality of the proposed method."
2106,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"This paper proposes a new method for training a deterministic autoencoder (DAE) based on variational autoencoders (VAEs). The authors argue that VAEs are limited by the prior assumption that latent representations learned by the model follow a simple uni-modal Gaussian distribution. To overcome this limitation, the authors propose an end-to-end training procedure, which efficiently shapes the latent space of the model during training and utilizes the capacity of expressive multi- modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. The experimental results show the expressiveness and sample quality of the proposed method."
2107,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"This paper proposes a new method for training a deterministic autoencoder (DAE) based on variational autoencoders (VAEs). The authors argue that VAEs are limited by the prior assumption that latent representations learned by the model follow a simple uni-modal Gaussian distribution. To overcome this limitation, the authors propose an end-to-end training procedure, which efficiently shapes the latent space of the model during training and utilizes the capacity of expressive multi- modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. The experimental results show the expressiveness and sample quality of the proposed method."
2108,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes a method for self-supervised adversarial detection. The proposed method is based on disentangling the class features and semantic features of the input image. The method is evaluated on CIFAR-10 and ImageNet against several state-of-the-art methods.
2109,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes a method for self-supervised adversarial detection. The proposed method is based on disentangling the class features and semantic features of the input image. The method is evaluated on CIFAR-10 and ImageNet against several state-of-the-art methods.
2110,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes a method for self-supervised adversarial detection. The proposed method is based on disentangling the class features and semantic features of the input image. The method is evaluated on CIFAR-10 and ImageNet against several state-of-the-art methods.
2111,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes a method for self-supervised adversarial detection. The proposed method is based on disentangling the class features and semantic features of the input image. The method is evaluated on CIFAR-10 and ImageNet against several state-of-the-art methods.
2112,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper presents a method for modeling the brain activity of syntactic representations of sentences. The authors propose a novel multi-dimensional embedding space based on syntactic features that encode information about the syntactic structure of sentences and fMRI recordings of participants reading a natural text. They show that these features explain additional variance in the brain activities of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, they show that regions well-predicted by syntactic feature are distributed in the language systems and are not distinguishable from those processing semantics."
2113,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper presents a method for modeling the brain activity of syntactic representations of sentences. The authors propose a novel multi-dimensional embedding space based on syntactic features that encode information about the syntactic structure of sentences and fMRI recordings of participants reading a natural text. They show that these features explain additional variance in the brain activities of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, they show that regions well-predicted by syntactic feature are distributed in the language systems and are not distinguishable from those processing semantics."
2114,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper presents a method for modeling the brain activity of syntactic representations of sentences. The authors propose a novel multi-dimensional embedding space based on syntactic features that encode information about the syntactic structure of sentences and fMRI recordings of participants reading a natural text. They show that these features explain additional variance in the brain activities of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, they show that regions well-predicted by syntactic feature are distributed in the language systems and are not distinguishable from those processing semantics."
2115,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper presents a method for modeling the brain activity of syntactic representations of sentences. The authors propose a novel multi-dimensional embedding space based on syntactic features that encode information about the syntactic structure of sentences and fMRI recordings of participants reading a natural text. They show that these features explain additional variance in the brain activities of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, they show that regions well-predicted by syntactic feature are distributed in the language systems and are not distinguishable from those processing semantics."
2116,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation using energy based models (EBMs). Specifically, the authors introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The authors claim that the proposed method is simple, fast to train, and efficient to sample. Experimental results show that their method outperforms the state-of-the-art in both conditional sampling and sequential editing."
2117,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation using energy based models (EBMs). Specifically, the authors introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The authors claim that the proposed method is simple, fast to train, and efficient to sample. Experimental results show that their method outperforms the state-of-the-art in both conditional sampling and sequential editing."
2118,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation using energy based models (EBMs). Specifically, the authors introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The authors claim that the proposed method is simple, fast to train, and efficient to sample. Experimental results show that their method outperforms the state-of-the-art in both conditional sampling and sequential editing."
2119,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation using energy based models (EBMs). Specifically, the authors introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The authors claim that the proposed method is simple, fast to train, and efficient to sample. Experimental results show that their method outperforms the state-of-the-art in both conditional sampling and sequential editing."
2120,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits model, where individual clients face different K-armed stochastic bandits coupled through common global parameters. The authors propose a collaborative algorithm called Fed-PE to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near optimal regret for both disjoint and shared parameter cases with logarithmic communication costs. In addition, a new concept called collinearly-dependent policies is introduced, based on which a tight minimax regret lower bound for the shared parameter case is derived. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
2121,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits model, where individual clients face different K-armed stochastic bandits coupled through common global parameters. The authors propose a collaborative algorithm called Fed-PE to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near optimal regret for both disjoint and shared parameter cases with logarithmic communication costs. In addition, a new concept called collinearly-dependent policies is introduced, based on which a tight minimax regret lower bound for the shared parameter case is derived. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
2122,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits model, where individual clients face different K-armed stochastic bandits coupled through common global parameters. The authors propose a collaborative algorithm called Fed-PE to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near optimal regret for both disjoint and shared parameter cases with logarithmic communication costs. In addition, a new concept called collinearly-dependent policies is introduced, based on which a tight minimax regret lower bound for the shared parameter case is derived. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
2123,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits model, where individual clients face different K-armed stochastic bandits coupled through common global parameters. The authors propose a collaborative algorithm called Fed-PE to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near optimal regret for both disjoint and shared parameter cases with logarithmic communication costs. In addition, a new concept called collinearly-dependent policies is introduced, based on which a tight minimax regret lower bound for the shared parameter case is derived. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
2124,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a method for offline model-based reinforcement learning (RL). The main idea is to train a value function using both the offline dataset and data generated using rollouts under the model while also regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the paper shows that COMBO satisfies a policy improvement guarantee in the offline setting. Experiments on several tasks show that the proposed method outperforms prior offline RL methods."
2125,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a method for offline model-based reinforcement learning (RL). The main idea is to train a value function using both the offline dataset and data generated using rollouts under the model while also regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the paper shows that COMBO satisfies a policy improvement guarantee in the offline setting. Experiments on several tasks show that the proposed method outperforms prior offline RL methods."
2126,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a method for offline model-based reinforcement learning (RL). The main idea is to train a value function using both the offline dataset and data generated using rollouts under the model while also regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the paper shows that COMBO satisfies a policy improvement guarantee in the offline setting. Experiments on several tasks show that the proposed method outperforms prior offline RL methods."
2127,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a method for offline model-based reinforcement learning (RL). The main idea is to train a value function using both the offline dataset and data generated using rollouts under the model while also regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the paper shows that COMBO satisfies a policy improvement guarantee in the offline setting. Experiments on several tasks show that the proposed method outperforms prior offline RL methods."
2128,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. As a crucial ingredient in the modeling strategy, each SDE contains a drift term that reflects the impact of backpropagation at an input on the features of all samples. The main finding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, the features become linearly separable, meaning vanishing training loss. Moreover, in the presence of local elasticity, the emergence of a simple geometric structure called the neural collapse of the features. The authors corroborate their theoretical analysis with experiments on a synthesized dataset of geometric shapes and CIFAR-10."
2129,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. As a crucial ingredient in the modeling strategy, each SDE contains a drift term that reflects the impact of backpropagation at an input on the features of all samples. The main finding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, the features become linearly separable, meaning vanishing training loss. Moreover, in the presence of local elasticity, the emergence of a simple geometric structure called the neural collapse of the features. The authors corroborate their theoretical analysis with experiments on a synthesized dataset of geometric shapes and CIFAR-10."
2130,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. As a crucial ingredient in the modeling strategy, each SDE contains a drift term that reflects the impact of backpropagation at an input on the features of all samples. The main finding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, the features become linearly separable, meaning vanishing training loss. Moreover, in the presence of local elasticity, the emergence of a simple geometric structure called the neural collapse of the features. The authors corroborate their theoretical analysis with experiments on a synthesized dataset of geometric shapes and CIFAR-10."
2131,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. As a crucial ingredient in the modeling strategy, each SDE contains a drift term that reflects the impact of backpropagation at an input on the features of all samples. The main finding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, the features become linearly separable, meaning vanishing training loss. Moreover, in the presence of local elasticity, the emergence of a simple geometric structure called the neural collapse of the features. The authors corroborate their theoretical analysis with experiments on a synthesized dataset of geometric shapes and CIFAR-10."
2132,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a method for learning to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines."
2133,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a method for learning to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines."
2134,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a method for learning to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines."
2135,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a method for learning to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines."
2136,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the problem of physics-informed neural networks (PINNs). The authors show that PINNs can fail to learn relevant physical phenomena for even slightly more complex problems. In particular, they analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. They provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN’s loss term starts from a simple PDE regularization and becomes progressively more complex as the NN gets trained. The second approach is a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that the proposed methods can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training."
2137,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the problem of physics-informed neural networks (PINNs). The authors show that PINNs can fail to learn relevant physical phenomena for even slightly more complex problems. In particular, they analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. They provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN’s loss term starts from a simple PDE regularization and becomes progressively more complex as the NN gets trained. The second approach is a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that the proposed methods can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training."
2138,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the problem of physics-informed neural networks (PINNs). The authors show that PINNs can fail to learn relevant physical phenomena for even slightly more complex problems. In particular, they analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. They provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN’s loss term starts from a simple PDE regularization and becomes progressively more complex as the NN gets trained. The second approach is a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that the proposed methods can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training."
2139,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the problem of physics-informed neural networks (PINNs). The authors show that PINNs can fail to learn relevant physical phenomena for even slightly more complex problems. In particular, they analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. They provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN’s loss term starts from a simple PDE regularization and becomes progressively more complex as the NN gets trained. The second approach is a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that the proposed methods can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training."
2140,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, a source-trained classifier is used to generate target pseudo-label with a source classifier, and then the reverse step trains a target classifier using target pseudo labels, and updates the shared representations to make the target classifiers perform well on the source data. The authors analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self training fail. Empirical results indicate that CST significantly improves over the state-of-the-arts on visual recognition and sentiment analysis benchmarks."
2141,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, a source-trained classifier is used to generate target pseudo-label with a source classifier, and then the reverse step trains a target classifier using target pseudo labels, and updates the shared representations to make the target classifiers perform well on the source data. The authors analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self training fail. Empirical results indicate that CST significantly improves over the state-of-the-arts on visual recognition and sentiment analysis benchmarks."
2142,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, a source-trained classifier is used to generate target pseudo-label with a source classifier, and then the reverse step trains a target classifier using target pseudo labels, and updates the shared representations to make the target classifiers perform well on the source data. The authors analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self training fail. Empirical results indicate that CST significantly improves over the state-of-the-arts on visual recognition and sentiment analysis benchmarks."
2143,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, a source-trained classifier is used to generate target pseudo-label with a source classifier, and then the reverse step trains a target classifier using target pseudo labels, and updates the shared representations to make the target classifiers perform well on the source data. The authors analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self training fail. Empirical results indicate that CST significantly improves over the state-of-the-arts on visual recognition and sentiment analysis benchmarks."
2144,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a new method for structured pruning. The proposed method is based on the idea of discriminative ative masking (DAM). The main idea of DAM is to gradually refine some of the neurons to be refined during the training process, while gradually masking out other neurons. Theoretical analysis is provided to show that the learning objective is directly related to minimizing the L0 norm of the masking layer. Experiments are conducted on a variety of tasks, including dimensionality reduction, recommendation system, graph representation learning, and image classification."
2145,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a new method for structured pruning. The proposed method is based on the idea of discriminative ative masking (DAM). The main idea of DAM is to gradually refine some of the neurons to be refined during the training process, while gradually masking out other neurons. Theoretical analysis is provided to show that the learning objective is directly related to minimizing the L0 norm of the masking layer. Experiments are conducted on a variety of tasks, including dimensionality reduction, recommendation system, graph representation learning, and image classification."
2146,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a new method for structured pruning. The proposed method is based on the idea of discriminative ative masking (DAM). The main idea of DAM is to gradually refine some of the neurons to be refined during the training process, while gradually masking out other neurons. Theoretical analysis is provided to show that the learning objective is directly related to minimizing the L0 norm of the masking layer. Experiments are conducted on a variety of tasks, including dimensionality reduction, recommendation system, graph representation learning, and image classification."
2147,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a new method for structured pruning. The proposed method is based on the idea of discriminative ative masking (DAM). The main idea of DAM is to gradually refine some of the neurons to be refined during the training process, while gradually masking out other neurons. Theoretical analysis is provided to show that the learning objective is directly related to minimizing the L0 norm of the masking layer. Experiments are conducted on a variety of tasks, including dimensionality reduction, recommendation system, graph representation learning, and image classification."
2148,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which are called functions. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments are conducted on image classification and visual abstract reasoning on Raven Progressive Matrices."
2149,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which are called functions. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments are conducted on image classification and visual abstract reasoning on Raven Progressive Matrices."
2150,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which are called functions. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments are conducted on image classification and visual abstract reasoning on Raven Progressive Matrices."
2151,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which are called functions. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments are conducted on image classification and visual abstract reasoning on Raven Progressive Matrices."
2152,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique to transfer knowledge acquired during an unsupervised pre-training phase to new tasks. The authors argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. Instead, the authors propose to leverage pre-trained policies for exploration and that is complementary to transferring neural network weights. The experiments show that, when combined with large-scale pretraining in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors. These pre-trainable policies can then be leveraged by BT to discover better solutions than without without."
2153,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique to transfer knowledge acquired during an unsupervised pre-training phase to new tasks. The authors argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. Instead, the authors propose to leverage pre-trained policies for exploration and that is complementary to transferring neural network weights. The experiments show that, when combined with large-scale pretraining in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors. These pre-trainable policies can then be leveraged by BT to discover better solutions than without without."
2154,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique to transfer knowledge acquired during an unsupervised pre-training phase to new tasks. The authors argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. Instead, the authors propose to leverage pre-trained policies for exploration and that is complementary to transferring neural network weights. The experiments show that, when combined with large-scale pretraining in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors. These pre-trainable policies can then be leveraged by BT to discover better solutions than without without."
2155,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique to transfer knowledge acquired during an unsupervised pre-training phase to new tasks. The authors argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. Instead, the authors propose to leverage pre-trained policies for exploration and that is complementary to transferring neural network weights. The experiments show that, when combined with large-scale pretraining in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors. These pre-trainable policies can then be leveraged by BT to discover better solutions than without without."
2156,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a differentiable surrogate loss function for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divideand-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, PiRank significantly improves over comparable approaches on publicly available internet-scale learning-to-rank benchmarks."
2157,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a differentiable surrogate loss function for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divideand-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, PiRank significantly improves over comparable approaches on publicly available internet-scale learning-to-rank benchmarks."
2158,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a differentiable surrogate loss function for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divideand-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, PiRank significantly improves over comparable approaches on publicly available internet-scale learning-to-rank benchmarks."
2159,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a differentiable surrogate loss function for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divideand-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, PiRank significantly improves over comparable approaches on publicly available internet-scale learning-to-rank benchmarks."
2160,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for estimating the ground-state energy of lithium hydride (LiH) in various configurations. The algorithm uses a feedback-driven curriculum learning method to adapt the complexity of learning problem to the current performance of the learning algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. In this well-known benchmark problem, the proposed algorithm achieves state-of-the-art results in terms of circuit depth and accuracy."
2161,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for estimating the ground-state energy of lithium hydride (LiH) in various configurations. The algorithm uses a feedback-driven curriculum learning method to adapt the complexity of learning problem to the current performance of the learning algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. In this well-known benchmark problem, the proposed algorithm achieves state-of-the-art results in terms of circuit depth and accuracy."
2162,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for estimating the ground-state energy of lithium hydride (LiH) in various configurations. The algorithm uses a feedback-driven curriculum learning method to adapt the complexity of learning problem to the current performance of the learning algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. In this well-known benchmark problem, the proposed algorithm achieves state-of-the-art results in terms of circuit depth and accuracy."
2163,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for estimating the ground-state energy of lithium hydride (LiH) in various configurations. The algorithm uses a feedback-driven curriculum learning method to adapt the complexity of learning problem to the current performance of the learning algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. In this well-known benchmark problem, the proposed algorithm achieves state-of-the-art results in terms of circuit depth and accuracy."
2164,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper proposes a new method for few-shot learning based on the Dirichlet-distributed random variables. Specifically, the authors propose to model the marginal probabilities of the classes as Dirichlets, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirical results show that the proposed method outperforms state-of-the-art methods across several data sets, models and settings."
2165,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper proposes a new method for few-shot learning based on the Dirichlet-distributed random variables. Specifically, the authors propose to model the marginal probabilities of the classes as Dirichlets, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirical results show that the proposed method outperforms state-of-the-art methods across several data sets, models and settings."
2166,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper proposes a new method for few-shot learning based on the Dirichlet-distributed random variables. Specifically, the authors propose to model the marginal probabilities of the classes as Dirichlets, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirical results show that the proposed method outperforms state-of-the-art methods across several data sets, models and settings."
2167,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper proposes a new method for few-shot learning based on the Dirichlet-distributed random variables. Specifically, the authors propose to model the marginal probabilities of the classes as Dirichlets, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirical results show that the proposed method outperforms state-of-the-art methods across several data sets, models and settings."
2168,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper studies the memory bottleneck in convolutional neural networks (CNNs). The authors propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. They further propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. They automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling."
2169,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper studies the memory bottleneck in convolutional neural networks (CNNs). The authors propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. They further propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. They automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling."
2170,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper studies the memory bottleneck in convolutional neural networks (CNNs). The authors propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. They further propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. They automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling."
2171,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper studies the memory bottleneck in convolutional neural networks (CNNs). The authors propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. They further propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. They automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling."
2172,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent, and takes actions based on the strategic agent’s report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal‘s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. The paper also shows that the principal's optimal utility is hard to approximate within a certain constant factor, complementing the algorithmic result."
2173,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent, and takes actions based on the strategic agent’s report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal‘s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. The paper also shows that the principal's optimal utility is hard to approximate within a certain constant factor, complementing the algorithmic result."
2174,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent, and takes actions based on the strategic agent’s report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal‘s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. The paper also shows that the principal's optimal utility is hard to approximate within a certain constant factor, complementing the algorithmic result."
2175,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent, and takes actions based on the strategic agent’s report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal‘s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. The paper also shows that the principal's optimal utility is hard to approximate within a certain constant factor, complementing the algorithmic result."
2176,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper studies the problem of differentiable architecture search for graph neural networks (GNNs). The main contribution of this paper is to provide a theoretical analysis of the gradient based NAS methods and to propose a differentiable search of the architecture with gradient descent (GASSO) method. The theoretical analysis is based on the fact that the usefulness of different types of information with respect to the target task. The experimental results on real-world graph datasets demonstrate that the proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines.
2177,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper studies the problem of differentiable architecture search for graph neural networks (GNNs). The main contribution of this paper is to provide a theoretical analysis of the gradient based NAS methods and to propose a differentiable search of the architecture with gradient descent (GASSO) method. The theoretical analysis is based on the fact that the usefulness of different types of information with respect to the target task. The experimental results on real-world graph datasets demonstrate that the proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines.
2178,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper studies the problem of differentiable architecture search for graph neural networks (GNNs). The main contribution of this paper is to provide a theoretical analysis of the gradient based NAS methods and to propose a differentiable search of the architecture with gradient descent (GASSO) method. The theoretical analysis is based on the fact that the usefulness of different types of information with respect to the target task. The experimental results on real-world graph datasets demonstrate that the proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines.
2179,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper studies the problem of differentiable architecture search for graph neural networks (GNNs). The main contribution of this paper is to provide a theoretical analysis of the gradient based NAS methods and to propose a differentiable search of the architecture with gradient descent (GASSO) method. The theoretical analysis is based on the fact that the usefulness of different types of information with respect to the target task. The experimental results on real-world graph datasets demonstrate that the proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines.
2180,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. Specifically, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. The authors derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximerin objective, they introduce an effective heuristic algorithm. They further derive impossibility results for other natural fairness objectives. They conclude with experimental results on real-world datasets."
2181,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. Specifically, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. The authors derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximerin objective, they introduce an effective heuristic algorithm. They further derive impossibility results for other natural fairness objectives. They conclude with experimental results on real-world datasets."
2182,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. Specifically, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. The authors derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximerin objective, they introduce an effective heuristic algorithm. They further derive impossibility results for other natural fairness objectives. They conclude with experimental results on real-world datasets."
2183,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. Specifically, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. The authors derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximerin objective, they introduce an effective heuristic algorithm. They further derive impossibility results for other natural fairness objectives. They conclude with experimental results on real-world datasets."
2184,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that subject to a bounded overlap condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
2185,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that subject to a bounded overlap condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
2186,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that subject to a bounded overlap condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
2187,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that subject to a bounded overlap condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
2188,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the choice of ReLU(0) in [0, 1] for backpropagation and training. The authors compare the value of the ReLU for different precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). The authors observe considerable variations of backpropagenation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU′(\0) = 0 seems to be the most efficient."
2189,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the choice of ReLU(0) in [0, 1] for backpropagation and training. The authors compare the value of the ReLU for different precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). The authors observe considerable variations of backpropagenation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU′(\0) = 0 seems to be the most efficient."
2190,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the choice of ReLU(0) in [0, 1] for backpropagation and training. The authors compare the value of the ReLU for different precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). The authors observe considerable variations of backpropagenation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU′(\0) = 0 seems to be the most efficient."
2191,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the choice of ReLU(0) in [0, 1] for backpropagation and training. The authors compare the value of the ReLU for different precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). The authors observe considerable variations of backpropagenation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU′(\0) = 0 seems to be the most efficient."
2192,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. The method combines ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The proposed method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck. As a result, the policies learned by the proposed method are robust and generalize well to new tasks."
2193,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. The method combines ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The proposed method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck. As a result, the policies learned by the proposed method are robust and generalize well to new tasks."
2194,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. The method combines ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The proposed method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck. As a result, the policies learned by the proposed method are robust and generalize well to new tasks."
2195,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. The method combines ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The proposed method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck. As a result, the policies learned by the proposed method are robust and generalize well to new tasks."
2196,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a new transformer architecture for graph neural networks. The proposed architecture is based on a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. Experiments on 4 standard datasets show that the proposed model performs on par or better than state-of-the-art GNNs.
2197,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a new transformer architecture for graph neural networks. The proposed architecture is based on a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. Experiments on 4 standard datasets show that the proposed model performs on par or better than state-of-the-art GNNs.
2198,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a new transformer architecture for graph neural networks. The proposed architecture is based on a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. Experiments on 4 standard datasets show that the proposed model performs on par or better than state-of-the-art GNNs.
2199,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a new transformer architecture for graph neural networks. The proposed architecture is based on a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. Experiments on 4 standard datasets show that the proposed model performs on par or better than state-of-the-art GNNs.
2200,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper studies the problem of two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. The authors propose an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium."
2201,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper studies the problem of two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. The authors propose an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium."
2202,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper studies the problem of two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. The authors propose an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium."
2203,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper studies the problem of two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. The authors propose an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium."
2204,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the Hessian of deep linear networks. The authors derive exact formulas and tight upper bounds for the rank deficiency of linear networks, and provide an elegant interpretation in terms of rank deficiency. Moreover, they demonstrate that their bounds remain faithful as an estimate of the numerical Hessian rank, for a larger class of models such as rectified and hyperbolic tangent networks. Further, they also investigate the implications of model architecture (e.g. width, depth, bias) on the rank deficiencies."
2205,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the Hessian of deep linear networks. The authors derive exact formulas and tight upper bounds for the rank deficiency of linear networks, and provide an elegant interpretation in terms of rank deficiency. Moreover, they demonstrate that their bounds remain faithful as an estimate of the numerical Hessian rank, for a larger class of models such as rectified and hyperbolic tangent networks. Further, they also investigate the implications of model architecture (e.g. width, depth, bias) on the rank deficiencies."
2206,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the Hessian of deep linear networks. The authors derive exact formulas and tight upper bounds for the rank deficiency of linear networks, and provide an elegant interpretation in terms of rank deficiency. Moreover, they demonstrate that their bounds remain faithful as an estimate of the numerical Hessian rank, for a larger class of models such as rectified and hyperbolic tangent networks. Further, they also investigate the implications of model architecture (e.g. width, depth, bias) on the rank deficiencies."
2207,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the Hessian of deep linear networks. The authors derive exact formulas and tight upper bounds for the rank deficiency of linear networks, and provide an elegant interpretation in terms of rank deficiency. Moreover, they demonstrate that their bounds remain faithful as an estimate of the numerical Hessian rank, for a larger class of models such as rectified and hyperbolic tangent networks. Further, they also investigate the implications of model architecture (e.g. width, depth, bias) on the rank deficiencies."
2208,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new metric, DLSBD, to quantify linear symmetry-based disentanglement (LSBD). The metric is based on the notion of linearly disentangled representations. The authors show that LSBD-VAE is a semi-supervised representation learning method that can learn LSBD representations with only limited supervision on transformations. The paper also shows that the proposed metric can be used to evaluate LSBD methods."
2209,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new metric, DLSBD, to quantify linear symmetry-based disentanglement (LSBD). The metric is based on the notion of linearly disentangled representations. The authors show that LSBD-VAE is a semi-supervised representation learning method that can learn LSBD representations with only limited supervision on transformations. The paper also shows that the proposed metric can be used to evaluate LSBD methods."
2210,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new metric, DLSBD, to quantify linear symmetry-based disentanglement (LSBD). The metric is based on the notion of linearly disentangled representations. The authors show that LSBD-VAE is a semi-supervised representation learning method that can learn LSBD representations with only limited supervision on transformations. The paper also shows that the proposed metric can be used to evaluate LSBD methods."
2211,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new metric, DLSBD, to quantify linear symmetry-based disentanglement (LSBD). The metric is based on the notion of linearly disentangled representations. The authors show that LSBD-VAE is a semi-supervised representation learning method that can learn LSBD representations with only limited supervision on transformations. The paper also shows that the proposed metric can be used to evaluate LSBD methods."
2212,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes an extension of the extended Kalman VAE (EKVAE) that combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The authors propose a constrained optimisation framework as a general approach for training DSSM. The experiments show that the proposed method outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled."
2213,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes an extension of the extended Kalman VAE (EKVAE) that combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The authors propose a constrained optimisation framework as a general approach for training DSSM. The experiments show that the proposed method outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled."
2214,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes an extension of the extended Kalman VAE (EKVAE) that combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The authors propose a constrained optimisation framework as a general approach for training DSSM. The experiments show that the proposed method outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled."
2215,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes an extension of the extended Kalman VAE (EKVAE) that combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The authors propose a constrained optimisation framework as a general approach for training DSSM. The experiments show that the proposed method outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled."
2216,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The proposed method is based on deep inversion, which is a technique for generating images from the training distribution. The authors argue that existing methods are insufficient for producing meaningful counterfactuallys, and propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep Inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. Experiments show that DISC is effective at producing visually meaningful explanations, and is robust to unknown test-time corruptions."
2217,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The proposed method is based on deep inversion, which is a technique for generating images from the training distribution. The authors argue that existing methods are insufficient for producing meaningful counterfactuallys, and propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep Inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. Experiments show that DISC is effective at producing visually meaningful explanations, and is robust to unknown test-time corruptions."
2218,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The proposed method is based on deep inversion, which is a technique for generating images from the training distribution. The authors argue that existing methods are insufficient for producing meaningful counterfactuallys, and propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep Inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. Experiments show that DISC is effective at producing visually meaningful explanations, and is robust to unknown test-time corruptions."
2219,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The proposed method is based on deep inversion, which is a technique for generating images from the training distribution. The authors argue that existing methods are insufficient for producing meaningful counterfactuallys, and propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep Inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. Experiments show that DISC is effective at producing visually meaningful explanations, and is robust to unknown test-time corruptions."
2220,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a causal inference algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. The proposed algorithm is based on maximizing an empirical objective, and the authors give a generalization bound for its performance. In a semi-synthetic experiment, the proposed algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, the algorithm is applied to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
2221,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a causal inference algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. The proposed algorithm is based on maximizing an empirical objective, and the authors give a generalization bound for its performance. In a semi-synthetic experiment, the proposed algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, the algorithm is applied to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
2222,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a causal inference algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. The proposed algorithm is based on maximizing an empirical objective, and the authors give a generalization bound for its performance. In a semi-synthetic experiment, the proposed algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, the algorithm is applied to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
2223,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a causal inference algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. The proposed algorithm is based on maximizing an empirical objective, and the authors give a generalization bound for its performance. In a semi-synthetic experiment, the proposed algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, the algorithm is applied to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
2224,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based method for image synthesis. Specifically, the proposed method takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. The proposed method learns two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. The style tokens are assigned to the content tokens by attention mechanism with a Transformer. Experiments on FFHQ and LSUN CHURCH show that the proposed TokenGAN achieves state-of-the-art results on several widelyused image synthesis benchmarks."
2225,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based method for image synthesis. Specifically, the proposed method takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. The proposed method learns two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. The style tokens are assigned to the content tokens by attention mechanism with a Transformer. Experiments on FFHQ and LSUN CHURCH show that the proposed TokenGAN achieves state-of-the-art results on several widelyused image synthesis benchmarks."
2226,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based method for image synthesis. Specifically, the proposed method takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. The proposed method learns two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. The style tokens are assigned to the content tokens by attention mechanism with a Transformer. Experiments on FFHQ and LSUN CHURCH show that the proposed TokenGAN achieves state-of-the-art results on several widelyused image synthesis benchmarks."
2227,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based method for image synthesis. Specifically, the proposed method takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. The proposed method learns two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. The style tokens are assigned to the content tokens by attention mechanism with a Transformer. Experiments on FFHQ and LSUN CHURCH show that the proposed TokenGAN achieves state-of-the-art results on several widelyused image synthesis benchmarks."
2228,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the effect of ridge regularization on the robustness of linear regression and classification models. The authors show that, even in the absence of noise, avoiding interpolation through ridge regularisation can significantly improve generalization. They prove this phenomenon for the robust risk of both linear regressions and classification, and hence provide the first theoretical result on robust overfitting."
2229,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the effect of ridge regularization on the robustness of linear regression and classification models. The authors show that, even in the absence of noise, avoiding interpolation through ridge regularisation can significantly improve generalization. They prove this phenomenon for the robust risk of both linear regressions and classification, and hence provide the first theoretical result on robust overfitting."
2230,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the effect of ridge regularization on the robustness of linear regression and classification models. The authors show that, even in the absence of noise, avoiding interpolation through ridge regularisation can significantly improve generalization. They prove this phenomenon for the robust risk of both linear regressions and classification, and hence provide the first theoretical result on robust overfitting."
2231,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the effect of ridge regularization on the robustness of linear regression and classification models. The authors show that, even in the absence of noise, avoiding interpolation through ridge regularisation can significantly improve generalization. They prove this phenomenon for the robust risk of both linear regressions and classification, and hence provide the first theoretical result on robust overfitting."
2232,SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The CPAE encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the proposed model performs favorably against state-of-the-art correspondence learning methods."
2233,SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The CPAE encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the proposed model performs favorably against state-of-the-art correspondence learning methods."
2234,SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The CPAE encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the proposed model performs favorably against state-of-the-art correspondence learning methods."
2235,SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The CPAE encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the proposed model performs favorably against state-of-the-art correspondence learning methods."
2236,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper proposes a new method for domain generalization (DG) based on empirical risk minimization (ERM). The main idea is to find flat minima in the ERM objective, and then use a dense and overfit-aware stochastic weight sampling strategy to find these minima. Experiments show that the proposed method achieves state-of-the-art performance on five DG benchmarks."
2237,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper proposes a new method for domain generalization (DG) based on empirical risk minimization (ERM). The main idea is to find flat minima in the ERM objective, and then use a dense and overfit-aware stochastic weight sampling strategy to find these minima. Experiments show that the proposed method achieves state-of-the-art performance on five DG benchmarks."
2238,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper proposes a new method for domain generalization (DG) based on empirical risk minimization (ERM). The main idea is to find flat minima in the ERM objective, and then use a dense and overfit-aware stochastic weight sampling strategy to find these minima. Experiments show that the proposed method achieves state-of-the-art performance on five DG benchmarks."
2239,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper proposes a new method for domain generalization (DG) based on empirical risk minimization (ERM). The main idea is to find flat minima in the ERM objective, and then use a dense and overfit-aware stochastic weight sampling strategy to find these minima. Experiments show that the proposed method achieves state-of-the-art performance on five DG benchmarks."
2240,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper presents a large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. The results act as recommendations for the best predictors to use in different settings, and show that certain families of predictors can be combined to achieve even better predictive power."
2241,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper presents a large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. The results act as recommendations for the best predictors to use in different settings, and show that certain families of predictors can be combined to achieve even better predictive power."
2242,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper presents a large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. The results act as recommendations for the best predictors to use in different settings, and show that certain families of predictors can be combined to achieve even better predictive power."
2243,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper presents a large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. The results act as recommendations for the best predictors to use in different settings, and show that certain families of predictors can be combined to achieve even better predictive power."
2244,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,This paper studies the privacy guarantee of Dirichlet posterior sampling from exponential families. The main contribution of this paper is to derive a truncated concentrated differential privacy (tCDP) guarantee for Dirichlets. The authors also provide accuracy guarantees of the Dirichlett posterior sampling in MultinomialDirichlet sampling and private normalized histogram publishing.
2245,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,This paper studies the privacy guarantee of Dirichlet posterior sampling from exponential families. The main contribution of this paper is to derive a truncated concentrated differential privacy (tCDP) guarantee for Dirichlets. The authors also provide accuracy guarantees of the Dirichlett posterior sampling in MultinomialDirichlet sampling and private normalized histogram publishing.
2246,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,This paper studies the privacy guarantee of Dirichlet posterior sampling from exponential families. The main contribution of this paper is to derive a truncated concentrated differential privacy (tCDP) guarantee for Dirichlets. The authors also provide accuracy guarantees of the Dirichlett posterior sampling in MultinomialDirichlet sampling and private normalized histogram publishing.
2247,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,This paper studies the privacy guarantee of Dirichlet posterior sampling from exponential families. The main contribution of this paper is to derive a truncated concentrated differential privacy (tCDP) guarantee for Dirichlets. The authors also provide accuracy guarantees of the Dirichlett posterior sampling in MultinomialDirichlet sampling and private normalized histogram publishing.
2248,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm to compute random walk efficiently and locally at the same time. The main contribution of this paper is to replace the non-local random walk algorithm in the previous work by a local algorithm. The authors show that the proposed algorithm is both memory and round efficient, and yields an efficient parallel local clustering algorithm. They also show that their algorithm is significantly more scalable than previous approaches."
2249,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm to compute random walk efficiently and locally at the same time. The main contribution of this paper is to replace the non-local random walk algorithm in the previous work by a local algorithm. The authors show that the proposed algorithm is both memory and round efficient, and yields an efficient parallel local clustering algorithm. They also show that their algorithm is significantly more scalable than previous approaches."
2250,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm to compute random walk efficiently and locally at the same time. The main contribution of this paper is to replace the non-local random walk algorithm in the previous work by a local algorithm. The authors show that the proposed algorithm is both memory and round efficient, and yields an efficient parallel local clustering algorithm. They also show that their algorithm is significantly more scalable than previous approaches."
2251,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm to compute random walk efficiently and locally at the same time. The main contribution of this paper is to replace the non-local random walk algorithm in the previous work by a local algorithm. The authors show that the proposed algorithm is both memory and round efficient, and yields an efficient parallel local clustering algorithm. They also show that their algorithm is significantly more scalable than previous approaches."
2252,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper theoretically analyzes the typical learning performance of `1-regularized linear regression (`1-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, the authors provide an accurate estimate of the typical sample complexity of    1-linear regression. Moreover, they provide an efficient method to accurately predict the nonasymptotic behavior of the linear regression for moderate M,N, such as precision and recall. "
2253,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper theoretically analyzes the typical learning performance of `1-regularized linear regression (`1-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, the authors provide an accurate estimate of the typical sample complexity of    1-linear regression. Moreover, they provide an efficient method to accurately predict the nonasymptotic behavior of the linear regression for moderate M,N, such as precision and recall. "
2254,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper theoretically analyzes the typical learning performance of `1-regularized linear regression (`1-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, the authors provide an accurate estimate of the typical sample complexity of    1-linear regression. Moreover, they provide an efficient method to accurately predict the nonasymptotic behavior of the linear regression for moderate M,N, such as precision and recall. "
2255,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper theoretically analyzes the typical learning performance of `1-regularized linear regression (`1-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, the authors provide an accurate estimate of the typical sample complexity of    1-linear regression. Moreover, they provide an efficient method to accurately predict the nonasymptotic behavior of the linear regression for moderate M,N, such as precision and recall. "
2256,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means objective, which extends the clustering capability of the well-known kmeans to datasets that are uncertain, vague and otherwise hard to cluster. The authors propose a semisupervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. They prove that having few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomially time complexity. Finally, they test their algorithms over real-world datasets, showing their effectiveness in real-life applications."
2257,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means objective, which extends the clustering capability of the well-known kmeans to datasets that are uncertain, vague and otherwise hard to cluster. The authors propose a semisupervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. They prove that having few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomially time complexity. Finally, they test their algorithms over real-world datasets, showing their effectiveness in real-life applications."
2258,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means objective, which extends the clustering capability of the well-known kmeans to datasets that are uncertain, vague and otherwise hard to cluster. The authors propose a semisupervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. They prove that having few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomially time complexity. Finally, they test their algorithms over real-world datasets, showing their effectiveness in real-life applications."
2259,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means objective, which extends the clustering capability of the well-known kmeans to datasets that are uncertain, vague and otherwise hard to cluster. The authors propose a semisupervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. They prove that having few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomially time complexity. Finally, they test their algorithms over real-world datasets, showing their effectiveness in real-life applications."
2260,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method for improving model-based reinforcement learning (RL) by encouraging a learned model and value function to be jointly self-consistent. Specifically, the authors propose multiple self consistency updates to encourage the value function and model to be consistent with each other. The authors evaluate the proposed method on both tabular and function approximation settings and find that, with appropriate choices, it can improve both policy evaluation and control."
2261,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method for improving model-based reinforcement learning (RL) by encouraging a learned model and value function to be jointly self-consistent. Specifically, the authors propose multiple self consistency updates to encourage the value function and model to be consistent with each other. The authors evaluate the proposed method on both tabular and function approximation settings and find that, with appropriate choices, it can improve both policy evaluation and control."
2262,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method for improving model-based reinforcement learning (RL) by encouraging a learned model and value function to be jointly self-consistent. Specifically, the authors propose multiple self consistency updates to encourage the value function and model to be consistent with each other. The authors evaluate the proposed method on both tabular and function approximation settings and find that, with appropriate choices, it can improve both policy evaluation and control."
2263,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method for improving model-based reinforcement learning (RL) by encouraging a learned model and value function to be jointly self-consistent. Specifically, the authors propose multiple self consistency updates to encourage the value function and model to be consistent with each other. The authors evaluate the proposed method on both tabular and function approximation settings and find that, with appropriate choices, it can improve both policy evaluation and control."
2264,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper proposes a method to approximate episode sampling distributions based on their difficulty. The proposed sampling method is algorithm agnostic, and can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. The experimental results demonstrate the efficacy of the proposed method across popular datasets, algorithms, network architectures, and protocols."
2265,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper proposes a method to approximate episode sampling distributions based on their difficulty. The proposed sampling method is algorithm agnostic, and can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. The experimental results demonstrate the efficacy of the proposed method across popular datasets, algorithms, network architectures, and protocols."
2266,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper proposes a method to approximate episode sampling distributions based on their difficulty. The proposed sampling method is algorithm agnostic, and can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. The experimental results demonstrate the efficacy of the proposed method across popular datasets, algorithms, network architectures, and protocols."
2267,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper proposes a method to approximate episode sampling distributions based on their difficulty. The proposed sampling method is algorithm agnostic, and can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. The experimental results demonstrate the efficacy of the proposed method across popular datasets, algorithms, network architectures, and protocols."
2268,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of logistic bandits in the setting where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’ vs ‘no click’). The authors propose a multinomial logit (MNL) model to model the probability of each one of K+1 2 possible outcomes (+1 stands for the ‘not click” outcome) and propose an upper confidence bound (UCB)-based algorithm that achieves regret O(dK p T) with small dependency on problem dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. The authors also provide numerical simulations that corroborate their theoretical results."
2269,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of logistic bandits in the setting where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’ vs ‘no click’). The authors propose a multinomial logit (MNL) model to model the probability of each one of K+1 2 possible outcomes (+1 stands for the ‘not click” outcome) and propose an upper confidence bound (UCB)-based algorithm that achieves regret O(dK p T) with small dependency on problem dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. The authors also provide numerical simulations that corroborate their theoretical results."
2270,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of logistic bandits in the setting where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’ vs ‘no click’). The authors propose a multinomial logit (MNL) model to model the probability of each one of K+1 2 possible outcomes (+1 stands for the ‘not click” outcome) and propose an upper confidence bound (UCB)-based algorithm that achieves regret O(dK p T) with small dependency on problem dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. The authors also provide numerical simulations that corroborate their theoretical results."
2271,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of logistic bandits in the setting where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’ vs ‘no click’). The authors propose a multinomial logit (MNL) model to model the probability of each one of K+1 2 possible outcomes (+1 stands for the ‘not click” outcome) and propose an upper confidence bound (UCB)-based algorithm that achieves regret O(dK p T) with small dependency on problem dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. The authors also provide numerical simulations that corroborate their theoretical results."
2272,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The main idea is to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework based on disentanglement of exploration and exploitation. The proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
2273,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The main idea is to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework based on disentanglement of exploration and exploitation. The proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
2274,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The main idea is to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework based on disentanglement of exploration and exploitation. The proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
2275,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The main idea is to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework based on disentanglement of exploration and exploitation. The proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
2276,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of modeling the time evolution of discrete sets of items (e.g., genetic mutations) through the lens of continuous-time Markov chains, and shows that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors explore a perhaps surprising remedy: including a number of additional independent items can help determine time order, and hence resolve underspecification. This is in sharp contrast to the common practice of limiting the analysis to a small subset of relevant items, which is followed largely due to poor scaling of existing methods."
2277,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of modeling the time evolution of discrete sets of items (e.g., genetic mutations) through the lens of continuous-time Markov chains, and shows that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors explore a perhaps surprising remedy: including a number of additional independent items can help determine time order, and hence resolve underspecification. This is in sharp contrast to the common practice of limiting the analysis to a small subset of relevant items, which is followed largely due to poor scaling of existing methods."
2278,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of modeling the time evolution of discrete sets of items (e.g., genetic mutations) through the lens of continuous-time Markov chains, and shows that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors explore a perhaps surprising remedy: including a number of additional independent items can help determine time order, and hence resolve underspecification. This is in sharp contrast to the common practice of limiting the analysis to a small subset of relevant items, which is followed largely due to poor scaling of existing methods."
2279,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of modeling the time evolution of discrete sets of items (e.g., genetic mutations) through the lens of continuous-time Markov chains, and shows that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors explore a perhaps surprising remedy: including a number of additional independent items can help determine time order, and hence resolve underspecification. This is in sharp contrast to the common practice of limiting the analysis to a small subset of relevant items, which is followed largely due to poor scaling of existing methods."
2280,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a new self-supervised pretraining framework for document understanding. The proposed method, called UDoc, extends the Transformer to take multi-modal embeddings as input. The authors claim that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks. UDoc is designed to support most document understanding tasks, including sentence classification, sentence similarity, and aligning modalities. Extensive empirical analysis demonstrates the effectiveness of the proposed method."
2281,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a new self-supervised pretraining framework for document understanding. The proposed method, called UDoc, extends the Transformer to take multi-modal embeddings as input. The authors claim that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks. UDoc is designed to support most document understanding tasks, including sentence classification, sentence similarity, and aligning modalities. Extensive empirical analysis demonstrates the effectiveness of the proposed method."
2282,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a new self-supervised pretraining framework for document understanding. The proposed method, called UDoc, extends the Transformer to take multi-modal embeddings as input. The authors claim that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks. UDoc is designed to support most document understanding tasks, including sentence classification, sentence similarity, and aligning modalities. Extensive empirical analysis demonstrates the effectiveness of the proposed method."
2283,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a new self-supervised pretraining framework for document understanding. The proposed method, called UDoc, extends the Transformer to take multi-modal embeddings as input. The authors claim that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks. UDoc is designed to support most document understanding tasks, including sentence classification, sentence similarity, and aligning modalities. Extensive empirical analysis demonstrates the effectiveness of the proposed method."
2284,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in the context of lp-norm clustering. The authors consider the case where the dataset consists of n points and they want to find k centers such that the objective is minimized, while also respecting the individual fairness constraint that every point v has a center within a distance at most r(v). The authors propose a local-search algorithm with provable (approximate) fairness and objective guarantees for the l∞ or k-CENTER objective. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal."
2285,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in the context of lp-norm clustering. The authors consider the case where the dataset consists of n points and they want to find k centers such that the objective is minimized, while also respecting the individual fairness constraint that every point v has a center within a distance at most r(v). The authors propose a local-search algorithm with provable (approximate) fairness and objective guarantees for the l∞ or k-CENTER objective. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal."
2286,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in the context of lp-norm clustering. The authors consider the case where the dataset consists of n points and they want to find k centers such that the objective is minimized, while also respecting the individual fairness constraint that every point v has a center within a distance at most r(v). The authors propose a local-search algorithm with provable (approximate) fairness and objective guarantees for the l∞ or k-CENTER objective. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal."
2287,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in the context of lp-norm clustering. The authors consider the case where the dataset consists of n points and they want to find k centers such that the objective is minimized, while also respecting the individual fairness constraint that every point v has a center within a distance at most r(v). The authors propose a local-search algorithm with provable (approximate) fairness and objective guarantees for the l∞ or k-CENTER objective. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal."
2288,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,This paper studies the problem of graph partitioning. The authors propose a polynomial-time Gaussian sampling-based algorithm for two problems: MAX-K-CUT and correlation clustering. The main contribution of the paper is the introduction of a new algorithm that uses polynomials to approximate the approximation ratio of the two problems. The algorithm is based on the idea of sparsifying the graph to reduce the dependence on the number of vertices in the graph. 
2289,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,This paper studies the problem of graph partitioning. The authors propose a polynomial-time Gaussian sampling-based algorithm for two problems: MAX-K-CUT and correlation clustering. The main contribution of the paper is the introduction of a new algorithm that uses polynomials to approximate the approximation ratio of the two problems. The algorithm is based on the idea of sparsifying the graph to reduce the dependence on the number of vertices in the graph. 
2290,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,This paper studies the problem of graph partitioning. The authors propose a polynomial-time Gaussian sampling-based algorithm for two problems: MAX-K-CUT and correlation clustering. The main contribution of the paper is the introduction of a new algorithm that uses polynomials to approximate the approximation ratio of the two problems. The algorithm is based on the idea of sparsifying the graph to reduce the dependence on the number of vertices in the graph. 
2291,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,This paper studies the problem of graph partitioning. The authors propose a polynomial-time Gaussian sampling-based algorithm for two problems: MAX-K-CUT and correlation clustering. The main contribution of the paper is the introduction of a new algorithm that uses polynomials to approximate the approximation ratio of the two problems. The algorithm is based on the idea of sparsifying the graph to reduce the dependence on the number of vertices in the graph. 
2292,SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a Motion-Aware Unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. The fusion module is utilized to further aggregate the augmented motion information (AMI) and current appearance information (current spatial state) to the final predicted frame. Moreover, an information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions. Experimental results show that the MAU outperforms the state-of-the-art methods on both video prediction and early action recognition tasks."
2293,SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a Motion-Aware Unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. The fusion module is utilized to further aggregate the augmented motion information (AMI) and current appearance information (current spatial state) to the final predicted frame. Moreover, an information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions. Experimental results show that the MAU outperforms the state-of-the-art methods on both video prediction and early action recognition tasks."
2294,SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a Motion-Aware Unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. The fusion module is utilized to further aggregate the augmented motion information (AMI) and current appearance information (current spatial state) to the final predicted frame. Moreover, an information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions. Experimental results show that the MAU outperforms the state-of-the-art methods on both video prediction and early action recognition tasks."
2295,SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a Motion-Aware Unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. The fusion module is utilized to further aggregate the augmented motion information (AMI) and current appearance information (current spatial state) to the final predicted frame. Moreover, an information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions. Experimental results show that the MAU outperforms the state-of-the-art methods on both video prediction and early action recognition tasks."
2296,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies function approximation with two-layer neural networks (both ReLU and polynomial activation functions) in the generative model setting under completeness of the neural network function class. The main contribution of this paper is to prove that the sample complexity scales linearly in the algebraic dimension. In particular, the authors show that under deterministic dynamics, sample complexity is linearly increasing with the number of layers."
2297,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies function approximation with two-layer neural networks (both ReLU and polynomial activation functions) in the generative model setting under completeness of the neural network function class. The main contribution of this paper is to prove that the sample complexity scales linearly in the algebraic dimension. In particular, the authors show that under deterministic dynamics, sample complexity is linearly increasing with the number of layers."
2298,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies function approximation with two-layer neural networks (both ReLU and polynomial activation functions) in the generative model setting under completeness of the neural network function class. The main contribution of this paper is to prove that the sample complexity scales linearly in the algebraic dimension. In particular, the authors show that under deterministic dynamics, sample complexity is linearly increasing with the number of layers."
2299,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies function approximation with two-layer neural networks (both ReLU and polynomial activation functions) in the generative model setting under completeness of the neural network function class. The main contribution of this paper is to prove that the sample complexity scales linearly in the algebraic dimension. In particular, the authors show that under deterministic dynamics, sample complexity is linearly increasing with the number of layers."
2300,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization performance of deep metric learning (DML) methods under out-of-distribution (OOD) distribution shifts. The authors systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under OOD distribution shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of the state of the art DML methods. They find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, they propose a few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in the proposed benchmark."
2301,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization performance of deep metric learning (DML) methods under out-of-distribution (OOD) distribution shifts. The authors systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under OOD distribution shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of the state of the art DML methods. They find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, they propose a few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in the proposed benchmark."
2302,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization performance of deep metric learning (DML) methods under out-of-distribution (OOD) distribution shifts. The authors systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under OOD distribution shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of the state of the art DML methods. They find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, they propose a few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in the proposed benchmark."
2303,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization performance of deep metric learning (DML) methods under out-of-distribution (OOD) distribution shifts. The authors systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under OOD distribution shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of the state of the art DML methods. They find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, they propose a few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in the proposed benchmark."
2304,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier model selection (UOMS). The proposed approach is based on meta-training a large set of models on historical outlier detection benchmark datasets, and then automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. The authors also introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection, as well as other meta learning techniques that are tailored for UOMS. Moreover, the proposed method is extremely efficient at test time."
2305,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier model selection (UOMS). The proposed approach is based on meta-training a large set of models on historical outlier detection benchmark datasets, and then automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. The authors also introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection, as well as other meta learning techniques that are tailored for UOMS. Moreover, the proposed method is extremely efficient at test time."
2306,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier model selection (UOMS). The proposed approach is based on meta-training a large set of models on historical outlier detection benchmark datasets, and then automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. The authors also introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection, as well as other meta learning techniques that are tailored for UOMS. Moreover, the proposed method is extremely efficient at test time."
2307,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier model selection (UOMS). The proposed approach is based on meta-training a large set of models on historical outlier detection benchmark datasets, and then automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. The authors also introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection, as well as other meta learning techniques that are tailored for UOMS. Moreover, the proposed method is extremely efficient at test time."
2308,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. The main contribution of this paper is to provide the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning."
2309,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. The main contribution of this paper is to provide the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning."
2310,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. The main contribution of this paper is to provide the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning."
2311,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. The main contribution of this paper is to provide the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning."
2312,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the problem of dropout graph neural networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNN, nodes are randomly and independently dropped in each of these runs. The authors derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and prove several properties regarding the expressive capabilities and limits of dropGNN. They experimentally validate their theoretical findings on expressiveness."
2313,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the problem of dropout graph neural networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNN, nodes are randomly and independently dropped in each of these runs. The authors derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and prove several properties regarding the expressive capabilities and limits of dropGNN. They experimentally validate their theoretical findings on expressiveness."
2314,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the problem of dropout graph neural networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNN, nodes are randomly and independently dropped in each of these runs. The authors derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and prove several properties regarding the expressive capabilities and limits of dropGNN. They experimentally validate their theoretical findings on expressiveness."
2315,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the problem of dropout graph neural networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNN, nodes are randomly and independently dropped in each of these runs. The authors derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and prove several properties regarding the expressive capabilities and limits of dropGNN. They experimentally validate their theoretical findings on expressiveness."
2316,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also proposes a dual-stream FPN to further enhance contextual information for downstream dense predictions. Without bells and whistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets."
2317,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also proposes a dual-stream FPN to further enhance contextual information for downstream dense predictions. Without bells and whistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets."
2318,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also proposes a dual-stream FPN to further enhance contextual information for downstream dense predictions. Without bells and whistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets."
2319,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also proposes a dual-stream FPN to further enhance contextual information for downstream dense predictions. Without bells and whistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets."
2320,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a new framework for learning concepts and physics models of objects and their interactions from videos and language. The key idea is to use a visual perception module, a concept learner, and a differentiable physics engine to jointly learn visual concepts and infer physical properties of objects. The proposed method is evaluated on both synthetic and real-world datasets and achieves state-of-the-art performance. The method is also highly data-efficient: physical parameters can be optimized from very few videos and even a single video can be sufficient."
2321,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a new framework for learning concepts and physics models of objects and their interactions from videos and language. The key idea is to use a visual perception module, a concept learner, and a differentiable physics engine to jointly learn visual concepts and infer physical properties of objects. The proposed method is evaluated on both synthetic and real-world datasets and achieves state-of-the-art performance. The method is also highly data-efficient: physical parameters can be optimized from very few videos and even a single video can be sufficient."
2322,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a new framework for learning concepts and physics models of objects and their interactions from videos and language. The key idea is to use a visual perception module, a concept learner, and a differentiable physics engine to jointly learn visual concepts and infer physical properties of objects. The proposed method is evaluated on both synthetic and real-world datasets and achieves state-of-the-art performance. The method is also highly data-efficient: physical parameters can be optimized from very few videos and even a single video can be sufficient."
2323,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a new framework for learning concepts and physics models of objects and their interactions from videos and language. The key idea is to use a visual perception module, a concept learner, and a differentiable physics engine to jointly learn visual concepts and infer physical properties of objects. The proposed method is evaluated on both synthetic and real-world datasets and achieves state-of-the-art performance. The method is also highly data-efficient: physical parameters can be optimized from very few videos and even a single video can be sufficient."
2324,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The authors argue that SIMILAr not only works in standard active learning but also easily extends to the realistic settings considered above and acts as a one-stop solution for active learning that is scalable to large real-world datasets. Empirically, the authors show that SimILAR significantly outperforms existing active learning algorithms by as much as 5%−18% in the case of rare classes and ≈ 5%-10% in out-of-distribution data on several image classification tasks like CIFAR-10, MNIST, and ImageNet."
2325,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The authors argue that SIMILAr not only works in standard active learning but also easily extends to the realistic settings considered above and acts as a one-stop solution for active learning that is scalable to large real-world datasets. Empirically, the authors show that SimILAR significantly outperforms existing active learning algorithms by as much as 5%−18% in the case of rare classes and ≈ 5%-10% in out-of-distribution data on several image classification tasks like CIFAR-10, MNIST, and ImageNet."
2326,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The authors argue that SIMILAr not only works in standard active learning but also easily extends to the realistic settings considered above and acts as a one-stop solution for active learning that is scalable to large real-world datasets. Empirically, the authors show that SimILAR significantly outperforms existing active learning algorithms by as much as 5%−18% in the case of rare classes and ≈ 5%-10% in out-of-distribution data on several image classification tasks like CIFAR-10, MNIST, and ImageNet."
2327,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The authors argue that SIMILAr not only works in standard active learning but also easily extends to the realistic settings considered above and acts as a one-stop solution for active learning that is scalable to large real-world datasets. Empirically, the authors show that SimILAR significantly outperforms existing active learning algorithms by as much as 5%−18% in the case of rare classes and ≈ 5%-10% in out-of-distribution data on several image classification tasks like CIFAR-10, MNIST, and ImageNet."
2328,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"In this paper, the authors propose two identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test, and the second one is derived from an optimal learning algorithm for the mallows model. The proposed tests scale gracefully with the number of items to be ranked."
2329,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"In this paper, the authors propose two identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test, and the second one is derived from an optimal learning algorithm for the mallows model. The proposed tests scale gracefully with the number of items to be ranked."
2330,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"In this paper, the authors propose two identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test, and the second one is derived from an optimal learning algorithm for the mallows model. The proposed tests scale gracefully with the number of items to be ranked."
2331,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"In this paper, the authors propose two identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test, and the second one is derived from an optimal learning algorithm for the mallows model. The proposed tests scale gracefully with the number of items to be ranked."
2332,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. The method is based on a parametric human body model for robust performance capture. Specifically, a temporal transformer is proposed to aggregates tracked visual features based on the skeletal body motion over time. Moreover, a cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
2333,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. The method is based on a parametric human body model for robust performance capture. Specifically, a temporal transformer is proposed to aggregates tracked visual features based on the skeletal body motion over time. Moreover, a cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
2334,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. The method is based on a parametric human body model for robust performance capture. Specifically, a temporal transformer is proposed to aggregates tracked visual features based on the skeletal body motion over time. Moreover, a cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
2335,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. The method is based on a parametric human body model for robust performance capture. Specifically, a temporal transformer is proposed to aggregates tracked visual features based on the skeletal body motion over time. Moreover, a cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
2336,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for architecture search for vision transformers. The main idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. The proposed method, named S3 (short for Searching the Search Space), from the searched space achieves superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks."
2337,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for architecture search for vision transformers. The main idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. The proposed method, named S3 (short for Searching the Search Space), from the searched space achieves superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks."
2338,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for architecture search for vision transformers. The main idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. The proposed method, named S3 (short for Searching the Search Space), from the searched space achieves superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks."
2339,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for architecture search for vision transformers. The main idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. The proposed method, named S3 (short for Searching the Search Space), from the searched space achieves superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks."
2340,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. The authors provide an algorithm that, given a collection of bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. This bound is tight for the non-monochromatic bags case. For the special case of OR over the d-dimensional boolean vectors, the authors give an algorithm which computes an additional Ω(1/d) in accuracy for the two cases."
2341,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. The authors provide an algorithm that, given a collection of bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. This bound is tight for the non-monochromatic bags case. For the special case of OR over the d-dimensional boolean vectors, the authors give an algorithm which computes an additional Ω(1/d) in accuracy for the two cases."
2342,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. The authors provide an algorithm that, given a collection of bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. This bound is tight for the non-monochromatic bags case. For the special case of OR over the d-dimensional boolean vectors, the authors give an algorithm which computes an additional Ω(1/d) in accuracy for the two cases."
2343,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. The authors provide an algorithm that, given a collection of bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. This bound is tight for the non-monochromatic bags case. For the special case of OR over the d-dimensional boolean vectors, the authors give an algorithm which computes an additional Ω(1/d) in accuracy for the two cases."
2344,SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a new benchmark for neural architecture search, NAS-Bench-x11, which uses the full training information for each architecture, rather than just the final validation accuracy. The authors also propose a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-Fidelity algorithms which claimed to be state-of-the-art upon release."
2345,SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a new benchmark for neural architecture search, NAS-Bench-x11, which uses the full training information for each architecture, rather than just the final validation accuracy. The authors also propose a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-Fidelity algorithms which claimed to be state-of-the-art upon release."
2346,SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a new benchmark for neural architecture search, NAS-Bench-x11, which uses the full training information for each architecture, rather than just the final validation accuracy. The authors also propose a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-Fidelity algorithms which claimed to be state-of-the-art upon release."
2347,SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a new benchmark for neural architecture search, NAS-Bench-x11, which uses the full training information for each architecture, rather than just the final validation accuracy. The authors also propose a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-Fidelity algorithms which claimed to be state-of-the-art upon release."
2348,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. The authors provide an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, they propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Experiments on tasks ranging from mortality prediction to image classification demonstrate that these decompositions are robust and accurate. Moreover, the authors demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them."
2349,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. The authors provide an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, they propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Experiments on tasks ranging from mortality prediction to image classification demonstrate that these decompositions are robust and accurate. Moreover, the authors demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them."
2350,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. The authors provide an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, they propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Experiments on tasks ranging from mortality prediction to image classification demonstrate that these decompositions are robust and accurate. Moreover, the authors demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them."
2351,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. The authors provide an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, they propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Experiments on tasks ranging from mortality prediction to image classification demonstrate that these decompositions are robust and accurate. Moreover, the authors demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them."
2352,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a Transformer-based approach for vision-language pre-training (VLP). The main idea is to use a fully-transformer architecture to learn visual relation and inter-modal alignment in VLP. The authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., inter-mode flow) and propose a masking optimization mechanism named Masked Feature Regression (MFR) to further promote the inter-model learning. The experimental results show that the proposed method outperforms the state-of-the-art VLP models, but also exhibits superiority on the IMF metric."
2353,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a Transformer-based approach for vision-language pre-training (VLP). The main idea is to use a fully-transformer architecture to learn visual relation and inter-modal alignment in VLP. The authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., inter-mode flow) and propose a masking optimization mechanism named Masked Feature Regression (MFR) to further promote the inter-model learning. The experimental results show that the proposed method outperforms the state-of-the-art VLP models, but also exhibits superiority on the IMF metric."
2354,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a Transformer-based approach for vision-language pre-training (VLP). The main idea is to use a fully-transformer architecture to learn visual relation and inter-modal alignment in VLP. The authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., inter-mode flow) and propose a masking optimization mechanism named Masked Feature Regression (MFR) to further promote the inter-model learning. The experimental results show that the proposed method outperforms the state-of-the-art VLP models, but also exhibits superiority on the IMF metric."
2355,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a Transformer-based approach for vision-language pre-training (VLP). The main idea is to use a fully-transformer architecture to learn visual relation and inter-modal alignment in VLP. The authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., inter-mode flow) and propose a masking optimization mechanism named Masked Feature Regression (MFR) to further promote the inter-model learning. The experimental results show that the proposed method outperforms the state-of-the-art VLP models, but also exhibits superiority on the IMF metric."
2356,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is private. The authors study the dynamics of Rényi differential privacy loss throughout the training process. They prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy value by upper-bounding its total value over all intermediate gradient computations). They prove optimal utility with a small gradient complexity for noisy gradient descent algorithms."
2357,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is private. The authors study the dynamics of Rényi differential privacy loss throughout the training process. They prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy value by upper-bounding its total value over all intermediate gradient computations). They prove optimal utility with a small gradient complexity for noisy gradient descent algorithms."
2358,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is private. The authors study the dynamics of Rényi differential privacy loss throughout the training process. They prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy value by upper-bounding its total value over all intermediate gradient computations). They prove optimal utility with a small gradient complexity for noisy gradient descent algorithms."
2359,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is private. The authors study the dynamics of Rényi differential privacy loss throughout the training process. They prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy value by upper-bounding its total value over all intermediate gradient computations). They prove optimal utility with a small gradient complexity for noisy gradient descent algorithms."
2360,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes a reinforcement learning-based algorithm for solving quadratic optimization problems. The main idea is to use RL to learn a policy to tune parameters to accelerate convergence. The proposed method is evaluated on a variety of problems, including QPLIB, Netlib LP, and Maros-Mészáros problems."
2361,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes a reinforcement learning-based algorithm for solving quadratic optimization problems. The main idea is to use RL to learn a policy to tune parameters to accelerate convergence. The proposed method is evaluated on a variety of problems, including QPLIB, Netlib LP, and Maros-Mészáros problems."
2362,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes a reinforcement learning-based algorithm for solving quadratic optimization problems. The main idea is to use RL to learn a policy to tune parameters to accelerate convergence. The proposed method is evaluated on a variety of problems, including QPLIB, Netlib LP, and Maros-Mészáros problems."
2363,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes a reinforcement learning-based algorithm for solving quadratic optimization problems. The main idea is to use RL to learn a policy to tune parameters to accelerate convergence. The proposed method is evaluated on a variety of problems, including QPLIB, Netlib LP, and Maros-Mészáros problems."
2364,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the principal components bias (PC-bias) in the over-parametrized deep linear network model. The authors show that the convergence rate of this model is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They show how the PC bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the PC-biased spectral bias to the spectral bias, showing that both biases can be seen independently, and affect the order in different ways."
2365,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the principal components bias (PC-bias) in the over-parametrized deep linear network model. The authors show that the convergence rate of this model is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They show how the PC bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the PC-biased spectral bias to the spectral bias, showing that both biases can be seen independently, and affect the order in different ways."
2366,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the principal components bias (PC-bias) in the over-parametrized deep linear network model. The authors show that the convergence rate of this model is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They show how the PC bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the PC-biased spectral bias to the spectral bias, showing that both biases can be seen independently, and affect the order in different ways."
2367,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the principal components bias (PC-bias) in the over-parametrized deep linear network model. The authors show that the convergence rate of this model is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They show how the PC bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the PC-biased spectral bias to the spectral bias, showing that both biases can be seen independently, and affect the order in different ways."
2368,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a method for training clean models on backdoor-poisoned data. The main idea is to learn a dual-task of learning the clean and the backdoor portions of data. To this end, the authors propose a two-stage gradient ascent mechanism for standard training to help isolate backdoor examples at an early training stage, and to break the correlation between backdoor examples and the target class at a later training stage. Experiments on multiple benchmark datasets against 10 state-of-the-art attacks show that ABL-trained models achieve the same performance as they were trained on purely clean data."
2369,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a method for training clean models on backdoor-poisoned data. The main idea is to learn a dual-task of learning the clean and the backdoor portions of data. To this end, the authors propose a two-stage gradient ascent mechanism for standard training to help isolate backdoor examples at an early training stage, and to break the correlation between backdoor examples and the target class at a later training stage. Experiments on multiple benchmark datasets against 10 state-of-the-art attacks show that ABL-trained models achieve the same performance as they were trained on purely clean data."
2370,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a method for training clean models on backdoor-poisoned data. The main idea is to learn a dual-task of learning the clean and the backdoor portions of data. To this end, the authors propose a two-stage gradient ascent mechanism for standard training to help isolate backdoor examples at an early training stage, and to break the correlation between backdoor examples and the target class at a later training stage. Experiments on multiple benchmark datasets against 10 state-of-the-art attacks show that ABL-trained models achieve the same performance as they were trained on purely clean data."
2371,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a method for training clean models on backdoor-poisoned data. The main idea is to learn a dual-task of learning the clean and the backdoor portions of data. To this end, the authors propose a two-stage gradient ascent mechanism for standard training to help isolate backdoor examples at an early training stage, and to break the correlation between backdoor examples and the target class at a later training stage. Experiments on multiple benchmark datasets against 10 state-of-the-art attacks show that ABL-trained models achieve the same performance as they were trained on purely clean data."
2372,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a method for 3D-aware image synthesis. The main idea is to use a generative implicit model that is able to learn a starkly improved shape representation. This is achieved by modeling illumination explicitly and performing shading with various lighting conditions. To compensate for the additional computational burden of calculating surface normals, the paper proposes an efficient volume rendering strategy via surface tracking. Experiments on multiple datasets show that the proposed method achieves photorealistic 3D image synthesis while capturing accurate underlying 3D shapes."
2373,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a method for 3D-aware image synthesis. The main idea is to use a generative implicit model that is able to learn a starkly improved shape representation. This is achieved by modeling illumination explicitly and performing shading with various lighting conditions. To compensate for the additional computational burden of calculating surface normals, the paper proposes an efficient volume rendering strategy via surface tracking. Experiments on multiple datasets show that the proposed method achieves photorealistic 3D image synthesis while capturing accurate underlying 3D shapes."
2374,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a method for 3D-aware image synthesis. The main idea is to use a generative implicit model that is able to learn a starkly improved shape representation. This is achieved by modeling illumination explicitly and performing shading with various lighting conditions. To compensate for the additional computational burden of calculating surface normals, the paper proposes an efficient volume rendering strategy via surface tracking. Experiments on multiple datasets show that the proposed method achieves photorealistic 3D image synthesis while capturing accurate underlying 3D shapes."
2375,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a method for 3D-aware image synthesis. The main idea is to use a generative implicit model that is able to learn a starkly improved shape representation. This is achieved by modeling illumination explicitly and performing shading with various lighting conditions. To compensate for the additional computational burden of calculating surface normals, the paper proposes an efficient volume rendering strategy via surface tracking. Experiments on multiple datasets show that the proposed method achieves photorealistic 3D image synthesis while capturing accurate underlying 3D shapes."
2376,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a scalable quasi-Bayesian procedure for instrumental variable regression. The proposed method builds upon the recently developed kernelized IV models. The authors provide theoretical analysis of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of their method. The algorithm can be further extended to work with neural network models."
2377,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a scalable quasi-Bayesian procedure for instrumental variable regression. The proposed method builds upon the recently developed kernelized IV models. The authors provide theoretical analysis of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of their method. The algorithm can be further extended to work with neural network models."
2378,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a scalable quasi-Bayesian procedure for instrumental variable regression. The proposed method builds upon the recently developed kernelized IV models. The authors provide theoretical analysis of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of their method. The algorithm can be further extended to work with neural network models."
2379,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a scalable quasi-Bayesian procedure for instrumental variable regression. The proposed method builds upon the recently developed kernelized IV models. The authors provide theoretical analysis of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of their method. The algorithm can be further extended to work with neural network models."
2380,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a method for cross-lingual open-source question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. The authors propose a dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose a multilingual autoregressive generation model, which can answer directly in the target language without any translation or in-language retrieval modules as used in prior work. Finally, they propose an iterative training method that automatically extends annotated annotations available only in high-resource languages to low-resource ones."
2381,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a method for cross-lingual open-source question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. The authors propose a dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose a multilingual autoregressive generation model, which can answer directly in the target language without any translation or in-language retrieval modules as used in prior work. Finally, they propose an iterative training method that automatically extends annotated annotations available only in high-resource languages to low-resource ones."
2382,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a method for cross-lingual open-source question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. The authors propose a dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose a multilingual autoregressive generation model, which can answer directly in the target language without any translation or in-language retrieval modules as used in prior work. Finally, they propose an iterative training method that automatically extends annotated annotations available only in high-resource languages to low-resource ones."
2383,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a method for cross-lingual open-source question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. The authors propose a dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose a multilingual autoregressive generation model, which can answer directly in the target language without any translation or in-language retrieval modules as used in prior work. Finally, they propose an iterative training method that automatically extends annotated annotations available only in high-resource languages to low-resource ones."
2384,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper investigates the role of domain adaptation theory in the generalization performance of Empirical Risk Minimization (ERM) models trained on three popular domain generalization datasets. Specifically, the authors study the extent to which Ben-David et al. (2007) explains the performance of ERMs. They find that this theory does not provide a tight explanation of the out-of-domain generalization observed across a large number of ERM models trained across three popular domains. This motivates them to investigate other possible measures—that, however, lack theory—which could explain generalization in this setting. "
2385,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper investigates the role of domain adaptation theory in the generalization performance of Empirical Risk Minimization (ERM) models trained on three popular domain generalization datasets. Specifically, the authors study the extent to which Ben-David et al. (2007) explains the performance of ERMs. They find that this theory does not provide a tight explanation of the out-of-domain generalization observed across a large number of ERM models trained across three popular domains. This motivates them to investigate other possible measures—that, however, lack theory—which could explain generalization in this setting. "
2386,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper investigates the role of domain adaptation theory in the generalization performance of Empirical Risk Minimization (ERM) models trained on three popular domain generalization datasets. Specifically, the authors study the extent to which Ben-David et al. (2007) explains the performance of ERMs. They find that this theory does not provide a tight explanation of the out-of-domain generalization observed across a large number of ERM models trained across three popular domains. This motivates them to investigate other possible measures—that, however, lack theory—which could explain generalization in this setting. "
2387,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper investigates the role of domain adaptation theory in the generalization performance of Empirical Risk Minimization (ERM) models trained on three popular domain generalization datasets. Specifically, the authors study the extent to which Ben-David et al. (2007) explains the performance of ERMs. They find that this theory does not provide a tight explanation of the out-of-domain generalization observed across a large number of ERM models trained across three popular domains. This motivates them to investigate other possible measures—that, however, lack theory—which could explain generalization in this setting. "
2388,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper studies backdoor data poisoning attacks for classification problems. The authors provide a theoretical framework within which one can analyze the statistical and computational issues surrounding these attacks. They show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors."
2389,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper studies backdoor data poisoning attacks for classification problems. The authors provide a theoretical framework within which one can analyze the statistical and computational issues surrounding these attacks. They show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors."
2390,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper studies backdoor data poisoning attacks for classification problems. The authors provide a theoretical framework within which one can analyze the statistical and computational issues surrounding these attacks. They show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors."
2391,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper studies backdoor data poisoning attacks for classification problems. The authors provide a theoretical framework within which one can analyze the statistical and computational issues surrounding these attacks. They show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors."
2392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of width on the performance of deep neural networks trained with L2 regularization. The authors study the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In particular, the authors show that even non-parametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power. They show that width and depth have opposite effects: depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian."
2393,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of width on the performance of deep neural networks trained with L2 regularization. The authors study the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In particular, the authors show that even non-parametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power. They show that width and depth have opposite effects: depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian."
2394,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of width on the performance of deep neural networks trained with L2 regularization. The authors study the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In particular, the authors show that even non-parametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power. They show that width and depth have opposite effects: depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian."
2395,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of width on the performance of deep neural networks trained with L2 regularization. The authors study the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In particular, the authors show that even non-parametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power. They show that width and depth have opposite effects: depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian."
2396,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the convergence of FedLin, an algorithm for federated learning (FL) in which a group of clients periodically coordinate with a central server to train a statistical model. The main contribution of the paper is to show that FedLin guarantees linear convergence to the global minimum when the loss functions are smooth and strongly convex. The authors also provide matching upper and lower bounds on the convergence rate of FedRL. Finally, the authors provide a theoretical analysis of the effect of gradient sparsification on the rate of convergence."
2397,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the convergence of FedLin, an algorithm for federated learning (FL) in which a group of clients periodically coordinate with a central server to train a statistical model. The main contribution of the paper is to show that FedLin guarantees linear convergence to the global minimum when the loss functions are smooth and strongly convex. The authors also provide matching upper and lower bounds on the convergence rate of FedRL. Finally, the authors provide a theoretical analysis of the effect of gradient sparsification on the rate of convergence."
2398,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the convergence of FedLin, an algorithm for federated learning (FL) in which a group of clients periodically coordinate with a central server to train a statistical model. The main contribution of the paper is to show that FedLin guarantees linear convergence to the global minimum when the loss functions are smooth and strongly convex. The authors also provide matching upper and lower bounds on the convergence rate of FedRL. Finally, the authors provide a theoretical analysis of the effect of gradient sparsification on the rate of convergence."
2399,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the convergence of FedLin, an algorithm for federated learning (FL) in which a group of clients periodically coordinate with a central server to train a statistical model. The main contribution of the paper is to show that FedLin guarantees linear convergence to the global minimum when the loss functions are smooth and strongly convex. The authors also provide matching upper and lower bounds on the convergence rate of FedRL. Finally, the authors provide a theoretical analysis of the effect of gradient sparsification on the rate of convergence."
2400,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the problem of approximating the sliced-Wasserstein distance (SW) using the concentration of measure phenomenon. The authors propose a deterministic approximation of SW based on the assumption that one-dimensional projections of a high-dimensional random vector are approximately Gaussian. They derive nonasymptotical guarantees for their approach and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. They validate their theoretical findings on synthetic datasets and illustrate the proposed approximation on a generative modeling problem."
2401,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the problem of approximating the sliced-Wasserstein distance (SW) using the concentration of measure phenomenon. The authors propose a deterministic approximation of SW based on the assumption that one-dimensional projections of a high-dimensional random vector are approximately Gaussian. They derive nonasymptotical guarantees for their approach and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. They validate their theoretical findings on synthetic datasets and illustrate the proposed approximation on a generative modeling problem."
2402,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the problem of approximating the sliced-Wasserstein distance (SW) using the concentration of measure phenomenon. The authors propose a deterministic approximation of SW based on the assumption that one-dimensional projections of a high-dimensional random vector are approximately Gaussian. They derive nonasymptotical guarantees for their approach and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. They validate their theoretical findings on synthetic datasets and illustrate the proposed approximation on a generative modeling problem."
2403,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the problem of approximating the sliced-Wasserstein distance (SW) using the concentration of measure phenomenon. The authors propose a deterministic approximation of SW based on the assumption that one-dimensional projections of a high-dimensional random vector are approximately Gaussian. They derive nonasymptotical guarantees for their approach and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. They validate their theoretical findings on synthetic datasets and illustrate the proposed approximation on a generative modeling problem."
2404,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to compare the representations learned by neural language models, translation models, and language tagging tasks. The method is based on an encoder-decoder transfer learning method from computer vision. The authors show that language models and translation models smoothly interpolate between word embedding, syntactic and semantic tasks, and future word embeddings. They call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP (natural language processing) tasks."
2405,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to compare the representations learned by neural language models, translation models, and language tagging tasks. The method is based on an encoder-decoder transfer learning method from computer vision. The authors show that language models and translation models smoothly interpolate between word embedding, syntactic and semantic tasks, and future word embeddings. They call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP (natural language processing) tasks."
2406,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to compare the representations learned by neural language models, translation models, and language tagging tasks. The method is based on an encoder-decoder transfer learning method from computer vision. The authors show that language models and translation models smoothly interpolate between word embedding, syntactic and semantic tasks, and future word embeddings. They call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP (natural language processing) tasks."
2407,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to compare the representations learned by neural language models, translation models, and language tagging tasks. The method is based on an encoder-decoder transfer learning method from computer vision. The authors show that language models and translation models smoothly interpolate between word embedding, syntactic and semantic tasks, and future word embeddings. They call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP (natural language processing) tasks."
2408,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes a method for few-shot conditional image generation based on diffusion-decoding models with contrastive representations (D2C). D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. The method can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, the proposed method achieves superior performance over state-of-the-art VAEs and diffusion models."
2409,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes a method for few-shot conditional image generation based on diffusion-decoding models with contrastive representations (D2C). D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. The method can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, the proposed method achieves superior performance over state-of-the-art VAEs and diffusion models."
2410,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes a method for few-shot conditional image generation based on diffusion-decoding models with contrastive representations (D2C). D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. The method can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, the proposed method achieves superior performance over state-of-the-art VAEs and diffusion models."
2411,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes a method for few-shot conditional image generation based on diffusion-decoding models with contrastive representations (D2C). D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. The method can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, the proposed method achieves superior performance over state-of-the-art VAEs and diffusion models."
2412,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies the problem of self-supervised learning with contrastive learning, where the goal is to push positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. In contrast to prior work that assumes conditional independence of the positive pairs given the same label, this paper proposes a novel concept of the augmentation graph on data. The authors propose a loss that performs spectral decomposition on the population augmentmentation graph and can be succinctly written as a contrastive loss objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
2413,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies the problem of self-supervised learning with contrastive learning, where the goal is to push positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. In contrast to prior work that assumes conditional independence of the positive pairs given the same label, this paper proposes a novel concept of the augmentation graph on data. The authors propose a loss that performs spectral decomposition on the population augmentmentation graph and can be succinctly written as a contrastive loss objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
2414,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies the problem of self-supervised learning with contrastive learning, where the goal is to push positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. In contrast to prior work that assumes conditional independence of the positive pairs given the same label, this paper proposes a novel concept of the augmentation graph on data. The authors propose a loss that performs spectral decomposition on the population augmentmentation graph and can be succinctly written as a contrastive loss objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
2415,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies the problem of self-supervised learning with contrastive learning, where the goal is to push positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. In contrast to prior work that assumes conditional independence of the positive pairs given the same label, this paper proposes a novel concept of the augmentation graph on data. The authors propose a loss that performs spectral decomposition on the population augmentmentation graph and can be succinctly written as a contrastive loss objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
2416,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. In particular, they prove that if an additive representation can be used instead then BNSL becomes fixed- parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone."
2417,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. In particular, they prove that if an additive representation can be used instead then BNSL becomes fixed- parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone."
2418,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. In particular, they prove that if an additive representation can be used instead then BNSL becomes fixed- parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone."
2419,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. In particular, they prove that if an additive representation can be used instead then BNSL becomes fixed- parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone."
2420,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification. The proposed algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a resulting set of labeled and weak-labeled points. The theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate that the proposed algorithm outperforms standard baselines, including the Margin Algorithm, or Uncertainty Sampling."
2421,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification. The proposed algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a resulting set of labeled and weak-labeled points. The theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate that the proposed algorithm outperforms standard baselines, including the Margin Algorithm, or Uncertainty Sampling."
2422,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification. The proposed algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a resulting set of labeled and weak-labeled points. The theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate that the proposed algorithm outperforms standard baselines, including the Margin Algorithm, or Uncertainty Sampling."
2423,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification. The proposed algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a resulting set of labeled and weak-labeled points. The theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate that the proposed algorithm outperforms standard baselines, including the Margin Algorithm, or Uncertainty Sampling."
2424,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov Growth (KG), which can be used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on these bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
2425,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov Growth (KG), which can be used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on these bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
2426,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov Growth (KG), which can be used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on these bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
2427,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov Growth (KG), which can be used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on these bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
2428,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised image representation learning that uses two regularization terms to prevent the collapse of the embedding vectors produced by encoders fed with different views of the same image. The first term is a term that maintains the variance of each embedding dimension above a threshold, and the second term is an additional term that decorrelates each pair of variables. The proposed method does not require weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state-of-the-art on several downstream tasks."
2429,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised image representation learning that uses two regularization terms to prevent the collapse of the embedding vectors produced by encoders fed with different views of the same image. The first term is a term that maintains the variance of each embedding dimension above a threshold, and the second term is an additional term that decorrelates each pair of variables. The proposed method does not require weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state-of-the-art on several downstream tasks."
2430,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised image representation learning that uses two regularization terms to prevent the collapse of the embedding vectors produced by encoders fed with different views of the same image. The first term is a term that maintains the variance of each embedding dimension above a threshold, and the second term is an additional term that decorrelates each pair of variables. The proposed method does not require weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state-of-the-art on several downstream tasks."
2431,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised image representation learning that uses two regularization terms to prevent the collapse of the embedding vectors produced by encoders fed with different views of the same image. The first term is a term that maintains the variance of each embedding dimension above a threshold, and the second term is an additional term that decorrelates each pair of variables. The proposed method does not require weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state-of-the-art on several downstream tasks."
2432,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes a method for active reinforcement learning where the agent obtains information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. The proposed method, Information Directed Reward Learning (IDRL), uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. It achieves similar or better performance with significantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model."
2433,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes a method for active reinforcement learning where the agent obtains information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. The proposed method, Information Directed Reward Learning (IDRL), uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. It achieves similar or better performance with significantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model."
2434,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes a method for active reinforcement learning where the agent obtains information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. The proposed method, Information Directed Reward Learning (IDRL), uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. It achieves similar or better performance with significantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model."
2435,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes a method for active reinforcement learning where the agent obtains information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. The proposed method, Information Directed Reward Learning (IDRL), uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. It achieves similar or better performance with significantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model."
2436,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction of neural networks. The method is based on graph neural networks (GNNs) and is able to predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks."
2437,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction of neural networks. The method is based on graph neural networks (GNNs) and is able to predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks."
2438,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction of neural networks. The method is based on graph neural networks (GNNs) and is able to predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks."
2439,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction of neural networks. The method is based on graph neural networks (GNNs) and is able to predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks."
2440,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff. The authors derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. They prove that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, they further provide an expression for such estimators. For general distributions, they show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint."
2441,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff. The authors derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. They prove that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, they further provide an expression for such estimators. For general distributions, they show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint."
2442,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff. The authors derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. They prove that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, they further provide an expression for such estimators. For general distributions, they show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint."
2443,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff. The authors derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. They prove that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, they further provide an expression for such estimators. For general distributions, they show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint."
2444,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a new architecture for representation learning on textual graphs. The proposed architecture is based on the cascaded model architecture, where the text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency."
2445,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a new architecture for representation learning on textual graphs. The proposed architecture is based on the cascaded model architecture, where the text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency."
2446,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a new architecture for representation learning on textual graphs. The proposed architecture is based on the cascaded model architecture, where the text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency."
2447,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a new architecture for representation learning on textual graphs. The proposed architecture is based on the cascaded model architecture, where the text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency."
2448,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy (DP) in the context of empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy. In particular, the authors consider the case of mean estimation, where the privacy is guaranteed for a user's entire contribution (m ≥ 1 samples), rather than the individual samples. The authors show that for high-dimensional mean estimation with smooth loss, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy costs decrease at a faster $O(\frac{1/n})$ rate. They also provide lower bounds showing the minimax optimality of their algorithms for mean estimation and stochastically convex optimality."
2449,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy (DP) in the context of empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy. In particular, the authors consider the case of mean estimation, where the privacy is guaranteed for a user's entire contribution (m ≥ 1 samples), rather than the individual samples. The authors show that for high-dimensional mean estimation with smooth loss, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy costs decrease at a faster $O(\frac{1/n})$ rate. They also provide lower bounds showing the minimax optimality of their algorithms for mean estimation and stochastically convex optimality."
2450,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy (DP) in the context of empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy. In particular, the authors consider the case of mean estimation, where the privacy is guaranteed for a user's entire contribution (m ≥ 1 samples), rather than the individual samples. The authors show that for high-dimensional mean estimation with smooth loss, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy costs decrease at a faster $O(\frac{1/n})$ rate. They also provide lower bounds showing the minimax optimality of their algorithms for mean estimation and stochastically convex optimality."
2451,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy (DP) in the context of empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy. In particular, the authors consider the case of mean estimation, where the privacy is guaranteed for a user's entire contribution (m ≥ 1 samples), rather than the individual samples. The authors show that for high-dimensional mean estimation with smooth loss, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy costs decrease at a faster $O(\frac{1/n})$ rate. They also provide lower bounds showing the minimax optimality of their algorithms for mean estimation and stochastically convex optimality."
2452,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper proposes a self-consistent Gaussian process theory for deep neural networks (DNNs) trained with noisy gradient descent on a large training set. The main contribution of the paper is to derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. The authors show that this theory is applicable to a toy model of a two-layer linear convolutional neural network (CNN) trained on CIFAR-10. They also show that there is a sharp transition between a feature learning regime and a lazy learning regime in this model.
2453,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper proposes a self-consistent Gaussian process theory for deep neural networks (DNNs) trained with noisy gradient descent on a large training set. The main contribution of the paper is to derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. The authors show that this theory is applicable to a toy model of a two-layer linear convolutional neural network (CNN) trained on CIFAR-10. They also show that there is a sharp transition between a feature learning regime and a lazy learning regime in this model.
2454,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper proposes a self-consistent Gaussian process theory for deep neural networks (DNNs) trained with noisy gradient descent on a large training set. The main contribution of the paper is to derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. The authors show that this theory is applicable to a toy model of a two-layer linear convolutional neural network (CNN) trained on CIFAR-10. They also show that there is a sharp transition between a feature learning regime and a lazy learning regime in this model.
2455,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper proposes a self-consistent Gaussian process theory for deep neural networks (DNNs) trained with noisy gradient descent on a large training set. The main contribution of the paper is to derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. The authors show that this theory is applicable to a toy model of a two-layer linear convolutional neural network (CNN) trained on CIFAR-10. They also show that there is a sharp transition between a feature learning regime and a lazy learning regime in this model.
2456,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the role of inductive biases on both the training framework and the data in the development of compositional communication. Specifically, the authors show that inductive bias on the model and the training data are needed to develop a compositional signal. The authors also show that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence."
2457,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the role of inductive biases on both the training framework and the data in the development of compositional communication. Specifically, the authors show that inductive bias on the model and the training data are needed to develop a compositional signal. The authors also show that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence."
2458,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the role of inductive biases on both the training framework and the data in the development of compositional communication. Specifically, the authors show that inductive bias on the model and the training data are needed to develop a compositional signal. The authors also show that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence."
2459,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the role of inductive biases on both the training framework and the data in the development of compositional communication. Specifically, the authors show that inductive bias on the model and the training data are needed to develop a compositional signal. The authors also show that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence."
2460,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. "
2461,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. "
2462,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. "
2463,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. "
2464,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, the authors minimize the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e. the label of a point is given by which of k centers it is closest to in Euclidean distance – the authors show that one can achieve a loss that is independent of the total number of queries. They complement this result by showing that learning general convex sets requires an almost linear loss per query."
2465,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, the authors minimize the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e. the label of a point is given by which of k centers it is closest to in Euclidean distance – the authors show that one can achieve a loss that is independent of the total number of queries. They complement this result by showing that learning general convex sets requires an almost linear loss per query."
2466,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, the authors minimize the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e. the label of a point is given by which of k centers it is closest to in Euclidean distance – the authors show that one can achieve a loss that is independent of the total number of queries. They complement this result by showing that learning general convex sets requires an almost linear loss per query."
2467,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, the authors minimize the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e. the label of a point is given by which of k centers it is closest to in Euclidean distance – the authors show that one can achieve a loss that is independent of the total number of queries. They complement this result by showing that learning general convex sets requires an almost linear loss per query."
2468,SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a regularization term for Visual Question Anwering (VQA) that supervises the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. They also demonstrate the effectiveness of this approach experimentally on the GQA dataset."
2469,SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a regularization term for Visual Question Anwering (VQA) that supervises the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. They also demonstrate the effectiveness of this approach experimentally on the GQA dataset."
2470,SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a regularization term for Visual Question Anwering (VQA) that supervises the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. They also demonstrate the effectiveness of this approach experimentally on the GQA dataset."
2471,SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a regularization term for Visual Question Anwering (VQA) that supervises the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. They also demonstrate the effectiveness of this approach experimentally on the GQA dataset."
2472,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper proposes a memory module for RNNs. The memory module consists of a fixed number of neurons of fixed precision. The authors prove that a 54-neuron bounded-precision RNN with growing memory modules can simulate a Universal Turing Machine, with time complexity linear in the simulated machine’s time and independent of the memory size. "
2473,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper proposes a memory module for RNNs. The memory module consists of a fixed number of neurons of fixed precision. The authors prove that a 54-neuron bounded-precision RNN with growing memory modules can simulate a Universal Turing Machine, with time complexity linear in the simulated machine’s time and independent of the memory size. "
2474,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper proposes a memory module for RNNs. The memory module consists of a fixed number of neurons of fixed precision. The authors prove that a 54-neuron bounded-precision RNN with growing memory modules can simulate a Universal Turing Machine, with time complexity linear in the simulated machine’s time and independent of the memory size. "
2475,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper proposes a memory module for RNNs. The memory module consists of a fixed number of neurons of fixed precision. The authors prove that a 54-neuron bounded-precision RNN with growing memory modules can simulate a Universal Turing Machine, with time complexity linear in the simulated machine’s time and independent of the memory size. "
2476,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper studies the coverage of uncertainty estimation algorithms in learning quantiles. The authors prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for α > 0.5 and small d/n, the α-quantile learned by quantile regressors roughly achieves coverage α- (α-1/2) · d/N regardless of the noise distribution, where d is the input dimension and n is the number of training data. Experiments on simulated and real data verify the theory."
2477,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper studies the coverage of uncertainty estimation algorithms in learning quantiles. The authors prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for α > 0.5 and small d/n, the α-quantile learned by quantile regressors roughly achieves coverage α- (α-1/2) · d/N regardless of the noise distribution, where d is the input dimension and n is the number of training data. Experiments on simulated and real data verify the theory."
2478,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper studies the coverage of uncertainty estimation algorithms in learning quantiles. The authors prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for α > 0.5 and small d/n, the α-quantile learned by quantile regressors roughly achieves coverage α- (α-1/2) · d/N regardless of the noise distribution, where d is the input dimension and n is the number of training data. Experiments on simulated and real data verify the theory."
2479,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper studies the coverage of uncertainty estimation algorithms in learning quantiles. The authors prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for α > 0.5 and small d/n, the α-quantile learned by quantile regressors roughly achieves coverage α- (α-1/2) · d/N regardless of the noise distribution, where d is the input dimension and n is the number of training data. Experiments on simulated and real data verify the theory."
2480,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the class-incremental learning (CIL) framework. The main idea is to use reinforcement learning to train a policy that allocates memory for old and new classes. The policy is trained on pseudo-CIL tasks, and then applied to target tasks. The proposed method is evaluated on CIFAR-100, ImageNet-Subset, and POD+AANets."
2481,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the class-incremental learning (CIL) framework. The main idea is to use reinforcement learning to train a policy that allocates memory for old and new classes. The policy is trained on pseudo-CIL tasks, and then applied to target tasks. The proposed method is evaluated on CIFAR-100, ImageNet-Subset, and POD+AANets."
2482,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the class-incremental learning (CIL) framework. The main idea is to use reinforcement learning to train a policy that allocates memory for old and new classes. The policy is trained on pseudo-CIL tasks, and then applied to target tasks. The proposed method is evaluated on CIFAR-100, ImageNet-Subset, and POD+AANets."
2483,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the class-incremental learning (CIL) framework. The main idea is to use reinforcement learning to train a policy that allocates memory for old and new classes. The policy is trained on pseudo-CIL tasks, and then applied to target tasks. The proposed method is evaluated on CIFAR-100, ImageNet-Subset, and POD+AANets."
2484,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,This paper considers the problem of speeding up SGD by parallelizing it across multiple workers. The authors propose a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. The analysis shows that this can achieve an error that scales as 1/(NT) with a number of communications that is completely independent of T. Empirical evidence suggests this bound is close to tight as well.
2485,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,This paper considers the problem of speeding up SGD by parallelizing it across multiple workers. The authors propose a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. The analysis shows that this can achieve an error that scales as 1/(NT) with a number of communications that is completely independent of T. Empirical evidence suggests this bound is close to tight as well.
2486,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,This paper considers the problem of speeding up SGD by parallelizing it across multiple workers. The authors propose a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. The analysis shows that this can achieve an error that scales as 1/(NT) with a number of communications that is completely independent of T. Empirical evidence suggests this bound is close to tight as well.
2487,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,This paper considers the problem of speeding up SGD by parallelizing it across multiple workers. The authors propose a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. The analysis shows that this can achieve an error that scales as 1/(NT) with a number of communications that is completely independent of T. Empirical evidence suggests this bound is close to tight as well.
2488,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the online lazy gradient descent algorithm for strongly convex optimization. The authors show that the algorithm is universal in the sense that it also achieves O(logN) expected regret against i.i.d opponents. This improves upon the more complex metaalgorithm of Huang et al [20] that only gets O(\sqrt{N}) and O(\logN). The authors also show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent."
2489,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the online lazy gradient descent algorithm for strongly convex optimization. The authors show that the algorithm is universal in the sense that it also achieves O(logN) expected regret against i.i.d opponents. This improves upon the more complex metaalgorithm of Huang et al [20] that only gets O(\sqrt{N}) and O(\logN). The authors also show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent."
2490,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the online lazy gradient descent algorithm for strongly convex optimization. The authors show that the algorithm is universal in the sense that it also achieves O(logN) expected regret against i.i.d opponents. This improves upon the more complex metaalgorithm of Huang et al [20] that only gets O(\sqrt{N}) and O(\logN). The authors also show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent."
2491,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the online lazy gradient descent algorithm for strongly convex optimization. The authors show that the algorithm is universal in the sense that it also achieves O(logN) expected regret against i.i.d opponents. This improves upon the more complex metaalgorithm of Huang et al [20] that only gets O(\sqrt{N}) and O(\logN). The authors also show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent."
2492,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence of the Bures-Wasserstein (BW) geometry with the Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors first show that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. Then, they show that BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, they verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesics convex in the BW geometry."
2493,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence of the Bures-Wasserstein (BW) geometry with the Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors first show that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. Then, they show that BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, they verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesics convex in the BW geometry."
2494,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence of the Bures-Wasserstein (BW) geometry with the Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors first show that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. Then, they show that BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, they verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesics convex in the BW geometry."
2495,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence of the Bures-Wasserstein (BW) geometry with the Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors first show that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. Then, they show that BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, they verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesics convex in the BW geometry."
2496,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality."
2497,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality."
2498,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality."
2499,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality."
2500,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. The proposed model is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments are conducted on the chemistry lecture single speaker dataset and LRS2 multi speaker dataset."
2501,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. The proposed model is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments are conducted on the chemistry lecture single speaker dataset and LRS2 multi speaker dataset."
2502,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. The proposed model is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments are conducted on the chemistry lecture single speaker dataset and LRS2 multi speaker dataset."
2503,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. The proposed model is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments are conducted on the chemistry lecture single speaker dataset and LRS2 multi speaker dataset."
2504,SP:24ea12428bd675459f0509aa7cee821fa236382e,This paper proposes a method for split learning in medical imaging. The proposed method is based on the Vision Transformer architecture. The authors show that the proposed method can achieve comparable performance to data-centralized training.
2505,SP:24ea12428bd675459f0509aa7cee821fa236382e,This paper proposes a method for split learning in medical imaging. The proposed method is based on the Vision Transformer architecture. The authors show that the proposed method can achieve comparable performance to data-centralized training.
2506,SP:24ea12428bd675459f0509aa7cee821fa236382e,This paper proposes a method for split learning in medical imaging. The proposed method is based on the Vision Transformer architecture. The authors show that the proposed method can achieve comparable performance to data-centralized training.
2507,SP:24ea12428bd675459f0509aa7cee821fa236382e,This paper proposes a method for split learning in medical imaging. The proposed method is based on the Vision Transformer architecture. The authors show that the proposed method can achieve comparable performance to data-centralized training.
2508,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable point-to-mesh layer for 3D surface reconstruction. The main idea is to use the differentiable Poisson Surface Reconstruction (PSR) method to bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field. The paper shows that the proposed method is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Experiments are conducted on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction."
2509,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable point-to-mesh layer for 3D surface reconstruction. The main idea is to use the differentiable Poisson Surface Reconstruction (PSR) method to bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field. The paper shows that the proposed method is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Experiments are conducted on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction."
2510,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable point-to-mesh layer for 3D surface reconstruction. The main idea is to use the differentiable Poisson Surface Reconstruction (PSR) method to bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field. The paper shows that the proposed method is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Experiments are conducted on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction."
2511,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable point-to-mesh layer for 3D surface reconstruction. The main idea is to use the differentiable Poisson Surface Reconstruction (PSR) method to bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field. The paper shows that the proposed method is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Experiments are conducted on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction."
2512,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a method for recovering representations of corrupted images from a pre-trained representation learning network that operates on clean images, like CLIP. The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They evaluate on a subset of ImageNet and observe that their method is robust to varying levels of distortion. The proposed method outperforms end-to-end baselines even with a fraction of the labeled data."
2513,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a method for recovering representations of corrupted images from a pre-trained representation learning network that operates on clean images, like CLIP. The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They evaluate on a subset of ImageNet and observe that their method is robust to varying levels of distortion. The proposed method outperforms end-to-end baselines even with a fraction of the labeled data."
2514,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a method for recovering representations of corrupted images from a pre-trained representation learning network that operates on clean images, like CLIP. The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They evaluate on a subset of ImageNet and observe that their method is robust to varying levels of distortion. The proposed method outperforms end-to-end baselines even with a fraction of the labeled data."
2515,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a method for recovering representations of corrupted images from a pre-trained representation learning network that operates on clean images, like CLIP. The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They evaluate on a subset of ImageNet and observe that their method is robust to varying levels of distortion. The proposed method outperforms end-to-end baselines even with a fraction of the labeled data."
2516,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks. The authors formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions, and propose an off policy approach to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. They conclude by showing that these networks of agents can be more robust to correlated samples when learning online."
2517,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks. The authors formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions, and propose an off policy approach to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. They conclude by showing that these networks of agents can be more robust to correlated samples when learning online."
2518,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks. The authors formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions, and propose an off policy approach to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. They conclude by showing that these networks of agents can be more robust to correlated samples when learning online."
2519,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks. The authors formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions, and propose an off policy approach to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. They conclude by showing that these networks of agents can be more robust to correlated samples when learning online."
2520,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper proposes a self-supervised predictive loss function to model both the ventral and dorsal pathways of the mouse visual cortex. The authors show that the proposed method is able to outperform other models in fitting the visual cortex of mice. The main contribution of the paper is that the authors propose to train a deep neural network architecture with two parallel pathways using a self supervised predictive loss functions, and show that it can outperform the other models. The experiments are conducted on a variety of rodent datasets, and the results show that this method can account for some of the functional specialization seen in mammalian visual systems."
2521,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper proposes a self-supervised predictive loss function to model both the ventral and dorsal pathways of the mouse visual cortex. The authors show that the proposed method is able to outperform other models in fitting the visual cortex of mice. The main contribution of the paper is that the authors propose to train a deep neural network architecture with two parallel pathways using a self supervised predictive loss functions, and show that it can outperform the other models. The experiments are conducted on a variety of rodent datasets, and the results show that this method can account for some of the functional specialization seen in mammalian visual systems."
2522,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper proposes a self-supervised predictive loss function to model both the ventral and dorsal pathways of the mouse visual cortex. The authors show that the proposed method is able to outperform other models in fitting the visual cortex of mice. The main contribution of the paper is that the authors propose to train a deep neural network architecture with two parallel pathways using a self supervised predictive loss functions, and show that it can outperform the other models. The experiments are conducted on a variety of rodent datasets, and the results show that this method can account for some of the functional specialization seen in mammalian visual systems."
2523,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper proposes a self-supervised predictive loss function to model both the ventral and dorsal pathways of the mouse visual cortex. The authors show that the proposed method is able to outperform other models in fitting the visual cortex of mice. The main contribution of the paper is that the authors propose to train a deep neural network architecture with two parallel pathways using a self supervised predictive loss functions, and show that it can outperform the other models. The experiments are conducted on a variety of rodent datasets, and the results show that this method can account for some of the functional specialization seen in mammalian visual systems."
2524,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model that can incorporate prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
2525,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model that can incorporate prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
2526,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model that can incorporate prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
2527,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model that can incorporate prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
2528,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes a pretraining method for image-level contrastive representation learning for object detection. The pretraining is based on the selective search bounding boxes as object proposals, and the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN). The proposed method is equipped with object detection properties such as object-level translation invariance and scale invariance. The proposed pretraining algorithm achieves state-of-the-art results on COCO detection using a Mask R-CNN framework."
2529,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes a pretraining method for image-level contrastive representation learning for object detection. The pretraining is based on the selective search bounding boxes as object proposals, and the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN). The proposed method is equipped with object detection properties such as object-level translation invariance and scale invariance. The proposed pretraining algorithm achieves state-of-the-art results on COCO detection using a Mask R-CNN framework."
2530,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes a pretraining method for image-level contrastive representation learning for object detection. The pretraining is based on the selective search bounding boxes as object proposals, and the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN). The proposed method is equipped with object detection properties such as object-level translation invariance and scale invariance. The proposed pretraining algorithm achieves state-of-the-art results on COCO detection using a Mask R-CNN framework."
2531,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes a pretraining method for image-level contrastive representation learning for object detection. The pretraining is based on the selective search bounding boxes as object proposals, and the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN). The proposed method is equipped with object detection properties such as object-level translation invariance and scale invariance. The proposed pretraining algorithm achieves state-of-the-art results on COCO detection using a Mask R-CNN framework."
2532,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper proposes a method for solving vehicle routing problems (VRPs). The proposed method is based on a transformer-based method that learns to select subproblems to improve the solution of a black box subsolver. The method is evaluated on a variety of VRP distributions, variants, and solvers. The results show that the proposed method can improve the performance of existing VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000."
2533,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper proposes a method for solving vehicle routing problems (VRPs). The proposed method is based on a transformer-based method that learns to select subproblems to improve the solution of a black box subsolver. The method is evaluated on a variety of VRP distributions, variants, and solvers. The results show that the proposed method can improve the performance of existing VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000."
2534,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper proposes a method for solving vehicle routing problems (VRPs). The proposed method is based on a transformer-based method that learns to select subproblems to improve the solution of a black box subsolver. The method is evaluated on a variety of VRP distributions, variants, and solvers. The results show that the proposed method can improve the performance of existing VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000."
2535,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper proposes a method for solving vehicle routing problems (VRPs). The proposed method is based on a transformer-based method that learns to select subproblems to improve the solution of a black box subsolver. The method is evaluated on a variety of VRP distributions, variants, and solvers. The results show that the proposed method can improve the performance of existing VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000."
2536,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The idea is to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. The method is based on Bayesian continual learning, and the authors propose to dynamically expand parameters to learn each new task and then selectively combine them, which is formally consistent with the underlying mechanism of biological active forgetting, through regulating the learning-triggered synaptic expansion and synaptic convergence. Experiments on CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks demonstrate the effectiveness of the proposed method."
2537,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The idea is to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. The method is based on Bayesian continual learning, and the authors propose to dynamically expand parameters to learn each new task and then selectively combine them, which is formally consistent with the underlying mechanism of biological active forgetting, through regulating the learning-triggered synaptic expansion and synaptic convergence. Experiments on CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks demonstrate the effectiveness of the proposed method."
2538,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The idea is to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. The method is based on Bayesian continual learning, and the authors propose to dynamically expand parameters to learn each new task and then selectively combine them, which is formally consistent with the underlying mechanism of biological active forgetting, through regulating the learning-triggered synaptic expansion and synaptic convergence. Experiments on CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks demonstrate the effectiveness of the proposed method."
2539,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The idea is to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. The method is based on Bayesian continual learning, and the authors propose to dynamically expand parameters to learn each new task and then selectively combine them, which is formally consistent with the underlying mechanism of biological active forgetting, through regulating the learning-triggered synaptic expansion and synaptic convergence. Experiments on CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks demonstrate the effectiveness of the proposed method."
2540,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided random gradient-free (PRGF) algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for PRGF and propose a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
2541,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided random gradient-free (PRGF) algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for PRGF and propose a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
2542,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided random gradient-free (PRGF) algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for PRGF and propose a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
2543,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided random gradient-free (PRGF) algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for PRGF and propose a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
2544,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH), which states that learning on a properly pruned neural network (the winning ticket) improves test accuracy over the original unpruned network. The authors analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error. They show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned network is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer."
2545,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH), which states that learning on a properly pruned neural network (the winning ticket) improves test accuracy over the original unpruned network. The authors analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error. They show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned network is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer."
2546,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH), which states that learning on a properly pruned neural network (the winning ticket) improves test accuracy over the original unpruned network. The authors analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error. They show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned network is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer."
2547,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH), which states that learning on a properly pruned neural network (the winning ticket) improves test accuracy over the original unpruned network. The authors analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error. They show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned network is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer."
2548,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes two new methods for query release. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy."
2549,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes two new methods for query release. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy."
2550,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes two new methods for query release. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy."
2551,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes two new methods for query release. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy."
2552,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes MaskFormer, a simple mask classification model that predicts a set of binary masks, each associated with a single global class label prediction. The proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results."
2553,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes MaskFormer, a simple mask classification model that predicts a set of binary masks, each associated with a single global class label prediction. The proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results."
2554,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes MaskFormer, a simple mask classification model that predicts a set of binary masks, each associated with a single global class label prediction. The proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results."
2555,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes MaskFormer, a simple mask classification model that predicts a set of binary masks, each associated with a single global class label prediction. The proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results."
2556,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies adversarial examples on random undercomplete two-layer ReLU neural networks. The main contribution of this paper is to extend the result of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). The authors prove that a single step of gradient descent suffices. The authors also show this result for any sub-exponential width random neural network with smooth activation function."
2557,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies adversarial examples on random undercomplete two-layer ReLU neural networks. The main contribution of this paper is to extend the result of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). The authors prove that a single step of gradient descent suffices. The authors also show this result for any sub-exponential width random neural network with smooth activation function."
2558,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies adversarial examples on random undercomplete two-layer ReLU neural networks. The main contribution of this paper is to extend the result of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). The authors prove that a single step of gradient descent suffices. The authors also show this result for any sub-exponential width random neural network with smooth activation function."
2559,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies adversarial examples on random undercomplete two-layer ReLU neural networks. The main contribution of this paper is to extend the result of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). The authors prove that a single step of gradient descent suffices. The authors also show this result for any sub-exponential width random neural network with smooth activation function."
2560,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that trains SGMs in a latent space, relying on the variational autoencoder framework. To enable training LSGMs end-to-end in a scalable and stable manner, the authors introduce a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on par with previous SGMs while outperforming them in sampling time by two orders of magnitude."
2561,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that trains SGMs in a latent space, relying on the variational autoencoder framework. To enable training LSGMs end-to-end in a scalable and stable manner, the authors introduce a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on par with previous SGMs while outperforming them in sampling time by two orders of magnitude."
2562,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that trains SGMs in a latent space, relying on the variational autoencoder framework. To enable training LSGMs end-to-end in a scalable and stable manner, the authors introduce a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on par with previous SGMs while outperforming them in sampling time by two orders of magnitude."
2563,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that trains SGMs in a latent space, relying on the variational autoencoder framework. To enable training LSGMs end-to-end in a scalable and stable manner, the authors introduce a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on par with previous SGMs while outperforming them in sampling time by two orders of magnitude."
2564,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and neural tangent kernels in image classification. The main idea is that neural networks are able to find sparse signal in the presence of high-variance noise, while neural tangents are unable to adapt to the signal in this manner. The authors prove that, for a simple data distribution with sparse signal amidst high variance noise, a simple convolutional neural network trained using stochastic gradient descent simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding Neural tangent kernel, with a fixed set of predetermined features, is not able to adapt. The theoretical results are supported by experiments on CIFAR-10 and MNIST images with various backgrounds."
2565,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and neural tangent kernels in image classification. The main idea is that neural networks are able to find sparse signal in the presence of high-variance noise, while neural tangents are unable to adapt to the signal in this manner. The authors prove that, for a simple data distribution with sparse signal amidst high variance noise, a simple convolutional neural network trained using stochastic gradient descent simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding Neural tangent kernel, with a fixed set of predetermined features, is not able to adapt. The theoretical results are supported by experiments on CIFAR-10 and MNIST images with various backgrounds."
2566,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and neural tangent kernels in image classification. The main idea is that neural networks are able to find sparse signal in the presence of high-variance noise, while neural tangents are unable to adapt to the signal in this manner. The authors prove that, for a simple data distribution with sparse signal amidst high variance noise, a simple convolutional neural network trained using stochastic gradient descent simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding Neural tangent kernel, with a fixed set of predetermined features, is not able to adapt. The theoretical results are supported by experiments on CIFAR-10 and MNIST images with various backgrounds."
2567,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and neural tangent kernels in image classification. The main idea is that neural networks are able to find sparse signal in the presence of high-variance noise, while neural tangents are unable to adapt to the signal in this manner. The authors prove that, for a simple data distribution with sparse signal amidst high variance noise, a simple convolutional neural network trained using stochastic gradient descent simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding Neural tangent kernel, with a fixed set of predetermined features, is not able to adapt. The theoretical results are supported by experiments on CIFAR-10 and MNIST images with various backgrounds."
2568,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies gradient tracking (GT) algorithms for decentralized machine learning over a network where the training data is distributed across n agents, each of which can compute stochastic model updates on their local data. While gradient tracking algorithms can overcome a key challenge, namely accounting for differences between workers’ local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter p (related to the spectral gap of the connectivity matrix). This paper provides a tighter analysis of the GT method in the stochiastic strongly convex, convex and non-convex settings. The authors improve the dependency on p from O(p-2) to O(\p-1c-1) in the noiseless case and from O((p-3/2) -> O(P-1/2c-2)) in the general stochastically case. This improvement was possible due to a new proof technique which could be of independent interest."
2569,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies gradient tracking (GT) algorithms for decentralized machine learning over a network where the training data is distributed across n agents, each of which can compute stochastic model updates on their local data. While gradient tracking algorithms can overcome a key challenge, namely accounting for differences between workers’ local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter p (related to the spectral gap of the connectivity matrix). This paper provides a tighter analysis of the GT method in the stochiastic strongly convex, convex and non-convex settings. The authors improve the dependency on p from O(p-2) to O(\p-1c-1) in the noiseless case and from O((p-3/2) -> O(P-1/2c-2)) in the general stochastically case. This improvement was possible due to a new proof technique which could be of independent interest."
2570,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies gradient tracking (GT) algorithms for decentralized machine learning over a network where the training data is distributed across n agents, each of which can compute stochastic model updates on their local data. While gradient tracking algorithms can overcome a key challenge, namely accounting for differences between workers’ local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter p (related to the spectral gap of the connectivity matrix). This paper provides a tighter analysis of the GT method in the stochiastic strongly convex, convex and non-convex settings. The authors improve the dependency on p from O(p-2) to O(\p-1c-1) in the noiseless case and from O((p-3/2) -> O(P-1/2c-2)) in the general stochastically case. This improvement was possible due to a new proof technique which could be of independent interest."
2571,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies gradient tracking (GT) algorithms for decentralized machine learning over a network where the training data is distributed across n agents, each of which can compute stochastic model updates on their local data. While gradient tracking algorithms can overcome a key challenge, namely accounting for differences between workers’ local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter p (related to the spectral gap of the connectivity matrix). This paper provides a tighter analysis of the GT method in the stochiastic strongly convex, convex and non-convex settings. The authors improve the dependency on p from O(p-2) to O(\p-1c-1) in the noiseless case and from O((p-3/2) -> O(P-1/2c-2)) in the general stochastically case. This improvement was possible due to a new proof technique which could be of independent interest."
2572,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm in the multi-armed bandit (MAB) problem. The authors show that UCB is asymptotically deterministic, regardless of the problem complexity, and provide an alternative proof for the O p n log n minimax regret of UCB. The paper also provides a complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling."
2573,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm in the multi-armed bandit (MAB) problem. The authors show that UCB is asymptotically deterministic, regardless of the problem complexity, and provide an alternative proof for the O p n log n minimax regret of UCB. The paper also provides a complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling."
2574,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm in the multi-armed bandit (MAB) problem. The authors show that UCB is asymptotically deterministic, regardless of the problem complexity, and provide an alternative proof for the O p n log n minimax regret of UCB. The paper also provides a complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling."
2575,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm in the multi-armed bandit (MAB) problem. The authors show that UCB is asymptotically deterministic, regardless of the problem complexity, and provide an alternative proof for the O p n log n minimax regret of UCB. The paper also provides a complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling."
2576,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a method for cross-domain cold-start recommendation (CDCSR) based on Stein path alignment and proxy Stein path. Specifically, the authors propose to align the latent embedding distributions across domains, and then further propose its improved version, i.e., proxyStein path, which can reduce the operation consumption and improve efficiency. Experiments on Douban and Amazon datasets show that the proposed method outperforms the state-of-the-art models."
2577,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a method for cross-domain cold-start recommendation (CDCSR) based on Stein path alignment and proxy Stein path. Specifically, the authors propose to align the latent embedding distributions across domains, and then further propose its improved version, i.e., proxyStein path, which can reduce the operation consumption and improve efficiency. Experiments on Douban and Amazon datasets show that the proposed method outperforms the state-of-the-art models."
2578,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a method for cross-domain cold-start recommendation (CDCSR) based on Stein path alignment and proxy Stein path. Specifically, the authors propose to align the latent embedding distributions across domains, and then further propose its improved version, i.e., proxyStein path, which can reduce the operation consumption and improve efficiency. Experiments on Douban and Amazon datasets show that the proposed method outperforms the state-of-the-art models."
2579,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a method for cross-domain cold-start recommendation (CDCSR) based on Stein path alignment and proxy Stein path. Specifically, the authors propose to align the latent embedding distributions across domains, and then further propose its improved version, i.e., proxyStein path, which can reduce the operation consumption and improve efficiency. Experiments on Douban and Amazon datasets show that the proposed method outperforms the state-of-the-art models."
2580,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for vision transformers that replaces the self-attention layer with three key operations: 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transformer. The proposed architecture is simple and computationally efficient. The experimental results show that the proposed architecture can be competitive with transformer-style models and CNNs in efficiency, generalization ability and robustness."
2581,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for vision transformers that replaces the self-attention layer with three key operations: 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transformer. The proposed architecture is simple and computationally efficient. The experimental results show that the proposed architecture can be competitive with transformer-style models and CNNs in efficiency, generalization ability and robustness."
2582,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for vision transformers that replaces the self-attention layer with three key operations: 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transformer. The proposed architecture is simple and computationally efficient. The experimental results show that the proposed architecture can be competitive with transformer-style models and CNNs in efficiency, generalization ability and robustness."
2583,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for vision transformers that replaces the self-attention layer with three key operations: 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transformer. The proposed architecture is simple and computationally efficient. The experimental results show that the proposed architecture can be competitive with transformer-style models and CNNs in efficiency, generalization ability and robustness."
2584,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of trustworthiness on large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. The reasons are two-fold. Firstly, correct predictions are generally dominant over incorrect predictions. Secondly, due to the data complexity, it is challenging to differentiate the incorrect predictions from the correct ones on real-world large-sized datasets. To improve the generalizability of trustability predictors, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions from incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, namely Vision Transformer and ResNet."
2585,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of trustworthiness on large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. The reasons are two-fold. Firstly, correct predictions are generally dominant over incorrect predictions. Secondly, due to the data complexity, it is challenging to differentiate the incorrect predictions from the correct ones on real-world large-sized datasets. To improve the generalizability of trustability predictors, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions from incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, namely Vision Transformer and ResNet."
2586,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of trustworthiness on large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. The reasons are two-fold. Firstly, correct predictions are generally dominant over incorrect predictions. Secondly, due to the data complexity, it is challenging to differentiate the incorrect predictions from the correct ones on real-world large-sized datasets. To improve the generalizability of trustability predictors, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions from incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, namely Vision Transformer and ResNet."
2587,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of trustworthiness on large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. The reasons are two-fold. Firstly, correct predictions are generally dominant over incorrect predictions. Secondly, due to the data complexity, it is challenging to differentiate the incorrect predictions from the correct ones on real-world large-sized datasets. To improve the generalizability of trustability predictors, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions from incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, namely Vision Transformer and ResNet."
2588,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of adversarial adversarially robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
2589,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of adversarial adversarially robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
2590,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of adversarial adversarially robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
2591,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of adversarial adversarially robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
2592,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of adversarial adversarially robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
2593,SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a method for learning end-to-end unrolled reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled operator. The variational problem is then initialized with the output of the reconstructed network and solved iteratively till convergence. The authors demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that their approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with supervised data-driven reconstruction approaches."
2594,SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a method for learning end-to-end unrolled reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled operator. The variational problem is then initialized with the output of the reconstructed network and solved iteratively till convergence. The authors demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that their approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with supervised data-driven reconstruction approaches."
2595,SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a method for learning end-to-end unrolled reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled operator. The variational problem is then initialized with the output of the reconstructed network and solved iteratively till convergence. The authors demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that their approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with supervised data-driven reconstruction approaches."
2596,SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a method for learning end-to-end unrolled reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled operator. The variational problem is then initialized with the output of the reconstructed network and solved iteratively till convergence. The authors demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that their approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with supervised data-driven reconstruction approaches."
2597,SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a method for learning end-to-end unrolled reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled operator. The variational problem is then initialized with the output of the reconstructed network and solved iteratively till convergence. The authors demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that their approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with supervised data-driven reconstruction approaches."
2598,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. The authors provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. To improve learning from noisy web data, they propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model."
2599,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. The authors provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. To improve learning from noisy web data, they propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model."
2600,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. The authors provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. To improve learning from noisy web data, they propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model."
2601,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. The authors provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. To improve learning from noisy web data, they propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model."
2602,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. The authors provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. To improve learning from noisy web data, they propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model."
2603,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, they also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
2604,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, they also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
2605,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, they also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
2606,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, they also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
2607,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, they also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
2608,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the problem of non-smooth stochastic convex optimization with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-sparse convex stochastastic optimization problems. The authors also propose a novel stepsize rules for two methods with gradient clipping. The analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, an extension for strongly convex problems."
2609,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the problem of non-smooth stochastic convex optimization with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-sparse convex stochastastic optimization problems. The authors also propose a novel stepsize rules for two methods with gradient clipping. The analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, an extension for strongly convex problems."
2610,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the problem of non-smooth stochastic convex optimization with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-sparse convex stochastastic optimization problems. The authors also propose a novel stepsize rules for two methods with gradient clipping. The analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, an extension for strongly convex problems."
2611,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the problem of non-smooth stochastic convex optimization with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-sparse convex stochastastic optimization problems. The authors also propose a novel stepsize rules for two methods with gradient clipping. The analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, an extension for strongly convex problems."
2612,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the problem of non-smooth stochastic convex optimization with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-sparse convex stochastastic optimization problems. The authors also propose a novel stepsize rules for two methods with gradient clipping. The analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, an extension for strongly convex problems."
2613,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes a new architecture for long-term forecasting of time series. The proposed architecture is based on the Transformer architecture, but with an Auto-Correlation mechanism. The authors propose to use the series decomposition as a basic inner block of the model, and use the auto-correlation mechanism to discover the long-range dependencies. Experiments show that the proposed architecture outperforms self-attention in both efficiency and accuracy."
2614,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes a new architecture for long-term forecasting of time series. The proposed architecture is based on the Transformer architecture, but with an Auto-Correlation mechanism. The authors propose to use the series decomposition as a basic inner block of the model, and use the auto-correlation mechanism to discover the long-range dependencies. Experiments show that the proposed architecture outperforms self-attention in both efficiency and accuracy."
2615,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes a new architecture for long-term forecasting of time series. The proposed architecture is based on the Transformer architecture, but with an Auto-Correlation mechanism. The authors propose to use the series decomposition as a basic inner block of the model, and use the auto-correlation mechanism to discover the long-range dependencies. Experiments show that the proposed architecture outperforms self-attention in both efficiency and accuracy."
2616,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes a new architecture for long-term forecasting of time series. The proposed architecture is based on the Transformer architecture, but with an Auto-Correlation mechanism. The authors propose to use the series decomposition as a basic inner block of the model, and use the auto-correlation mechanism to discover the long-range dependencies. Experiments show that the proposed architecture outperforms self-attention in both efficiency and accuracy."
2617,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes a new architecture for long-term forecasting of time series. The proposed architecture is based on the Transformer architecture, but with an Auto-Correlation mechanism. The authors propose to use the series decomposition as a basic inner block of the model, and use the auto-correlation mechanism to discover the long-range dependencies. Experiments show that the proposed architecture outperforms self-attention in both efficiency and accuracy."
2618,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. The authors show that three non-neural approaches and T5, a state-of-the-art neural language model do not achieve good performance, and propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. They also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues."
2619,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. The authors show that three non-neural approaches and T5, a state-of-the-art neural language model do not achieve good performance, and propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. They also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues."
2620,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. The authors show that three non-neural approaches and T5, a state-of-the-art neural language model do not achieve good performance, and propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. They also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues."
2621,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. The authors show that three non-neural approaches and T5, a state-of-the-art neural language model do not achieve good performance, and propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. They also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues."
2622,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. The authors show that three non-neural approaches and T5, a state-of-the-art neural language model do not achieve good performance, and propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. They also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues."
2623,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the differences between ViTs and CNNs on image classification tasks. The authors find that ViTs have more uniform representations across all layers, and that self-attention plays a crucial role in early aggregation of global information, and ViT residual connections strongly propagate features from lower to higher layers. They also study the implications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
2624,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the differences between ViTs and CNNs on image classification tasks. The authors find that ViTs have more uniform representations across all layers, and that self-attention plays a crucial role in early aggregation of global information, and ViT residual connections strongly propagate features from lower to higher layers. They also study the implications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
2625,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the differences between ViTs and CNNs on image classification tasks. The authors find that ViTs have more uniform representations across all layers, and that self-attention plays a crucial role in early aggregation of global information, and ViT residual connections strongly propagate features from lower to higher layers. They also study the implications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
2626,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the differences between ViTs and CNNs on image classification tasks. The authors find that ViTs have more uniform representations across all layers, and that self-attention plays a crucial role in early aggregation of global information, and ViT residual connections strongly propagate features from lower to higher layers. They also study the implications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
2627,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the differences between ViTs and CNNs on image classification tasks. The authors find that ViTs have more uniform representations across all layers, and that self-attention plays a crucial role in early aggregation of global information, and ViT residual connections strongly propagate features from lower to higher layers. They also study the implications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
2628,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) combinatorial optimization problems. The authors provide a problem-dependent regret lower bound of order ⌦(log T/2) to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
2629,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) combinatorial optimization problems. The authors provide a problem-dependent regret lower bound of order ⌦(log T/2) to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
2630,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) combinatorial optimization problems. The authors provide a problem-dependent regret lower bound of order ⌦(log T/2) to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
2631,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) combinatorial optimization problems. The authors provide a problem-dependent regret lower bound of order ⌦(log T/2) to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
2632,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) combinatorial optimization problems. The authors provide a problem-dependent regret lower bound of order ⌦(log T/2) to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
2633,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in which the goal is to find optimal federating coalitions of agents. The authors provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyze the relationship between the stability and optimality of an arrangement. They show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal. Finally, they give the first constant-factor bound on the performance gap between stability and optimal."
2634,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in which the goal is to find optimal federating coalitions of agents. The authors provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyze the relationship between the stability and optimality of an arrangement. They show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal. Finally, they give the first constant-factor bound on the performance gap between stability and optimal."
2635,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in which the goal is to find optimal federating coalitions of agents. The authors provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyze the relationship between the stability and optimality of an arrangement. They show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal. Finally, they give the first constant-factor bound on the performance gap between stability and optimal."
2636,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in which the goal is to find optimal federating coalitions of agents. The authors provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyze the relationship between the stability and optimality of an arrangement. They show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal. Finally, they give the first constant-factor bound on the performance gap between stability and optimal."
2637,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in which the goal is to find optimal federating coalitions of agents. The authors provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyze the relationship between the stability and optimality of an arrangement. They show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal. Finally, they give the first constant-factor bound on the performance gap between stability and optimal."
2638,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The proposed method is based on permutation-equivariant attention, which is trained with pairs of randomly rotated objects. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning."
2639,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The proposed method is based on permutation-equivariant attention, which is trained with pairs of randomly rotated objects. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning."
2640,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The proposed method is based on permutation-equivariant attention, which is trained with pairs of randomly rotated objects. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning."
2641,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The proposed method is based on permutation-equivariant attention, which is trained with pairs of randomly rotated objects. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning."
2642,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The proposed method is based on permutation-equivariant attention, which is trained with pairs of randomly rotated objects. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning."
2643,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. The method uses black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
2644,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. The method uses black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
2645,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. The method uses black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
2646,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. The method uses black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
2647,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. The method uses black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
2648,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the effect of enforcing invariance in kernel ridge regression when the target is invariant to the action of a compact group. The authors derive a strictly non-zero generalization benefit of incorporating invariance for feature averaging. They study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In particular, the authors show that the invariance induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right."
2649,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the effect of enforcing invariance in kernel ridge regression when the target is invariant to the action of a compact group. The authors derive a strictly non-zero generalization benefit of incorporating invariance for feature averaging. They study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In particular, the authors show that the invariance induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right."
2650,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the effect of enforcing invariance in kernel ridge regression when the target is invariant to the action of a compact group. The authors derive a strictly non-zero generalization benefit of incorporating invariance for feature averaging. They study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In particular, the authors show that the invariance induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right."
2651,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the effect of enforcing invariance in kernel ridge regression when the target is invariant to the action of a compact group. The authors derive a strictly non-zero generalization benefit of incorporating invariance for feature averaging. They study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In particular, the authors show that the invariance induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right."
2652,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the effect of enforcing invariance in kernel ridge regression when the target is invariant to the action of a compact group. The authors derive a strictly non-zero generalization benefit of incorporating invariance for feature averaging. They study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In particular, the authors show that the invariance induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right."
2653,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. This allows a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. Experiments are conducted on out-of-distribution detection, adversarial example detection, and calibration."
2654,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. This allows a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. Experiments are conducted on out-of-distribution detection, adversarial example detection, and calibration."
2655,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. This allows a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. Experiments are conducted on out-of-distribution detection, adversarial example detection, and calibration."
2656,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. This allows a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. Experiments are conducted on out-of-distribution detection, adversarial example detection, and calibration."
2657,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. This allows a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. Experiments are conducted on out-of-distribution detection, adversarial example detection, and calibration."
2658,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators of optimal transport maps. The authors propose a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-based estimators. They provide rates of convergence for natural discretediscrete and semi-discrete estimators as well as kernel smoothed or wavelet based estimators and show that, under additional smoothness assumption of Sobolev type or Besov type, the proposed stability estimate can speed up the rate of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete discretes/semi-discretes. As a by-product, they also provide faster rates for Wasserstein distance between two probability distributions."
2659,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators of optimal transport maps. The authors propose a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-based estimators. They provide rates of convergence for natural discretediscrete and semi-discrete estimators as well as kernel smoothed or wavelet based estimators and show that, under additional smoothness assumption of Sobolev type or Besov type, the proposed stability estimate can speed up the rate of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete discretes/semi-discretes. As a by-product, they also provide faster rates for Wasserstein distance between two probability distributions."
2660,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators of optimal transport maps. The authors propose a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-based estimators. They provide rates of convergence for natural discretediscrete and semi-discrete estimators as well as kernel smoothed or wavelet based estimators and show that, under additional smoothness assumption of Sobolev type or Besov type, the proposed stability estimate can speed up the rate of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete discretes/semi-discretes. As a by-product, they also provide faster rates for Wasserstein distance between two probability distributions."
2661,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators of optimal transport maps. The authors propose a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-based estimators. They provide rates of convergence for natural discretediscrete and semi-discrete estimators as well as kernel smoothed or wavelet based estimators and show that, under additional smoothness assumption of Sobolev type or Besov type, the proposed stability estimate can speed up the rate of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete discretes/semi-discretes. As a by-product, they also provide faster rates for Wasserstein distance between two probability distributions."
2662,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators of optimal transport maps. The authors propose a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-based estimators. They provide rates of convergence for natural discretediscrete and semi-discrete estimators as well as kernel smoothed or wavelet based estimators and show that, under additional smoothness assumption of Sobolev type or Besov type, the proposed stability estimate can speed up the rate of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete discretes/semi-discretes. As a by-product, they also provide faster rates for Wasserstein distance between two probability distributions."
2663,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The proposed method is applied to 10 datapoints (0.02% of original dataset) and achieves over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The proposed methods extend across many other settings for MNIST, Fashion-MNIST, CIFar-10, CifAR-100, and SVHN. The authors perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data."
2664,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The proposed method is applied to 10 datapoints (0.02% of original dataset) and achieves over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The proposed methods extend across many other settings for MNIST, Fashion-MNIST, CIFar-10, CifAR-100, and SVHN. The authors perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data."
2665,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The proposed method is applied to 10 datapoints (0.02% of original dataset) and achieves over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The proposed methods extend across many other settings for MNIST, Fashion-MNIST, CIFar-10, CifAR-100, and SVHN. The authors perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data."
2666,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The proposed method is applied to 10 datapoints (0.02% of original dataset) and achieves over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The proposed methods extend across many other settings for MNIST, Fashion-MNIST, CIFar-10, CifAR-100, and SVHN. The authors perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data."
2667,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The proposed method is applied to 10 datapoints (0.02% of original dataset) and achieves over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The proposed methods extend across many other settings for MNIST, Fashion-MNIST, CIFar-10, CifAR-100, and SVHN. The authors perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data."
2668,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an open-set semi-supervised learning (OSSL) approach called OpenMatch. The proposed method unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data."
2669,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an open-set semi-supervised learning (OSSL) approach called OpenMatch. The proposed method unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data."
2670,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an open-set semi-supervised learning (OSSL) approach called OpenMatch. The proposed method unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data."
2671,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an open-set semi-supervised learning (OSSL) approach called OpenMatch. The proposed method unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data."
2672,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an open-set semi-supervised learning (OSSL) approach called OpenMatch. The proposed method unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data."
2673,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes a method for unsupervised goal reaching in robotic manipulation and locomotion tasks. The proposed method is based on the idea that an explorer and an achiever policy should be trained to explore unseen states, which are then used as diverse targets for the achiever to practice. The explorer policy is trained to reach previously visited states, and the goal is learned to reach unseen states through foresight. The authors show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation tasks."
2674,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes a method for unsupervised goal reaching in robotic manipulation and locomotion tasks. The proposed method is based on the idea that an explorer and an achiever policy should be trained to explore unseen states, which are then used as diverse targets for the achiever to practice. The explorer policy is trained to reach previously visited states, and the goal is learned to reach unseen states through foresight. The authors show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation tasks."
2675,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes a method for unsupervised goal reaching in robotic manipulation and locomotion tasks. The proposed method is based on the idea that an explorer and an achiever policy should be trained to explore unseen states, which are then used as diverse targets for the achiever to practice. The explorer policy is trained to reach previously visited states, and the goal is learned to reach unseen states through foresight. The authors show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation tasks."
2676,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes a method for unsupervised goal reaching in robotic manipulation and locomotion tasks. The proposed method is based on the idea that an explorer and an achiever policy should be trained to explore unseen states, which are then used as diverse targets for the achiever to practice. The explorer policy is trained to reach previously visited states, and the goal is learned to reach unseen states through foresight. The authors show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation tasks."
2677,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes a method for unsupervised goal reaching in robotic manipulation and locomotion tasks. The proposed method is based on the idea that an explorer and an achiever policy should be trained to explore unseen states, which are then used as diverse targets for the achiever to practice. The explorer policy is trained to reach previously visited states, and the goal is learned to reach unseen states through foresight. The authors show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation tasks."
2678,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,This paper proposes a method to reduce the number of trainable parameters in language models. The authors propose to use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical analysis is provided to support the proposed method. Experiments are conducted on Transformer models to demonstrate the effectiveness of the method.
2679,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,This paper proposes a method to reduce the number of trainable parameters in language models. The authors propose to use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical analysis is provided to support the proposed method. Experiments are conducted on Transformer models to demonstrate the effectiveness of the method.
2680,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,This paper proposes a method to reduce the number of trainable parameters in language models. The authors propose to use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical analysis is provided to support the proposed method. Experiments are conducted on Transformer models to demonstrate the effectiveness of the method.
2681,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,This paper proposes a method to reduce the number of trainable parameters in language models. The authors propose to use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical analysis is provided to support the proposed method. Experiments are conducted on Transformer models to demonstrate the effectiveness of the method.
2682,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,This paper proposes a method to reduce the number of trainable parameters in language models. The authors propose to use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical analysis is provided to support the proposed method. Experiments are conducted on Transformer models to demonstrate the effectiveness of the method.
2683,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes a new representation for source code summarization based on the Transformer architecture. The authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. Specifically, the authors use the idea of positional encoding and modify them to incorporate these path encoding. The experimental results show the effectiveness of the proposed method."
2684,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes a new representation for source code summarization based on the Transformer architecture. The authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. Specifically, the authors use the idea of positional encoding and modify them to incorporate these path encoding. The experimental results show the effectiveness of the proposed method."
2685,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes a new representation for source code summarization based on the Transformer architecture. The authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. Specifically, the authors use the idea of positional encoding and modify them to incorporate these path encoding. The experimental results show the effectiveness of the proposed method."
2686,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes a new representation for source code summarization based on the Transformer architecture. The authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. Specifically, the authors use the idea of positional encoding and modify them to incorporate these path encoding. The experimental results show the effectiveness of the proposed method."
2687,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes a new representation for source code summarization based on the Transformer architecture. The authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. Specifically, the authors use the idea of positional encoding and modify them to incorporate these path encoding. The experimental results show the effectiveness of the proposed method."
2688,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new method for high-resolution image generation based on GANs. The main idea is to replace the global self-attention in the low-resolution stage of the generative process with multi-axis blocked self attention, which allows efficient mixing of local and global attention. To further improve the performance, the authors introduce an additional self-modulation component based on cross attention. The experimental results show that the proposed HiT achieves state-of-the-art FID scores on unconditional ImageNet 128 × 128 and FFHQ 256 × 256."
2689,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new method for high-resolution image generation based on GANs. The main idea is to replace the global self-attention in the low-resolution stage of the generative process with multi-axis blocked self attention, which allows efficient mixing of local and global attention. To further improve the performance, the authors introduce an additional self-modulation component based on cross attention. The experimental results show that the proposed HiT achieves state-of-the-art FID scores on unconditional ImageNet 128 × 128 and FFHQ 256 × 256."
2690,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new method for high-resolution image generation based on GANs. The main idea is to replace the global self-attention in the low-resolution stage of the generative process with multi-axis blocked self attention, which allows efficient mixing of local and global attention. To further improve the performance, the authors introduce an additional self-modulation component based on cross attention. The experimental results show that the proposed HiT achieves state-of-the-art FID scores on unconditional ImageNet 128 × 128 and FFHQ 256 × 256."
2691,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new method for high-resolution image generation based on GANs. The main idea is to replace the global self-attention in the low-resolution stage of the generative process with multi-axis blocked self attention, which allows efficient mixing of local and global attention. To further improve the performance, the authors introduce an additional self-modulation component based on cross attention. The experimental results show that the proposed HiT achieves state-of-the-art FID scores on unconditional ImageNet 128 × 128 and FFHQ 256 × 256."
2692,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new method for high-resolution image generation based on GANs. The main idea is to replace the global self-attention in the low-resolution stage of the generative process with multi-axis blocked self attention, which allows efficient mixing of local and global attention. To further improve the performance, the authors introduce an additional self-modulation component based on cross attention. The experimental results show that the proposed HiT achieves state-of-the-art FID scores on unconditional ImageNet 128 × 128 and FFHQ 256 × 256."
2693,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors prove an upper bound that is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. They provide evidence that ground-truth communities in real-world graphs are often convex and empirically compare their proposed approach with other active learning algorithms.
2694,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors prove an upper bound that is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. They provide evidence that ground-truth communities in real-world graphs are often convex and empirically compare their proposed approach with other active learning algorithms.
2695,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors prove an upper bound that is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. They provide evidence that ground-truth communities in real-world graphs are often convex and empirically compare their proposed approach with other active learning algorithms.
2696,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors prove an upper bound that is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. They provide evidence that ground-truth communities in real-world graphs are often convex and empirically compare their proposed approach with other active learning algorithms.
2697,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors prove an upper bound that is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. They provide evidence that ground-truth communities in real-world graphs are often convex and empirically compare their proposed approach with other active learning algorithms.
2698,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL). Specifically, the proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution. Crucially, this enables the gradients to flow backwards through the video encoding conditioned on a TAL supervision loss, favourably solving the task discrepancy problem. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods."
2699,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL). Specifically, the proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution. Crucially, this enables the gradients to flow backwards through the video encoding conditioned on a TAL supervision loss, favourably solving the task discrepancy problem. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods."
2700,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL). Specifically, the proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution. Crucially, this enables the gradients to flow backwards through the video encoding conditioned on a TAL supervision loss, favourably solving the task discrepancy problem. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods."
2701,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL). Specifically, the proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution. Crucially, this enables the gradients to flow backwards through the video encoding conditioned on a TAL supervision loss, favourably solving the task discrepancy problem. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods."
2702,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL). Specifically, the proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution. Crucially, this enables the gradients to flow backwards through the video encoding conditioned on a TAL supervision loss, favourably solving the task discrepancy problem. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods."
2703,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The authors provide general formulae for the derivatives of regularized M-ESTIMators β̂(y,X) where differentiation is taken with respect to both y and X. They characterize the distribution of the residual ri = yi-xi β-epsilon in the intermediate high-dimensional regime where dimension and sample size are of the same order. They propose a novel adaptive criterion to select the tuning parameters of the regularization parameters. The proposed adaptive criterion does not require the knowledge of the noise distribution or of the covariance of the design."
2704,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The authors provide general formulae for the derivatives of regularized M-ESTIMators β̂(y,X) where differentiation is taken with respect to both y and X. They characterize the distribution of the residual ri = yi-xi β-epsilon in the intermediate high-dimensional regime where dimension and sample size are of the same order. They propose a novel adaptive criterion to select the tuning parameters of the regularization parameters. The proposed adaptive criterion does not require the knowledge of the noise distribution or of the covariance of the design."
2705,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The authors provide general formulae for the derivatives of regularized M-ESTIMators β̂(y,X) where differentiation is taken with respect to both y and X. They characterize the distribution of the residual ri = yi-xi β-epsilon in the intermediate high-dimensional regime where dimension and sample size are of the same order. They propose a novel adaptive criterion to select the tuning parameters of the regularization parameters. The proposed adaptive criterion does not require the knowledge of the noise distribution or of the covariance of the design."
2706,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The authors provide general formulae for the derivatives of regularized M-ESTIMators β̂(y,X) where differentiation is taken with respect to both y and X. They characterize the distribution of the residual ri = yi-xi β-epsilon in the intermediate high-dimensional regime where dimension and sample size are of the same order. They propose a novel adaptive criterion to select the tuning parameters of the regularization parameters. The proposed adaptive criterion does not require the knowledge of the noise distribution or of the covariance of the design."
2707,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The authors provide general formulae for the derivatives of regularized M-ESTIMators β̂(y,X) where differentiation is taken with respect to both y and X. They characterize the distribution of the residual ri = yi-xi β-epsilon in the intermediate high-dimensional regime where dimension and sample size are of the same order. They propose a novel adaptive criterion to select the tuning parameters of the regularization parameters. The proposed adaptive criterion does not require the knowledge of the noise distribution or of the covariance of the design."
2708,SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a beam-based method for summarizing text. The proposed method is based on the idea that the attention distribution should be predicted before inference, which allows step-wise improvements on the beam search through the global scoring mechanism. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
2709,SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a beam-based method for summarizing text. The proposed method is based on the idea that the attention distribution should be predicted before inference, which allows step-wise improvements on the beam search through the global scoring mechanism. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
2710,SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a beam-based method for summarizing text. The proposed method is based on the idea that the attention distribution should be predicted before inference, which allows step-wise improvements on the beam search through the global scoring mechanism. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
2711,SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a beam-based method for summarizing text. The proposed method is based on the idea that the attention distribution should be predicted before inference, which allows step-wise improvements on the beam search through the global scoring mechanism. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
2712,SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a beam-based method for summarizing text. The proposed method is based on the idea that the attention distribution should be predicted before inference, which allows step-wise improvements on the beam search through the global scoring mechanism. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
2713,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a self-attention mechanism for manifolds. The key idea is to use a gauge equivariant transformer to make the model agnostic to the orientation of local coordinate systems (i.e., gauge-equivariant). To achieve this, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields. In addition, they project the position vector of each point onto its local coordinate system to disentangle the position of the coordinate system in ambient space. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
2714,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a self-attention mechanism for manifolds. The key idea is to use a gauge equivariant transformer to make the model agnostic to the orientation of local coordinate systems (i.e., gauge-equivariant). To achieve this, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields. In addition, they project the position vector of each point onto its local coordinate system to disentangle the position of the coordinate system in ambient space. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
2715,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a self-attention mechanism for manifolds. The key idea is to use a gauge equivariant transformer to make the model agnostic to the orientation of local coordinate systems (i.e., gauge-equivariant). To achieve this, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields. In addition, they project the position vector of each point onto its local coordinate system to disentangle the position of the coordinate system in ambient space. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
2716,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a self-attention mechanism for manifolds. The key idea is to use a gauge equivariant transformer to make the model agnostic to the orientation of local coordinate systems (i.e., gauge-equivariant). To achieve this, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields. In addition, they project the position vector of each point onto its local coordinate system to disentangle the position of the coordinate system in ambient space. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
2717,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a self-attention mechanism for manifolds. The key idea is to use a gauge equivariant transformer to make the model agnostic to the orientation of local coordinate systems (i.e., gauge-equivariant). To achieve this, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields. In addition, they project the position vector of each point onto its local coordinate system to disentangle the position of the coordinate system in ambient space. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
2718,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to combine the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of their method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations."
2719,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to combine the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of their method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations."
2720,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to combine the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of their method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations."
2721,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to combine the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of their method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations."
2722,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to combine the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of their method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations."
2723,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training neural networks with completely sparse forward and backward passes. The authors formulate the training process as a continuous minimization problem under global sparsity constraint. They then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, they use the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation."
2724,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training neural networks with completely sparse forward and backward passes. The authors formulate the training process as a continuous minimization problem under global sparsity constraint. They then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, they use the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation."
2725,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training neural networks with completely sparse forward and backward passes. The authors formulate the training process as a continuous minimization problem under global sparsity constraint. They then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, they use the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation."
2726,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training neural networks with completely sparse forward and backward passes. The authors formulate the training process as a continuous minimization problem under global sparsity constraint. They then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, they use the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation."
2727,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training neural networks with completely sparse forward and backward passes. The authors formulate the training process as a continuous minimization problem under global sparsity constraint. They then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, they use the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation."
2728,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on Non-Equilibrium Orbits (NEO). The main idea is to combine elements from the forward and backward Orbits through points sampled from a proposal distribution ρ. The authors provide unbiased estimators of the normalizing constant and self-normalized IS estimators for expectations under π. They also provide explicit mixing time estimates under mild conditions.
2729,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on Non-Equilibrium Orbits (NEO). The main idea is to combine elements from the forward and backward Orbits through points sampled from a proposal distribution ρ. The authors provide unbiased estimators of the normalizing constant and self-normalized IS estimators for expectations under π. They also provide explicit mixing time estimates under mild conditions.
2730,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on Non-Equilibrium Orbits (NEO). The main idea is to combine elements from the forward and backward Orbits through points sampled from a proposal distribution ρ. The authors provide unbiased estimators of the normalizing constant and self-normalized IS estimators for expectations under π. They also provide explicit mixing time estimates under mild conditions.
2731,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on Non-Equilibrium Orbits (NEO). The main idea is to combine elements from the forward and backward Orbits through points sampled from a proposal distribution ρ. The authors provide unbiased estimators of the normalizing constant and self-normalized IS estimators for expectations under π. They also provide explicit mixing time estimates under mild conditions.
2732,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on Non-Equilibrium Orbits (NEO). The main idea is to combine elements from the forward and backward Orbits through points sampled from a proposal distribution ρ. The authors provide unbiased estimators of the normalizing constant and self-normalized IS estimators for expectations under π. They also provide explicit mixing time estimates under mild conditions.
2733,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a new property termed Mini-Batch Consistency (MBC) that is required for large scale mini-batch set encoding. The proposed method adheres to the required symmetries of invariance and equivariance as well as maintaining MBC for any partition of the input set. Additionally, the proposed method is scalable and efficient attention-based set encoding mechanism that is amenable to mini- batch processing of sets, and capable of updating set representations as data arrives."
2734,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a new property termed Mini-Batch Consistency (MBC) that is required for large scale mini-batch set encoding. The proposed method adheres to the required symmetries of invariance and equivariance as well as maintaining MBC for any partition of the input set. Additionally, the proposed method is scalable and efficient attention-based set encoding mechanism that is amenable to mini- batch processing of sets, and capable of updating set representations as data arrives."
2735,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a new property termed Mini-Batch Consistency (MBC) that is required for large scale mini-batch set encoding. The proposed method adheres to the required symmetries of invariance and equivariance as well as maintaining MBC for any partition of the input set. Additionally, the proposed method is scalable and efficient attention-based set encoding mechanism that is amenable to mini- batch processing of sets, and capable of updating set representations as data arrives."
2736,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a new property termed Mini-Batch Consistency (MBC) that is required for large scale mini-batch set encoding. The proposed method adheres to the required symmetries of invariance and equivariance as well as maintaining MBC for any partition of the input set. Additionally, the proposed method is scalable and efficient attention-based set encoding mechanism that is amenable to mini- batch processing of sets, and capable of updating set representations as data arrives."
2737,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a new property termed Mini-Batch Consistency (MBC) that is required for large scale mini-batch set encoding. The proposed method adheres to the required symmetries of invariance and equivariance as well as maintaining MBC for any partition of the input set. Additionally, the proposed method is scalable and efficient attention-based set encoding mechanism that is amenable to mini- batch processing of sets, and capable of updating set representations as data arrives."
2738,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes a method for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy. The paper presents evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
2739,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes a method for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy. The paper presents evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
2740,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes a method for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy. The paper presents evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
2741,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes a method for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy. The paper presents evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
2742,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes a method for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy. The paper presents evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
2743,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors argue that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. To address this issue, the authors propose to automatically learn shared and specialized attention heads for different languages. The proposed attention sharing strategies are evaluated in various tasks including speech recognition, text-to-text and speech to-text translation."
2744,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors argue that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. To address this issue, the authors propose to automatically learn shared and specialized attention heads for different languages. The proposed attention sharing strategies are evaluated in various tasks including speech recognition, text-to-text and speech to-text translation."
2745,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors argue that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. To address this issue, the authors propose to automatically learn shared and specialized attention heads for different languages. The proposed attention sharing strategies are evaluated in various tasks including speech recognition, text-to-text and speech to-text translation."
2746,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors argue that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. To address this issue, the authors propose to automatically learn shared and specialized attention heads for different languages. The proposed attention sharing strategies are evaluated in various tasks including speech recognition, text-to-text and speech to-text translation."
2747,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors argue that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. To address this issue, the authors propose to automatically learn shared and specialized attention heads for different languages. The proposed attention sharing strategies are evaluated in various tasks including speech recognition, text-to-text and speech to-text translation."
2748,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift. The authors show that the limiting test error, bias, and variance in this setting can be explained by a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. Moreover, the analysis reveals an exact linear relationship between the in-distribution and out of distribution generalization performance, offering an explanation for this surprising recent observation."
2749,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift. The authors show that the limiting test error, bias, and variance in this setting can be explained by a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. Moreover, the analysis reveals an exact linear relationship between the in-distribution and out of distribution generalization performance, offering an explanation for this surprising recent observation."
2750,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift. The authors show that the limiting test error, bias, and variance in this setting can be explained by a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. Moreover, the analysis reveals an exact linear relationship between the in-distribution and out of distribution generalization performance, offering an explanation for this surprising recent observation."
2751,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift. The authors show that the limiting test error, bias, and variance in this setting can be explained by a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. Moreover, the analysis reveals an exact linear relationship between the in-distribution and out of distribution generalization performance, offering an explanation for this surprising recent observation."
2752,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift. The authors show that the limiting test error, bias, and variance in this setting can be explained by a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. Moreover, the analysis reveals an exact linear relationship between the in-distribution and out of distribution generalization performance, offering an explanation for this surprising recent observation."
2753,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling and other Bayesian sequential decision-making algorithms to misspecification of the prior. The authors prove that the expected reward of Thompson sampling with a misspecified prior differs by at most $\epsilon$ from TS with a well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. For priors with bounded support, the bound is independent of the cardinality or structure of the action space, and it is tight up to universal constants in the worst case. The bound does not require the prior to have any parametric form."
2754,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling and other Bayesian sequential decision-making algorithms to misspecification of the prior. The authors prove that the expected reward of Thompson sampling with a misspecified prior differs by at most $\epsilon$ from TS with a well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. For priors with bounded support, the bound is independent of the cardinality or structure of the action space, and it is tight up to universal constants in the worst case. The bound does not require the prior to have any parametric form."
2755,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling and other Bayesian sequential decision-making algorithms to misspecification of the prior. The authors prove that the expected reward of Thompson sampling with a misspecified prior differs by at most $\epsilon$ from TS with a well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. For priors with bounded support, the bound is independent of the cardinality or structure of the action space, and it is tight up to universal constants in the worst case. The bound does not require the prior to have any parametric form."
2756,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling and other Bayesian sequential decision-making algorithms to misspecification of the prior. The authors prove that the expected reward of Thompson sampling with a misspecified prior differs by at most $\epsilon$ from TS with a well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. For priors with bounded support, the bound is independent of the cardinality or structure of the action space, and it is tight up to universal constants in the worst case. The bound does not require the prior to have any parametric form."
2757,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling and other Bayesian sequential decision-making algorithms to misspecification of the prior. The authors prove that the expected reward of Thompson sampling with a misspecified prior differs by at most $\epsilon$ from TS with a well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. For priors with bounded support, the bound is independent of the cardinality or structure of the action space, and it is tight up to universal constants in the worst case. The bound does not require the prior to have any parametric form."
2758,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample/query complexity of PAC-learning and Equivalence-Query-learning models. In the PAC model all samples are provided at the beginning of the learning process. The authors prove that in order to achieve an error $\epsilon$ exponentially (in ✏) fewer samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness.
2759,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample/query complexity of PAC-learning and Equivalence-Query-learning models. In the PAC model all samples are provided at the beginning of the learning process. The authors prove that in order to achieve an error $\epsilon$ exponentially (in ✏) fewer samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness.
2760,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample/query complexity of PAC-learning and Equivalence-Query-learning models. In the PAC model all samples are provided at the beginning of the learning process. The authors prove that in order to achieve an error $\epsilon$ exponentially (in ✏) fewer samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness.
2761,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample/query complexity of PAC-learning and Equivalence-Query-learning models. In the PAC model all samples are provided at the beginning of the learning process. The authors prove that in order to achieve an error $\epsilon$ exponentially (in ✏) fewer samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness.
2762,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample/query complexity of PAC-learning and Equivalence-Query-learning models. In the PAC model all samples are provided at the beginning of the learning process. The authors prove that in order to achieve an error $\epsilon$ exponentially (in ✏) fewer samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness.
2763,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection for transfer learning. The authors propose several benchmarks for evaluating on this task and analyze why existing model selection and transferability estimation methods perform poorly here and propose simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
2764,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection for transfer learning. The authors propose several benchmarks for evaluating on this task and analyze why existing model selection and transferability estimation methods perform poorly here and propose simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
2765,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection for transfer learning. The authors propose several benchmarks for evaluating on this task and analyze why existing model selection and transferability estimation methods perform poorly here and propose simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
2766,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection for transfer learning. The authors propose several benchmarks for evaluating on this task and analyze why existing model selection and transferability estimation methods perform poorly here and propose simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
2767,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection for transfer learning. The authors propose several benchmarks for evaluating on this task and analyze why existing model selection and transferability estimation methods perform poorly here and propose simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
2768,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary codes (LLC) for instances and classes. The method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low dimensional binary codes. The authors demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. They further quantitatively measure the quality of our codes by applying it to the efficient image retrieval and out-of-distribution (OOD) detection problems."
2769,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary codes (LLC) for instances and classes. The method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low dimensional binary codes. The authors demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. They further quantitatively measure the quality of our codes by applying it to the efficient image retrieval and out-of-distribution (OOD) detection problems."
2770,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary codes (LLC) for instances and classes. The method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low dimensional binary codes. The authors demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. They further quantitatively measure the quality of our codes by applying it to the efficient image retrieval and out-of-distribution (OOD) detection problems."
2771,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary codes (LLC) for instances and classes. The method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low dimensional binary codes. The authors demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. They further quantitatively measure the quality of our codes by applying it to the efficient image retrieval and out-of-distribution (OOD) detection problems."
2772,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary codes (LLC) for instances and classes. The method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low dimensional binary codes. The authors demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. They further quantitatively measure the quality of our codes by applying it to the efficient image retrieval and out-of-distribution (OOD) detection problems."
2773,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a new convolutional neural network architecture called Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. The authors prove the optimality of GDWS as a convolution approximator and provide exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. They demonstrate the effectiveness of the proposed method on CIFAR-10, SVHN, and ImageNet datasets."
2774,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a new convolutional neural network architecture called Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. The authors prove the optimality of GDWS as a convolution approximator and provide exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. They demonstrate the effectiveness of the proposed method on CIFAR-10, SVHN, and ImageNet datasets."
2775,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a new convolutional neural network architecture called Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. The authors prove the optimality of GDWS as a convolution approximator and provide exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. They demonstrate the effectiveness of the proposed method on CIFAR-10, SVHN, and ImageNet datasets."
2776,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a new convolutional neural network architecture called Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. The authors prove the optimality of GDWS as a convolution approximator and provide exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. They demonstrate the effectiveness of the proposed method on CIFAR-10, SVHN, and ImageNet datasets."
2777,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a new convolutional neural network architecture called Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. The authors prove the optimality of GDWS as a convolution approximator and provide exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. They demonstrate the effectiveness of the proposed method on CIFAR-10, SVHN, and ImageNet datasets."
2778,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a method for predicting the set of graph edits transforming the target into incomplete molecules called synthons. The method is based on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and amenable to manual correction."
2779,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a method for predicting the set of graph edits transforming the target into incomplete molecules called synthons. The method is based on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and amenable to manual correction."
2780,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a method for predicting the set of graph edits transforming the target into incomplete molecules called synthons. The method is based on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and amenable to manual correction."
2781,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a method for predicting the set of graph edits transforming the target into incomplete molecules called synthons. The method is based on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and amenable to manual correction."
2782,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a method for predicting the set of graph edits transforming the target into incomplete molecules called synthons. The method is based on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and amenable to manual correction."
2783,SP:772277d969c95924755113c86663fb0e009f24cc,This paper proposes a Bayesian formulation of the deconditioning problem for statistical downscaling of low-resolution (LR) spatial fields with high-resolution (HR) information. The authors extend the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1] to a downscaled setup and devise an efficient conditional mean embedding estimator for multiresolution data. The proposed method can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.
2784,SP:772277d969c95924755113c86663fb0e009f24cc,This paper proposes a Bayesian formulation of the deconditioning problem for statistical downscaling of low-resolution (LR) spatial fields with high-resolution (HR) information. The authors extend the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1] to a downscaled setup and devise an efficient conditional mean embedding estimator for multiresolution data. The proposed method can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.
2785,SP:772277d969c95924755113c86663fb0e009f24cc,This paper proposes a Bayesian formulation of the deconditioning problem for statistical downscaling of low-resolution (LR) spatial fields with high-resolution (HR) information. The authors extend the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1] to a downscaled setup and devise an efficient conditional mean embedding estimator for multiresolution data. The proposed method can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.
2786,SP:772277d969c95924755113c86663fb0e009f24cc,This paper proposes a Bayesian formulation of the deconditioning problem for statistical downscaling of low-resolution (LR) spatial fields with high-resolution (HR) information. The authors extend the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1] to a downscaled setup and devise an efficient conditional mean embedding estimator for multiresolution data. The proposed method can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.
2787,SP:772277d969c95924755113c86663fb0e009f24cc,This paper proposes a Bayesian formulation of the deconditioning problem for statistical downscaling of low-resolution (LR) spatial fields with high-resolution (HR) information. The authors extend the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1] to a downscaled setup and devise an efficient conditional mean embedding estimator for multiresolution data. The proposed method can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.
2788,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method for neural architecture search for deep sparse networks (DSNs). The main idea is to automatically search the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world datasets show promising results of PROFIT in both accuracy and efficiency."
2789,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method for neural architecture search for deep sparse networks (DSNs). The main idea is to automatically search the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world datasets show promising results of PROFIT in both accuracy and efficiency."
2790,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method for neural architecture search for deep sparse networks (DSNs). The main idea is to automatically search the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world datasets show promising results of PROFIT in both accuracy and efficiency."
2791,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method for neural architecture search for deep sparse networks (DSNs). The main idea is to automatically search the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world datasets show promising results of PROFIT in both accuracy and efficiency."
2792,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method for neural architecture search for deep sparse networks (DSNs). The main idea is to automatically search the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world datasets show promising results of PROFIT in both accuracy and efficiency."
2793,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. The authors provide a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose regularized self-labeling—the interpolation between regularization methods, including (i) layer-wise regularization and (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points."
2794,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. The authors provide a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose regularized self-labeling—the interpolation between regularization methods, including (i) layer-wise regularization and (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points."
2795,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. The authors provide a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose regularized self-labeling—the interpolation between regularization methods, including (i) layer-wise regularization and (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points."
2796,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. The authors provide a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose regularized self-labeling—the interpolation between regularization methods, including (i) layer-wise regularization and (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points."
2797,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. The authors provide a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose regularized self-labeling—the interpolation between regularization methods, including (i) layer-wise regularization and (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points."
2798,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the arm from among finitely many that has the smallest CVaR, VaR, or weighted sum of CVaRs and mean. The authors propose an optimal δcorrect algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as δ approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The paper develops new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
2799,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the arm from among finitely many that has the smallest CVaR, VaR, or weighted sum of CVaRs and mean. The authors propose an optimal δcorrect algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as δ approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The paper develops new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
2800,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the arm from among finitely many that has the smallest CVaR, VaR, or weighted sum of CVaRs and mean. The authors propose an optimal δcorrect algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as δ approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The paper develops new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
2801,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the arm from among finitely many that has the smallest CVaR, VaR, or weighted sum of CVaRs and mean. The authors propose an optimal δcorrect algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as δ approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The paper develops new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
2802,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the arm from among finitely many that has the smallest CVaR, VaR, or weighted sum of CVaRs and mean. The authors propose an optimal δcorrect algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as δ approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The paper develops new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
2803,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a new vision transformer architecture that learns intrinsic inductive bias (IB) from convolutions. The authors propose to downsample the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model."
2804,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a new vision transformer architecture that learns intrinsic inductive bias (IB) from convolutions. The authors propose to downsample the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model."
2805,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a new vision transformer architecture that learns intrinsic inductive bias (IB) from convolutions. The authors propose to downsample the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model."
2806,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a new vision transformer architecture that learns intrinsic inductive bias (IB) from convolutions. The authors propose to downsample the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model."
2807,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a new vision transformer architecture that learns intrinsic inductive bias (IB) from convolutions. The authors propose to downsample the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model."
2808,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference."
2809,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference."
2810,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference."
2811,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference."
2812,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference."
2813,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems. The authors introduce damped projection and adaptive regularization to the classical AM, and propose a Stochastic Anderson Mixing (SAM) scheme to solve nonconvolutional problems. Under mild assumptions, they establish the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. To further accelerate the convergence, they incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of the proposed method."
2814,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems. The authors introduce damped projection and adaptive regularization to the classical AM, and propose a Stochastic Anderson Mixing (SAM) scheme to solve nonconvolutional problems. Under mild assumptions, they establish the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. To further accelerate the convergence, they incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of the proposed method."
2815,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems. The authors introduce damped projection and adaptive regularization to the classical AM, and propose a Stochastic Anderson Mixing (SAM) scheme to solve nonconvolutional problems. Under mild assumptions, they establish the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. To further accelerate the convergence, they incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of the proposed method."
2816,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems. The authors introduce damped projection and adaptive regularization to the classical AM, and propose a Stochastic Anderson Mixing (SAM) scheme to solve nonconvolutional problems. Under mild assumptions, they establish the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. To further accelerate the convergence, they incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of the proposed method."
2817,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems. The authors introduce damped projection and adaptive regularization to the classical AM, and propose a Stochastic Anderson Mixing (SAM) scheme to solve nonconvolutional problems. Under mild assumptions, they establish the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. To further accelerate the convergence, they incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of the proposed method."
2818,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes SNIPS, a stochastic algorithm for sampling samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing, and their diversity exposes the inherent uncertainty in the inverse problem being solved."
2819,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes SNIPS, a stochastic algorithm for sampling samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing, and their diversity exposes the inherent uncertainty in the inverse problem being solved."
2820,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes SNIPS, a stochastic algorithm for sampling samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing, and their diversity exposes the inherent uncertainty in the inverse problem being solved."
2821,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes SNIPS, a stochastic algorithm for sampling samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing, and their diversity exposes the inherent uncertainty in the inverse problem being solved."
2822,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes SNIPS, a stochastic algorithm for sampling samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing, and their diversity exposes the inherent uncertainty in the inverse problem being solved."
2823,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a meta-learning framework for detecting drug traffickers on Instagram. The proposed method first builds a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media. Then, a relation-based graph convolutional neural network is used to learn node (i.e., user) representations over the built HG, in which graph structure refinement is introduced to compensate the sparse connection among entities in the HG for more robust node representation learning. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments on real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods."
2824,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a meta-learning framework for detecting drug traffickers on Instagram. The proposed method first builds a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media. Then, a relation-based graph convolutional neural network is used to learn node (i.e., user) representations over the built HG, in which graph structure refinement is introduced to compensate the sparse connection among entities in the HG for more robust node representation learning. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments on real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods."
2825,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a meta-learning framework for detecting drug traffickers on Instagram. The proposed method first builds a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media. Then, a relation-based graph convolutional neural network is used to learn node (i.e., user) representations over the built HG, in which graph structure refinement is introduced to compensate the sparse connection among entities in the HG for more robust node representation learning. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments on real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods."
2826,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a meta-learning framework for detecting drug traffickers on Instagram. The proposed method first builds a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media. Then, a relation-based graph convolutional neural network is used to learn node (i.e., user) representations over the built HG, in which graph structure refinement is introduced to compensate the sparse connection among entities in the HG for more robust node representation learning. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments on real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods."
2827,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a meta-learning framework for detecting drug traffickers on Instagram. The proposed method first builds a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media. Then, a relation-based graph convolutional neural network is used to learn node (i.e., user) representations over the built HG, in which graph structure refinement is introduced to compensate the sparse connection among entities in the HG for more robust node representation learning. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments on real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods."
2828,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the class of functions that can be represented by a neural network with ReLU activations and a given architecture. In particular, the authors consider the question of how many layers are needed to represent a class of exactly representable functions. This question is motivated by the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors provide upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
2829,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the class of functions that can be represented by a neural network with ReLU activations and a given architecture. In particular, the authors consider the question of how many layers are needed to represent a class of exactly representable functions. This question is motivated by the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors provide upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
2830,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the class of functions that can be represented by a neural network with ReLU activations and a given architecture. In particular, the authors consider the question of how many layers are needed to represent a class of exactly representable functions. This question is motivated by the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors provide upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
2831,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the class of functions that can be represented by a neural network with ReLU activations and a given architecture. In particular, the authors consider the question of how many layers are needed to represent a class of exactly representable functions. This question is motivated by the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors provide upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
2832,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the class of functions that can be represented by a neural network with ReLU activations and a given architecture. In particular, the authors consider the question of how many layers are needed to represent a class of exactly representable functions. This question is motivated by the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors provide upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
2833,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. Specifically, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as an adversarial optimization problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations."
2834,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. Specifically, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as an adversarial optimization problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations."
2835,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. Specifically, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as an adversarial optimization problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations."
2836,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. Specifically, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as an adversarial optimization problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations."
2837,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. Specifically, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as an adversarial optimization problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations."
2838,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SPCA). The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For the highly sparse regime of k ≤ √ n, the algorithms recover the sparse vector for signal-tonoise ratio $\tilde{O}(\frac{1}{\sqrt{T}(k/t))$ in time $Omega(n)$. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. The authors also extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes."
2839,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SPCA). The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For the highly sparse regime of k ≤ √ n, the algorithms recover the sparse vector for signal-tonoise ratio $\tilde{O}(\frac{1}{\sqrt{T}(k/t))$ in time $Omega(n)$. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. The authors also extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes."
2840,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SPCA). The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For the highly sparse regime of k ≤ √ n, the algorithms recover the sparse vector for signal-tonoise ratio $\tilde{O}(\frac{1}{\sqrt{T}(k/t))$ in time $Omega(n)$. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. The authors also extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes."
2841,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SPCA). The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For the highly sparse regime of k ≤ √ n, the algorithms recover the sparse vector for signal-tonoise ratio $\tilde{O}(\frac{1}{\sqrt{T}(k/t))$ in time $Omega(n)$. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. The authors also extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes."
2842,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SPCA). The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For the highly sparse regime of k ≤ √ n, the algorithms recover the sparse vector for signal-tonoise ratio $\tilde{O}(\frac{1}{\sqrt{T}(k/t))$ in time $Omega(n)$. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. The authors also extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes."
2843,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space."
2844,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space."
2845,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space."
2846,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space."
2847,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space."
2848,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle point optimization problem with probability simplex constraints. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, the authors develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) for zero-sum two-player matrix games with entropy regularisation. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly."
2849,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle point optimization problem with probability simplex constraints. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, the authors develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) for zero-sum two-player matrix games with entropy regularisation. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly."
2850,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle point optimization problem with probability simplex constraints. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, the authors develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) for zero-sum two-player matrix games with entropy regularisation. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly."
2851,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle point optimization problem with probability simplex constraints. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, the authors develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) for zero-sum two-player matrix games with entropy regularisation. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly."
2852,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle point optimization problem with probability simplex constraints. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, the authors develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) for zero-sum two-player matrix games with entropy regularisation. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly."
2853,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on the idea of implicit position encoding, which is used to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods."
2854,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on the idea of implicit position encoding, which is used to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods."
2855,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on the idea of implicit position encoding, which is used to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods."
2856,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on the idea of implicit position encoding, which is used to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods."
2857,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on the idea of implicit position encoding, which is used to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods."
2858,SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning causal models using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The method is motivated and illustrated by a structural causal model themed around personal health. The authors propose to replace Pearl's do-operator with an arbitrarily intervened causal graph as input, effectively subsuming Pearl’s do operator, and the gate function predicts the parameters of the SPN. The experimental evaluation demonstrates that the proposed method is both expressive and causally adequate."
2859,SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning causal models using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The method is motivated and illustrated by a structural causal model themed around personal health. The authors propose to replace Pearl's do-operator with an arbitrarily intervened causal graph as input, effectively subsuming Pearl’s do operator, and the gate function predicts the parameters of the SPN. The experimental evaluation demonstrates that the proposed method is both expressive and causally adequate."
2860,SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning causal models using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The method is motivated and illustrated by a structural causal model themed around personal health. The authors propose to replace Pearl's do-operator with an arbitrarily intervened causal graph as input, effectively subsuming Pearl’s do operator, and the gate function predicts the parameters of the SPN. The experimental evaluation demonstrates that the proposed method is both expressive and causally adequate."
2861,SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning causal models using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The method is motivated and illustrated by a structural causal model themed around personal health. The authors propose to replace Pearl's do-operator with an arbitrarily intervened causal graph as input, effectively subsuming Pearl’s do operator, and the gate function predicts the parameters of the SPN. The experimental evaluation demonstrates that the proposed method is both expressive and causally adequate."
2862,SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning causal models using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The method is motivated and illustrated by a structural causal model themed around personal health. The authors propose to replace Pearl's do-operator with an arbitrarily intervened causal graph as input, effectively subsuming Pearl’s do operator, and the gate function predicts the parameters of the SPN. The experimental evaluation demonstrates that the proposed method is both expressive and causally adequate."
2863,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model for factorizing functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov factor analysis (MFA) framework, and is able to capture temporal dynamics in fMRI data. In particular, the authors propose to use a discrete latent variable to represent the subject and cognitive state variability. Experiments on both synthetic and real fMRI datasets demonstrate the effectiveness of the proposed model."
2864,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model for factorizing functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov factor analysis (MFA) framework, and is able to capture temporal dynamics in fMRI data. In particular, the authors propose to use a discrete latent variable to represent the subject and cognitive state variability. Experiments on both synthetic and real fMRI datasets demonstrate the effectiveness of the proposed model."
2865,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model for factorizing functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov factor analysis (MFA) framework, and is able to capture temporal dynamics in fMRI data. In particular, the authors propose to use a discrete latent variable to represent the subject and cognitive state variability. Experiments on both synthetic and real fMRI datasets demonstrate the effectiveness of the proposed model."
2866,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model for factorizing functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov factor analysis (MFA) framework, and is able to capture temporal dynamics in fMRI data. In particular, the authors propose to use a discrete latent variable to represent the subject and cognitive state variability. Experiments on both synthetic and real fMRI datasets demonstrate the effectiveness of the proposed model."
2867,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model for factorizing functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov factor analysis (MFA) framework, and is able to capture temporal dynamics in fMRI data. In particular, the authors propose to use a discrete latent variable to represent the subject and cognitive state variability. Experiments on both synthetic and real fMRI datasets demonstrate the effectiveness of the proposed model."
2868,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes a new generative model based on continuous normalizing flows (CNFs). The main difference from existing CNFs is that the density of the model is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
2869,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes a new generative model based on continuous normalizing flows (CNFs). The main difference from existing CNFs is that the density of the model is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
2870,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes a new generative model based on continuous normalizing flows (CNFs). The main difference from existing CNFs is that the density of the model is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
2871,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes a new generative model based on continuous normalizing flows (CNFs). The main difference from existing CNFs is that the density of the model is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
2872,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes a new generative model based on continuous normalizing flows (CNFs). The main difference from existing CNFs is that the density of the model is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
2873,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-identifiability of unsupervised representation learning in the presence of additional, typically observed variables. In particular, the authors consider the setting where each source is independently influencing the mixing process. The authors propose a framework which they term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of non identifiability issues arising in nonlinear blind source separation."
2874,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-identifiability of unsupervised representation learning in the presence of additional, typically observed variables. In particular, the authors consider the setting where each source is independently influencing the mixing process. The authors propose a framework which they term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of non identifiability issues arising in nonlinear blind source separation."
2875,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-identifiability of unsupervised representation learning in the presence of additional, typically observed variables. In particular, the authors consider the setting where each source is independently influencing the mixing process. The authors propose a framework which they term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of non identifiability issues arising in nonlinear blind source separation."
2876,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-identifiability of unsupervised representation learning in the presence of additional, typically observed variables. In particular, the authors consider the setting where each source is independently influencing the mixing process. The authors propose a framework which they term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of non identifiability issues arising in nonlinear blind source separation."
2877,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-identifiability of unsupervised representation learning in the presence of additional, typically observed variables. In particular, the authors consider the setting where each source is independently influencing the mixing process. The authors propose a framework which they term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of non identifiability issues arising in nonlinear blind source separation."
2878,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for estimating the log-normalization of an unnormalized target distribution. The proposed method is based on Annealed Importance Sampling (AIS) with Hamiltonian MCMC (HAMMC). The main drawback of AIS is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. The authors propose a framework to use an AIS-like procedure with Uncorrected HAMMC, which is called uncorrected Hamiltonian Annealing (UHAM). The authors show that UHAMMC can achieve tight and differentiable lower bounds on log-normality, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements."
2879,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for estimating the log-normalization of an unnormalized target distribution. The proposed method is based on Annealed Importance Sampling (AIS) with Hamiltonian MCMC (HAMMC). The main drawback of AIS is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. The authors propose a framework to use an AIS-like procedure with Uncorrected HAMMC, which is called uncorrected Hamiltonian Annealing (UHAM). The authors show that UHAMMC can achieve tight and differentiable lower bounds on log-normality, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements."
2880,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for estimating the log-normalization of an unnormalized target distribution. The proposed method is based on Annealed Importance Sampling (AIS) with Hamiltonian MCMC (HAMMC). The main drawback of AIS is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. The authors propose a framework to use an AIS-like procedure with Uncorrected HAMMC, which is called uncorrected Hamiltonian Annealing (UHAM). The authors show that UHAMMC can achieve tight and differentiable lower bounds on log-normality, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements."
2881,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for estimating the log-normalization of an unnormalized target distribution. The proposed method is based on Annealed Importance Sampling (AIS) with Hamiltonian MCMC (HAMMC). The main drawback of AIS is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. The authors propose a framework to use an AIS-like procedure with Uncorrected HAMMC, which is called uncorrected Hamiltonian Annealing (UHAM). The authors show that UHAMMC can achieve tight and differentiable lower bounds on log-normality, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements."
2882,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for estimating the log-normalization of an unnormalized target distribution. The proposed method is based on Annealed Importance Sampling (AIS) with Hamiltonian MCMC (HAMMC). The main drawback of AIS is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. The authors propose a framework to use an AIS-like procedure with Uncorrected HAMMC, which is called uncorrected Hamiltonian Annealing (UHAM). The authors show that UHAMMC can achieve tight and differentiable lower bounds on log-normality, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements."
2883,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a new local Lipschitz bound for certified robustness of deep neural networks. The main contribution of the paper is to eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. The authors also propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local bound. Experiments are conducted on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures."
2884,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a new local Lipschitz bound for certified robustness of deep neural networks. The main contribution of the paper is to eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. The authors also propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local bound. Experiments are conducted on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures."
2885,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a new local Lipschitz bound for certified robustness of deep neural networks. The main contribution of the paper is to eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. The authors also propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local bound. Experiments are conducted on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures."
2886,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a new local Lipschitz bound for certified robustness of deep neural networks. The main contribution of the paper is to eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. The authors also propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local bound. Experiments are conducted on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures."
2887,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a new local Lipschitz bound for certified robustness of deep neural networks. The main contribution of the paper is to eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. The authors also propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local bound. Experiments are conducted on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures."
2888,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. The proposed method is based on adding one-one-in importance sampling. The authors show that the proposed method can be efficiently obtained from re-weighted posterior samples of model parameters. The method is evaluated on a variety of datasets, and compared with existing conformal methods."
2889,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. The proposed method is based on adding one-one-in importance sampling. The authors show that the proposed method can be efficiently obtained from re-weighted posterior samples of model parameters. The method is evaluated on a variety of datasets, and compared with existing conformal methods."
2890,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. The proposed method is based on adding one-one-in importance sampling. The authors show that the proposed method can be efficiently obtained from re-weighted posterior samples of model parameters. The method is evaluated on a variety of datasets, and compared with existing conformal methods."
2891,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. The proposed method is based on adding one-one-in importance sampling. The authors show that the proposed method can be efficiently obtained from re-weighted posterior samples of model parameters. The method is evaluated on a variety of datasets, and compared with existing conformal methods."
2892,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. The proposed method is based on adding one-one-in importance sampling. The authors show that the proposed method can be efficiently obtained from re-weighted posterior samples of model parameters. The method is evaluated on a variety of datasets, and compared with existing conformal methods."
2893,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoisers in the context of regularization-by-denoising (RED) and plug-and-play priors (PnP). In particular, the authors propose to use the gradients of smooth scalar-valued deep neural networks as potentials. The authors show that the proposed method converges to stationary points of an underlying objective function consisting of the learned potentials, and provide a simple inversion method that utilizes the proposed denoiser. Experiments are conducted on MNIST, CIFAR-10, and Imagenet."
2894,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoisers in the context of regularization-by-denoising (RED) and plug-and-play priors (PnP). In particular, the authors propose to use the gradients of smooth scalar-valued deep neural networks as potentials. The authors show that the proposed method converges to stationary points of an underlying objective function consisting of the learned potentials, and provide a simple inversion method that utilizes the proposed denoiser. Experiments are conducted on MNIST, CIFAR-10, and Imagenet."
2895,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoisers in the context of regularization-by-denoising (RED) and plug-and-play priors (PnP). In particular, the authors propose to use the gradients of smooth scalar-valued deep neural networks as potentials. The authors show that the proposed method converges to stationary points of an underlying objective function consisting of the learned potentials, and provide a simple inversion method that utilizes the proposed denoiser. Experiments are conducted on MNIST, CIFAR-10, and Imagenet."
2896,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoisers in the context of regularization-by-denoising (RED) and plug-and-play priors (PnP). In particular, the authors propose to use the gradients of smooth scalar-valued deep neural networks as potentials. The authors show that the proposed method converges to stationary points of an underlying objective function consisting of the learned potentials, and provide a simple inversion method that utilizes the proposed denoiser. Experiments are conducted on MNIST, CIFAR-10, and Imagenet."
2897,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoisers in the context of regularization-by-denoising (RED) and plug-and-play priors (PnP). In particular, the authors propose to use the gradients of smooth scalar-valued deep neural networks as potentials. The authors show that the proposed method converges to stationary points of an underlying objective function consisting of the learned potentials, and provide a simple inversion method that utilizes the proposed denoiser. Experiments are conducted on MNIST, CIFAR-10, and Imagenet."
2898,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music from videos of human movements. The method is based on the idea of music improvisation, which is a natural process that includes the prescription of streams of the beat, the rhythm and the melody. The proposed method works directly with human movements, by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. In particular, RhythmicNet first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Then, it implements a transformerbased model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by conditioning on generated drum sounds."
2899,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music from videos of human movements. The method is based on the idea of music improvisation, which is a natural process that includes the prescription of streams of the beat, the rhythm and the melody. The proposed method works directly with human movements, by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. In particular, RhythmicNet first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Then, it implements a transformerbased model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by conditioning on generated drum sounds."
2900,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music from videos of human movements. The method is based on the idea of music improvisation, which is a natural process that includes the prescription of streams of the beat, the rhythm and the melody. The proposed method works directly with human movements, by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. In particular, RhythmicNet first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Then, it implements a transformerbased model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by conditioning on generated drum sounds."
2901,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music from videos of human movements. The method is based on the idea of music improvisation, which is a natural process that includes the prescription of streams of the beat, the rhythm and the melody. The proposed method works directly with human movements, by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. In particular, RhythmicNet first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Then, it implements a transformerbased model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by conditioning on generated drum sounds."
2902,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music from videos of human movements. The method is based on the idea of music improvisation, which is a natural process that includes the prescription of streams of the beat, the rhythm and the melody. The proposed method works directly with human movements, by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. In particular, RhythmicNet first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Then, it implements a transformerbased model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by conditioning on generated drum sounds."
2903,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a method for offline reinforcement learning (RL) that uses an on-policy Q estimate of the behavior policy to improve the performance of an actor-critic algorithm. The method is based on the idea that the high variance inherent in doing off-policy evaluation is magnified by the repeated optimization of policies against those estimates. The authors argue that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.
2904,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a method for offline reinforcement learning (RL) that uses an on-policy Q estimate of the behavior policy to improve the performance of an actor-critic algorithm. The method is based on the idea that the high variance inherent in doing off-policy evaluation is magnified by the repeated optimization of policies against those estimates. The authors argue that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.
2905,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a method for offline reinforcement learning (RL) that uses an on-policy Q estimate of the behavior policy to improve the performance of an actor-critic algorithm. The method is based on the idea that the high variance inherent in doing off-policy evaluation is magnified by the repeated optimization of policies against those estimates. The authors argue that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.
2906,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a method for offline reinforcement learning (RL) that uses an on-policy Q estimate of the behavior policy to improve the performance of an actor-critic algorithm. The method is based on the idea that the high variance inherent in doing off-policy evaluation is magnified by the repeated optimization of policies against those estimates. The authors argue that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.
2907,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a method for offline reinforcement learning (RL) that uses an on-policy Q estimate of the behavior policy to improve the performance of an actor-critic algorithm. The method is based on the idea that the high variance inherent in doing off-policy evaluation is magnified by the repeated optimization of policies against those estimates. The authors argue that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.
2908,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of low-rank matrix recovery. The authors consider the case where the objective function is a maximum of smooth functions. They show that the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method."
2909,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of low-rank matrix recovery. The authors consider the case where the objective function is a maximum of smooth functions. They show that the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method."
2910,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of low-rank matrix recovery. The authors consider the case where the objective function is a maximum of smooth functions. They show that the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method."
2911,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of low-rank matrix recovery. The authors consider the case where the objective function is a maximum of smooth functions. They show that the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method."
2912,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of low-rank matrix recovery. The authors consider the case where the objective function is a maximum of smooth functions. They show that the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method."
2913,SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a generalized Robinson decomposition of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, the proposed method isolates the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and provides a quasi-oracle convergence guarantee under mild assumptions. Experiments on small-world and molecular graphs demonstrate that the proposed approach outperforms prior work in CATE estimation."
2914,SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a generalized Robinson decomposition of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, the proposed method isolates the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and provides a quasi-oracle convergence guarantee under mild assumptions. Experiments on small-world and molecular graphs demonstrate that the proposed approach outperforms prior work in CATE estimation."
2915,SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a generalized Robinson decomposition of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, the proposed method isolates the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and provides a quasi-oracle convergence guarantee under mild assumptions. Experiments on small-world and molecular graphs demonstrate that the proposed approach outperforms prior work in CATE estimation."
2916,SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a generalized Robinson decomposition of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, the proposed method isolates the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and provides a quasi-oracle convergence guarantee under mild assumptions. Experiments on small-world and molecular graphs demonstrate that the proposed approach outperforms prior work in CATE estimation."
2917,SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a generalized Robinson decomposition of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, the proposed method isolates the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and provides a quasi-oracle convergence guarantee under mild assumptions. Experiments on small-world and molecular graphs demonstrate that the proposed approach outperforms prior work in CATE estimation."
2918,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper proposes a new method for causal effect identification based on both graphical criteria and matrix equations. Specifically, the authors first characterize the relationships between graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating matrix-based criteria into a graphical identification approach."
2919,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper proposes a new method for causal effect identification based on both graphical criteria and matrix equations. Specifically, the authors first characterize the relationships between graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating matrix-based criteria into a graphical identification approach."
2920,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper proposes a new method for causal effect identification based on both graphical criteria and matrix equations. Specifically, the authors first characterize the relationships between graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating matrix-based criteria into a graphical identification approach."
2921,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper proposes a new method for causal effect identification based on both graphical criteria and matrix equations. Specifically, the authors first characterize the relationships between graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating matrix-based criteria into a graphical identification approach."
2922,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper proposes a new method for causal effect identification based on both graphical criteria and matrix equations. Specifically, the authors first characterize the relationships between graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating matrix-based criteria into a graphical identification approach."
2923,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of reinforcement learning. In particular, the authors consider the Prioritized Level Replay (PLR) algorithm, which randomly samples randomly-generated training levels, as UED. The authors show that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). The authors also provide a novel theory for PLR that provides a robustness guarantee at Nash equilibria."
2924,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of reinforcement learning. In particular, the authors consider the Prioritized Level Replay (PLR) algorithm, which randomly samples randomly-generated training levels, as UED. The authors show that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). The authors also provide a novel theory for PLR that provides a robustness guarantee at Nash equilibria."
2925,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of reinforcement learning. In particular, the authors consider the Prioritized Level Replay (PLR) algorithm, which randomly samples randomly-generated training levels, as UED. The authors show that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). The authors also provide a novel theory for PLR that provides a robustness guarantee at Nash equilibria."
2926,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of reinforcement learning. In particular, the authors consider the Prioritized Level Replay (PLR) algorithm, which randomly samples randomly-generated training levels, as UED. The authors show that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). The authors also provide a novel theory for PLR that provides a robustness guarantee at Nash equilibria."
2927,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of reinforcement learning. In particular, the authors consider the Prioritized Level Replay (PLR) algorithm, which randomly samples randomly-generated training levels, as UED. The authors show that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). The authors also provide a novel theory for PLR that provides a robustness guarantee at Nash equilibria."
2928,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem in the shuffle model with a distribution-dependent regret of $O(\sqrt{T})$ and distribution-independent regret of $\Omega(\frac{kT}{\epsilon})$ where $T$ is the number of rounds, $k$ is a suboptimality gap of the arm a, and $K$ is total number of arms. The authors show that their algorithm achieves a regret that matches the regret of the best known algorithms for the centralized model, and significantly outperforms the best-known algorithm in the local model."
2929,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem in the shuffle model with a distribution-dependent regret of $O(\sqrt{T})$ and distribution-independent regret of $\Omega(\frac{kT}{\epsilon})$ where $T$ is the number of rounds, $k$ is a suboptimality gap of the arm a, and $K$ is total number of arms. The authors show that their algorithm achieves a regret that matches the regret of the best known algorithms for the centralized model, and significantly outperforms the best-known algorithm in the local model."
2930,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem in the shuffle model with a distribution-dependent regret of $O(\sqrt{T})$ and distribution-independent regret of $\Omega(\frac{kT}{\epsilon})$ where $T$ is the number of rounds, $k$ is a suboptimality gap of the arm a, and $K$ is total number of arms. The authors show that their algorithm achieves a regret that matches the regret of the best known algorithms for the centralized model, and significantly outperforms the best-known algorithm in the local model."
2931,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem in the shuffle model with a distribution-dependent regret of $O(\sqrt{T})$ and distribution-independent regret of $\Omega(\frac{kT}{\epsilon})$ where $T$ is the number of rounds, $k$ is a suboptimality gap of the arm a, and $K$ is total number of arms. The authors show that their algorithm achieves a regret that matches the regret of the best known algorithms for the centralized model, and significantly outperforms the best-known algorithm in the local model."
2932,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem in the shuffle model with a distribution-dependent regret of $O(\sqrt{T})$ and distribution-independent regret of $\Omega(\frac{kT}{\epsilon})$ where $T$ is the number of rounds, $k$ is a suboptimality gap of the arm a, and $K$ is total number of arms. The authors show that their algorithm achieves a regret that matches the regret of the best known algorithms for the centralized model, and significantly outperforms the best-known algorithm in the local model."
2933,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold calibrated forecaster. This procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
2934,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold calibrated forecaster. This procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
2935,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold calibrated forecaster. This procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
2936,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold calibrated forecaster. This procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
2937,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold calibrated forecaster. This procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
2938,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The proposed method optimizes a set of centroid points to compactly approximate the argmax distributions with a simple objective function, without explicitly drawing exact samples from the arg max distribution. Theoretically, the proposed method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmax probability distribution and the centroids approximation under proper conditions. Experiments on few-shot image classification, personalized dialogue systems and multi-target domain adaptation demonstrate the applicability and effectiveness of the method."
2939,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The proposed method optimizes a set of centroid points to compactly approximate the argmax distributions with a simple objective function, without explicitly drawing exact samples from the arg max distribution. Theoretically, the proposed method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmax probability distribution and the centroids approximation under proper conditions. Experiments on few-shot image classification, personalized dialogue systems and multi-target domain adaptation demonstrate the applicability and effectiveness of the method."
2940,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The proposed method optimizes a set of centroid points to compactly approximate the argmax distributions with a simple objective function, without explicitly drawing exact samples from the arg max distribution. Theoretically, the proposed method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmax probability distribution and the centroids approximation under proper conditions. Experiments on few-shot image classification, personalized dialogue systems and multi-target domain adaptation demonstrate the applicability and effectiveness of the method."
2941,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The proposed method optimizes a set of centroid points to compactly approximate the argmax distributions with a simple objective function, without explicitly drawing exact samples from the arg max distribution. Theoretically, the proposed method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmax probability distribution and the centroids approximation under proper conditions. Experiments on few-shot image classification, personalized dialogue systems and multi-target domain adaptation demonstrate the applicability and effectiveness of the method."
2942,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The proposed method optimizes a set of centroid points to compactly approximate the argmax distributions with a simple objective function, without explicitly drawing exact samples from the arg max distribution. Theoretically, the proposed method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmax probability distribution and the centroids approximation under proper conditions. Experiments on few-shot image classification, personalized dialogue systems and multi-target domain adaptation demonstrate the applicability and effectiveness of the method."
2943,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers the problem of multi-objective reinforcement learning where the objectives are balanced using preferences. The authors formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector. The paper considers two settings: online and preference-free exploration. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The proposed algorithm is provably efficient with a nearly optimal trajectory complexity."
2944,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers the problem of multi-objective reinforcement learning where the objectives are balanced using preferences. The authors formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector. The paper considers two settings: online and preference-free exploration. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The proposed algorithm is provably efficient with a nearly optimal trajectory complexity."
2945,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers the problem of multi-objective reinforcement learning where the objectives are balanced using preferences. The authors formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector. The paper considers two settings: online and preference-free exploration. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The proposed algorithm is provably efficient with a nearly optimal trajectory complexity."
2946,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers the problem of multi-objective reinforcement learning where the objectives are balanced using preferences. The authors formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector. The paper considers two settings: online and preference-free exploration. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The proposed algorithm is provably efficient with a nearly optimal trajectory complexity."
2947,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers the problem of multi-objective reinforcement learning where the objectives are balanced using preferences. The authors formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector. The paper considers two settings: online and preference-free exploration. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The proposed algorithm is provably efficient with a nearly optimal trajectory complexity."
2948,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating explanations for dialogue response generation. The method is based on the idea that the explanation should be unbiased, consistent, and cause-agnostic. The authors propose to perturb the input and output of the model to estimate the uncertainty of the human response, and then use the perturbation to generate explanations. The experiments show that the proposed method outperforms existing methods on automatic and humanevaluation metrics."
2949,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating explanations for dialogue response generation. The method is based on the idea that the explanation should be unbiased, consistent, and cause-agnostic. The authors propose to perturb the input and output of the model to estimate the uncertainty of the human response, and then use the perturbation to generate explanations. The experiments show that the proposed method outperforms existing methods on automatic and humanevaluation metrics."
2950,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating explanations for dialogue response generation. The method is based on the idea that the explanation should be unbiased, consistent, and cause-agnostic. The authors propose to perturb the input and output of the model to estimate the uncertainty of the human response, and then use the perturbation to generate explanations. The experiments show that the proposed method outperforms existing methods on automatic and humanevaluation metrics."
2951,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating explanations for dialogue response generation. The method is based on the idea that the explanation should be unbiased, consistent, and cause-agnostic. The authors propose to perturb the input and output of the model to estimate the uncertainty of the human response, and then use the perturbation to generate explanations. The experiments show that the proposed method outperforms existing methods on automatic and humanevaluation metrics."
2952,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating explanations for dialogue response generation. The method is based on the idea that the explanation should be unbiased, consistent, and cause-agnostic. The authors propose to perturb the input and output of the model to estimate the uncertainty of the human response, and then use the perturbation to generate explanations. The experiments show that the proposed method outperforms existing methods on automatic and humanevaluation metrics."
2953,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence of error-compressed gradient compression methods for distributed training of large-scale machine learning models. In particular, the authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
2954,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence of error-compressed gradient compression methods for distributed training of large-scale machine learning models. In particular, the authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
2955,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence of error-compressed gradient compression methods for distributed training of large-scale machine learning models. In particular, the authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
2956,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence of error-compressed gradient compression methods for distributed training of large-scale machine learning models. In particular, the authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
2957,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence of error-compressed gradient compression methods for distributed training of large-scale machine learning models. In particular, the authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
2958,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a method to improve the performance of the liquid state machine (LSM) model. The authors propose a neuron-astrocyte-based approach to tune the weights of the LSM, which is based on the observation that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. To this end, the authors propose to learn a near-critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST and Fashion MNIST show that the proposed method outperforms the state of the art."
2959,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a method to improve the performance of the liquid state machine (LSM) model. The authors propose a neuron-astrocyte-based approach to tune the weights of the LSM, which is based on the observation that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. To this end, the authors propose to learn a near-critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST and Fashion MNIST show that the proposed method outperforms the state of the art."
2960,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a method to improve the performance of the liquid state machine (LSM) model. The authors propose a neuron-astrocyte-based approach to tune the weights of the LSM, which is based on the observation that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. To this end, the authors propose to learn a near-critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST and Fashion MNIST show that the proposed method outperforms the state of the art."
2961,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a method to improve the performance of the liquid state machine (LSM) model. The authors propose a neuron-astrocyte-based approach to tune the weights of the LSM, which is based on the observation that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. To this end, the authors propose to learn a near-critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST and Fashion MNIST show that the proposed method outperforms the state of the art."
2962,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a method to improve the performance of the liquid state machine (LSM) model. The authors propose a neuron-astrocyte-based approach to tune the weights of the LSM, which is based on the observation that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. To this end, the authors propose to learn a near-critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST and Fashion MNIST show that the proposed method outperforms the state of the art."
2963,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of graph topology imbalance in semi-supervised node classification. The authors propose a new metric, Totoro, to measure the degree of topological imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. The experimental results demonstrate the effectiveness and generalizability of ReNode."
2964,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of graph topology imbalance in semi-supervised node classification. The authors propose a new metric, Totoro, to measure the degree of topological imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. The experimental results demonstrate the effectiveness and generalizability of ReNode."
2965,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of graph topology imbalance in semi-supervised node classification. The authors propose a new metric, Totoro, to measure the degree of topological imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. The experimental results demonstrate the effectiveness and generalizability of ReNode."
2966,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of graph topology imbalance in semi-supervised node classification. The authors propose a new metric, Totoro, to measure the degree of topological imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. The experimental results demonstrate the effectiveness and generalizability of ReNode."
2967,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of graph topology imbalance in semi-supervised node classification. The authors propose a new metric, Totoro, to measure the degree of topological imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. The experimental results demonstrate the effectiveness and generalizability of ReNode."
2968,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery of piece-wise constant signals corrupted by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k∗ log(N)/κ, where k∗ is the minimal number of rectangular subgraphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, κ is the smallest magnitude of the signal difference among contiguous elements of partition and N is the size of the lattice, and the theoretical guarantees further extend to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
2969,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery of piece-wise constant signals corrupted by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k∗ log(N)/κ, where k∗ is the minimal number of rectangular subgraphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, κ is the smallest magnitude of the signal difference among contiguous elements of partition and N is the size of the lattice, and the theoretical guarantees further extend to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
2970,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery of piece-wise constant signals corrupted by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k∗ log(N)/κ, where k∗ is the minimal number of rectangular subgraphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, κ is the smallest magnitude of the signal difference among contiguous elements of partition and N is the size of the lattice, and the theoretical guarantees further extend to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
2971,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery of piece-wise constant signals corrupted by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k∗ log(N)/κ, where k∗ is the minimal number of rectangular subgraphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, κ is the smallest magnitude of the signal difference among contiguous elements of partition and N is the size of the lattice, and the theoretical guarantees further extend to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
2972,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery of piece-wise constant signals corrupted by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k∗ log(N)/κ, where k∗ is the minimal number of rectangular subgraphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, κ is the smallest magnitude of the signal difference among contiguous elements of partition and N is the size of the lattice, and the theoretical guarantees further extend to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
2973,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. In particular, the authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely counterfactual maximum likelihood estimation (CMLE). The authors derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. The authors conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning."
2974,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. In particular, the authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely counterfactual maximum likelihood estimation (CMLE). The authors derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. The authors conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning."
2975,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. In particular, the authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely counterfactual maximum likelihood estimation (CMLE). The authors derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. The authors conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning."
2976,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. In particular, the authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely counterfactual maximum likelihood estimation (CMLE). The authors derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. The authors conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning."
2977,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. In particular, the authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely counterfactual maximum likelihood estimation (CMLE). The authors derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. The authors conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning."
2978,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,This paper proposes a method for multi-task learning. The main idea is to use the worst local improvement of individual tasks to regularize the algorithm trajectory. The method is based on regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA). The main contribution of the paper is to show that the proposed method converges to a minimum over the average loss.
2979,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,This paper proposes a method for multi-task learning. The main idea is to use the worst local improvement of individual tasks to regularize the algorithm trajectory. The method is based on regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA). The main contribution of the paper is to show that the proposed method converges to a minimum over the average loss.
2980,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,This paper proposes a method for multi-task learning. The main idea is to use the worst local improvement of individual tasks to regularize the algorithm trajectory. The method is based on regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA). The main contribution of the paper is to show that the proposed method converges to a minimum over the average loss.
2981,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,This paper proposes a method for multi-task learning. The main idea is to use the worst local improvement of individual tasks to regularize the algorithm trajectory. The method is based on regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA). The main contribution of the paper is to show that the proposed method converges to a minimum over the average loss.
2982,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,This paper proposes a method for multi-task learning. The main idea is to use the worst local improvement of individual tasks to regularize the algorithm trajectory. The method is based on regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA). The main contribution of the paper is to show that the proposed method converges to a minimum over the average loss.
2983,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the few-shot learning of algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small witness set. They explore how several GPT architectures, program induction systems, and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs."
2984,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the few-shot learning of algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small witness set. They explore how several GPT architectures, program induction systems, and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs."
2985,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the few-shot learning of algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small witness set. They explore how several GPT architectures, program induction systems, and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs."
2986,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the few-shot learning of algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small witness set. They explore how several GPT architectures, program induction systems, and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs."
2987,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the few-shot learning of algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small witness set. They explore how several GPT architectures, program induction systems, and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs."
2988,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method for distilling feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in the feature representation to dichotomize feature units either robust or non robust, based on the noise variation magnitude. The authors demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of non-robot features that is directly related to the model prediction, which is shown to break model robustness."
2989,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method for distilling feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in the feature representation to dichotomize feature units either robust or non robust, based on the noise variation magnitude. The authors demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of non-robot features that is directly related to the model prediction, which is shown to break model robustness."
2990,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method for distilling feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in the feature representation to dichotomize feature units either robust or non robust, based on the noise variation magnitude. The authors demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of non-robot features that is directly related to the model prediction, which is shown to break model robustness."
2991,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method for distilling feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in the feature representation to dichotomize feature units either robust or non robust, based on the noise variation magnitude. The authors demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of non-robot features that is directly related to the model prediction, which is shown to break model robustness."
2992,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method for distilling feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in the feature representation to dichotomize feature units either robust or non robust, based on the noise variation magnitude. The authors demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of non-robot features that is directly related to the model prediction, which is shown to break model robustness."
2993,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the phenomenon of support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) in the context of feature models. The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and give experimental support for its universality. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
2994,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the phenomenon of support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) in the context of feature models. The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and give experimental support for its universality. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
2995,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the phenomenon of support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) in the context of feature models. The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and give experimental support for its universality. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
2996,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the phenomenon of support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) in the context of feature models. The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and give experimental support for its universality. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
2997,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the phenomenon of support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) in the context of feature models. The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and give experimental support for its universality. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
2998,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the active pure exploration problem in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. The algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption."
2999,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the active pure exploration problem in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. The algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption."
3000,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the active pure exploration problem in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. The algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption."
3001,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the active pure exploration problem in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. The algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption."
3002,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the active pure exploration problem in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. The algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption."
3003,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a new query embedding model, namely Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. By further noticing that the closure of complement of cones remains cones, the geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
3004,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a new query embedding model, namely Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. By further noticing that the closure of complement of cones remains cones, the geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
3005,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a new query embedding model, namely Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. By further noticing that the closure of complement of cones remains cones, the geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
3006,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a new query embedding model, namely Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. By further noticing that the closure of complement of cones remains cones, the geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
3007,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a new query embedding model, namely Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. By further noticing that the closure of complement of cones remains cones, the geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
3008,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper studies the value iteration (VI) problem in the conjugate domain. In particular, the authors propose a novel numerical scheme for implementation of the corresponding value iteration algorithm. The proposed method is based on discretization of size X and U for the state and input spaces, respectively, which reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition. The authors provide convergence, time complexity, and error analysis of the proposed algorithm."
3009,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper studies the value iteration (VI) problem in the conjugate domain. In particular, the authors propose a novel numerical scheme for implementation of the corresponding value iteration algorithm. The proposed method is based on discretization of size X and U for the state and input spaces, respectively, which reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition. The authors provide convergence, time complexity, and error analysis of the proposed algorithm."
3010,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper studies the value iteration (VI) problem in the conjugate domain. In particular, the authors propose a novel numerical scheme for implementation of the corresponding value iteration algorithm. The proposed method is based on discretization of size X and U for the state and input spaces, respectively, which reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition. The authors provide convergence, time complexity, and error analysis of the proposed algorithm."
3011,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper studies the value iteration (VI) problem in the conjugate domain. In particular, the authors propose a novel numerical scheme for implementation of the corresponding value iteration algorithm. The proposed method is based on discretization of size X and U for the state and input spaces, respectively, which reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition. The authors provide convergence, time complexity, and error analysis of the proposed algorithm."
3012,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper studies the value iteration (VI) problem in the conjugate domain. In particular, the authors propose a novel numerical scheme for implementation of the corresponding value iteration algorithm. The proposed method is based on discretization of size X and U for the state and input spaces, respectively, which reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition. The authors provide convergence, time complexity, and error analysis of the proposed algorithm."
3013,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a method for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multi-modal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to video retrieval. The results show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task. The paper also shows a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
3014,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a method for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multi-modal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to video retrieval. The results show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task. The paper also shows a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
3015,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a method for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multi-modal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to video retrieval. The results show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task. The paper also shows a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
3016,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a method for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multi-modal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to video retrieval. The results show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task. The paper also shows a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
3017,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a method for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multi-modal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to video retrieval. The results show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task. The paper also shows a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
3018,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes a method to reduce the computational cost of self-attention in transformers by replacing the softmax structure with a Gaussian kernel and adapting the Nyström method to a non-positive semidefinite matrix to accelerate the computation. The proposed method is evaluated on the Long Range Arena benchmark and achieves comparable or better performance than the full self attention while requiring fewer computation resources.
3019,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes a method to reduce the computational cost of self-attention in transformers by replacing the softmax structure with a Gaussian kernel and adapting the Nyström method to a non-positive semidefinite matrix to accelerate the computation. The proposed method is evaluated on the Long Range Arena benchmark and achieves comparable or better performance than the full self attention while requiring fewer computation resources.
3020,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes a method to reduce the computational cost of self-attention in transformers by replacing the softmax structure with a Gaussian kernel and adapting the Nyström method to a non-positive semidefinite matrix to accelerate the computation. The proposed method is evaluated on the Long Range Arena benchmark and achieves comparable or better performance than the full self attention while requiring fewer computation resources.
3021,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes a method to reduce the computational cost of self-attention in transformers by replacing the softmax structure with a Gaussian kernel and adapting the Nyström method to a non-positive semidefinite matrix to accelerate the computation. The proposed method is evaluated on the Long Range Arena benchmark and achieves comparable or better performance than the full self attention while requiring fewer computation resources.
3022,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes a method to reduce the computational cost of self-attention in transformers by replacing the softmax structure with a Gaussian kernel and adapting the Nyström method to a non-positive semidefinite matrix to accelerate the computation. The proposed method is evaluated on the Long Range Arena benchmark and achieves comparable or better performance than the full self attention while requiring fewer computation resources.
3023,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. Specifically, the proposed method combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The proposed method is applied to the policy cloning setting, which allows for offline queries of an expert or expert policy. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
3024,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. Specifically, the proposed method combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The proposed method is applied to the policy cloning setting, which allows for offline queries of an expert or expert policy. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
3025,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. Specifically, the proposed method combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The proposed method is applied to the policy cloning setting, which allows for offline queries of an expert or expert policy. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
3026,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. Specifically, the proposed method combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The proposed method is applied to the policy cloning setting, which allows for offline queries of an expert or expert policy. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
3027,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. Specifically, the proposed method combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The proposed method is applied to the policy cloning setting, which allows for offline queries of an expert or expert policy. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
3028,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes a method for designing robust objects for computer vision tasks. The method is based on the observation that robustness to input perturbations is important in computer vision settings. The authors propose a method to design robust objects that are explicitly optimized to be confidently classified. The proposed method is evaluated on standard benchmarks, a simulated robotics environment, and physical-world experiments."
3029,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes a method for designing robust objects for computer vision tasks. The method is based on the observation that robustness to input perturbations is important in computer vision settings. The authors propose a method to design robust objects that are explicitly optimized to be confidently classified. The proposed method is evaluated on standard benchmarks, a simulated robotics environment, and physical-world experiments."
3030,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes a method for designing robust objects for computer vision tasks. The method is based on the observation that robustness to input perturbations is important in computer vision settings. The authors propose a method to design robust objects that are explicitly optimized to be confidently classified. The proposed method is evaluated on standard benchmarks, a simulated robotics environment, and physical-world experiments."
3031,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes a method for designing robust objects for computer vision tasks. The method is based on the observation that robustness to input perturbations is important in computer vision settings. The authors propose a method to design robust objects that are explicitly optimized to be confidently classified. The proposed method is evaluated on standard benchmarks, a simulated robotics environment, and physical-world experiments."
3032,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes a method for designing robust objects for computer vision tasks. The method is based on the observation that robustness to input perturbations is important in computer vision settings. The authors propose a method to design robust objects that are explicitly optimized to be confidently classified. The proposed method is evaluated on standard benchmarks, a simulated robotics environment, and physical-world experiments."
3033,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the instability of image-based reinforcement learning algorithms under data augmentation. The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique for stabilizing this class of algorithms. The proposed method improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image based RL in environments with unseen visuals. They further show that the proposed method scales to RL with ViT-based architectures."
3034,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the instability of image-based reinforcement learning algorithms under data augmentation. The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique for stabilizing this class of algorithms. The proposed method improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image based RL in environments with unseen visuals. They further show that the proposed method scales to RL with ViT-based architectures."
3035,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the instability of image-based reinforcement learning algorithms under data augmentation. The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique for stabilizing this class of algorithms. The proposed method improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image based RL in environments with unseen visuals. They further show that the proposed method scales to RL with ViT-based architectures."
3036,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the instability of image-based reinforcement learning algorithms under data augmentation. The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique for stabilizing this class of algorithms. The proposed method improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image based RL in environments with unseen visuals. They further show that the proposed method scales to RL with ViT-based architectures."
3037,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the instability of image-based reinforcement learning algorithms under data augmentation. The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique for stabilizing this class of algorithms. The proposed method improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image based RL in environments with unseen visuals. They further show that the proposed method scales to RL with ViT-based architectures."
3038,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the trade-off between the number of local updates and the minibatch size in federated learning (FL) algorithms. Specifically, the authors consider the case where both the WN's and the server's update directions are chosen based on certain stochastic momentum estimator. The authors show that for FedAvg (a momentum-less special case of the STEM), the algorithm requires $\epsilon$(3/2) samples and $\ell_1$ communication rounds to compute an ✏-stationary solution. In contrast, for the classical FedAvg, the paper shows that the algorithm has worse sample and communication complexities."
3039,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the trade-off between the number of local updates and the minibatch size in federated learning (FL) algorithms. Specifically, the authors consider the case where both the WN's and the server's update directions are chosen based on certain stochastic momentum estimator. The authors show that for FedAvg (a momentum-less special case of the STEM), the algorithm requires $\epsilon$(3/2) samples and $\ell_1$ communication rounds to compute an ✏-stationary solution. In contrast, for the classical FedAvg, the paper shows that the algorithm has worse sample and communication complexities."
3040,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the trade-off between the number of local updates and the minibatch size in federated learning (FL) algorithms. Specifically, the authors consider the case where both the WN's and the server's update directions are chosen based on certain stochastic momentum estimator. The authors show that for FedAvg (a momentum-less special case of the STEM), the algorithm requires $\epsilon$(3/2) samples and $\ell_1$ communication rounds to compute an ✏-stationary solution. In contrast, for the classical FedAvg, the paper shows that the algorithm has worse sample and communication complexities."
3041,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the trade-off between the number of local updates and the minibatch size in federated learning (FL) algorithms. Specifically, the authors consider the case where both the WN's and the server's update directions are chosen based on certain stochastic momentum estimator. The authors show that for FedAvg (a momentum-less special case of the STEM), the algorithm requires $\epsilon$(3/2) samples and $\ell_1$ communication rounds to compute an ✏-stationary solution. In contrast, for the classical FedAvg, the paper shows that the algorithm has worse sample and communication complexities."
3042,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the trade-off between the number of local updates and the minibatch size in federated learning (FL) algorithms. Specifically, the authors consider the case where both the WN's and the server's update directions are chosen based on certain stochastic momentum estimator. The authors show that for FedAvg (a momentum-less special case of the STEM), the algorithm requires $\epsilon$(3/2) samples and $\ell_1$ communication rounds to compute an ✏-stationary solution. In contrast, for the classical FedAvg, the paper shows that the algorithm has worse sample and communication complexities."
3043,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the information-theoretical generalization bound for large models trained by Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. In particular, the authors show that with constraint to guarantee low empirical risk, the optimal noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors then apply matrix analysis to derive the form of optimal covariance."
3044,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the information-theoretical generalization bound for large models trained by Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. In particular, the authors show that with constraint to guarantee low empirical risk, the optimal noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors then apply matrix analysis to derive the form of optimal covariance."
3045,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the information-theoretical generalization bound for large models trained by Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. In particular, the authors show that with constraint to guarantee low empirical risk, the optimal noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors then apply matrix analysis to derive the form of optimal covariance."
3046,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the information-theoretical generalization bound for large models trained by Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. In particular, the authors show that with constraint to guarantee low empirical risk, the optimal noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors then apply matrix analysis to derive the form of optimal covariance."
3047,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the information-theoretical generalization bound for large models trained by Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. In particular, the authors show that with constraint to guarantee low empirical risk, the optimal noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors then apply matrix analysis to derive the form of optimal covariance."
3048,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a method for video compression. The proposed method is based on a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The paper also proposes a flow prediction module to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed method achieves comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
3049,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a method for video compression. The proposed method is based on a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The paper also proposes a flow prediction module to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed method achieves comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
3050,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a method for video compression. The proposed method is based on a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The paper also proposes a flow prediction module to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed method achieves comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
3051,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a method for video compression. The proposed method is based on a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The paper also proposes a flow prediction module to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed method achieves comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
3052,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a method for video compression. The proposed method is based on a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The paper also proposes a flow prediction module to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed method achieves comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
3053,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the problem of multitask generalization of Online Gradient Descent (OMD). The main contribution of this paper is to prove that the regret of MT-OMD is of order $\frac{1 + \sqrt{2}(N-1) \log T)$, where $N$ is the number of tasks, $T$ is time horizon, and $O(N)$ is a regularizer. The paper also provides numerical experiments on several real-world datasets."
3054,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the problem of multitask generalization of Online Gradient Descent (OMD). The main contribution of this paper is to prove that the regret of MT-OMD is of order $\frac{1 + \sqrt{2}(N-1) \log T)$, where $N$ is the number of tasks, $T$ is time horizon, and $O(N)$ is a regularizer. The paper also provides numerical experiments on several real-world datasets."
3055,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the problem of multitask generalization of Online Gradient Descent (OMD). The main contribution of this paper is to prove that the regret of MT-OMD is of order $\frac{1 + \sqrt{2}(N-1) \log T)$, where $N$ is the number of tasks, $T$ is time horizon, and $O(N)$ is a regularizer. The paper also provides numerical experiments on several real-world datasets."
3056,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the problem of multitask generalization of Online Gradient Descent (OMD). The main contribution of this paper is to prove that the regret of MT-OMD is of order $\frac{1 + \sqrt{2}(N-1) \log T)$, where $N$ is the number of tasks, $T$ is time horizon, and $O(N)$ is a regularizer. The paper also provides numerical experiments on several real-world datasets."
3057,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the problem of multitask generalization of Online Gradient Descent (OMD). The main contribution of this paper is to prove that the regret of MT-OMD is of order $\frac{1 + \sqrt{2}(N-1) \log T)$, where $N$ is the number of tasks, $T$ is time horizon, and $O(N)$ is a regularizer. The paper also provides numerical experiments on several real-world datasets."
3058,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with stronglyconvex potential consisting of finite summation of N smooth components, and proposes an efficient discretization method, which requires O(N + d 1 3N 2 3 /ε 2 3 ) gradient evaluations to achieve ε-error (in √E\�·∥2 distance) for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+ d1 3N^{2 3 /\epsilon^2})$, which indicates that the method is optimal in dependence of N, ε, and d. The authors apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms. Experimental results on both synthetic and real-world data show that the proposed method consistently outperforms the existing ULD approaches."
3059,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with stronglyconvex potential consisting of finite summation of N smooth components, and proposes an efficient discretization method, which requires O(N + d 1 3N 2 3 /ε 2 3 ) gradient evaluations to achieve ε-error (in √E\�·∥2 distance) for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+ d1 3N^{2 3 /\epsilon^2})$, which indicates that the method is optimal in dependence of N, ε, and d. The authors apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms. Experimental results on both synthetic and real-world data show that the proposed method consistently outperforms the existing ULD approaches."
3060,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with stronglyconvex potential consisting of finite summation of N smooth components, and proposes an efficient discretization method, which requires O(N + d 1 3N 2 3 /ε 2 3 ) gradient evaluations to achieve ε-error (in √E\�·∥2 distance) for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+ d1 3N^{2 3 /\epsilon^2})$, which indicates that the method is optimal in dependence of N, ε, and d. The authors apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms. Experimental results on both synthetic and real-world data show that the proposed method consistently outperforms the existing ULD approaches."
3061,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with stronglyconvex potential consisting of finite summation of N smooth components, and proposes an efficient discretization method, which requires O(N + d 1 3N 2 3 /ε 2 3 ) gradient evaluations to achieve ε-error (in √E\�·∥2 distance) for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+ d1 3N^{2 3 /\epsilon^2})$, which indicates that the method is optimal in dependence of N, ε, and d. The authors apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms. Experimental results on both synthetic and real-world data show that the proposed method consistently outperforms the existing ULD approaches."
3062,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with stronglyconvex potential consisting of finite summation of N smooth components, and proposes an efficient discretization method, which requires O(N + d 1 3N 2 3 /ε 2 3 ) gradient evaluations to achieve ε-error (in √E\�·∥2 distance) for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+ d1 3N^{2 3 /\epsilon^2})$, which indicates that the method is optimal in dependence of N, ε, and d. The authors apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms. Experimental results on both synthetic and real-world data show that the proposed method consistently outperforms the existing ULD approaches."
3063,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper proposes a method for continual learning that combines Bayesian weight regularization and projected gradient descent. The main idea of the method is to use the prior precision to guide the optimization process. The method is evaluated on both feedforward and recurrent networks.
3064,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper proposes a method for continual learning that combines Bayesian weight regularization and projected gradient descent. The main idea of the method is to use the prior precision to guide the optimization process. The method is evaluated on both feedforward and recurrent networks.
3065,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper proposes a method for continual learning that combines Bayesian weight regularization and projected gradient descent. The main idea of the method is to use the prior precision to guide the optimization process. The method is evaluated on both feedforward and recurrent networks.
3066,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper proposes a method for continual learning that combines Bayesian weight regularization and projected gradient descent. The main idea of the method is to use the prior precision to guide the optimization process. The method is evaluated on both feedforward and recurrent networks.
3067,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper proposes a method for continual learning that combines Bayesian weight regularization and projected gradient descent. The main idea of the method is to use the prior precision to guide the optimization process. The method is evaluated on both feedforward and recurrent networks.
3068,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to compute the gradient of the volume-change term of normalizing flows. The first method is based on automatic differentiation and techniques from numerical linear algebra, while the second method is a combination of the two. Both methods are evaluated on out-of-distribution detection and density estimation."
3069,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to compute the gradient of the volume-change term of normalizing flows. The first method is based on automatic differentiation and techniques from numerical linear algebra, while the second method is a combination of the two. Both methods are evaluated on out-of-distribution detection and density estimation."
3070,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to compute the gradient of the volume-change term of normalizing flows. The first method is based on automatic differentiation and techniques from numerical linear algebra, while the second method is a combination of the two. Both methods are evaluated on out-of-distribution detection and density estimation."
3071,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to compute the gradient of the volume-change term of normalizing flows. The first method is based on automatic differentiation and techniques from numerical linear algebra, while the second method is a combination of the two. Both methods are evaluated on out-of-distribution detection and density estimation."
3072,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to compute the gradient of the volume-change term of normalizing flows. The first method is based on automatic differentiation and techniques from numerical linear algebra, while the second method is a combination of the two. Both methods are evaluated on out-of-distribution detection and density estimation."
3073,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method for inference and learning in neural networks with slow components. The proposed method is based on the idea of latent equilibria, which is a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. The authors derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases."
3074,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method for inference and learning in neural networks with slow components. The proposed method is based on the idea of latent equilibria, which is a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. The authors derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases."
3075,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method for inference and learning in neural networks with slow components. The proposed method is based on the idea of latent equilibria, which is a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. The authors derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases."
3076,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method for inference and learning in neural networks with slow components. The proposed method is based on the idea of latent equilibria, which is a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. The authors derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases."
3077,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method for inference and learning in neural networks with slow components. The proposed method is based on the idea of latent equilibria, which is a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. The authors derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases."
3078,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph classification, called Nested Graph Neural Networks (NGNN). The key idea is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph from each node and applies a base GNN to each subgraph to learn a representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis shows that NGNN is strictly more powerful than 1-WL. Experiments on several benchmark datasets show the effectiveness of the proposed method."
3079,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph classification, called Nested Graph Neural Networks (NGNN). The key idea is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph from each node and applies a base GNN to each subgraph to learn a representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis shows that NGNN is strictly more powerful than 1-WL. Experiments on several benchmark datasets show the effectiveness of the proposed method."
3080,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph classification, called Nested Graph Neural Networks (NGNN). The key idea is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph from each node and applies a base GNN to each subgraph to learn a representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis shows that NGNN is strictly more powerful than 1-WL. Experiments on several benchmark datasets show the effectiveness of the proposed method."
3081,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph classification, called Nested Graph Neural Networks (NGNN). The key idea is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph from each node and applies a base GNN to each subgraph to learn a representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis shows that NGNN is strictly more powerful than 1-WL. Experiments on several benchmark datasets show the effectiveness of the proposed method."
3082,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph classification, called Nested Graph Neural Networks (NGNN). The key idea is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph from each node and applies a base GNN to each subgraph to learn a representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis shows that NGNN is strictly more powerful than 1-WL. Experiments on several benchmark datasets show the effectiveness of the proposed method."
3083,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes a method for optimizing nested variational inference (NVI) for importance sampling. The main idea is to minimize the forward or reverse KL divergence at each level of nesting. This allows to learn intermediate densities, which can serve as heuristics to guide the sampler. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate density. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristic that approximate the likelihood of future observations in a hidden Markov model and (c) perform amortized inference in hierarchical deep generative models."
3084,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes a method for optimizing nested variational inference (NVI) for importance sampling. The main idea is to minimize the forward or reverse KL divergence at each level of nesting. This allows to learn intermediate densities, which can serve as heuristics to guide the sampler. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate density. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristic that approximate the likelihood of future observations in a hidden Markov model and (c) perform amortized inference in hierarchical deep generative models."
3085,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes a method for optimizing nested variational inference (NVI) for importance sampling. The main idea is to minimize the forward or reverse KL divergence at each level of nesting. This allows to learn intermediate densities, which can serve as heuristics to guide the sampler. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate density. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristic that approximate the likelihood of future observations in a hidden Markov model and (c) perform amortized inference in hierarchical deep generative models."
3086,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes a method for optimizing nested variational inference (NVI) for importance sampling. The main idea is to minimize the forward or reverse KL divergence at each level of nesting. This allows to learn intermediate densities, which can serve as heuristics to guide the sampler. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate density. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristic that approximate the likelihood of future observations in a hidden Markov model and (c) perform amortized inference in hierarchical deep generative models."
3087,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes a method for optimizing nested variational inference (NVI) for importance sampling. The main idea is to minimize the forward or reverse KL divergence at each level of nesting. This allows to learn intermediate densities, which can serve as heuristics to guide the sampler. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate density. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristic that approximate the likelihood of future observations in a hidden Markov model and (c) perform amortized inference in hierarchical deep generative models."
3088,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order optimization of a Lipschitz function f defined on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations required to find and certify an approximate maximizer of f at accuracy $\epsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral ∫ X dx/(max(f)− f(x) + ε). This result, which was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, the authors derive a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that they link to the above integral. They also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
3089,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order optimization of a Lipschitz function f defined on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations required to find and certify an approximate maximizer of f at accuracy $\epsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral ∫ X dx/(max(f)− f(x) + ε). This result, which was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, the authors derive a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that they link to the above integral. They also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
3090,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order optimization of a Lipschitz function f defined on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations required to find and certify an approximate maximizer of f at accuracy $\epsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral ∫ X dx/(max(f)− f(x) + ε). This result, which was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, the authors derive a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that they link to the above integral. They also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
3091,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order optimization of a Lipschitz function f defined on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations required to find and certify an approximate maximizer of f at accuracy $\epsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral ∫ X dx/(max(f)− f(x) + ε). This result, which was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, the authors derive a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that they link to the above integral. They also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
3092,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order optimization of a Lipschitz function f defined on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations required to find and certify an approximate maximizer of f at accuracy $\epsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral ∫ X dx/(max(f)− f(x) + ε). This result, which was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, the authors derive a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that they link to the above integral. They also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
3093,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a method to attack the uncertainty estimation of deep neural networks (DNNs). The method is based on the observation that DNNs are more confident of their incorrect predictions than about its correct ones. The authors propose two attacks: (1) a black-box attack, where the attacker has no knowledge of the target network, and (2) an attack that attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimation."
3094,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a method to attack the uncertainty estimation of deep neural networks (DNNs). The method is based on the observation that DNNs are more confident of their incorrect predictions than about its correct ones. The authors propose two attacks: (1) a black-box attack, where the attacker has no knowledge of the target network, and (2) an attack that attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimation."
3095,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a method to attack the uncertainty estimation of deep neural networks (DNNs). The method is based on the observation that DNNs are more confident of their incorrect predictions than about its correct ones. The authors propose two attacks: (1) a black-box attack, where the attacker has no knowledge of the target network, and (2) an attack that attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimation."
3096,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a method to attack the uncertainty estimation of deep neural networks (DNNs). The method is based on the observation that DNNs are more confident of their incorrect predictions than about its correct ones. The authors propose two attacks: (1) a black-box attack, where the attacker has no knowledge of the target network, and (2) an attack that attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimation."
3097,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a method to attack the uncertainty estimation of deep neural networks (DNNs). The method is based on the observation that DNNs are more confident of their incorrect predictions than about its correct ones. The authors propose two attacks: (1) a black-box attack, where the attacker has no knowledge of the target network, and (2) an attack that attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimation."
3098,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in a stochastic block model. In particular, the authors consider the setting where the number of nodes in the network is growing over time, and the goal is to detect the presence of a small number of well-connected “communities”. The authors propose a simple model for this setting, which they refer to as streaming StSBM, and prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. They validate their theoretical findings on synthetic and real data."
3099,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in a stochastic block model. In particular, the authors consider the setting where the number of nodes in the network is growing over time, and the goal is to detect the presence of a small number of well-connected “communities”. The authors propose a simple model for this setting, which they refer to as streaming StSBM, and prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. They validate their theoretical findings on synthetic and real data."
3100,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in a stochastic block model. In particular, the authors consider the setting where the number of nodes in the network is growing over time, and the goal is to detect the presence of a small number of well-connected “communities”. The authors propose a simple model for this setting, which they refer to as streaming StSBM, and prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. They validate their theoretical findings on synthetic and real data."
3101,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in a stochastic block model. In particular, the authors consider the setting where the number of nodes in the network is growing over time, and the goal is to detect the presence of a small number of well-connected “communities”. The authors propose a simple model for this setting, which they refer to as streaming StSBM, and prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. They validate their theoretical findings on synthetic and real data."
3102,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in a stochastic block model. In particular, the authors consider the setting where the number of nodes in the network is growing over time, and the goal is to detect the presence of a small number of well-connected “communities”. The authors propose a simple model for this setting, which they refer to as streaming StSBM, and prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. They validate their theoretical findings on synthetic and real data."
3103,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. In order to get a better understanding of how the architecture and parameterization affect the representation costs, the authors also study the reverse problem, identifying which regularizers onlinear predictors (e.g., lp quasi-norms, group quasi norms, the k-support-norm, elastic net) can be induced by simple l2-regularization and designing the parameterizations that do so."
3104,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. In order to get a better understanding of how the architecture and parameterization affect the representation costs, the authors also study the reverse problem, identifying which regularizers onlinear predictors (e.g., lp quasi-norms, group quasi norms, the k-support-norm, elastic net) can be induced by simple l2-regularization and designing the parameterizations that do so."
3105,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. In order to get a better understanding of how the architecture and parameterization affect the representation costs, the authors also study the reverse problem, identifying which regularizers onlinear predictors (e.g., lp quasi-norms, group quasi norms, the k-support-norm, elastic net) can be induced by simple l2-regularization and designing the parameterizations that do so."
3106,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. In order to get a better understanding of how the architecture and parameterization affect the representation costs, the authors also study the reverse problem, identifying which regularizers onlinear predictors (e.g., lp quasi-norms, group quasi norms, the k-support-norm, elastic net) can be induced by simple l2-regularization and designing the parameterizations that do so."
3107,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. In order to get a better understanding of how the architecture and parameterization affect the representation costs, the authors also study the reverse problem, identifying which regularizers onlinear predictors (e.g., lp quasi-norms, group quasi norms, the k-support-norm, elastic net) can be induced by simple l2-regularization and designing the parameterizations that do so."
3108,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a method for reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). The proposed method derives crisp logical rules for each query by finding multiple graph path patterns that connect similar source entities through the given relation. The method achieves new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122."
3109,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a method for reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). The proposed method derives crisp logical rules for each query by finding multiple graph path patterns that connect similar source entities through the given relation. The method achieves new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122."
3110,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,"This paper presents a Transformer-based entity linking model that combines a large scale pretraining from Wikipedia links. The model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The paper provides detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations."
3111,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,"This paper presents a Transformer-based entity linking model that combines a large scale pretraining from Wikipedia links. The model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The paper provides detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations."
3112,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes to use ensemble Active Learning methods to perform acquisition at a large scale (10k to 500k samples at a time) to perform training data subset search for large labeled datasets. They do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. They observe that their approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset."
3113,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes to use ensemble Active Learning methods to perform acquisition at a large scale (10k to 500k samples at a time) to perform training data subset search for large labeled datasets. They do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. They observe that their approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset."
3114,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes a new routing algorithm for capsule networks. The proposed routing algorithm is based on inverted dot-product attention, which imposes Layer Normalization as normalization, and concurrent iterative routing. The experimental results show that the proposed method improves performance on benchmark datasets such as CIFAR-10/100."
3115,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes a new routing algorithm for capsule networks. The proposed routing algorithm is based on inverted dot-product attention, which imposes Layer Normalization as normalization, and concurrent iterative routing. The experimental results show that the proposed method improves performance on benchmark datasets such as CIFAR-10/100."
3116,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a new method for hyperparameter optimization in deep neural networks. The main idea is to use the parallel tempering technique of statistical physics to sample non-local paths instead of points in the hyperparameters space. The authors show that this can lead to faster training and improved resistance to overfitting and show a systematic decrease in the absolute validation error, improving over benchmark results. Experiments are conducted on dropout and learning rate optimization."
3117,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a new method for hyperparameter optimization in deep neural networks. The main idea is to use the parallel tempering technique of statistical physics to sample non-local paths instead of points in the hyperparameters space. The authors show that this can lead to faster training and improved resistance to overfitting and show a systematic decrease in the absolute validation error, improving over benchmark results. Experiments are conducted on dropout and learning rate optimization."
3118,SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper studies the effect of reinforcement learning (RL) on the performance of machine translation (MT) models. The authors show that the most common RL methods for MT do not optimize the expected reward, and show that other methods take an infeasibly long time to converge. They further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve."
3119,SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper studies the effect of reinforcement learning (RL) on the performance of machine translation (MT) models. The authors show that the most common RL methods for MT do not optimize the expected reward, and show that other methods take an infeasibly long time to converge. They further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve."
3120,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the large-sample behavior of the Q-value estimates with closed-form characterizations of the asymptotic variances. The authors propose a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. This allows them to efficiently construct confidence regions for Q-values and optimal value functions, and to develop policies to minimize their estimation errors. Numerical experiments show superior performances of the exploration strategy than other benchmark approaches."
3121,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the large-sample behavior of the Q-value estimates with closed-form characterizations of the asymptotic variances. The authors propose a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. This allows them to efficiently construct confidence regions for Q-values and optimal value functions, and to develop policies to minimize their estimation errors. Numerical experiments show superior performances of the exploration strategy than other benchmark approaches."
3122,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,This paper proposes a collaborative generated hashing (CGH) method to improve the efficiency of the Top-k recommendation. CGH is designed to learn hash functions of users and items through the Minimum Description Length (MDL) principle. The authors also propose a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages of the proposed method over competing baselines.
3123,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,This paper proposes a collaborative generated hashing (CGH) method to improve the efficiency of the Top-k recommendation. CGH is designed to learn hash functions of users and items through the Minimum Description Length (MDL) principle. The authors also propose a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages of the proposed method over competing baselines.
3124,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,This paper proposes a transfer learning method for pharmacogenomics. The main idea is to use adversarial domain adaptation and multi-task learning to address the discrepancies in input and output spaces between source and target domains. The proposed method is the first adversarial inductive transfer learning (AITL) method to address both input-output discrepancies. Experimental results show that AITL outperforms state-of-the-art pharmacogenomic and transfer learning baselines and may guide precision oncology more accurately.
3125,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,This paper proposes a transfer learning method for pharmacogenomics. The main idea is to use adversarial domain adaptation and multi-task learning to address the discrepancies in input and output spaces between source and target domains. The proposed method is the first adversarial inductive transfer learning (AITL) method to address both input-output discrepancies. Experimental results show that AITL outperforms state-of-the-art pharmacogenomic and transfer learning baselines and may guide precision oncology more accurately.
3126,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward.
3127,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward.
3128,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a method to detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. The proposed method, called Reconstructive Attack, aims to cause a misclassification and a low reconstruction error. The authors show that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. They also show that CapsNets always perform better than convolutional networks."
3129,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a method to detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. The proposed method, called Reconstructive Attack, aims to cause a misclassification and a low reconstruction error. The authors show that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. They also show that CapsNets always perform better than convolutional networks."
3130,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the effect of initialization and the activation function on the Neural Tangent Kernel (NTK) as the network depth becomes large. In particular, the authors show that a special initialization known as the Edge of Chaos leads to good performance. They also provide experiments illustrating their theoretical results."
3131,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the effect of initialization and the activation function on the Neural Tangent Kernel (NTK) as the network depth becomes large. In particular, the authors show that a special initialization known as the Edge of Chaos leads to good performance. They also provide experiments illustrating their theoretical results."
3132,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. The method is based on contrasting different linguistic views, which aim at building embeddings which better capture semantic and are less sensitive to the sentence outward form. The paper is well-written and easy to follow."
3133,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. The method is based on contrasting different linguistic views, which aim at building embeddings which better capture semantic and are less sensitive to the sentence outward form. The paper is well-written and easy to follow."
3134,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper proposes a transfer learning method for financial sentiment classification. The method is based on the BERT language model, and is evaluated on the FinancialPhrasebank dataset. The results show that the proposed method outperforms the state-of-the-art."
3135,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper proposes a transfer learning method for financial sentiment classification. The method is based on the BERT language model, and is evaluated on the FinancialPhrasebank dataset. The results show that the proposed method outperforms the state-of-the-art."
3136,SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a method for singing voice generation without pre-assigned scores and lyrics, in both training and inference time. In particular, they propose three unconditioned or weakly conditioned voice generation schemes. They outline the associated challenges and propose a pipeline to tackle these new tasks. They also develop the source separation and transcription models for data preparation, adversarial networks for audio generation, and customized metrics for evaluation."
3137,SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a method for singing voice generation without pre-assigned scores and lyrics, in both training and inference time. In particular, they propose three unconditioned or weakly conditioned voice generation schemes. They outline the associated challenges and propose a pipeline to tackle these new tasks. They also develop the source separation and transcription models for data preparation, adversarial networks for audio generation, and customized metrics for evaluation."
3138,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"This paper proposes a novel adversarial defense technique that leverages a latent high order factorization of the network. The proposed method is based on randomizing the latent subspace of the neural network, which results in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization. Experiments on standard image classification benchmarks and audio classification tasks demonstrate the effectiveness of the proposed method."
3139,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"This paper proposes a novel adversarial defense technique that leverages a latent high order factorization of the network. The proposed method is based on randomizing the latent subspace of the neural network, which results in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization. Experiments on standard image classification benchmarks and audio classification tasks demonstrate the effectiveness of the proposed method."
3140,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,This paper proposes a novel architecture for spatiotemporal forecasting based on graph attention networks. The proposed architecture is based on a clustered graph transformer framework that integrates both graph attention network and transformer under an encoder-decoder architecture to address the unsmoothness issue of the data. The paper also proposes a gradient-based clustering method to distribute different feature extractors to regions in different contexts. Experiments on real datasets obtained from a ride-hailing business show that the proposed method can achieve 10%-25% improvement than many state-of-the-art baselines.
3141,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,This paper proposes a novel architecture for spatiotemporal forecasting based on graph attention networks. The proposed architecture is based on a clustered graph transformer framework that integrates both graph attention network and transformer under an encoder-decoder architecture to address the unsmoothness issue of the data. The paper also proposes a gradient-based clustering method to distribute different feature extractors to regions in different contexts. Experiments on real datasets obtained from a ride-hailing business show that the proposed method can achieve 10%-25% improvement than many state-of-the-art baselines.
3142,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper provides theoretical analysis of the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network. The authors show the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI. The statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithm converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques of experience Replay and target networks. Furthermore, the authors propose the Minimax-DQN algorithm for zero-sum Markov game."
3143,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper provides theoretical analysis of the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network. The authors show the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI. The statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithm converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques of experience Replay and target networks. Furthermore, the authors propose the Minimax-DQN algorithm for zero-sum Markov game."
3144,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes a new attention architecture called PhraseTransformer to represent the compositional attentions in phrases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is incorporated the non-linear attention in the second phase. Besides representing the words of the sentence, the authors introduce hypernodes to represent candidate phrases in attention. The experimental performance has been greatly improved."
3145,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes a new attention architecture called PhraseTransformer to represent the compositional attentions in phrases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is incorporated the non-linear attention in the second phase. Besides representing the words of the sentence, the authors introduce hypernodes to represent candidate phrases in attention. The experimental performance has been greatly improved."
3146,SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper presents a modular neural network architecture MAIN that learns algorithms given a set of input-output examples. MAIN consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. It uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. "
3147,SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper presents a modular neural network architecture MAIN that learns algorithms given a set of input-output examples. MAIN consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. It uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. "
3148,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"This paper proposes a Monte Carlo Deep Neural Network Arithmetic (MCDA) method for determining the sensitivity of deep neural networks to quantization in floating point arithmetic. MCDA is based on Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The authors evaluate MCDA on pre-trained image classification models on the CIFAR-10 and ImageNet datasets. For the same network topology and dataset, they demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials."
3149,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"This paper proposes a Monte Carlo Deep Neural Network Arithmetic (MCDA) method for determining the sensitivity of deep neural networks to quantization in floating point arithmetic. MCDA is based on Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The authors evaluate MCDA on pre-trained image classification models on the CIFAR-10 and ImageNet datasets. For the same network topology and dataset, they demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials."
3150,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of ""objective mismatch"" in model-based reinforcement learning (MBRL). Specifically, the authors argue that there is an objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. The authors propose an initial method to mitigate the mismatch issue by reweighting dynamics model training."
3151,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of ""objective mismatch"" in model-based reinforcement learning (MBRL). Specifically, the authors argue that there is an objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. The authors propose an initial method to mitigate the mismatch issue by reweighting dynamics model training."
3152,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions. Specifically, the authors propose a method to transfer adversarial examples from task-specific to model-specific features within a CNN architecture that directly impacts the transferability of adversarial attacks. The proposed method is evaluated on undefended ImageNet models and achieves state-of-the-art targeted blackbox transfer-based attack results."
3153,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions. Specifically, the authors propose a method to transfer adversarial examples from task-specific to model-specific features within a CNN architecture that directly impacts the transferability of adversarial attacks. The proposed method is evaluated on undefended ImageNet models and achieves state-of-the-art targeted blackbox transfer-based attack results."
3154,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper proposes a new method for comparing policy gradients using Wasserstein distances (WDs) in a latent behavioral space. The main idea is to learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. The dual formulation of the dual formulation allows the authors to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. The authors also incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and behavior-guided Evolution Strategies, which outperform existing methods in a variety of challenging environments."
3155,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper proposes a new method for comparing policy gradients using Wasserstein distances (WDs) in a latent behavioral space. The main idea is to learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. The dual formulation of the dual formulation allows the authors to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. The authors also incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and behavior-guided Evolution Strategies, which outperform existing methods in a variety of challenging environments."
3156,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper proposes an adaptive learning rate for interpolation with gradients (ALI-G) algorithm. The main idea is to compute the learning rate in closed form at each iteration. The learning rate is a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. The authors provide convergence guarantees of ALI-Gs in the stochastic convex setting. The experiments on a variety of architectures and tasks demonstrate the effectiveness of the proposed algorithm."
3157,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper proposes an adaptive learning rate for interpolation with gradients (ALI-G) algorithm. The main idea is to compute the learning rate in closed form at each iteration. The learning rate is a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. The authors provide convergence guarantees of ALI-Gs in the stochastic convex setting. The experiments on a variety of architectures and tasks demonstrate the effectiveness of the proposed algorithm."
3158,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper investigates the role of bottom-up, horizontal, and top-down connections in perceptual grouping. Specifically, the authors evaluate two synthetic visual tasks, which stress low-level “Gestalt” vs. high-level object cues for perceptual grouping, and show that increasing the difficulty of either task strains learning for networks that rely solely on top-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping. Top-down connection rescues learning on task with high level object cues by modifying coarse predictions about the position of the target object."
3159,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper investigates the role of bottom-up, horizontal, and top-down connections in perceptual grouping. Specifically, the authors evaluate two synthetic visual tasks, which stress low-level “Gestalt” vs. high-level object cues for perceptual grouping, and show that increasing the difficulty of either task strains learning for networks that rely solely on top-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping. Top-down connection rescues learning on task with high level object cues by modifying coarse predictions about the position of the target object."
3160,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems, the authors propose to enforce DeepHoser regularizers by enforcing the same shrinking rate to all parameters, which is inefficient in increasing sparsity. The experiments are conducted on both element-wise and structural pruning."
3161,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems, the authors propose to enforce DeepHoser regularizers by enforcing the same shrinking rate to all parameters, which is inefficient in increasing sparsity. The experiments are conducted on both element-wise and structural pruning."
3162,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam, called SAdam, which achieves a data-dependent $O(\sqrt{T})$ regret bound for strongly convex functions. The main idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, the proposed SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop. Empirical results show that SAdam outperforms the state-of-the-art."
3163,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam, called SAdam, which achieves a data-dependent $O(\sqrt{T})$ regret bound for strongly convex functions. The main idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, the proposed SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop. Empirical results show that SAdam outperforms the state-of-the-art."
3164,SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a new BERT-based model, BlockBERT, which extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time. The authors conduct experiments on several benchmark question answering datasets with various paragraph lengths. They show that the proposed model uses 18.1% less memory and reduces the training time by 12.7-36.1%, while having comparable and sometimes better prediction accuracy."
3165,SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a new BERT-based model, BlockBERT, which extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time. The authors conduct experiments on several benchmark question answering datasets with various paragraph lengths. They show that the proposed model uses 18.1% less memory and reduces the training time by 12.7-36.1%, while having comparable and sometimes better prediction accuracy."
3166,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the behavior of pruning over the course of training, and finds that pruning’s effect on generalization relies more on the instability it generates (defined as the drops in test accuracy immediately following pruning) than on the final size of the pruned model. The authors demonstrate that even the pruning of unimportant parameters can lead to such instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks."
3167,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the behavior of pruning over the course of training, and finds that pruning’s effect on generalization relies more on the instability it generates (defined as the drops in test accuracy immediately following pruning) than on the final size of the pruned model. The authors demonstrate that even the pruning of unimportant parameters can lead to such instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks."
3168,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,This paper proposes a method for neural architecture search (NAS) with reinforcement learning. The main idea is to use reinforcement learning to search in an embedding space by using architecture encoders and decoders. The proposed method is evaluated on CIFAR-10 and compared with other NAS methods.
3169,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,This paper proposes a method for neural architecture search (NAS) with reinforcement learning. The main idea is to use reinforcement learning to search in an embedding space by using architecture encoders and decoders. The proposed method is evaluated on CIFAR-10 and compared with other NAS methods.
3170,SP:e2e5bebccc76a51df3cb8b64572720da97174604,This paper proposes a homotopy training algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupling systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotoopy path guided by the HTA. The paper has proved the convergence of HTA for the non-convex case and existence of the homoopy solution path for the convex case. The proposed HTA has provided better accuracy on several examples including VGG models on CIFAR-10.
3171,SP:e2e5bebccc76a51df3cb8b64572720da97174604,This paper proposes a homotopy training algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupling systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotoopy path guided by the HTA. The paper has proved the convergence of HTA for the non-convex case and existence of the homoopy solution path for the convex case. The proposed HTA has provided better accuracy on several examples including VGG models on CIFAR-10.
3172,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,This paper proposes a 2-simplicial transformer architecture for deep reinforcement learning. The main idea is to extend the Transformer architecture by adding a higher-dimensional attention layer that generalizes the dot-product attention. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep RL.
3173,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,This paper proposes a 2-simplicial transformer architecture for deep reinforcement learning. The main idea is to extend the Transformer architecture by adding a higher-dimensional attention layer that generalizes the dot-product attention. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep RL.
3174,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"This paper proposes a method for pose estimation based on conditional variational autoencoders (CVAEs). The proposed method is based on the circular latent representation of CVAEs, which is used to estimate the corresponding 2D rotations of an object within an image with respect to a fixed frame of reference. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing."
3175,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"This paper proposes a method for pose estimation based on conditional variational autoencoders (CVAEs). The proposed method is based on the circular latent representation of CVAEs, which is used to estimate the corresponding 2D rotations of an object within an image with respect to a fixed frame of reference. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing."
3176,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a curriculum learning method for multi-agent reinforcement learning (MARL). The main idea is to progressively increase the population of training agents in a stage-wise manner. The authors argue that agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. To address this issue, the authors propose to mix-and-match and fine-tune over these sets and promote the sets of agents with the best adaptability to the next stage. Empirical results on a popular MARL algorithm, MADDPG, show that the proposed method consistently outperforms baselines by a large margin as the number of agents grows exponentially."
3177,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a curriculum learning method for multi-agent reinforcement learning (MARL). The main idea is to progressively increase the population of training agents in a stage-wise manner. The authors argue that agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. To address this issue, the authors propose to mix-and-match and fine-tune over these sets and promote the sets of agents with the best adaptability to the next stage. Empirical results on a popular MARL algorithm, MADDPG, show that the proposed method consistently outperforms baselines by a large margin as the number of agents grows exponentially."
3178,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,This paper presents a new method for analyzing the performance of multi-link graph neural networks (MNNs) in the context of video captioning. The main contribution of the paper is the introduction of a new way to evaluate the quality of the reasoning performed by the MNNs. The paper is well-written and easy to follow. 
3179,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,This paper presents a new method for analyzing the performance of multi-link graph neural networks (MNNs) in the context of video captioning. The main contribution of the paper is the introduction of a new way to evaluate the quality of the reasoning performed by the MNNs. The paper is well-written and easy to follow. 
3180,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors show that ATMC is intrinsically robust to overfitting on the training data and provides a better calibrated measure of uncertainty compared to the optimization baseline.
3181,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors show that ATMC is intrinsically robust to overfitting on the training data and provides a better calibrated measure of uncertainty compared to the optimization baseline.
3182,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper proposes a method for identifying early-bird (EB) tickets in deep neural networks. EB tickets are small subnetworks that emerge early in the training process. The authors propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Then, they leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy."
3183,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper proposes a method for identifying early-bird (EB) tickets in deep neural networks. EB tickets are small subnetworks that emerge early in the training process. The authors propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Then, they leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy."
3184,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,This paper proposes a method to improve the adversarial robustness of the classifier through embedding regularization. The method is based on the observation that the intrinsic dimension of image data is much smaller than its pixel space dimension and the vulnerability of neural networks grows with the input dimension. The authors propose to embed high-dimensional input images into a low-dimensional space to perform classification. Experimental results on several benchmark datasets show that the proposed method achieves state-of-the-art performance against strong adversarial attack methods.
3185,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,This paper proposes a method to improve the adversarial robustness of the classifier through embedding regularization. The method is based on the observation that the intrinsic dimension of image data is much smaller than its pixel space dimension and the vulnerability of neural networks grows with the input dimension. The authors propose to embed high-dimensional input images into a low-dimensional space to perform classification. Experimental results on several benchmark datasets show that the proposed method achieves state-of-the-art performance against strong adversarial attack methods.
3186,SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. The model is fast to train and avoids recurrence for self-attention. On 5 datasets across 3 domains, the model matches or exceeds state-of-the-art performance, sometimes by a large margin."
3187,SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. The model is fast to train and avoids recurrence for self-attention. On 5 datasets across 3 domains, the model matches or exceeds state-of-the-art performance, sometimes by a large margin."
3188,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of representation learning from a new perspective. The authors assume that no meaningful representation of the items is given. Instead, they provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers to the above-mentioned triplet comparisons. They demonstrate that their proposed approach is significantly faster than available methods, and can scale to real-world large datasets."
3189,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of representation learning from a new perspective. The authors assume that no meaningful representation of the items is given. Instead, they provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers to the above-mentioned triplet comparisons. They demonstrate that their proposed approach is significantly faster than available methods, and can scale to real-world large datasets."
3190,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"This paper proposes a meta-learning method for data value estimation. The proposed method is based on reinforcement learning. The method is evaluated on corrupted sample discovery, domain adaptation, and robust learning."
3191,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"This paper proposes a meta-learning method for data value estimation. The proposed method is based on reinforcement learning. The method is evaluated on corrupted sample discovery, domain adaptation, and robust learning."
3192,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the per-layer variance of the Jacobian squared norm for random fully connected ReLU networks. The authors compare three types of architectures: vanilla networks, ResNets and DenseNets. They show that while the variance of Jacobian variance is exponential in depth for ResNet and polynomial for DenseNet, there exists an initialization strategy for both, such that the norm is preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. They also show that the statistics of the per layer Jacobian norm is a function of the architecture and the layer’s size, but surprisingly, not the layer's depth."
3193,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the per-layer variance of the Jacobian squared norm for random fully connected ReLU networks. The authors compare three types of architectures: vanilla networks, ResNets and DenseNets. They show that while the variance of Jacobian variance is exponential in depth for ResNet and polynomial for DenseNet, there exists an initialization strategy for both, such that the norm is preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. They also show that the statistics of the per layer Jacobian norm is a function of the architecture and the layer’s size, but surprisingly, not the layer's depth."
3194,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes a reinforcement learning-based method for optimizing neural networks. The main idea is to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. The authors propose an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experiments on real hardware shows that CHAMELEON provides 4.45x speed up in optimization time over AutoTVM while also improving inference time of the modern deep networks by 5.6%."
3195,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes a reinforcement learning-based method for optimizing neural networks. The main idea is to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. The authors propose an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experiments on real hardware shows that CHAMELEON provides 4.45x speed up in optimization time over AutoTVM while also improving inference time of the modern deep networks by 5.6%."
3196,SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper proposes a certified robustness method for top-k predictions. The proposed method is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. The authors derive a tight robustness in the $\ell_2$ norm for topk predictions when using randomized smoothhing with Gaussian noise. They also empirically evaluate their method on CIFAR10 and ImageNet."
3197,SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper proposes a certified robustness method for top-k predictions. The proposed method is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. The authors derive a tight robustness in the $\ell_2$ norm for topk predictions when using randomized smoothhing with Gaussian noise. They also empirically evaluate their method on CIFAR10 and ImageNet."
3198,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. Based on several approximations, the authors establish a quantitative relationship between model parameters’ gsNR and the generalization gap. Moreover, they show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of Dnns naturally produces large GSNRs during training, which is probably the key to DNN's remarkable generalization ability."
3199,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. Based on several approximations, the authors establish a quantitative relationship between model parameters’ gsNR and the generalization gap. Moreover, they show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of Dnns naturally produces large GSNRs during training, which is probably the key to DNN's remarkable generalization ability."
3200,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a method for answering complex logical queries on large-scale incomplete knowledge graphs (KGs). The main idea is to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. The main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a subset of answer entities of the query, and the authors show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entity. The authors propose to transform queries into a Disjunctive Normal Form, which can handle arbitrary logical queries with ∧, ∨, and ∃ operators in a scalable manner. Experiments on three large KGs demonstrate the effectiveness of the proposed method."
3201,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a method for answering complex logical queries on large-scale incomplete knowledge graphs (KGs). The main idea is to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. The main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a subset of answer entities of the query, and the authors show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entity. The authors propose to transform queries into a Disjunctive Normal Form, which can handle arbitrary logical queries with ∧, ∨, and ∃ operators in a scalable manner. Experiments on three large KGs demonstrate the effectiveness of the proposed method."
3202,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper studies the convergence of SGD with a consistent estimator. The authors show that consistent estimators converge to the same rate as unbiased estimators in strongly convex, convex and non-convex settings. They also show that the convergence rate of the unbiased estimator can be as expensive to compute as the full gradient when the training examples are interconnected."
3203,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper studies the convergence of SGD with a consistent estimator. The authors show that consistent estimators converge to the same rate as unbiased estimators in strongly convex, convex and non-convex settings. They also show that the convergence rate of the unbiased estimator can be as expensive to compute as the full gradient when the training examples are interconnected."
3204,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. To efficiently train OFA networks, they also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks (> 10) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.5x faster than MobileNetV3, 2.6× faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."
3205,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. To efficiently train OFA networks, they also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks (> 10) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.5x faster than MobileNetV3, 2.6× faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."
3206,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper proposes a method for answering compositional questions that require multiple steps of reasoning against text, especially when they involve discrete, symbolic operations. The authors extend Neural Module Networks (NMNs) by introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. The proposed method outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules."
3207,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper proposes a method for answering compositional questions that require multiple steps of reasoning against text, especially when they involve discrete, symbolic operations. The authors extend Neural Module Networks (NMNs) by introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. The proposed method outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules."
3208,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper studies the connection sensitivity as a form of gradients for network pruning. The authors propose a new initialization condition to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. They also analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. They empirically study the effect of supervision for pruning and demonstrate that their signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures."
3209,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper studies the connection sensitivity as a form of gradients for network pruning. The authors propose a new initialization condition to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. They also analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. They empirically study the effect of supervision for pruning and demonstrate that their signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures."
3210,SP:d5899cba36329d863513b91c2db57675086abc49,"This paper studies the choice of intra-layer sparse neural networks. The authors derive a new sparse neural network initialization scheme that allows them to explore the space of very deep sparse networks. They evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, they develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. They then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them."
3211,SP:d5899cba36329d863513b91c2db57675086abc49,"This paper studies the choice of intra-layer sparse neural networks. The authors derive a new sparse neural network initialization scheme that allows them to explore the space of very deep sparse networks. They evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, they develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. They then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them."
3212,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"This paper proposes a new initialization scheme for recurrent neural networks (RNNs) based on the mean field theory of signal propagation in LSTMs and GRUs. Specifically, the authors derive the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, they derive a novel initialization scheme that eliminates or reduces training instabilities. The authors demonstrate the efficacy of their initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower."
3213,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"This paper proposes a new initialization scheme for recurrent neural networks (RNNs) based on the mean field theory of signal propagation in LSTMs and GRUs. Specifically, the authors derive the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, they derive a novel initialization scheme that eliminates or reduces training instabilities. The authors demonstrate the efficacy of their initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower."
3214,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,This paper proposes a siamese network to improve the discriminative power of convolutional neural networks on a pair of neighboring scene images. It then exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label. Empirical results show that this approach provides a viable alternative to existing methods.
3215,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,This paper proposes a siamese network to improve the discriminative power of convolutional neural networks on a pair of neighboring scene images. It then exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label. Empirical results show that this approach provides a viable alternative to existing methods.
3216,SP:99c10e038939aa88fc112db10fe801b42360c8dc,This paper proposes a self-supervised method for monocular depth estimation. The main idea is to use a fixed pretrained semantic segmentation network to guide pixel-adaptive convolutions. The authors also propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling.
3217,SP:99c10e038939aa88fc112db10fe801b42360c8dc,This paper proposes a self-supervised method for monocular depth estimation. The main idea is to use a fixed pretrained semantic segmentation network to guide pixel-adaptive convolutions. The authors also propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling.
3218,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper studies the problem of label-flipping attacks, where an adversary relabels a small number of examples in a training set in order to degrade the performance of the resulting classifier. The authors propose a strategy to build linear classifiers based on deep features that are certifiably robust against a strong variant of label flipping, where the adversary can target each test example independently. In other words, for each test point, the classifier makes a prediction and includes a certification that its prediction would be the same had some number of training labels been changed adversarially."
3219,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper studies the problem of label-flipping attacks, where an adversary relabels a small number of examples in a training set in order to degrade the performance of the resulting classifier. The authors propose a strategy to build linear classifiers based on deep features that are certifiably robust against a strong variant of label flipping, where the adversary can target each test example independently. In other words, for each test point, the classifier makes a prediction and includes a certification that its prediction would be the same had some number of training labels been changed adversarially."
3220,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper proposes a differential privacy approach for outlier detection and novelty detection. The main contribution of this paper is the theoretical analysis of how differential privacy helps with the detection, and the extensive experiments to validate the effectiveness of differential privacy."
3221,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper proposes a differential privacy approach for outlier detection and novelty detection. The main contribution of this paper is the theoretical analysis of how differential privacy helps with the detection, and the extensive experiments to validate the effectiveness of differential privacy."
3222,SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. The authors show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions. To overcome this limitation, the authors propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. The proposed method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. The paper also proposes a simple scaling-based calibration that preforms well in the experimental tests."
3223,SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. The authors show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions. To overcome this limitation, the authors propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. The proposed method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. The paper also proposes a simple scaling-based calibration that preforms well in the experimental tests."
3224,SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a model that combines the structured-representational power of Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional Transformer language model. The authors show that there is shared structure between different NLP datasets that HUBERT is able to learn and leverage. They validate the effectiveness of their model on the GLUE benchmark and HANS dataset."
3225,SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a model that combines the structured-representational power of Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional Transformer language model. The authors show that there is shared structure between different NLP datasets that HUBERT is able to learn and leverage. They validate the effectiveness of their model on the GLUE benchmark and HANS dataset."
3226,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,This paper proposes a method for multi-agent reinforcement learning with hierarchical reinforcement learning. The main idea is to learn low-level physical controllers for balance and walking. The lower-level controllers are task-agnostic and can be shared by higher-level policies. The paper also proposes a partial parameter sharing approach wherein the lower level of the hierarchy is shared enabling learning using decentralized methods.
3227,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,This paper proposes a method for multi-agent reinforcement learning with hierarchical reinforcement learning. The main idea is to learn low-level physical controllers for balance and walking. The lower-level controllers are task-agnostic and can be shared by higher-level policies. The paper also proposes a partial parameter sharing approach wherein the lower level of the hierarchy is shared enabling learning using decentralized methods.
3228,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"This paper proposes a new graph pooling method for graph neural networks (GNNs). The proposed method leverages a spatial representation of the graph which makes the neural network aware of the differences between the nodes and also their locations in the graph. The spatial representation is obtained by a graph embedding method. The local feature extractor of the GNN distinguishes similar local structures in different locations. Moreover, the spatial representation can be utilized to simplify the graph down-sampling problem."
3229,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"This paper proposes a new graph pooling method for graph neural networks (GNNs). The proposed method leverages a spatial representation of the graph which makes the neural network aware of the differences between the nodes and also their locations in the graph. The spatial representation is obtained by a graph embedding method. The local feature extractor of the GNN distinguishes similar local structures in different locations. Moreover, the spatial representation can be utilized to simplify the graph down-sampling problem."
3230,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,This paper proposes a new pooling method for convolutional neural networks that is shift-equivalent and anti-aliasing. The proposed method is based on the Discrete Fourier Transform (DFT) and the proposed frequency pooling. Experiments on image classifications show that the proposed method improves accuracy and robustness w.r.t shifts of CNNs.
3231,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,This paper proposes a new pooling method for convolutional neural networks that is shift-equivalent and anti-aliasing. The proposed method is based on the Discrete Fourier Transform (DFT) and the proposed frequency pooling. Experiments on image classifications show that the proposed method improves accuracy and robustness w.r.t shifts of CNNs.
3232,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a method to improve the generalization ability of deep RL agents by introducing a randomized (convolutional neural network) neural network that randomly perturbs input observations. The authors also consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. The proposed method is evaluated on 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks."
3233,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a method to improve the generalization ability of deep RL agents by introducing a randomized (convolutional neural network) neural network that randomly perturbs input observations. The authors also consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. The proposed method is evaluated on 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks."
3234,SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation method for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. In this task, an explanation depends on both of the input images, so standard methods do not apply. The proposed method pairs a saliency map identifying important image regions with an attribute that best explains the match. The authors find that their explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition."
3235,SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation method for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. In this task, an explanation depends on both of the input images, so standard methods do not apply. The proposed method pairs a saliency map identifying important image regions with an attribute that best explains the match. The authors find that their explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition."
3236,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes WaveFlow, a small-footprint generative flow for raw audio, which is trained with maximum likelihood without density distillation and auxiliary losses as used in Parallel WaveNet. It provides a unified view of flow-based models for audio, including autoregressive flow (e.g., WaveNet) and bipartite flow. The authors systematically study these likelihood-based generative models for raw waveforms in terms of test likelihood and speech fidelity. They demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms."
3237,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes WaveFlow, a small-footprint generative flow for raw audio, which is trained with maximum likelihood without density distillation and auxiliary losses as used in Parallel WaveNet. It provides a unified view of flow-based models for audio, including autoregressive flow (e.g., WaveNet) and bipartite flow. The authors systematically study these likelihood-based generative models for raw waveforms in terms of test likelihood and speech fidelity. They demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms."
3238,SP:963e85369978dddcd9e3130bc11453696066bbf3,This paper proposes a novel graph translation-Generative-Adversarial-Networks (GT-GAN) method that transforms the input graphs into their target output graphs. GT-GAN consists of a graph translator equipped with graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate that the proposed method significantly outperforms other baseline methods in terms of both effectiveness and scalability.
3239,SP:963e85369978dddcd9e3130bc11453696066bbf3,This paper proposes a novel graph translation-Generative-Adversarial-Networks (GT-GAN) method that transforms the input graphs into their target output graphs. GT-GAN consists of a graph translator equipped with graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate that the proposed method significantly outperforms other baseline methods in terms of both effectiveness and scalability.
3240,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence modeling unit, called Meta Gated Recursive Controller (METAGROSS). The proposed model is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. The proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks)."
3241,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence modeling unit, called Meta Gated Recursive Controller (METAGROSS). The proposed model is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. The proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks)."
3242,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a new self-supervised objective for speech recognition. The proposed objective is based on the local prior matching (LPM) objective. The LPM objective leverages a strong language model to provide learning signal given unlabeled speech. The paper shows that LPM can reduce the WER by 26% and 31% relative on a clean and noisy test set, respectively. By augmenting LPM with an additional 500 hours of noisy data, the proposed objective can further improve the performance by 15% relative. Furthermore, extensive ablative studies are conducted to show the importance of various configurations of LPM."
3243,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a new self-supervised objective for speech recognition. The proposed objective is based on the local prior matching (LPM) objective. The LPM objective leverages a strong language model to provide learning signal given unlabeled speech. The paper shows that LPM can reduce the WER by 26% and 31% relative on a clean and noisy test set, respectively. By augmenting LPM with an additional 500 hours of noisy data, the proposed objective can further improve the performance by 15% relative. Furthermore, extensive ablative studies are conducted to show the importance of various configurations of LPM."
3244,SP:e6af249608633f1776b608852a00946a5c09a357,This paper proposes a new method for model training in the presence of data poisoning. The proposed method is based on a generative adversarial network (GAN) and consists of two discriminators: a fairness discriminator that predicts the sensitive attribute from classification results and a robustness discriminator which distinguishes examples and predictions from a clean validation set. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed method.
3245,SP:e6af249608633f1776b608852a00946a5c09a357,This paper proposes a new method for model training in the presence of data poisoning. The proposed method is based on a generative adversarial network (GAN) and consists of two discriminators: a fairness discriminator that predicts the sensitive attribute from classification results and a robustness discriminator which distinguishes examples and predictions from a clean validation set. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed method.
3246,SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. The proposed method jointly defines data in an Eulerian world space using a static background grid, and a Lagrangian material space, using moving particles. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation."
3247,SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. The proposed method jointly defines data in an Eulerian world space using a static background grid, and a Lagrangian material space, using moving particles. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation."
3248,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper studies the robustness of gradient clipping in the context of label noise. The authors prove that standard gradient clipping does not in general provide robustness. On the other hand, they show that simple gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. As a special case, this yields a simple, noise-robust modification of the standard cross-entropy loss which performs well empirically."
3249,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper studies the robustness of gradient clipping in the context of label noise. The authors prove that standard gradient clipping does not in general provide robustness. On the other hand, they show that simple gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. As a special case, this yields a simple, noise-robust modification of the standard cross-entropy loss which performs well empirically."
3250,SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state alignment based imitation learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and is combined with a reinforcement learning framework by a regularized policy update objective. The paper shows the superiority of the proposed method on standard imitation learning settings and imitation learning where the expert and imitators have different dynamics models.
3251,SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state alignment based imitation learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and is combined with a reinforcement learning framework by a regularized policy update objective. The paper shows the superiority of the proposed method on standard imitation learning settings and imitation learning where the expert and imitators have different dynamics models.
3252,SP:91761d68086330ce378507c152e72218ed7b2196,This paper proposes a novel extension of SGD called Deep Gradient Boosting (DGB) to the input normalization layer (INN) of deep neural networks. The idea is that back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. The weight update is calculated by solving the corresponding boosting problem using a linear base learner. The resulting weight update formula can also be seen as a normalization procedure of the data that arrives at each layer during the forward pass.
3253,SP:91761d68086330ce378507c152e72218ed7b2196,This paper proposes a novel extension of SGD called Deep Gradient Boosting (DGB) to the input normalization layer (INN) of deep neural networks. The idea is that back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. The weight update is calculated by solving the corresponding boosting problem using a linear base learner. The resulting weight update formula can also be seen as a normalization procedure of the data that arrives at each layer during the forward pass.
3254,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"This paper proposes a new architecture search method, called Partially-Connected DARTS (PC-DARTS), which is based on sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, the proposed method performs operation search in a subset of channels while bypassing the held out part in a shortcut. The paper also proposes edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search."
3255,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"This paper proposes a new architecture search method, called Partially-Connected DARTS (PC-DARTS), which is based on sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, the proposed method performs operation search in a subset of channels while bypassing the held out part in a shortcut. The paper also proposes edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search."
3256,SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a hybrid method that combines the best aspects of gradient-based methods and deep reinforcement learning (DRL). The authors base their algorithm on the deep deterministic policy gradients (DDPG) algorithm and propose a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. Empirical results show that the proposed method boosts the performance of DDPG without sacrificing its robustness to local minima.
3257,SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a hybrid method that combines the best aspects of gradient-based methods and deep reinforcement learning (DRL). The authors base their algorithm on the deep deterministic policy gradients (DDPG) algorithm and propose a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. Empirical results show that the proposed method boosts the performance of DDPG without sacrificing its robustness to local minima.
3258,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,This paper proposes a method to learn a task-agnostic world graph for reinforcement learning. The proposed method is based on a binary recurrent variational autoencoder (VAE) and a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. The method is evaluated on a suite of challenging maze tasks and shows that using world graphs significantly accelerates RL.
3259,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,This paper proposes a method to learn a task-agnostic world graph for reinforcement learning. The proposed method is based on a binary recurrent variational autoencoder (VAE) and a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. The method is evaluated on a suite of challenging maze tasks and shows that using world graphs significantly accelerates RL.
3260,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering neural networks. The method is based on discretization of the network layers, which allows the model to be interpretable without disordering the function blocks. The paper also proposes an end-to-end PathNet structure through this discretized network."
3261,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering neural networks. The method is based on discretization of the network layers, which allows the model to be interpretable without disordering the function blocks. The paper also proposes an end-to-end PathNet structure through this discretized network."
3262,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for self-supervised imitation learning without the need for expert demonstrations. The key observation is that in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. In particular, in the setting where the tasks correspond to different goals, every trajectory is a successful demonstration for the state that it actually reaches. Based on this observation, the authors propose a very simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. The method simply maximizes the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the one it actually reached."
3263,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for self-supervised imitation learning without the need for expert demonstrations. The key observation is that in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. In particular, in the setting where the tasks correspond to different goals, every trajectory is a successful demonstration for the state that it actually reaches. Based on this observation, the authors propose a very simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. The method simply maximizes the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the one it actually reached."
3264,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper proposes a new asynchronous stochastic gradient descent (SGD) algorithm, Zeno++, which is designed to tolerate Byzantine failures of the workers. The main idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. The authors prove the convergence of the proposed algorithm for non-convex problems under Byzantine failures."
3265,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper proposes a new asynchronous stochastic gradient descent (SGD) algorithm, Zeno++, which is designed to tolerate Byzantine failures of the workers. The main idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. The authors prove the convergence of the proposed algorithm for non-convex problems under Byzantine failures."
3266,SP:d16ed9bd4193d99774840783347137e938955b87,"This paper proposes a new adversarial attack method for image classification and image captioning tasks. The proposed method is based on the idea that the adversarial examples should be semantically meaningful. The authors propose to modify the color and texture of the image to make them more meaningful. They show that their method is effective against JPEG compression, feature squeezing and adversarially trained model. They also conduct user studies to show that the generated images are photorealistic to humans."
3267,SP:d16ed9bd4193d99774840783347137e938955b87,"This paper proposes a new adversarial attack method for image classification and image captioning tasks. The proposed method is based on the idea that the adversarial examples should be semantically meaningful. The authors propose to modify the color and texture of the image to make them more meaningful. They show that their method is effective against JPEG compression, feature squeezing and adversarially trained model. They also conduct user studies to show that the generated images are photorealistic to humans."
3268,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition. The method is based on the idea of learning a controller to execute an optimal sequence of read and write operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. The proposed method is evaluated on the Stanford Task-Oriented Dialogue dataset.
3269,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition. The method is based on the idea of learning a controller to execute an optimal sequence of read and write operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. The proposed method is evaluated on the Stanford Task-Oriented Dialogue dataset.
3270,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition. The method is based on the idea of learning a controller to execute an optimal sequence of read and write operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. The proposed method is evaluated on the Stanford Task-Oriented Dialogue dataset.
3271,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,This paper proposes a method to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. The core idea is to jointly learn both the underlying motor primitive and recomposing these primitives to form the original demonstration. The proposed method is based on constraints on both the parsimony of primitive decomposition and the simplicity of a given primitive. Experiments are conducted on reaching and pushing tasks.
3272,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,This paper proposes a method to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. The core idea is to jointly learn both the underlying motor primitive and recomposing these primitives to form the original demonstration. The proposed method is based on constraints on both the parsimony of primitive decomposition and the simplicity of a given primitive. Experiments are conducted on reaching and pushing tasks.
3273,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,This paper proposes a method to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. The core idea is to jointly learn both the underlying motor primitive and recomposing these primitives to form the original demonstration. The proposed method is based on constraints on both the parsimony of primitive decomposition and the simplicity of a given primitive. Experiments are conducted on reaching and pushing tasks.
3274,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for transfer learning from one environment to another. The main idea is to use a probe and an inference model to rapidly estimate underlying latent variables of the test dynamics, which are then used as input to a universal control policy. The proposed method is evaluated on a variety of domains with a single episode test constraint and shows favorable performance against baselines for robust transfer."
3275,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for transfer learning from one environment to another. The main idea is to use a probe and an inference model to rapidly estimate underlying latent variables of the test dynamics, which are then used as input to a universal control policy. The proposed method is evaluated on a variety of domains with a single episode test constraint and shows favorable performance against baselines for robust transfer."
3276,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for transfer learning from one environment to another. The main idea is to use a probe and an inference model to rapidly estimate underlying latent variables of the test dynamics, which are then used as input to a universal control policy. The proposed method is evaluated on a variety of domains with a single episode test constraint and shows favorable performance against baselines for robust transfer."
3277,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper proposes a three-head network architecture for AlphaZero reinforcement learning. The main idea is to learn a third action-value head on a fixed dataset the same as for two-head neural networks. The authors show that the proposed architecture is also advantageous at zero-style iterative learning, producing neural network models stronger than those from the two-headed counterpart in the same MCTS. The experiments are conducted on the game of Hex as a test domain."
3278,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper proposes a three-head network architecture for AlphaZero reinforcement learning. The main idea is to learn a third action-value head on a fixed dataset the same as for two-head neural networks. The authors show that the proposed architecture is also advantageous at zero-style iterative learning, producing neural network models stronger than those from the two-headed counterpart in the same MCTS. The experiments are conducted on the game of Hex as a test domain."
3279,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper proposes a three-head network architecture for AlphaZero reinforcement learning. The main idea is to learn a third action-value head on a fixed dataset the same as for two-head neural networks. The authors show that the proposed architecture is also advantageous at zero-style iterative learning, producing neural network models stronger than those from the two-headed counterpart in the same MCTS. The experiments are conducted on the game of Hex as a test domain."
3280,SP:89d6d55107b6180109affe7522265c751640ad96,This paper proposes an adaptation-to-learn (ATL) method for policy transfer in reinforcement learning. The idea is to adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. The authors show through theory and experiments that their method leads to a significantly reduced sample complexity of transferring the policies between the tasks.
3281,SP:89d6d55107b6180109affe7522265c751640ad96,This paper proposes an adaptation-to-learn (ATL) method for policy transfer in reinforcement learning. The idea is to adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. The authors show through theory and experiments that their method leads to a significantly reduced sample complexity of transferring the policies between the tasks.
3282,SP:89d6d55107b6180109affe7522265c751640ad96,This paper proposes an adaptation-to-learn (ATL) method for policy transfer in reinforcement learning. The idea is to adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. The authors show through theory and experiments that their method leads to a significantly reduced sample complexity of transferring the policies between the tasks.
3283,SP:626021101836a635ad2d896bd66951aff31aa846,This paper proposes a method for scale-equivariant convolutional networks with steerable filters. The main contribution of the paper is the general theory for building scale-convolutional convolution networks. The authors also generalize other common blocks to be scale-Equivariant. The experimental results on MNIST-scale dataset and STL-10 dataset demonstrate the effectiveness of the proposed method.
3284,SP:626021101836a635ad2d896bd66951aff31aa846,This paper proposes a method for scale-equivariant convolutional networks with steerable filters. The main contribution of the paper is the general theory for building scale-convolutional convolution networks. The authors also generalize other common blocks to be scale-Equivariant. The experimental results on MNIST-scale dataset and STL-10 dataset demonstrate the effectiveness of the proposed method.
3285,SP:626021101836a635ad2d896bd66951aff31aa846,This paper proposes a method for scale-equivariant convolutional networks with steerable filters. The main contribution of the paper is the general theory for building scale-convolutional convolution networks. The authors also generalize other common blocks to be scale-Equivariant. The experimental results on MNIST-scale dataset and STL-10 dataset demonstrate the effectiveness of the proposed method.
3286,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper proposes a method for 3D scan completion. The method is based on the idea of point cloud point clouds, where the input point clouds are generated by a point cloud model and the output point cloud is generated by an encoder-decoder model. The authors show that the proposed method can be applied to real-world datasets (ScanNet, Matterport3D, KITTI, 3D-EPN) and demonstrate realistic completions under varying levels of incompleteness."
3287,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper proposes a method for 3D scan completion. The method is based on the idea of point cloud point clouds, where the input point clouds are generated by a point cloud model and the output point cloud is generated by an encoder-decoder model. The authors show that the proposed method can be applied to real-world datasets (ScanNet, Matterport3D, KITTI, 3D-EPN) and demonstrate realistic completions under varying levels of incompleteness."
3288,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper proposes a method for 3D scan completion. The method is based on the idea of point cloud point clouds, where the input point clouds are generated by a point cloud model and the output point cloud is generated by an encoder-decoder model. The authors show that the proposed method can be applied to real-world datasets (ScanNet, Matterport3D, KITTI, 3D-EPN) and demonstrate realistic completions under varying levels of incompleteness."
3289,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data for authentication systems. The authors study the problem from a theoretical perspective and explore its practical implications. They cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. Their analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. They design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data."
3290,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data for authentication systems. The authors study the problem from a theoretical perspective and explore its practical implications. They cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. Their analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. They design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data."
3291,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data for authentication systems. The authors study the problem from a theoretical perspective and explore its practical implications. They cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. Their analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. They design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data."
3292,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper proposes a sensible adversarial learning (SAL) algorithm to improve the robustness and accuracy of deep neural networks. The idea is to train a robust model with sensible examples, without a significant drop in natural accuracy. The paper theoretically establishes that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under sensible adversary learning. The proposed algorithm is evaluated on CIFAR-10 and shows state-of-the-art results against various attacks with perturbations restricted to `∞ with = 8/255, e.g., PGD attacks."
3293,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper proposes a sensible adversarial learning (SAL) algorithm to improve the robustness and accuracy of deep neural networks. The idea is to train a robust model with sensible examples, without a significant drop in natural accuracy. The paper theoretically establishes that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under sensible adversary learning. The proposed algorithm is evaluated on CIFAR-10 and shows state-of-the-art results against various attacks with perturbations restricted to `∞ with = 8/255, e.g., PGD attacks."
3294,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper proposes a sensible adversarial learning (SAL) algorithm to improve the robustness and accuracy of deep neural networks. The idea is to train a robust model with sensible examples, without a significant drop in natural accuracy. The paper theoretically establishes that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under sensible adversary learning. The proposed algorithm is evaluated on CIFAR-10 and shows state-of-the-art results against various attacks with perturbations restricted to `∞ with = 8/255, e.g., PGD attacks."
3295,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish the feature maps distributions of different networks. Discriminators and networks are trained concurrently in a minimax twoplayer game. By training a network to fool the corresponding discriminator, it can learn the other network’s feature map distribution. Also, a novel cyclic learning scheme is proposed for training more than two networks together. Experiments on various network architectures on the classification task and discovered a significant improvement of performance especially in the case of training a pair of a small network and a large one."
3296,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish the feature maps distributions of different networks. Discriminators and networks are trained concurrently in a minimax twoplayer game. By training a network to fool the corresponding discriminator, it can learn the other network’s feature map distribution. Also, a novel cyclic learning scheme is proposed for training more than two networks together. Experiments on various network architectures on the classification task and discovered a significant improvement of performance especially in the case of training a pair of a small network and a large one."
3297,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish the feature maps distributions of different networks. Discriminators and networks are trained concurrently in a minimax twoplayer game. By training a network to fool the corresponding discriminator, it can learn the other network’s feature map distribution. Also, a novel cyclic learning scheme is proposed for training more than two networks together. Experiments on various network architectures on the classification task and discovered a significant improvement of performance especially in the case of training a pair of a small network and a large one."
3298,SP:e43fc8747f823be6497224696adb92d45150b02d,This paper proposes a new word embedding model for sentiment analysis. The proposed model is based on the maximum likelihood estimation and the Bayesian estimation. The model is evaluated on both semantic and sentiment analysis tasks.
3299,SP:e43fc8747f823be6497224696adb92d45150b02d,This paper proposes a new word embedding model for sentiment analysis. The proposed model is based on the maximum likelihood estimation and the Bayesian estimation. The model is evaluated on both semantic and sentiment analysis tasks.
3300,SP:e43fc8747f823be6497224696adb92d45150b02d,This paper proposes a new word embedding model for sentiment analysis. The proposed model is based on the maximum likelihood estimation and the Bayesian estimation. The model is evaluated on both semantic and sentiment analysis tasks.
3301,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method, called Prestopping, for noise-free training under any type of label noise. The first phase, called early stopping, trains a deep neural network before the noisy labels are severely memorized. The second phase is called maximal safe set, which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Experiments on four image benchmark data sets verify that the proposed method significantly outperforms four state-of-the-art methods in test error."
3302,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method, called Prestopping, for noise-free training under any type of label noise. The first phase, called early stopping, trains a deep neural network before the noisy labels are severely memorized. The second phase is called maximal safe set, which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Experiments on four image benchmark data sets verify that the proposed method significantly outperforms four state-of-the-art methods in test error."
3303,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method, called Prestopping, for noise-free training under any type of label noise. The first phase, called early stopping, trains a deep neural network before the noisy labels are severely memorized. The second phase is called maximal safe set, which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Experiments on four image benchmark data sets verify that the proposed method significantly outperforms four state-of-the-art methods in test error."
3304,SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a method for zero-shot generalization to unseen actions. The method is based on unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. The authors propose a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. The experiments demonstrate the generalizability of the representation learning method and policy."
3305,SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a method for zero-shot generalization to unseen actions. The method is based on unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. The authors propose a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. The experiments demonstrate the generalizability of the representation learning method and policy."
3306,SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a method for zero-shot generalization to unseen actions. The method is based on unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. The authors propose a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. The experiments demonstrate the generalizability of the representation learning method and policy."
3307,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes word2ket, a method for storing word embedding matrix during training and inference in a highly efficient way. The method is inspired by quantum computing. The authors propose two related methods, word2ketch and word_2ketXS, to store word embeddings. The main contribution of the paper is to propose word2Ketch, which is an efficient way to store the word2vec embedding vectors. The proposed method is evaluated on a variety of tasks, and compared to the state-of-the-art."
3308,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes word2ket, a method for storing word embedding matrix during training and inference in a highly efficient way. The method is inspired by quantum computing. The authors propose two related methods, word2ketch and word_2ketXS, to store word embeddings. The main contribution of the paper is to propose word2Ketch, which is an efficient way to store the word2vec embedding vectors. The proposed method is evaluated on a variety of tasks, and compared to the state-of-the-art."
3309,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes word2ket, a method for storing word embedding matrix during training and inference in a highly efficient way. The method is inspired by quantum computing. The authors propose two related methods, word2ketch and word_2ketXS, to store word embeddings. The main contribution of the paper is to propose word2Ketch, which is an efficient way to store the word2vec embedding vectors. The proposed method is evaluated on a variety of tasks, and compared to the state-of-the-art."
3310,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning that learns a policy from a set of demonstrations of different behaviors. The method is based on a neural network that is trained on a dataset of 7,777 human demonstrations for the build-order planning task in StarCraft II. The main idea is to learn a policy conditioned on a behavior description that can be precisely modulated. Experiments show that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, the method can adapt its behavior in-between games."
3311,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning that learns a policy from a set of demonstrations of different behaviors. The method is based on a neural network that is trained on a dataset of 7,777 human demonstrations for the build-order planning task in StarCraft II. The main idea is to learn a policy conditioned on a behavior description that can be precisely modulated. Experiments show that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, the method can adapt its behavior in-between games."
3312,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning that learns a policy from a set of demonstrations of different behaviors. The method is based on a neural network that is trained on a dataset of 7,777 human demonstrations for the build-order planning task in StarCraft II. The main idea is to learn a policy conditioned on a behavior description that can be precisely modulated. Experiments show that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, the method can adapt its behavior in-between games."
3313,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis of Frankle & Carbin (2018), which suggests that small, sparsified neural networks can achieve equally good accuracy as long as the network is initialized properly. The authors provide insights into the structure of these early winning tickets with supporting evidence. They show that under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates. They further explain why gradual pruning can achieve better accuracy over the one-shot pruning."
3314,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis of Frankle & Carbin (2018), which suggests that small, sparsified neural networks can achieve equally good accuracy as long as the network is initialized properly. The authors provide insights into the structure of these early winning tickets with supporting evidence. They show that under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates. They further explain why gradual pruning can achieve better accuracy over the one-shot pruning."
3315,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis of Frankle & Carbin (2018), which suggests that small, sparsified neural networks can achieve equally good accuracy as long as the network is initialized properly. The authors provide insights into the structure of these early winning tickets with supporting evidence. They show that under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates. They further explain why gradual pruning can achieve better accuracy over the one-shot pruning."
3316,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to detect unknown objects. The key idea is to enhance existing CNNs to support a product operation that models the product relationship among the features produced by convolutional layers. UDN uses a learned ensemble of these product operations, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting unknowns. To further improve the performance of UDN at detecting unknowns, the authors propose an information-theoretic regularization strategy that incorporates the objective of reject unknowns into the learning process of the UDN."
3317,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to detect unknown objects. The key idea is to enhance existing CNNs to support a product operation that models the product relationship among the features produced by convolutional layers. UDN uses a learned ensemble of these product operations, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting unknowns. To further improve the performance of UDN at detecting unknowns, the authors propose an information-theoretic regularization strategy that incorporates the objective of reject unknowns into the learning process of the UDN."
3318,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to detect unknown objects. The key idea is to enhance existing CNNs to support a product operation that models the product relationship among the features produced by convolutional layers. UDN uses a learned ensemble of these product operations, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting unknowns. To further improve the performance of UDN at detecting unknowns, the authors propose an information-theoretic regularization strategy that incorporates the objective of reject unknowns into the learning process of the UDN."
3319,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for variational inference (VI) for deep neural networks. The main idea is to train a coarse approximation of the posterior distribution of the model parameters, and then iteratively refine the approximation using a series of iterative steps. The authors show that this method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. They also show that their method consistently outperforms recent VI methods for deep learning in terms of log-likelihood and the ELBO."
3320,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for variational inference (VI) for deep neural networks. The main idea is to train a coarse approximation of the posterior distribution of the model parameters, and then iteratively refine the approximation using a series of iterative steps. The authors show that this method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. They also show that their method consistently outperforms recent VI methods for deep learning in terms of log-likelihood and the ELBO."
3321,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for variational inference (VI) for deep neural networks. The main idea is to train a coarse approximation of the posterior distribution of the model parameters, and then iteratively refine the approximation using a series of iterative steps. The authors show that this method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. They also show that their method consistently outperforms recent VI methods for deep learning in terms of log-likelihood and the ELBO."
3322,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a method to reduce the variance of the gradient of the policy gradient estimator for categorical sequence generation. The authors propose to use a set of correlated Monte Carlo (MC) rollouts for variance control, which are random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. The proposed method is evaluated on both neural program synthesis and image captioning."
3323,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a method to reduce the variance of the gradient of the policy gradient estimator for categorical sequence generation. The authors propose to use a set of correlated Monte Carlo (MC) rollouts for variance control, which are random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. The proposed method is evaluated on both neural program synthesis and image captioning."
3324,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a method to reduce the variance of the gradient of the policy gradient estimator for categorical sequence generation. The authors propose to use a set of correlated Monte Carlo (MC) rollouts for variance control, which are random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. The proposed method is evaluated on both neural program synthesis and image captioning."
3325,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) method based on maximum entropy regularized Markov decision processes (MDPs). The main idea is to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, and then to interdict a set of actions that improve or reduce the wcd. The authors empirically demonstrate that their proposed approach control the goal recognition process based on opponent’s deceptive behavior."
3326,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) method based on maximum entropy regularized Markov decision processes (MDPs). The main idea is to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, and then to interdict a set of actions that improve or reduce the wcd. The authors empirically demonstrate that their proposed approach control the goal recognition process based on opponent’s deceptive behavior."
3327,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) method based on maximum entropy regularized Markov decision processes (MDPs). The main idea is to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, and then to interdict a set of actions that improve or reduce the wcd. The authors empirically demonstrate that their proposed approach control the goal recognition process based on opponent’s deceptive behavior."
3328,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batch sizes for GANs. The method is inspired by the use of Coreset-selection in active learning. To create effectively large batches of ‘real’ images, the authors create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset selection on those projected activations at training time. The experimental results show that the proposed method substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it can reach a new state of the art in anomaly detection."
3329,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batch sizes for GANs. The method is inspired by the use of Coreset-selection in active learning. To create effectively large batches of ‘real’ images, the authors create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset selection on those projected activations at training time. The experimental results show that the proposed method substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it can reach a new state of the art in anomaly detection."
3330,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batch sizes for GANs. The method is inspired by the use of Coreset-selection in active learning. To create effectively large batches of ‘real’ images, the authors create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset selection on those projected activations at training time. The experimental results show that the proposed method substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it can reach a new state of the art in anomaly detection."
3331,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper investigates the role of information in RNNs trained with maximum likelihood and contrastive loss. The authors show that RNN are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. They show that constraining past information by injecting noise into the hidden state can improve the ability of RNN to extract predictive information for both maximum likelihood/ contrastive losses."
3332,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper investigates the role of information in RNNs trained with maximum likelihood and contrastive loss. The authors show that RNN are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. They show that constraining past information by injecting noise into the hidden state can improve the ability of RNN to extract predictive information for both maximum likelihood/ contrastive losses."
3333,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper investigates the role of information in RNNs trained with maximum likelihood and contrastive loss. The authors show that RNN are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. They show that constraining past information by injecting noise into the hidden state can improve the ability of RNN to extract predictive information for both maximum likelihood/ contrastive losses."
3334,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate delusional bias in approximate Q-learning by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class, and propose a search framework that allows multiple Q-Approximator to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-Learning in a variety of Atari games, sometimes dramatically."
3335,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate delusional bias in approximate Q-learning by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class, and propose a search framework that allows multiple Q-Approximator to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-Learning in a variety of Atari games, sometimes dramatically."
3336,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate delusional bias in approximate Q-learning by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class, and propose a search framework that allows multiple Q-Approximator to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-Learning in a variety of Atari games, sometimes dramatically."
3337,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. The paper also resolves the scalability problems of previous methods by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations."
3338,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. The paper also resolves the scalability problems of previous methods by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations."
3339,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. The paper also resolves the scalability problems of previous methods by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations."
3340,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a new method for compressing convolutional neural networks (CNNs) based on depthwise separable convolution. The proposed method is based on EHP, which is a mathematical formulation to approximate the standard convolution kernel. The authors also propose a generalized version of the proposed method, called rank-k FALCON, which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. In addition, the authors propose a branch-based version of FALCon, which can be fit into the existing ShuffleUnitV2 convolution unit. The experimental results show that the proposed methods outperform existing methods based on the depthwise convolution and standard CNN models by up to 8x compression and 8x computation reduction while ensuring similar accuracy."
3341,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a new method for compressing convolutional neural networks (CNNs) based on depthwise separable convolution. The proposed method is based on EHP, which is a mathematical formulation to approximate the standard convolution kernel. The authors also propose a generalized version of the proposed method, called rank-k FALCON, which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. In addition, the authors propose a branch-based version of FALCon, which can be fit into the existing ShuffleUnitV2 convolution unit. The experimental results show that the proposed methods outperform existing methods based on the depthwise convolution and standard CNN models by up to 8x compression and 8x computation reduction while ensuring similar accuracy."
3342,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a new method for compressing convolutional neural networks (CNNs) based on depthwise separable convolution. The proposed method is based on EHP, which is a mathematical formulation to approximate the standard convolution kernel. The authors also propose a generalized version of the proposed method, called rank-k FALCON, which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. In addition, the authors propose a branch-based version of FALCon, which can be fit into the existing ShuffleUnitV2 convolution unit. The experimental results show that the proposed methods outperform existing methods based on the depthwise convolution and standard CNN models by up to 8x compression and 8x computation reduction while ensuring similar accuracy."
3343,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper proposes four improvements to the generic Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. The contributions include a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost BatchNormalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters γ and β; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. "
3344,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper proposes four improvements to the generic Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. The contributions include a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost BatchNormalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters γ and β; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. "
3345,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper proposes four improvements to the generic Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. The contributions include a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost BatchNormalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters γ and β; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. "
3346,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,This paper proposes a generative model that can be used to debug commonly occurring data issues even when the data cannot be directly inspected. The generative models are trained using federated methods and with formal differential privacy guarantees. The authors explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm.
3347,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,This paper proposes a generative model that can be used to debug commonly occurring data issues even when the data cannot be directly inspected. The generative models are trained using federated methods and with formal differential privacy guarantees. The authors explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm.
3348,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,This paper proposes a generative model that can be used to debug commonly occurring data issues even when the data cannot be directly inspected. The generative models are trained using federated methods and with formal differential privacy guarantees. The authors explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm.
3349,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the provided motion material and modifies it to become natural."
3350,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the provided motion material and modifies it to become natural."
3351,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the provided motion material and modifies it to become natural."
3352,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper proposes two methods for hierarchical explanation of neural network predictions. The first method, Sampling and Contextual Decomposition (SCD), aims to quantify the importance of each word and phrase. The second method, SOC, aims to extract classification rules and improve human trust of models. Experiments on both LSTM models and BERT Transformer models show that the proposed methods outperform prior hierarchical explanation algorithms."
3353,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper proposes two methods for hierarchical explanation of neural network predictions. The first method, Sampling and Contextual Decomposition (SCD), aims to quantify the importance of each word and phrase. The second method, SOC, aims to extract classification rules and improve human trust of models. Experiments on both LSTM models and BERT Transformer models show that the proposed methods outperform prior hierarchical explanation algorithms."
3354,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper proposes two methods for hierarchical explanation of neural network predictions. The first method, Sampling and Contextual Decomposition (SCD), aims to quantify the importance of each word and phrase. The second method, SOC, aims to extract classification rules and improve human trust of models. Experiments on both LSTM models and BERT Transformer models show that the proposed methods outperform prior hierarchical explanation algorithms."
3355,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a method for improving the efficiency of deep convolutional neural networks. The proposed method is based on the idea of order equivalence, where the order of the layers in the network is used to compute the saliency map. The authors show that their method is at least 97x faster than Guided Backprop and much more accurate. They also show how to visualize individual scale/layer contributions by using Layer Ordered Visualization of Information."
3356,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a method for improving the efficiency of deep convolutional neural networks. The proposed method is based on the idea of order equivalence, where the order of the layers in the network is used to compute the saliency map. The authors show that their method is at least 97x faster than Guided Backprop and much more accurate. They also show how to visualize individual scale/layer contributions by using Layer Ordered Visualization of Information."
3357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a method for improving the efficiency of deep convolutional neural networks. The proposed method is based on the idea of order equivalence, where the order of the layers in the network is used to compute the saliency map. The authors show that their method is at least 97x faster than Guided Backprop and much more accurate. They also show how to visualize individual scale/layer contributions by using Layer Ordered Visualization of Information."
3358,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation based on position modeling. The proposed method is based on the idea that the position of a word should be modeled as a latent variable in the text generation process. The method is evaluated on machine translation and paraphrase generation tasks, where it outperforms several strong baselines."
3359,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation based on position modeling. The proposed method is based on the idea that the position of a word should be modeled as a latent variable in the text generation process. The method is evaluated on machine translation and paraphrase generation tasks, where it outperforms several strong baselines."
3360,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation based on position modeling. The proposed method is based on the idea that the position of a word should be modeled as a latent variable in the text generation process. The method is evaluated on machine translation and paraphrase generation tasks, where it outperforms several strong baselines."
3361,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a random path generative adversarial network (RPGAN) that can be used as a tool for generative model analysis. The main idea is that the latent space of a typical GAN consists of input vectors, randomly sampled from the standard Gaussian distribution, whereas the latent of RPGAN consists random paths in a generator network. As a result, this design allows to understand factors of variation, captured by different generator layers, providing their natural interpretability. Experiments on standard benchmarks demonstrate that RPGAN reveals several interesting insights about the roles that different layers play in the image generation process."
3362,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a random path generative adversarial network (RPGAN) that can be used as a tool for generative model analysis. The main idea is that the latent space of a typical GAN consists of input vectors, randomly sampled from the standard Gaussian distribution, whereas the latent of RPGAN consists random paths in a generator network. As a result, this design allows to understand factors of variation, captured by different generator layers, providing their natural interpretability. Experiments on standard benchmarks demonstrate that RPGAN reveals several interesting insights about the roles that different layers play in the image generation process."
3363,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a random path generative adversarial network (RPGAN) that can be used as a tool for generative model analysis. The main idea is that the latent space of a typical GAN consists of input vectors, randomly sampled from the standard Gaussian distribution, whereas the latent of RPGAN consists random paths in a generator network. As a result, this design allows to understand factors of variation, captured by different generator layers, providing their natural interpretability. Experiments on standard benchmarks demonstrate that RPGAN reveals several interesting insights about the roles that different layers play in the image generation process."
3364,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolutional neural network architecture based on a graph representation of the sampled sphere. The authors study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. They evaluate DeepSphere on relevant problems and demonstrate the efficiency and flexibility of this formulation."
3365,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolutional neural network architecture based on a graph representation of the sampled sphere. The authors study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. They evaluate DeepSphere on relevant problems and demonstrate the efficiency and flexibility of this formulation."
3366,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolutional neural network architecture based on a graph representation of the sampled sphere. The authors study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. They evaluate DeepSphere on relevant problems and demonstrate the efficiency and flexibility of this formulation."
3367,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of learning invariant representations for domain adaptation. The authors propose a new bound on the target risk that reveals a trade-off between compression and invariance of learned representations. In particular, they show that the adaptability of a representation can be better controlled when the compression risk is taken into account. They also show that learning weighted representations plays a key role in relaxing the constraint of invariance and then preserving the risk of compression. They support these statements with a theoretical analysis illustrated on a standard domain adaptation benchmark."
3368,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of learning invariant representations for domain adaptation. The authors propose a new bound on the target risk that reveals a trade-off between compression and invariance of learned representations. In particular, they show that the adaptability of a representation can be better controlled when the compression risk is taken into account. They also show that learning weighted representations plays a key role in relaxing the constraint of invariance and then preserving the risk of compression. They support these statements with a theoretical analysis illustrated on a standard domain adaptation benchmark."
3369,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of learning invariant representations for domain adaptation. The authors propose a new bound on the target risk that reveals a trade-off between compression and invariance of learned representations. In particular, they show that the adaptability of a representation can be better controlled when the compression risk is taken into account. They also show that learning weighted representations plays a key role in relaxing the constraint of invariance and then preserving the risk of compression. They support these statements with a theoretical analysis illustrated on a standard domain adaptation benchmark."
3370,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring user interface attribute values for a given input image. The method is based on a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), which are used to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, the authors also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. The authors instantiate their approach to inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications."
3371,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring user interface attribute values for a given input image. The method is based on a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), which are used to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, the authors also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. The authors instantiate their approach to inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications."
3372,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring user interface attribute values for a given input image. The method is based on a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), which are used to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, the authors also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. The authors instantiate their approach to inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications."
3373,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, which aims to transfer not only performance but also robustness from a source model to a target domain. The authors argue that robust networks contain robust feature extractors. By training classifiers on top of these feature extractor, they produce new models that inherit the robustness of their parent networks. They then consider the case of “fine tuning” a network by re-training end-to-end in the target domain, and use lifelong learning strategies. They show that using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training."
3374,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, which aims to transfer not only performance but also robustness from a source model to a target domain. The authors argue that robust networks contain robust feature extractors. By training classifiers on top of these feature extractor, they produce new models that inherit the robustness of their parent networks. They then consider the case of “fine tuning” a network by re-training end-to-end in the target domain, and use lifelong learning strategies. They show that using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training."
3375,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, which aims to transfer not only performance but also robustness from a source model to a target domain. The authors argue that robust networks contain robust feature extractors. By training classifiers on top of these feature extractor, they produce new models that inherit the robustness of their parent networks. They then consider the case of “fine tuning” a network by re-training end-to-end in the target domain, and use lifelong learning strategies. They show that using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training."
3376,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm that encourages the emergence of a more structured type of language. The authors provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. The experiments confirm the analysis and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.
3377,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm that encourages the emergence of a more structured type of language. The authors provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. The experiments confirm the analysis and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.
3378,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm that encourages the emergence of a more structured type of language. The authors provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. The experiments confirm the analysis and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.
3379,SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The proposed method provides a tighter estimate of the log partition function and achieves much better empirical results. "
3380,SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The proposed method provides a tighter estimate of the log partition function and achieves much better empirical results. "
3381,SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The proposed method provides a tighter estimate of the log partition function and achieves much better empirical results. "
3382,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple indicator reward function for goal-conditioned reinforcement learning: only give a positive reward when the robot’s observation exactly matches a target goal observation. The authors show that by relabeling the original goal with the achieved goal to obtain positive rewards (Andrychowicz et al., 2017), the indicator reward can learn with the reward function even in continuous state spaces. They propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. They show comparable performance between their method and an oracle which uses the ground-truth state for computing rewards."
3383,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple indicator reward function for goal-conditioned reinforcement learning: only give a positive reward when the robot’s observation exactly matches a target goal observation. The authors show that by relabeling the original goal with the achieved goal to obtain positive rewards (Andrychowicz et al., 2017), the indicator reward can learn with the reward function even in continuous state spaces. They propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. They show comparable performance between their method and an oracle which uses the ground-truth state for computing rewards."
3384,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple indicator reward function for goal-conditioned reinforcement learning: only give a positive reward when the robot’s observation exactly matches a target goal observation. The authors show that by relabeling the original goal with the achieved goal to obtain positive rewards (Andrychowicz et al., 2017), the indicator reward can learn with the reward function even in continuous state spaces. They propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. They show comparable performance between their method and an oracle which uses the ground-truth state for computing rewards."
3385,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper studies the problem of certified robustness verification of neural networks with complex self-attention layers. In particular, the authors consider the case of Transformers. The authors propose a method for certifying the robustness of a neural network with complex attention layers. The main contribution of the paper is to propose a new method for verifying the certified accuracy of a transformer model. The proposed method is based on the idea of using a linear combination of the Transformer’s self attention layer and the attention layer of the transformer model, and the authors show that their method is more robust than the naive Interval Bound Propagation (IBP) method. "
3386,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper studies the problem of certified robustness verification of neural networks with complex self-attention layers. In particular, the authors consider the case of Transformers. The authors propose a method for certifying the robustness of a neural network with complex attention layers. The main contribution of the paper is to propose a new method for verifying the certified accuracy of a transformer model. The proposed method is based on the idea of using a linear combination of the Transformer’s self attention layer and the attention layer of the transformer model, and the authors show that their method is more robust than the naive Interval Bound Propagation (IBP) method. "
3387,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper studies the problem of certified robustness verification of neural networks with complex self-attention layers. In particular, the authors consider the case of Transformers. The authors propose a method for certifying the robustness of a neural network with complex attention layers. The main contribution of the paper is to propose a new method for verifying the certified accuracy of a transformer model. The proposed method is based on the idea of using a linear combination of the Transformer’s self attention layer and the attention layer of the transformer model, and the authors show that their method is more robust than the naive Interval Bound Propagation (IBP) method. "
3388,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a new pretraining objective for BERT-based entity-related question answering models. The proposed objective is based on the zero-shot fact completion task, which is a weakly supervised pretraining task that explicitly forces the model to incorporate knowledge about real-world entities. Experiments show that the proposed objective can outperform BERT on four entity-based question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvement."
3389,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a new pretraining objective for BERT-based entity-related question answering models. The proposed objective is based on the zero-shot fact completion task, which is a weakly supervised pretraining task that explicitly forces the model to incorporate knowledge about real-world entities. Experiments show that the proposed objective can outperform BERT on four entity-based question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvement."
3390,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a new pretraining objective for BERT-based entity-related question answering models. The proposed objective is based on the zero-shot fact completion task, which is a weakly supervised pretraining task that explicitly forces the model to incorporate knowledge about real-world entities. Experiments show that the proposed objective can outperform BERT on four entity-based question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvement."
3391,SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper proposes a novel method for novel category discovery. The authors argue that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data. Instead, the authors propose to use rank statistics to transfer the model’s knowledge of the labelled classes to the problem of clustering the unlabeled images, and train the data representation by optimizing a joint objective function on the labelled-and-unlabelled subsets of the data. The proposed method is evaluated on standard classification benchmarks and outperforms existing methods for novel class discovery by a significant margin."
3392,SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper proposes a novel method for novel category discovery. The authors argue that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data. Instead, the authors propose to use rank statistics to transfer the model’s knowledge of the labelled classes to the problem of clustering the unlabeled images, and train the data representation by optimizing a joint objective function on the labelled-and-unlabelled subsets of the data. The proposed method is evaluated on standard classification benchmarks and outperforms existing methods for novel class discovery by a significant margin."
3393,SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper proposes a novel method for novel category discovery. The authors argue that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data. Instead, the authors propose to use rank statistics to transfer the model’s knowledge of the labelled classes to the problem of clustering the unlabeled images, and train the data representation by optimizing a joint objective function on the labelled-and-unlabelled subsets of the data. The proposed method is evaluated on standard classification benchmarks and outperforms existing methods for novel class discovery by a significant margin."
3394,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a novel method for visual planning based on hallucinative topological memory (HTM). The main idea is to use a contrastive predictive coding (CPC) model to generate hallucinated images of the target domain, which are then used for planning. The proposed method is evaluated on a set of simulated tasks and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of long-term planning."
3395,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a novel method for visual planning based on hallucinative topological memory (HTM). The main idea is to use a contrastive predictive coding (CPC) model to generate hallucinated images of the target domain, which are then used for planning. The proposed method is evaluated on a set of simulated tasks and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of long-term planning."
3396,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a novel method for visual planning based on hallucinative topological memory (HTM). The main idea is to use a contrastive predictive coding (CPC) model to generate hallucinated images of the target domain, which are then used for planning. The proposed method is evaluated on a set of simulated tasks and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of long-term planning."
3397,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes an adversarial filtering method to reduce the bias of existing datasets. The proposed method is based on the idea that the datasets are overly populated with similar (thus non-tail) problems, which leads to a major overestimation of true AI performance. To address this challenge, the authors propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. The experiments show that AFLITE effectively reduces measurable dataset biases in both the synthetic and real datasets."
3398,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes an adversarial filtering method to reduce the bias of existing datasets. The proposed method is based on the idea that the datasets are overly populated with similar (thus non-tail) problems, which leads to a major overestimation of true AI performance. To address this challenge, the authors propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. The experiments show that AFLITE effectively reduces measurable dataset biases in both the synthetic and real datasets."
3399,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes an adversarial filtering method to reduce the bias of existing datasets. The proposed method is based on the idea that the datasets are overly populated with similar (thus non-tail) problems, which leads to a major overestimation of true AI performance. To address this challenge, the authors propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. The experiments show that AFLITE effectively reduces measurable dataset biases in both the synthetic and real datasets."
3400,SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a novel method for few-shot classification. The main idea is to add a null class centered around zero and enforce centering with batch normalization. The authors also propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples’ distribution rather than just their centroid into account. Experiments on Omniglot and MiniImageNet show that the proposed method outperforms matching networks and prototypical networks."
3401,SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a novel method for few-shot classification. The main idea is to add a null class centered around zero and enforce centering with batch normalization. The authors also propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples’ distribution rather than just their centroid into account. Experiments on Omniglot and MiniImageNet show that the proposed method outperforms matching networks and prototypical networks."
3402,SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a novel method for few-shot classification. The main idea is to add a null class centered around zero and enforce centering with batch normalization. The authors also propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples’ distribution rather than just their centroid into account. Experiments on Omniglot and MiniImageNet show that the proposed method outperforms matching networks and prototypical networks."
3403,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. The proposed method first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. Then, any existing embedding methods can be applied to the coarsen graph, before it progressively refine the embeddings obtained at the first level to increasingly finer graphs."
3404,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. The proposed method first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. Then, any existing embedding methods can be applied to the coarsen graph, before it progressively refine the embeddings obtained at the first level to increasingly finer graphs."
3405,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. The proposed method first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. Then, any existing embedding methods can be applied to the coarsen graph, before it progressively refine the embeddings obtained at the first level to increasingly finer graphs."
3406,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relative to previously generated pixels (or pixels from the conditioning context, when available) in an autoregressive image generation model. The proposed method is based on the observation that the sharpness and smoothness of transitions between adjacent pixels tend to be smooth and gradual, which is a fact that has been exploited in image compression models based on predictive coding. This paper proposes to predict the relative pixel intensities at each position, rather than the absolute pixel intensity. The authors show that the proposed method can be combined with a unified probabilistic model to achieve optimal performance. Experiments are conducted on unconditional image generation, image colorization, and super-resolution, and show that their proposed method leads to improvements in likelihood compared to the absolute prediction counterparts."
3407,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relative to previously generated pixels (or pixels from the conditioning context, when available) in an autoregressive image generation model. The proposed method is based on the observation that the sharpness and smoothness of transitions between adjacent pixels tend to be smooth and gradual, which is a fact that has been exploited in image compression models based on predictive coding. This paper proposes to predict the relative pixel intensities at each position, rather than the absolute pixel intensity. The authors show that the proposed method can be combined with a unified probabilistic model to achieve optimal performance. Experiments are conducted on unconditional image generation, image colorization, and super-resolution, and show that their proposed method leads to improvements in likelihood compared to the absolute prediction counterparts."
3408,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relative to previously generated pixels (or pixels from the conditioning context, when available) in an autoregressive image generation model. The proposed method is based on the observation that the sharpness and smoothness of transitions between adjacent pixels tend to be smooth and gradual, which is a fact that has been exploited in image compression models based on predictive coding. This paper proposes to predict the relative pixel intensities at each position, rather than the absolute pixel intensity. The authors show that the proposed method can be combined with a unified probabilistic model to achieve optimal performance. Experiments are conducted on unconditional image generation, image colorization, and super-resolution, and show that their proposed method leads to improvements in likelihood compared to the absolute prediction counterparts."
3409,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper proposes a method for estimating the transition operator of a Markov chain. The method is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The authors prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems including off-line PageRank and off-policy policy evaluation."
3410,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper proposes a method for estimating the transition operator of a Markov chain. The method is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The authors prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems including off-line PageRank and off-policy policy evaluation."
3411,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper proposes a method for estimating the transition operator of a Markov chain. The method is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The authors prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems including off-line PageRank and off-policy policy evaluation."
3412,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper studies the problem of training models that are less sensitive to spurious patterns in natural language processing (NLP). The authors propose a method for training models on counterfactually-revised data, where the authors use counterfactual data to train a model that is more sensitive to the spurious features. The authors show that the model trained on the original data is less sensitive than models trained on combined data. The paper also shows that the classifiers trained on both original and manipulated data are sensitive to this signal."
3413,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper studies the problem of training models that are less sensitive to spurious patterns in natural language processing (NLP). The authors propose a method for training models on counterfactually-revised data, where the authors use counterfactual data to train a model that is more sensitive to the spurious features. The authors show that the model trained on the original data is less sensitive than models trained on combined data. The paper also shows that the classifiers trained on both original and manipulated data are sensitive to this signal."
3414,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper studies the problem of training models that are less sensitive to spurious patterns in natural language processing (NLP). The authors propose a method for training models on counterfactually-revised data, where the authors use counterfactual data to train a model that is more sensitive to the spurious features. The authors show that the model trained on the original data is less sensitive than models trained on combined data. The paper also shows that the classifiers trained on both original and manipulated data are sensitive to this signal."
3415,SP:b720eb5b6e44473a9392cc572af89270019d4c42,This paper proposes a theoretical analysis of the behavior of CNN channels as spatial frequency and orientation selective filters in deep CNNs. The authors show that CNN channels are sensitive to spatial frequencies that have lower contrast masking thresholds in human visual perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features. 
3416,SP:b720eb5b6e44473a9392cc572af89270019d4c42,This paper proposes a theoretical analysis of the behavior of CNN channels as spatial frequency and orientation selective filters in deep CNNs. The authors show that CNN channels are sensitive to spatial frequencies that have lower contrast masking thresholds in human visual perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features. 
3417,SP:b720eb5b6e44473a9392cc572af89270019d4c42,This paper proposes a theoretical analysis of the behavior of CNN channels as spatial frequency and orientation selective filters in deep CNNs. The authors show that CNN channels are sensitive to spatial frequencies that have lower contrast masking thresholds in human visual perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features. 
3418,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to model link type correlations in drug-drug interactions (DDIs). The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrate that GENN is superior to many baselines without consideration of link type correlation."
3419,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to model link type correlations in drug-drug interactions (DDIs). The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrate that GENN is superior to many baselines without consideration of link type correlation."
3420,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to model link type correlations in drug-drug interactions (DDIs). The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrate that GENN is superior to many baselines without consideration of link type correlation."
3421,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a self-supervised context prediction method based on a wav2vec-style representation learning algorithm. The proposed method uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Experiments on TIMIT phoneme classification and WSJ speech recognition show that BERT pre-training achieves a new state-of-the-art performance.
3422,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a self-supervised context prediction method based on a wav2vec-style representation learning algorithm. The proposed method uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Experiments on TIMIT phoneme classification and WSJ speech recognition show that BERT pre-training achieves a new state-of-the-art performance.
3423,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a self-supervised context prediction method based on a wav2vec-style representation learning algorithm. The proposed method uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Experiments on TIMIT phoneme classification and WSJ speech recognition show that BERT pre-training achieves a new state-of-the-art performance.
3424,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering. The main idea is to train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, the critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for the new lists."
3425,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering. The main idea is to train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, the critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for the new lists."
3426,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering. The main idea is to train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, the critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for the new lists."
3427,SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a new off-policy reinforcement learning algorithm, called Composite Q-learning (CQ3), which combines truncated and shifted Q-functions. The authors prove that the combination of these short-and long-term predictions is a representation of the full return, leading to the Composite TD3 algorithm. The proposed algorithm is evaluated on three simulated robot tasks and compared with TD3, Model-based Value Expansion, and TD3(∆)."
3428,SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a new off-policy reinforcement learning algorithm, called Composite Q-learning (CQ3), which combines truncated and shifted Q-functions. The authors prove that the combination of these short-and long-term predictions is a representation of the full return, leading to the Composite TD3 algorithm. The proposed algorithm is evaluated on three simulated robot tasks and compared with TD3, Model-based Value Expansion, and TD3(∆)."
3429,SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a new off-policy reinforcement learning algorithm, called Composite Q-learning (CQ3), which combines truncated and shifted Q-functions. The authors prove that the combination of these short-and long-term predictions is a representation of the full return, leading to the Composite TD3 algorithm. The proposed algorithm is evaluated on three simulated robot tasks and compared with TD3, Model-based Value Expansion, and TD3(∆)."
3430,SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a reinforcement learning system for dexterous manipulation tasks. The proposed system is based on a combination of two existing methods: a simple reinforcement learning algorithm and a hand-engineered reward function. The paper provides an in-depth analysis of the challenges associated with this learning paradigm and proposes simple and scalable solutions to these challenges. The experimental results show that the proposed system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand."
3431,SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a reinforcement learning system for dexterous manipulation tasks. The proposed system is based on a combination of two existing methods: a simple reinforcement learning algorithm and a hand-engineered reward function. The paper provides an in-depth analysis of the challenges associated with this learning paradigm and proposes simple and scalable solutions to these challenges. The experimental results show that the proposed system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand."
3432,SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a reinforcement learning system for dexterous manipulation tasks. The proposed system is based on a combination of two existing methods: a simple reinforcement learning algorithm and a hand-engineered reward function. The paper provides an in-depth analysis of the challenges associated with this learning paradigm and proposes simple and scalable solutions to these challenges. The experimental results show that the proposed system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand."
3433,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the problem of adversarial robustness in the presence of perturbations. The authors propose a novel risk decomposition theorem, which shows that the expected robust risk can be decomposed into two parts: the stability part which measures the prediction stability and the accuracy part which evaluates the standard classification accuracy. The stability part does not depend on any label information, and can be optimized using unlabeled data. In addition, the authors prove that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabelled data is provided."
3434,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the problem of adversarial robustness in the presence of perturbations. The authors propose a novel risk decomposition theorem, which shows that the expected robust risk can be decomposed into two parts: the stability part which measures the prediction stability and the accuracy part which evaluates the standard classification accuracy. The stability part does not depend on any label information, and can be optimized using unlabeled data. In addition, the authors prove that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabelled data is provided."
3435,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the problem of adversarial robustness in the presence of perturbations. The authors propose a novel risk decomposition theorem, which shows that the expected robust risk can be decomposed into two parts: the stability part which measures the prediction stability and the accuracy part which evaluates the standard classification accuracy. The stability part does not depend on any label information, and can be optimized using unlabeled data. In addition, the authors prove that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabelled data is provided."
3436,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a family of methods for estimating the advantage of a policy in reinforcement learning. The main idea is to estimate the advantage based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. The proposed method is evaluated on MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments."
3437,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a family of methods for estimating the advantage of a policy in reinforcement learning. The main idea is to estimate the advantage based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. The proposed method is evaluated on MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments."
3438,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a family of methods for estimating the advantage of a policy in reinforcement learning. The main idea is to estimate the advantage based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. The proposed method is evaluated on MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments."
3439,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in higher precision to preserve accuracy. The proposed approach is applicable to various DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs including static compressed mobile-friendly networks such as ShuffleNet. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7x computational cost reduction on LSTM on the Penn Tree Bank dataset."
3440,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in higher precision to preserve accuracy. The proposed approach is applicable to various DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs including static compressed mobile-friendly networks such as ShuffleNet. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7x computational cost reduction on LSTM on the Penn Tree Bank dataset."
3441,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in higher precision to preserve accuracy. The proposed approach is applicable to various DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs including static compressed mobile-friendly networks such as ShuffleNet. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7x computational cost reduction on LSTM on the Penn Tree Bank dataset."
3442,SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a new unsupervised domain adaptation (UDA) problem called Wide Unsupervised Domain Adaptation (WUDA). WUDA is a more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD. The authors show that WuDA ruins all UDA methods if taking no care of label noise in SD, and to this end, they propose a Butterfly framework, which maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to unlabeled, and SD-to -TD-distributional) and then the other two can focus on classification in TD."
3443,SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a new unsupervised domain adaptation (UDA) problem called Wide Unsupervised Domain Adaptation (WUDA). WUDA is a more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD. The authors show that WuDA ruins all UDA methods if taking no care of label noise in SD, and to this end, they propose a Butterfly framework, which maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to unlabeled, and SD-to -TD-distributional) and then the other two can focus on classification in TD."
3444,SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a new unsupervised domain adaptation (UDA) problem called Wide Unsupervised Domain Adaptation (WUDA). WUDA is a more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD. The authors show that WuDA ruins all UDA methods if taking no care of label noise in SD, and to this end, they propose a Butterfly framework, which maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to unlabeled, and SD-to -TD-distributional) and then the other two can focus on classification in TD."
3445,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"This paper proposes an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. The authors dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, the authors devise an algorithm that trains an actor and critic algorithm together, and shows that the proposed algorithm outperforms the baselines."
3446,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"This paper proposes an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. The authors dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, the authors devise an algorithm that trains an actor and critic algorithm together, and shows that the proposed algorithm outperforms the baselines."
3447,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"This paper proposes an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. The authors dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, the authors devise an algorithm that trains an actor and critic algorithm together, and shows that the proposed algorithm outperforms the baselines."
3448,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for accelerating training and improving generalization over the original dropout. The proposed method is based on duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experimental results show that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks using the ILSVRC 2012 (ImageNet), Cifar-10, CIFAR-100, and SVHN datasets."
3449,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for accelerating training and improving generalization over the original dropout. The proposed method is based on duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experimental results show that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks using the ILSVRC 2012 (ImageNet), Cifar-10, CIFAR-100, and SVHN datasets."
3450,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for accelerating training and improving generalization over the original dropout. The proposed method is based on duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experimental results show that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks using the ILSVRC 2012 (ImageNet), Cifar-10, CIFAR-100, and SVHN datasets."
3451,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a method for training interpretable CNNs. The key idea is to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization. Extensive experiments demonstrate the effectiveness of the proposed method in generating sparse and highly class-related representation of the input."
3452,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a method for training interpretable CNNs. The key idea is to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization. Extensive experiments demonstrate the effectiveness of the proposed method in generating sparse and highly class-related representation of the input."
3453,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a method for training interpretable CNNs. The key idea is to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization. Extensive experiments demonstrate the effectiveness of the proposed method in generating sparse and highly class-related representation of the input."
3454,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,This paper proposes a communication-minimization-based method for decentralized value function factorization in multi-agent reinforcement learning. The authors propose two information-theoretic regularizers to improve mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. They show how to optimize these regularizers in a way that is easily integrated with QMIX. They demonstrate that the proposed method outperforms baseline methods on the StarCraft unit micromanagement benchmark.
3455,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,This paper proposes a communication-minimization-based method for decentralized value function factorization in multi-agent reinforcement learning. The authors propose two information-theoretic regularizers to improve mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. They show how to optimize these regularizers in a way that is easily integrated with QMIX. They demonstrate that the proposed method outperforms baseline methods on the StarCraft unit micromanagement benchmark.
3456,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,This paper proposes a communication-minimization-based method for decentralized value function factorization in multi-agent reinforcement learning. The authors propose two information-theoretic regularizers to improve mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. They show how to optimize these regularizers in a way that is easily integrated with QMIX. They demonstrate that the proposed method outperforms baseline methods on the StarCraft unit micromanagement benchmark.
3457,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for generating programming puzzles for teaching computers programming. The authors propose a GAN-like algorithm called “Troublemaker” which can generate puzzles adaptively targeted at any given puzzle-solver. The algorithm generates a diverse set of puzzles that are difficult for the solver to solve. In the experiments, the authors show that Troublemaker learns to generate challenging problems for a variety of state-of-the-art problem-solving techniques."
3458,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for generating programming puzzles for teaching computers programming. The authors propose a GAN-like algorithm called “Troublemaker” which can generate puzzles adaptively targeted at any given puzzle-solver. The algorithm generates a diverse set of puzzles that are difficult for the solver to solve. In the experiments, the authors show that Troublemaker learns to generate challenging problems for a variety of state-of-the-art problem-solving techniques."
3459,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for generating programming puzzles for teaching computers programming. The authors propose a GAN-like algorithm called “Troublemaker” which can generate puzzles adaptively targeted at any given puzzle-solver. The algorithm generates a diverse set of puzzles that are difficult for the solver to solve. In the experiments, the authors show that Troublemaker learns to generate challenging problems for a variety of state-of-the-art problem-solving techniques."
3460,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a method to decompose a policy into a set of primitive policies and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. The primitive policies are trained to use as little information as possible about the current state, and the meta-meta-policy is trained to produce appropriate decisions in all the states. The paper proposes an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs to make a decision and the primitive that requests the most information is the one that acts in the environment. Experiments show that this policy architecture improves over both flat and hierarchical policies in terms of generalization."
3461,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a method to decompose a policy into a set of primitive policies and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. The primitive policies are trained to use as little information as possible about the current state, and the meta-meta-policy is trained to produce appropriate decisions in all the states. The paper proposes an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs to make a decision and the primitive that requests the most information is the one that acts in the environment. Experiments show that this policy architecture improves over both flat and hierarchical policies in terms of generalization."
3462,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a method to decompose a policy into a set of primitive policies and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. The primitive policies are trained to use as little information as possible about the current state, and the meta-meta-policy is trained to produce appropriate decisions in all the states. The paper proposes an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs to make a decision and the primitive that requests the most information is the one that acts in the environment. Experiments show that this policy architecture improves over both flat and hierarchical policies in terms of generalization."
3463,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-based planning framework that learns a latent reward prediction model and then plans in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which is the only necessary information for successful planning. The authors demonstrate their framework in multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of them produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations."
3464,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-based planning framework that learns a latent reward prediction model and then plans in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which is the only necessary information for successful planning. The authors demonstrate their framework in multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of them produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations."
3465,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-based planning framework that learns a latent reward prediction model and then plans in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which is the only necessary information for successful planning. The authors demonstrate their framework in multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of them produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations."
3466,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). First, the authors propose to use a self-supervised loss that focuses on modeling inter-sentence coherence. Second, they propose to reduce the number of parameters in BERT-large. Experiments on GLUE, RACE, and SQuAD show that the proposed methods lead to models that scale much better compared to BERT."
3467,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). First, the authors propose to use a self-supervised loss that focuses on modeling inter-sentence coherence. Second, they propose to reduce the number of parameters in BERT-large. Experiments on GLUE, RACE, and SQuAD show that the proposed methods lead to models that scale much better compared to BERT."
3468,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). First, the authors propose to use a self-supervised loss that focuses on modeling inter-sentence coherence. Second, they propose to reduce the number of parameters in BERT-large. Experiments on GLUE, RACE, and SQuAD show that the proposed methods lead to models that scale much better compared to BERT."
3469,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,This paper proposes a new architecture for multi-modal multi-task learning called OmniNet. The main idea is to use a spatio-temporal cache mechanism to learn spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-tasks learning. The experiments show that training these four tasks together results in about three times compressed model while retaining the performance in comparison to training them individually.
3470,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,This paper proposes a new architecture for multi-modal multi-task learning called OmniNet. The main idea is to use a spatio-temporal cache mechanism to learn spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-tasks learning. The experiments show that training these four tasks together results in about three times compressed model while retaining the performance in comparison to training them individually.
3471,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,This paper proposes a new architecture for multi-modal multi-task learning called OmniNet. The main idea is to use a spatio-temporal cache mechanism to learn spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-tasks learning. The experiments show that training these four tasks together results in about three times compressed model while retaining the performance in comparison to training them individually.
3472,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,This paper proposes a new method for estimating predictive confidence intervals for regression models. The proposed method is based on the use of higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence interval. The authors provide theoretical guarantees on (1) and (2). Experiments demonstrate that the proposed method outperforms existing Bayesian and non-Bayesian baselines.
3473,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,This paper proposes a new method for estimating predictive confidence intervals for regression models. The proposed method is based on the use of higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence interval. The authors provide theoretical guarantees on (1) and (2). Experiments demonstrate that the proposed method outperforms existing Bayesian and non-Bayesian baselines.
3474,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,This paper proposes a new method for estimating predictive confidence intervals for regression models. The proposed method is based on the use of higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence interval. The authors provide theoretical guarantees on (1) and (2). Experiments demonstrate that the proposed method outperforms existing Bayesian and non-Bayesian baselines.
3475,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a new GAN architecture for video synthesis and video prediction. The proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. The experimental results show that the proposed model achieves state-of-the-art results on Kinetics-600 and UCF-101 datasets."
3476,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a new GAN architecture for video synthesis and video prediction. The proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. The experimental results show that the proposed model achieves state-of-the-art results on Kinetics-600 and UCF-101 datasets."
3477,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a new GAN architecture for video synthesis and video prediction. The proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. The experimental results show that the proposed model achieves state-of-the-art results on Kinetics-600 and UCF-101 datasets."
3478,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. The authors propose two stages of adaptation: (1) on the few base class dataset if available and (2) on even fewer data of new tasks. The first stage is to learn a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base classes data are few, the network cannot learn where to focus implicitly."
3479,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. The authors propose two stages of adaptation: (1) on the few base class dataset if available and (2) on even fewer data of new tasks. The first stage is to learn a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base classes data are few, the network cannot learn where to focus implicitly."
3480,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. The authors propose two stages of adaptation: (1) on the few base class dataset if available and (2) on even fewer data of new tasks. The first stage is to learn a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base classes data are few, the network cannot learn where to focus implicitly."
3481,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a method for generating structures that satisfy constraints on the properties of molecules or game maps. Specifically, the authors propose a method that penalizes the generator whenever it outputs invalid structures. The method is based on knowledge compilation techniques and leverages the expected disagreement between the model and the constraints. The proposed method is evaluated on constrained images, molecules, and video game levels and compared to unconstrained GANs."
3482,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a method for generating structures that satisfy constraints on the properties of molecules or game maps. Specifically, the authors propose a method that penalizes the generator whenever it outputs invalid structures. The method is based on knowledge compilation techniques and leverages the expected disagreement between the model and the constraints. The proposed method is evaluated on constrained images, molecules, and video game levels and compared to unconstrained GANs."
3483,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a method for generating structures that satisfy constraints on the properties of molecules or game maps. Specifically, the authors propose a method that penalizes the generator whenever it outputs invalid structures. The method is based on knowledge compilation techniques and leverages the expected disagreement between the model and the constraints. The proposed method is evaluated on constrained images, molecules, and video game levels and compared to unconstrained GANs."
3484,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power."
3485,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power."
3486,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power."
3487,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes a method to improve the trustworthiness of black-box models by enriching the output of a classification black box with a measure of uncertainty. The method is based on a probabilistic neural network that works in parallel to the black box and uses a Dirichlet layer as the fusion layer with the classifier. The authors demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications. Based on the resulting uncertainty measure, they advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system."
3488,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes a method to improve the trustworthiness of black-box models by enriching the output of a classification black box with a measure of uncertainty. The method is based on a probabilistic neural network that works in parallel to the black box and uses a Dirichlet layer as the fusion layer with the classifier. The authors demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications. Based on the resulting uncertainty measure, they advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system."
3489,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes a method to improve the trustworthiness of black-box models by enriching the output of a classification black box with a measure of uncertainty. The method is based on a probabilistic neural network that works in parallel to the black box and uses a Dirichlet layer as the fusion layer with the classifier. The authors demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications. Based on the resulting uncertainty measure, they advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system."
3490,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes a blockwise adaptive gradient descent method for training deep neural networks. The main idea is to split the network parameters into blocks, and use a block-wise adaptive stepsize instead of coordinate-wise adaptivity. The authors show theoretically that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective. The paper also shows its uniform stability and can lead to lower generalization error. The experimental results show that blockwise adaptation is faster and improves generalization performance over Nesterov's accelerated gradient and Adam."
3491,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes a blockwise adaptive gradient descent method for training deep neural networks. The main idea is to split the network parameters into blocks, and use a block-wise adaptive stepsize instead of coordinate-wise adaptivity. The authors show theoretically that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective. The paper also shows its uniform stability and can lead to lower generalization error. The experimental results show that blockwise adaptation is faster and improves generalization performance over Nesterov's accelerated gradient and Adam."
3492,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes a blockwise adaptive gradient descent method for training deep neural networks. The main idea is to split the network parameters into blocks, and use a block-wise adaptive stepsize instead of coordinate-wise adaptivity. The authors show theoretically that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective. The paper also shows its uniform stability and can lead to lower generalization error. The experimental results show that blockwise adaptation is faster and improves generalization performance over Nesterov's accelerated gradient and Adam."
3493,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new dataset for video understanding, named CATER. CATER is built on top of a standard 3D object dataset, and is designed to be controllable. The authors argue that existing video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. To address this issue, the authors propose to build a video dataset with fully observable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze the state-of-the-art deep video architectures."
3494,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new dataset for video understanding, named CATER. CATER is built on top of a standard 3D object dataset, and is designed to be controllable. The authors argue that existing video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. To address this issue, the authors propose to build a video dataset with fully observable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze the state-of-the-art deep video architectures."
3495,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new dataset for video understanding, named CATER. CATER is built on top of a standard 3D object dataset, and is designed to be controllable. The authors argue that existing video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. To address this issue, the authors propose to build a video dataset with fully observable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze the state-of-the-art deep video architectures."
3496,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), which leverage the boundary information from a set of pre-trained classifiers using the original data. The proposed BC-loss is provably unbiased and can be easily coupled with different GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GAN, but also achieves superior model compatibility than the original GGANs."
3497,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), which leverage the boundary information from a set of pre-trained classifiers using the original data. The proposed BC-loss is provably unbiased and can be easily coupled with different GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GAN, but also achieves superior model compatibility than the original GGANs."
3498,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), which leverage the boundary information from a set of pre-trained classifiers using the original data. The proposed BC-loss is provably unbiased and can be easily coupled with different GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GAN, but also achieves superior model compatibility than the original GGANs."
3499,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the proposed method learns an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate the effectiveness of the proposed L2L method."
3500,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the proposed method learns an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate the effectiveness of the proposed L2L method."
3501,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the proposed method learns an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate the effectiveness of the proposed L2L method."
3502,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper proposes a method for inverse reinforcement learning (IRL) on Markov Decision Processes (MDPs), where the goal is to estimate state, action, and feature constraints in the environment that motivate an agent’s behavior. The method is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent's demonstrations given our knowledge of an MDP. The authors propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
3503,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper proposes a method for inverse reinforcement learning (IRL) on Markov Decision Processes (MDPs), where the goal is to estimate state, action, and feature constraints in the environment that motivate an agent’s behavior. The method is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent's demonstrations given our knowledge of an MDP. The authors propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
3504,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper proposes a method for inverse reinforcement learning (IRL) on Markov Decision Processes (MDPs), where the goal is to estimate state, action, and feature constraints in the environment that motivate an agent’s behavior. The method is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent's demonstrations given our knowledge of an MDP. The authors propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
3505,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). The authors show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces the accuracy by driving it to prefer lowlevel edges over high-level object boundary contours."
3506,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). The authors show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces the accuracy by driving it to prefer lowlevel edges over high-level object boundary contours."
3507,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). The authors show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces the accuracy by driving it to prefer lowlevel edges over high-level object boundary contours."
3508,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"This paper proposes a context-aware CNN (or conCNN for short) that enforces the semantics context constraints in the CNN-based object detector by leveraging the popular conditional random field (CRF) model in CNN. In particular, conCNN features a context aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations. The experiments on COCO datasets showcase that conCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads."
3509,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"This paper proposes a context-aware CNN (or conCNN for short) that enforces the semantics context constraints in the CNN-based object detector by leveraging the popular conditional random field (CRF) model in CNN. In particular, conCNN features a context aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations. The experiments on COCO datasets showcase that conCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads."
3510,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"This paper proposes a context-aware CNN (or conCNN for short) that enforces the semantics context constraints in the CNN-based object detector by leveraging the popular conditional random field (CRF) model in CNN. In particular, conCNN features a context aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations. The experiments on COCO datasets showcase that conCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads."
3511,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,This paper investigates the adversarial vulnerability of batch normalization (BatchNorm). The authors hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of adversarial vulnerabilities in the BatchNorm layer. The authors empirically proved this by experiments on various neural network architectures and datasets.
3512,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,This paper investigates the adversarial vulnerability of batch normalization (BatchNorm). The authors hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of adversarial vulnerabilities in the BatchNorm layer. The authors empirically proved this by experiments on various neural network architectures and datasets.
3513,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,This paper investigates the adversarial vulnerability of batch normalization (BatchNorm). The authors hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of adversarial vulnerabilities in the BatchNorm layer. The authors empirically proved this by experiments on various neural network architectures and datasets.
3514,SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization properties of two simple regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, they prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. The generalization bound is independent of the network size, and is comparable to the bound one can get when there is no label noise. Experimental results verify the effectiveness of these methods on noisily labeled datasets."
3515,SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization properties of two simple regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, they prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. The generalization bound is independent of the network size, and is comparable to the bound one can get when there is no label noise. Experimental results verify the effectiveness of these methods on noisily labeled datasets."
3516,SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization properties of two simple regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, they prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. The generalization bound is independent of the network size, and is comparable to the bound one can get when there is no label noise. Experimental results verify the effectiveness of these methods on noisily labeled datasets."
3517,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes to improve the performance of CNN-GP and CNTK by adding a local average pooling (LAP) operation. The proposed LAP operation is inspired by Global Average Pooling (GAP) and is equivalent to full translation data augmentation. The authors also propose to use a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. Experiments on CIFAR-10 and Fashion-MNIST show that the proposed method outperforms the state-of-the-art."
3518,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes to improve the performance of CNN-GP and CNTK by adding a local average pooling (LAP) operation. The proposed LAP operation is inspired by Global Average Pooling (GAP) and is equivalent to full translation data augmentation. The authors also propose to use a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. Experiments on CIFAR-10 and Fashion-MNIST show that the proposed method outperforms the state-of-the-art."
3519,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes to improve the performance of CNN-GP and CNTK by adding a local average pooling (LAP) operation. The proposed LAP operation is inspired by Global Average Pooling (GAP) and is equivalent to full translation data augmentation. The authors also propose to use a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. Experiments on CIFAR-10 and Fashion-MNIST show that the proposed method outperforms the state-of-the-art."
3520,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). The proposed method is based on a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-actions values. The new Q-estimates are then used in combination with real experience to update the prior."
3521,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). The proposed method is based on a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-actions values. The new Q-estimates are then used in combination with real experience to update the prior."
3522,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). The proposed method is based on a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-actions values. The new Q-estimates are then used in combination with real experience to update the prior."
3523,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a novel network architecture for single-image-based view synthesis at arbitrary camera positions along the X-axis. The proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and VICLAB STEREO indoors dataset to prove the efficacy of the proposed network."
3524,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a novel network architecture for single-image-based view synthesis at arbitrary camera positions along the X-axis. The proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and VICLAB STEREO indoors dataset to prove the efficacy of the proposed network."
3525,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a novel network architecture for single-image-based view synthesis at arbitrary camera positions along the X-axis. The proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and VICLAB STEREO indoors dataset to prove the efficacy of the proposed network."
3526,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the variational auto-encoder (VAE) problem from an information theoretic perspective. The authors propose a variational lower bound of the capacity-constrained InfoMax (CCIM) objective, which is a theoretical objective to learn the maximal informative generative model while maintaining bounded the network capacity. The theoretical assumptions are confirmed by the computational experiments, where between the different families of VAE, the one optimising a Variational InfoMax AutoEncoder (VIMAE) generates sharper samples and learns a clustered and robust representation."
3527,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the variational auto-encoder (VAE) problem from an information theoretic perspective. The authors propose a variational lower bound of the capacity-constrained InfoMax (CCIM) objective, which is a theoretical objective to learn the maximal informative generative model while maintaining bounded the network capacity. The theoretical assumptions are confirmed by the computational experiments, where between the different families of VAE, the one optimising a Variational InfoMax AutoEncoder (VIMAE) generates sharper samples and learns a clustered and robust representation."
3528,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the variational auto-encoder (VAE) problem from an information theoretic perspective. The authors propose a variational lower bound of the capacity-constrained InfoMax (CCIM) objective, which is a theoretical objective to learn the maximal informative generative model while maintaining bounded the network capacity. The theoretical assumptions are confirmed by the computational experiments, where between the different families of VAE, the one optimising a Variational InfoMax AutoEncoder (VIMAE) generates sharper samples and learns a clustered and robust representation."
3529,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes a method for embedding information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. The authors prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that their algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets."
3530,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes a method for embedding information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. The authors prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that their algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets."
3531,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes a method for embedding information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. The authors prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that their algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets."
3532,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies phase transitions in the Information Bottleneck (IB) objective. The authors define phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. They provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA). Based on the theory, they propose an algorithm for discovering phase transition points."
3533,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies phase transitions in the Information Bottleneck (IB) objective. The authors define phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. They provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA). Based on the theory, they propose an algorithm for discovering phase transition points."
3534,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies phase transitions in the Information Bottleneck (IB) objective. The authors define phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. They provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA). Based on the theory, they propose an algorithm for discovering phase transition points."
3535,SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method for improving the variance of the advantage function estimation in reinforcement learning. The main idea is to identify the independence property between current action and future states in environments, which can be further leveraged to effectively reduce the variance. To further remove the risk of the high variance introduced by the new estimator, the authors combine it with existing Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the proposed method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
3536,SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method for improving the variance of the advantage function estimation in reinforcement learning. The main idea is to identify the independence property between current action and future states in environments, which can be further leveraged to effectively reduce the variance. To further remove the risk of the high variance introduced by the new estimator, the authors combine it with existing Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the proposed method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
3537,SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method for improving the variance of the advantage function estimation in reinforcement learning. The main idea is to identify the independence property between current action and future states in environments, which can be further leveraged to effectively reduce the variance. To further remove the risk of the high variance introduced by the new estimator, the authors combine it with existing Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the proposed method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
3538,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"This paper proposes a bias-reduced augmentation of Liu et al. (2018a) for infinite horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing biases due to errors in density ratio estimation. The proposed method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect."
3539,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"This paper proposes a bias-reduced augmentation of Liu et al. (2018a) for infinite horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing biases due to errors in density ratio estimation. The proposed method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect."
3540,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"This paper proposes a bias-reduced augmentation of Liu et al. (2018a) for infinite horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing biases due to errors in density ratio estimation. The proposed method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect."
3541,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a method for few-shot classification based on the Metric-Softmax loss. The main idea is to learn task-agnostic feature on base data with a novel metric-softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. The paper also proposes a task-adaptive transformation which adapts the classifier to each few shot setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the state-of-the-art by a large margin."
3542,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a method for few-shot classification based on the Metric-Softmax loss. The main idea is to learn task-agnostic feature on base data with a novel metric-softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. The paper also proposes a task-adaptive transformation which adapts the classifier to each few shot setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the state-of-the-art by a large margin."
3543,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a method for few-shot classification based on the Metric-Softmax loss. The main idea is to learn task-agnostic feature on base data with a novel metric-softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. The paper also proposes a task-adaptive transformation which adapts the classifier to each few shot setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the state-of-the-art by a large margin."
3544,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper proposes a data-driven approach to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data. Then, it uses a neural network to infers a correction to move a coarse result closer to the reference data, thus quickly obtainable result. The authors provide insights into the targeted learning problem with different learning approaches: fully supervised learning methods with a naive and an optimized data acquisition as well as an unsupervised learning method with a differentiable Navier-Stokes solver."
3545,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper proposes a data-driven approach to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data. Then, it uses a neural network to infers a correction to move a coarse result closer to the reference data, thus quickly obtainable result. The authors provide insights into the targeted learning problem with different learning approaches: fully supervised learning methods with a naive and an optimized data acquisition as well as an unsupervised learning method with a differentiable Navier-Stokes solver."
3546,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper proposes a data-driven approach to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data. Then, it uses a neural network to infers a correction to move a coarse result closer to the reference data, thus quickly obtainable result. The authors provide insights into the targeted learning problem with different learning approaches: fully supervised learning methods with a naive and an optimized data acquisition as well as an unsupervised learning method with a differentiable Navier-Stokes solver."
3547,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual compression. The method is based on stacking autoencoders, where each module is trained to reconstruct the latent space of the previous module using fewer bits, allowing the learned representation to become more compact as training progresses. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, the approach does not require pretraining, even on challenging datasets."
3548,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual compression. The method is based on stacking autoencoders, where each module is trained to reconstruct the latent space of the previous module using fewer bits, allowing the learned representation to become more compact as training progresses. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, the approach does not require pretraining, even on challenging datasets."
3549,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual compression. The method is based on stacking autoencoders, where each module is trained to reconstruct the latent space of the previous module using fewer bits, allowing the learned representation to become more compact as training progresses. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, the approach does not require pretraining, even on challenging datasets."
3550,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes a method for multi-label metric learning. The proposed method is based on a Bidirectional Representation Learning (BDRL) approach, where the label dependency is also integrated and deep convolutional networks are used to handle image data. The method scales linearly in the number of instances and trains deep neural networks that encode both input data and output labels, then, obtains a metric space for testing data."
3551,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes a method for multi-label metric learning. The proposed method is based on a Bidirectional Representation Learning (BDRL) approach, where the label dependency is also integrated and deep convolutional networks are used to handle image data. The method scales linearly in the number of instances and trains deep neural networks that encode both input data and output labels, then, obtains a metric space for testing data."
3552,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes a method for multi-label metric learning. The proposed method is based on a Bidirectional Representation Learning (BDRL) approach, where the label dependency is also integrated and deep convolutional networks are used to handle image data. The method scales linearly in the number of instances and trains deep neural networks that encode both input data and output labels, then, obtains a metric space for testing data."
3553,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the dynamics of asynchronous training in the context of dynamical stability. The authors show that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. They derive closed-form rules on how the learning rates could be changed, while keeping the accessible set the same. They also extend this analysis to include momentum. They find that momentum should be either turned off, or modified to improve training stability."
3554,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the dynamics of asynchronous training in the context of dynamical stability. The authors show that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. They derive closed-form rules on how the learning rates could be changed, while keeping the accessible set the same. They also extend this analysis to include momentum. They find that momentum should be either turned off, or modified to improve training stability."
3555,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the dynamics of asynchronous training in the context of dynamical stability. The authors show that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. They derive closed-form rules on how the learning rates could be changed, while keeping the accessible set the same. They also extend this analysis to include momentum. They find that momentum should be either turned off, or modified to improve training stability."
3556,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,This paper proposes a representation learning method for value estimation in reinforcement learning. The key idea is to learn what features of the future trajectory provide useful information to predict the associated return. The authors show how this can help dramatically even in simple policy evaluation settings. The experiments on 57 Atari 2600 games demonstrate the effectiveness of the proposed method.
3557,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,This paper proposes a representation learning method for value estimation in reinforcement learning. The key idea is to learn what features of the future trajectory provide useful information to predict the associated return. The authors show how this can help dramatically even in simple policy evaluation settings. The experiments on 57 Atari 2600 games demonstrate the effectiveness of the proposed method.
3558,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,This paper proposes a representation learning method for value estimation in reinforcement learning. The key idea is to learn what features of the future trajectory provide useful information to predict the associated return. The authors show how this can help dramatically even in simple policy evaluation settings. The experiments on 57 Atari 2600 games demonstrate the effectiveness of the proposed method.
3559,SP:6388fb91f2eaac02d9406672760a237f78735452,This paper proposes a method for adversarial attacks on graph neural networks. The authors propose to use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. The proposed method is evaluated on real-world graph classification tasks.
3560,SP:6388fb91f2eaac02d9406672760a237f78735452,This paper proposes a method for adversarial attacks on graph neural networks. The authors propose to use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. The proposed method is evaluated on real-world graph classification tasks.
3561,SP:6388fb91f2eaac02d9406672760a237f78735452,This paper proposes a method for adversarial attacks on graph neural networks. The authors propose to use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. The proposed method is evaluated on real-world graph classification tasks.
3562,SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper proposes a method to improve the performance of encoder-decoder convolutional neural networks (CNNs) for denoising problems. The main idea is to use Stein’s unbiased risk estimator (SURE) as an unbiased estimator of the prediction error for the unseen test data. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. To address this issue, the authors propose a close form expression of SURE as a piecewise linear representation. The authors also propose a bootstrap and aggregation scheme to prevent the neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems."
3563,SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper proposes a method to improve the performance of encoder-decoder convolutional neural networks (CNNs) for denoising problems. The main idea is to use Stein’s unbiased risk estimator (SURE) as an unbiased estimator of the prediction error for the unseen test data. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. To address this issue, the authors propose a close form expression of SURE as a piecewise linear representation. The authors also propose a bootstrap and aggregation scheme to prevent the neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems."
3564,SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper proposes a method to improve the performance of encoder-decoder convolutional neural networks (CNNs) for denoising problems. The main idea is to use Stein’s unbiased risk estimator (SURE) as an unbiased estimator of the prediction error for the unseen test data. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. To address this issue, the authors propose a close form expression of SURE as a piecewise linear representation. The authors also propose a bootstrap and aggregation scheme to prevent the neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems."
3565,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning method for few-shot classification. The main idea is to balance the effect of meta-knowledge and task-specific learning within each task. The authors formulate this objective into a Bayesian inference framework and tackle it using variational inference. They validate their Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on multiple realistic task-and class-imbalanced datasets.
3566,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning method for few-shot classification. The main idea is to balance the effect of meta-knowledge and task-specific learning within each task. The authors formulate this objective into a Bayesian inference framework and tackle it using variational inference. They validate their Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on multiple realistic task-and class-imbalanced datasets.
3567,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning method for few-shot classification. The main idea is to balance the effect of meta-knowledge and task-specific learning within each task. The authors formulate this objective into a Bayesian inference framework and tackle it using variational inference. They validate their Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on multiple realistic task-and class-imbalanced datasets.
3568,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning from demonstrations. The main idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. This is achieved by giving the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and r = 0 for all other behavior. Theoretically, the authors show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo."
3569,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning from demonstrations. The main idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. This is achieved by giving the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and r = 0 for all other behavior. Theoretically, the authors show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo."
3570,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning from demonstrations. The main idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. This is achieved by giving the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and r = 0 for all other behavior. Theoretically, the authors show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo."
3571,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. To address these issues, the authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. The proposed method combines these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. The experiments show that the proposed method works for large, deforming point sets from different sources."
3572,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. To address these issues, the authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. The proposed method combines these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. The experiments show that the proposed method works for large, deforming point sets from different sources."
3573,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. To address these issues, the authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. The proposed method combines these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. The experiments show that the proposed method works for large, deforming point sets from different sources."
3574,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer (OT-SI) that learns the optimal transport cost function using a small amount of side information which is often available. The side information captures subset correspondence—i.e. certain subsets of points in the two data sets are known to be related. The proposed method differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. On systematic experiments in images, marriage-matching and single-cell RNA-seq, the proposed method substantially outperform state-of-the-art benchmarks."
3575,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer (OT-SI) that learns the optimal transport cost function using a small amount of side information which is often available. The side information captures subset correspondence—i.e. certain subsets of points in the two data sets are known to be related. The proposed method differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. On systematic experiments in images, marriage-matching and single-cell RNA-seq, the proposed method substantially outperform state-of-the-art benchmarks."
3576,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer (OT-SI) that learns the optimal transport cost function using a small amount of side information which is often available. The side information captures subset correspondence—i.e. certain subsets of points in the two data sets are known to be related. The proposed method differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. On systematic experiments in images, marriage-matching and single-cell RNA-seq, the proposed method substantially outperform state-of-the-art benchmarks."
3577,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"This paper proposes a novel method to detect anomalies in large datasets under a fully unsupervised setting. The key idea is to learn the representation underlying normal data. To this end, the authors leverage the latest clustering technique suitable for handling high dimensional data. The authors train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset based on clustering and representation learning. The reconstruction error from the learned auto-encoder serves as a scoring function to assess the normality of the data."
3578,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"This paper proposes a novel method to detect anomalies in large datasets under a fully unsupervised setting. The key idea is to learn the representation underlying normal data. To this end, the authors leverage the latest clustering technique suitable for handling high dimensional data. The authors train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset based on clustering and representation learning. The reconstruction error from the learned auto-encoder serves as a scoring function to assess the normality of the data."
3579,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"This paper proposes a novel method to detect anomalies in large datasets under a fully unsupervised setting. The key idea is to learn the representation underlying normal data. To this end, the authors leverage the latest clustering technique suitable for handling high dimensional data. The authors train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset based on clustering and representation learning. The reconstruction error from the learned auto-encoder serves as a scoring function to assess the normality of the data."
3580,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper proposes a new method for policy optimization in the context of constrained policy optimization. The proposed method, Projection-Based Constrained Policy Optimization (PCPO), is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, and the second step reconciles any constraint violation by projecting the policy back onto the constraint set. The authors theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. The empirical results over several control tasks demonstrate that PCPO achieves superior performance compared to state-of-the-art methods."
3581,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper proposes a new method for policy optimization in the context of constrained policy optimization. The proposed method, Projection-Based Constrained Policy Optimization (PCPO), is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, and the second step reconciles any constraint violation by projecting the policy back onto the constraint set. The authors theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. The empirical results over several control tasks demonstrate that PCPO achieves superior performance compared to state-of-the-art methods."
3582,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper proposes a new method for policy optimization in the context of constrained policy optimization. The proposed method, Projection-Based Constrained Policy Optimization (PCPO), is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, and the second step reconciles any constraint violation by projecting the policy back onto the constraint set. The authors theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. The empirical results over several control tasks demonstrate that PCPO achieves superior performance compared to state-of-the-art methods."
3583,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper analyzes the inner mechanism leading to the nice properties of word embedding methods. Specifically, it shows that the embedding can be viewed as a low rank transformation from the word-context co-occurrence space to embedding space. The authors provide a theoretical explanation for this behavior, and derive a method to automatically find its optimal value. The experiments on real datasets verify the analysis."
3584,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper analyzes the inner mechanism leading to the nice properties of word embedding methods. Specifically, it shows that the embedding can be viewed as a low rank transformation from the word-context co-occurrence space to embedding space. The authors provide a theoretical explanation for this behavior, and derive a method to automatically find its optimal value. The experiments on real datasets verify the analysis."
3585,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper analyzes the inner mechanism leading to the nice properties of word embedding methods. Specifically, it shows that the embedding can be viewed as a low rank transformation from the word-context co-occurrence space to embedding space. The authors provide a theoretical explanation for this behavior, and derive a method to automatically find its optimal value. The experiments on real datasets verify the analysis."
3586,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a new method for similarity measurement based on Random Projection Trees (RP Trees). The main idea is to use a group of approximate random projection trees (RPTs) to measure the similarity between two images. The authors propose to enforce the RPTs to share the same projection vectors and introduce randomness into the partition to eliminate the reliance on prior knowledge. Experiments on three datasets show that the proposed method outperforms the state-of-the-art methods.
3587,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a new method for similarity measurement based on Random Projection Trees (RP Trees). The main idea is to use a group of approximate random projection trees (RPTs) to measure the similarity between two images. The authors propose to enforce the RPTs to share the same projection vectors and introduce randomness into the partition to eliminate the reliance on prior knowledge. Experiments on three datasets show that the proposed method outperforms the state-of-the-art methods.
3588,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a new method for similarity measurement based on Random Projection Trees (RP Trees). The main idea is to use a group of approximate random projection trees (RPTs) to measure the similarity between two images. The authors propose to enforce the RPTs to share the same projection vectors and introduce randomness into the partition to eliminate the reliance on prior knowledge. Experiments on three datasets show that the proposed method outperforms the state-of-the-art methods.
3589,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method of variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation to reduce the simulation bias of finite-length MCMC chains. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. The experimental results show that the proposed method produces promising results on popular benchmarks."
3590,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method of variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation to reduce the simulation bias of finite-length MCMC chains. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. The experimental results show that the proposed method produces promising results on popular benchmarks."
3591,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method of variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation to reduce the simulation bias of finite-length MCMC chains. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. The experimental results show that the proposed method produces promising results on popular benchmarks."
3592,SP:64f2744e938bd62cd47c1066dc404a42134953da,This paper proposes a method for latent confounders for causal inference. The method is based on variational autoencoders (VAEs) that are trained to learn latent variables with missing values. They can be used either as a pre-processing step prior to causal inference or embedding them in a multiple imputation strategy to take into account the variability due to missing values in the data. Experiments show that the proposed method outperforms the state-of-the-art methods.
3593,SP:64f2744e938bd62cd47c1066dc404a42134953da,This paper proposes a method for latent confounders for causal inference. The method is based on variational autoencoders (VAEs) that are trained to learn latent variables with missing values. They can be used either as a pre-processing step prior to causal inference or embedding them in a multiple imputation strategy to take into account the variability due to missing values in the data. Experiments show that the proposed method outperforms the state-of-the-art methods.
3594,SP:64f2744e938bd62cd47c1066dc404a42134953da,This paper proposes a method for latent confounders for causal inference. The method is based on variational autoencoders (VAEs) that are trained to learn latent variables with missing values. They can be used either as a pre-processing step prior to causal inference or embedding them in a multiple imputation strategy to take into account the variability due to missing values in the data. Experiments show that the proposed method outperforms the state-of-the-art methods.
3595,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning (RL) policies, by combining ENAS (Vinyals et al., 2015; Pham et al, 2018; Zoph & Le, 2017) and ES (Salimans et al.) in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the authors represent compact architectures via efficient learned edge partitionings. For several RL tasks, they manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices."
3596,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning (RL) policies, by combining ENAS (Vinyals et al., 2015; Pham et al, 2018; Zoph & Le, 2017) and ES (Salimans et al.) in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the authors represent compact architectures via efficient learned edge partitionings. For several RL tasks, they manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices."
3597,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning (RL) policies, by combining ENAS (Vinyals et al., 2015; Pham et al, 2018; Zoph & Le, 2017) and ES (Salimans et al.) in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the authors represent compact architectures via efficient learned edge partitionings. For several RL tasks, they manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices."
3598,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,This paper proposes a method for learning representations for time-series. The method is based on the idea of learning a subset of invertible maps on R. The authors propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific loss and signal. The proposed method can be cast into a deep neural network.
3599,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,This paper proposes a method for learning representations for time-series. The method is based on the idea of learning a subset of invertible maps on R. The authors propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific loss and signal. The proposed method can be cast into a deep neural network.
3600,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,This paper proposes a method for learning representations for time-series. The method is based on the idea of learning a subset of invertible maps on R. The authors propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific loss and signal. The proposed method can be cast into a deep neural network.
3601,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to (products of) constant curvature spaces. Specifically, the authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvatures, and leverage gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Empirical results on node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior are provided."
3602,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to (products of) constant curvature spaces. Specifically, the authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvatures, and leverage gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Empirical results on node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior are provided."
3603,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to (products of) constant curvature spaces. Specifically, the authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvatures, and leverage gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Empirical results on node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior are provided."
