paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"This paper proposes a method for multi-agent multi-task learning based on role-based learning. The main idea is to decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Then, with knowledge of effects of actions, the authors train a role selector that determines the subset of actions that can fulfill certain actions under certain observations. The role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. The authors further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. The proposed method outperforms the current state-of-the-art MARL algorithms on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the behavior of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of empirical risk minimization problems, the authors prove that SSGD converges, respectively, with rates O(1/ ) and O(log(\1/)) for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the stoChastic Gradient Descent (SGD). The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmoothened machine learning models."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper investigates the effect of randomly initialized layers in transformers on the performance of the model. The authors introduce the idea of “reservoir layers”, which are non-linear layers that are interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks. They also show that freezing layers may actually improve performance."
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"This paper proposes a new approach to construct steerable convolution filters, which transform features in trival, dihedral and regular representations between the cyclic and dihedral groups. The authors show that kernel constructed by filter transform can also be interpreted in the group representation theory. They show that filter transformed kernels can be used to convolve input/output features in different group representation. This interpretation provides a novel and simple approach to implement steerable CNN operators. Experiments on multiple datasets verify the feasibility of the proposed approach."
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper proposes an optimal neural synthesis approach for multimodal program synthesis, where the goal is to find a program that satisfies user-provided constraints while also maximizing the program’s score with respect to a neural model. Specifically, the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of the method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model allows for efficient search over the space of syntactically valid programs, and it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs. The experimental results show that the proposed method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper proposes a protein graph convolutional neural network (PGCN) to predict the substrate specificity of a given enzyme, which is the set of all sequence motifs that are recognized/cut or not recognized by the enzyme. The proposed method is based on the Rosetta energy function, which describes the topology and energetic features, to determine substrate specificity. The authors use the PGCN to recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C virus, and show that its performance in classification tasks is equivalent or better than other methods."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the underestimation bias in double Q-learning, a classical method for reducing overestimation in deep Q learning. The authors show that underestimation may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address this issue, the authors propose a simple but effective approach as a partial fix for the underestimitation bias. This approach leverages an approximate dynamic programming to bound the target value. The proposed approach is evaluated on the Atari benchmark tasks and demonstrates its significant improvement over baseline algorithms."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"This paper proposes a two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, it generates images in low-frequency bands by training a sampler in the wavelet domain. Then, it super-resolves these images back to the pixel-space with a novel wavelet super-resolution decoder network. Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. The proposed method achieves a FID of 10.59 on ImageNet."
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper provides a theoretical analysis of self-supervised learning (SSL) for few-shot learning (FSL). In particular, the authors analyze the performance of SSL-based methods on FSL tasks in terms of the loss function. The main idea is to compare the loss functions of SSL and supervised methods on the downstream tasks. The authors show that the difference between the two losses is the upper bound of the distance between the supervised loss and the loss of the SSL loss on the training data. In addition, they show that if the training loss is small enough, the supervision loss can be reduced. Finally, they propose two ways to improve the test accuracy under the setting of FSL."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper studies the question of whether the elegant convergence theory for ultra-wide neural networks always converge to global minima near the initialization under first-order methods for finite-width neural networks. The authors show that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call Angular Distance (AD) function. The properties can be easily verified numerically."
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper proposes a method to combine knowledge representation and reasoning for deep neural networks. The key idea is to represent declarative knowledge as assertions in first order logic. The relations and functions that make up the vocabulary of the domain are implemented by neural networks that can have arbitrary structure. The logical connectives in the assertions compose these networks into a single deep network that is trained to maximize their truth. This provides a mechanism by which supervised, semi-supervised, unsupervised learning can take place simultaneously in a single network under a single training regime."
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper studies the inductive bias of residual neural networks (ResNets) in terms of the degree to which they learn iterative solutions. The authors define three indices of iterative convergence and show that, even though ResNets can express iterative solution, they do not learn them when trained conventionally on computer vision tasks. They then introduce regularizations to encourage iterative convergent computation and test whether this provides a useful inductive biases. To make the networks more iterative, they manipulate the degree of weight sharing across layers using soft gradient coupling. They also impose a Lipschitz constraint on the residual functions using spectral normalization."
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"This paper proposes two normalization techniques, i.e., self-norm and cross-norm, to improve the robustness of deep neural networks to out-of-distribution (OOD) data. SelfNorm uses attention to recalibrate statistics (channel-wise mean and variance), while CrossNorm exchanges the statistics between feature maps. Experiments on different domains (vision and language), tasks (classification and segmentation), and settings (supervised and semi-supervised) show their effectiveness."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper proposes a simple yet effective architecture for encoding image pixels in vision-based RL. In particular, the authors propose R-LAtte: Reinforcement Learning with Attention module, which augments a simple attention module in the convolutional encoder of an RL agent. The proposed module can extract interpretable task-relevant information such as agent locations and movements without the need for data augmentations or contrastive losses; (ii) significantly improve the sampleefficiency and final performance of the agents."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"This paper proposes an extension of GradNorm, a widely used gradient-based approach for training multi-task neural networks, by dynamically homogenizing not only the gradient magnitudes but also their directions across tasks. Specifically, it adds a layer of task-specific rotation matrices that aligns all the task gradients. Importantly, it also analyzes GradNorm through the lens of game theory, providing theoretical guarantees on the algorithm stability and convergence. Experiments on several real-world datasets and network architectures show that the proposed method outperforms previous approaches for multitask learning."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper proposes a geometry-distortion constraint for unsupervised image-to-image (I2I) translation, which promotes the consistency of geometry structures and reduces the unwanted distortions in translation by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of MGC, the authors propose an approximate representation of mutual information called relative Squared-loss Mutual Information (rSMI) that can be efficiently estimated analytically. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed method."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the effect of point sampling patterns in point cloud GANs. The authors propose a sampling spectrum to depict the different sampling sensitivities of discriminators. They further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics. They discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on sampling-related metrics."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper investigates the adversarial robustness of CapsNets. The authors propose a novel vote attack where they attack the votes of capsules directly, which is effective and efficient by circumventing the routing process. Furthermore, they integrate the attack attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of the proposed method."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"This paper studies the problem of meta-reinforcement learning in the online adaptation setting, where a single policy is trained for a fixed family of tasks. The paper proposes a new algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. The proposed method learns an informed policy (i.e., a policy receiving as input the description of the current task) that is used to both construct task embeddings from the descriptors, and to regularize the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task, and by exploiting them efficiently."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper proposes an approach for offline reinforcement learning for sequential recommendation systems (SRS). In particular, the authors propose an adaptive policy that learns to adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable to real-world environments where dynamics are changing and have stochasticity in the offline setting. Experiments are conducted in synthetic environments and a real world ride-hailing platform. "
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper proposes a goal-conditioned reinforcement learning algorithm that leverages the stability of supervised imitation learning to learn goal-reaching policies from scratch, without the need for expert demonstrations or a value function. The authors leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. They propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. "
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper proposes two improvements to FastSpeech, a non-autoregressive text-to-speech (TTS) model that uses a teacher model for duration prediction and knowledge distillation. In particular, the authors propose to directly train the model with the ground-truth target instead of the simplified output from the teacher model, and introduce more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. The authors also propose a method to directly generate speech waveform from text in parallel. Experimental results show that the proposed method achieves a 3x training speed-up over the previous fastspeech model."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"This paper studies the unsupervised dimension reduction (UDR) problem in the language of tempered distributions, i.e., the problem of approximating an empirical probability density function pemp(x) by another tempered distribution q(x), whose support is in a k-dimensional subspace, over a pertinent set of generalized functions. This formulation allows to establish a connection with another classical problem of data science — the sufficient dimension reduction problem (SDR). The authors introduce a nonnegative penalty function R(f) that “forces” the support of f to be in k-dimensions. Then they present an algorithm for minimization of I(f + \lambdaR(f), based on the idea of two-step iterative computation, which is a) an adaptation to real data and to fake data sampled around a k dimensional subspace found at a previous iteration, and b) calculation of a new k dimension subspace. They demonstrate the method on synthetic data and standard datasets."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,"This paper studies the trade-off between robustness and sensitivity in deep neural networks. The authors propose Feature Contrastive Learning (FCL) that encourages the model to be more sensitive to the features that have higher contextual utility. The utility is defined as the change in the loss function when we perturb a specific input feature. The sensitivity and utility are context dependent and change from one image to another. The goal is to ensure that if an input feature has high utility, the model will also be sensitive to it, and if it has low utility then the model won’t."
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper proposes a method for imitation learning from high-dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. The latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, the proposed method is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides a theoretical analysis of the lottery ticket hypothesis (LTH) by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. It shows that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, it is shown that the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer."
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes AutoLabel, a distance-based approach where the training labels are smoothed to different extents based on the distance between the augmented data and the clean data. The proposed method is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. The authors show that AutoLabel is a generic framework that can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on CIFAR-10, CifAR-100 and ImageNet show that the proposed method can improve models’ accuracy and calibration performance."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper studies self-supervised representation learning from a causal perspective. The authors propose a novel objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. The RELIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet. "
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper proposes a visual transformer network (VTNet) for target-driven visual navigation. The main idea is to use DETR object detector to extract spatial-aware descriptors and use them as the key and query for the decoder of the visual transformer. The decoder is then used to map the current visual representation and previous visual representation states to an action map, which is used to train a navigation policy network. Experiments on the artificial environment AI2-Thor demonstrate the effectiveness of the proposed method. "
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"This paper proposes a communication-computation efficient secure aggregation (CCESA) method for federated learning (FL). The key idea is to design the topology of the secret-sharing nodes (denoted as the assignment graph G) as sparse random graphs instead of the complete graph corresponding to the existing solution. Theoretical guarantees on the reliability/privacy of the proposed scheme are provided, and extensive real-world experiments demonstrate that the proposed method can maintain the same levels of reliability and data privacy in practical FL systems."
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,This paper proposes a two-player learning algorithm for finding truthful auctions. The proposed algorithm builds on top of the work of Duetting et al. (2019) by introducing a time-independent Lagrangian and introducing an additional neural network. The paper also proposes a new metric to compare the performance of truthful and non-truthful auctions. Experiments show that the proposed algorithm outperforms prior work.
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper proposes a method for fine-tuning pre-trained representations to downstream tasks. The proposed method is based on two heads: a classifier head with an improved contrastive cross-entropy loss to better leverage the label information in an instance contrast way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. The method is evaluated on CUB and CUB-C datasets and achieves state-of-the-art results."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper introduces a new measure for the robustness of classifiers called “genuine adversarial accuracy”, which measures the adversarial robustness without trading off accuracy on clean data and accuracy on the adversarially perturbed samples. The authors prove that a single nearest neighbor (1-NN) classifier is the most robust classifier according to the proposed measure for given data and a norm-based distance metric when the class for each data point is unique. Based on this result, the authors suggest that using poor distance metrics might be one factor for the tradeoff between test accuracy and lp-norm-based adversarial perturbation robustness."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper studies the problem of link prediction in homogeneous graphs, where the dyadic fairness criterion expects the predictions to be statistically independent of the sensitive attributes from the given two vertices. The authors first theoretically analyze the relationship between the fairness and the graph connections, and then propose an algorithm, FairAdj, to learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Experiments on six real-world social and citation networks show the effectiveness of the proposed method."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes a new method to improve the generative ability of autoencoders. The authors propose a disentangled exploration autoencoder (DEAE), which uses disentanglement representation and regularization to guarantee the validity of exploration in latent space and achieve controllable synthesis. The encoder of DEAE first turns the input sample into disentangling latent code, then explores the latent code space through directed interpolation. To aid the interpolated latent code in successfully outputting a meaningful sample, after the decoder, the authors regularize the output by ’reusing’ the encoder to force the obtained latent representation to maintain perfect disentangle. Experiments demonstrate that DEAE can improve the performance of downstream tasks by synthesizing attribute-controllable augmented samples."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a new memory model for the Kanerva machine (Wu et al., 2018a;b) based on a hierarchical latent variable model. The proposed model learns to compress an episode of samples into a latent multi-dimensional memory, where the key distribution is a deterministic function of the read key distribution. The authors demonstrate that the proposed model improves performance in memory conditional image generation tasks."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper studies the loss landscape of attention-based neural networks. Theoretical results show that, under mild assumptions, every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Besides, the theoretical results also provide guidelines for designing future attention models. "
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper extends the concept of expected free energy (EFE), which is a core quantity in active inference, and claims that EFE can be treated as a negative value function. Motivated by this connection, the authors propose a simple but novel method for learning a prior preference from experts. The proposed method, called prior preference learning (PPL), learns the prior preference of an active inference from an expert simulation. The authors show that the optimal distribution over the first-step action induced from active inference can be interpreted using Q-learning, and they propose a novel inverse RL algorithm for designing EFE-based rewards."
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. The authors show how to improve the generalization theoretically using OOD data in each learning scenario and complement the theoretical analysis with experiments on CIFAR-10, CifAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view. Moreover, the proposed method can be used in the absence of unlabeled in distribution (UID) data."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper proposes a meta-RL method that learns a shared linear representation of the policy and predicts a set of linear weights for adapting to a new task. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating the meta-policy via gradient descent to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods fail to generalize to. The experiments show that FLAP acquires adapted policies that perform much better on out-of-distribution tasks at a rapid adaptation rate up to 8X faster than prior methods."
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a federated learning algorithm for kernel k-means, where the goal is to compute the top $k$ eigenvalues of the kernel matrix $K$ in a distributed manner. The authors propose a distributed stochastic proximal gradient descent (DSPGD) algorithm to solve the optimization problem of the optimization of $k=1,k$ under federated settings, and a communication efficient mech anism (CEM) is designed to reduce the communication cost. Theoretical analysis shows that DSPGD with CEM converges with an O(1/T) rate, where T is the number of iterations. The clustering quality of the federated kernel k means is also shown to be comparable to that of the standard kernel means with a(1 + ǫ) approximate ratio. The experimental results show that the proposed federated kerne l-k means achieves the highest clustering performance with the lowest communication cost in most cases."
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"This paper proposes CompOFA, a method to reduce the search space of Once-For-All (OFA) by constraining search to models close to the accuracy-latency Pareto frontier. The authors leverage insights of compound relationships between model dimensions to build a design space smaller by several orders of magnitude. They show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper proposes a new meta-learning algorithm, ADML (ADversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. The authors claim that ADML is robust to adversarial attacks, i.e., it only leads to a minor performance degradation when there are adversarial examples, and sheds light on tackling the cases with limited and even contaminated samples. The proposed method is evaluated on two widely-used image datasets, MiniImageNet and CIFAR100, in terms of accuracy and robustness."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper proposes a data-driven framework for permutation selection in decoding, which combines domain knowledge with machine learning concepts such as node embedding and self-attention. The proposed method is based on the Bose Chaudhuri Hocquenghem (BCH) code, which is a corrupted version of Bose’s Bose code. The authors show that the proposed method can improve the bit error rate of the BCH code compared to the baseline decoders."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper proposes an approach to fine-tune a pretrained language model on a text classification task using an unsupervised clustering task as an intermediate task. The clustering is done using Bag of words (BOW) representations to partition the unlabeled training data into relatively homogeneous clusters of text instances. Next, the labeled data is treated as labeled data for an intermediate text classification tasks and train BERT – with or without additional MLM pretraining – with respect to multi-class classification. Extensive experimental results demonstrate the practical value of this strategy on a variety of data, most prominently when the training data available for the target task is of topical nature."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper compares several probabilistic and deterministic models for micro-data model-based reinforcement learning (MBRL) on the Acrobot environment. The authors show that the mixture density nets (MDFN) outperform all other models when multimodality is required. They also show that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. Finally, they improve the sample complexity of MBRL by two to four folds using an aggressive training schedule."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes a disentangled representation learning method that explicitly disentangles affine transformations in a self-supervised and rigorous manner. Unlike the disentanglement representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal/Vertical skew, and translation can be explicitly selected and learned. Experiments on the MNIST, CelebA, and dSprites datasets demonstrate the effectiveness of the proposed method."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,This paper proposes a new contrastive learning method for self-supervised learning with stronger augmentation. The main idea is to minimize the distribution divergence between the weakly and strongly augmented images over the representation bank to supervise the retrieval of strongly augmented queries from a pool of candidates. The proposed method achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned.
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"This paper proposes a method for de-identification of magnetic resonance images (MRI) that preserves privacy by remodeling privacy-sensitive facial features rather than removing them. The authors propose a conditional, multi-scale, 3D GAN architecture that takes a patient’s MRI scan as input and generates a 3D volume in which the brain is not modified but the face has been de-identified. Compared to the classical removal-based techniques, the proposed method preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper proposes a multi-head attention based global pooling layer to capture the interaction between nodes according to their structural dependencies. The authors formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi head attention based pooling layers. The GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. The experimental results show that GMT significantly outperforms the state-of-the-art node clustering methods on graph classification benchmarks with high memory and time efficiency, and obtains larger performance gain on graph reconstruction and generation tasks."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper proposes a new explanation for the poor performance of GNNs on long-range tasks. The authors claim that a bottleneck occurs when aggregating messages across a long path, which causes the over-squashing of exponentially growing information into fixed-size vectors. This bottleneck is analogous to the bottleneck of sequential RNNs, where the entire input sequence is encapsulated into a fixed-length vector, and the model fails to propagate messages originating from distant nodes. The paper also shows that prior work, which extensively tuned GNN models of long range problems, suffer from over-Squashing and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights."
SP:90d8fa381446923902e42b259392e5e975e6caa1,This paper proposes a method for cross-domain sentiment analysis. The main idea is to learn a prototypical distribution for the source domain that is domain-agnostic by minimizing the Sliced Wasserstein Distance (SWD) between the source and the target distributions. The prototypical distributions are estimated using a subset of the source samples for which the source classifier is confident about its predictions. The authors provide a theoretical proof to show that the proposed method minimizes the SWD between the target and source distributions. 
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper proposes a method to measure the presence of gender bias in NLI models by constructing a challenge task which involves pairing gender neutral premise against gender-specific hypothesis. The authors use the challenge task to evaluate state-of-the-art NLI model on presence of bias using occupations. Their findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets are significantly prone to gender induced prediction errors. They also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper studies the variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. The authors show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, the authors propose two methods respectively based on the transitional probability model and Gaussian mixture model."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,This paper studies the use of neural ensembles for few-shot image classification. The authors propose to use an ensemble of relatively small deep networks to improve the sample efficiency in the low-data regime by using a few labeled examples per class. They show that deep ensembling is a simple yet effective technique that outperforms current state-of-the-art approaches for learning from small datasets. They compare different ensemble configurations to their deeper and wider competitors given a total fixed computational budget.
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"This paper proposes a new method for sparsity in quantized neural networks (BNNs) that uses positive 0/1 binary weights, instead of the -1/1 weights used by state-of-the-art binary networks. The proposed method is able to achieve a high compression factor and reduces the number of operations and parameters at inference time. Experiments on linear and convolutional networks over MNIST and CIFAR-10 datasets show that SBNNs can achieve high compression rates and good generalization."
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a simple post hoc calibration method for predicting predictive uncertainty for deep neural networks. The proposed method uses outlier exposure to properly calibrate the model probabilities. The method works by no longer requiring that model outputs approximate class probabilities. It adds a simple extra calibration step, and detect the level of corruption of data, which allows the use calibrations tuned to the corruption level of the data. The approach is model agnostic, so it can be applied to future models."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, it compares the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. The authors also investigate the impact of node attributes on the performance of the different models and kernels."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a new method for self-supervised image animation. The main idea is to use the top-k percent occluded pixels of the foreground to regularize image generation. The proposed method is built on top of the First Order Motion Model and experimented on the VoxCeleb (Nagrani et al., 2017), BAIR (Ebert et al. 2017), and Tai-Chi-HD (Siarohin et. al., 2019b) datasets. The experimental results show that the proposed method outperforms state-of-the-art image animation approaches in pixel-wise difference, low-level similarity, keypoint distance, and feature embedding distance."
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper proposes a self-supervised approach for learning disentangled causal mechanisms (ICM), i.e. independent causal mechanisms that directly model multiple data generation processes (mechanisms) in a coarse granularity. The authors aim to learn a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. They outline sufficient conditions under which the mechanisms can be learned using a single self supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, they prove the identifiability of their model w.r.t. the mechanisms in the self supervised scenario. They compare their approach to disentangling representations on various downstream tasks, showing that their approach is more robust to intervention, covariant shift, and noise due to the disentanglement."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper proposes a graph aligning approach that generates rich or detailed labels given normal labels W for predicting molecular graph structure (W) given a 2D image of a chemical compound (U) from the source domain. The proposed approach is based on a fully mediating representation V such that f factors into U → V → W. However, observing V requires detailed and expensive labels. In this paper, the authors investigate the scenario of domain adaptation where we have access to the expensive labels V to the target domain where only normal labels w are available. The authors show that, using only 4000 data points, they obtain up to 4x improvement of performance after domain adaptation to target domain compared to pretrained model only on the original domain. "
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper proposes a monotonic neural network (MNN) to solve the energy optimization problems for chiller plants. The energy consumption estimation of most chillers can be physically viewed as an input-output monotonicity problem, and the authors design a Neural Network to mimic the physical behavior of the system. The authors verify the proposed method in a cooling system of a data center, experimental results show the superiority of the framework in energy optimization compared to the existing ones."
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a method to predict the causal effects of predictors on multiple forecasting targets, such as supply and demand in two-sided ride-hailing platforms. The authors integrate the causal attention with the Conditional Average Treatment Effect (CATE) estimation method in causal inference. They propose a novel and fast multi-head attention evolved from Taylor’s expansion instead of softmax, reducing time complexity from O(V) to O(\V) where V is the number of nodes in a graph. They further design a spatial graph fusion mechanism to significantly reduce the parameters’ scale."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes an unsupervised method to jointly learn a mixture of discrete and continuous factors of variability. The proposed method, called cpl-mixVAE, consists of multiple autoencoders, each of which receives an augmented copy of the given sample with the same class label. The agents are encouraged to reach consensus on the categorical assignments. The authors provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. The method is evaluated on MNIST and dSprites datasets, achieving state-of-the-art categorical assignment while preserving interpretability of the continuous factors."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper provides a general characterization of steerable kernel spaces for group equivariant convolutional networks (GCNNs). The main contribution is a generalized Wigner-Eckart theorem for G-steerable kernels, which shows that steerable kernels can be parameterized in terms of generalized reduced matrix elements, Clebsch-Gordan coefficients, and harmonic basis functions on homogeneous spaces. The paper is motivated by a striking analogy between the constraints underlying steerability kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. "
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper studies the effect of selective classification on accuracy disparities between groups. The authors analyze the margin distribution, which captures the model’s confidences over all predictions and determines which examples it abstains on at each threshold. For symmetric margin distributions, they prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Motivated by their analysis, they train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selective classification uniformly improves each group on these models."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper proposes a hierarchical nonnegative nonnegative (NCPD) decomposition (Hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. The proposed method utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD. This approach allows to explore the topics learned at different ranks simultaneously, and illustrate the hierarchical relationship of topics."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"This paper proposes a collective adversarial robustness certificate for graph neural networks (GNNs) for node classification. The main idea is to fuse multiple single-node certificates into a stronger collective certificate, i.e. the number of predictions that are simultaneously guaranteed to remain stable under perturbation. The proposed approach is based on the locality property of GNNs and leverage their locality property to make the proposed approach stronger."
SP:cc93dd2f68e415e2457166e78627865dc1b44697,This paper proposes Quantile Regression GAN (QRGAN) to improve the performance of GANs. The main idea is to use quantile regression to minimize the 1-Wasserstein distance between the distribution of the real and generated data. The proposed method is evaluated on CIFAR-10 and ImageNet datasets and compared with other GAN variants. The results show that QRGAN is more robust to mode collapse.
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"This paper proposes three tests to evaluate relevance metrics for similarity-based explanation. The first test is the model randomization test originally proposed by Adebayo et al. (2018) for evaluating saliency-based methods, and the other two tests are the identical class test and identical subclass, which are newly designed in this study. The experiments show that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. Some metrics perform poorly in the tests and analyzed the reasons of their failure."
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"This paper proposes a low-rank global attention (LRGA) module to improve the generalization of graph neural networks (GNNs) for improving their generalization power. The authors show that adding the LRGA module improves the expressiveness of GNNs and aligns them with a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In addition, the authors also show that LRGA can be used to augment existing GNN layers with state-of-the-art results in current GNN benchmarks."
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper proposes a label smoothing method to improve the calibration performance of convolutional neural networks (CNNs). The proposed method is based on the idea of objectness, which quantifies the likelihood of an image window containing an object belonging to any class. The authors propose a smoothing factor that is adaptive based on relative object size within an image. Experiments on ImageNet-1K demonstrate the effectiveness of the proposed method."
SP:5254658923e594294b69d124a8d004166852822a,"This paper proposes a two-layer fully-convolutional ReLU denoising network that is amenable to convex optimization. The convex dual network not only offers the optimum training with convex solvers, but also facilitates interpreting training and prediction. In particular, it implies training neural networks with weight decay regularization induces path sparsity while the prediction is piecewise linear filtering. Experiments with MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem."
SP:085cad6bc143c8713580bddfaa71f06496dac314,"This paper proposes an end-to-end approach for learning to synthesize speech from text or phonemes. The authors propose a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. The proposed model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training."
SP:01148cea55db606aa78d27e900818684a8bce9ab,This paper proposes a non-parametric method for node representation learning. The proposed method is based on the Wasserstein graph diffusion to smooth the distribution representations of nodes with information from their local neighborhoods. The authors claim that this allows them to reduce the distortion caused by missing attributes and obtain integrated representations expressing information of both topology structure and attributes. The method is evaluated on node classification and matrix completion tasks.
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes a meta-learning approach for intrinsic motivation for reinforcement learning (RL) in sparse reward procedurally-generated environments. In particular, the authors propose a teacher policy that learns to generate goals that are challenging for the student policy to achieve, but not impossible to achieve. The teacher policy is trained by minimizing the KL divergence between the teacher policy and the goal-conditioned student policy. The proposed approach is evaluated on the MiniGrid environment, which is procedurally generated. The results show that the proposed approach outperforms baselines in terms of intrinsic motivation."
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"This paper considers the problem of private information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. The authors show that the optimal rate-distortion-leakage tradeoff is convex and that in the limit of large file sizes this allows for a concise information-theoretical formulation in terms of mutual information. Moreover, the authors propose a new data-driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in term of download rate from the data itself. The proposed method is evaluated on a synthetic Gaussian dataset and on the MNIST and CIFAR-10 datasets."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) that, instead of sampling the input graph, decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. This allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. Further, the authors propose a lazy-update scheme during training to further improve its efficiency. Experimental results show that the proposed method achieves improved efficiency without significantly compromising model performances."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper proposes a method for answering 1-hop logical queries on incomplete Knowledge Graphs (KGs) using a pre-trained neural link predictor (NLP). The authors translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pretrained NLP model. They then propose two solutions to the optimisation problem, including gradient-based and combinatorial search, and show that the proposed approach produces more accurate results than state-of-the-art methods without the need of training on a large and diverse set of complex queries."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper proposes a method for verifying the local robustness of neural networks with piecewise-linear activation functions for the l_2 norm. The main idea is to partition the input space into a set of convex polyhedral regions in which the network’s behavior is linear. Then, a systematic search for decision boundaries within the regions around a given input is sufficient for assessing robustness. Crucially, the regions can be analyzed using simple geometric projections, thus admitting an efficient, highly-parallel GPU implementation that excels particularly for the L2 norm, where previous work has been less effective. Empirically, this approach is far more precise than many approximate verification approaches, while at the same time performing multiple orders of magnitude faster than complete verifiers."
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper proposes an approach for learning representations of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, better than a variety of alternative approaches. The dimensions learned are interpretable, and correspond to typical patterns of interaction with objects. Finally, the dimensions can be used to predict a state-of-the-art mental representation of objects."
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifiers by such observations makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. Two regularizers are proposed to increase the discriminability and enhance the feedback. The proposed method is compatible with centralized training and decentralized execution (CTDE). Empirically, the proposed method outperforms existing methods in a variety of multi agent cooperative scenarios."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper proposes a method to improve the certified robustness of randomized smoothed classifiers. The proposed method, called Smoothed WEighted Ensembling (SWEEN), is based on the idea of finding a good base models for randomized smoothing by ensembling. Theoretical analysis shows that the optimal SWEEN model can be obtained from training under mild assumptions. An adaptive prediction algorithm is also developed to reduce the prediction and certification cost. Extensive experiments show that the proposed method outperforms the upper envelope of the corresponding candidate models."
SP:ea892e3d199ed6121279b20061a87f43afae8796,"This paper proposes a method to discover subtask hierarchy by learning from demonstration. The proposed method is based on ordering memory policy network (OMPN), which is a recurrent policy network that can be used to perform task decomposition, recovering the subtask boundaries in an unstructured demonstration. Experiments on Craft and Dial demonstrate that the proposed method can achieve higher task decompositions performance under both unsupervised and weakly supervised settings. "
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) generalization and domain adaptation. Theoretically, the authors prove that under certain conditions, CSG can identify the semantic factor (s) and the semantic-identification error guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines. "
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of online learning with adversarial corruptions of the rewards. In particular, the authors consider stochastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs) where the reward at each time step can be corrupted with probability $\in [0,12$. The authors propose online learning algorithms with near optimal regret in three different scenarios: 1) Multi-armed bandit model, 2) linear contextual bandit, and 3) learning in finite state MDPs. "
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes Rewriter-Evaluator, which consists of a rewriter and an evaluator. At every pass, the rewriter produces a new translation to improve the past translation and the evaluators estimates the translation quality to decide whether to terminate the rewriting process. The authors also propose a prioritized gradient descent (PGD) method to facilitate training the two components jointly. Extensive experiments have been conducted on two translation tasks, Chinese-English and English-German, to verify that the proposed framework significantly improves the performance of NMT models and significantly outperforms prior methods."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"This paper proposes a two-stage approach to learn a multimodal predictive distribution for semantic segmentation. In the first stage, the model explicitly models the data with a categorical likelihood, and in the second stage, an adversarial network is trained to sample an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. The proposed method is evaluated on the multigrader LIDC dataset and a modified Cityscapes dataset."
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new compressed communication with error feedback (EF) method for dealing with contractive compressors. In particular, the authors propose a construction which can transform any contractive compressor into an induced unbiased compressor. Theoretical results show that the proposed method leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. The authors also extend their results to federated learning with partial participation following an arbitrary distribution over the nodes."
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper proposes a new research framework: hyperparameter transfer across adjustments (HT-AA), which provides four simple HT-AA baseline algorithms and eight benchmarks changing various aspects of ML algorithms, such as hyper-parameter search spaces and neural architectures. The authors show the advantage of transferring knowledge across developer adjustments: some of our simple baseline algorithms outperform some of the prominent HPO algorithms without transfer. "
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper studies the effect of label representations on the performance of deep neural networks. The authors propose a new paradigm for the task of image classification by using the image as the label and the audio as the prediction label. They show that high-entropy label representations are generally more useful because they provide a stronger error signal. They also show that neural networks trained with speech labels learn more robust features against adversarial attacks, and are more data-efficient."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes a multi-input multi-output (MIMO) configuration to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the sub-networks, the proposed method improves model robustness without increasing the computational cost. The proposed method is evaluated on CIFAR10, Cifar100, ImageNet, and their out-of-distribution variants."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. The proposed method first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixel-level and image-level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plug-and-play manner. The experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a new policy similarity metric (PSM) for measuring behavioral similarity between states. The PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. The authors also present a contrastive representation learning procedure to embed any state similarity metric, which they instantiate with PSM to obtain policy similarity embeddings (PSEs). The authors demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite."
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"This paper studies the problem of disentanglement, i.e. disentangling natural factors of variation in data (e.g. object shape vs pose). The authors propose a new approach based on distributed equivariant operators, which relies on classical results from group representation theory, to disentangle affine transformations acting on images. They show that this approach introduces discontinuities in the encoder for a broad family of transformations (translation, rotations, translations) and propose an alternative, more flexible approach that relies on distributed operators. Theoretical and empirical results demonstrate the effectiveness of the proposed approach."
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes an efficient expectationmaximization (EM) algorithm to estimate the maximum a posteriori (MAP) estimate of neural spike trains in the Hawkes process. In particular, three sets of auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. The authors demonstrate the accuracy and efficiency performance of the algorithm on synthetic and real data."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the dynamics of gradient descent (GD) for training two-layer neural network models in the under-parameterized and mildly-over-parametrized regimes. The authors show that there are two distinctive phases in the GD dynamics: an early phase in which the dynamics follows closely that of the corresponding random feature model, followed by a late phase where the neurons are divided into two groups: a group of a few (maybe none) “activated” neurons that dominate the dynamics and a group that are “quenched” that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions. This neural network-like behavior is continued into the mildly over-parametersized regime, in which it undergoes a transition to a random featurelike behavior where the inner-layer parameters are effectively frozen during the training process. This is qualitatively different from GD dynamics associated with the mean-field scaling where all neurons participate equally."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper proposes a method to solve a CMDP problem by decomposing the CMDP into a pair of MDPs; reconnaissance MDP and planning MDP. In R-MDP, the proposed method trains the threat function, which is the Q-function analogue of danger that can determine whether a given state-action pair is safe or not. In P-MD, the authors train a reward-seeking policy while using a fixed threat function to determine the safeness of each action. The proposed method is designed so that more training in P-mDP will always help the search space in R-mD. The authors also present an efficient approximation method that can greatly reduce the difficulty of solving R- mDP."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper provides a systematic comparison between the cross-entropy and the square loss for classification tasks. The authors show that the cross entropy loss performs comparably or better than the square one on a range of classification tasks for NLP, ASR and computer vision tasks. They also show that cross entropy is less sensitive to the randomness in the initialization. They conclude that the performance of modern classification models can be improved by training on a square loss. "
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes a self-supervised representation learning method for reinforcement learning (RL) based on the structure in visual input and sequential interaction with the environment. The proposed method, called Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future using an encoder which is an exponential moving average of the agent’s parameters and makes predictions using a learned transition model. The future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. SPR also improves performance by adding data augmentation to the future prediction task, which enforces consistency across different views of each observation, and achieves a median human-normalized score of 0.415 on Atari."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper proposes an efficient method for generating local node embeddings using local PageRank computations. The proposed method, called InstantEmbedding, uses a high-order similarity matrix based on Personalized PageRank (PPR) as foundations on which local node representations are computed via hashing. Theoretical guarantees on the locality of the computation, as well as the proof of the global consistency of the generated representations, are provided. Empirical results show that the proposed method is able to produce high-quality representations on par with state-of-the-art methods, with efficiency several orders of magnitude better in clock time and memory consumption."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"This paper proposes a data-driven approach to improve the quality of graph coarsening. The authors propose a framework for measuring the quality and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graphs may be suboptimal, the authors propose to parametrize the weight assignment map with graph neural networks and train it to improve its quality in an unsupervised way. Extensive experiments on both synthetic and real networks demonstrate that the proposed method significantly improves common graph Coarsening methods under various metrics, reduction ratios, graph sizes, and graph types."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper proposes a geometric deep learning algorithm to estimate the acoustic scattering properties of 3D objects at interactive rates. The method uses a point cloud approximation of each object, and each point is encoded in a high-dimensional latent space. The proposed method can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce GeForce 2080 Ti GPU. The paper also proves that the learning method is permutation and rotation invariant."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper proposes Risk Extrapolation (REx) as a form of robust optimization over a perturbation set of extrapolated domains (MMREx), and propose a penalty on the variance of training risks (V-REx). The authors prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (covariate shift). REx is able to outperform other methods such as Invariant Risk Minimization in situations where these types of shift co-occur."
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper proposes a method for solving complex partial differential equations (PDEs) using neural networks. The proposed method is based on the Fourier neural operator (FNO), which is a convolutional neural network that maps between functions in the space of local basis functions. The method is evaluated on Burgers’ equation, Darcy flow and Navier-Stokes equation, and is shown to be up to three orders of magnitude faster than traditional numerical solvers. "
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the implicit bias of gradient flow (GD) on linear neural network training. The authors propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, the convergence direction of the network parameters as singular vectors defined by the network. For separable classification, the authors show that if the linear network is orthogonally decomposable, the gradient flow finds the max-margin solution in the singular value space. For underdetermined regression, the paper proves that GD finds a global minimum which minimizes a norm-like function."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper studies the problem of optimizing slimmable neural networks from a multi-objective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. The proposed method is based on two-stage training, where one can first train a super-network with uniformly sampling width weights for each layer, and then one can train a sub-network by jointly optimizing the width multipliers for different layers and shared weights. The authors conduct extensive experiments with 15 network and dataset combinations and two types of cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method compared to existing alternatives."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper studies the problem of federated semi-supervised learning (FSSL) in the setting where the labeled data is only available at the server. The authors propose a novel method to tackle the problems, which they refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of Federated Learning and Semi-Supervised Learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. The experimental results show that FedMatch outperforms both local federated learning approaches and baselines."
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"This paper proposes a self-supervised learning method for discrete event sequences. The proposed method is based on contrastive learning, which adopts the idea of positive pairs and negative pairs to represent semantically similar objects (positive pairs of images, video, audio, etc.) closer to each other, while dissimilar ones (negative pairs) further away. The method is evaluated on several public datasets and showed that CoLES representations consistently outperform other methods on downstream tasks."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper proposes a method for unsupervised dependency parsing and constituency parsing. The authors propose a parsing framework that can jointly generates constituency tree and dependency graph. Then they integrate the induced dependency relations into transformer, in a differentiable manner, through a dependency-constrained self-attention mechanism. The proposed model can achieve strong performance on masked language model tasks."
SP:078966ff62775bba6031e47d374bda95f4a7dde3,This paper proposes a method for learning the mapping between scene graph nodes and visual objects under weak supervision. The proposed method learns a metric among visual objects and scene graphs by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that the proposed method post an improvement on scene graph grounding task over current state-of-the-art approaches. The grounding found by the proposed model can reinforce the performance of the existing method.
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"This paper proposes two variants of sliced fused Gromov Wasserstein (SFG) for relational regularized autoencoders (RAEs). The first variant, MSSFG, replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, PSSFG, replaces vMF with a power spherical distribution to improve the sampling time in high-dimensional settings. Experiments on latent manifold structure, image generation, and reconstruction are conducted to demonstrate the effectiveness of the proposed methods."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes an approach to speed up training for a particular kind of deep networks which contain repeated structures, such as the transformer module. The authors first train such a deep network with the weights shared across all the repeated layers and then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. The adaptive untying criterion is obtained from a theoretic analysis over deep linear networks. Empirical results show that the proposed method is able to reduce the training time of BERT by 50%."
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper investigates the interaction inside adversarial perturbations to explain and boost the adversarial transferability. The authors discover and prove a negative correlation between the transferability and the interaction between the perturbation units. Based on this, the authors propose to directly penalize interactions during the attacking process to improve the adversary transferability, which is shown to be more effective than existing transferability boosting methods. "
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper studies the role of higher layers in catastrophic forgetting in deep neural networks. The authors show that higher layers are disproportionately responsible for catastrophic forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. They investigate different methods for mitigating catastrophic forgetting and find that while all stabilize higher layer representations, some methods encourage greater feature reuse in higher layers, while others store task representations as orthogonally, preventing interference. They also study the connection between forgetting and task semantics, finding that semantic similarity between subsequent tasks consistently controls the degree of forgetting."
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes a general efficient training algorithm for large-scale language models. The proposed method is inspired by the Lottery Ticket Hypothesis (LTH) and the Early-Bird Lottery Tickets (EBT) for computer vision tasks. By slimming the self-attention and fully-connected sub-layers inside a transformer, the authors are the first to identify structured winning tickets in the early stage of BERT training, and apply those tickets towards efficient BERT pretraining and fine-tuning. Extensive experiments on GLUE and SQuAD tasks demonstrate that EarlyBERT can save 35%-45% training time without sacrificing accuracy."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper studies the robustness of f-divergence measures for learning with noisy labels. The authors propose a family of f divergences that are robust to label noise. The main idea is to use the variational form of the f divergence and show that it is decoupled from the label noise, which is a nice property. Then, the authors show that under certain conditions, the corresponding f divergence measures are robust. Finally, they show that when the f divergence measures are possibly not robust, they can be made robust."
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes ensemble-based weighted Bellman backups, which re-weights target Q-values based on uncertainty estimates from a Q-ensemble. The authors show that the proposed method stabilizes and improves learning on both continuous and discrete control benchmarks. They also investigate the signal-to-noise aspect by studying environments with noisy rewards, and find that weighted bellman backups significantly outperform standard Bellman backup. "
SP:afc08f203562b841180811aef943bfb63a1659ea,This paper proposes a method for task calibration for few-shot classification. The authors propose to measure the distributional mismatch between support and query sets via class-wise similarities. The method is algorithm-agnostic and readily expanded to include a range of meta-learning models. Extensive experiments are conducted to show the effectiveness of the proposed method.
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a method for video-text representation learning. The authors argue that the dominant paradigm for learning video representations, noise contrastive learning, encourages dissimilar representations even for samples that are semantically related (e.g. visually similar videos or ones that share the same depicted action). The authors propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of other support samples’ visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise-contrastive learning."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper proposes a novel method, seg tok, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. The authors also propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that the proposed method improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs’ downstream performance, especially it can improve the performance on sequence labeling tasks."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"This paper proposes an unbiased boundary sampling strategy to enable efficient and scalable distributed GCN training while maintaining the full-graph accuracy. The authors identify the main challenges in parallel training of GCNs: prohibitive memory requirement and communication volume and further identify the cause of these challenges to be excessive number of boundary nodes within each partitioned subgraph. The proposed method is evaluated on node classification, link prediction, graph classification and recommendation systems."
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper presents a graph neural network (GNN) based quantum chemistry simulation model that uses surrounding 3D molecular structure to estimate per-atom forces. The key challenge is to accurately capture highly complex and non-linear quantum interactions of atoms in 3D space, on which forces are dependent. This paper adopts (1) expressive message passing architecture, (2) appropriate choice of basis and activation functions, and (3) model scaling in terms of network depth and width. The authors show that ForceNet reduces the estimation error of atomic forces by 30% compared to existing ML models, and generalizes well to out-of-distribution structures."
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper studies the impact of different regularization strategies when fine-tuning a pre-trained network for a new task. The authors provide a neural network generalization bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Inspired by this, the authors develop a simple yet effective fine tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre- trained weights, thus obtaining provably better generalization performance than conventional transfer learning."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"This paper investigates the phenomenon of decoupled find-eval phenomenon, where the best hyperparameters for mask discovery (Hfind) and mask evaluation (Heval) are different for mask generation and mask pruning, respectively. The authors show that different Hfind values yield masks with different layerwise pruning ratios and that the phenomenon is causally mediated by these ratios. This phenomenon holds across a number of models, datasets, configurations, and also for one-shot structured pruning."
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper proposes a new metric (m-coherence) to study the alignment of per-example gradients during training. This metric is based on the Coherent Gradients (CG) theory that provides a simple, unified explanation for memorization and generalization in neural networks. The authors show that m-coidelity is more interpretable, cheaper to compute (O(m) instead of O(m)) and mathematically cleaner. They also show the evolution of alignment of each example gradients in ResNet and EfficientNet models on ImageNet with label noise."
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper considers the problem of learning sufficient statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The authors frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. They apply their approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks."
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper proposes an unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translate input images into the estimated domains. TUNIT uses a differentiable clustering method based on mutual information maximization for estimating domain labels, a contrastive loss for embedding style codes, and a generative adversarial networks (GAN) to learn the image translation functions across various domains. Experimental results show that the model achieves comparable or better performance than the set-level supervised model trained with full labels, generalizes well on various datasets, and is robust against the choice of hyperparameters."
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper studies the implicit bias of gradient descent training of wide neural networks and the corresponding implicit bias in function space. The authors show that the solution of training a width-n shallow ReLU network is within n-1/2 of the function which fits the training data and whose difference from initialization has smallest 2-norm of the weighted second derivative with respect to the input. The curvature penalty function 1/ζ is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and the authors compute it explicitly for various common initialization procedures. "
SP:8b885142facbb3b8db41ec9d83822cee81324694,"This paper studies the effect of weight decay in the training of deep neural networks. The authors first show that the L2 regularization is unstable weight decay for all optimizers that use momentum, such as stochastic gradient descent (SGD). Second, they show that decoupled weight decay is highly unstable for all adaptive gradient methods. Finally, they propose the Stable Weight Decay (SWD) method to fix the instability problem from a dynamical perspective."
SP:a3206dc71e32ba1830895bf442d3840f3331a532,"This paper proposes a method to combine the strengths of both Translation Memory (TM) and Neural Machine Translation (NMT) by treating the matched sentence pair of TM as the additional signal and apply one encoder enhanced by the pre-trained language model (PLM) to encode the TM information and source sentence together. The authors extend the sentence level retrieval method to the n-gram retrieval method so that they don’t need to calculate the similarity score. Further, they explore new methods to manipulate the information flow from TM to the NMT decoder. The proposed methods are evaluated on a mixed test set of multiple domains."
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper attempts to interpret modern deep (convolutional) networks from the principles of rate reduction and (shift) invariant classification. It shows that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators (linear or nonlinear), and parameters of the network are all explicitly constructed layer-by-layer in a forward propagation fashion. All components of this “white box” network have precise optimization, statistical, and geometric interpretation. The derivation also indicates that such a convolutional network is significantly more efficient to learn and construct in the spectral domain."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. They show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and they show that the amount of acceleration depends on both the magnitude of that difference (which is fixed at initialization) and the spectrum of the data. They prove their results without assuming small, balanced or spectral initializations."
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper studies the problem of generating interpretable explanations for black-box machine learning models. The authors propose a new method, CLIME, which is based on uniform sampling of user-defined subspaces, which allows the user to define the precise subspace of the input domain to be explained. The proposed method can be applied to any ML model and extensive experiments demonstrate its versatility on real-world problems. "
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"This paper proposes AMBERT (Multi-grained BERT), a language model for natural language understanding (NLU) based on BERT. The model takes both the sequence of words (finegrained tokens) and the sequences of phrases as input and employs one encoder for processing the words and the other encoders for the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show the model outperforms the existing best performing models in almost all cases."
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper proposes a new Transformer-based semantic parsing model. The main idea is to incorporate Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS and Atis datasets."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes a new adversarial training method to improve the robustness of deep neural networks (DNNs) against composite adversarial attacks (compositions of multiple perturbations). The proposed method, called CAT, is able to flexibly integrate and optimize multiple adversarial robustness losses, which leads to DNNs robust with respect to multiple individual perturbation models as well as their compositions. The proposed approach is evaluated on MNIST and CIFAR-10 datasets, and the results show that the proposed method outperforms the baselines."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"This paper proposes a method for learning abstract rules from high-dimensional data. The proposed method is based on a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. The authors show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a new framework for structured prediction tasks in NLP. The proposed framework is based on translation between augmented natural languages (TANL) and aims to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The authors show that their approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on CoNLL04, ADE, NYT, and ACE2005 datasets. They accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning)."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper studies the impact of unlabeled entity problem on NER models. The authors claim that two causes of performance degradation are identified: (1) the reduction of annotated entities and (2) treating unlabelled entities as negative instances. Based on the above observations, the authors propose a general approach that can almost eliminate the misguidance brought by unlabeling entities. The key idea is to use negative sampling that, to a large extent, avoids training NER model with unlabelling entities. Experiments on synthetic datasets and real-world datasets show that the proposed approach is robust to unlabelED entity problem and surpasses prior baselines."
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel acoustic word embedding called Acoustic Neighbor Embeddings where speech or text of arbitrary length are mapped to a vector space of fixed, reduced dimensions by adapting stochastic neighbor embedding (SNE) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training. It also gives more accurate results with low-dimensional embeddings when the two encoder networks are used in tandem in a word (name) recognition task."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper proposes a reinforcement learning algorithm for stationary mean-field games, where the goal is to learn a pair of mean field state and stationary policy that constitutes the Nash equilibrium. The proposed algorithm is in stark contrast with previous literature which solves each single-agent reinforcement learning problem induced by the iterates mean field states to the optimum. The authors prove that their fictitious play algorithm converges to the equilibrium at a sublinear rate."
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper studies the problem of probabilistic inference on the joint distribution defined by a normalizing flow model. Given a pre-trained flow model $p(x)$, one wishes to estimate the conditional distribution p(x2 | x1) for some arbitrary partitioning of the variables $x = (x1,x2$. The authors first show that this task is computationally hard for a large class of flow models. Motivated by this hardness result, they propose a framework for approximate probablistic inference. Specifically, their method trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, they can efficiently train it using variational inference and also handle conditioning under arbitrary differentiable transformations. The resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling. They provide an extensive empirical evidence showcasing the flexibility of our method on a variety of inference tasks with applications to inverse problems."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper introduces a new ultra-high resolution image segmentation dataset for the cell membrane, called U-RISC, which is the largest annotated Electron Microscopy (EM) dataset with multiple iterative annotations and uncompressed high-resolution raw data. The authors also propose a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of cell membrane segmentation results. Experiments on human subjects show that PHD is better than the popular segmentation methods."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper proposes a new benchmark for continual learning (CL) that aims to evaluate the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. The authors also propose a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. The learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks and better transfer and ability to scale streams with a hundred tasks."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper proposes a generative model-based meta-learning approach for few-shot classification tasks. The proposed approach, LASIUM, generates pairs of in-class and out-of-class samples from the latent space in a principled way, allowing us to create synthetic classes forming the training and validation data of a meta-task. The authors show that the proposed approach outperforms or is competitive with current unsupervised learning baselines on the most widely used benchmark datasets."
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies the injectivity of ReLU neural networks. The authors show that injectivity is necessary and sufficient for layerwise layerwise injectivity and global injectivity. They also show that the stability of inverting an injective network is characterized by worst-case Lipschitz constants of the inverse. Finally, they show that an end-to-end—rather than layerwise—doubling of the dimension suffices for injectivity, and establish a theoretical basis for the study of nonlinear inverse and inference problems."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes the continuous conditional generative adversarial network (CcGAN), a generative model for image generation conditional on continuous, scalar conditions (termed regression labels). The regression labels are scalar and infinitely many, and conventional label input methods (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. The proposed CcGAN solves the above problems by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a novel method to incorporate regression labels into the generator and the discriminator. "
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper proposes to combine semi-supervised learning (SSL) and active learning (AL) to improve the sample complexity of supervised deep learning (DL). In particular, the authors argue that the annotation efficiency brought by AL algorithms that seek diversity on labeled samples can be improved upon when using SSL as the training scheme. The authors propose an AL algorithm that focuses on controlling the convergence rate of a classification network by actively querying instances to improve rate of convergence upon inclusion to the labeled set. The experiments show that a deep neural network trained using a combination of the proposed method and a recently proposed SSL algorithm can achieve high performance using far less labeled samples than SL."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes a federated learning method for training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. The authors view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. They point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. They propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. They demonstrate both through empirical results on real and synthetic data as well as analytical results that their scheme leads to efficient training, in both convex and non-convex settings."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper focuses on accelerating contrastive learning algorithms with little or even no loss of accuracy. The authors claim that the similarity on the intermediate layers is a good surrogate of the final similarity, and exploit this observation by introducing additional intermediate contrastive losses. In this way, they can truncate the back-propagation and update only a part of the parameters for each gradient descent update. They also do selection based on intermediate losses to filter easy regions for each image, which further reduces the computational cost. They apply their method to recently-proposed MOCO (He et al., 2020), SimCLR (Chen et al. 2020a), SwAV (Caron et al, 2020) and MocO V2 (Cynthia et al 2020b)."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies the global convergence and global optimality of actor-critic in the single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. The authors consider two function approximation settings where both the actor or critic are represented by linear or deep neural networks. For both cases, the actor sequence converges to a globally optimal policy at a sublinear O(K-1/2) rate, where K is the number of iterations."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes to represent logs at a few levels of abstraction including field level, log level, and sequence level. The representations are in vector format and serve as interfaces to downstream applications. The authors use a version of Transformer Networks (TNs) to encode numerical and textual information that is suitable for log embeddings. They show how a number of log processing applications can be readily solved with the representation."
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper proposes a method to constrain the behavior of convolutional layers by splitting them into a succession of wavelet packet decompositions, which are modulated by freely-trained mixture weights. The proposed method is evaluated on the AlexNet architecture for image classification. The experiments show that the proposed method achieves the accuracy rate of standard AlexNet, but with a significantly lower number of parameters."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes an attention mechanism for both the players and the coach in multi-agent reinforcement learning (MARL) with variable number of agents. In particular, the authors propose a variational objective to regularize learning and propose an adaptive communication method to let the coach decide when to communicate with different agents. The proposed method is evaluated on resource collection tasks in a particle environment and shows zero-shot generalization to new team compositions."
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper provides a comprehensive and large-scale empirical study of success and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through extensive experiments, the authors show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of the influence functions. In particular, they find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) accuracy of influence estimates can vary significantly depending on the examined test points."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"This paper studies the connection between the pretraining task of next word prediction and text classification. The authors hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pre-training task. Theoretical results show that language models that are -optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with O(sqrt(log(1/\epsilon)) error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. The paper also proposes a new objective function that performs well on some classification tasks."
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper proposes a black-box membership inference attack (MIA) method for conditional image generation models (e.g. image translation). The main idea is to use a novel difficulty score that can be computed for each image, and its computation does not require a training set. The proposed method achieves high MIA accuracy on an extensive number of benchmarks."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes a Dirichlet Neural Architecture Search (DrNAS) method for neural architecture search (NAS) by formulating it into a distribution learning problem. The authors treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichle distribution, and use pathwise derivative estimators to compute the gradient of the distribution. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. To alleviate the large memory consumption of differentiable NAS, the authors propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between the search and evaluation phases."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper proposes a multiplicative filter network (MFN) for representing low-dimensional but complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance functions or neural radiance fields. In this paper, the authors propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems. In these networks, they simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximation over an exponential number of Fourier/Gabor basis functions."
SP:f5be855300f63c185a006834302bd4b033b56258,"This paper proposes a new algorithm for gradient-based meta-learning. The authors propose to use a student network to explore the search space for a large number of updates, and then use a teacher network to “leap” toward the regions visited by the student network. The teacher network not only arrives at a high-performing model but also defines a very lightweight computation graph for the outer loop for the meta-gradients. The proposed algorithm is generic and can be applied to any meta learning algorithm."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"This paper proposes a variant of the behavior regularized offline reinforcement learning (BRAC) algorithm (Wu et al., 2019), which adds a regularization term to the policy evaluation objective that penalizes the gradient of the Q value at out-of-distribution (OOD) actions. The authors propose to use the analytical upper bound on the KL divergence as the regularizor to reduce the variance associated with sample based estimations, and employ state-dependent Lagrange multipliers for the regularization terms to avoid distributing KL divergence penalty across all states of the sampled batch. The proposed method is evaluated on the Mujoco benchmark and outperforms the baselines."
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,This paper introduces Adjoined networks as a training approach that can regularize and compress any CNN-based neural architecture. The one-shot learning paradigm trains both the original and the smaller networks together. The parameters of the smaller network are shared across both the architectures. The authors prove strong theoretical guarantees on the regularization behavior of the adjoint training paradigm. They complement their theoretical analysis by an extensive empirical evaluation of both the compression and regularization behaviour of adjoint networks.
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper proposes a simple extension of the greedy exploration algorithm for reinforcement learning (RL). The authors claim that the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. They propose a temporally extended form of -greedy that simply repeats the sampled action for a random duration. They show that for many duration distributions this suffices to improve exploration on a large set of domains."
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper studies the implicit regularization of gradient flow (GD) in matrix factorization. The authors show that GD with infinitesimal initialization is equivalent to Greedy Low-Rank Learning (GL), a heuristic rank minimization algorithm, under some reasonable assumptions. They also extend the results to the case where depth > 3, and show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude."
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper proposes a two-stage framework for improving robustness of classifiers to subgroup differences. The first stage models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate the subgroups features. The second stage is to use the transformations to remove the classifier’s dependence on subgroup-specific features.  The proposed method is evaluated on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. "
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper proposes a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable nonfuzzy rules for data representation. To train the non-differentiable RRL effectively, the authors project it to a continuous space and propose a novel training method, called Gradient Grafting, that directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Experiments on 9 small and 4 large data sets show that RRL outperforms the competitive approaches."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a regret minimization (RGM) algorithm and its extension for structured environments. RGM builds from invariant risk minimization by recasting simultaneous optimality condition in terms of predictive regret, finding a representation that enables the predictor to compete against an oracle with hindsight access to held-out environments. The structured extension adaptively highlights variation due to complex environments via specialized domain perturbations. The authors evaluate the method on multiple applications: molecular property prediction, protein homology and stability prediction and show that RGM significantly outperforms existing baselines."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes a new cross-probe BERT architecture for cross-modal text-to-image retrieval. The proposed method is based on the two-tower architecture of BERT, where one tower extracts features of text queries and the other tower extracts feature of reference images. In the training phase, the parameters of two towers are optimized so that a query and its relevant images are close in the feature space whereas the distance between the query and the irrelevant images is large. The cached image features can be directly compared with the query’s feature, and the retrieval is efficient."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new type of Actor, named forward-looking Actor or FORK for short, for Actor-Critic algorithms. It can be easily integrated into a model-free ActorCritic algorithm. The experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK can bring to the state of the art algorithms."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper proposes a new aggregation method for federated learning (FL) based on Bayesian inference. The proposed method is based on sampling higher-quality global models and combining them via Bayesian model Ensemble, leading to robust aggregation. The authors show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. The empirical studies validate FEDBE’s superior performance, especially when users’ data are not i.i.d. and when the neural networks go deeper."
SP:3ac5f437fc349a33810d0645664d1c448528af74,"This paper investigates how BERT contextualized representations capture grammatical concepts such as subject-verb agreement (SVA) and reflexive anaphora (RA). The authors introduce multi-partite patterns, abstractions of sets of paths through a neural model (a graph) that quantify and localize the effect of an input concept (e.g. subject’s number) on an output concept (the corresponding verb’ number) to a collection of paths passing through a sequence of model nodes and/or edges. They describe guided pattern refinement, a search procedure for finding patterns representative of concept-critical paths that let us selectively explore the importance of chosen aspects of a model. They further extend the experimental framework to integrate impacts of multiple words towards a given concept (as opposed to impact of a single word on SVA)."
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper studies the problem of finite-time convergence of deep neural networks. The authors formulate the supervised learning framework as a control problem where weights of the network are control inputs, and learning translates into a tracking problem. They provide a priori guarantees of finite time convergence in a deterministic control theoretic setting. In particular, they perform Lyapunov analysis of the loss function to derive an upper bound on the settling time of the deep neural network."
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper studies disentanglement of representations, which is one of the most important topics of machine learning. In this paper, the authors point out that the method taken by GIN for informative latent variables selection is not theoretically supported and can be disproved by experiments. Instead, they propose to use the mutual information between each learned latent variables and the auxiliary variable to correctly identify the “informative latent variables”. They show the advantage of their method on various downstream tasks including classification, outlier detection and adversarial attack defence."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"This paper proposes a bidirectional pooling layer for image classification and semantic segmentation. The proposed method is based on the classical lifting scheme, which decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail sub-band, which is useful for image-to-image translation challenges. Experiments on CIFAR-10 and ImageNet show that the proposed method achieves better results than the baselines."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset T into binary sequences in the cube {±1}. When T consists of well-spread (i.e., non-sparse) vectors, the embedding method applies a stable noise-shaping quantization scheme to Ax where A is a sparse Gaussian random matrix. Moreover, the authors show that Euclidean distances among the elements of T are approximated by the `1 norm on the images of the cube. This again contrasts with standard methods, where the Hamming distance is used instead. The method is both fast and memory efficient, with time complexity O(\m) and space complexity O(m) on well- spread data. The authors also show that the approach still works provided that data is transformed via a Walsh-Hadamard matrix, but now the cost is O(n log n) per data point."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,This paper proposes to use plasticity rules as a proxy for Gradient Descent (GD) in order to improve the generalization and robustness of RNNs. Plasticity rules are laws controlling changes of the strength of a synapse based on the firing history as seen at the post-synaptic neuron. The authors argue that the plasticity rule can be learned by GD on the rule parameters and that this process may be a missing ingredient for the development of ANNs that generalize well and are robust to adversarial perturbations. They provide both empirical and theoretical evidence for this hypothesis. 
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new approach for visual question generation (VQG) task, which aims to generate human-like questions from an image and potentially other side information (e.g. answer type or the answer itself). The authors propose a novel learning paradigm to generate visual questions with answer-awareness and region-reference. In particular, they aim to ask the right visual question with Double Hints textual answers and visual regions of interests, effectively mitigating the existing one-to-many mapping issue. They develop a simple methodology to self-learn the visual hints without introducing any additional human annotations. They also propose a new double-hints guided Graph- to-Sequence learning framework that first models them as a dynamic graph and learns the implicit topology. The experiments on VQA2.0 and COCO-QA datasets demonstrate that their proposed model on this new setting can significantly outperform existing state-of-the-art baselines by a large margin."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper studies the question of whether optimally tuned regularization can mitigate the double descent phenomenon in linear regression. Theoretically, the authors prove that for certain linear regression models with isotropic data distribution, optimally-tuned l2 regularization achieves monotonic test performance as we grow either the sample size or the model size. The authors also demonstrate empirically that optimally tuning the regularizer can mitigate double descent for more general models, including neural networks."
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"This paper introduces a spatial dependency networks (SDN) for building image generators and applies it to variational autoencoders (VAEs). The authors show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. In a vanilla VAE setting, a powerful SDN decoder also improves learning disentangled representations."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper proposes a simplified version of BCQ (Fujimoto et al., 2018a) for offline reinforcement learning. The main idea is to use a generative model to estimate the Q-values of the policy and then use this estimate as the input to a Q-learning algorithm. The authors also propose a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, EMaQ explicitly considers the number of samples and the proposal distribution, allowing them to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting – the main focus of this work – EMaq matches and outperforms prior state-of-the-art in the D4RL benchmarks (Fu et al. 2020a). In the online RL setting, the authors demonstrate that EMAQ is competitive with SAC."
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper proposes a batch selection algorithm for improving model fairness by adjusting batch size based on the fairness measure of the intermediate model, measured in the current epoch. The proposed algorithm, which the authors call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. The authors claim that the proposed algorithm is compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes. Experiments conducted both on synthetic and benchmark real data demonstrate the effectiveness of the proposed method."
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the Lipschitz constants of monotone deep equilibrium models (monDEQs), a recently proposed class of DEQs that can be viewed as representing an infinitely-deep network. The authors show that the monDEQ has Lipschtiz constants that are bounded as a simple function of the strong monotonicity parameter of the network, and derive simple-yet-tight bounds on both the input-output mapping and the weight-output maps defined by these networks, and demonstrate that they are small relative to those for comparable standard DNNs. They also show that one can use these bounds to design monOTone DEQ models, even with e.g. multiscale convolutional structure, that still have constraints on the LPschitz constant. Finally, they highlight how to use the bounds to develop PAC-Bayes generalization bounds that do not depend on any depth of the networks and which avoid the exponential depth-dependence of comparable DNN bounds."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper proposes a unified approach for goal-conditioned reinforcement learning and imitation learning. The main idea is to use recent advances in density estimation to effectively learn to reach a given state. In particular, the paper proposes to use normalizing flows to estimate the likelihood from roll-outs and thereby obtain an estimate of the value function. The paper shows that the proposed approach can circumvent the problem of sparse rewards while addressing hindsight bias in stochastic domains. In imitation learning, the approach can learn from extremely sparse amounts of expert data and achieves state-of-the-art results on a common benchmark."
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"This paper proposes a variational multi-task learning (VMTL) method for multi-tasks learning. The proposed method is based on the variational Bayesian inference framework, where the task-relatedness is explored in a principled way by specifying priors. The authors introduce Gumbel-softmax priors to condition the prior of each task on related tasks. Experiments on four benchmark datasets demonstrate the effectiveness of the proposed method."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. The authors systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers,. Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on the newly proposed benchmark suite."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper proposes a code summarization model that combines the representation learning on both the abstract syntax tree (AST) and the source code (context) of the program. The main idea is to combine the representation of the AST and the context in a way that the AST features are language-agnostic, i.e., can be easily computed for any programming language. The proposed model is evaluated on monolingual and multilingual versions of five programming languages. The results show that the proposed model outperforms the existing methods on all five languages."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper proposes a reinforcement learning approach for audio-visual navigation, where the agent learns to navigate through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. This paper proposes to learn to set audio and visual waypoints that are dynamically set and learned end-to-end within the navigation policy, and an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. The authors demonstrate their approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper studies the lottery ticket hypothesis, which suggests that neural networks may rely on lucky random initial weights of subnetworks called “lottery tickets” that converge quickly to a solution (Frankle & Carbin, 2018). To investigate how weight initializations affect performance, the authors train small convolutional networks that are trained to predict n steps of the two-dimensional cellular automaton Conway’s Game of Life, the update rules of which can be implemented efficiently in a small CNN. They find that networks of this architecture trained on this task rarely converge. Rather, networks require substantially more parameters to consistently converge. Furthermore, they find that the initialization parameters that gradient descent converges to the solution are sensitive to small perturbations, such as a single sign change. Finally, they observe a critical value d0 such that training minimal networks with examples in which cells are alive with probability d0 dramatically increases the chance of convergence."
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper proposes a method for semi-supervised learning (SSL) that considers not only the perturbed inputs but also the similarity among the inputs having the same label. The authors propose a new objective function, dubbed BatchMean Triplet loss, which has the advantage of computational efficiency while taking into account all input samples. The proposed method achieves state-of-the-art performance across many standard SSL benchmarks with a variety of labeled data amounts."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper studies the problem of online incremental learning, where the goal is to learn a sequence of tasks, and datapoints from each task are received sequentially. At any time, at any point in time, the model may have access to a few, many, or many data points for a task. The goal of this paper is to design an algorithm that can generalize with variable shots: As data from new tasks is incrementally introduced incrementally at any time in the time horizon, the proposed algorithm can achieve variable-shot adaptation while minimizing the total amount of meta-training and adaptation. The proposed algorithm is a combination of several existing meta-learning algorithms, and is able to handle the variable shot settings that naturally arise in sequential learning: from many-shot training at the start, to zero-shot learning towards the end. The empirical results show that the proposed method can solve the full task set with fewer overall labels and achieves greater cumulative performance than standard supervised methods."
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper proposes a series of probes to analyze the representations of pretrained Transformer-based contextual representations. The probes are designed to test the sensitivity of Transformer representations to several kinds of syntactic structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. The authors experiment with three different perturbations: (1) random permutations of n-grams of varying width, (2) swapping of two spans which do or do not form a syntactic phrase, (3) swapping two adjacent words which do/do not break apart syntactic phrases. Results from the probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process."
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper studies the few-shot image synthesis task for GAN with minimum computing cost. The authors propose a light-weight GAN structure that gains superior quality on 1024x1024 resolution. The model converges from scratch with just a few hours of training on a single GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute the work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder."
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper proposes a specialised dual solver for neural network bounding. The main contribution is a unified dual treatment that includes a tighter formulation of the LP relaxation (Ehlers, 2017) and the tighter formulation (Dvijotham et al., 2018; Bunel et. al., 2020) and a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. The proposed method shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time."
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper proposes a method for augmenting pretrained language models (PTLM) with concept-centric commonsense knowledge. The authors propose two kinds of self-supervised pre-training objectives: (1) concept-to-concept (C2S) and (2) concept order (COR). C2S encourages the pretrained model to compose (write"") sentences given a set of concepts and expects the generated sentences to be fluent and plausible in terms of commonsense. COR aims to teach models to detect and revise a sentence with incorrect ordering of concepts. In addition, the authors propose a joint pretraining framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that the proposed method, concept-aware language model (CALM) can pack more commonsens knowledge into the parameters of a pre-trained text-to text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper proposes a method for unsupervised segmentation and object discovery of 3D physical objects from videos. The proposed method is based on the idea that coherent object motion constrains expectations about future object states, and that foveation patterns allow people to scan both small or far-away and large or close-up objects in the same scene. The method uses both multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes, and infer properties of those objects. The model reliably segments objects on both synthetic and real scenes. The discovered object properties can be used to reason about physical events."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes a method to improve the robustness of deep neural networks (DNNs) against adversarial attacks. Specifically, the proposed method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The proposed method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box attacks, and the results show that the method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data."
SP:276ffd59fbf49e3ee02756da8920218102214917,"This paper proposes a model agnostic knowledge distillation method, ProKT, by projecting the supervision signals of a teacher model into the student’s parameter space. The projection is implemented by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that the proposed ProKT consistently achieves the state-of-the-art performance."
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a channel pruning method for compression and acceleration of convolutional neural networks. The proposed method is inspired by hypernet, where the hypernet is used to generate the architecture of the main network, and the hyperstructure network can be optimized by regular backpropagation. Moreover, the authors use a regularization term to specify the computational resource of the compact network. Extensive experimental results on CIFAR-10 and ImageNet show that the method is competitive with state-of-the-art methods."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper proposes a method for theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. The authors augment the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. Their experiments show that their theorem prover trained with this exploration mechanism but no human proofs, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs and approaches the performance of a model trained by a combination of imitation and reinforcement learning."
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes an automated data augmentation approach called MODALS (Modalityagnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentations to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, the authors demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities for various tasks."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper studies the global convergence of three-layer neural networks in the mean field regime. The main contribution is to prove a global convergence result for unregularized feedforward networks with arbitrary width and depth, under suitable regularity and convergence mode assumptions. In particular, the authors propose a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove global convergence guarantees, which do not rely on convexity."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes a method for learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “what if” outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these costbenefit tradeoffs associated with the expert’s actions, the authors integrate counterfactual reasoning into batch inverse reinforcement learning (IRL) to learn these reward functions and explain expert behavior. The proposed method is able to handle the off-policy nature of policy evaluation in the batch setting, and can accommodate settings where the expert policies depend on histories of observations rather than just current states. Experiments in both real and simulated medical environments demonstrate the effectiveness of the proposed method."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,This paper investigates the effect of using morphological information in graph neural networks (GNNs) for multi-task reinforcement learning (MTRL) in the presence of incompatible state-action spaces. The authors show that existing GNN-based MTRL methods do not benefit from the information encoded in the graph structure. They also propose a new method based on transformers (Vaswani et al. 2017) instead of GNNs. The proposed method AMORPHEUS is motivated by the hypothesis that any benefit GNN can extract from the morphological domain knowledge is outweighed by the difficulty of message passing.
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper proposes a method for visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, the authors propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. The proposed method is called MoVie, short for Modulated conVolutional bottlenecks. The authors show that MoVIE can improve the state-of-the-art on counting-specific VQA tasks while being more efficient."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,This paper proposes a model-targeted poisoning attack that can target a desired model based on online convex optimization. The attack comes with provable convergence to any achievable target classifier. The distance from the induced classifier to the target model is inversely proportional to the square root of the number of poisoning points. The authors also provide a lower bound on the minimum number of points needed to achieve a given target classifiers.
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper proposes a new binarization method for efficient deep learning on point clouds. The authors claim that the performance drop of binarized models for point cloud applications mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. To tackle these challenges, the authors introduce Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that the proposed method outperforms the existing methods by convincing margins, at the level even comparable with the full precision counterpart."
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper proposes and studies ways to improve the performance of Transformer-based NLP models by augmenting them with external memory. Specifically, the authors propose to add memory tokens at the beginning of the input sequence and train the model to see if it is able to use them as universal memory storage. They also propose to create memory bottleneck for the global information and control the memory update with a dedicated Transformer layer. Experiments on GLUE benchmark show that the proposed method improves the performance."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes a method for unsupervised representation learning that bridges contrastive learning with clustering. Specifically, the authors introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework, and iteratively perform E-step to find the distribution of prototypes via clustering and M-step as optimizing the network via contrastive loss. The authors propose ProtoNCE, a generalized version of the InfoNCE loss, which encourages representations to be closer to their assigned prototypes. The proposed method outperforms state-of-the-art methods on multiple benchmarks."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"This paper proposes a method to improve the robustness of deep neural networks (DNNs) against adversarial attacks. Specifically, the authors propose a block containing multiple paths to learn robust features and the parameters of these paths are required to be orthogonal with each other. The so-called Orthogonal Multi-Path (OMP) block could be posed in any layer of a neural network. Via forward learning and backward correction, one OMP block makes the neural networks learn features that are appropriate for all the paths and hence are expected to be robust. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a self-supervised graph attention network (SuperGAT) for noisy graphs. Specifically, the authors exploit two attention forms compatible with a self supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. Experiments on 17 real-world datasets demonstrate that the recipe generalizes across 15 datasets of them."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a new method for DSMAD (dialog system for medical automatic diagnosis) based on reinforcement learning. The method consists of two modules: an inquiry module and an introspective module. The inquiry module is responsible for selecting the most valuable symptom to be inquired about, while the introspection module decides whether to inform the symptom or inform the disease. The authors also propose two evaluation metrics to validate the reliability and robustness of DSMAD methods. Extensive experimental results demonstrate that INS-DS achieves the new state-of-the-art under various experimental settings."
SP:10ae09d90d465125433a9b4f15b1405ab017920d,"This paper proposes a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. The authors also extend the existing confusion energy-based framework to account for long-tail scenario, which can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance."
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"This paper proposes a Bayesian approach to inverse reinforcement learning (IRL) by learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to the latent reward. The proposed method is evaluated on real medical data and classic control simulations, where it is shown that it is now possible to achieve Bayesian reward inference in such settings."
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper proposes a method for policy improvement in partially observable Markov Decision Processes (POMDPs). The proposed method, called Learned Belief Search (LBS), uses an approximate auto-regressive counterfactual belief that is learned as a supervised task. LBS uses a novel public-private model architecture for underlying policies in order to efficiently evaluate these policies during rollouts. In the benchmark domain of Hanabi, LBS obtains more than 60% of the benefit of exact search while reducing compute requirements by 35x."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper proposes a new algorithm, called shoot tree search (STS), which is an extension of Monte Carlo Tree Search (MCTS) and Random Shooting (RS). The authors claim that STS is able to control the bias-variance trade-off between depth and breadth of the search. The proposed method is evaluated on the Sokoban and Google Research Football environments."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper proposes a method for pre-training Transformer-based models on mathematical reasoning tasks. The authors design three synthetic tasks that are intended to require the model to have deduction, induction, and abduction abilities. They design these synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. They show that models trained with LIME significantly outperform vanilla transformers on three very different large mathematical reasoning benchmarks."
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper analyzes the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. The analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate learning rate. Experimental results on simple data sets and architectures support our claim on sparse EWN solutions, even with SGD."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,This paper proposes a method to tackle the mode collapse problem in GANs by adding additional discriminators to prevent catastrophic forgetting in the discriminator. The proposed method is based on the observation that catastrophic forgetting occurs when the generator fails to maintain classification accuracy on previously seen samples. The authors propose a training procedure that dynamically spawns additional discriminator to remember previous modes of generation. Experiments show that the proposed method can be plugged into existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation. 
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes an approach to regularize BERT by pruning attention heads based on a proxy score for head importance. The proposed approach leverages reinforcement learning to automatically prune attention heads from BERT. Instead of relying on heuristics or rule-based policies, the proposed approach learns a pruning policy that determines which attention heads should or should not be pruned for regularization. Experimental results show that AUBER outperforms existing pruning methods by achieving up to 9.39% better accuracy."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"This paper proposes a method for learning correspondence between two domains (i.e., simulation and real-world) by learning to translate between them with unpaired and randomly collected data from the two domains. The proposed method is based on dynamics cycles that align dynamic robot behavior across two domains using a cycle-consistency constraint. Once this correspondence is found, the proposed method can directly transfer the policy trained on one domain to the other without any additional fine-tuning on the second domain. The method is evaluated on a variety of problem domains, both in simulation and on real robot."
SP:006434d56992836ab9420d7d4215bc70664de304,"This paper focuses on the problem of model-agnostic Shapley explainability, i.e., the explanation of a model’s predictions by attributing the predictions to its input features in a mathematically principled and model agnostic way. The authors propose two solutions to the problem: (1) generative modeling, which provides flexible access to data imputations; (2) directly learning the Shapley value-function, providing performance and stability at the cost of flexibility. The main contributions of this paper are twofold: (i) the authors show that off-manifold explanations are often incorrect, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data. "
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"This paper proposes a method for opponent modelling in multi-agent reinforcement learning. The proposed method is based on variational autoencoders, which are trained to reconstruct the local actions and observations of the opponent based on embeddings that depend only on the local observations. The model is trained via deep reinforcement learning and does not require access to opponent observations. Experiments show that the proposed method achieves comparable performance to an ideal baseline which has full access to the opponent’s information and significantly higher returns than a baseline method."
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"This paper proposes a consistency regularization method for contrastive learning on semi-supervised learning on unlabeled data. The proposed method, called Consistent Contrast (CO2), takes the similarity of the query crop to each crop from other images as a pseudo label, and encourages consistency between these two similarities. CO2 improves Momentum Contrastive Learning (MoCo) by 2.9% top-1 accuracy on ImageNet, 3.8% and 1.1% accuracy on PASCAL VOC, and transfers to image classification, object detection, and semantic segmentation tasks."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies the last-iterate convergence of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization in bilinear games over the probability simplex. OGDA and OMWU have received a lot of attention due to their favorable convergence properties in the unconstrained setting, but the behavior of these algorithms for the constrained setting is still not fully understood. In this paper, the authors show that when the equilibrium is unique, OGDA converges exponentially fast with a learning rate whose value is set to a universal constant, improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. The authors also extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition on the smoothness of the objective function."
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes a federated learning framework for training User Verification (UV) models in the federated setup, where each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. The proposed method, called FedUV, allows users to jointly learn a set of vectors and maximize the correlation of their instance embedding with a secret user-defined linear combination of those vectors. The authors show that choosing the linear combinations from the codewords of an error-correcting code allows users and the server to collaboratively train the model without revealing their embedding vectors. "
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper studies the geometry of class manifolds (CMs) of deep neural network classifiers. The authors propose a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. They provide a theory for the technique and verify that their theoretical predictions agree with measurements on real neural networks. Through extensive experiments, they leverage this method to show deep connections between the geometry, generalization, and robustness. They show that well-performing, robust models have higher dimensional CMs than worse performing models."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper proposes a novel approach to the exploration-exploitation trade-off in deep reinforcement learning (RL). The authors argue that the agent should explore more in an unfamiliar state, while less in a familiar state, so as to understand the environment more efficiently. To this end, they propose Curiosity-Aware entropy Temperature for SAC (CAT-SAC), which utilizes the curiosity mechanism in developing an instance-level entropy temperature. The curiosity is added to the target entropy to increase the entropy temperature for unfamiliar states and decrease the entropy for familiar states. By tuning the entropy specifically and adaptively, the proposed method can encourage the agent to explore when its curiosity is large, otherwise, it is encouraged to exploit."
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta-reinforcement learning (meta-RL) algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. The proposed method is based on a simple insight: dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. The dynamics models are then used to train policies for out of distribution tasks without using meta-RL at all, by generating synthetic experience for the new tasks. "
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper proposes a method to address the problem of meta-overfitting in few-shot learning (FSL) by treating the problem as a gradient noise problem. Specifically, the authors propose Eigen-Reptile (ER) that updates the meta-parameters with the main direction of historical taskspecific parameters to alleviate gradient noise. The main direction is computed by a special mechanism for the parameter’s large size. The authors also propose Introspective Self-paced Learning (ISPL) that constructs a plurality of prior models to determine which sample should be abandoned. The effectiveness of ER and ISPL are proved theoretically and experimentally."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper proposes Adversarial Batch Normalization (AdvBN) to improve the robustness of deep neural networks to distributional shifts. AdvBN consists of two layers: one that normalizes the mean and variance of the features, and the other one that re-normalizes the features with the most damaging mean/variance. The authors propose to use adversarial training to fine-tune the feature normalization layers. Experiments on ImageNet-C, Stylized-ImageNet, and ImageNetInstagram show that AdvBN improves the performance of ResNet-50 on these tasks."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"This paper proposes a metric called Variance of Gradients (VoG) as a proxy metric for detecting outliers in the data distribution. The authors claim that it is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Experiments are conducted on Cifar-10, Krizhevsky et al. (2009) and ImageNet."
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"This paper proposes a method to improve the sample quality of deep generative models (DGMs) by refining inferior samples using the gradient flow of entropy-regularized f-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. Compared to existing works that focus on specific GAN variants, this paper shows that the proposed method can be applied to GANs with vector-valued critics and can be used in combination with VAEs and Normalizing Flows."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents a variable encoder-decoder (VECO) pretraining approach to unify the two mainstreams in both model architectures and pre-training tasks. VECO splits the standard Transformer block into several sub-modules trained with both inner-sequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub- modules for understanding and generation tasks during inference. The proposed approach achieves new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval."
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper proposes an intrinsic reward for deep reinforcement learning (RL) based on the prediction error of auditory events. Specifically, the authors propose to use the prediction errors as intrinsic rewards to encourage the agent to understand the causal effect of its actions through auditory event prediction. The main idea is that sounds are often more directly or easily observable causal effects of actions and interactions. The authors claim that sounds that result from object interactions also allow to estimate underlying causally relevant variables, such as material properties, which can be critical for planning actions."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper studies the problem of novel category discovery on single-and-multi-modal data with labels from different but relevant categories. The authors present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, the authors extend the conventional contrastive learning by self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabeled data. In particular, the proposed method uses category discrimination on labelled data and cross-modality discrimination on multimodal data to augment instance discrimination. The proposed method also employs the WTA hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelling data to better predict cluster assignments."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"This paper proposes a method for weakly supervised semantic segmentation. The authors formulate the task as a semi-supervised metric learning problem, where pixels of the same semantics need to be mapped to the same (distinctive) features. They propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity, which act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. "
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper proposes a simple but effective distillation strategy for unsupervised learning. The method, termed as BINGO, targets at transferring the relationship learned by the teacher to the student. The goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. The proposed method achieves new state-of-the-art performance on small scale models."
SP:328866aad6544c81ded8980934df31dc4472435f,This paper proposes an adversarial approach to simulation-based inference (SBI) for high-dimensional data. The authors reformulate the variational objective in the adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations and supports implicit priors. The proposed approach is evaluated on two SBI benchmark problems and on two high-dimensions simulators.
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"This paper studies the identification and estimation of treatment effects (TEs) under limited overlap, i.e., when subjects with certain features belong to a single treatment group. The authors use a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs, and build a generative prognostic model. The model is then learned as a new type of variational autoencoder (VAE) and the TE error bounds are derived. The proposed method is compared with recent methods using (semi-)synthetic datasets."
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. They introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. They show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,This paper investigates the role of GNN-based modules in question answering (QA) tasks. The authors analyze the state-of-the-art GNN modules for QA and analyze their reasoning capability. They show that existing knowledge-aware GNNs may only carry out simple reasoning such as counting. They then design a simple yet effective graph-based neural counter that achieves better performance on CommonsenseQA and OpenBookQA.
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a three-stage compression framework to enable DNN inference with near-optimal compression and much better performance during inference runtime. The key insight of the proposed method is the concept of Succinct Data Structures, which supports fast queries directly on compressed representation without decompression. The proposed method first transforms DNN models as our proposed formulations in either Element-wise or Block-wise manner, so that the compressed representation can take advantage of the succinct data structures. Then, the method compresses transformed DNNs using the proposed methods. Finally, our method exploits our specialized execution pipelines for different model formulations, to retrieve relevant data for inference. The experimental results show that, the proposed approach keeps near-optimality compression and achieves at least 8.7X/11.5X speedup on AlexNet/VGG-16 inference, compared with Huffman Coding."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes a new training method for improving the performance of tiny neural networks. The authors argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a generative adversarial network (GAN) for dynamic point cloud sequences without requiring point correspondence annotations. The proposed method, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, a learnable masking module is proposed to adapt upsampling ratio according to the point distribution. Extensive experiments on point cloud data from two different domains: particles in the fluid dynamical system and human action scanned data show the effectiveness of the proposed method."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,"This paper proposes a method for fully pre-training an encoder-only transformer and fine-tuning it for object detection via a task adapter. The proposed method is inspired by the success of textual prompts in NLP, which treats query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. Experiments on the challenging COCO dataset demonstrate that the proposed method achieves competitive performance."
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper proposes a new federated learning algorithm, FedPAGE, which further reduces the communication complexity by utilizing the recent optimal optimal PAGE method (Li et al., 2021). They show that FedPAAGE uses much fewer communication rounds than previous local methods for both federated convex and nonconvex optimization. In the convex setting, the number of communication rounds is O(3/4 S), improving the best-known result O(N S ) of SCAFFOLD (Karimireddy et al. 2020) by a factor of N, where N is the total number of clients, S is the sampled subset of clients in each communication round, and is the target error. "
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper proposes a method to characterize the decision boundary geometry of ANN classifiers by utilizing adversarial perturbations. The authors define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundaries from any given input sample, and measure the distance to the boundary in a large subspace, where the distance grows smoothly and sub-linearly as one increases the dimensionality of the subspace. The geometry of the boundary is characterized in terms of curvature, its shape, and its proximity to the input. The paper provides new insight into the consequences of adversarial training by quantifying the increase in boundary distance within adversarial subsets."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"This paper presents a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information, and the second stage learns similar representations within the same cluster and dissimilar representations for data from different clusters. The proposed method is evaluated on three datasets: UT-K, CUB-200-2011, ImageNet-100, and Wider Attribute. The results show that the proposed method outperforms conventional self-supervision methods."
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper studies the problem of estimating sparse parameters from observational data. The authors propose a learning-based algorithm called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm) to learn algorithms automatically from data. PLISA is designed by unrolling a path-following algorithm, with some components being more flexible and learnable. With this structure, the authors theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, they analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a method to learn a compact and decodable latent representation space for the original hybrid action space. Specifically, it constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variational Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space via a decoder. The method is evaluated in a variety of environments with discrete-continuous action space and the results demonstrate the superiority of the proposed method."
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper proposes SGEM, Stochastic Gradient with Energy and Momentum to solve a class of general non-convex stochastic optimization problems, which is based on the AEGD method that originated in the work [AEGD: Adaptive Gradient Descent with Energy. arXiv: 2010.05109]. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. The authors show that SGEM features an unconditional energy stability property, and derive energy-dependent convergence rates in the general non Convex setting, as well as a regret bound in the online convex setting. A lower threshold for the energy variable is also provided. The experimental results show that the proposed method converges faster than the vanilla SGD and generalizes better than SGDM in training neural networks."
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a Conditional Masked Language Model with Correction (CMLMC) to improve the performance of non-autoregressive (NAR) Transformer-based machine translation (MT) models. The authors identify two shortcomings of CMLM, i.e., the indistinguishability of tokens, and the mismatch between training and inference procedures, and propose the CMLMC to address these problems. The proposed method achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets."
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper proposes to use spiking neural networks (SNN) to perform local signal processing for keyword-spotting. The proposed SNN is based on the WaveNet architecture and uses simple neural dynamics, fixed time-constants and a simple feed-forward architecture. The authors claim that the proposed method is well-suited for neuromorphic implementation and achieves state-of-the-art performance on several datasets. "
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper proposes a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. The proposed algorithm, Shifty, allows the user to provide (one or multiple simultaneous) fairness definitions from a large class appropriate to the application domain, such as demographic parity, disparate impact, equalized odds, predictive equality, and individual fairness. The authors evaluate Shifty on a real-world dataset of university entrance exams of students’ college entrance exam scores and their subsequent grade point average (GPA) in the Brazilian university system."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes a meta-learning approach for multi-stage stochastic optimization (MSSO) that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming (ν-SDDP) continually self-improves by solving successive problems. An empirical investigation demonstrates that the proposed approach can significantly reduce the problem solving cost without sacrificing solution quality across a range of synthetic and real-world process optimization problems."
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a protocol for private next-token prediction to prevent privacy violations by language models that were fine-tuned on a private corpus after pre-training on a public corpus. The authors show that the proposed protocol, called SUBMIX, limits the leakage of information that is unique to any individual user in the private corpus via a relaxation of group differentially private prediction. Importantly, the proposed method admits a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing data-extraction attacks while maintaining the utility of the language model."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate with respect to a classification model’s intermediate activations on indistribution samples. The authors leverage a recent insight about label smoothing, which they call the Label Smoothed Embedding Hypothesis, and show that one of the implications is that the k-nn density estimator performs better as an OOD detection method both theoretically and empirically when the model is trained with labels smoothing. They show that their proposal outperforms many OOD baselines and also provide new finite-sample high-probability statistical results for their method."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,This paper proposes a new approach to learn representations for semi-supervised image classification by augmenting the denoising score matching framework of diffusion-based generative models with a new formulation of the score-matching objective. The authors show that the proposed approach is able to learn an infinite-dimensional latent code that achieves improvements of state-of-the-art models on semi-Supervised Image Classification tasks. The paper also shows how adversarial training can improve sample quality and sampling speed using a new approximation of the prior at smaller noise scales.
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"This paper proposes a method for goal-conditioned reinforcement learning, where the goal is to learn a policy that can reach distant goals. The proposed method, C-Planning, is based on expectation maximization (EM) and graph-planning (GP). The main idea is to generate a curriculum of waypoints at training time, which are then used to guide the policy to reach the goal. The method is evaluated on long-horizon manipulation and navigation tasks, where it is shown that it is able to outperform prior methods."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper proposes a k-mixup regularization method for training deep neural networks. Standard mixup is a data augmentation approach that trains models on weighted averages of random pairs of training points. In this paper, the authors propose to perturb the training data in the direction of other k-batches using displacement interpolation, i.e. interpolation under the Wasserstein metric. The authors demonstrate theoretically and in simulations that k-Mixup preserves cluster and manifold structures, and extend theory studying the efficacy of standard mixup to the k=1 case. The empirical results show that training with k- mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,This paper proposes a nonlinear kernelized classification layer for deep neural networks. The classification layer finds an optimal nonlinear classifier for the embeddings by mapping them into a Reproducing Kernel Hilbert Hilbert Space (RKHS) that optimally separates them into different classes. The authors theoretically show that the proposed classification layer optimizes over all possible radial kernel functions on the space of embedding to learn an optimal classifier. Experiments on image classification and natural language processing tasks demonstrate the effectiveness of the proposed method.
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"This paper investigates the sources of bias in node representations obtained via Graph Neural Networks (GNNs). The analysis reveals that both nodal features and graph structure lead to bias in the obtained representations. Based on the analysis, fairness-aware data augmentation frameworks are developed to reduce the intrinsic bias. Experiments on node classification and link prediction are carried out over real networks in the context of graph contrastive learning."
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating treatment effects from observational data in the presence of unmeasured confounders. The authors propose a Confounder Balanced IV Regression (CB-IV) algorithm to jointly remove the bias from the unme measured confounder with IV regression and achieve better bias-variance trade-off in imbalanced treatment distributions due to the observed confoundsers by balancing for treatment effect estimation. CB-IV algorithm consists of three main modules: (1) treatment regression: regressing the treatment with IVs, (2) confounding balance: learning a balanced representation of confoundering variables, and (3) outcome regression. Extensive experiments demonstrate that the proposed algorithm outperforms the state-of-the-art methods, including IV regression methods, for treatment effects estimation."
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper analyzes the effect of MAML on few-shot learning in the linear regression setting, where the hardness of the tasks is related to the rate at which gradient descent converges on the task. The authors show that MAMl achieves significant gain over non-adaptive learning (NAL) in terms of the fast adaptability of their solutions in various scenarios. They also show that the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions, and also give numerical and analytical results suggesting that these insights apply to two-layer neural networks."
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes a method for sparse blind source separation (BSS) based on unrolling of the Proximal Alternating Linearized Minimization (PALM) algorithm. The proposed method leverages the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameters and variables. In particular, the proposed LPALM algorithm is able to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. The authors demonstrate the algorithm's effectiveness in astrophysical multispectral imaging."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper proposes a transformer-based model for language modeling with Legendre Memory Unit (LMU). The proposed model is based on the nonparametric Linear Time-Invariant (LTI) component of the LMU, which projects a sliding window of the input sequence onto Legendre polynomials to provide a temporal representation and compression of input signal. The authors introduce a novel attention module called implicit self-attention and construct a Legendre memory unit based model hat exhibits an O(n) and $O(n lnn)$ dependency for memory and computation respectively. The experiments show that for the same amount of training the proposed model improves the loss over transformers about as much as transformers improve over LSTMs."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"This paper proposes a GAN-based approach to disentangle the image into spatial and appearance factors without domain knowledge and supervision signals. The proposed approach, LatentKeypointGAN, is trained end-to-end on the classical GAN objective with internal conditioning on a set of space keypoints. These keypoints have associated appearance embeddings that respectively control the position and style of the generated objects and their generated parts. The authors demonstrate that the proposed approach provides an interpretable latent space that can be used to re-arrange the generated images by re-positioning and exchanging keypoint embedding, such as generating portraits by combining the eyes, and mouth from different images."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper studies the properties of deep fully-connected neural networks with layer normalization using the mean field formalism, and carry out a non-perturbative analysis of signal propagation. The authors demonstrate that increasing the depth leads to gradient explosion or representation shrinkage. They also show that many popular normalization techniques fail to mitigate these problems. Their method can also be applied to residual networks to guide the choice of initialization variances."
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper proposes a line search method for finding the optimal step sizes for stochastic gradient descent (SGD) in deep learning. The paper builds on recent empirical findings that the full-batch loss behaves locally parabolically in the direction of noisy update step directions and that the trend of the optimal update step size changes slowly. Based on these findings, the paper introduces a line-search method that approximates the full batch loss with a parabola estimated over several mini-batches. Learning rates are derived from such parabolas during training. The proposed method outperforms SGD tuned with a piece-wise constant learning rate schedule and other line search approaches for Deep Learning across models, datasets, and batch sizes on validation and test accuracy."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper provides a theoretical analysis of noise contrastive estimation (NCE) in the setting where the target and noise distributions belong to an exponential family. The authors show that the landscape of NCE can become extremely flat when the noise distribution is poorly chosen. They show that standard first order and second order optimization methods, such as Newton’s method, are not suitable for the NCE problem. To address this issue, the authors propose an eNCE, which replaces the log loss in NCE with an exponential loss and for which normalized gradient descent (NGD) can be used. "
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper studies the problem of combining differential privacy and Byzantine resilience in distributed machine learning. The authors show that the integration of standard practices in DP and BR is not straightforward and that many existing results on the convergence of distributed SGD under Byzantine faults, especially those relying on (alpha, f)-Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, the authors revisit the theory of (α,f)-BR to obtain an approximate convergence guarantee. The analysis provides key insights on how to improve this guarantee through hyperparameter optimization."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper considers the problem of code editing with few exemplars. The authors propose a method that combines edit representations extracted from support exemplars and compositionally generalizes them to the query code snippet editing via multi-extent similarities ensemble. Specifically, they parse the support and query code snippets using language-specific grammar into abstract syntax trees. They apply the similarities measurement in multiple extents from individual nodes to collective tree representations for query and support sample matching, and ensemble the matching results through a similarity-ranking error estimator. They evaluate the proposed method on C# and Python datasets and show up to 8.6% absolute accuracy improvements compared to non-composition baselines."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a neurosymbolic generative model for generating high-level structure in the form of relational constraints between different subcomponents of an example (e.g., lines of a poem or measures of music). The authors propose a program synthesis algorithm that infers the relational constraints present in the training data, and then learn a generative models based on the resulting constraint data. They show that the proposed approach significantly improves over state-of-the-art in terms of capturing high level structures in the data, while performing comparably or better in the low-level structures."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper studies the problem of set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. This paper addresses two common scaling problems encountered in such tasks: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. The authors propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. They also introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Finally, they combine both contributions in a single model that enables to address problems with larger input set sizes."
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a post-processing method to mitigate bias of state-of-the-art models. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre-trained model to give more representation power to the discriminated subgroups. The training is supervised by the von Mises-Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper proposes a method for class-incremental learning (CIL) based on knowledge distillation (KD). The authors propose to compute the KD loss using placebo data chosen from a free image stream (e.g., Google Images), which is both simple and surprisingly effective even when there is no class overlap between the placebos and the old data. When the image stream is available, the authors use an evaluation function to quickly judge the quality of candidate images (good or bad placebos) and collect good ones. For training this function, they sample pseudo CIL tasks from the data in the 0-th phase and design a reinforcement learning algorithm. Their method does not require any additional supervision or memory budget, and can significantly improve a number of top-performing CIL methods."
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes a path auxiliary algorithm for sampling from discrete energy-based models (EBMs). The main idea is to use a composition of local moves to efficiently explore large neighborhoods. The authors also give a fast version of their algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, they show that their path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper proposes Variational Predictive Routing (VPR), an event-based hierarchical generative model that is able to dynamically adapt its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model’s latent hierarchy. VPR uses an event detection mechanism that relies solely on the system's latent representations (without the need of a separate model), and it is shown that VPR can detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate timeagnostic rollouts of the future. The approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,This paper proposes a method for image retrieval that combines global and local features. The proposed method is an end-to-end and single-stage pipeline. The authors claim that it accelerates extraction speed and reduces memory consumption by removing the re-ranking process and learning local feature matching with convolutional neural networks instead of RANSAC algorithm. Experiments on the Revisited Oxford and Paris datasets validate the effectiveness of the proposed method.
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper proposes RotoGrad, an algorithm that tackles the problem of negative transfer in multitask learning (MTL) by homogenizing gradient magnitudes and directions, while ensuring training convergence. The proposed method re-weights task gradients at each step of the learning process, and smoothly rotates the shared feature space differently for each task, seamlessly aligning gradients in the long run. Experiments on multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset demonstrate the effectiveness of the proposed method."
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"This paper proposes a model fusion framework for fusing heterogeneous neural networks by solving a cross-layer alignment problem, which is an unbalanced assignment problem. The proposed method balances the number of layers of neural networks before applying layer-wise model fusion. The experiments show that the fused network from the proposed framework achieves a more favorable performance compared to the individual networks trained on heterogeneous data without the need for any retraining. "
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper analyzes the implicit regularization effect of SGD in deep reinforcement learning (RL) and shows that it can lead to poor generalization and degenerate feature representations in the offline RL setting. Theoretical analysis shows that the feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. Inspired by this, the authors propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. Empirically, it is shown that DR3 improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes HyperDQN, an extension of randomized least-square value iteration (RLSVI) to deep reinforcement learning (RL). The authors propose to use a probabilistic hypermodel (i.e., meta model) to generate approximate posterior samples of the Q-value function, which are then used to select the optimal action sequences. The proposed method is evaluated on the Atari suite, and it is shown to outperform DQN and other randomized exploration methods."
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper proposes to learn causal representation from observational data by regularizing the learning procedure with mutual information measures according to our hypothetical causal graph. The optimization involves a counterfactual loss, based on which the authors deduce a theoretical guarantee that the causality-inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by our approach is robust under adversarial attacks and distribution shift."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,This paper proposes a progressive training framework for efficient and effective federated learning. The proposed ProgFed reduces the computation and two-way communication costs while maintaining the strong performance of the final models. Theoretical analysis shows that the proposed method converges at the same asymptotic rate as standard training on full models. Extensive results on a broad range of architectures and diverse tasks from simple classification to medical image segmentation show that the highly effective training approach saves up to 20% computation and up to 63% communication costs for converged models.
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the generalization of adversarial training through the lens of the adversarial Rademacher complexity of deep neural networks. The main contribution of this paper is to provide an upper bound of the robust generalization error of adversarially trained neural networks, which includes the product of the norm of the weights and the perturbation intensity. Experiments on MNIST and CIFAR-10 show that the generalized adversarial weight norms are larger than the standard trained weight norms, thus providing an explanation for the bad generalization performance. "
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a kernel-based differential entropy estimator, called Kernel Differential Entropy Estimation (KNIFE), which is a differentiable kernel based estimator of differential entropy (DE) and mutual information (MI). The proposed estimator is able to be applied to both discrete and continuous-time data. The proposed method is evaluated on three tasks: visual domain adaptation, textual fair classification, and textual fine-tuning. "
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"This paper proposes a new soft-greedy operator, called resmax, which takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value. The authors prove that resmax is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a nonexpansion (called mellowmax). The authors empirically validate the effectiveness of resmax in tabular and deep RL."
SP:792ae8808aa6902758146aef1548c975492b833c,This paper proposes a method to control the model’s learnability on a specific dataset with a special key. The proposed method leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. The authors empirically demonstrate the success and practicability of the method on visual classification tasks.
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper proposes a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which the authors call Feature Propagation. The proposed approach outperforms state-of-the-art methods on six standard node-classification benchmarks and can withstand surprisingly high rates of missing features."
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"This paper proposes a method for selecting a subset of the unlabeled data to label for active learning. The main idea is to minimize the discrete Wasserstein distance between the set to be labeled and the set of data to be labelled. The paper proves that this distance bounds the difference between training with a finite versus an unlimited labeling budget. The optimization problem admits a Generalized Benders Decomposition solution algorithm where the optimization problem is instead solve sub-problems that are orders of magnitude smaller (Geoffrion, 1972). The algorithm guarantees convergence to a globally optimal solution and can be further accelerated with customized constraints."
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a method for de novo genome assembly based on geometric deep learning. The proposed method is based on a graph convolutional network that is trained on a dataset generated from human genomic data to reconstruct the genome by finding a path through the assembly graph. The authors show that their model can compute scores from the lengths of the overlaps between the sequences and the graph topology which, when traversed with a greedy search algorithm, outperforms the greedy search over the overlap lengths only. Moreover, the method reconstructs the correct path in the fraction of time required for the state-of-the-art genome assemblers."
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a method to improve the continual learning representations and remove inconsistency in the use of experience replay (ER) between meta-training and meta-testing. The authors propose to store the samples’ representations, instead of the samples themselves, into the replay buffer. Moreover, they introduce a meta-learned Predictive Sample Selection to replace the widely used reservoir sampling to populate the replay buffers. Experimental results on a number of real-world meta-continual learning benchmark data sets demonstrate that the proposed method outperforms the state of the art."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper proposes a method for IGM-based multi-agent joint Q-learning based on centralized training with decentralized execution (CTDE). The authors formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q-values to explicitly maximize the Joint Q-value, besides guaranteeing the Bellman optimality. Theoretically, the authors give a gradient ascent solution for this problem. Empirically, they instantiate the core idea with deep neural networks and propose Explicit Credit Assignment joint Q learning (ECAQ) to facilitate multi-Agent cooperation in complex problems."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper studies adversarial attacks against transductive learning-based adversarial defenses. The authors formulate and analyze threat models for these defenses and point out important subtleties. They propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating adversarial robustness. They show that GMSA, even with weak instantiations, can break existing adversarial learning based defenses, which were resilient to previous attacks such as AutoAttack."
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies the problem of batch normalization (BN) in the context of neural network training. The authors cast BN as an approximation of the limiting case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. They demonstrate an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation. They further use their insights to improve batch renormalization for very small minibatches."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper proposes a low-rank adaptation method for fine-tuning large pre-trained language models. The proposed method freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture to reduce the number of trainable parameters for downstream tasks. Compared to GPT-3 with Adam, the proposed method can reduce the total number of parameters by 10,000 times and the GPU memory requirement by 3 times. The authors also provide an empirical investigation into rank-deficiency in language model adaptation."
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a regular-constrained linear-chain conditional random field (RegCCRF) that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language L. RegCCRF has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. The authors also show that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"This paper proposes two novel neural models for camera-based physiological measurement that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. The proposed models achieve state-of-the-art accuracy on three public datasets. They further evaluate the latency of the proposed networks and show that their most light weight network also achieves a 33% improvement in efficiency."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper proposes a method to perform network pruning to reduce the latency of convolutional neural networks (CNNs). The proposed method, Hardware-Aware Latency Pruning (HALP), is motivated as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget. The authors propose a latency lookup table to track the latency reduction potential and global saliency score to gauge the accuracy drop. The method is evaluated on both classification and detection tasks on ImageNet and VOC datasets, over varying networks."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes a molecular graph generation method via energy-based models (EBMs) to perform permutation invariant and multi-objective molecule generation. The authors propose a permutation-invariant energy function and learn the energy function by contrastive divergence and generate samples by Langevin dynamics. To generate molecules with a specific desirable property, they propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. Further, they explore to use their GraphEBM for generating molecules towards multiple objectives via compositional generation, which is practically desired in drug discovery."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"This paper proposes a neural network-based approach for bottom-up program synthesis. The approach, called CROSSBEAM, uses the neural model to choose how to combine previously explored programs into new programs, taking into account the search history and partial program executions. The model is trained on-policy using data extracted from its own bottom up searches on training tasks. Experiments on string manipulation and logic programming tasks show that the approach learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art approaches."
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper proposes a method to stabilize deep reinforcement learning (DQL) by augmenting the squared Bellman error with a functional regularizer. Unlike target networks, the regularization proposed here is explicit and enables us to use up-to-date parameters as well as control the regularisation. This leads to a faster yet more stable training method. The proposed method is evaluated on a range of Atari environments and shows improved sample efficiency and performance."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"This paper proposes a method to improve the expressive power of message-passing GNNs. The main idea is to treat each node in a graph as a neighborhood subgraph, and the message passing scheme recursively aggregates the feature vectors of nodes in the neighborhood and combines the aggregated information with the feature vector of the node itself to obtain a representation. Theoretical results show that the proposed method is strictly more expressive than the Weisfeiler-Lehman test in distinguishing graph structures. Empirical results on node classification and link prediction tasks demonstrate the effectiveness of the method."
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods: (1) existing PI methods require retraining of neural networks for every given confidence level and suffer from the crossing issue in calculating multiple PIs; (2) they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI; (3) they tend to underestimate uncertainties of out of distribution (OOD) samples leading to over-confident PIs. The proposed PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence levels."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes an online meta-learning method for continual adaptation. The proposed method is based on meta-training and meta-adaptation, where the meta-network is trained on a stream of tasks, and the task boundaries are assumed to be known. The method is evaluated on the Rainbow-MNIST and CIFAR-100 datasets. The results show that the proposed method outperforms the baselines. "
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,This paper proposes a differentiable scaffolding tree (DST) to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). The experiments show that the proposed method is both effective and sample efficient.
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"This paper proposes a knowledge-augmented approach to predict patients’ response for a target lab result. The authors use a transformer encoder (Vaswani et al., 2017) to capture the patient information and a modified graph attention network (GATv)(Brody et al. 2021) to learn a strong patient representation that captures both the sequential information accumulated over the visits and information from other similar patients. Experiments on real-world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors by a significant margin."
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper studies the problem of open-set single domain generalization (OS-SDG), where only one source domain is available to train the model. The authors propose a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. It also adopts a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by the proposed method. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance for single-domain generalization."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper studies the Wasserstein policy optimization (WPO) and Sinkhorn trust region (SPO) methods for policy optimization in reinforcement learning. The main idea is to optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, the authors show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Experiments across tabular domains and robotic locomotion tasks demonstrate the performance improvement of WPO and faster convergence of SPO."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper proposes a new paradigm of learning called forget-and-relearn, which unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. The authors leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. The paper also provides a clear path towards performance improvements."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper studies the offline-online setting, where the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. The authors show that standard RL agents trained in this setting can outperform agents trained only offline or online, sometimes by a large margin."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper studies the problem of domain generalization (DG) via learning to reduce domain shift with an episodic training procedure. In particular, the authors propose a discrepancy-optimal meta-learning algorithm, which learns to optimize Y-discrepancy between the unseen target domain and source domains only using source-domain samples. Theoretical analysis shows that there is a tradeoff between classification performance and computational complexity. The theoretical results also shed light on a bilevel optimization algorithm for DG. Empirically, the proposed method is evaluated on two DG benchmarks."
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the role of policy and value networks in DNN-based best-first search on the Sokoban domain. The authors show the effectiveness of the policy network, further enhanced by the value network, as a guiding heuristic for the search. They also show the existence of left heavy tails and propose an abstract tree model that can empirically explain the appearance of these tails."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"This paper proposes a method for meta-imitation learning from videos of human demonstrations. The main idea is to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data. The approach relies only on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior. Experiments show that the method achieves the comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration."
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper proposes a new training method for over-parameterized deep networks trained using gradient-based optimizers for classification and ranking problems. The authors claim that the current state-of-the-art optimizers like Adam lag behind SGD in terms of generalization performance, mainly in the image classification domain. To address this issue, the authors propose a novel training method that combines weight decay (WD) and normal hyperparameter tuning to tune the hyperparameters of the network. The proposed method is evaluated on a variety of tasks, including image classification, language modeling, machine translation, text classification, and recommender systems."
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper proposes a method for group equivariant convolutional neural networks (G-CNNs) that is able to learn both partial and full equivariance at every layer. The authors propose to learn the probability distribution over group convolution elements at each layer in the G-CNN layers. This allows them to adjust their level of equivariancy during training. The proposed method is evaluated on MNIST and CIFAR-10 datasets, and compared with the state of the art. "
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes a new method for training deep latent variable models (DLVMs). The proposed method is based on the amortized Langevin dynamics (ALD), which replaces the datapoint-wise MCMC iterations with updates of an inference model that maps observations into latent variables, which enables scalable inference from large-scale datasets. The authors prove that ALD has the target posterior as a stationary distribution under some assumptions, and extend it to sampling from an unconditional distribution such as an energy-based model, enabling more flexible generative modeling by applying it to the prior distribution of the latent variable. Based on ALD, the authors construct a new deep latent variables model named the Langevin autoencoder (LAE), which uses ALD for posterior inference and sampling from the latent space EBM."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper proposes a method for hypergraph reasoning, i.e., predicting the relationship between several entities based on the input facts. The authors observe that in logical reasoning, logical rules (e.g., my parent’s parent is my grandparent) usually apply sparsely. Inspired by these observations, the authors propose Sparse and Local Neural Logic Machines (SpaLoc) to leverage the sparsity in hypergraph neural networks. SpaLoc represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks and finite-domain quantification operations to infer new facts. "
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper proposes a new family of differentiable top-k cross-entropy classification losses for k > 1, where k is assumed to be drawn from a probability distribution. The main idea is to relax the assumption that k is a positive integer, such as 1 or 5, and propose to draw k from a distribution over all ranks represented by a matrix P. The proposed method is evaluated on the CIFAR-100 and ImageNet-21K-P datasets, and achieves state-of-the-art results on both top-1 and top-5 accuracy."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. The proposed method builds on the celebrated Douglas-Rachford splitting technique, which tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. In addition, the authors establish a linear convergence rate for the formulation of the OT problem, and detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion."
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper proposes a framework for disentangling performance gaps in federated learning (FL) by separating performance gaps from client data (out-of-sample gap) from performance gaps (performance gaps) from unseen client distributions (participation gap). Using this framework, the authors observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in FL. The authors propose a semantic synthesis strategy that enables realistic simulation without naturally-partitioned data."
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper studies the effectiveness of prompt-based language models (PLMs) in the zero-shot setting, i.e., the setting where the task is to predict the output of a masked language model (MLM). The authors propose a simple multi-null prompting strategy (MLP), which is a combination of hand-crafted, discrete prompt, continuous prompt, and various answer engineering (e.g., selecting label words for the classification tasks) to improve the performance of BERT models. The authors conduct experiments on 20 different datasets, and show that the proposed MLP outperforms the existing approaches. "
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"This paper proposes a method to improve the performance of the attention mechanism. The proposed method is based on a sharpener module, which aims to align the relevant parts of the encoded image with the target output. The authors claim that the alignment and interpretability of attention can be significantly improved. Experiments on synthetic handwritten digits and real-world scene text recognition datasets show that the proposed method outperforms the mainstream ones. "
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper proposes a method for learning to solve the vehicle routing problem (VRP) with fixed number of vehicles. The proposed method is based on the Permutation Invariant Pooling Model (PIM), which was originally developed for the mTSP problem. The main contribution of this paper is to extend the PIM to the VRP setting and to use supervised learning to construct a complete tour plan for VRP. Experiments are conducted to show the effectiveness of the proposed method."
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a method for link prediction based on counterfactual inference. In particular, the authors propose a method to predict the existence of edges between two pairs of nodes based on the observed graph structure and a set of ""counterfactual"" nodes. The proposed method is based on a causal model that considers the information of the node pair (i.e., learned graph representations) as context, global graph structural properties as treatment, and link existence as outcome. The method is evaluated on three benchmark datasets and achieves state-of-the-art performance."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper proposes a two-stage Second-Order unsupervised Feature selection via knowledge contrastive disTillation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for unsupervisioned feature selection. The first stage learns a sparse attention matrix that can represent second order relations between features. The second stage builds a relational graph based on the learned attention matrix and perform graph segmentation for feature selection, which is different from selecting features according to weights. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state-of-the-art methods."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"This paper proposes a semi-supervised multi-modal variational autoencoder (MEME) that combines information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing, which is something that most existing approaches either cannot handle, or do so to a limited extent. The authors demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image–image) and CUB (image-text) datasets."
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes Deep Explore Options, a method for combining intrinsic and extrinsic rewards in deep reinforcement learning (RL). Intrinsic Motivation (IM) is an alternative reward signal used to spur curiosity and entice behavior exploration in RL. Explore Options (EO, Bagot et al. (2020)) have been proposed as an alternative to the wide-spread weighted-sum (WS) scheme rt = rte + betar t i + r_t i for all games in the usual expression rt=r t e + r_{t+1} + beta(t). This paper proposes to extend Explore Options to deep RL by combining intrinsic reward learning as an auxiliary task, with a resulting architecture achieving 50% faster wall-clock speed and building a stronger, shared representation. The proposed method is evaluated on hard and easy exploration games of the Atari Suite. "
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,"This paper proposes a new method for learning Hamiltonian dynamical systems from data. The proposed method is based on the stiffness-aware neural network (SANN), which is able to identify and split the training data into stiff and non-stiff portions based on a stiff-aware index to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows the authors to apply different time integration strategies such as step size adaptation to better capture the dynamic characteristics of the Hamiltonian vector fields. The authors evaluate SANN on complex physical systems including a three-body problem and billiard model and show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy."
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper proposes a method to improve the performance of Transformer-based language models on multi-step computations. In particular, the authors propose to allow the model to produce an arbitrary sequence of intermediate tokens, which they call a scratchpad, before producing the final answer. The scratchpad contains the intermediate results from a standard long addition algorithm (see Figure 2). To train the model, the intermediate steps of the algorithm are encoded as text and use standard supervised training. The authors show that scratchpads improve the ability of language models to perform multistep computations on a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"This paper proposes feature-level adversarial perturbations using deep image generators and a novel optimization objective. They show that they are versatile and use them to generate targeted feature-levels attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically-realizable. They also show that these attacks can reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of natural objects. Based on these findings, they emphasize the importance of cautious deployment for vision networks and their fortification against these types of feature-fool attacks."
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper proposes a reinforcement learning approach for simulated annealing (SA) algorithms. The main idea is to use reinforcement learning to learn the neighborhood proposal distribution and temperature schedule of the SA algorithm. The proposal distribution is learned using reinforcement learning, and the temperature schedule is learned by minimizing the KL divergence between the proposed distribution and the true proposal distribution. The proposed approach is evaluated on four problems: Rosenbrock's function, the Knapsack problem, the Bin Packing problem and the Travelling Salesperson problem. The results show that the proposed approach outperforms baselines in terms of solution quality and wall clock time."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper studies the non-stationarity problem in multi-agent reinforcement learning (MARL) and proposes a novel notion, the $\delta$-stability, which is a measure of the divergence between the joint policies of the agents. The authors propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The proposed method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper proposes a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. The proposed method achieves state-of-the-art performance on the largest public lip-reading benchmark LRS3 (433 hours) with only 30 hours of labeled data, outperforming the former state of the art approach (33.6%) trained with a thousand times more transcribed video data (Makino et al., 2019). The lip reading WER is further reduced to 26.9% when using all 433 hours of labelled data from LRS2 and combined with self-training."
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,This paper proposes a reinforcement learning algorithm for max-cut combinatorial optimization problems. The main idea is to use a single GNN to pre-process the max cut problem and then use a recurrent unit to explore the solution space in an exploratory phase. Theoretical and empirical results show that the proposed algorithm achieves a new SOTA for RL algorithms on max cut problems with orders of magnitude improvement in speed.
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,This paper proposes to train VAEs with discrete latent variables by using evolutionary algorithms to optimize the parameters of the variational autoencoder (VAE). The authors propose to use a truncated VAE with truncated posteriors and truncated latent variables. The authors show that the proposed approach is more efficient than amortized training and is competitive in zero-shot denoising. 
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"This paper proposes a method to identify controlled effects of an agent’s actions on the environment using counterfactual measures of blame. The proposed method, Controlled Effect Network (CEN), is an unsupervised method that compares what actually happened with what actually should have happened. The authors show that CEN is able to identify the controlled effects in a wide range of environments. Moreover, CEN can be used as an intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a structure-regularized pruning (SRP) method to improve the performance of SRPN-L and SRPN, a lightweight network and a very deep one. The main idea is to prune the weights of the residual blocks of the SRPN to remove the unimportant filters. The authors also employ L2 regularization to drive the weights towards zero so that eventually their absence will cause minimal performance degradation. The proposed method is evaluated on CIFAR-10 and ImageNet and compared with existing methods."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a method for cross-domain few-shot learning (CDFSL) that tackles large domain shift between base and novel categories. The method consists of three steps: (1) pretraining a backbone network on a single-source dataset, (2) learning a masking module on the target dataset, and (3) fine-tuning a classifier along with the backbone network such that the backbone produces features that are similar to the relevant ones. The proposed method is evaluated on the recently introduced Cross-domain Few-Shot Learning benchmark. The results show that the proposed method outperforms all meta-learning approaches and produces competitive results against recent cross- domain methods."
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"This paper studies the generalization properties of neural networks trained by gradient descent (GD) and Bayesian inference (BINARY). In particular, the authors show that GD can improve generalization by selecting networks with a large margin (i.e. networks that generalize well on the training data) and BINARY can further improve generalisation by selecting network with a larger margin. The authors also provide a PAC-Bayes bound on the average test error of the neural network–Gaussian process (NNGP) posterior for binary classification. "
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a cross-lingual manifold mixup (X-Mixup) method, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show X-MixUp achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the performance gap between the source and target languages significantly."
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of Byzantine robust distributed or federated learning, where a central server trains a machine learning model over data distributed across multiple workers. In this setting, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages to the central server, leading to significant loss of performance. The authors design new attacks which circumvent current defenses, and propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. They also theoretically and empirically validate their approach, showing that combining bucketing with existing robust methods is effective against challenging attacks."
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper studies the relationship between disentanglement and multi-task learning based on hard parameter sharing. The authors perform a thorough empirical study of the representations obtained by neural networks trained on automatically generated supervised tasks. Using a set of standard metrics, the authors show that disentangling appears naturally during the process of multi- task neural network training. The results are inconclusive as the obtained results vary for different datasets."
SP:9851adb72e2918780f661f83f7da06eb866787be,"This paper proposes a framework of certifying robust policies (CROP) for reinforcement learning against adversarial state perturbations, which provides state level robustness certification and the first certification for cumulative rewards. Specifically, the authors develop a local smoothing algorithm that uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness of actions taken along this trajectory. Next, they develop a global smoothing algorithms for certifying the robusteness of a finite-horizon cumulative reward under adversarial attacks. Finally, they propose a local smoother approach that makes use of adaptive search in order to obtain tight certification bounds for the reward. The proposed method is evaluated on three representative Atari games."
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a new approach to conformal prediction in which aims to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. Conformal prediction provides the ability to adapt to model uncertainty by constructing a calibrated candidate set in place of a single prediction, with guarantees that the set contains the correct answer with high probability. In this paper, the authors propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i.e., the total number of false positives) is bounded according to a user-specified tolerance. Subject to this constraint, the algorithm optimizes for a generalized notion of set coverage that allows for any number of true answers for a given query (including zero). The authors demonstrate the effectiveness of this approach across several classification tasks in natural language processing, computer vision, and computational chemistry."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper studies the behavior of the length distortion of ReLU networks with random initialization. The authors show that the expected length distortion does not grow with depth, and indeed shrinks slightly. They also generalize this result by proving upper bounds for higher moments of the distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by their experiments."
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes SAFER, a safe reinforcement learning algorithm for offline skill discovery. The key idea is to learn a safety variable that encodes safety requirements and a set of safe primitive skills. The safety variable is learned by contrastive training on safe and unsafe data. In the inference stage, SAFER composes a safe and successful policy from the safety skills according to the inferred safety variable and abstract action. The method is evaluated on several complex safety-critical robotic grasping tasks inspired by the game Operation."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a multi-branch architecture for image restoration. The main idea is to separate different attention features from stacking multiple blocks to achieve multiple restoration tasks in a general framework. The experiments show that the proposed model has competitive performance results on four datasets, including image dehazing, deraindrop, and deblurring, which are very common applications for autonomous cars."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes a method for personalized federated learning (PFL) with novel clients at inference time, i.e., after a model has been trained on a set of clients, it needs to be evaluated on novel unlabeled clients. The authors propose a novel approach to this problem, called IT-PFL-HN, which is based on a hypernetwork module and an encoder module. Specifically, the encoder learns a representation for a client given its own data, and the client representation is fed to the hypernetwork that generates a personalized model for that client. The proposed approach is evaluated on four benchmark datasets."
SP:960d0a63a82593f6e72275b65f0501f0469d1924,This paper proposes a conditional diffusion based generative model (RCDM) to visualize representations learned by self-supervised learning (SSL) methods. The authors show that the representation learned by RCDM is more robust to small adversarial perturbations and can be used for image manipulation. They also demonstrate that the model’s generation quality is on par with state-of-the-art generative models.
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"This paper studies the problem of differentially private fractional frequency moments estimation. The authors propose a new algorithm, Fp sketch, for estimating the pth frequency moment of a given set, i.e. p = 0, 1, 2, where p is the cardinality of the set. The algorithm is based on the well-known sketching algorithm of Indyk, 2006, and the authors show that it preserves differential privacy as is. The main contribution of the paper is to show that the algorithm is differentially privacy when p \in (0,1) and when p = 2. The paper also shows that the performance of the proposed algorithm is better than the optimal non-private baseline by a logarithmic factor. "
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes a method for learning policies that are both locally optimal and sufficiently different from existing ones. The proposed method, Reward-Switching Policy Optimization (RSPO), switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with intrinsic rewards, while for trajectories with high likelihood under existing policies, it utilizes an intrinsic diversity reward to promote exploration. The method is evaluated on a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,This paper proposes a differentiable sampling algorithm for diffusion models. The sampling algorithm is based on gradient descent and reparametrization trick. The authors propose a generalization of Denoising Diffusion Probabilistic Models (DDPMs) to Generalized Gaussian Diffusion Models (GGDMs). The authors show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. The proposed algorithm is evaluated on unconditional image generation on LSUN Church and CIFAR-10.
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,This paper proposes an approach to improve the accuracy of large language models (LLMs) by adapting natural language prompts into continuous representations that allow LLMs to accurately predict factual information. The authors propose a class of models called P-Adapters that take LLM embeddings as input and output continuous prompts that are used to query the LLM. They also investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (“experts”) and select one of them to query LLMs. They perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations.
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes a method for continuous classification of time series (CCTS), which is a continual learning task with the unclear distribution division. The authors propose a novel Adaptive model training policy ACCTS to overcome two main problems: catastrophic forgetting and the over fitting. In particular, the authors propose an adaptive multi-distribution extraction policy, which extracts data distributions adaptive to the time series evolution and the model change, and an adaptive importance-based replay policy. Experiments on four real-world datasets show that the proposed method can classify more accurately than all baselines."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes a method to improve the scalability of Transformer-based language models by increasing the size of the external memory of (key, value) pairs. The external memory is implemented as a kNN-augmented layer, which stores the key and value pairs in the top layer of the Transformer layers. The kNN is used to perform approximate k-nearest-neighbor (kNN) search into the memory, which is not differentiable. The authors demonstrate that the proposed method improves the performance on various benchmarks and tasks, including generic webtext, math papers, books, code, and theorems."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes a sampling scheme based on Metropolis-Hastings Monte Carlo (MH) algorithm to sample from masked language models (MLMs). The samples are proposed from the same masked conditionals used for training the MLMs, and they are accepted or rejected based on their energy values according to the target distribution. The proposed method is evaluated on open-ended unconditional generation and a conditional generation task of machine translation. "
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper studies the problem of learning data augmentation for NLP tasks. The authors propose a novel reward function for training the augmentation policy to construct more diverse and challenging samples for providing informative training signals, while avoiding the risk of losing the semantics of original samples. Specifically, the authors propose to jointly optimize a policy that constructs the augmented samples with low confidence but a high semantic similarity with original ones. In addition, they introduce a sample re-weighting scheme to focus on difficult augmented samples after the original ones are learned confidently for more effective learning from the augmented ones. The proposed approach outperforms the recent state-of-the-art augmentation schemes on various text classification tasks and GLUE benchmark."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"This paper proposes a method for context-based offline reinforcement learning (COMRL) that improves upon FOCAL (Li et al., 2021a) by incorporating intra-task attention mechanism and inter-task contrastive learning objectives. In particular, the authors propose a batch-recalibrated attention to recalibrate the weights of transition samples, and use sequence-wise self-attention for task-level representation learning, replacing its queue with a meta-batch-sampled on-the-fly objective. Theoretical analysis and experiments are presented to demonstrate the superior performance and robustness of the end-to-end and model-free framework."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,This paper proposes an inference-time improvement framework for parametric sequential generative modeling methods called belief fine-tuning (BFT). BFT leverages approximate dynamic programming to determine the model parameters at each time step. It can improve the accuracy of the belief model at test time because it specializes the capacity of the model to the space of local observations. BFT enables approximate public belief state search in imperfect-information games where the number of possible information states is too large to track tabularly.
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper proposes Pixelated Butterfly, a method for sparse training of neural networks. The main idea is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. The authors propose simple variants of butterfly (block and flat) to take advantage of modern hardware. The method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP)."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper proposes a score-based generative model for conditional image generation. The authors propose a novel conditional diffusion probabilistic model by explicitly modeling the class center in the forward and reverse process, and make an elegant modification to the original formulation, which enables controllable generation and gets interpretability. They conduct extensive experiments on multiple tasks, and achieve competitive results compared with the state-of-the-art methods."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper proposes a method for domain generalization (DG) based on decomposing the high-dimensional latent feature space to lower-dimensional sub-spaces, after which an individual hypothesis is learned (optimally) for each latent sub-space. The label-informative features are learned from source latent representations so that label-space latent representations can capture sufficient and necessary information to effectively train sub-sub-space hypotheses. The authors provide a rigorous theoretical analysis to explain how their approach can learn meaningful sub-domain domain domain domain information that helps to reduce the domain shift. The method is evaluated on several well-known DG benchmarks, where it achieves state-of-the-art results."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper improves upon the kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-root kernel. The authors show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RkHS. They also show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square root KT without making explicit use of a square root kernel. Finally, they show that a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents an open-source benchmark suite for the max-min independence set problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. The authors also conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper proposes a method to compress the activation maps of a 1x1 convolution layer of a neural network. The proposed method is based on the Haar-wavelet transform, which has been used for image compression in standard hardware and has been shown to be able to reduce the compression ratio and the compression rate of the convolution layers. The main idea is to keep the same top k entries (in magnitude) of the transformed activations maps with respect to all channels (dubbed as joint shrinkage) and perform convolution in the wavelet domain on the compressed signals, saving significant computations. This procedure is applied along with modest quantization to further reduce the computational costs further."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper considers the problem of learning in extensive-form games, which are general-sum games with both sequential and simultaneous moves and imperfect information. The authors propose a new algorithm for learning in such games that is based on the counterfactual regret minimization (CFR) framework, which is also an uncoupled no-regret learning dynamic. They show that their algorithm converges at a rate of $O(T^3/4)$ to an approximate correlated equilibrium (EFCE) that is $O(\sqrt{T})$-approximate when all agents play T repetitions of the game according to the accelerated dynamics. This significantly improves over the best prior rate of $\tilde{O}(T 1/2)$. They also show that the stability of certain fixed point strategies can be characterized through a refined perturbation analysis of a structured Markov chain."
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes a method to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. By discretizing the action space, the proposed method can be applied to any discrete action deep RL algorithm to the continuous control problem. The proposed method is evaluated on three different setups: RL with demonstrations, RL with play data, and Imitation Learning. The results show that AQuaDem outperforms state-of-the-art continuous control methods, both in terms of performance and sample efficiency."
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper proposes AdvStyle, a method for domain generalization in semantic segmentation based on adversarial style augmentation (AdvStyle). AdvStyle generates hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic-to-real semantic segmentations benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains."
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper proposes an event-based guided Variational Autoencoder (VAE) for mid-air gesture recognition. The key idea is to use a convolutional SNN to encode spatio-temporal information into a latent space representation, which is then used to compute the similarity of gesture data. The proposed method achieves 87% classification accuracy on the DVSGesture dataset and it can encode the sparse, noisy inputs into an interpretable latent space representations, visualized through T-SNE plots. "
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper proposes a method for deep learning for tabular data by using ferns (oblivious decision trees) instead of neurons, by constructing a Sparse Hierarchical Table Ensemble (S-HTE) which is dense at the beginning of the training process and becomes gradually sparse using annealing mechanism, leading to an efficient final predictor. The authors show its accuracy is comparable to alternatives, while having an order-of-magnitude lower computational complexity."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper tackles the problem of learning value functions from undirected state-only experience (i.e. state transitions without action labels). The authors first theoretically characterize the applicability of Q-learning in this setting. They show that tabular Q learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-Learning (LAQ), an offline RL method that can learn effective value functions using discrete latent actions obtained through a latent-variable future prediction model. The experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles and competing methods."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper proposes SWARM Parallelism, a distributed training algorithm for large models using unreliable heterogeneous heterogeneous devices. SWARM creates temporary randomized pipelines between available nodes that are rebalanced in case of failure. To further reduce the network usage of the approach, the authors develop several compression-aware architecture modifications and evaluate their tradeoffs. Finally, they combine their insights to train a large Transformer language model on a swarm of preemptible T4 GPUs with less than 400Mb/s network throughput."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a method to correct the transition dynamics of offline data for online tuning in decentralized multi-agent reinforcement learning (MARL). The transition dynamics in offline data do not accord with the online transition dynamics, which creates severe errors in value estimates, leading to uncoordinated and sub-optimal policies. The authors propose to use both offline and online experiences to tune the policies of agents, and introduce online transition correction (OTC) to implicitly correct the biased transition dynamics by modifying sampling probabilities. Two types of distances, i.e., embedding-based and value-based distance, are designed to measure the similarity between transitions, and an adaptive rank-based prioritization is proposed to sample transitions according to the transition similarity. OTC is simple yet effective to increase data efficiency and improve agent policies."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a logarithmic unbiased quantization (LUQ) method to quantize both the forward and backward phase of neural network training to 4 bits, achieving state-of-the-art results in 4-bit training. The authors analyze different types of stochastic rounding schemes to explain the importance of having an unbiased rounding scheme for the neural gradients with the proposed method. They also suggest a method that exploits the low precision format by avoiding multiplications during two-thirds of training process, thus reducing by 5x the area used by the multiplier."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper proposes a self-attention feature-selection mechanism that adaptively dilutes non-discriminative features for the task of few-shot learning problems. The authors show that in the presence of task-irrelevant features, inherent to meta-learning problems, attentional models are susceptible to misclassification. To address this challenge, the authors propose a simple solution to this challenge that is non-parametric and based on self attention. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes a multi-agent reinforcement learning (MARL) framework to study emergent communication between agents using a continuous communication channel trained through reinforcement learning. The authors propose an environment and training methodology to carry out an initial exploration of these questions. They use a simple messaging environment where a ""speaker"" agent needs to convey a concept to a ""listener"" agent. The speaker is equipped with a vocoder that maps symbols to a continuous waveform, this is passed over a lossy continuous channel, and the listener needs to map the continuous signal to the concept. They show that basic compositionality emerges in the learned language representations. They also show that noise is essential in the communication channel when conveying unseen concept combinations."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"This paper proposes BadPre, a novel backdoor attack against pre-trained NLP models. BadPre is the first task-agnostic backdoor attack, which embeds the backdoors into a pre- trained model which can be transferred to the downstream language tasks. The attack does not need prior information about the downstream tasks when implanting the backdoor to the pretrained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. Experimental results show that BadPre can compromise a wide range of downstream NLP tasks in an effective and stealthy way."
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper proposes a reward-free method for unsupervised skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn’t forget a learned skill. The proposed method is evaluated in both evolving and static environments, and it outperforms existing state-of-the-art skill discovery methods on both skill quality and ability to solve downstream tasks."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a new convolutional layer, called log-polar space convolution (LPSC), where the convolution kernel is elliptical and divides its local receptive field into different regions according to the relative directions and logarithmic distances. The receptive field grows exponentially with the number of distance levels. The proposed LPSC not only naturally encodes local spatial structures, but also greatly increases the single-layer receptive field, resulting in small local receptive fields in lower layers. Experiments on different tasks and datasets demonstrate the effectiveness of the proposed method."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper studies the generalization properties of neural networks (NNs) from the information bottleneck (IB) perspective. In particular, the authors propose an algorithm for the efficient approximation of the IIW. Then, they build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. The authors empirically identify the fitting to compressing phase transition during NNs’ training and the concrete connection between IIW compression and the generalisation. Moreover, they propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"This paper investigates why indiscriminate data poisoning attacks work in principle. It shows that the perturbations of advanced attacks are almost linear separable when assigned with the target labels of the corresponding samples. Moreover, it further confirms that linear separability is indeed the workhorse for recent attacks. It also suggests that the shortcut learning problem is more serious than previously believed as deep models heavily relies on shortcuts even if they are of an imperceptible scale and mixed with the normal features."
SP:7b50be406138ad01db3ee112899f622637896fe9,"Offline policy optimization is a critical problem in many real-world decision-making problems, as online learning is costly and concerning in many applications. In this paper, the authors identify an important overfitting phenomenon in optimizing the importance weighted return, and propose an algorithm to avoid this overfitting. The authors provide a theoretical justification of the proposed algorithm through a better per-state-neighborhood normalization condition and show the limitation of previous attempts to this approach through an illustrative example. The experiments show the proposed method with less overfitting and better test performance compared with state-of-the-art batch reinforcement learning algorithms."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents a method for continual learning of how language is grounded in vision. Given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case CLIP by OpenAI), CoLLIE learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. Unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language uses. The authors verify the model’s performance on two different tasks of continual learning and show that it can efficiently learn and generalize from only a few examples."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper presents VLAF2, a novel method for novel object captioning (NOC) that leverages the intrinsic knowledge of BERT and CLIP for utilizing linguistics observed from captions for describing visual information of images with novel objects. The proposed method is evaluated on the nocaps dataset and achieves state-of-the-art results in terms of metrics linking to fluency, fidelity, and fluency. The authors also perform quantitative and qualitative analysis to demonstrate how the proposed method generates novel object captions with improved fluency and fidelity."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies transfer learning in the setting of few-shot learning, where the goal is to learn representations for classification that are transferable to new, unseen classes. The authors propose a new perspective on transfer learning with foundation models based on the recently discovered phenomenon of neural collapse (Papyan et al., 2020), where the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. They demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and – more importantly – to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few shot setting."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper proposes a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into 3D points. The paper further improves the performance of transformer by a newly proposed module called amplified positional encoding. Extensive experiments demonstrate that the network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"This paper proposes a new method for distributed training of graph convolutional neural networks (GCNs). The proposed method, called PipeGCN, aims to reduce the communication overhead of distributed GCN training by pipelining inter-partition communication with intra-partitions computation. The paper also provides a theoretical convergence guarantee for the proposed method and shows that the convergence rate is close to that of the vanilla distributed GCNs training without staleness. Extensive experiments are conducted to show the effectiveness of the method."
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper proposes a method for test-time robustification, i.e., using the test input to improve model robustness. The authors propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations. This objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentation, while also maintaining confidence in its predictions. The experiments show that this approach achieves accuracy gains of 1-8% over standard model evaluation and also outperforms prior augmentation and adaptation strategies."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper proposes a model-based reinforcement learning algorithm that jointly optimizes a lower bound on the expected return of the dynamics model and the policy. The lower bound is defined as the lower bound of the expected reward under the true environment dynamics. The paper proposes to use a discriminator that distinguishes between real and fake transitions, and the discriminator is included in the objective for the policy, and both the model and policy are jointly trained to maximize reward and minimize discriminator accuracy. Experiments show that the proposed algorithm is competitive with prior state-of-the-art methods on some benchmark tasks."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a combination of behavioral cloning (BC) and observation history (OH) methods to solve the copycat problem in POMDPs. BC-SO and BC-OH each have their strengths and drawbacks, and combining them optimally could achieve the best of both worlds. The proposed method first compute a coarse action based on the instantaneous observation, and then refine it into a final action using historical information. The experiments show that this method outperforms all baselines on CARLA autonomous driving and various MuJoCo continuous control tasks."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a model-based meta-learning method for dynamical systems that can generalize across heterogeneous domains by partitioning them into different tasks. The proposed method has two parts: an encoder which infers the time-invariant hidden features of the task with weak supervision, and a forecaster which learns the shared dynamics of the entire domain. The encoder adapts and controls the forecaster during inference using adaptive instance normalization and adaptive padding. Theoretically, the authors prove that the generalization error of such procedure is related to the task relatedness in the source domain, as well as the domain differences between source and target."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method first detects 2D boxes on the image, then selects the corresponding RoI LiDAR points as the weak supervision. A network is trained to predict 3D boxes which can tightly align with associated RoI points. This network is learned by minimizing the newly-proposed 3D alignment loss between the 3D box estimates and the corresponding object-LiDAR-points."
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper proposes a method to perform token-free end-to-end training of Transformer-based character-level models. Specifically, the authors propose a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The authors also introduce a deep Transformer model that integrates GBST and operates on the byte level. Experiments on English GLUE, multilingual, and noisy text datasets show that GBST outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper proposes a black-box hard-label backdoor detection method to detect backdoors in deep neural networks (DNNs). The authors show that the objective of backdoor detection is bounded by an adversarial objective, which can be optimized using Monte-Carlo gradient estimation in the hard label setting. The authors also show that a singularity is often observed in the adversarial map of a backdoor-infected example, which they call the “adversarial singularity phenomenon”. Based on this observation, the authors propose to use adversarial extreme value analysis (AEVA) to detect the backdoors. The AEVA algorithm is based on the monte-carlo gradient estimator. The proposed method is evaluated on several tasks and backdoor attacks, and it is shown effective in detecting backdoor attacks."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new class-wise uncertainty measure, called KLoS, which is a Kullback-Leibler divergence criterion on the class-probability simplex. The authors claim that the proposed measure captures class confusion and lack of evidence in a single score and does not require out-of-distribution (OOD) training data. The paper also proposes an auxiliary neural network, KloSNet, to learn a refined criterion directly aligned with the evidential training objective. Experiments on CIFAR-10 and ImageNet show the effectiveness of the proposed method."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper studies the problem of learning convolutional neural networks (CNNs) under some natural distributional assumptions. Specifically, the authors assume that the distribution of patches in the input images has low-dimensional structure (e.g., when the patches are sampled from a low-dimensions manifold). Under this assumption, they study a two-step semi-supervised learning algorithm PAC, that learns a linear classifier over datadependent features obtained from unlabeled data. They show that the algorithm provably learns CNNs, under the assumption that the distributions of important patches have a moderate covering number (i.e., they are well-clustered). The authors also prove that the dependence of the algorithm on the dimension of the patch distribution is essentially optimal."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"This paper proposes a new approach for face clustering based on graph convolutional networks (GCNs). The authors propose a novel algorithm named Ada-NETS to cluster faces by constructing clean graphs for GCNs. First, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCN to cluster face. Experiments on multiple public clustering datasets show the effectiveness of the proposed method."
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a new method for generalizable person re-identification (DG) and cross-domain generalization (CD) based on distributionally robust optimization (DRO). In particular, the authors propose to use the change-of-measure technique and the analytical solution of KL DRO to propose a simple yet efficient approach, Unit DRO, which minimizes the loss over a reweighted dataset where important samples (i.e., samples on which models perform poorly) will be upweighted and others will be downweighted. Empirical results show that the proposed method achieves superior performance on large-scale DG ReID and CD benchmarks compared to standard baselines."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"This paper proposes a simple technique to improve the performance of graph neural networks (GNNs) by corrupting the input graph with noise and adding a noise correcting node-level loss. The idea is to encourage the model to maintain and refine distinct node representations through message passing to the final output, which causes it to resist oversmoothing. Experiments are conducted on 3D molecular property prediction tasks and demonstrate the effectiveness of the proposed method."
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper proposes a method for learning a vector representation for the set2vec problem, which is the task of extracting vector representation from an input set comprised of a variable number of feature vectors. The authors propose to use the maximum-a-posterior (MAP) estimate of the mixture which is approximately attained by a few ExpectationMaximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff back-propagation for any given downstream task. The proposed mixture set data fitting framework allows unsupervised set representation learning naturally via marginal likelihood maximization aka the empirical Bayes. On several classification problems and NLP tasks, the proposed method shows improvement over the state-of-the-arts including OTKE."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes a method for feature selection in contrastive analysis (CA) setting, where the goal is to select a small number of informative features for use in unknown downstream tasks. The proposed method, CFS (Contrastive Feature Selection), consists of two components: (1) feature clustering and (2) feature selection. In the feature selection part, the proposed method is based on the contrastive loss, which is a weighted sum of the KL-divergence between the features of the target dataset and the background dataset. The method is evaluated on a semi-synthetic dataset and four real-world biomedical datasets, and it is shown that it consistently outperforms existing unsupervised feature selection methods."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the effect of early stopping (i.e. stopping the training process at the optimal early stopping time) on the generalization properties of deep neural networks. The authors propose a new model where the label is generated by a low-dimensional transformation of the input data and define the case where the model size is larger than the number of features as being overparametrized. Theoretically, the authors show that under certain assumptions, the optimal late stopping time depends on the model dimension and the sample size of the dataset. Experiments on MNIST and CIFAR-10/100 show that the theoretical results are consistent with what happens in practice."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper proposes a quasi-Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient (NPG) algorithm. For other entropy functions, this method results in new policy gradient algorithms. The authors provide a simple proof that all these algorithms enjoy the Newton-type quadratic convergence near the optimal policy. They demonstrate that the proposed method typically converges in single-digit iterations."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper proposes a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The method can be applied in conjunction with any existing on-policy neural agent in the literature for text-based games. The experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state of-the-art results on widely used environments."
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. The authors demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embedding of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with the state-of-the-art single-sense word embedding and embedding-based topic model.
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper proposes a method to transfer a 2D convolutional neural network trained on an image dataset to a 3D point-cloud dataset. The main idea is to convert the convolution filters from 2D to 3D by copying the filter weights along a third dimension and finetune only the input/output layers and batch normalization layers. The method is evaluated on the ModelNet 3D Warehouse dataset, and the results show that the method outperforms the baseline methods."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper proposes a method for training autoregressive generative models (ARGMs) with energy-based models (E-ARM). The main idea is to combine the training of ARGMs and E-ARM in a single training function, where the two models share a single base function, i.e., the energy score function. The proposed method is evaluated on language modeling, neural machine translation (NMT) and image generation tasks."
SP:51e748c55bd4134047098559577fa3f37aa7433a,This paper introduces a new Wasserstein distributional robustness (WDR) framework that connects adversarial training (AT) methods with standard adversarial robustness methods. The authors introduce a new cost function of the distance between two distributions and propose a new series of risk functions. They show that standard AT methods are special cases of their counterparts in the framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional AT-based algorithms. Extensive experiments show that the proposed algorithms robustify further their standard AT counterparts in various settings.
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a method for unsupervised representation learning for multivariate time-series data. The proposed method is based on the idea of bilinear temporal-spectral fusion (BTSF), which is an iterative fusion of spectral and temporal features extracted from the data. BTSF is evaluated on three tasks: classification, forecasting and anomaly detection, and shows superior performance over the state-of-the-art."
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent. The learning rate is optimized via a simple extra gradient descent step, justified by an analysis that exploits the structure of neural networks. The authors formulate first and second-order gradients with respect to learning rate as functions of consecutive weight gradients, leading to a cost-effective implementation. They also show that the scheme can be extended to accommodate for different learning rates per layer."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta-learning method for sequential multi-task learning, where the goal is to learn to learn, using their experience on previous tasks to learn new tasks more quickly. The proposed method, continual meta-policy search (CoMPS), meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta learning to prepare for subsequent task learning. The method is evaluated on several continuous control tasks."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a new threat model for poisoned classifiers, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, they propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. They construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images with human interaction. They demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper studies the problem of knowledge distillation for unconditional GANs, especially StyleGAN2. The authors claim that the main challenge of uni-GAN distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. To address this issue, the authors propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent, and further propose a latent-direction-based distillation loss that preserves the semantic relations in latent space. Extensive experiments demonstrate the effectiveness of the proposed method."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a method for approximating offline algorithms in online settings by encoding the behavior of offline algorithms into graphs. The authors train a multi-task learning model to simultaneously detect behavioral structures which have already occurred and predict those that may come next. They demonstrate the methodology on both synthetic data and historical stock market data, where the contrast between explanation and prediction is particularly stark."
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper proposes a method to amortize the computation of the inducing points locations and the parameters of the variational posterior approximation for Gaussian Processes (GPs). In particular, the authors propose to use a neural network that receives the observed data as an input and outputs the inducing point locations and parameters of q. They evaluate their method in several experiments, showing that it performs similar or better than other state-of-the-art sparse variational GP approaches. However, with the number of inducing points is reduced drastically due to their dependency on the input data."
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"This paper proposes a protocol for secure (Byzantinetolerant) decentralized training that emphasizes communication efficiency. The authors propose a novel strategy for decentralized Byzantine-tolerant training on all participants, where the extra communication cost does not depend on the number of parameters. They also provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. They conduct large-scale experiments on image classification and language modeling."
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper presents a physics-informed machine learning approach for smoothed particle hydrodynamics (SPH), a mesh-free Lagrangian method for obtaining approximate numerical solutions of the equations of fluid dynamics, which has been widely applied to weakly-and strongly compressible turbulence in astrophysics and engineering applications. The authors present a learn-able hierarchy of parameterized and “physics-explainable” SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators. The learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. They show that their physics informed learning method is capable of: (a) solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; (b) learning Lagrangians statistics of turbulence; (c) combining Lagrangeian trajectory based, probabilistic, and Eulerian field based loss functions; and (d) extrapolating beyond training sets into more complex regimes of interest."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,This paper proposes an entropy maximization regularizer (Mix-MaxEnt) to improve classification accuracy and uncertainty estimates. The proposed approach is based on synthesizing between-cluster samples via the convex combination of two images from different classes and maximizing the entropy on these samples. This approach guides the maximum likelihood estimation to prefer a solution that maps out-of-distribution samples to high-entropy regions (creating an entropy barrier); and (2) is more robust to the superficial input perturbations.
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a method for self-supervised auto-encoder-based image-to-image transfer. The proposed method, Latent Image Animator (LIA), is based on linear navigation in the latent space of StyleGAN and BigGAN. Specifically, the motion in generated video is constructed by linear displacement of codes in latent space. The authors propose to learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the hidden space. Extensive quantitative and qualitative analysis suggests that the model systematically and significantly outperforms state-of-the-art methods on VoxCeleb, Taichi and TED-talk datasets."
SP:86f9f89f84e117c86478b9afaf087f65524f5472,This paper studies the problem of meta-learning with fewer tasks and proposes a task interpolation strategy to generate additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. The proposed method is able to generate new tasks by interpolating between pairs of randomly sampled meta-training tasks. The authors also provide theoretical analysis to show that the proposed method corresponds to a data-adaptive meta-regularization and further improves the generalization. Experiments are conducted on 8 datasets from diverse domains.
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper proposes a new approach for fair representation learning based on normalizing flows. In particular, the encoder is trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictors. The proposed approach is evaluated on a variety of challenging real-world datasets. "
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a graph neural network-based method for subgraph isomorphism counting. The proposed method is based on graph neural networks (GNNs) to learn a low-dimensional representation for both the query and the input graph, in order to predict the number of isomorphisms on the input graphs. In particular, it modulates the graph representation conditioned on the query, so that the graph can be adapted to each query individually to improve their matching. At the edge level, it uses edge-centric message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency. Experimental results show that the proposed method achieves superior performance in comparison to the state-of-the-art baselines."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper studies the problem of agnostic personalized federated learning (PFL), where each client can have their own personalized labels, which might not be compatible with others (even for the same class), and can be also possibly from a variety of multiple domains. The authors propose a novel method, namely SimFed, which measures task-level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. SimFed factorizes the model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. Experiments on both single-domain and multi-domain datasets show that the proposed method outperforms the current state-of-the-art approaches."
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper proposes an object dynamics distillation network (ODDN) which distills explicit object dynamic representations (e.g., velocity) from raw video input. The authors also build a relation module that calculates object-pair interactions and applies it to the corresponding dynamic representations of objects. The proposed method is evaluated on tasks of video events reasoning and video prediction, which are two important evaluations for video understanding. The results show that visual representations of ODDN perform better in answering reasoning questions around physical events in a video compared to representaions of the previous scene representation methods."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"This paper studies the problem of generalization of GNNs with positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. The authors propose a new GNN layer called PEG, which uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original nodes features and rotation equivariant w/o the positional features simultaneously. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes a novel text style transfer framework based on large-scale language models. The proposed LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), the model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that the model not only makes training more efficient, but also generates more readable and diverse expressions than previous models."
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper proposes a method for approximate query answering (QA) on hyper-relational knowledge graphs (KGs). In particular, it extends the multi-hop logical reasoning (MLP) problem on classical, triple-based KGs, which can handle higher-order logical queries. The proposed method is based on recent advancements in Graph Neural Networks (GNNs) and query embedding techniques, and the authors propose a method to embed and answer conjunctive and existential queries. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper proposes a method for hyperparameter optimization (HPO) for deep neural networks. The proposed method is based on a Bayesian optimization approach based on Gaussian Processes (GP), that extends and adapts to the multi-budget BO (a.k.a. multi-fidelity) case. The authors propose a surrogate for GP that embeds the learning curve dynamics and a new acquisition function that incorporates multi-budgets information. The empirical results show that the proposed method outperforms state-of-the-art HPO baselines on 50 datasets."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper studies the problem of cross-platform cross-device decoding of learned image compression. The authors propose to use post-training quantization and make the model inference integer-arithmetic-only, which is much simpler than the existing training and fine-tuning based approaches. They further improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. With their proposed methods, the current state-ofthe-art image compression models can infer in a cross-domain consistent manner, which makes the further development and practice of learned images compression more promising."
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes an unsupervised noise reconstruction and removal network for denoising focused ion beam scanning electron microscopy (FIB-SEM) images. The authors propose a network architecture inspired by gated recurrent units, which reconstructs and removes the noise by synthesizing the sequential data. The network uses a triplet of images as input and is trained to map one noise realization to the other, using a modified Noise2 loss function. NRRN is applicable to the case of improving the image quality based on two or three scans of the same slice. The paper provides detailed performance analysis using numerical as well as empirical metrics."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"This paper provides a theoretical analysis of the label trick, a recently proposed technique that allows parallel use of node features and labels for node property prediction in graph neural networks (GNNs). The authors show that the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. The authors also provide a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"This paper proposes a new multi-agent reinforcement learning task called SymmToM, where all agents can speak, listen, see other agents, and move freely through a grid world. To solve this task, the authors propose to model machine theory of mind in a more flexible and symmetric scenario; a multiagent environment where the agents can all act as both speaker and listener. The authors show that the proposed task can be solved by either using well-known deep reinforcement learning (RL) models, nor by tailoring those models to our task, and show that even maintaining the simple rules of the environment, modifying its parameters in much more difficult ways can yield better results."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper proposes a zero-shot object detection method for industrial robots. Zero-shot learning is a subset of unsupervised learning, and it aims to detect novel objects in the image with the knowledge learned from and only from seen objects. In this paper, the authors propose to use the YCB Video Dataset, which contains 21 objects in various categories. The proposed method achieves a reasonable accuracy on classifying unseen objects."
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper proposes an autoregressive latent video prediction method that can predict high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, it scales up prior models by employing a high fidelity image generator (VQ-GAN) with a causal transformer model, and introduces additional techniques of top-k sampling and data augmentation to further improve video prediction quality. The proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high resolution video prediction on complex and large scale datasets."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,This paper investigates the use of vision transformers (ViTs) for the task of image generation. The authors claim that ViTs can be used to train generative adversarial networks (GANs) with comparable performance to the state-of-the-art CNN-based GANs. The paper proposes several modifications to stabilize the training dynamics and facilitate the convergence of ViT-based generative models. The experiments are conducted on three public image synthesis benchmarks. 
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper studies the question of why good likelihoods do not imply good sample quality in variational autoencoders (VAEs). The authors argue that the main cause of this issue stems from the model being overwhelmed by the vast volume of visually-imperceptible information contained in natural image distributions. Based on this hypothesis, the authors propose a two-stage training procedure that prioritizes the modeling of the visual perceptible information and then models the majority of the likelihood signal. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed approach."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"This paper proposes a training-free inference framework for diffusion probabilistic models (DPMs) that estimates the analytic forms of the reverse variance and the corresponding optimal KL divergence of a DPM using the Monte Carlo method and a pretrained score-based model. The authors also derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, the analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper investigates whether vision transformers (ViTs) can be used to replace convolutional neural networks (CNNs) in the medical imaging domain. The authors conduct experiments on several standard medical image benchmark datasets and tasks. They show that, while CNNs perform better if trained from scratch, off-the-shelf vision transformer can perform on par with CNNs when pretrained on ImageNet, both in a supervised and self-supervised setting."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper analyzes the expressivity of pretraining neural language models (NLMs) in terms of their ability to model dependencies between sentences that appeared in the same training example. The authors show that the pretrained NLM can model stronger dependencies between text segments that appear in same training examples than it can between different training examples. This result is motivated by the fact that pretraining NLMs can learn to model stronger relationships between sentences if they appear in the training examples, but not if they do not. The paper shows that this result can be used to motivate new pretraining heuristics for NLMs."
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,This paper proposes a symbolic representation and interpretation framework for learning to optimize (L2O). The main idea is to convert a numerical L2O predictor into a symbolic form that preserves the same problem-specific performance and can be made tunable and subject to further testing by a lightweight re-parameterization. The proposed method is evaluated on a set of optimization tasks and shows that it outperforms human-designed and tuned optimizers.
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper studies the problem of adversarial robustness in reinforcement learning. In particular, the authors consider the setting where the adversarial perturbation can be a stochastic function of current and previous observations and states, as well as previous actions. The authors propose a randomized smoothing-based defense method that adds a Gaussian noise to the observations at each time-step before passing it through the policy function. They show that their method guarantees that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack."
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper studies the problem of estimating the target-domain accuracy of a given classifier under distribution shift. The authors propose a method called Average Thresholded Confidence (ATC) that learns a threshold on the model’s confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In the experiments, ATC estimates target performance 2-4ˆ more accurately than prior methods. "
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper proposes a method for partial distribution matching (PDM) registration, where the goal is to recover a transformation that matches one set to the other. The authors formulate the registration problem as a partial Wasserstein-1 (PW) discrepancy, and derive the Kantorovich-Rubinstein (KR) duality for the PW discrepancy, which they show can be efficiently optimized. Based on these theoretical results, the authors propose a PWAN, which approximates the discrepancy by a neural network, and learns the transformation adversarially with the network. It also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,This paper proposes a transfer learning method for hyperparameter optimization (HPO) based on meta-learning. The authors propose a deep kernel Gaussian process surrogate with landmark meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. The proposed method is evaluated on a large-scale benchmark that involves 16 search spaces and 101 datasets from OpenML for a total of 3.4 million hyper-parameter evaluations.
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"This paper proposes a fingerprinting mechanism to enable responsible release and regulation of generative models. The fingerprint auto-encoder is incorporated into a GAN framework to allow model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. The proposed method achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Experiments show that the method achieves effectiveness in deep fake detection and attribution."
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,This paper proposes two methods to provide model-agnostic local explanations for similarity learners applicable to tabular and text data. The first method provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. The second method proposes analogies as a new form of explanation in machine learning. The goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model’s prediction.
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper studies the certified robustness of ensembles of deep neural networks (DNNs) against adversarial attacks. In particular, the authors show that the certified accuracy of an ensemble of DNNs is higher than that of a single DNN, and they analyze the necessary and sufficient conditions for certifiably robustness. They show that diversified gradients and large confidence margins are sufficient and necessary conditions for the certifiable robustness, and then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. They also prove that an ensemble model can always achieve higher certified accuracy than a single base model under mild conditions. Based on the theoretical findings, they propose the lightweight Diversity Regularized Training (DRT) to train certified robust ensemble ML models. Extensive experiments show that their DRT-enhanced ensemble achieves higher certified accuracies than existing single- and ensemble-based certified defense methods."
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper studies the expressive power of higher-order message passing graph neural networks (GNNs). In particular, the authors analyze a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, they prove that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, they show how the proposed model can exploit sparsity to reduce the computational complexity compared to the existing high-order models. More generally, they provide a (near) matching information-theoretic lower bound for counting subgraph with graph representations that pool over representations of derived (sub-)graphs."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper proposes a probe model for understanding the knowledge integration (KI) process in pretrained language models (LMs). The key idea is to use a graph convolution operation (GCS) to interpret the knowledge-enhanced LMs and expose what kind of knowledge is integrated into these models. The proposed GCS model can be used to correctly interpret the KI process, and the authors use it to analyze two typical KI methods: K-Adapter and ERNIE. They find that only a small amount of factual knowledge is captured in these models during integration. They further find that while K- adapter struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the effect of the adaptation learning rate in meta-learning with mixed linear regression. First, it presents a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. Second, it provides the underlying dependence between the optimal learning rate and the input data. Finally, it proves that compared with empirical risk minimization (ERM), MAMM produces an initialization with a smaller average distance to the task optima, consistent with previous practical findings."
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a method for source-free domain adaptation (SFDA), which aims to adapt a model trained on labelled data in a source domain to unlabelled data in the target domain without access to the source-domain data during adaptation. The authors propose a method called Feature Restoration (FR) to address the issue of measurement shift (MS), which is characterized by a change in measurement system and is pervasive in deployed machine learning systems. They propose a bottom-up training scheme (BUFR) for FR that preserves learnt structure in the later layers of a network. They demonstrate that BUFR outperforms existing SFDA methods on real and synthetic data in terms of accuracy, calibration, and data efficiency."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper studies the problem of adversarial robustness propagation in federated learning (FL), where the goal is to propagate the adversarial training (AT) from high-resource users that can afford AT, to those low resource users that cannot afford it, during the FL process. The authors propose a simple yet effective propagation approach that transfers robustness through carefully designed batch-normalization statistics. They demonstrate the rationality and effectiveness of their method through extensive experiments."
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper studies the problem of inferring the structure of the interaction network from observed equilibrium actions of a network game. The authors propose a novel transformer-like architecture which correctly accounts for the symmetries of the problem and learns a mapping from the equilibrium actions to the network structure without knowing the utility function of the game. They test their method on three different types of network games using both synthetic and real-world data, and demonstrate its effectiveness in network structure inference and superior performance over existing methods."
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes GraphANGEL, a novel relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphues containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. The model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper studies the problem of few-shot learning for histology images. The authors propose to combine contrastive learning (CL) with latent augmentation (LA) to improve the generalizability of the learned representation. In particular, CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. Experiments show that models learned by CL generalize better than supervised learning for the histology image in unseen classes, and LA brings consistent gains over baselines."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"This paper studies the problem of learning long-term dependencies in irregularly sampled time-series using recurrent neural networks (RNNs) with continuous-time hidden states. The authors show that such RNNs suffer from vanishing and exploding gradients, and propose a solution to this problem by equipping arbitrary RNN with a memory compartment separated from its time-continuous state. This way, the model is able to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path. "
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper proposes a method to perform full binarization of BERT, i.e., 1-bit quantization of the weights, embedding, and activation of the BERT model. The proposed method, called BiBERT, is based on two ideas: Bi-Attention and Direction-Matching Distillation (DMD). Theoretical analysis and empirical results show that the performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and the proposed method aims to eliminate the performance bottlenecks."
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper proposes a method for multi-person keypoint detection and instance segmentation using Transformer. The key idea is to use the self-attention mechanism of Transformer to predict the location of each keypoint, and then use the predicted keypoints to group the detected keypoints into their instances based on the pairwise attention scores. The proposed method is evaluated on the COCO detection challenge and the PersonLab segmentation task. "
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper proposes a new method for minimizing the mean-variance (MV) trade-off in the context of reinforcement learning. The proposed method, called direct expected quadratic utility maximization (EQUMRL), trains an agent to maximize the expected utility function, in which the maximizer corresponds to the Pareto efficient policy. The authors claim that the proposed method does not suffer from the double-sampling issue because it does not include gradient estimation of the variance. The method is well-motivated from the perspective of finance and finance theory, and is well suited to financial applications."
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper studies the problem of test-time domain adaptation for an autoencoder system whose channel is generatively-modeled using a mixture density network (MDN). Different from the setting of conventional training-time (unsupervised or semi-supervised) domain adaptation, here we have a fully-trained channel model and auto-encoder from a source domain, that we would like to adapt to a target domain using only a small labeled dataset (and no unlabeled data). Moreover, the distribution of the channel is expected to change frequently (e.g., a wireless link), making it challenging to collect sufficient data for frequent retraining of the auto-encoders. To address this, the authors propose a fast and sample-efficient method for adapting the autoencoders without modifying the encoder and decoder neural networks, and adapting only the MDN channel model. The method utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present to the decoders samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt the MDNs channel using very limited number of samples, and improve or maintain the error rate of the Auto-Encoders under changing channel conditions."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new approach to the abductive natural language inference task (alphaNLI), which aims to infer the most plausible explanation between the cause and the event. In this paper, the authors argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses; and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, they propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” in this paper. Based on the observation that the hypotheses are generally semantically related, they design a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. The experimental results show that our IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper proposes a method for adversarially robust out-of-distribution (OOD) detection. The proposed method combines a certifiable OOD detector with a standard classifier from first principles into an OOD aware classifier. This way it achieves the best of two worlds: certifiably adversarial robust OOD detection, even for OOD samples close to the in distribution, without loss in either prediction accuracy or detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks."
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper studies transfer attacks in the query-free black-box setting. In this setting, the attacker has a set of surrogate models trained on different datasets (with different label sets and image sizes), and none of them is equal to the dataset used by the victim model. The authors propose a novel method called Image Classification Eraser (ICE) to erase classification information for any encountered images from arbitrary dataset. Extensive experiments on Cifar-10, CIFAR-100, and TieredImageNet demonstrate the effectiveness of the proposed ICE on the Generalized Transferable Attack (GTA) problem."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper proposes a method to improve the robustness and efficiency of masked language modeling (MLM) pretrained language models (PrLMs) by addressing the issue of false-negative (false-negative) predictions. In particular, the paper argues that the current PrLMs simply consider all corrupted texts as negative samples, so that the resulting PrLM has to be trained on such false negatives with less efficiency and less robustness, which either waste training time on meaningless data or are vulnerable to adversarial attacks like diversity distraction and synonym substitution. The paper proposes an enhanced pre-training methods to counteract false negative predictions and encourage pretraining language models to train on true negatives, by correcting the harmful gradient updates subject to false negatives. Experimental results on GLUE and SQuAD benchmarks show that the proposed methods bring about better performance together with stronger robustness."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes a novel open-world semi-supervised learning (SSL) setting, which formalizes the notion that novel classes may appear in the unlabeled test data. The authors propose ORCA, an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes, leading to improved performance."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes a second-order method for training large-scale deep neural networks (DNNs). The proposed method, SLIM-QN, uses the BFGS update rule that directly approximates the Hessian inverse using past parameters and gradients, without explicitly constructing the matrix and then computing its inverse. To achieve stable convergence, the authors introduce momentum in Hessian updates together with an adaptive damping mechanism. The authors also provide rigorous theoretical results on the convergence of the proposed method in a stochastic setting. "
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes a method called Locality-Sensitive pruning (LSP) for graph pruning based on Locality Sensitive Hashing. LSP aims to sparsify a graph so that similar local environments of the original graph result in similar environments in the resulting sparsified graph, which is an essential feature for graph-related tasks. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of LSP."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper studies the problem of data augmentation for contrastive self-supervised learning. The authors propose a simple adversarial augmentation method that can modify training data to be hard positives/negatives without distorting the key information about their original identities. In particular, they decompose a sample x to be its variational auto-encoder (VAE) reconstruction plus the residual residual R(x) = x-G(x), where the residual retains most identity-distinctive information due to an information-theoretic interpretation of the VAE objective. Then, they adversarially perturb G(x)-in VAE’s bottleneck space and add it back to the original label. They apply this “identity-disentangled adversarial augmentmentation (IDAA)” to different self-Supervised learning methods and demonstrate its efficiency and generalization performance. "
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper studies the problem of detecting distribution shifts in the data distribution of machine learning models. In particular, the authors propose a method that can detect harmful shifts while ignoring benign ones, and allow continuous monitoring of model performance without increasing the false alarm rate. The proposed method is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. The authors demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"This paper proposes a method to infer physical parameters directly from a single video. The authors propose to use neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) in order to obtain interpretable physical models directly from visual observations. The proposed method has the following advantages: (i) it is able to identify physical parameters from only a single single video; (ii) the use of neural implicit representation enables the processing of high-resolution videos and the synthesis of photo-realistic imagery; (iii) the embedded neural ODE has a known parametric form that allows for the identification of interpretability physical parameters, and (iv) long-term prediction in state space; (v) it enables the rendering of novel scenes with modified physical parameters becomes possible."
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper considers the context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. This challenging case is often met in applications and the authors tackle it using a Bayesian approach and variational inference. They adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, and derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. They then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"This paper proposes a framework to pretrain knowledge-based multilingual language models (KMLMs). The authors first generate a large amount of code-switched synthetic sentences and reasoning based multilingual training data using the Wikidata knowledge graphs. Then based on the intra-and inter-sentence structures of the generated data, they design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. The pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by the authors, namely, logic reasoning."
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"This paper proposes an approach to train agents to support other agents by maximizing the number of states that the other agent can reach in its future states. The authors propose an agent that learns to increase the choices another agent has by preferring to maximize the number states that it can reach. They evaluate their approach on three different multi-agent environments where another agent's success depends on the altruistic agent’s behaviour. They show that their approach can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them."
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the phenomenon of double descent (DD) in finite-width neural networks. In particular, the authors derive a lower bound for the population risk of a neural network in the over-parameterized regime, and show that it diverges at the interpolation threshold. The lower bound is derived by considering the influence functions and the spectrum of the Hessian at the optimum. The authors further investigate how the loss function affects double descent and uncover interesting properties of neural networks and their Hessian spectra. "
SP:b485114712055f39a7afb951dbc3db482ff523fd,"This paper studies the asymptotic behavior of the trainability of GNNs with respect to depth and depth depth. Theoretical results show that the training dynamics of GCNs with infinite depth and infinite width can be characterized by the graph neural tangent kernel (GNTK). The authors then propose a new sampling method, Critical DropEdge (CDE), which is based on the analysis of the GNTK. The proposed method is evaluated on the node classification and link prediction tasks on CIFAR-10/100 datasets. "
SP:25a92b3583afdc6892e59f1e769125d52c8011af,This paper proposes a method to estimate the second-order dynamics of the cardiac blood volume pulse (BVP) using a video-based vital sign measurement system. The authors propose to use the second derivative of both the input frames and the target vital sign signals into the training procedure to improve the model's ability to estimate left ventricle ejection time (LVET) intervals. They show that adding second-derivative inputs also improves performance when estimating second order dynamics. 
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper studies the emergence of language in multi-agent reinforcement learning. The authors propose a method called task transfer, which maps the input to related symbols in referential games. They show that this symbolic mapping can be used to improve the success of language learning in difficult tasks. They also explore vocabulary expansion and show that with the help of symbolic mapping, agents can learn to use new symbols when the environment becomes more complex."
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper proposes a hierarchical modular approach to learn agents that navigate and manipulate objects in a divide-and-conquer manner for the diverse nature of the entailed tasks. Specifically, it first infer a sequence of subgoals to be executed based on language instructions by high-level policy composition controller (PCC). Then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies. Finally, it infer manipulation actions with the corresponding object masks using the appropriate interaction policy. The proposed agent achieves state-of-the-art performance on the ALFRED benchmark."
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes Nuisance-randomized distillation (NURD), a method to learn representations that perform well regardless of the nuisance-label relationship. The authors define a nuisance-varying family, a set of distributions that differ only in the nuisance and label relationship. NURD proposes two types of distributions, the nuisance randomized distribution and the uncorrelating distribution, and defines the set of representations that are most informative of the label under the nuisance distribution. The proposed method is evaluated on chest X-ray classification and pneumonia classification tasks."
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper proposes OTTER (Optimal TransporT distillation for Efficient zero-shot recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. The proposed method is based on pretrained image and text encoders and achieves strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images."
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, this paper simply casts object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. The proposed method achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"This paper proposes a symbolic policy distillation method for visual reinforcement learning (RL). The main idea is to distill the symbolic knowledge of the policy network into the symbolic policy, which is composed of geometric and numerical symbols and operators. The proposed method is based on a policy regression algorithm called RoundTourMix, which distills the symbolic rules as teacher-student. The symbolic policy can be treated as discrete and abstracted representations of the neural policy network, but are found to be more interpretable, robust and transferable. The method is evaluated on six visual RL tasks and compared with the state-of-the-art."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a method for unsupervised image-to-image translation, where the goal is to disentangle the coarse-level object arrangements (posture) and the fine-grained level styling (identity) of the generated image from two exemplar sources. The authors propose a Vector-Quantized Spatial Normalization (VQSN) module for the generator for better pose-identity disentanglement. The VQSN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training-set images. Second, the authors design a joint-training scheme with self-supervision methods for the GANInversion encoder and the generator to reconstruct images from two differently augmented variants of the original ones, one defining the pose and the other for identity. Comprehensive experiments conducted on various datasets show better synthesis image quality and disentangling scores."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a simple MLP-based speech processing model for keyword spotting and speech enhancement tasks. The proposed model is based on three properties of speech signals: (1) temporal invariance, (2) frequency asymmetry, and (3) short-term dependency. The authors propose to split the channels into non-overlapped chunks and process each chunk individually, merge them together, and finally combine the chunks to obtain the final output. The model is evaluated on two tasks: keyword spotting (V2-35 and LibriWords) and voice enhancement (VoiceBank). In all experiments, the proposed model outperforms transformer-based solutions with fewer parameters and lower GFLOPS."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,This paper provides a lower bound on the generalization error of any transfer learning algorithm as a function of the amount of source and target samples. The lower bound depends on a natural notion of distance that can be easily computed on real world data sets. The authors also consider a more general setting where there are more than one source domains for knowledge transfer to the target task and develop new bounds on generalisation error in this setting. Experiments on real image classification and action recognition data sets also corroborate the theoretical findings.
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. The training objective maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper proposes temporal priors as a non-Markovian generalization of behavioral priors for guiding exploration in reinforcement learning. The authors focus on the family of state-independent priors, which exploit the idea of temporal consistency and are generally applicable and capable of transferring across a wide range of tasks. They show how dynamically sampling actions from a probabilistic mixture of policy and temporal prior can accelerate off-policy reinforcement learning in unseen downstream tasks. The proposed method is evaluated in long-horizon continuous control tasks under sparse reward settings."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a learning-to-learn method for learning rate scheduling in training deep neural networks. The proposed method is based on constructing a directed graph for the underlying neural network of the target problem, which encodes current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. Besides, an efficient reward collection procedure is leveraged to speed up training. The method is evaluated on Fashion-MNIST and CIFAR10 for image classification, and GLUE for language understanding."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper proposes a method for unsupervised object-centric object segmentation from point clouds. The proposed method, SPAIR3D, is a VAE-based model that generates spatial mixture distributions on point clouds to discover 3D objects in static scenes. The authors propose a new Chamfer Mixture Loss function tailored for learning mixture models over point cloud data with a novel graph neural network that can be used to model and generate a variable number of 3D points. Experiments on the UOR and UOR-OT datasets show that the proposed method generalizes well to previously unseen scenes with a large number of objects without performance degeneration."
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper proposes a method to improve the executability of high-level language models (LLMs) by converting them into low-level actionable plans (i.e., actions that can be executed in the environment). The authors propose two methods to do so: (1) enumerate all admissible actions, (2) condition the generated actions on past actions that have been deemed admissible, and (3) translate the generated plans to the actions that are semantically similar to the query task. Experiments on the VirtualHome environment show that the proposed method improves executability over the baseline LLM baseline."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a new approach to interpret the latent space of VAEs as a Riemannian manifold. The authors show that VAEs naturally unveil a latent space with a structure that can be modeled as a manifold through the learned covariance matrices in the posterior distributions. Then, they propose a sampling scheme consisting in sampling from a uniform distribution defined on the learned manifold and given by the metric. They show that this procedure improves the generation process from a vanilla VAE and makes it able to perform as well as more advanced VAE models in terms of Frechet Inception Distance (Heusel et al., 2017) and Precision and Recall (Sajjadi et al. 2019) scores on four benchmark datasets."
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper proposes Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer- MGK accelerates training and inference, has fewer parameters, and requires less FLOPs to compute while achieving comparable or better accuracy across tasks. The authors empirically demonstrate the advantage of TransformerMGK in a range of practical applications including language modeling and tasks that involve very long sequences."
SP:82731dcce233e748f63382e09b6224a513fe9689,"This paper proposes a path integration method that fuses proprioceptive and external inputs to construct a cognitive map of a two-dimensional continuous spatial environment. The authors propose to fuse image and action related signals, allowing reconstruction of the action relating the two successive images, as well as prediction of the new image from its current value and the action. They propose a minimalistic recurrent architecture, called Resetting Path Integrator (RPI), that can easily and reliably be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the (possibly noisy) self-motion signal, and occasionally resets it when the image signal is present. The proposed method is compared to off-the-shelf LSTM networks on identical tasks and consistently shows better performance while also offering more interpretable internal dynamics and higher-quality representations."
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the role of the structure of the input distribution in the learning of effective features for neural networks. In particular, the authors consider the setting where the labels are determined by a set of class-relevant patterns and the inputs are generated from these along with some background patterns. They prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of the effective features, which are learned among exponentially many candidates efficiently by exploiting the data. The authors also show that no linear models on data-independent features of polynomial sizes can learn to as good errors if the specific input structure is removed."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper studies the robustness of deep neural networks to adversarial perturbations. In particular, the authors propose a methodology to analyze the robusts of fixed feature extractors, which in turn provides bounds on the robustnesses of any classifier trained on top of it. The tightness of these bounds relies on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers. The authors utilize their bounds to identify the layers of robustly trained models that contribute the most to a lack of robustness, and compare the same layer across different training methods."
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper proposes a method for offline reinforcement learning (RL) that learns the V-function instead of the Q-function to avoid extrapolation error. The authors propose Expectile V-Learning (EVL), which smoothly interpolates between the optimal value learning and behavior cloning. They also introduce implicit planning along offline trajectories to enhance learned V-values and accelerate convergence. They provide theoretical analysis for the convergence properties of their proposed method, and empirical results in the D4RL benchmark show that their method achieves superior performance in most tasks, particularly in sparse reward tasks."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a method for improving the robustness of neural network classifiers by reweighting the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. The authors formulate weighted adversarial training as a bilevel optimization problem, where the upper-level task corresponds to learning a robust classifier, and the lower-level function maps from a sample’s multi-class margin to an importance weight. Extensive experiments demonstrate that the proposed method improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines."
SP:3ad36be6b6900aabe43da043461cf178ce977082,"This paper proposes a non-linear message passing layers for equivariant graph neural networks. The message passing layer is composed of steerable MLPs, which is able to incorporate geometric and physical information in both the message and update functions. The node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. The proposed model is evaluated on the molecular datasets QM9 and IS2RE."
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"This paper proposes a differentiable physics model for composite materials such as cloths. The authors propose a new differentiable fabrics model where they dive into the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. They propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. They demonstrate their model’s explicability in learning meaningful physical parameters, versatility in incorporating complex physical structures and heterogeneous materials, data-efficiency in learning, and high-fidelity."
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a method for lifelong reinforcement learning (RL) based on logical composition. In particular, the authors extend the work of Nangue Tasse et al. (2020) to the discounted and stochastic tasks, and provide bounds on the performance of the transferred policy on a new task and the number of tasks that need to be learned throughout an agent’s lifetime to generalize over a distribution. Theoretical results are also provided. The authors verify their approach in a series of experiments, where they perform transfer learning both after learning a set of base tasks and after learning an arbitrary set of tasks. They also demonstrate that as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of the current task."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper proposes a distributed approach to multivariate time series classification (MTSC) based on a wavelet scattering transformation of the time series and distributed feature selection. The proposed method, called LightWaveS, is fast both during training and inference. It achieves accuracy comparable to state-of-the-art deep learning solutions, and scales well with more nodes and large number of channels. "
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a new approach to pretrain text encoders with an adversarial learning curriculum via a Mixture of Signals from multiple auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, this paper jointly trains multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, the authors learn mixture weights over the auxiliary MLMs’ outputs by backpropagating the gradient from the main encoder via Gumbel-Softmax. Experiments on the GLUE and SQuAD benchmark demonstrate the effectiveness of the proposed approach."
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper proposes an adaptive fine-tuning method for extracting relational knowledge from large pre-trained language models by a clozestyle sentence serving as a query. The proposed method, called BERTriple, uses a pretrained language model on the standard fill-mask task using a small training dataset of existing facts from a knowledge graph. Experiments show that the proposed method outperforms all baselines, even by using significantly fewer training facts. "
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper proposes to learn the knowledge base embeddings in different geometric spaces and apply manifold alignment to align the shared entities. The proposed approach is evaluated on the out-of-taxonomy entity typing task, where the goal is to predict the types of the entities from the knowledge graph. Experimental results on two datasets based on YAGO3 demonstrate that the approach has significantly good performances, especially in low dimensions."
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a one-shot learning framework for link prediction in temporal knowledge graphs. The proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. The experiments show that the proposed algorithm outperforms the state of the art baselines for two well-studied benchmarks while achieving significantly better performance for sparse relations."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes Progressive Module Networks (PMN), a framework for multi-task learning by progressively designing modules on top of existing modules. The main idea is to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. The modules communicate by learning to query other modules and process their outputs, while the internal module processes are a blackbox. This is similar to a computer program that uses available libraries without having to know their internal operations. The proposed approach is evaluated on a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper proposes a method to improve the efficiency of convolutional neural networks (CNNs) by pruning and rewiring channels. The proposed method is based on the idea that identity connections (i.e., residual connections) preserve information from the previous layer. The authors propose two methods to prune the channels and rewire the parameters of the convolution layer. Experiments on CIFAR-10/100 and ImageNet show that the proposed method improves the model compression and accuracy."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper considers the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). The authors propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. The method outperforms previous methods for PU learning on various real-world datasets."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes to use cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss. The authors show that their approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and Atari games."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric framework to analyze the high-dimensional geometry of adversarial examples. In particular, the authors highlight the importance of codimension: for low-dimensional data manifolds embedded in high dimensional space there are many directions off the manifold in which to construct adversarial perturbations. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low dimensional data manifold well, but classifies points near the manifold incorrectly. The authors prove that a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial learning are robust."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes a method for learning interpretable representations of time-series data. The proposed method is based on the self-organizing map (SOM) algorithm, which maps states from an uninterpretable continuous space to a lower-dimensional space with a predefined topologically interpretable structure, such as an easily visualizable two-dimensional grid. The authors propose a gradient-based method to overcome the non-differentiability in discrete representation learning, and integrate a probabilistic Markov model in the representation space, which provides additional interpretability and provides a natural representation of uncertainty. Experiments on synthetic and real-world data show that the proposed method outperforms baselines."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper investigates the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. The authors show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or by using non-linear interpolations. They prove that there is a trade-off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. They also provide a general method that is easily applicable to many commonly used latent distributions."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes to learn the parameters of shallow networks in hyperbolic space. Specifically, the authors propose to use the geometry of embeddings of object representations in the embedding space to compute the attention mechanisms for different neural networks architectures. The authors show that this approach can improve the generalization of the proposed method on neural machine translation, learning on graphs (both on synthetic and real-world graph tasks), and visual question answering (CLEVR)."
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper studies the problem of deep neural network (DNN) fingerprinting attacks that exploit cache side-channels. The authors define the threat model for these attacks: the adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine where the victim’s deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. They introduce DeepRecon, an attack that reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload. They demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, the attacker can build a meta-model that accurately fingerprints the architecture and family of the pretrained model in a transfer learning setting. They also evaluate the importance of the observed attributes in the fingerprinting process. Finally, they propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a hierarchical network model for video prediction. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. It assumes that spatiotemporal memories are encoded in the recurrent connections within each level and between different levels of the hierarchy. The network learns by comparing the incoming signals with its prediction, updating its internal model of the world by minimizing the prediction errors at each level in the style of predictive self-supervised learning. It processes data in blocks of video frames rather than a frame-to-frame basis."
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper proposes a method to learn gene-like representations from raw RNA-seq data, in a reference-free fashion. The authors propose to learn a latent space that captures information of both DNA sequence similarity and DNA sequence abundance in the embedding latent space. They show that this latent space allows the detection of genomic abnormalities such as translocations as well as patient-specific mutations."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes a method for model compression based on the architecture space, i.e., encoding the network and then performing gradient descent in continuous space to optimize a compression objective that maximizes accuracy and minimizes parameter count. The encoder/decoder is trained to learn a mapping from discrete architecture space to a continuous embedding and back, and jointly trained to regress accuracy and parameter count in order to incorporate information about the architecture’s effectiveness on the dataset. The final continuous feature is then mapped to a discrete architecture using the decoder. The authors demonstrate the merits of this approach on visual recognition tasks such as CIFAR-10/100, FMNIST and SVHN."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper proposes a framework for continual acting and learning based on the synergistic relationship between local model-based control, global value function learning, and exploration. Specifically, the authors study how local trajectory optimization can cope with approximation errors in the value function, stabilize and accelerate the learning process. They also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, they demonstrate how trajectory optimization is used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper proposes an approach to zero-shot machine translation between language pairs for which there is no aligned data. The approach builds upon the multilingual NMT system (Johnson et al., 2016) by applying reinforcement learning, using only monolingual data on the zero shot translation pairs, inspired by dual learning (He et al. 2016). Experiments on the UN corpus show that the proposed approach outperforms the existing unsupervised NMT baseline model on in-domain and out-of-domain evaluation."
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper provides an analysis of IRGAN, a generative adversarial network (GAN) framework for information retrieval (IR) tasks. The authors point out issues in the formulation of the loss function used in IRGAN and propose a co-training scheme, where the generator and discriminator are trained in an adversarial fashion, and the discriminator is trained with a minimax loss function. The proposed scheme is evaluated on two tasks, web-search and question answering, and compared to the original IRGAN."
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"This paper proposes a VAE-based method for learning sparse representations. The authors propose a Spike and Slab prior to induce sparsity in the latent space of VAEs, allowing approximate variational inference with sparse coding and probabilistic inference with arbitrarily complicated sparse coding models. They show that the resulting sparse representations are advantageous over standard VAE representations on two benchmark classification tasks (MNIST and Fashion-MNIST) by demonstrating improved classification accuracy and significantly increased robustness to the number of latent dimensions. They also demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation."
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper proposes a non-adversarial feature matching-based approach to train generative models. The proposed approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. The experimental results demonstrate that the proposed approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"This paper studies the expressive power of graph neural networks (GNNs) in terms of the Weisfeiler-Lehman (WL) graph isomorphism test. The authors show that GNNs can be thought of as an aggregation function over the multiset of features and characterize the discriminative power of several GNN variants. They then propose a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the WL test. They empirically validate their theoretical findings on a number of graph classification benchmarks, and demonstrate that their model achieves state-of-the-art performance."
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"This paper proposes a framework for interpretable continual learning (ICL) that uses saliency maps to provide explanations of previously performed tasks and proposes a new metric to assess the quality of the explanations. ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy."
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a new evaluation framework for adversarial attacks on seq2seq models taking meaning preservation into account. The authors also propose new constraints for attacks on word-based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. Finally, the authors show that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper proposes a method to improve the performance of Q-learning, Double-Q-learning and on-policy actor-critic by combining the policies using original rewards and inverse (negative) rewards. The authors prove the convergence of the inverse policies and show that the hybrid policies based on the proposed method obtain the rewards up to 63.8%, 97.8% and 54.7% more than the original algorithms. The improved policies are also more stable than original policies. "
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper proposes a method for learning object-based representations and disentangled object representations from videos. The proposed method, called Part, Structure, and Dynamics (PSD), learns to recognize the object parts via a layered image representation, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure, and model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that the proposed PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure and capturing their motion distributions."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"This paper proposes a generative classifier that can be applied to any softmax neural classifier pre-trained on noisy datasets. The main idea is to use the minimum covariance determinant estimator (LDA) to estimate the parameters of the classifier on top of the hidden feature spaces of the discriminative deep model. The proposed method can improve the classification accuracy, with no re-training of the deep model nor changing its architectures. In addition, the proposed method is robust against adversarial perturbations due to its large margin property."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes a method for hierarchical reinforcement learning (HRL) that uses incremental unsupervised learning over a small memory of the most recent experiences of the agent. The method learns subgoals and skills together, together with an intrinsic motivation learning mechanism, based on experiences in the environment. The authors demonstrate the efficiency of their method on two RL problems with sparse delayed feedback."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a neural framework for learning to solve the Circuit Satisfiability problem. The framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement learning and trains the model directly toward solving the SAT problem. Experimental results show the superior out-of-sample generalization performance of the framework."
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes a combination of deep neuroevolution and deep reinforcement learning (deep RL) algorithms to improve the sample efficiency of off-policy deep RL algorithms. In particular, the authors combine the cross-entropy method (CEM) and Twin Delayed Deep Deterministic Policy Gradient (TD3) to obtain the proposed CEM-RL algorithm. The authors evaluate the proposed method on the SWIMMER-V2 benchmark and show that it outperforms the state-of-the-art deep RL methods. "
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes an interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) for multi-variate forecasting and knowledge extraction. The proposed model is equipped with hidden state matrix and update process, so as to learn variables-wise hidden states. The authors also develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Extensive experiments using real datasets demonstrate the prediction performance and interpretability of the proposed model."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes a data augmentation method called feature smoothing to improve the adversarial robustness of neural networks. The proposed method is based on the idea of interpolating features from a pair of samples, with the new label remaining the same as the dominant data point. The paper also proposes a unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. The authors show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up, and features smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance."
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for deep and locally connected ReLU networks. The framework is built upon teacher-student setting, by projecting the student's forward/backward pass onto the teacher's computational graph. This framework could help facilitate theoretical analysis of many practical issues, e.g. disentangled representations in deep networks. "
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper proposes a modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy. This approach allows efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states. In the experiment with video games playing, the results show that the proposed method allows separation of main task’s objectives and behaviors between different BMs. The experiments also show network extendability through independent learning of new behavior patterns."
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper proposes a differentiable formulation for the neuromodulation of plasticity, which extends previous work on differentiable Hebbian plasticity (Miconi et al., 2017; Miconi, 2017; Ellefsen et al. 2015; Velez & Clune, 2017). The authors claim that this formulation offers a powerful new framework for training neural networks. In particular, the authors show that the proposed formulation can improve the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, the proposed framework outperforms standard LSTMs on a benchmark language modeling task."
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper proposes a method for training quantized neural networks. The proposed method consists of two phases: phase1 trains the full precision model with quantization and phase2 trains the binary model constructed by phase1. The authors also propose a new loss function to regularize the weights, resulting in reduced quantization error. Experiments on CIFAR and ImageNet datasets show the effectiveness of the proposed method."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a method for style transfer onto open-ended content, i.e., the method generalizes to style and content not present in the training data. The method starts by constructing a content embedding using an existing deep metric-learning technique. This trained content encoder is incorporated into a variational autoencoder (VAE), paired with a to-be-trained style encoder. An auxiliary loss, leakage filtering, ensures that no style information remaining in the content representation is used for reconstruction and vice versa. The proposed method is evaluated on few-shot learning tasks."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a method to speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). The proposed method is based on the idea that after an action at is performed in a state and the agent reaches the new state st+1, the agent can decide whether the action is permissible or not permissible in state st. An action is not permissible if the action can never lead to an optimal solution and thus should not be tried. The authors incorporate the proposed SAP property into two state-of-the-art deep RL algorithms and show that the proposed method can markedly speed up training."
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper studies the behavior of random deep weight-tied autoencoders under the assumption of random weights. The authors provide a precise characterization in the limit of large dimensions, which reveals interesting phase transition phenomena when the depth becomes large. They also show that deep autoencoder display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes a new black-box adversarial attack method that is based on a simple intuition: If the distance to a decision boundary is very small, we don’t have to be too careful about the exact direction along which we traverse. The proposed method can be used for targeted and untargeted attacks, resulting in previously unprecedented query efficiency in both settings. It requires a median of 600 queries (ResNet-50) to produce an adversarial ImageNet image, and it successfully attacks Google Cloud Vision with 2500 queries, averaging to a cost of only $3 per image."
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,This paper proposes a method for option discovery in hierarchical reinforcement learning (HRL). The idea is to use successor representations (SR) to learn the sub-goals and the intra-option policies. The method is evaluated on grid world and high-dimensional tasks. The results show that the method outperforms baselines.
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper studies the problem of domain division, which aims to segment instances drawn from different probabilistic distributions. In particular, the authors propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, are introduced to uncover and fine-tune the decision boundary of each domain, and the uncertain domain is newly introduced in the framework to adopt those instances whose domain labels cannot be predicted confidently. Extensive experiments demonstrate that the proposed approach achieves the state-of-the-art performance on OSL and G-ZSL benchmarks."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes polar prototype networks, a class of networks that explicitly states the layout structure, i.e., the layout, of the output. The structure is defined by polar prototypes, points on the hypersphere, which are described by a single polar prototype for classification and equal shares for regression. For regression, training can be performed as a polar interpolation between two prototypes, arriving at a regression with higher-dimensional outputs. Experiments on MNIST and CIFAR-10 show that the proposed method is able to achieve comparable or better performance than the baseline methods."
SP:d1034342785d133cf8372b8624897963cc2ee83a,"This paper proposes a reward learning algorithm for deep reinforcement learning (RL) based on the observation that the initial state of the environment is already optimized for what humans want, and can be used to infer human preferences. The paper proposes an algorithm, Reward Learning by Simulating the Past (RLSP), which learns a reward function from initial state based on Maximum Causal Entropy (MCE) IRL. The proposed algorithm is evaluated on a suite of proof-of-concept environments to show the properties of RLSP."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed method combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, the latent variable space of a variational autoencoder (VAE) is expressed in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variables values. The authors validate the framework in extensive experiments on MNIST, Omniglot, and CIFAR-10."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,"This paper proposes to use dynamical neural networks (DNNs) to solve dictionary learning problems such as l1-minimizing sparse coding and dictionary learning. The main idea is to use spiking neurons to construct the dynamical network, which is a set of neurons that interact over time continuously. The authors show that the true gradients for learning are provably computable by individual neurons using only local information. They also provide a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning tasks."
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,This paper proposes an end-to-end convolutional neural network for lane detection. The main idea is to use multiple encoder-decoders modules to extract the spatial information and pinpoint the localization of the lane. The proposed method is evaluated on the CUlane dataset.
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper proposes a new approach for batch contextual bandit learning, where the goal is to learn a policy that takes good actions in possibly unseen contexts. The proposed approach, Maximum Likelihood Inverse Propensity Scoring (MLIPS), estimates a maximum likelihood surrogate policy based on the logged action-context pairs, and then uses this surrogate policy as the proposal in inverse propensity weights. The authors prove that MLIPS is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared error than the IPS estimator. Empirical evaluations also suggest the effectiveness of these approaches on policy evaluation and optimization."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"This paper proposes a meta-learning method for few-shot image classification. The authors propose to learn how to create an individualized feature embedding specific to a given query image for better classifying. Specifically, they introduce a kernel generator as meta-learner to learn to construct feature embeddings for query images. The kernel generator acquires meta-knowledge of generating adequate convolutional kernels for different query images during training, which can generalize to unseen categories without fine-tuning. The proposed method is evaluated on Omniglot and miniImageNet."
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper proposes a gradient-free, population-based genetic algorithm (GA) for deep reinforcement learning (RL). The authors show that the proposed method, called Deep GA, is able to evolve the weights of a DNN with a simple, gradient free, population based genetic algorithm and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA is faster than ES, A3C, and DQN (it can train Atari in ∼4 hours on one workstation or ∼1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique. "
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observations from those in memory — which incorporates rich information about environment dynamics. This allows us to overcome the known “couch-potato” issues of prior work — when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. The proposed method outperforms the state-of-the-art curiosity method ICM."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper proposes a method for modeling transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain in which a robot pushes stacks of objects."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a novel feature selection method for instance-wise feature selection. The proposed method, called INVASE, consists of 3 neural networks, a selector network, a predictor network, and a baseline network, which are used to train the selector network using the actor-critic methodology. The selector network is trained to minimize a KL divergence between the full conditional distribution and the selected-features-only conditional distribution of the outcome. Experiments on a mixture of synthetic and real-word data show that INVASE significantly outperforms state-of-the-art benchmarks."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a domain adaptation method for semantic segmentation. The authors propose to learn discriminative feature representations of patches based on label histograms in the source domain, through the construction of a disentangled space. Then, they use an adversarial learning scheme to push the feature representations in target patches to the closer distributions in source ones. Experiments are conducted on various benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper proposes two new optimisation algorithms for deep neural network training based on the observation that the mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature, the authors propose two new optimistic algorithms for AMSGrad and Adam, respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in OPTIMISTIC ONLINE LEARNING, which leads to speed up in training deep neural nets in practice. Experiments show that the proposed algorithms are faster than the baselines."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper introduces two new datasets for evaluating the robustness of image classifiers to common perturbations. The first one, called ImageNet-C, is for corruption robustness, and the second one is for perturbation robustness. The authors also introduce a total of three methods and architectures that are shown to improve the performance of the models on the first one. "
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper shows that dropout training can be understood as performing MAP estimation concurrently for an entire family of conditional models whose objectives are lower bounded by the dropout objective. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully-stochastic dropout. The deterministic subvariant’s bound is equal to its objective, and the highest among these models. It also exhibits the best model fit in the experiments."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a robust pruning method for convolutional neural networks (CNNs) by combining global and soft pruning strategies. Global pruning measures the global redundancy of the filter in the whole model by using the soft strategy. In addition, in the model recovery process after pruning, the authors use the cumulative saliency strategy to improve the accuracy of pruning. The authors also propose a reasonable normalization formula to prevent certain layers of filters in the network from being completely clipped due to excessive pruning rate."
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a cross-lingual document classification framework (CACO) between related language pairs. To best use limited training data, the authors jointly train a character-based embedder and a word-based classifier. The embedder derives vector representations for input words from their written forms, and the classifier makes predictions based on the word vectors. The joint character representation allows the embedder to generalize knowledge about source language words to target language words with similar forms. The authors also propose a multi-task objective that can further improve the model if additional training data is available."
SP:544e421f9c747640d949f433e3091763508b7237,This paper proposes a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. The authors also propose a fast algorithm to reduce the complexity of constructing MAA from O(2 ) to O(T). Extensive experiments on two large-scale video datasets show that the proposed MAAN achieves a superior performance on weakly-supervised temporal action localization.
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper proposes a method for disentangling word-level and chunk-level representations of natural language using Holographic Reduced Representation (HRR). HRR is used to represent and manipulate structures. The authors claim that HRR provides an inductive bias towards the learning of disentangled representations, which roughly corresponds to a division between syntax and semantics. The proposed method is evaluated on the Penn Treebank (PTB) and a subset of One-Billion-Word (1B) data sets."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,This paper studies the problem of joint active perception and planning in partially observable Markov decision processes (POMDPs). The authors propose a greedy strategy for observation selection that aims to minimize the uncertainty in state. They also develop a novel point-based value iteration algorithm that incorporates the greedy strategy to achieve near-optimal uncertainty reduction for sampled belief points. The proposed solver enables the agent to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning.
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"This paper proposes a curriculum loss for training deep neural networks. The authors claim that the distribution shift in the weights of the top layers of the network during the backward pass is similar to the covariate shift of the forward pass, and that training hard examples aggravates the distribution shifting and damages the training. To address this problem, the authors propose a two-part curriculum loss that consists of an adaptive weight that mitigates large early punishment and an additional representation loss for low-weighted samples. The first part is the adaptive weight, which assigns small values to hard examples, reducing the influence of noisy gradients. The second part is a representation loss, which encourages the low-weights to learn better representations from its superior neighbours."
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper proposes a method to learn heuristics for combinatorial optimization problems. The authors propose a model based on attention layers with benefits over the Pointer Network and show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which they find is more efficient than using a value function. They significantly improve over recent learned heuristic for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. They also learn strong heuristic for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP)."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper proposes a differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization. The proposed method is based on constructing a super net whose macro architecture (number of layers, filter size of each layer, etc.) is the same as the target network. Each layer of super net contains edges representing convolution operators with quantized weights and activations with different precisions. Experiments show the quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models."
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"This paper proposes a new attention mechanism for sequence to sequence learning. The main idea is to factorize the joint distribution of the attention and output variables in a sequence prediction task. The output distribution is conditioned on the output, and the attention is propagated to the next decoding stage as a posterior attention distribution conditioned on this output distribution. The proposed approach is evaluated on 5 translation and 2 morphological inflection tasks. "
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper proposes a method for the unpaired image-to-image translation problem. The proposed method, called HarmonicGAN, aims to learn bi-directional translations between the source and the target domains by introducing a smoothness term over the sample graph to attain harmonic functions to enforce consistent mappings during the translation. The method is evaluated on medical imaging, object transfiguration, and semantic labeling tasks."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"This paper studies the exploding and vanishing gradient problem (EVGP) of LSTM. The authors show that when the weights are large, the gradient components through the linear path (cell state) in the computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which the authors show empirically), their suppression can prevent LSTMs from capturing them. To address this problem, the authors propose a simple stochastic algorithm (h-detach) that is specific to LstM optimization and targeted towards addressing this problem. The proposed algorithm prevents gradients flowing through this path from getting suppressed, thus allowing the LSTm to capture such dependencies better. The experimental results show that the proposed method can improve the convergence speed, robustness to seed and learning rate."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a method for training binary weight networks (BWN) from scratch under the Bayesian deep learning perspective. The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, it generates binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. The proposed method is evaluated on several visual recognition tasks including ImageNet."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a probabilistic federated learning framework for neural networks. The proposed method trains local models for each data source, in parallel, and then matches the estimated local model parameters (groups of weight vectors) across data sources to construct a global network. The matching is governed by the posterior of a Beta-Bernoulli process (BBP), a Bayesian nonparametric model that allows the local parameters to either match existing global ones or create a new global parameter if existing ones are poor matches. The construction allows the size of the global network to flexibly grow or shrink as needed to best explain the observed data."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper proposes a stable variant of LOLA (learning with opponent-learning awareness) for non-convex n-player games. Theoretically, the authors prove that LOLA converges to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally. "
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes a VAE-based alarm system to predict the quality of segmentation results for medical segmentation tasks. The proposed method is based on shape feature, which is a strong prior information shared among different data. The shape feature is captured using the value of loss function when the segmentation result is tested using a Variational Auto-Encoder (VAE). The VAE is trained using only the ground truth masks, therefore the bad segmentations results with bad shapes become the rare events for VAE and will result in large loss value. Finally, the representation in the one-dimensional feature space is learned by learning the classifier/regressor in the feature space. The method is evaluated on several segmentation algorithms and shows consistent performance."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper proposes an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The network is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper presents an end-to-end approach to program synthesis from natural language specifications. The authors propose a neural network architecture that is capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed in (Polosukhin & Skidanov, 2018) and produces correct programs in over 92% of cases."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper studies the adversarial robustness of deep neural networks on the MNIST dataset. The authors show that even the widely recognized and by far most successful L∞ defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization, and (4) features adversarial perturbation that make little sense to humans. Based on these observations, the authors propose a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, the authors propose a new reparameterization approach for the weights matrices in the discriminators of the generator and the discriminative neural network. Theoretically, this paper shows that the spectrum control improves the generalization ability of GAN. The experiments on CIFAR-10, STL-10 and ImgaeNet datasets confirm that the proposed method is capable of generating images with competitive quality."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"This paper proposes an accelerated value iteration method for reinforcement learning. The proposed method is based on Anderson Accelerated Value Iteration (A2VI), which is an Anderson-mixing method for value iteration. A2VI can be viewed as an approximation of the policy evaluation by interpolating on historical data. The authors also extend the proposed method to deep Q-learning (DQN) and show that it is more efficient than the modified policy iteration (MPI) method for policy evaluation."
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper proposes a method, SupportNet, to solve the catastrophic forgetting problem in the class incremental learning scenario. SupportNet combines the strength of deep learning and support vector machine (SVM), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The authors validate the method with comprehensive experiments on various tasks, which show that SupportNet drastically outperforms the state-of-the-art incremental learning methods."
SP:d228d213f79716774043cea253305fecece659ec,"This paper compares different measures of unit selectivity in the hidden representations of AlexNet, a recurrent neural network (RNN) trained on a single task. The authors compare four measures, namely localist selectivity, precision, class-conditional mean activity selectivity (CCMAS), precision (Zhou et al., 2015), and CCMAS; Morcos et al. (2018), and a new measure called top-class selectivity. They find that there are no 100% selective ‘localist units’ in AlexNet. They also show that the precision measure overestimates the level of selectivity and the CCMAS measure has a much higher level than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. Finally, they also generate activation maximization (AM) images that maximally activated individual units and found that under (5%) of units in fc6 and conv5 produced interpretable images of objects, whereas fc8 produced over 50% interpretability images."
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper proposes a line graph neural network (LGNN) for solving node-wise community detection problems in a supervised learning setting. In particular, the authors propose to augment GNNs with the non-backtracking operator defined on the line graph of edge adjacencies. They show that, in a data-driven manner and without access to the underlying generative models, they can match or even surpass the performance of the belief propagation algorithm on binary and multiclass stochastic block models, which is believed to reach the computational threshold in these cases."
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper studies the problem of online dictionary learning (DL), where the goal is to learn a sparse linear model of the input data, i.e., a linear combination of a few columns of a dictionary, where the sparse weights forming the linear combination are known as coefficients. The authors propose a simple Neurally plausible alternating Optimization-based Online Dictionary Learning (NOODL) algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. The algorithm is scalable and amenable for large scale distributed implementations in neural architectures."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,This paper proposes a new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. The loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. The training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. The authors demonstrate that these techniques provide large improvements to a similarity search tasks.
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"This paper proposes a method for neural architecture search (NAS) based on hypernetworks. In particular, the authors propose a method called Graph HyperNetwork (GHN) that predicts the parameters of unseen networks by directly operating on their computational graph representations. The method is evaluated on CIFAR-10 and ImageNet and achieves competitive results with state-of-the-art NAS methods. The authors also extend the proposed method to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff."
SP:65ccf43cd4e033d22239069057f5200d49f33724,This paper proposes a method to improve generative adversarial imitation learning by using additional information from non-expert demonstrations which are easier to obtain. The key idea of the method is to perform multiclass classification to learn discriminator functions where the demonstrations are regarded as being drawn from an extra class. Experiments on continuous control tasks demonstrate that the proposed method learns better policies than the generative imitation learning baseline when the number of expert demonstrations is small.
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper proposes an approach to learn the inverse of the forward process of a neural network, i.e., the mapping from inputs to outputs. The proposed approach is based on the idea of invertible neural networks (INNs), where the mapping is bijective (i.e. its inverse exists), both forward and inverse mapping are efficiently computable, and both mappings have a tractable Jacobian, which allows explicit computation of the posterior probabilities. The authors show that the posterior of an INN can be estimated theoretically in the asymptotic limit, and empirically on synthetic and real-world data from medicine and astrophysics. "
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a method for uncertainty quantification in deep neural networks (NNs). The proposed method builds on the recent work of Lakshminarayanan et al. (2017) who showed that using an ensemble of NNs trained with a proper scoring rule leads to results comparable to those of Bayesian NNs. The authors propose to replace the fixed mixing weights by an adaptive, input-dependent distribution (specifying the probability of each component) represented by an NN, and by considering uncountably many mixture components. The resulting model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks (CDNs)."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper proposes a new compression method for deep neural networks. The main idea is to use a full variational distribution over weights instead of using deterministic weights, which allows for more efficient coding schemes and consequently higher compression rates. The proposed method is based on the bits-back argument, which states that, assuming a large dataset and a neural network equipped with a weight-prior p, the effective coding cost of the network weights is KL(q||p) = Eq[log qp], where q is the weight of the neural network and p is a random sample from the variational posterior. The authors show that such a coding scheme always exists and that the bits back argument indeed represents a theoretical lower bound its coding efficiency, and propose a practical scheme which produces an approximate sample and derives a distribution message encoded with this scheme."
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper proposes ProxylessNAS, a differentiable neural architecture search (NAS) algorithm that directly learns the architectures for large-scale target tasks and target hardware platforms. The main contributions of this paper are the following: (1) It proposes a proxy-less approach Architecture Updates Architecture Updates Target Task & Hardware Learner Normal Train NAS. (2) It addresses the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. (3) It also removes the restriction of repeating blocks in previous NAS works and allow all of the blocks to be learned and specified blocks can be trained and specified hardware metrics can be specified. "
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper proposes to modify the linear penalties to second-order ones, and argues that this results in a more practical training procedure in non-convex, large-data settings. For one, the use of secondorder penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding the instability and potential lack of convergence associated with two-player min-max games. Secondly, the authors derive a method for efficiently computing the gradients associated with the second order penalties in stochastic mini-batch settings. The resulting algorithm performs well empirically, learning an appropriately fair classifier on standard benchmarks."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"This paper proposes a reweighted wake-sleep (RWS) algorithm for learning deep generative models with discrete latent variables. The authors show that RWS outperforms current state-of-the-art methods in learning discrete latent-variable models. RWS is evaluated on the Attend, Infer, Repeat (AIR) model on MNIST and a pedagogical GMM model on CIFAR-10. The results show that the RWS algorithm outperforms IWAE and REINFORCE in terms of the ELBO."
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper proposes to use truncated randomized search to train structured prediction energy networks (SPENs), which provide efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction. In particular, the authors show that this truncated random search yields previously unknown local improvements, providing effective supervision to SPENs, avoiding their traditional need for labeled training data. "
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper proposes an active learning approach for robust policy search (RPS). In particular, the authors propose a framework, EffAcTS, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset. They apply this framework to an existing method, namely EPOpt, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control tasks. They also present a Multi-Task Learning perspective to the problem of Robust Policy Search, and draw connections from the proposed framework to existing work on Multi-task learning."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper proposes a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. The authors prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation."
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target to execute, and updates the model based on new observations. The proposed approach outperforms strong baselines that do not explicitly plan using the semantic content."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a method to improve the generalization ability of end-to-end deep learning-based driving models. The proposed method consists of two modules: perception module and driving module. The perception module is used for learning easier driving-related perception knowledge, i.e., ability of pixel level understanding of input including what & where and how far knowledge. The driving module is trained to do simpler driving tasks before generating commands for diffult-related driving tasks. Experiments are conducted to show the effectiveness of the proposed method."
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the trade-off between adversarial robustness and standard generalization in deep learning. The authors show that adversarially robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. Further, they argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"This paper proposes a method for training deep neural networks without back-propagation. The proposed method builds on top of the Equilibrium Propagation method proposed by Scellier & Bengio (2017), which uses only local learning rules and does not rely on neurons having a mechanism for back-predicting an error gradient. The main limitation of the proposed method is that inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. The authors propose to train a feedforward network to approximate the state of the fixed point using a local learning rule, and then use this network for inference. The experiments show that this network appears to work as well or better than the original Equilibrium propagation while requiring fewer steps to converge."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper proposes a zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or better convergence speed than SGD-type algorithms. The convergence rate is shown to be $\sqrt(d/\sqrt{T})$ under some mild conditions, where d is the number of optimization variables. In addition, the authors also analyze the effects of different types of gradient estimators on the convergence and propose several variants of the proposed algorithm. The application side of the paper explores the connection between the proposed method and black-box adversarial attacks in robust deep learning."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper proposes a new optimization method to reduce the computation efforts of convolutional neural networks. The proposed method takes advantage of the fact that some convolution operations are actually wasteful since their outputs are pruned by the following activation or pooling layers. The authors propose to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. Furthermore, a fine-tuning process is conducted to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example model and Network in Network on the Cifar-10 and CifAR-100 datasets."
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper proposes a method to improve the robustness of automatic speech recognition (ASR) systems against adversarial attacks by exploiting the temporal dependency in audio data to gain discriminate power. The proposed method is based on input transformation developed from image adversarial defense. The authors show the effectiveness of the proposed method on both the LIBRIS (Graetz et al. 1986) and Mozilla Common Voice datasets against three state-of-the-art attack methods (Carlini & Wagner, 2018; Alzantot et al., 2018; Yuan et al, 2018) considered in the experiments."
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes to incorporate object compositionality into the generator of generative adversarial networks (GANs) by explicitly considering objects and their relations explicitly, and generate images by means of composition. This provides a way to efficiently learn a more accurate generative model of real-world images, and serves as an initial step towards learning corresponding object representations. The authors evaluate their approach on several multi-object image datasets and find that the generator learns to identify and disentangle information corresponding to different objects at a representational level. A human study reveals that the resulting generative models is better at generating images that are more faithful to the reference distribution."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper proposes a method for learning disentangled representations from visual data, where different high-level generative factors are independently encoded. The authors propose a learning setting which they refer to as “reference-based disentangling”: given a pool of unlabelled images, the goal is to learn a representation where a set of target factors are disentanglement from others. The only supervision comes from an auxiliary reference set, which contains images where the factors of interest are constant. The proposed method, called reference-based variational autoencoders, is a novel deep generative model designed to exploit the weak supervisory signal provided by the reference set."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper proposes a meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control. The authors propose an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again."
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper studies the problem of distributed training of RNN-based RL agents from distributed prioritized experience replay. The authors show the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. The resulting agent, Recurrent Replay Distributed DQN (R2D2), quadruples the previous state of the art on Atari-57, and matches the state-of-the-art on DMLab-30."
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper proposes a hierarchical generative model for modeling coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. The proposed approach is inspired by recent work on leveraging programmatically produced weak labels, which they extend to the spatiotemporal regime. They show how to instantiate their framework to effectively model complex interactions between basketball players and generate realistic multiagent trajectories of basketball gameplay over long time periods. They validate their approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts."
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper presents a method for state estimation and future forecasting in videos. The proposed method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world and to forecast future states. The method outperforms various baselines on two sports datasets."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"This paper proposes a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. The base network is trained by approximating the black- box functionality with a differentiable neural network in a way that drives the base network to comply with the black box function interface during the end- to-end optimization process. At inference time, it replaces the differentiable estimator with its external black box non-differentiable counterpart such that the output of the network matches the input arguments of the existing black box functions. "
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,This paper proposes a meta-learning algorithm that learns a mixture of hierarchical Bayesian models over the parameters of an arbitrary function approximator such as a neural network. The authors propose a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent and a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. The experiments demonstrate better generalization performance on the standard miniImageNet benchmark for 1-shot classification.
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes an unsupervised method for learning auxiliary tasks to improve the generalization performance of the principal task. The auxiliary tasks are defined as hierarchical sub-class image classification tasks, and the goal is to find the auxiliary tasks that maximise the generalisation performance across a validation validation set. The proposed method, called Meta Auxiliary Learning (MAXL), learns to generate auxiliary tasks which, when trained alongside a principal task in a multi-task setup, maximizes the generalizability of the validation set across a set of validation tasks. The experiments on three different CIFAR datasets show that MAXL outperforms baseline auxiliary learning methods and is competitive even with a method which uses human-defined sub-classes."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,"This paper proposes a method for open set recognition, i.e., the recognition of malware samples as well as images. The authors propose a neural network based representation and a loss function that utilizes this representation for performing open-set recognition. The proposed method is evaluated on three datasets from two different domains. The results show that the proposed method achieves statistically significant improvement over existing methods."
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of training low-precision convolutional neural networks. The authors show that the accuracy of the proposed method is comparable to that of the full precision baseline networks after one epoch of fine-tuning. They also demonstrate that the weights of the low precision networks are very close (in cosine similarity) to the corresponding baseline networks, making training from scratch unnecessary. They further show that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. "
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"This paper proposes a method to predict post-bounce trajectories and infer surface properties of objects in a scene. The method is based on the observation that humans exploit both visual recognition and direct physical interactions to estimate the physical properties of object in the world around them. The proposed method learns from the collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. The authors show on the newly collected dataset that their model out-performs baselines, including trajectory fitting with Newtonian physics, in predicting post-bucket trajectories. "
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper shows that the adversarial vulnerability of neural networks increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, they prove that the `1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Extensive experiments confirm that their conclusions still hold after usual training."
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper proposes a method for interactive agent modeling by encouraging an agent to learn to probe. In particular, the probing agent learns to interact with the environment and with a target agent (i.e., a demonstrator) to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used for enhancing the agent modeling to learn a more accurate mind model of the target agent. The framework consists of two learning processes: i) imitation learning for an approximated agent model and ii) pure curiosity-driven RL for an efficient probing policy to discover new behaviors that otherwise can not be observed."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to ANNs that mimics the function of biological neuromodulators and are termed modulators, which enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns. In this manner, it enables the slope of the activation function to be context dependent. This modification produces statistically significant improvements in the context of convolutional neural networks and LSTM cells."
SP:287a577834fd2820a939a1113b39146a22727491,"This paper presents a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. NANSY does not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, which allows for fully self-supervised training. Experiments on zero-shot voice conversion, pitch shift, and time-scale modification demonstrate the effectiveness of the proposed method."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"This paper studies the generalization properties of gradient-based unrolled differentiation (UD) methods for hyperparameter optimization. The main contribution is to present a notion of uniformly stable (in expectation) and an expectation bound of UD algorithms (with stochastic gradient descent) on the outer level and validation data as follows: (1) Expected risk of UD on validation set (2) and O(1) + O(K) where T and K are the numbers of steps in the inner level and outer level respectively, respectively, (3) and (4) regularization terms in both the outer and inner levels. The authors also present an expectation upper bound for the classical cross-validation algorithm (CV) and show that it is better than the gradient based methods."
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper proposes a new approach to transfer knowledge from a teacher to a student via knowledge distillation. The main idea is to train teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. The authors propose a two-stage learning procedure: student-aware training of teacher network followed by distillation from teacher to student. The proposed approach is applicable to diverse architectures of teacher and student while it can be incorporated into various knowledge-distillation algorithms. The experimental results demonstrate the effectiveness of the proposed approach. "
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies domain generalization to out-of-distribution (OOD) data, i.e., generalization from the training data to the test data. In particular, the authors introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. The authors prove that OOD generalization largely depends on the expansion function. As recently pointed out by [21], any OOD learning algorithm without a model selection module is incomplete. Extensive experiments on benchmark OOD datasets demonstrate that the proposed method has a significant advantage over baselines."
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper proposes a meta-learning method for non-stationary tasks. The proposed method is based on variational continual Bayesian Meta-Learning (VC-BML) algorithm, which maintains a Dynamic Gaussian Mixture Model (DGMM) for meta-parameters, with the number of component distributions determined by a Chinese Restaurant Process (CPR). The authors claim that dynamic mixtures increase the capability to adapt to diverse and dissimilar tasks due to a larger parameter space, alleviating the negative knowledge transfer problem. The authors also develop a more robust posterior approximation method – structured variational inference for the sake of avoiding forgetting knowledge."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a probabilistic method for solving ODE boundary value problems (BVPs), which are ordinary differential equations subject to boundary conditions. The authors introduce a Gauss–Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of well-established, non-probabilistic methods. The model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. The proposed method is compatible with other statistical modelling tool-chain."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper studies the problem of learning a near optimal policy for two reward-mixing Markov decision processes (RM-MDPs), where the transition kernel and initial state distribution are defined as in standard MDPs, and the reward function is randomly chosen from one of M reward models at the beginning of every episode. The authors propose a polynomial-time algorithm that finds an -optimal policy after exploring poly(poly(H, S^2 A^2) episodes, where H is time-horizon and S, A are the number of states and actions respectively. This is the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,This paper proposes a single-cause perturbation (SCP) method to estimate the multi-cause conditional average treatment effect (CATE) in the setting of multi-causal inference. The proposed method augments the observational data with the estimated potential outcomes under single-source interventions and performs covariate adjustment on the augmented dataset to obtain the estimator. The authors show that the procedure is valid under standard assumptions in causal inference. Experimental results on extensive synthetic and semi-synthetic data demonstrate the effectiveness of the proposed method.
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"This paper proposes a multi-wavelet-based neural operator learning scheme that compresses the associated operator’s kernel using fine-grained wavelets. The projected kernel is trained at multiple scales derived from using repeated computation of multiwavelet transform. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. Compared with the existing neural operator approaches, this model shows significantly higher accuracy and achieves state-of-the-art in a range of datasets."
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper proposes a method for training binary neural networks (BNNs) by approximating the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. The authors also embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using the proposed method achieves the state-of-the-art accuracy."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"This paper uses multi-area RNNs with neuroscience-inspired architecture constraints to learn biologically plausible solutions to a perceptual decision-making task (checkerboard task). The authors show that incorporating multiple areas and Dale’s Law is critical for biasing the networks to learn biological plausible solutions. They also show that output-relevant information is preferentially propagated between areas, suggesting that cortex uses modular computation to generate minimal sufficient representations of task information."
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,This paper proposes structured attention graphs (SAGs) for visualizing the saliency maps of convolutional neural networks (CNNs) for image classification. The authors argue that a single saliency map provides an incomplete understanding since there are often many other maps that can explain a classification equally well. They propose to utilize a beam search algorithm to systematically search for multiple explanations for each image. They conduct a user study comparing the use of SAGs to traditional saliency mapping for answering counterfactual questions about image classifications.
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. The authors show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks. The choice of loss has little effect when networks are fully fine-tuned on the new tasks, and the authors find that differences among loss functions are apparent only in the last few layers of the network."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper proposes a method to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. The authors propose a novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. They test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper proposes a hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node from the source tree. Both the source and target trees are treated as latent and induced during training. The authors develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. They apply this latent neural grammar to various domains—a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation—and find that it performs respectably compared to baseline approaches."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a new efficient algorithm for the Group Elastic Net (GIN) regression problem, where the number of features is much larger than the observations. The main idea is to use the sparsity structure of the Augmented Lagrangian (AGL) to reduce the computational burden. The authors also extend the algorithm to the function-on-scalar regression (FoR) problem. The proposed algorithm is evaluated on synthetic and real-world datasets."
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"This paper proposes a clustering method for structured point process data. Specifically, the authors propose a mixture model of multi-level marked point processes to cluster repeatedly observed marked event sequences. A semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) of point processes is proposed for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and real data analyses."
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper proposes a meta-learning approach for multi-task adaptive nonlinear control, where the goal is to control a nonlinear system subject to adversarial disturbance and unknown environment-dependent nonlinear dynamics, under the assumption that the environment dependent dynamics can be well captured with some shared representation. The authors provide instantiations of their approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for adaptive control. They also validate the average control error metric (ACE) and focus on fully-actuated systems in this paper."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper focuses on the certified robust training of deep neural networks. The authors identify two issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improve IBP training. To mitigate these issues and conduct faster certified training with shorter warmup, the authors propose three improvements based on IBP learning: 1) a new weight initialization method, 2) adding Batch Normalization (BN) to each layer in the model, and 3) design regularization to explicitly tighten certified bounds and balance ReLU activations during warmup. The proposed method is able to obtain 65.03% verified error on CIFAR-10 and 82.36% on TinyImageNet with very short training schedules."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection in the presence of adversarial perturbations to the data. Specifically, the authors propose to use a dynamic Huber-contamination model for the perturbation distribution, which allows the contamination distribution to be different at each time point. The authors show that the detection boundary is a function of the contamination proportion and derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of contamination proportion. They also propose a computationally-feasible method, matching the lower bound under certain conditions, saving for logarithmic factors."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the power of learning via mini-batch stochastic gradient descent (SGD) and batch Gradient Descent (GD) on the empirical loss of a differentiable model or neural network. The authors show that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision of the gradient calculations relative to the minibatch size b and sample size m. When the precision is small enough, SGD can simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for b = 1. Similarly, with polynomially many bits of precision (i.e., when ρ is exponentially small), GD can both simulate PAC learning regardless of the mini batch size. On the other hand, when b^{-1} is large enough, the learning power of SGD is equivalent of that of SQ learning."
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"This paper considers the problem of minimizing the Wasserstein distance between two probability distributions over a metric space, i.e., the distance between the reference distribution and the model distribution. The authors propose a modified version of the standard Lloyd’s algorithm, in which Voronoi cells are replaced by Power cells, and show that it converges to a point cloud that is sufficiently far from other points in the ambient space. They also provide upper bounds for the convergence of the algorithm. "
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,This paper proposes a new dynamic feature transform for video understanding. The proposed RSA leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Experiments and ablation studies show that the proposed RSA network substantially outperforms convolution and self-attention counterparts on the standard motion-centric benchmarks for video action recognition.
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the fluctuation of the learning dynamics of multilayer neural networks. The main contribution of this paper is the derivation of the second-order mean field limit, which captures the limiting fluctuation distribution. The limit is obtained by decomposing the limit into two components, i.e., the time-evolving measure over weights and the nonlinear time evolution of the weights. Theoretical results are provided to show that the second order limit is the limit of the network's learning dynamics."
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"This paper proposes a method to learn generalized Casimirs for energy and entropy in metriplectic dynamical systems. In particular, the authors propose to learn the dissipative brackets of the metrizlectic system, which is a generalization of the Poisson brackets of Hamiltonian/Lagrangian mechanics. The proposed method is evaluated on a set of synthetic and real-world datasets, and compared with penalty-based and black-box approaches."
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection-based algorithm for fair and robust training. The authors formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. They propose a greedy algorithm that is efficient and effective in practice. Experiments show that the algorithm obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper studies the relationship between the weights of a single-layer Bayesian neural network (BNN) and the activation function of a stationary Gaussian process (GP) with respect to the spectral density of the covariance of the limiting limiting function of the GP. The authors show that the activation functions of BNNs with a Student-t prior on the weights correspond to a GP with a periodic activation function. They also show that this correspondence goes beyond sinusoidal (Fourier) activations and also covers triangular wave and periodic ReLU activation functions. They show that periodic activation functions induce global stationarity, which is a proxy for capturing sensitivity to perturbed inputs in deep neural networks for out-of-domain detection. "
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"This paper proposes a method to automatically assign a score to a student's interactive code assignments. The proposed method is based on the idea of learning to classify Markov Decision Processes (MDPs), where the goal is to decide whether the dynamics and reward model of the input MDP should be classified as correct or broken. The authors propose a cooperative objective between an agent and an autoregressive model to sample differential trajectories that allows a classifier to determine membership: Play to Grade. The method is evaluated on a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels."
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"This paper proposes a method for interpretable deep reinforcement learning (DRL) based on mimic learning. The main idea is to use an identifiable multi-object network (IMONet) to detect objects from high-dimensional input space and learn an object representation to embed the objects with a limited number of latent variables. This representation is well-disentangled and identifiable, allowing the mimic tree to uniquely extract the underlying causal relation from DRL models, and interpretable, in order to generate understandable DRL explanations. A Monte Carlo Regression Tree Search (MCRTS) algorithm is proposed to explore different splits to find the IB-optimal mimic trees. Experiments show that the mimic trees achieves strong approximation performance with significantly fewer nodes than baseline models."
SP:84560de78af979354fff83d1370d8675c1e9191f,"This paper introduces a Bayesian framework for modeling the structure of dynamic predictions over time. The authors model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. This approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths. The GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics."
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration with fixed confidence in generic stochastic bandit environments. The goal of the learner is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. The authors devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as it relies on a single iteration of Frank-Wrighte algorithm applied to the lower-bound optimization problem. FWS is applied to various pure exploration tasks, including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper proposes a method for Bayesian optimization (BO) of combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. The authors propose a method called LADDER, which is based on the idea of structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Experiments on real-world benchmarks show that the proposed method performs better or similar to state-of-the-art methods."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper studies the role of representation of state-action value functions in regret minimization in finite-horizon Markov Decision Processes (MDPs) with linear structure. The authors first derive a necessary condition on the representation, called universally spanning optimal features (UNISOFT), to achieve constant regret in any MDP with linear reward function. They then demonstrate that this condition is also sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms (LSVI-UCB and ELEANOR). Finally, they propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper introduces a differentiable contact model that can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. The authors demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a simulator for downstream gradient-based optimization tasks, such as planning and control."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper investigates the Benevolent training hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. The paper shows that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. They also show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. "
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper studies the problem of distribution-independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. The authors show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. The main application of this result is the distribution-dependent halfspace learning of PAC learning in the presence of label noise, and more specifically in the case of Massart (or bounded noise) model. The sample complexity of [DGT19] scales polynomially with the examples (in addition, of course, to the dimension and the inverse of desired accuracy). This bit-complexity dependence in the sample complexity is an artifact of the algorithmic approach in [D GT19]. Information-theoretically, no such dependence is needed. "
SP:e5229305af00067ae2dbabd903e585964aec8928,"This paper proposes a black-box adversarial attack method for graph neural networks (GNNs) based on Bayesian optimisation (BO). The proposed method is query-efficient and parsimonious with respect to the perturbation applied. The method is evaluated on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. The authors also analyze the generated adversarial examples to link the vulnerability of graph-based models to the topological properties of the perturbed graph."
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of label shift adaptation in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose two adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper considers the problem of detecting and localization of gradual changes in the distribution of a sequence of time-ordered observations. The authors propose a general method for detecting and localizing gradual changes that does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. The proposed method possesses proven theoretical guarantees for both detection and localization on each time step."
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper proposes a biologically plausible algorithm for blind source separation (BSS) based on kurtosis-based ICA methods. The main idea is to reformulate the objective function for ICA as a min-max optimization problem and solve it by online stochastic gradient optimization. The proposed algorithm relies on modulating synaptic plasticity by the total activity of the output neurons, which could be accomplished by neuromodulators, extracellular calcium, local field potential, or nitric oxide. The experimental results show that the proposed algorithm performs well on synthetic signals and audio datasets."
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper proposes a method to characterize the space of solutions associated with various tasks. The authors first study a simple two-neuron network on a task that leads to multiple solutions. They trace the nature of the final solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. They then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time Reproduction. For each task, they find a rich set of solutions."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper proposes a method for estimating the conditional distribution p(x_u|x_o) over a set of covariates over unobserved features x_u and observed variables x_o. The proposed method, called Arbitrary Conditioning with Energy (ACE), is the first to reduce the conditioning problem to one-dimensional conditionals with arbitrary observations and estimate these with energy functions using an energy function. The authors show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks."
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new adaptive weighted loss for SISR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, it introduces variance estimation characterizing the uncertainty on a pixel-by-pixel basis so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, uncertainty estimation allows us to leverage conventional wisdom such as sparsity prior for regularizing SisR solutions. The authors demonstrate that such uncertainty-driven loss can achieve better results than MSE or L1 loss for a wide range of network architectures."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"This paper studies the problem of adversarial robustness, i.e., the ability of a model to be robust or invariant to imperceptible perturbations in the input. The authors propose a general PAC-Bayesian framework to bound the averaged risk on the perturbation for majority votes (over the whole class of hypotheses). The authors show that their analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e. adversarial attacks), and (ii) that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time."
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper proposes a probabilistic entity representation model (PERM) for logical queries over Knowledge Graphs (KGs). The proposed model encodes entities as a multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. The model also defines the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. The proposed method outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"This paper proposes a gradient-based hyperparameter optimization method for tasks with long horizons. The proposed method is based on forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. The authors provide theoretical guarantees about the noise reduction properties of the algorithm, and demonstrate its efficiency empirically by differentiating through 10 gradient steps of unrolled optimization. The experimental results on CIFAR-10 show that the proposed method can outperform greedy gradient based methods."
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper proposes a method to improve the performance of neural sequence models by adding logical reasoning. In particular, the authors propose to use a symbolic reasoning module to evaluate generations generated by a neural sequence model for logical consistency. The proposed method is evaluated on two tasks: robust story generation and grounded instruction-following. Results show that the proposed method can improve the coherence and accuracy of neurally-based generations."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper proposes a method for off-policy evaluation (OPE) in continuous treatment settings. The main idea is to adaptively discretize the treatment space using deep learning and multiscale change point detection. The method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing. "
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a variational inference framework for hybrid dynamical systems. The model is based on a Markov jump process modulating a subordinated diffusion process. The authors provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, they develop a new continuous-time inference algorithm, combining a Gaussian process approximation on the diffusion level with posterior inference for Markov Jump processes. By minimizing the path-wise Kullback-Leibler divergence, they obtain (i) Bayesian latent state estimates for arbitrary points on the real axis and (ii) point estimates of unknown system parameters, utilizing variational expectation maximization."
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the impact of the spectrum of the sensing matrices on the performance of expectation propagation (EP) algorithms. The authors define a notion for the spikiness of A and show the importance of this measure in the performance. They show that for phase-retrieval problems, matrices with spikier spectrums are better for EP, while for compressed sensing problems, less spiky (flatter) spectrums offer better recoveries. "
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper proposes a method for generalized zero-shot learning (GZSL) by progressively improving cross-domain transferability and category discriminability of visual representations. It constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, it alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. It further projects category prototypes into multiple spaces to progressively repel visual representations from different categories. Experiments on four benchmarks demonstrate the effectiveness of the proposed method."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus blurring kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network (DNN) is developed by unrolling a fixed-point iteration of the GKM-based deblurring. Extensive experiments show that the proposed method achieves state-of-the-art results on existing datasets."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper proposes a method for self-supervised video representation learning (SSVRL) by exploiting compressed videos and capturing mutual information between two input streams. Specifically, the authors propose a Motion Vector based Cross Guidance Contrastive learning approach (MVCGC) to directly decode RGB frames and motion vectors from compressed videos on-the-fly. To enhance the representation ability of the motion vectors, hence the effectiveness of our method, they design a cross guidance contrastive learning algorithm based on multi-instance InfoNCE loss, where motion vectors can take supervision signals from RGB frames, and vice versa. Comprehensive experiments on two downstream tasks show that our MVCGC yields new state-of the art while being significantly more efficient than its competitors."
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"This paper proposes an extension of the cubic spline kernel for Bayesian neural networks (BNNs), which is a Bayesian linear model with countably infinite ReLU features. The authors show that the proposed GP is asymptotically maximally uncertain far away from the training data, while the BNNs’ predictive power is unaffected near the data. This extension can be applied to any pre-trained ReLU BNN at a low cost. "
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of selecting a causal quantity of interest among a set of causal formulas. The authors assume an sequential setting in which the investigator may alter the data collection mechanism in a data-dependent way with the aim of identifying the formula with lowest asymptotic variance in as few samples as possible. They formalize this setting by using the best-arm-identification bandit framework where the standard goal of learning the arm with the lowest loss is replaced with the goal to learn the arm that will produce the best estimate. They introduce new tools for constructing finite-sample confidence bounds on estimates of the asymPTotic variance that account for the estimation of potentially complex nuisance functions, and adapt the best arm identification algorithms of LUCB and Successive Elimination to use these bounds. They validate their method by providing upper bounds on the sample complexity and an empirical study on artificially generated data."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper proposes ErrorCompensatedX, a compression method for variance reduced stochastic gradient descent (SGD) and momentum SGD. The main idea is to add back the previous step’s compression error from the previous two steps. Theoretical analysis shows that the proposed method can achieve the same asymptotic convergence rate with the training without compression, and provides a unified theoretical analysis framework for this class of variance reduced algorithms, with or without error compensation. "
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper proposes a method to improve the explainability of graph neural networks (GNNs) by combining the contrastive learning and class-wise generative probabilistic models. Specifically, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the global characteristics from a global view, and the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of the explainer."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"This paper proposes a method to generate robust counterfactual explanations for graph neural networks (GNNs) by explicitly modelling the common decision logic of GNNs on similar input graphs. The proposed method, called RCExplainer, is able to generate explanations that are robust to noise and align well with human intuition. Experimental results on several public datasets demonstrate the superior performance of the proposed method."
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper proposes a method for voice style transfer (VST) based on self-supervised representation learning and adversarial feedback. The proposed method can decompose the content and style with only a small loss of content information. Experiments show the superiority of the proposed method in disentanglement and transfer performance, and improve audio quality."
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"This paper proposes a Siamese voxel-to-BEV tracker to improve the tracking performance in sparse 3D point clouds. Specifically, it consists of a shape-aware feature learning network and a voxels to BEV target localization network. The proposed method is evaluated on the KITTI and nuScenes datasets and achieves state-of-the-art performance."
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a positional encoding method based on learnable Fourier features for multi-dimensional positional encoding. The authors propose a learnable encoding function that maps multi-dimensions positions into a vector space, which is modulated with a multi-layer perceptron. The representation is particularly advantageous for spatial multi-dimension position, e.g., pixel positions on an image, where L2 distances or more complex positional relationships need to be captured. The proposed method outperforms existing methods by both improving the accuracy and allowing faster convergence."
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of learning the causal MAG of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. This paper proposes a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of the approach is that at each iteration a specific type of variable is identified and removed, which allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests. "
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"This paper proposes a batch Thompson Sampling (Batch-TS) algorithm for stochastic multi-arm bandit and linear contextual bandit problems. The authors show that the proposed algorithm achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only $O(log T)$ number of batch queries. To achieve this exponential reduction, the authors dynamically decide the duration of each batch in order to balance the exploration-exploitation trade-off."
SP:653a519e3c799c25e0d0b4240322642040b121a3,This paper studies the problem of representation learning for multiple source domain adaptation (MSDA) and domain generalization (DG) problems. The authors develop upper bounds for the target general loss and define two kinds of domain-invariant representations: general representation and compressed representation. They further study the trade-offs of these representations for offering practical hints regarding how to use them in practice. 
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes an aligned structured sparsity learning (ASSL) method for lightweight image super-resolution (SR) networks. ASSL introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. To align the pruned filter locations across different layers, the authors propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. The proposed method is applied to train efficient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,This paper proposes a method for efficient exploration in deep cooperative multi-agent reinforcement learning (MARL) called Episodic Multi-Agent Reinforcement Learning (EMC). The authors propose to use prediction errors of individual Q-values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. They demonstrate the advantages of their method by didactic examples and demonstrate its significant outperformance over state-of-the-art MARL baselines on challenging tasks in StarCraft II micromanagement benchmark.
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"This paper studies the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. The main result is a Statistical Query (SQ) lower bound of d for this problem. The SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a hybrid model for predicting the patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. The proposed model integrates a system of expert-designed ODEs with machine-learned Neural ODE to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. The model is evaluated on synthetic data as well as real-world intensive care data of COVID-19 patients."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper proposes a theoretical framework for analyzing a MAML-like algorithm, assuming all available tasks require approximately the same representation. It provides risk bounds on predictors found by finetuning via gradient descent, demonstrating that the method provably leverages the shared structure. The authors also show that learning one representation for all tasks (i.e. using a “frozen representation” objective) fails."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of the approach is a collection of lexicon entries, which map each word to a syntactic type and a neuro-symbolic semantic program. Given an input sentence, the proposed approach derives the meaning of the sentence by composing lexical meanings based on syntax. To facilitate learning in an exponentially-growing compositional space, the authors introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper proposes a distributed stochastic Newton algorithm for homogeneous distributed convex optimization, where each machine can calculate the gradients of the same population objective and the Hessian-vector products. The authors show that their method can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives. "
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes a new similarity measure named Density-aware Chamfer Distance (DCD) to measure the similarity between two point sets. It is derived from Chamfer distance (CD) and benefits from several desirable properties: 1) it can detect disparity of density distributions and is thus a more intensive measure of similarity compared to CD; 2) it is stricter with detailed structures and significantly more computationally efficient than EMD; 3) the bounded value range encourages a more stable and reasonable evaluation over the whole test set. The authors adopt DCD to evaluate the point cloud completion task, where experimental results show that DCD pays attention to both the overall structure and local geometric details and provides a more reliable evaluation even when CD and EMD contradict each other."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper studies the problem of knowledge distillation, a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. The authors identify difficulties in optimization as a key reason for why the student is unable to match the teacher. They also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalization."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper proposes a coreset for decision trees, which is a (k, \epsilon)-coreset that approximates the loss function of a k-decision tree for a given matrix $D$ and $k$ rows. The coreset size is polynomial in $k$, and the construction of the coreset is O(N^k) time. The paper shows that this coreset can be used to compute the optimal k-tree of $D$, which is an approximation of the optimal tree of the original $D$. The paper also shows that the construction time of the coresets is faster than the training and tuning of the decision trees."
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the problem of top-m identification for misspecified linear bandit models, which is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms, but in which data inevitably deviates from linearity. In this setting, the authors first derive a tractable lower bound on the sample complexity of any $\delta$-correct algorithm for the general Top m identification problem. They then describe the first algorithm for this setting which is both practical and adapts to the amount of misspecification. They derive an upper bound to its sample complexity which confirms this adaptivity and that matches the lower bound when \delta = 0. Finally, they evaluate their algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a method for learning disentangled graph representations with self-supervised learning. The authors first identify the latent factors of the input graph and derive its factorized representations. Then they propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method."
SP:0a7edbbdabab11273689c40c517001eb46491113,This paper proposes a method to assess the robustness of deep neural networks (DNNs) to adversarial and random perturbations. The method is based on a stochastic simulation inspired by Statistical Reliability Engineering (SRE). The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The authors derive theoretical guarantees that are nonasymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of the method.
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper introduces a framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. The authors show how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image to image translation, and attribute-guided generation) involving eight datasets. The thorough evaluation suggests that the framework can be useful for tackling diverse conditional generation tasks."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper proposes a neural network Maximum Mean Discrepancy (MMD) statistic by identifying a new connection between neural tangent kernel (NTK) and MMD. This connection enables the authors to develop a computationally efficient and memory-efficient approach to compute the MMD statistic and perform NTK based two-sample tests towards addressing the long-standing challenge of memory and computational complexity. Theoretically, such a connection allows us to understand the NTK test statistic properties, such as the Type-I error and testing power, by adapting existing theories for kernel MMD and NTK. Numerical experiments on synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed NTK-MMD statistic."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"This paper proposes a variational autoencoder (VAE) with a task-disentanglement objective to extract class-dependent information as x-G(x) from the input x, which is a trade-off between reconstructing x and classifying x, where the former competes with the latter in decomposing x so that the former retains only necessary information for classification in x - G(x). The authors apply it to both clean images and adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class-independent part of x. The decomposition results also provide novel interpretations to the classification and attack models. Inspired by these observations, the authors propose to conduct adversarial detection and defense on x and G, which consistently outperform the results on the original x."
SP:2789874561620ba7894c4672f935056bb911e919,"This paper proposes a federated learning (FL) algorithm based on the federated Thompson sampling (FTS) algorithm with differential privacy (DP) to preserve user-level privacy. The authors also leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of the proposed algorithm through distributed exploration (DE). The resulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off. "
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"This paper proposes a Gaussian process-Bayesian Bernoulli Mixture model (GP-BM) for multi-label active learning (ML-AL) to capture the label correlations and the overall contribution of each data sample to a correlated label space and choose the most informative data samples for cost-effective data annotation. The BM encodes label correlations using a BayesianBernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. To tackle highly sparse labels under AL, the BM is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. A principled sampling function is designed accordingly to naturally capture both the feature uncertainty (through GP) and label covariances (through BM) for effective data sampling. Experiments on real-world multilabel datasets demonstrate the state-of-the-art performance of the proposed model."
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,This paper proposes to use polar coordinate system to improve the end-to-end latency of lidar perception models by operating on wedge-shaped point cloud sectors rather then the full point cloud. The proposed method uses multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following sector from past scan. It also improves the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods.
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper proposes a framework for learning with structured latent variables. The authors extend the Gumbel-Max trick to define distributions over structured domains, and leverage the score function estimators for optimization. In particular, they highlight a family of recursive algorithms with a common feature they call stochastic invariant. The feature allows them to construct reliable gradient estimates and control variates without additional constraints on the model. In the experiments, they consider various structured latent variable models and achieve results competitive with relaxation-based counterparts."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper proposes a method for adapting CNN denoisers trained on large datasets to a single test image. To avoid overfitting, the proposed method optimizes a single multiplicative scaling parameter (the “Gain”) of each channel in the convolutional layers of the CNN. The authors show that GainTuning improves state-of-the-art CNNs on standard image-denoising benchmarks, boosting their denoising performance on nearly every image in a held-out test set. These adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type."
SP:90afa1102683b456bc72a54abef466326827546a,"This paper proposes a combinatorial optimization for panoptic segmentation (COPS), which is the task of simultaneously segmenting different semantic classes and instances of the same class. The authors propose a fully differentiable architecture for simultaneous semantic and instance segmentation consisting of a convolutional neural network and an asymmetric multi-way cut problem solver (AMWC). The latter solves a combinatorsial optimization problem that combines semantic and boundary predictions to produce a pan-optic labeling. The formulation allows to directly maximize a smooth surrogate of the panopti quality metric by backpropagating the gradient through the optimization problem. Experimental results on Cityscapes and COCO datasets show the effectiveness of the proposed approach."
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper introduces Recursive Bayesian Networks (RBNs), a new probabilistic model that unifies the strengths of PCFGs and dynamic Bayesian networks (DBNs) by allowing for nested hierarchical dependencies in combination with arbitrary discrete or continuous random variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. This paper provides two solutions: 1) for arbitrary RBNs, it generalizes inside and outside probabilities from PCFG to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) for Gaussian RBN, it additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. The paper provides a quantitative evaluation on synthetic data and application to the challenging task of hierarchical analysis."
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a constrained backpropagation (CBP) algorithm based on the pseudo-Lagrange multiplier method (LMM) to obtain the optimal set of weights that satisfy a given set of constraints. The constraint functions are nondifferentiable at xm (= arg minx cs (x), rendering LMM inapplicable. The proposed CBP algorithm is the utilization of a Lagrangian function (loss function plus constraint function) as its objective function. The authors considered various types of constraints — binary, ternary, one-bit shift, and two- bit shift weight constraints. As a post-training method, CBP applied to AlexNet, ResNet, and ResNet-50 on ImageNet outperforms the state-of-the-art methods."
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper studies the problem of active learning for Gaussian process classification (GPC). The authors propose two scenarios: pool-based and query-synthesis scenarios. In the first scenario, the learner sequentially selects the best instance for labeling by optimizing an acquisition function to enhance data/label efficiency. The computation of EER-based acquisition functions is typically prohibitive as it requires retraining the GPC with every new query. Moreover, as the EER is not smooth, it can not be combined with gradient-based optimization techniques to efficiently explore the continuous instance space for query synthesis. The authors derive the joint predictive distribution of label pairs as a one-dimensional integral, as a result of which the computation of the acquisition function avoids retraining GPC for each query, remarkably reducing the computational overhead. They also derive the gradient chain rule to efficiently calculate the gradient. The experiments clearly demonstrate the computational efficiency of the proposed algorithms."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper studies the effect of unbounded gradients in variational autoencoders (VAEs) on the regularization of the model. The authors show that if the ultimate goal is to simultaneously avoid over-regularization (high reconstruction errors) and under-regularisation (excessive latent dimensions are not pruned from the model), then an autoencoder-based energy function with infinite gradients around optimal representations is provably required. This result suggests that heuristic modifications to or constraints on the VAE energy function may be ill-advised, and large gradients should be accommodated to the extent possible."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the min-max regret of the multi-armed bandit problem with graph feedback. The authors propose two notions of the fractional weak domination number and the k-packing independence number to capture upper bound and lower bound for the regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual. Based on this connection, they utilize the strong duality theorem to prove a general regret upper bound of $O(\delta^1/\alpha)$ and a lower bound of $\Omega(\frac{1}{\epsilon^3}{3 T^2 3})$ where $\alpha$ is the integrality gap of the dual linear program. "
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper proposes a neighborhood SHAP method to improve the interpretability of Shapley values. The authors show that the Nadaraya-Watson estimator at x can be interpreted as an importance sampling estimator where the expectation is taken over the proposed neighbourhood. Empirically, they show that Neighbourhood SHAP identifies meaningful sparse feature relevance attributions that provide insight into local model behaviour, and increase on-manifold explainability and robustness to the construction of adversarial classifiers."
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, it augments the actions to generate a large amount of virtual state-action trajectories and enforce the trajectory to be similar to the original time step to meet the cycle consistency constraint. The proposed method achieves the state-of-the-art performance on the Atari and Deepmind Control Suite benchmarks."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,This paper investigates how the network’s architecture impacts its robustness to noisy labels. The authors provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. The framework measures a network's robustness via the predictive power in its representations — the test performance of a linear model trained on the learned representations using a small set of clean labels. They hypothesize that a network is more robust if its architecture is more aligned with the target function than the noise. They provide both theoretical and empirical evidence across various neural network architectures and different domains.
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes a data-driven reinforcement learning algorithm that learns to predict future success from transitions and success examples, without learning a separate reward function. The authors propose a method for classifying future events using a variant of temporal difference learning that they call recursive classification. They show that the method satisfies a new data driven Bellman equation, where success examples are used in place of the standard reward function term. They also provide convergence guarantees. Experiments show that their approach outperforms prior methods that learn explicit reward functions."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization in convex and non-convex settings. For the convex case, the authors focus on the family of non-smooth generalized linear losses (GLLs). Their algorithm for the l2 setting achieves optimal excess population risk in near-linear time, while the best known algorithms for general convex losses run in super- linear time. The algorithm for l1 setting has nearly-optimal excess risk, and circumvents the dimension dependent lower bound of [AFKT21]. In the non-Convex setting, the paper provides several new algorithms for approximating stationary points of the population risk. For l1-case with smooth losses and polyhedral constraint, they provide the first nearly dimension independent rate, $\mathcal{O}(\log d (n\epsilon)1/3 )$. For the constrained l2-case, they obtain a linear-time algorithm with rate $\frac{1 n1/4 + d 1/5 (nε)2/5 )$, which matches the best existing non-private algorithm when d = O(\sqrt{n})$. They also extend all their results above for the  l2 settings to the lp setting, with only polylogarithmic overhead in the rates."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies cooperative multi-agent bandit learning under stochastic time-varying networks, instantaneous reward sharing over a network with random delays, and adversarially corrupted rewards. The authors propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well. The proposed algorithms are straightforward to implement and obtain competitive empirical performance. "
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a post-training quantization method for vision transformers to reduce the memory storage and computational costs of the model. To preserve the functionality of the attention mechanism, the authors introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, they explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post training quantization algorithms."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the finite-time analysis of double Q-learning with a constant learning rate, which improves the existing convergence rate of Xiong et al. (2020) by an order of magnitude in terms of the dependence on all major parameters (1-\gamma, 1-\delta,D,L). In particular, the authors develop a systematic approach and a series of novel techniques to bound two nested stochastic approximation recursions to achieve the same convergence rate as the best known results of the vanilla Q learning (which are $\tilde{O}(\frac{1}{\epsilon^2}{2}})$ for synchronous and asynchronous double Q learning. "
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper studies semi-supervised out-of-distribution (OOD) detection in the presence of limited labeled data and unlabeled data. In particular, the authors propose a new representation space in which ID and OOD samples can be separated well, and an efficient optimization algorithm is derived to solve the objective. Comprehensive experiments across various OOD detection benchmarks clearly show that the proposed approach outperforms other methods by a large margin and achieves remarkable detection performance."
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper proposes a one-stage multi-task framework for visual grounding tasks. Specifically, the authors leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. The proposed method outperforms state-of-the-art methods by a large margin on both REC and RES tasks."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of multiclass boosting, where the weak learner is assumed to belong to an easy-to-learn base class and is an agnostic PAC learner for that class with respect to the standard classification loss. The goal of the overall boosting algorithm is to learn a combination of weak hypotheses by repeatedly calling the weak learners. The paper shows the resources required for boosting, especially how they depend on the number of classes k, for both the booster and weak learners, and shows that the boosting algorithm itself requires O(log k) samples, as well as analyzing a variant of AdaBoost for our setting."
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper proposes a new method for unsupervised object segmentation and object-centric scene generation. The proposed method is based on embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. The clustering procedure also leads to randomly ordered object representations, but without the need of initializing a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative refinement. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the method."
SP:408deb9e5577ee7118b836fee77135df641fe545,This paper proposes an adaptive conformal inference (ACI) method for constructing prediction sets that are robust to changes in the marginal distribution of the data. ACI is based on the idea that the distribution shift can be modeled as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. The authors show that ACI achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. The method is tested on two real world datasets and finds that its predictions are robust.
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"This paper proposes a method for multi-person pose estimation in crowded scenes. The proposed method is based on the Pose-level Inference Network (PINet), which infers the complete pose cues for a person from his/her visible body parts. PINet first applies the Part-based Pose Generation (PPG) to infer multiple coarse poses for each person, refined by the Pose Refinement module through incorporating pose priors, and finally fused in the Pose Fusion module. Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet."
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,This paper proposes a new algorithm for solving robust Markov decision processes (RMDPs) with L_infty-constrained rectangular ambiguity sets. The proposed algorithm combines a novel homotopy continuation method with a bisection method to solve S-rectangular ambiguity in quasi-linear time in the number of states and actions. The algorithm improves on the cubic time required by leading general linear programming methods. The experimental results confirm the practical viability of the method and show that it outperforms a leading commercial optimization package by several orders of magnitude.
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper studies the problem of learning-augmented online algorithms with weak predictions for the online knapsack problem, where the prediction is in the form of knowing an upper and lower bound for the number of items of each value. The authors systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction; they also extend the results to more general settings such as generalized one-way trading and two-stage online knapack. Their work shows that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms."
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes a new model-based episodic memory of trajectories to address the limitations of episodic control. The memory estimates trajectory values, guiding the agent towards good policies. Built upon the memory, the authors construct a complementary learning model via a dynamic hybrid control unifying model based, episodic and habitual learning into a single architecture. Experiments demonstrate that the model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non-Markovian settings."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"This paper proposes a new semi-supervised learning (SSL) method for unlabeled data. The proposed method is based on data programming (DP) scheme to generate probabilistic labels for unlabelled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), this paper develops a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, the authors design a label model to resolve the conflict and overlap among the noise labels, and infer probablistic labels. The experimental results on four standard SSL benchmarks show that the proposed method achieves better classification performance than existing SSL methods."
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"This paper proposes a multi-view pose transformer (MVPT) for estimating multi-person 3D poses from multi view images. The key idea is to represent skeleton joints as learnable query embeddings and let the model progressively attend to and reason over the multiview information from the input images to directly regress the actual 3D joint locations. To improve the accuracy of such a simple pipeline, MVP presents a hierarchical scheme to concisely represent query embedding, introduces an inputdependent query adaptation approach, and designs a novel geometrically guided attention mechanism, called projective attention, to more precisely fuse the cross-view information for each joint. MVP also introduces a RayConv operation to integrate the view-dependent camera geometry into the feature representations for augmenting the projective attentions."
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of recovering the supports of unknown sparse vectors from a fixed set of vectors, where each vector has at most $k$ non-zero elements. In particular, the authors study two problems: (1) robust learning of supports of all vectors from the family using a sequence of noisy responses, and (2) designing queries such that all sparse vectors can be approximately reconstructed based on the error-free responses. The main contribution of the paper is to prove the existence of learning algorithms for the first problem which work without any assumptions. The second problem is to rigorously analyze their query complexity. "
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper studies the bandit quickest changepoint detection (BQCD) problem, where the goal is to detect abrupt changes in temporal behavior patterns. The authors derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. They then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of informative actions. They derive expected delay bounds for the proposed scheme and show that these bounds match the lower bounds at low false alarm rates, establishing optimality of the proposed method."
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper considers the problem of solving stochastic nested optimization problems, i.e. problems with a nested structure. The authors propose a new algorithm, called ALternating Stochastic Gradient dEscenT (ALSET), which unifies several SGD-type updates into a single SGD approach that they term ALternated SGD Approach (AGAT). The main contribution of this paper is to provide a tighter analysis of ALSET for the nested problems. In particular, they show that to achieve an-stationary point of the nested problem, it requires O(\epsilon^{-2}) samples in total. They also show that under certain regularity conditions, the proposed method can be applied to the min-max, compositional, and reinforcement learning problems. "
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper proposes a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy consists of two modules: (1) siame knowledge generation to learn the inter-relationship among clips; (2) siamesese knowledge reasoning to produce the refined soft label by propagating the weights of inter- relationship to the predicted candidates of all clips. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on five VideoQA benchmarks."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a low-rank constraint to reduce the computational and memory complexity of a large class of structured probabilistic representations from observed data. The authors show that by viewing the central inference step as a matrix-vector product and using a low rank constraint, we can trade off model expressivity and speed via the rank. Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper introduces a novel uncertainty measure for deep contextual bandits. The uncertainty measure is a frequentist quantity that only depends on the value prediction of each action. The authors show that the uncertainty measure estimated by SAU matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. The paper also shows that SAU-based exploration outperforms state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost."
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper proposes a method to disentangle the dynamic behavior factors (pose) from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder, and exploit the temporal structures of pose dynamics. The proposed method is applied to unlabeled, multi-view, high-resolution behavioral videos across different animals and multiple sessions. The authors further combine DBE with a stochastic temporal model to propose Variational Disentangled Behavior Embedding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,This paper proposes a conditional generative model for high-resolution 3D shape synthesis from coarse voxels. The core of DMTET includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. The proposed method is evaluated on a dataset of complex 3D animal shapes.
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper proposes sliced mutual information (SMI) as a surrogate measure of dependence. SMI is defined as an average of MI terms between one-dimensional random projections. The authors show that it preserves many of the structural properties of classic MI, while gaining scalable computation and efficient estimation from samples. Furthermore, SMI can grow as a result of deterministic transformations. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) for solving constrained black-box optimization problems. The proposed method is based on a likelihood-ratio-based unbiased estimator of the gradient of the two step optimal acquisition function. In numerical experiments, the proposed method achieves improved query efficiency by 2x or more over previous methods. "
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper proposes a multi-dimensional distributional distributional value network (MD3QN) which extends distributional RL to model the joint return distribution from multiple reward sources. The authors prove the convergence of the joint distributional Bellman operator and propose an empirical algorithm to approximate it. The empirical algorithm is based on minimizing the Maximum Mean Discrepancy (MMD) loss over joint return distributions and its Bellman target. In experiments, the authors show that the proposed method accurately models the joint returns in environments with richly correlated reward functions and outperforms previous RL methods."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper proposes a method for 3D surface reconstruction from volumetric images. The proposed method, called CorticalFlow, is a geometric deep-learning model that, given a 3D image, learns to deform a reference template template towards a targeted object. To conserve the template mesh’s topological properties, the model is trained over a set of diffeomorphic transformations. The method is evaluated on the task of brain cortical surface reconstruction. "
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper studies the problem of machine unlearning, i.e. the task of removing the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. In this paper, the authors propose a general reduction from deletion guarantees against adaptive sequences to deletion guarantee against non-adaptive sequences, using differential privacy and its connection to max information. They show in theory how prior work for non-convex models fails against adaptive deletion sequences and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. on CIFAR-10, MNIST, Fashion-MNIST."
SP:7150006590e268ab732c9be6c9048f67a377f956,This paper studies risk-averse Bayes-adaptive reinforcement learning (BARL) in the setting where the total return is uncertain. The authors propose to optimize the conditional value at risk (CVaR) of the return in BARL to mitigate epistemic and aleatoric uncertainty. They reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Their experiments demonstrate that their approach significantly outperforms baseline approaches for this problem.
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model."
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,This paper proposes a method to detect coordinated accounts on social media based on neural temporal point processes (NTPs). The authors propose a variational inference approach to learn a Gibbs distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. Experiments on a real-world dataset show the effectiveness of the proposed method compared to existing methods.
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper studies the problem of binary classification on disjoint smooth manifolds with low-dimensional structure, where the goal is to correctly classify every point on each of the sub-manifolds with high probability. The main contribution is the first end-to-end analysis of this problem for a nontrivial class of manifolds: one-dimensional smooth curves that are non-intersecting, cusp-free, and without antipodal of points. The analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime where the network depth plays the role of a fitting resource in solving the classification problem. In particular, via fine-grained control of the decay properties of the NTK, the paper shows that when the network is sufficiently deep, the network can be locally approximated by a translationally invariant operator on the manifolds and inverted over smooth functions, which guarantees convergence and generalization."
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes two remedies for the instability of ACGAN. First, it identifies that gradient exploding in the classifier can cause an undesirable collapse in early training, and projects input vectors onto a unit hypersphere to resolve the problem. Second, it proposes the Data-to-Data Cross-Entropy loss (D2D-CE) to exploit relational information in class-labeled dataset. The proposed ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets."
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper proposes an extensive-form double oracle algorithm for two-player zero-sum games that is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. The authors also introduce Neural XDO (NXDO), where the best response is learned through deep RL. In tabular experiments on Leduc poker, the authors show that XDO achieves an approximate NE in a number of iterations an order of magnitude smaller than PSRO."
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper proposes a permutation-invariant variational autoencoder for graph structured data. The proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. The experiments demonstrate the effectiveness of the proposed model for graph reconstruction, generation and interpolation."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"This paper proposes to decouple the depth and scope of GNNs - to generate representation of a target entity (i.e., a node or an edge), the authors first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. Theoretically, the authors show that the decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, the proposed method achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"This paper proves that any log-concave distribution can be approximated using well-conditioned affine-coupling flows. In particular, the authors show that any well-behaved distribution p(x) can be treated as a distribution in Rd+d0, and can be closely approximated by an affine coupling flow. The authors also show that affine couplings can be used to approximate a padded version of the input distribution with iid Gaussians, which is a strategy which Koehler et al. [2020] empirically observed to result in better flows. "
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper studies the problem of coupons allocation in a constrained Markov decision process (CMDP), where the goal is to allocate coupons within a fixed budget while maximizing users’ retention on the e-commerce platform. The authors propose a budget constrained offline reinforcement learning and evaluation with λ-generalization (BCORLE(lambda)) framework to solve the CMDP problem. Specifically, the proposed method can help enterprises develop a coupons allocation policy which greatly improves user’s retention rate on the platform while ensuring the cost does not exceed the budget."
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a method for source-free domain adaptation (SFDA), where the source pretrained model is adapted to the target domain in the absence of source data. The method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. The authors capture this intrinsic structure by defining local affinity of the target data and encourage label consistency among data with high local affinity. They observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. Furthermore, to aggregate information with more context, they consider expanded neighborhoods with small affinity values."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a method for learning representations from sets. The proposed method is based on an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set-structured data effectively. In particular, the authors treat elements of a set as samples from a probability distribution and propose a pooling mechanism for aggregating a set of features into a fixed-dimensional representation. The authors evaluate the proposed method on a wide variety of set- structured data, including point-cloud, graph, and image classification tasks, and demonstrate that it provides superior performance over existing set representation learning approaches."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper proposes a family of RNNs that can be formulated using stochastic bilevel optimization (SBO). The authors convert the SBO problem into an RNN where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively. The authors prove that under mild conditions there is no vanishing or exploding gradient in training SBO-RNN. Empirically, the authors demonstrate the approach with superior performance on several benchmark datasets."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"This paper studies the online problem of dynamic power management (DPM) in the case of multiple power-saving states. During idle periods of unknown lengths, an algorithm has to choose between power- saving states of different energy consumption and wake-up costs. The authors develop a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods. The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem."
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper proposes a transferability measure for multi-source transfer learning problems, which quantifies the transferability in terms of the number of samples, the complexity of the model, and the distance between source and target tasks. In particular, the authors consider the setting where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure transferability. Then, they demonstrate the analytical expression of this measure, characterized by the sample sizes, the model complexity, and task similarities, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. In addition, they develop an alternating iterative algorithm to implement their theoretical results for training deep neural networks in multi- source transfer learning tasks."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,This paper proposes a neural network model for visual search. The proposed model is based on the idea of eccentricity-dependent sampling and top-down modulation through target-dependent attention. The model is trained on augmented versions of ImageNet where the biases of natural images are either removed or reversed. The empirical results show that the polarity of search asymmetry arises from experience with the natural environment. 
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,This paper studies the problem of training certifiable robust models against adversarial examples. The authors identify that the smoothness of the loss landscape is an important factor that influences the performance of certifiable training and propose a method that satisfies the two criteria: tightness on the upper bound on the worst-case loss and smoothness on loss landscape. The proposed method achieves a decent performance under a wide range of perturbations.
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper considers the problem of online linear regression in the stochastic setting. The authors derive high probability regret bounds for online ridge regression and the forward algorithm. This enables them to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. The study advocates for the use of the forward algorithms in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, the authors explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. This modification yields improved regret bounds."
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper proposes a fast extragradient (FEG) method for nonconvex-nonconcave minimax problems, which has a fast $O(1/k)$ rate on the squared gradient norm, where $k$ denotes the number of iterations. The main idea is to use a saddle-gradient operator to find a first-order stationary point of the problem. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available."
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies uniformity testing for statistical data that consists of rankings over m items, where the alternative class is restricted to Mallows models. The authors show that uniform distribution can be distinguished from Mallows model with O(m 1/2) samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if the number of items is large enough. They also consider uniformity test with central and local differential privacy (DP) constraints. "
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"This paper studies a general greedy score-based algorithm for learning directed acyclic graphs (DAGs). Unlike edge-greedy algorithms such as the popular GES and hill-climbing algorithms, the proposed approach is vertex- greedy and requires at most a polynomial number of score evaluations. The authors also show how recent polynomials-time algorithms for learning DAG models are a special case of this algorithm, thereby illustrating how these order-based algorithms can be rigorously interpreted as score based algorithms. Finally, extensive experiments suggest that this algorithm indeed optimizes the score in a variety of settings."
SP:b60989706296b963b6671c01f22384978a334be1,"This paper proposes neural architecture dilation for adversarial robustness (NADAR) to improve the accuracy-robustness trade-off between standard accuracy and adversarial accuracy. The main idea is to dilate the architecture of neural networks to increase the robustness while maintaining a competitive accuracy with a straightforward accuracy. In addition, the authors also propose a FLOP-aware FLOPs-aware loss function to reduce the computation cost of the proposed method. The proposed method is evaluated on CIFAR-10, Cifar-100 and ImageNet datasets."
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs), where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. The authors propose a new provably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP assumption, where the exploration phase consists of two phases: exploration phase and planning phase. They show that to obtain an $\epsilon$-optimal policy for arbitrary reward function, UCRLRFE needs to sample at most $\tilde{O}(H^5d^2\eps^2)$ episodes during exploration phase, where H is the length of the episode, d is the dimension of the feature mapping, and the horizon is the required accuracy. The upper bound matches the lower bound in terms of the dependence on $H$ and $d$ if $H \geq d$. The authors also propose a variant of UCRL RFE using Bernstein-type bonus and show that it needs $O(Hd(H + d)^2$ episodes to achieve an $\eps$ optimal policy."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a method for forecasting future events in a streaming data stream with seasonal patterns. The proposed method, Shifting Seasonal Matrix Factorization (SSMF), is able to adaptively learn multiple seasonal patterns (called regimes), as well as switching between them, by detecting regime shifts in seasonal patterns as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. Experiments on three real-world data streams show that the proposed method outperforms state-of-the-art baseline methods."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper proposes a novel neural network architecture, WeaveNet, for the assignment task. The core module, feature weaving layer, is stacked to model frequent communication between elements in a parameter-efficient way for solving the combinatorial problem of assignment. The experimental results showed its impressive performance among the learning-based baselines."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper systematically studies the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, they study MLP-based, convolution-based (DGCNN), and transformer-based(PCT) 3D architectures. Through extensive experimentation, the authors demonstrate that appropriate applications of self supervision can significantly enhance the robustness in 3d point cloud recognition, achieving considerable improvements compared to the standard adversarial pretraining baseline. The analysis also reveals the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial robustness."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"This paper proposes a method to speed up the computation of projections over submodular base polytopes by reusing structural information from previous minimizers. Specifically, the authors propose to use the away-step Frank-Wolfe algorithm to use this information and enable early termination. Theoretical results show that the proposed method improves the runtime of computing certain Bregman projections by a factor of $\Omega(n/log(n))$ for cardinality-based submodularity. "
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"This paper considers the problem of learning the natural parameters of a k-parameter minimal exponential family from i.i.d. samples in a computationally and statistically efficient manner. In particular, the authors focus on the setting where the support and natural parameters are appropriately bounded. They provide finite sample guarantees to achieve an (`2) error of alpha in the parameter estimation with sample complexity $O(poly(k/\alpha))$ and computational complexity $poly(\k/alpha)$. They show that, at the population level, their method can be viewed as the maximum likelihood estimation of a re-parametrized distribution belonging to the same class of exponential family."
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a hybrid differentiable renderer, DIB-R++, which combines rasterization and ray-tracing through an efficient deferred rendering framework. The renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. The proposed method is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth. The authors demonstrate the effectiveness of their hybrid framework through the learning-based problem of single image 3D reconstruction without supervision."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a differentiable training method for localization that imposes implicit constraints to the shape of the probability map by minimizing the expectation of the localization error. The expectation can be approximated by calculating the average error of all samples drawn from the output distribution. Samples are modelled in the continuous space with a mixture distribution, and the sampling process can be resolved by differentiable from categorical distributions. Comprehensive experiments demonstrate the effectiveness and flexibility of the proposed method."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"This paper proposes a directed graph data augmentation method called Laplacian perturbation and theoretically analyzes how it provides contrastive information without changing the directed graph structure. A directed graph contrastive learning framework is also presented, which dynamically learns from all possible contrastive views generated by the proposed method. The proposed method is trained using multi-task curriculum learning to progressively learn from multiple easy-to-difficult views. Experiments on various benchmarks reveal the dominance of the proposed methods."
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes a multi-environment Symbolic Interactive Language Grounding benchmark (SILG), which unifies a collection of diverse grounded language learning environments under a common interface. The authors propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG. The shared architecture achieves comparable performance to environment-specific architectures. Moreover, the best models significantly underperform humans on the proposed benchmark."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper proposes a sparse version of the Vision Transformer (ViT) that is scalable and competitive with the largest dense networks while requiring as little as half of the compute at inference time. The authors also propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. Finally, the authors demonstrate the potential of V-MoE to scale vision models and train a 15B parameter model that attains 90.35% on ImageNet."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper studies the optimization landscape of training narrow neural networks. The authors show that as long as the width m > 2n/d, there exists at least one global minimizer with zero training loss, and identify a nice local region with no local-min or saddle points. However, it is not clear whether gradient descent can stay in this nice region. Finally, the authors consider a constrained optimization formulation where the feasible region is the nice region, and prove that every KKT point is a nearly global min."
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper proposes a continuous mean-covariance bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent’s objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, the authors consider three feedback settings, i.e., full-information, semi-bandit and full bandit feedback. They propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of the algorithms."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper proposes a non-commutative extension of the Multiplicative Update (MMU) algorithm for computing Positive Semidefinite (PSD) factorization of a matrix with non-negative entries, which is a collection of r-dimensional PSD matrices satisfying the condition Xij = tr(AiBj) for all i \in [m], j\in [n]. The PSD factorization task generalizes the Nonnegative Matrix Factorization (NMF) problem in which the goal is to find the collection of $r$-dimensional $r \times r$-dimension PSD matrix $Ai$ and $Bj$ satisfying the conditions $Xij = ai bj$. The authors show that the MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSd matrices, and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys. The authors demonstrate the utility of the proposed method with experiments on real and synthetic data."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes a meta-Domain Specific-Domain Invariant (mDSDI) framework that extends beyond the invariance view to further capture the usefulness of domain-specific information. The key insight is to disentangle features in the latent space while jointly learning both domain-invariant and domainspecific features in a unified framework. The proposed mDSDI is optimized through the meta-learning framework to adapt from source domains, targeting a robust generalization on unseen domains."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"This paper proposes to improve the quality of diffusion-based likelihood-based generative models (DiffusionGAN) by improving the model architecture and by using a classifier guidance scheme for trading off diversity for fidelity using gradients from the classifier. The authors show that the proposed method achieves FID of 2.97 on ImageNet 128, 4.59 on imageNet 256, and 7.72 on the ImageNet 512, and match BigGAN-deep even with as few as 25 forward passes per sample. Finally, the authors also show that their method can achieve state-of-the-art quality on unconditional image synthesis tasks."
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes to leverage out-of-distribution data for improving few-shot learning. Specifically, the authors propose a new loss function to learn the inductive bias of deep neural networks for FSL. The proposed method is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper studies the problem of prioritized experience replay (PER) in reinforcement learning. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this paper, the authors provide theoretical justifications for previous criteria, and propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks."
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,"This paper considers the problem of sequential prediction with expert advice in a nonstationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. The authors give a linear-time algorithm that improves on the best known regret bounds. This algorithm incorporates a relative entropy projection step. The authors also give an algorithm to compute this projection step in linear time, which may be of independent interest."
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper considers the problem of contextual linear bandits motivated by routing applications in navigational engines and recommendation systems. In this problem, the learner wishes to learn a hidden d-dimensional value $w^*$ and the goal is to minimize the total distance between the true point w^* and the hyperplanes the separation oracle returns. The authors design algorithms for this problem which achieve regret $O(\log T)$ and $O(d log d)$ regret, respectively. They also consider the variant where we are allowed to provide a list of several recommendations. "
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper introduces a set of orthogonal combinators for composing machine learning operators into pipelines. It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers. It presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper proposes a meta-learning approach that learns a weight initialization such that a small number of weight changes results in low generalization error. The authors show that this form of meta learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. They show that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems."
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"This paper considers the problem of learning a shared response model for multi-view neural imaging data. The authors propose to model each view as a linear transform of shared independent components contaminated by additive Gaussian noise. They show that this model is identifiable if the components are either non-Gaussian or have enough diversity in noise variances. They then show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset CCA fail. To solve this problem, the authors propose a new approach called ShICA-J, which is based on joint diagonalization and second-order statistics. They also propose a maximum-likelihood method, ShICAML, that is both more accurate and more costly."
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper studies the problem of training agents that generalize well to human co-players in common-payoff games without using human data. The authors argue that the crux of the problem is to produce a diverse set of training partners. To this end, the authors propose to train an agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method they call Fictitious Co-Play (FCP). The experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. They find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes a new policy gradient method for cooperative multi-agent reinforcement learning in discrete and continuous action spaces. The proposed method learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX. The authors also employ a nonmonotonic factorization and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, the proposed method uses a centralized policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space individually."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper proposes a biologically plausible key-value memory network that stores inputs using a combination of biologically plausible three-factor plasticity rules. The network performs on par with classical Hopfield networks on auto-associative memory tasks and can be naturally extended to continual recall, hetero-associationative memory, and sequence learning. The results suggest a compelling alternative to the classical hopfield network as a model of biological long-term memory."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper studies the problem of pairwise learning, where the loss function depends on a pair of instances. In this paper, the authors propose simple stochastic and online gradient descent methods to solve the optimization and generalization problems. The main idea is to only pair the current instance with the previous one in building a gradient direction, which is efficient in both the storage and computational complexity. The authors also extend their algorithms and stability analysis to develop differentially private SGD algorithms for the problem. "
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper presents REDO, a class-agnostic framework to REconstruct the Dynamic Objects from RGBD or calibrated videos. Compared to prior work, the problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but REDO aims to reconstruct the complete shape; 2) it aims to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) it aim to reconstruct different categories of objects with one unified framework. To address these challenges, the authors develop two novel modules. First, they introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues. Second, they develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. They study the efficacy of REDO in extensive experiments on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++, and on real-world video data 3DPW."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. The authors derive high-probability bounds on the performance of LSA under weaker conditions on the sequence {(An,bn) : n 2 N⇤} than previous works. However, in contrast, they establish polynomial concentration bounds with order depending on the stepsizes. "
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper extends the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. The authors propose general convergent off-policy inter-option learning algorithms, intra-option algorithms for learning values and models, as well as sample-based planning variants of the learning algorithms. They also extend the notion of option-interrupting behavior from the discounted to the average reward formulation. They show the efficacy of the proposed algorithms with experiments on a continuing version of the Four-Room domain."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper proposes an auxiliary self-supervised task to improve the performance of visual transformers (VTs) on small-data regime. The task consists of two tasks: 1) extract additional information from an image, and 2) learn spatial relations within an image. The proposed task is used in the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Experiments show that the proposed task can improve the final accuracy of the VTs on various datasets."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"This paper proposes a new method for label-free alignment of hierarchical data in hyperbolic spaces. The proposed method consists of three components: translation, scaling, and rotation, which are based on the Riemannian geometry of the Lorentz model. The authors analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. "
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper studies the problem of differentially private (DP) query answering systems that are required to produce micro-data, i.e., the output of a DP system that produces noisy answers to a set of queries. The authors show that DP systems that do not require micro-datasets will have errors that are at least logarithmic in the number of queries, compared to DP systems which do. The paper provides lower bounds for pure DP, approximate DP, and concentrated DP, as well as mitigation strategies to mitigate the problem."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"This paper proposes a method to improve goal-conditioned reinforcement learning (RL) by training a planner and an RL agent. The planner learns to decompose a long-horizon task to a tree of sub-tasks in a top-down manner, whose layers construct coarse-to-fine sub-task sequences as plans to complete the original task. The planning policy is trained to minimize the RL agent’s cost of completing the sequence in each layer from top to bottom layers, which gradually increases the sub- tasks and thus forms an easy to hard curriculum for the planner. Next, a bottom-up traversal of the tree trains the RL agents from easier sub-samples with denser rewards on the bottom layers to harder ones on the top layers and collects its cost to train the planner in the next episode. The authors compare the proposed method with RL (SAC, HER, PPO), planning (RRT*, NEXT, SGT), and their combination (SoRB) on navigation and continuous control tasks."
SP:9911693a04a300b5a93634fb0267ef83e5489d77,This paper proposes a Bayesian framework for generating local explanations for LIME and KernelSHAP. The main idea is to use a closed form expression for the hyperparameters to estimate the number of perturbations required to generate explanations that satisfy the desired confidence levels. The proposed approach is evaluated on several real-world datasets and user studies.
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper proposes a method to improve the performance of Adder neural networks (ANNs) by addressing the issue of unordered heavy tails in ANNs. Specifically, the authors propose to pre-define the feature distributions in order to model the heavy-tailedness of ANNs and instead make use of a mixture of Multivariate Skew Laplace Distributions (MSE) for ANNs instead of Gaussian. The authors also propose a novel method to tackle existing heavy tails with only a modification of classifier where ANN features are clustered with their tails well-formulated through proposed angle-based constraint on the distribution parameters to encourage high diversity of tails. Experiments conducted on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance."
SP:cbccb65457564992d534504c0d060da44cafce8c,"This paper identifies and formalizes a fundamental gradient descent phenomenon leading to a learning proclivity in over-parameterized neural networks. Gradient Starvation (GS) arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This paper provides a theoretical explanation for the emergence of such feature imbalances in neural networks and identifies simple properties of learning dynamics during gradient descent that lead to this imbalance, and proves that such a situation can be expected given certain statistical structure in training data. Based on the proposed formalism, the authors develop a novel but simple regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper performs a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, the authors also quantify subjective measures of the human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. They find that humans have a clear preference toward a rule based AI teammate (SmartBot) over a state-of-the-art learning based AI (Other-Play) across nearly all subjective metrics, and generally view the learning based agent negatively. This result has implications for future AI design and reinforcement learning benchmarking."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper proposes a method for visual question generation (VQG) based on visual and answer hints. The proposed method is based on the idea that the salient visual regions of interest can be viewed as a constraint to improve the generation procedure for producing high-quality questions. Given the predicted visual region of interest, the model can focus on estimating the probability of being ground-truth questions, which in turn implicitly measures the quality of predicted visual hints. Experimental results on two benchmark datasets show that the proposed method outperforms the state-of-the-art approaches by a large margin on both automatic machine metrics and human evaluation."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes a generalized data weighting (GDW) method to mitigate label noise and class imbalance by manipulating gradients at the class level. Specifically, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. Extensive experiments in various settings verify the effectiveness of GDW."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper introduces a spatio-temporal language grounding task where the goal is to learn the meaning of spatiotemporal descriptions of behavioral traces of an embodied agent by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatiostemporal references to objects in the scene. To study the role of architectural biases in this task, the authors train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. They test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalisation to grammar primitives. They observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper proposes a method for multiple object tracking and segmentation in videos. The proposed method first distills the space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, the proposed method adopts a prototypical appearance module to learn the contrastive foreground and background prototypes, which are propagated over time. Experiments on Youtube-VIS and BDD100K datasets demonstrate the effectiveness of the proposed approach."
SP:1175ad16382b349ab1a39895150172d266abe571,"This paper studies the relationship between gradient descent and gradient flow in deep neural networks. The authors view gradient descent as an approximate numerical solution to the initial value problem of gradient flow, and show that the degree of approximation depends on the curvature around the gradient flow trajectory. They then show that over deep neural network with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. Finally, they show that gradient descent with conventional step size is indeed close to gradient flow."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers a stochastic multi-armed bandit problem with delayed impact of actions. In this setting, actions taken in the past impact the arm rewards in the subsequent future. This delayed impact is prevalent in the real world. This paper generalizes the bandit setting to encode the dependency of this “bias” due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. The authors propose an algorithm that achieves a regret of $\tilde{O}(\KT^2/3)$ and show a matching regret lower bound of $\Omega(\sqrt{T})$ where T is the learning horizon."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"This paper proposes an end-to-end solution for video instance segmentation (VIS) based on transformers. Specifically, the authors propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. Experiments on the latest benchmark sets and achieved state-of-the-art performance while having a considerably fast runtime (89.4 FPS)."
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a general graph embedding method that can debias various structural biases in graphs by using random graphs. Specifically, the authors show that random walks are biased by the degree of each node, where a node is sampled proportionally to its degree. The proposed method, called residual2vec, improves link prediction and clustering performance and allows us to explicitly model salient structural properties in graph embeddings. "
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies the problem of estimating the power sum functional of a discrete distribution p = (p1,..., pK) under local differential privacy (LDP). In particular, the authors consider two settings: (i) sequentially interactive (i.e. they are allowed to use already published confidential data) or (ii) non-interactive. In the interactive setting, they show that for all $\gamma > 1$, there exists a plug-in type estimator that attains a parametric rate of 1/\sqrt{n}^{1/2} for all $p \gamma \in \mathbb{R}^k$. In the non-Interactive setting, there exist two plug in type estimators that are similar to the MLE analyzed by Jiao et al. [18] in the multinomial model. However, due to the privacy constraint, the rates are slower and similar to those obtained in the Gaussian model by Collier et al [9]. "
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper studies the problem of online multiclass classification in the setting where the learner’s feedback is determined by an arbitrary directed graph. In this setting, feedback graphs allow a much richer set of applications, including filtering and label efficient classification. The authors introduce GAPPLETRON, a new algorithm that works with arbitrary feedback graphs. They prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. They also prove a general lower bound of order max {BK, \sqrt{T} } showing that their upper bounds are not significantly improvable."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of explainable clustering in the setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). A k-clustering is said to be explainable if it is given by a decision tree where each internal node splits data points with a threshold cut in a single dimension (feature), and each of the k leaves corresponds to a cluster. The authors give an algorithm that outputs an explainability clustering that loses at most a factor of $O(\log k)$ compared to an optimal (not necessarily explainable) clustering for the k-medians objective, and a factor $O(k log k)$. This improves over the previous best upper bounds of $\Omega(\sqrt{k})$ and \Omega(k)$ respectively. The algorithm is remarkably simple. It is oblivious to the data points and runs in time O(dk log k)."
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper proposes a multilingual language model that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in the model, which brings unprecedented PrLM interpretability and convenience in downstream task use. The model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets."
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper proposes a transformer-based method for learning heuristics for vehicle routing problems (VRPs). The key idea is to separate the node and positional embeddings for the VRP solution separately, instead of fusing them together as done in existing ones, so as to avoid potential potential noises and incompatible correlations. The positional features are embedded through a novel cyclic positional encoding (CPE) method to allow Transformer to effectively capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). The authors train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. They apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) and show that their DACT outperforms existing Transformer based improvement models, and exhibits much better generalization performance across different problem sizes."
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"This paper proposes a method to compute the exact Bayes error of generative models learned using normalizing flows. The method relies on a fundamental result, which states that the Bayesian error is invariant under invertible transformation. The authors then use this result to estimate the exact error of the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, the authors show that by varying the temperature of the models, they can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes errors. They use their approach to conduct a thorough investigation of state-of-the-art classification models, and find that in some cases, these models are capable of obtaining accuracy very near optimal."
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper proposes an initialization method for convolutional neural networks. The proposed method, GradInit, is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many Convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup."
SP:f69731403592fa5bdd4ca327708582d615aa131c,"This paper proposes a method to predict disease progression using longitudinal data. The proposed method is based on the Riemannian manifold and learns patient-specific trajectories distributed around a central geodesic. The method learns the metric as the push-forward of the Euclidean metric by a diffeomorphism, which is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, they introduce parallel Procedural Units (PUs). A PU consists of a memory head and a procedure. The memory head maintains a summary of a type of features. For an intermediate feature, we search its closest memory and forward it to the corresponding procedure in both training and testing. Different procedures are tailored to different features and therefore tackle them better."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper studies equivariant polynomial functions, i.e. functions that can be expressed in terms of a lightweight collection of scalars (scalar products and scalar contractions) of the scalar, vector, and tensor inputs. The authors show that it is possible to parameterize universally approximating functions that are equivariants under the symmetries of the Euclidean, Lorentz, and Poincaré groups, at any dimension d. The paper also provides numerical examples that show that the proposed method is simple, efficient, and scalable. "
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper proposes a generalization of the Intersection over Union (IoU) loss for bbox regression to a new family of power IoU losses with a single power parameter alpha. The authors analyze properties such as order preservingness and loss/gradient reweighting, and show that the proposed loss outperforms existing IoU-based losses by a noticeable margin. The proposed loss is also more robust to small datasets and noisy bboxes."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper studies Distributionally Robust Imitation Learning (DROIL) and establishes a close connection between DROIL and Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) by showing that MaxEntropy IRL is a special case of DROIL when a certain loss function is used and a certain policy is used. The authors develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. Their approach lets them optimize both stationary and non-stationary policies and, unlike prevalent previous methods, it does not require repeatedly solving an inner reinforcement learning problem. They experimentally show the significant benefits of the new optimization method on synthetic data and a highway driving environment."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper proposes a general post-processing algorithms for individual fairness (IF) for fine-tuning NLP models. The authors cast the individual fairness problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired “treat similar individuals similarly” interpretation. Theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, the authors demonstrate the efficacy of the proposed algorithms on two large-scale text data sets by reducing biases in fine- tuned BERT models."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a method for cross-domain text-to-SQL task. The authors propose a structure-aware Dual Graph Aggregation Network (SADGA) for the question-schema linking problem. SADGA adopts the graph structure to provide a unified encoding model for both the natural language question and the database schema. The proposed method is based on global graph linking, local graph linking and DualGraph Aggregation Mechanism. "
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of learning stochastic computations graphs with multiple sequential discrete components. The authors analyze the behavior of the parameters of these models, mainly due to small gradients and local minima. They propose two strategies to overcome these challenges. First, they show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, they propose dropout residual connections specifically tailored to stochastically, discrete-continuous computation graphs."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"This paper studies the robustness of Bayesian neural networks (BNNs) to covariate shift. The authors show that BNNs with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo (HMC) achieve poor generalization performance when the target data distribution is different from the training distribution. They also show that the same issue does not affect many approximate inference procedures or classical maximum a-posteriori (MAP) training. Finally, they propose novel priors that improve BNN robustness to many sources of covariate shifts."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper categorizes meta-learning evaluation into two settings: in-distribution (ID), in which the train and test tasks are sampled iid from the same underlying task distribution, and out-of-dist distribution (OOD). The authors identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because—as the authors show on numerous benchmarks—meta-learning methods that perform better on existing OOD datasets may perform significantly worse in the ID setting. To address these concerns, the authors provide suggestions on how to construct FSL benchmarks to allow for ID evaluation and more reliable OOD evaluations."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper proposes a method for open rule induction based on the pre-trained language models (LM). The authors argue that the current LM-based rule generation methods are “learning rules from rules”, which limits these methods to only produce “canned” rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. The authors propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision. The proposed method is evaluated on relation extraction tasks (i.e. relation extraction), and it is shown that the automatically inducted rules outperform the manually-annotated rules."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,"This paper proposes Implicit Constraint Q-learning (ICQ) for offline multi-agent reinforcement learning. The main contribution of this paper is to propose a novel offline RL algorithm that alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation. Moreover, the authors extend ICQ to the multi agent tasks by decomposing the joint-policy under the implicit constraint. Experimental results show that the proposed ICQ can control the extrapolations error within a reasonable range under any number of agents and learn from complex multi agent datasets."
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper studies adversarial robustness against evasion attacks, with a focus on applications where input features have to comply with certain domain constraints. The authors propose a methodology to generate non-uniform perturbations that take into account the characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. They assume Gaussian data distribution in the consistency analysis, and precomputed covariance matrix and Shapley values. Under these assumptions, their results on three different applications demonstrate that nonuniform adversarial perturbation sets in AT improves adversarial training and adversarial certification."
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper considers the problem of supervised learning, where the goal is to find an estimator $\theta$ that minimizes the expected risk $L$ in a separable Hilbert space $\mathcal{H} = \sum_{x,y} \in \mathbb{X}^d \times Y$, where $x$ is an input point and $y$ is a label. The authors consider the generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. They show that fast and optimal rates can be achieved for GSC by using the iterated Tikhonov regularization scheme, which is intrinsically related to the proximal point method in optimization. "
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper introduces a new kind of linear transform named Deformable Butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. The authors apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity."
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes MetA Reusable Knowledge (MARK), a method to address catastrophic forgetting in continual learning (CL) by encouraging weight updates that are useful for multiple tasks and enriching the knowledge base (KB) with new knowledge as the model learns new tasks. Specifically, Mark keeps a set of shared weights among tasks and uses a metalearning approach to incrementally enrich the KB to foster weight reusability among tasks. A set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. Mark achieves state-of-the-art results in several popular benchmarks."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data-driven approach to systematically improve the use of primal heuristics in B&B by learning from data about the ordering and duration of every heuristic call for a set of training instances, which specifies the time and success rate for which each heuristic should be executed. The authors also propose an efficient algorithm for computing such a schedule. Compared to the default settings of a state-of-the-art academic MIP solver, the proposed approach can reduce the average primal integral by up to 49% on two classes of challenging instances."
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper studies the problem of reinforcement learning in the setting where the learner receives binary feedback only once at the end of an episode. The authors study the case where the trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sublinear regret. This is an extreme test case for theory, but it is also arguably more representative of real-world applications than the traditional requirement in RL practice. Theoretically, the authors show that learning is possible in this more challenging setting."
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. The experimental results show that the proposed method largely outperforms existing graph representation learning methods."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of representations learned by maximizing mutual information (MI) between random variables in the context of deep reinforcement learning (RL). In particular, the authors focus on two commonly used objectives for representation learning based on mutual information maximization, i.e., maximizing the mutual information between states, actions, and rewards at different time-steps. They show that two of these objectives are insufficient for the general class of MDPs, in the most general case, and prove that another typical objective is sufficient. The authors also provide some theoretical results to support their findings. "
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper proposes a new convolutional layer for 3D semantic analysis. The proposed method, called sparse steerable convolution (SS-Conv), is based on the idea of rotation-steerable convolutions (SO(3)-equivariant convolutions. The convolution kernels are constructed as linear combinations of basis ones based on spherical harmonics, which satisfy the rotation equivariance constraint of the convolution layer. A novel Feature-Steering module is also designed into the pipeline to support iterative pose refinement, by taking advantage of the steerability of the learned features. Experiments on three tasks of 3D object semantic analysis, including instance-level 6D pose estimation, category-level pose and size estimation, and category level 6D tracking, show the effectiveness of the proposed method."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, the authors devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers of the vision transformer to reduce redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, they propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. By hierarchically pruning 66% of the input tokens, the method greatly reduces the FLOPs and improves the throughput by over 40%."
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper studies the problem of distribution-free inference, i.e. the inference of the conditional mean of the underlying regression function. The authors consider the setting where the features X are continuously distributed, where the support size of the distribution of X is smaller than the square of the sample size. They show that there are several regimes in between the finite setting and the continuous setting, where vanishing-width confidence intervals are achievable if and only if the effective support size is small. They also show that if X takes only a small number of possible values, then inference is trivial. "
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes Representation Neutralization for Fairness (RNF) to reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs. The proposed method leverages samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification heads of the DNN model. The key idea of RNF is to discourage the classification classification head from capturing undesirable correlations between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, the authors leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate the effectiveness of the proposed method."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper proposes a new convolutional layer that takes advantage of Bessel functions, well known in physics, to build Bessel-CNNs (B-CNN) that are invariant to all the continuous set of possible rotation angles by design. In B-CNN, the actual convolution can be converted to only one convolution. Experiments are conducted on MNIST, CIFAR-10 and Fashion-MNIST."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper proposes a new large-scale solver for kernel ridge regression. The approach combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. The main novelty is in the form of the considered partition, that is defined in the feature space, rather than in the input space as in traditional partitioning methods. This allows to promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. "
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper proposes a method for learning to communicate in RL using discrete tokens derived from a learned, continuous space. The authors claim that the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring desirable properties of communication such as compositionality and zero-shot understanding. They show in a decision theoretic framework that their technique optimizes communication over a wide range of scenarios. They also validate that their trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail."
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper proposes a hybrid network architecture called CoAtNets, which combines the strengths from both convolutional networks and transformers. The authors show that depthwise convolution and self-attention can be unified via simple relative attention, and vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that the proposed model achieves state-of-the-art performance under different resource constraints across various datasets."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper proposes a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality, which is amenable to efficient minimization. The authors also derive a new concentration of measure inequality that combines PAC-Bayesian bounding with Bennett’s inequality for empirical estimation."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a weakly-supervised method for audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, the proposed method explores event co-occurrence across audio, visual, and audio- visual streams. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes a personalized federated learning algorithm QuPeD for learning quantized models with knowledge distillation (KD) among clients with different quantization parameters and model dimensions/structures. The authors propose a relaxed optimization problem, where quantization values are also optimized over, and use alternating proximal updates to minimize the proximal objective and analyze its convergence properties. In the federated setting, the authors show the convergence rate of $O(1/\sqrt{T}$ for the centralized setting and $O(\sqrt{\log T})$ for federated settings. Numerical experiments are conducted to validate the effectiveness of the proposed algorithm. "
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"This paper proposes a deep generative model for constrained clustering. The key idea is to use a mixture-of-Gaussians prior on the latent representation of the data conditioned on the clustering preferences, expressed as pairwise pairwise constraints. The clustering process is guided by these constraints by indicating which samples should or should not belong to the same cluster. The proposed method is trained using stochastic gradient variational inference. Experiments are conducted to demonstrate the effectiveness of the proposed method."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper proposes two methods for approximating the Neural Tangent Kernel (NTK) and Convolutional NTK (CNTK). The first method is based on sketching the polynomial expansions of arc-cosine kernels, and the second one combines random features (based on leverage score sampling) with a sketching algorithm to approximate the convolutional counterpart of NTK. The authors also prove a spectral approximation guarantee for the NTK matrix, by combining random features and sketching. The proposed methods are evaluated on various large-scale regression and classification tasks and show that a linear regressor trained on the CNTK features matches the accuracy of exact CNTk on CIFAR-10 dataset while achieving 150x speedup."
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"This paper proposes a multi-person 3D motion trajectory prediction framework for long-term human motion prediction. The proposed Multi-Range Transformer (MRT) consists of a local-range Transformer encoder for individual motion and a global-range encoder to model social interactions, and a Transformer decoder to perform prediction for each person by taking a corresponding pose as a query which attends to both local and global range encoder features. The MRT outperforms the state-of-the-art methods on the task of long term motion prediction, and generates diverse social interactions. "
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a method for automatically generating guiding programs for long-horizon reinforcement learning (LHRL). The method is based on a conditional generative model of the environment and a high level specification of the goal of the task to automatically synthesize a program that achieves the goal, with the synthesized program robust to uncertainty in the model. The proposed method is evaluated on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper studies the problem of causal imitation learning in sequential settings, where the imitator must make multiple decisions per episode. The authors develop a graphical criterion that is necessary and sufficient for determining the feasibility of causal imitators. They also provide an efficient algorithm for determining imitability and corroborate their theory with simulations."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper presents a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. The authors show that the combination of an object-level loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"This paper studies the problem of empirical risk minimization (ERM) in the context of using adaptively collected data to minimize the average of a loss function over a hypothesis class and provide first-of-their-kind generalization guarantees and fast convergence rates. The authors propose a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the good dependence on the exploration rate in the data. For regression, the authors provide fast rates that leverage the strong convexity of squared-error loss. For policy learning, they provide regret guarantees that close an open gap in the existing literature whenever exploration decays to zero."
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper proposes a novel and coherent scheme for kernel-reweighted regression by reparametrizing the sample weights using a doubly non-negative matrix. When the weighting matrix is confined in an uncertainty set using either the log-determinant divergence or the Bures-Wasserstein distance, this paper shows that the adversarially reweighted estimate can be solved efficiently using first-order methods. Numerical experiments show that the reweighting strategy delivers promising results on numerous datasets."
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper proposes a categorical gradient estimator based on importance sampling and statistical couplings, which is extended to the categorical setting. The authors introduce gradient estimators based on reparameterizing categorical variables as sequences of binary variables and Rao-Blackwellization. Experiments show that the proposed estimators provide state-of-the-art performance, whereas previous estimators (Yin et al., 2019) underperform."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a novel predictor-based neural architecture search (NAS) method that progressively shrinks the sampling space, by learning a series of weak predictors that can connect the best architectures to the best sub-space. The proposed method, called WeakNAS, produces coarse-to-fine iteration to gradually refine the ranking of sampling space. Extensive experiments demonstrate that the proposed method is both sample-efficient and robust to various combinations of predictors and architecture encoding means."
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes a new intrinsic control method for unsupervised reinforcement learning. The proposed method, EDDICT (Entropic Desired Dynamics for Intrinsic ConTrol), assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. Compared to prior methods, the proposed method’s globally consistent codes allow it to be far more exploratory, as demonstrated by improved state coverage and increased unsupervisory performance on hard exploration games such as Montezuma's Revenge."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,This paper proposes a fragment-based generative model to generate pharmacochemically acceptable molecules with large docking scores. The proposed method is based on the idea that the molecule should satisfy strong structural constraints such as chemical realisticness and pharmacochemical suitability. The authors also propose a novel error-prioritized experience replay (PER) to encourage the generation of qualified molecules while only allowing the model to explore the space to find drugs. The model achieves state-of-the-art performance on two of the three targets in terms of the docking scores of the generated molecules.
SP:b938bca513e7de1231212064caf8877a78d8b612,This paper studies the problem of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. The approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. The sample complexity is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature.
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper studies the problem of learning with differential privacy (DP) in the setting where each user has $m$ samples and the privacy protection is enforced at the level of each user’s data. It shows that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an ("", )DP algorithm using only $O(log(1/\epsilon^2/\eps)$ users. The main contribution of this paper is a generalization of global stability [BLM20] that allows the use of public randomness. The global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples."
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper provides a theoretical analysis of the performance of end-to-end model-based reinforcement learning (EML) methods, where the transition and reward models of a latent Markov decision process (MDP) are learned to fit the data. The authors show that, for a linear parametrization, gradient descent converges to global optima despite nonlinearity and non-convexity introduced by the implicit representation. They also provide conditions under which stochastic gradient descent with this implicit representation converges substantially faster than its explicit counterpart."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes a method to improve the performance of Knowledge Graph (KG) embeddings by combining two techniques: (1) Probabilistic Soft Logic (PSL-KGI) and (2) KG embedding techniques such as ComplEx and ConvE. The authors propose a KG refinement framework called IterefinE, which iteratively combines the two techniques – one which uses ontological information and inferences rules, and the other which does not. The proposed method is evaluated on a variety of KG benchmarks and shows improved performance over the baselines."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a novel evaluation paradigm for the task of knowledge base completion (KBC). In particular, the authors construct the data set FB14k-QAQ with an alternative evaluation data structure: instead of single facts, they use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. They randomly remove some of these correct answers from the dataset, simulating the realistic scenario of real-world entities missing from a KB. The authors evaluate a number of state-of-the-art KB embeddings models on the new benchmark."
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes a simple, general, and effective framework: Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pretrained language model. The model generates dialog responses that successfully persuade people to donate to a charity. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,This paper proposes to use the expected Bayes factor (EBF) as a measure of uncertainty for the top-k uncertainty for image classification. The authors show that EBF can be used to measure the confidence that the classification is correct on the test set. They also prove that high confidence implies a high probability of correct classification on test sets. 
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the spectrum of neural networks in the infinite-width limit. In particular, the authors show that the spectrum is the same as that of the Neural Network Gaussian Process (NNGP) kernel. The authors also show that gradient descent training of wide neural networks is described by a kernel called the Neural Tangent Kernel (NTK) that is related to the NNGP kernel. They show that in the large depth limit, NTK spectrum simplifies in much the same way as the neural network Gaussian process kernel."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper proposes a graph-based method to estimate the quality of protein models, that possesses favorable properties such as representation learning, explicit modeling of both sequential and 3D structure, geometric invariance and computational efficiency. The authors demonstrate significant improvements over the state-ofthe-art for both hand-engineered and representation-learning approaches, as well as carefully evaluating the individual contributions of GRAPHQA components."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper studies the landscape of the loss function of linear neural networks from a geometric perspective. In particular, the authors view the loss as a composition of the parameter space and the functional space of the network. The functional space is either the set of all linear maps from input to output or a determinantal variety, i.e., a set of linear maps with bounded rank. The authors introduce a distinction between pure critical points and spurious critical points, which are determined by the geometry of the function space and by the parameterization of this space by the network’s weights. They show that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all the linear maps (filling architectures), but it holds only for the quadratic loss (non-filling architecture)."
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper proposes a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Given an input graph, SEED samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub-graph vectors, and employs the embedding of the sub graph vector distribution as the output vector representation for the input graph. Theoretical analysis shows the close connection between SEED and graph isomorphism. The empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods."
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper studies the problem of solving two-player zero-sum extensive games with imperfect information (TEGI) using counterfactual regret minimization (CFR). The authors propose Lazy-CFR, a CFR algorithm that adopts a lazy update strategy to avoid traversing the whole game tree in each round. Theoretical analysis shows that the regret is almost the same as the regret of the vanilla CFR and only needs to visit a small portion of the game tree. Empirical results consistently show that LazyCFR is fast in practice."
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper proposes a method for Unsupervised Domain Adaptation (UDA) by modeling the deep features from each domain as Gaussian mixture distributions. The authors propose two new domain discrepancy losses to measure the discrepancy between the two domains. The first one minimizes the distance between the corresponding Gaussian component means of the source and target data, and the second one minimises the pseudo negative log likelihood of generating the target features from the source feature distribution. The proposed method is evaluated on the Digits Image transfer task and VisDA 2017 dataset."
SP:40be996e8bb86e887077b762b87c7c34a786ac98,This paper proposes an efficient conditional continuous normalizing flow (CNF) model that partitions the latent space into a class-specific supervised code and an unsupervised code that is shared among all classes for efficient use of labeled information. The proposed method also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. Experiments on CIFAR-10 and Time-series data demonstrate the effectiveness of the proposed method.
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies the convergence of TD learning in the lazy regime, i.e., the regime where the parameters of the model vary only slightly during the learning process. The authors consider the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. They prove exponential convergence to local, respectively global minimizers of the above algorithm under a certain scaling of the approximating function, leading to a regime called lazy training. They also give examples of such convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the cases of neural networks."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper proposes a method to train an agent to generate observations that can help predict whether a given hypothesis is true or false. The agent is trained using reinforcement learning, where it is given a hypothesis about the dynamics of the world and the goal is to take actions that can generate observations to verify whether the hypothesis is correct or not. The main contribution of the paper is that the authors propose to use the underlying structure of hypotheses, which can be formulated as a triplet of a pre-condition, an action sequence, and a post-condition. This structure allows the agent to be pretrained to verify hypotheses with this structure, and can then be fine-tuned to verify more general hypotheses. "
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper studies the feasibility of performing mathematical reasoning in a fixed dimensional latent space. The authors propose a method to predict the embeddings of rewrites (i.e. transformations) that can be successfully performed on a statement, such that it can be used to predict whether a statement can be rewritten by other theorems. The proposed method is based on graph neural networks, and the authors conduct experiments on HOList, a benchmark for automated theorem provers based on deep learning. "
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"This paper proposes a novel approach to learn depth from images and very sparse depth measurements, just a few pixels per image. To learn from such extremely sparse supervision, the authors introduce an appropriate inductive bias by designing a specialized global-local network architecture. Experiments on several datasets show that the proposed model can learn monocular dense depth estimation when trained with very sparse ground truth. Moreover, the global parameters extracted by the network are predictive of the metric agent motion."
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper extends the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. They show that by applying a multi-layer Transformer to these Bloom filter digests, they are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, trained using sampled softmax with the same computational budget."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper proposes a learning-based agglomerative clustering framework to learn the geometry prior of parts and transfer this prior to unseen categories. The core idea is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen classes. The proposed method achieves the state-of-the-art performance on the fine-grained 3D part dataset."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a technique called neuron editing that learns how neurons encode an edit for a particular transformation in a latent space of an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in the latent space, the authors encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron’s activations. The technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications. "
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper proposes a meta-learning approach for few-shot semantic segmentation. The idea is to meta-learn the initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of training data for a new task. The authors propose a novel neural network architecture built for parameter efficiency and fast learning which they call EfficientLab, a formalization of the generalization error of meta- learning algorithms, which they leverage to decrease error on unseen tasks, and a small benchmark dataset, FP-k, that contains 400 training examples for 5 tasks each. The empirical results show that the proposed approach outperforms random and ImageNet-trained initializations on the FSS-1000 dataset and larger datasets."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a method for semi-supervised few-shot learning (SS-FSL) based on prototypical networks (PN). The proposed method is based on the idea of random walk, which is a graph-based loss function that encourages the network to learn representations that are compact and well-separated. Experiments on mini-ImageNet and Omniglot show that the proposed method outperforms the state-of-the-art methods."
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a new self-supervised training objective, Contrastive Sensor Fusion (CSF), which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. The proposed method outperforms fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper proposes two methods for retraining after pruning neural networks. The first method, weight rewinding, trains the weights and the learning rate at the end of training to their values from earlier in training and retrains them from there using the original training schedule. The second method, learning rate rewindings, trains unpruned weights from their final trained values using a small fixed learning rate. Both methods outperform fine-tuning in terms of accuracy and compression ratios."
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper proposes a new notion of margin, called ""all-layer margin"", to analyze the relationship between output margin and generalization in deep neural networks. The authors show that the all-layer margins have a clear and direct relationship with generalization for deep models. This enables the following concrete applications: 1) by analyzing the all layer margin, the authors obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth; 2) the neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks; 3) the authors present a theoretically inspired training algorithm for increasing the all layers margin."
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper proposes a framework for knowledge-grounded open domain dialogue generation. The authors propose a disentangled response decoder to isolate parameters that depend on knowledge from the entire generation model. The decoder is decomposed into conditionally independent components including a language model, a context processor, and a knowledge processor. The three components are coordinated by a decoding manager that dynamically determines which component is activated for response prediction. The language model predicts the next word of a response based on the prior sub-sequence and the context processor ensures coherence of the conversation history with the context encoder. Both components are independent with the knowledge and thus can be pre-trained using the ungrounded dialogues and plain text. The knowledge processor is responsible for grounding response generation and can be used to adapt the model to a new task. "
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a new neural machine translation model (NMT) architecture that integrates the source to target translation model, the target to source translation model and two language models. The main idea is that both translation models and language models share the same latent semantic space, therefore both translation directions can learn from the non-parallel data more effectively. The proposed model is evaluated on a variety of language pairs and scenarios, including resource-rich and low-resource situations."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper investigates the role of the entropy term in the performance of maximum-entropy deep reinforcement learning (DRL) algorithms. Specifically, the authors first show that the SAC algorithm, which combines off-policy learning with maximum entropy RL, has a similar structure to TD3, but also employs maximum entropy reinforcement learning. Based on this insight, they propose a new algorithm called Streamlined Off Policy (SOP), which is a streamlined algorithm using the standard objective without entropy term. They also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training, which is called Empirical Recent Experience (PER) and outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper studies the vulnerability of copyright detection systems to adversarial attacks. The authors re-interpret the Shazam algorithm for music fingerprinting as a neural network and build a differentiable implementation of it in TensorFlow (Abadi et al., 2016). By using a gradient-based attack and an objective that is designed to achieve good transferability to black-box models, they create adversarial music that is easily recognizable to a human, while evading detection by a machine. With sufficient perturbations, they demonstrate that adversarial examples can fool industrial systems, including the AudioTag music recognition service (AudioTag, 2009), and YouTube’s Content ID system(Google, 2019)."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper proposes an activation decomposition framework for visual explanation of deep metric learning and explore the relationship between each activated region by point-topoint activation response between two images. The proposed method is generated by decomposing the similarity similarity along each of the query image and the retrieved image. The experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper studies the problem of learning control in an online lifelong learning setting, where the underlying dynamics of the environment may change over the course of the agent's lifetime. The authors propose a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. By measuring the performance of the planner and the uncertainty of the model free components, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes to replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and total-variation Sparse Attention (TVMAX). With sparsemax, we obtain sparse attention weights, selecting relevant features. By selecting relevant groups of features, the TVMAX transformation improves interpretability. TVMAX outperforms the other compared attention mechanisms in terms of human rating and attention relevance."
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes a method to predict the evolution of dynamic graphs. Specifically, the authors use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graph, and employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. The authors evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets. The proposed model is compared against several baseline methods."
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The proposed method is based on neural networks trained using an adversarial objective function. Additionally, a predictor is trained on the generated samples from the imputer network which is able to reflect the impact of uncertainties over missing values. Experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing data."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper studies the problem of estimating the long-term reward of a target policy, given historical data collected by (possibly unknown) behavior policies. The authors formulate the problem as solving for the fixed point of a certain operator, and use tools from Reproducing Kernel Hilbert Spaces (RKHSs) to develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off-policy data are collected. They analyze its asymptotic consistency and finite-sample generalization."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper proposes a generative adversarial network (GAN) approach to compute Gaussian mixture model (GMM) conditional likelihood (p(x|k,\theta) and p(k|x, \theta), p(z|z, k|x) in high-dimensional data space. The main idea is to use a GAN to generate the latent space z, and then compute the conditional likelihood and the responsibility probability at the data’s latent space instead of at the input x. The proposed approach is based on the GAN framework of Goodfellow et al. (2014)."
SP:2da1608209058d214f8671062cc9eb0833ba4831,This paper presents a method to train large capacity neural networks with significantly improved accuracy and lower dynamic computational cost by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. The authors also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. The proposed method is evaluated on CIFAR-10 and ImageNet datasets for image classification and Cityscapes for semantic segmentation.
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"This paper proposes a probabilistic importance inference approach for pruning DNNs. Specifically, it test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemetric scoring test and keep only those significant ones. The proposed approach achieves better lossless compression rates than existing techniques."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a method for learning hierarchical representations for hierarchical reinforcement learning. The proposed method is based on the idea of compressed action sequences, which can be compressed to yield a compact code of action trajectories. The learned representations can be used to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary length. The method is evaluated on a set of tasks with non-trivial hierarchical structure, and is shown to be able to accelerate learning in recursively more complex tasks."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"This paper proposes a variational autoencoder (VAE) model with a multimodal decoder. The decoder is modeled as an energy-based model (EBM) instead of a unimodal Gaussian distribution. The model is trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. The authors also introduce a simple extension called the Set-HBAE where each data point is a collection of examples (e.g., a set of images). In this setting, the model is able to learn to reconstruct the input data points and reconstruct the set."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper studies the effect of normalization techniques on off-policy RL algorithms. The authors propose a normalization technique called cross-normalization, which is an extension of batch normalization that re-centers data for two different distributions, i.e., actions in on-policy transitions and actions in the current policy. Experiments on DDPG and TD3 show that the proposed approach improves performance over the baseline."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper proposes a method to learn discriminative features that are unbiased and invariant to the confounder(s) present in the data. The authors propose a new adversarial loss function that encourages a vanished correlation between the bias and learned features. They apply their method to synthetic data, medical images, and a gender classification (Gender Shades Pilot Parliaments Benchmark) dataset. The results show that the learned features are uncorrelated with the bias or confounders variables."
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper proposes a lightweight Transformer-based character-level language model, called Group-Transformer, that factorizes the calculation paths by grouped embedding operators and employs inter-group linear operators to prevent performance degradation from the group strategy. The proposed model is inspired by the group convolution approaches that have effectively compressed huge image processing models for usability on mobile devices. The authors conducted extensive experiments on two benchmark datasets, enwik8 and text8 and found that Group-transformer with 6M parameters outperformed all LSTM-based models with under 35M parameters."
SP:946c26d371297c88d0ac246257104099b4585edc,"This paper proposes to train deep-latent variational autoencoders (WAEs) using Optimal Transport (OT) framework, which is an alternative, non-likelihood-based framework for training generative models with appealing theoretical properties, in principle allowing easier training convergence between distributions. The authors show that the method enables the generative model to fully leverage its deep latent hierarchy, and that in-so-doing, it is more effective than the original Wasserstein Autoencoder with Maximum Mean Discrepancy divergence."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"This paper presents an autoregressive video generation model based on a three-dimensional self-attention mechanism that can be implemented efficiently on TPUs. The proposed method is a generalization of the Transformer architecture of Vaswani et al. (2017) using block-local attention, which is a 3D-based attention mechanism with direct interactions between representations of pixels across the spatial and temporal dimensions. The authors claim that the scalability of the proposed method enables them to make an initial attempt at modeling videos of unusually high complexity and diversity found in the videos of the Kinetics dataset."
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,This paper proposes an adversarial generative model for generalized zero-shot learning on multi-label text classification. The authors propose a latent feature generation framework to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. The framework generates semantically meaningful features by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. Extensive experiments demonstrate the effectiveness of the proposed method.
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,This paper proposes a self-supervised representation learning method to improve sample efficiency in model-free reinforcement learning (RL). The authors propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. The proposed method achieves efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps. The paper is well-written and easy to follow.
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the new task. The authors conduct extensive experiments on 2D toy regression and few-shot image classification and demonstrate the superiority of ARML over state-of-the-art baselines."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a method for controllable language generation that combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. The attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM, and are either in the form of bagofwords or single layer classifiers. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM’s hidden activations and thus guide the generation. The authors demonstrate control over a range of topics and sentiment styles and extensive automated and human annotated evaluations show attribute alignment and fluency."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes to learn data representations with a novel type of denoising autoencoder, where the noisy input data is generated by corrupting clean data in the gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper studies the problem of ""under-sensitivity"" in natural language inference, i.e. the problem that a model can become more confident as it is exposed to deletions of large fractions of input text. The authors propose a formal verification method based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. The method is able to efficiently prove, given a model, whether a particular sample is free from the under-sensitive problem. The paper also compares different training methods to address the under sensitivity problem and compare metrics to measure it."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper studies the problem of model-free off-policy deep reinforcement learning (RL) from a theoretical point of view. In particular, the authors consider the setting where there is a replay memory, which is used to store past experience and derive all network updates, but the replay memory only holds a finite number of transitions. The transitions are represented in a data graph and the authors link its structure to soft divergence. By selecting a subgraph with a favorable structure, they construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in, resulting in a QGRAPH. The authors show that the Q-value for each transition in the simplified MDP is a lower bound of the q-value of the original continuous Q-learning problem. By using these lower bounds in TD learning, their method is less prone to soft divergences and exhibits increased sample efficiency while being more robust to hyperparameters."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper studies the effect of the embedding complexity on generalization to the target domain. The authors show that this complexity affects an upper bound on the target risk; this is reflected in experiments, too. Next, they specify our theoretical framework to multilayer neural networks. As a result, they develop a strategy that mitigates sensitivity to the embeddings, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper studies generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. The authors develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalisation error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. They obtain new data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Their result recovers (and is typically tighter than) a recent result in Mou et al (2018) and improves upon the results in Pensia et al. (2018)."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper studies the role of the hippocampus in continual learning in the context of reinforcement learning (RL). The authors analyse population-level activity of hippocampal CA1 neurons of rodents learning to perform allocentric and egocentric spatial tasks. The components uncovered using dPCA from the firing activity reveal that hippocampal neurons encode relevant task variables such as decisions, navigational strategies and reward location. The authors also compare this hippocampal features with standard RL algorithms, highlighting similarities and differences."
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"This paper proposes a policy optimization method for Monte Carlo Tree Search (MCTS) in continuous action spaces. The main idea is to first train a policy using existing model-free RL methods, and then use the pre-trained policy distribution to draw actions with which to build the tree. Once the tree has been constructed, it is run simulations to generate experiences using an Upper Confidence Bounds for Trees (UCT) approach. Populating the tree with the action samples drawn from a pretrained policy enables to perform a computationally feasible search. "
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper studies the lottery ticket hypothesis, which claims that sparse subnetworks of over-parameterized neural networks can achieve good predictions when trained in isolation, as long as they are appropriately initialized. The authors aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain “good” tickets without supervision? They find that winning tickets found in these scenarios are, perhaps surprisingly, competitive with winning tickets generated on the full ImageNet dataset when evaluated on ImageNet classification task. They also show that using large datasets is important to study lottery tickets, since deep networks trained on CIFAR-10 are natually sparse."
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,This paper studies the problem of adversarial attacks on reading comprehension models. The authors formulate a noisy adversarial attack which searches among semantic variations of comprehension questions for which a model still erroneously produces the same answer as the original question – and with an even higher probability. They show that – despite comprising unanswerable questions – SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarially generated questions. This indicates that current models—even where they can correctly predict the answer—rely on spurious surface patterns and are not necessarily aware of all information provided in a given comprehension question. They further show that data augmentation and adversarial training are able to substantially decrease a model’s vulnerability to undersensitivity attacks on held out evaluation data.
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. It learns the transition dynamics of the environment and generates a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, and it is able to efficiently traverse through the imagined environment without ever taking any action in reality. The proposed approach is compatible with any existing RL algorithm and any task with discrete action space. The authors conduct experiments on two gridworld environments and a self-driving car simulator to demonstrate the effectiveness of the proposed approach."
SP:c2796f28fb067138303df8d424d646f4ada31558,This paper proposes a physics-aware difference graph networks (PA-DGN) that leverages neighboring information to learn finite differences inspired by physics equations. The proposed method leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. The authors demonstrate the superiority of the proposed method in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper considers the problem of training structured neural networks (NN) with nonsmooth regularization (e.g. $\ell_1$-norm and $\ell_{1-norm}$-constraint) and constraints. The authors formulate training as a constrained nonconvex optimization problem, and propose a convergent proximal-type stochastic gradient descent (ProxSGD) algorithm. They show that under proper learning rates, with probability 1, every limit point of the sequence generated by the proposed Prox SGD algorithm is a stationary point. In addition, they show that the proposed algorithm can be used to train either sparse or binary neural networks."
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper proposes an end-to-end differentiable compression framework for lossy image compression, which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bitsback efficient. The proposed method can be straight-forwardly trained using standard gradient-based optimizers. The authors apply it to training Probabilistic Ladder Networks on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art on low bitrates."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper proposes a new method for super-resolution (SR) based on compressed JPG (C-JPG) images. The proposed method consists of two components: (1) a functional sub-model to recover information from the C-jPG images, and (2) a cycle loss to improve the performance of SR solver. Experiments show that the proposed method outperforms the state-of-the-art methods. "
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper presents a deep learning approach to estimate the probability of passing events in soccer matches from low-level tracking data and weak labeling of each event’s success. The proposed architecture is inspired by recently developed fully convolutional networks that have been proven to be successful for image segmentation, where the label only corresponds to a single location in the original input. This set up presents an extreme case of weakly supervised learning, where there is just a single pixel correspondence between ground-truth outcomes and the predicted probability map. The network is able to perform remarkably well from low level inputs by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve both coarse and fine detail."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion method without using side information. The proposed method trains a graph neural network (GNN) based on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps them to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. Moreover, IGMC is inductive – it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper considers the unconstrained minimization of a smooth objective function in the setting where only function evaluations are possible. The authors propose and analyze stochastic zeroth-order method with heavy ball momentum. They show new complexity results for non-convex, convex and strongly convex functions. They test their method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. (2012) environments with varying difficulty."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,This paper proposes an action semantics network (ASN) that explicitly represents the action semantics between agents in multi-agent reinforcement learning (MARL). The proposed ASN can be easily combined with existing deep reinforcement learning algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance of state-of-the-art DRL approaches compared with several network architectures.
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper proposes to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. Specifically, the authors investigate the low-rank structure, which widely exists for big data matrices, and verify empirically the existence of low-ranking Q functions in the context of control and deep reinforcement learning tasks. The authors propose a general framework to exploit these structures, which leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on “low-rank” tasks."
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes a new algorithm for batch reinforcement learning (DRL) called Best-Action Imitation Learning (BAIL), which first selects from the batch the actions it believes to be high-performing actions for their corresponding states; it then uses those state-action pairs to train a policy network using imitation learning. The authors demonstrate that BAIL achieves state-of-the-art performance on the Mujoco benchmark."
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper proposes DeepXML, an algorithm for deep extreme multi-label learning for short text documents. The main idea is to split training of head and tail labels, learn word embeddings on head labels and transfer them through a novel residual connection to data impoverished tail labels; increase the amount of negative training data available by extending state-of-the-art negative sub-sampling techniques; and re-ranking the set of predicted labels to eliminate the hardest negatives for the original classifier. The proposed algorithm is implemented efficiently by extending the highly scalable Slice algorithm for pretrained embedding to learn the proposed deepXML architecture. As a result, it could be more than 10x faster at training than XML-CNN and AttentionXML. "
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper proposes an end-to-end trainable variational hashing-based collaborative filtering approach that uses the concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user’s preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. The proposed method outperforms state-of-the-art baselines on 4 datasets."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper proposes a set of statistical tools to quantify the mode collapse of GANs. The authors propose two simple yet effective “black-box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data. They demonstrate the application of their tool set in analyzing the bias in unconditional face image generation: a popular benchmark task nowadays for GAN, yet remaining rather unclear how to measure its mode collapse using existing tools."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper studies the problem of training two-layer neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. The authors propose to randomize the neural networks, which allows them to escape their NTK and couple with quadratic models. They show that the optimization landscape of randomized two-layered neural networks is nice and amenable to escaping-saddle algorithms. They prove concrete generalization and expressivity results on these randomized networks which lead to sample complexity bounds (of learning certain simple functions) that match the Neural Tangent Kernel (NTK) and can in addition be better by a dimension factor."
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"This paper proposes a method to evaluate the effectiveness of graph convolutional filters for semi-supervised node classification. The authors propose a novel assessment tool, called Graph Filter Discriminant Score (GFD), to evaluate whether there exists an optimal filter that performs the best on all graph data; (2) which graph properties should be considered for finding the optimal graph filter; and (3) how to design appropriate filters that adapt to a given graph. Based on these findings, the authors propose Adaptive Filter Graph Neural Network (AFGNN), which can adaptively learn a proper model for the given graph to achieve state-of-the-art performance."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies the problem of distributionally robust optimization (DRO) for over-parameterized neural networks. The authors argue that naively applying group DRO to overparametrized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst case performance arises from poor generalization on some groups. To address this issue, the authors propose to learn models that instead minimize the worst case training loss over a set of pre-defined groups, and show that this approach can achieve substantially higher worst-group accuracies. "
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper proposes a new local explanation method for black-box classifiers. The proposed method is based on the idea of distribution controllers and integrate it with a neural network to directly guide the distribution of relevance scores. Then, the classification loss is used to optimize the proposed predictor. The benefit of this strategy is to enable discriminative scores over supporting features, and facilitate the setting of involved hyperparameters."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"This paper proposes a framework for image reconstruction and classification problems that involve detection of multiple object instances, without any supervision regarding their whereabouts. The proposed method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state-of-the-art."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,This paper proposes a reinforcement learning-based approach for neural program synthesis. The main idea is to learn a policy network and a value network to reduce the depth and breadth of the Monte Carlo Tree Search (MCTS) and use them to guide the exploration of the program space. The authors also propose a multi-entropy policy sampling technique to alleviate online update correlations. The proposed approach is evaluated on a few simple tasks and shows better performance than existing baselines.
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper studies the impact of model architecture on the speed of training in the context of gradient descent optimization. The authors use the ideas from prior work that shows gradient descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H to characterize the convergence rate. They introduce a simple analysis technique that enumerates H in terms of all possible “paths” in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper studies the connection between strongly overparametrized neural networks (NNs) and kernel methods. In particular, the authors prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: the first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows us to transfer generalization bounds from minimum complexity interpolating kernels methods to NNs; (b) in the opposite regime, the generalization error of wide NNs increases significantly with the initialization variance."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"This paper proposes two improvements to the pseudo-LiDAR framework for stereo-based 3D object detection. First, the authors propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. Second, they propose to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. The proposed method outperforms the previous state-of-the-art detection accuracy for faraway objects by 40%."
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a principled adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, the authors train K base detectors where the i-th detector is trained to discriminate natural data of class i from adversarial examples perturbed from other classes. At inference time, they first get the predicted label of the input, and then use the kth detector to identify whether the input is a natural sample (of class k) or an adversarial sample (perturbed from the other classes). They further devise a generative approach to detect/classify adversarial samples by interpreting each base detector as an unnormalized density model of the classconditional data."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper proposes a novel intrinsic reward to encourage exploration in procedurally-generated sparse reward environments. The proposed intrinsic reward encourages the agent to take actions that lead to significant changes in its learned state representation. The intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control. The authors evaluate their method on multiple challenging procedurally generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. "
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper studies the problem of large-scale query-document retrieval: given a query, retrieve the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps: the retrieval phase first reduces the solution space, returning a subset of candidate documents, and the scoring phase re-ranks the documents. The retrieval phase is less well studied, and this is the focus of this paper. In this paper, the authors conduct a comprehensive study on the embedding-based retrieval models. They show that the key ingredient of learning a strong embeddings-based Transformer model is a set of pre-training tasks. With adequately designed paragraph-level pretraining tasks, the Transformer models can remarkably improve over the widely-used BM-25 and other embedding models."
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper proposes a new graph convolution operation for graph neural networks. The main idea is to replace the graph pooling layers with a single bipartite graph operation, which is a parameterized transformation between different input and output graphs. The proposed method is general enough to subsume conventional graph convolutions and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. Experiments on graph classification and graph autoencoders are conducted to demonstrate the effectiveness of the method."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,This paper addresses the problem of few-shot classification under domain shifts for metric-based methods. The core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. The authors further apply a learning-to-learn approach to search for the hyper-parameters of the feature transformation layers. The experiments and ablation studies under the domain generalization setting are conducted on five benchmark datasets.
SP:df46627cb984a56bba36d510bfc52e00751e9107,This paper proposes a continuous convolutional network for Lagrangian fluid simulation. The authors treat fluids as spatially continuous functions sampled at a finite set of (continuously evolving) positions and process them with a novel convolution layer. This matches the continuous nature of the problem more closely and simplifies the definition of neural networks by abstracting the underlying particle representation. They extend the grid-based filter representation commonly used for discrete convolutions to the continuous domain by simple linear interpolation. Linear interpolation allows efficient lookup of spatially varying filter values at arbitrary positions while retaining the compactness and efficiency of the grid representation.
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes a new ensemble method, called BatchEnsemble, to reduce the computational and memory cost for training and testing neural networks. The proposed method is based on the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. The method is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. The speedup at test time is 3x and memory reduction is 3X at an ensemble of size 4. The authors also apply the proposed method to lifelong learning on Split-CIFAR-100 and Split-ImageNet."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network-based solver for solving the forward and inverse problems of PDEs. The proposed solver is grid free, mesh free and shape free, and the solution is approximated by a deep neural network. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. The resulting solution is an explicit smooth differentiable function with a known analytical form. This framework enables the solution of high order non-linear PDE."
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper studies the performance of SAT solvers for binary neural networks (BNNs), a class of neural networks that allows exact representation in Boolean logic. The main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this paper, the authors analyze architectural design choices of BNN and discuss how they affect performance of logic-based reasoners. They propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solver without sacrificing accuracy on the primary task. The experimental results demonstrate that their approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets."
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNN). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNMP can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a generative flow-based density estimation method that is composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but the authors propose a simple variational scheme that performs well in practice. The authors show empirically that LGFs yield improved performance across a variety of density estimation tasks."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper studies the problem of vision-and-language navigation (VLN) in the presence of a performance bias towards training environments. The authors propose two methods to address this issue: (1) environment re-splitting and (2) feature replacement. Experiments are conducted on R2R, R4R, CVDN, and R2L datasets. Results show that the proposed methods are able to reduce the performance gap between training and testing environments."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,This paper studies the problem of using human feedback to accelerate and optimize the training of deep reinforcement learning (DRL) agents. The authors propose to use electroencephalogram (EEG) signals from a human observer as an auxiliary reward function to improve the performance of the RL agent. The EEG signals are obtained by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent’s learning in the RL tasks. 
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper proposes an information-theoretic framework to compare the performance of different image classifiers on the laconic classification task, where the goal is to minimize the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, we compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance, in turn generalizing similar methods explored in previous works. The authors propose two complementary frameworks for computing the minimal-ENTropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set, they find that machine classifier are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the IlsvRC-trained models used."
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,This paper provides a comprehensive analysis of perturbation-based defense methods for convolutional neural networks. The authors identify a family of defense methods that are based on the instability assumption. The defenses include deterministic lossy compression algorithms and randomized perturbations to the input that all lead to similar gains in robustness. They also provide a generic attacker that attacks a particularly strong lossy channel based on additive Laplace noise and adaptive attacks designed on this channel.
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper proposes a method for self-supervised 3D object detection. The proposed method is based on a neural 3D mapping network, which takes as input 2.5D (color and depth) video streams captured by a moving camera, and lifts them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its feature maps to novel viewpoints, to predict and match against target views. The authors also propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data."
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised domain translation (UDT), which is the task of finding meaningful correspondences between two domains, without access to explicit pairings between them. The authors define UDT in a rigorous, non-ambiguous manner, explore the implicit biases present in the approach and demonstrate the limits of theses approaches. Specifically, they show that mappings produced by these methods are biased towards low energy transformations, leading them to cast UDT into an Optimal Transport (OT) framework by making this implicit bias explicit. Then, they propose a simple approach to solve UDT, and illustrate its properties in two distinct settings."
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a novel regularization method, RotationOut, for neural networks, which views the input layer as an entire vector and introduces regularization by randomly rotating the vector. The proposed method can be used in convolutional layers and recurrent layers with small modifications. The authors further use a noise analysis method to interpret the difference between the proposed method and Dropout in co-adaptation reduction. Extensive experiments in vision and language tasks are conducted to show the effectiveness."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a method to create universal adversarial perturbations (UAP) for a given CNN in a data-free manner. The proposed method is based on finding the first singular vector of the linearly approximated neural network and optimizing the perturbation with the proposed dilate loss, which maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, it constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs. Extensive experiments demonstrate that the proposed method has theoretical support, and achieves higher fooling rate than the existing work."
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a transferable neural architecture search (NAS) method based on meta-learning. The proposed method learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, the proposed method is simpler to implement and faster to train, and produces state-of-the-art results on network compression by pruning, adversarial defense and learning with label noise. The paper is well-written and easy to follow."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta-learning approach to generate curiosity mechanisms for RL agents. The core idea is that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent’s life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. The outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and the inner loop will perform standard reinforcement learning using the adapted reward signal. This paper proposes to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. The rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. The proposed approach is evaluated on grid navigation, acrobot, lunar lander, ant and hopper."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,This paper proposes a new task of Any-Code-to-Code Generation (AnyC2C) - generating code given its surrounding code without any restriction on the vocabulary or structure. The authors propose a new approach called Structural Language Model (SLM) that leverages the strict syntax of programming languages to model a code snippet as a tree. The model estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. The proposed model achieves state-of-the-art performance in generating Java and C# code.
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper provides a theoretical analysis to explain why gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). The authors introduce a mathematical tool called canonical space and prove that the objective functions in learning NNs are convex in the canonical model space. They further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. The authors prove that gradient descent algorithms surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"This paper proposes two interactive graph-based segmentation algorithms for semantic and panoptic segmentation. The first algorithm is based on a discrete discrete Potts model, and the second one is a class-aware integer linear programming (ILP) formulation that ensures global optimum. Both algorithms can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. The experiments show that incorporating the connectivity prior greatly improves the performance of the proposed algorithms."
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper proposes a method to detect adversarial perturbations in an image using saliency maps. The main idea is to use a learned saliency model to capture the shifts in saliency due to the perturbation and use it as a defense against adversarial attacks. The proposed method is evaluated on MNIST, CIFAR-10, and ASSIRA datasets and compared with a baseline that uses the same model but without the saliency map."
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper considers the problem of quantifying the global adversarial robustness of a neural network, i.e., the probability that its prediction at any point sampled from the (unknown) input distribution is susceptible to adversarial attacks. The authors prove that the global robustness property defined here is measurable for a range of commonly used notions of adversarial examples and NN architectures, and show how concentration inequalities can be used to provide statistical guarantees on the robustness estimations with respect to global estimations of the network architecture and training methods on MNIST, Fashion-MNIST and CIFAR. They empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via stochastic gradient descent and with iterative pruning techniques, while a positive trend is observed between them in Bayesian settings."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the problem of robust reinforcement learning, i.e., finding the optimal policy with some extent of robustness to environmental dynamics. The authors leverage the Wasserstein distance to measure the disturbance to the reference transition kernel. The transition kernel disturbance is connected to the state disturbance, and the authors reduce an infinite-dimensional optimization problem to a finite-dimensional risk-aware problem. They show the existence of optimal robust policies, provide a sensitivity analysis for the perturbations, and design a novel robust learning algorithm—Wasserstein Robust Advantage Actor-Critic algorithm (WRAAC). The effectiveness of the proposed algorithm is verified in the Cart-Pole environment."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method to approximate mixed strategy Nash equilibria in multi-player continuous games, which always exist and include the pure ones as a special case. The proposed method leverages the pushforward measure technique to represent a mixed strategy in continuous spaces, which allows to generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players’ joint strategy profile and a Nash equilibrium. Applying the gradient descent algorithm to the general GNI function, which converges to a mixed stationary Nash equilibrium under the convexity assumption on the payoff functions, the same popular setting as in previous studies. In numerical experiments, the method consistently and significantly outperforms recent works on approximating Nash equilibrium for quadratic games, general blotto games, and GAMUT games."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper proposes a neural execution tree (NExT) framework for augmenting training data for text classification using natural language explanations. The NL explanations are converted into executable logical forms by semantic parsing, and NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper extends the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness. Temporal properties are commonly desired from DNNs in settings where the outputs have a sequential nature. Experiments show that while models trained using standard training often violate desired specifications, the verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper studies the problem of visual domain generalization in deep reinforcement learning. The authors formalize the visual domain randomization problem and show that minimizing the policy’s Lipschitz constant with respect to the randomization parameters leads to low variance in the learned policies. They propose a regularization method where the agent is only trained on one variation of the environment and its learned state representations are regularized during training to minimize this constant. They conduct experiments that demonstrate that their technique leads to more efficient and robust learning than standard domain randomisation, while achieving equal generalization scores."
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper studies the problem of deep metric learning (DML) and proposes a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows the authors to recover state-of-the-art complicated losses and also to induce novel variants. Empirical studies on several benchmark data sets demonstrate that the proposed method outperforms the state of the art results."
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"This paper studies the problem of finding a local minimum in non-convex finite-sum minimization with inexact gradient and Hessian estimation. The authors propose a sample-efficient stochastic trust region (STR) algorithm that finds an approximate local minimum within $\tilde{O}(\sqrt{\frac{n}{\sqrt{n/\epsilon}})$ stochastically Hessian oracle queries. This improves the state-of-the-art result by a factor of $O(n^{2/3/1.5)$ compared to the previous work. Moreover, the authors also develop Hessian-free STR algorithms that achieve the lowest runtime complexity."
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper proposes a new method for training deep neural networks without batch normalization or weight initialization. The proposed method is based on linear programming, and the authors introduce Farkas layers: a method that ensures at least one neuron is active at a given layer. The authors empirically demonstrate a significant improvement in training capacity in the absence of batch normalisation or appropriate weight initialization across a broad range of network sizes on benchmark datasets."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper studies the problem of certifying the robustness of deep neural networks to adversarial perturbations. The main idea is to use the curvature of the neural network as a regularizer to improve the certified robustness. The authors show that if the eigenvalues of the Hessian of the network are bounded, they can compute a robustness certificate in the l2 norm efficiently using convex optimization. Then, they derive a computationally-efficient differentiable upper bound on the curvatures of a deep network."
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a method for compressed sensing recovery using untrained deep generative models. The proposed method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. The authors show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. The method does not require pre-training over large datasets, and introduces a novel learned regularization technique, which incorporates prior information on the network weights."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes a hierarchical reinforcement learning (HRL) framework TAIC (Temporal Abstraction with Information-theoretic Constraints), which learns the temporal abstraction from past experience or expert demonstrations without task-specific knowledge. The authors formulate the temporal abstractions problem as learning latent representations of action sequences and present a novel approach of regularizing the latent space by adding information theoretic constraints. Specifically, the authors maximize the mutual information between the latent variables and the state changes. The learned abstraction allows us to learn new tasks on higher level more efficiently."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper proposes a layer-wise sampling strategy to speed up the training of graph convolutional networks (GCNs). The sampling strategy samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. The authors also apply the self-attention mechanism to learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments demonstrate the effectiveness and efficiency of the proposed model."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper proposes a new unsupervised video model, STOVE, which combines an image model and a dynamics model in a compositional manner, and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. The model is trained end-to-end on pure video data in a self-supervised fashion and learns to detect objects, to model their interactions, and to predict future states and observations. It also extends the model to the model-based reinforcement learning setting, by conditioning state predictions on actions and adding reward predictions to the dynamics predictor, allowing it to be used in the RL setting."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,This paper proposes a new variational autoencoder (VAE) model that combines the best properties of VAE and generative adversarial networks (GAN). The proposed model optimizes the $\lambda$-Jeffreys divergence between the model distribution and the true data distribution. The authors propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. Experiments on CIFAR-10 and TinyImagent datasets show the proposed model achieves the state-of-the-art trade-off between generation and reconstruction quality.
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper shows that adversarial attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. The authors present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. They introduce new datasets of realistic images of faces and digits where the Bayesian-optimality classifier can be calculated efficiently. They show that for some of these datasets the optimal classification is robust and for others it is vulnerable to adversarial examples. In systematic experiments with many such datasets, they find that standard CNN training consistently finds a vulnerable classifier."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the impact of pruning on different classes and images. The authors identify a subset of examples (called pruning identified exemplars (PIEs) that are more challenging for both sparse and non-sparse models to classify. They show that removing PIEs from the test-set improves top-1 accuracy for both a fully parameterized fully-parameterized model and a sparse model. They also show that pruning makes deep neural networks less robust to common image perturbations (blur, noise, contrast) and adversarial examples."
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper proposes a method for interactive fiction (IF) games, which are text-based simulations in which an agent interacts with the world purely through natural language. In order to solve solve a popular IF game such as Zork1 it’s necessary to generate natural language actions consisting of up to five-words from a relatively modest vocabulary of 697 words recognized by Zork's parser. Even this modestly sized vocabulary leads to O(6975) = 1.64x1014 possible actions at every step, which is a dauntingly-large combinatorially-large action space for a learning agent to explore. This paper proposes KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. The paper claims that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorial-large natural language action spaces. Results across a wide variety of IF games show that the proposed method outperforms current IF agents."
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper proposes to use a data-dependent Gaussian prior (D2GPo) to improve the performance of the maximum likelihood estimation (MLE) for language generation. The proposed method is based on the idea of Kullback-Leibler divergence (KL) loss, which is a KL-divergence term between the predicted and ground-truth distributions. The authors show that the proposed method improves the performance on supervised and unsupervised machine translation, text summarization, and image captioning tasks. "
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper proposes to replace the commonly used cross-entropy loss with focal loss. The authors argue that focal loss preserves the confidence of the model’s correct predictions, which is extremely desirable for downstream tasks. They provide a thorough analysis of the factors causing miscalibration, and use the insights to theoretically justify the empirically excellent performance of focal loss in terms of accuracy and calibration. They perform extensive experiments on a variety of computer vision and NLP datasets and with a wide variety of different network architectures."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper proposes a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The authors show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. They conduct experiments on networks with random weights as well as networks trained on MNIST, showing that the proposed approach yields superior estimates."
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. The method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). The authors also show how to learn representations from sequences of visual features and sequences of words derived from automatic speech recognition (ASR)."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper proposes a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network. Both the associated selection masks as well as the neural network are trained simultaneously such that a good model performance is achieved while, at the same time, only a minimal amount of data is selected. During the inference phase, only the parts selected by the selection masks have to be transferred between the server and the client. The experiments show that it is often possible to significantly reduce the amount of training data needed to reduce the transfer without affecting the model performance much."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a new loss function for out-of-distribution (OOD) detection based on the Outlier Exposure (OE) technique. The proposed loss function includes two regularization terms: the first minimizes the l1 norm between the output distribution of the softmax layer of a DNN and uniform distribution, and the second minimizes Euclidean distance between the training accuracy and its average confidence in its predictions on the training set. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the previous work of Hendrycks et al. (2019) in both image and text classification tasks."
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. Extensive experiments on benchmark datasets demonstrate the superior performance of the proposed method."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper proposes a model-based reinforcement learning (RL) setting where agents internalize an environment with different biases, resulting in imperfect evidence collected for taking optimal actions. The key idea is to let agents imagine together; make them take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents’ biases vary, the collective policy developed while sequentially visiting the internal simulations complement one another’s shortcomings. Experiments show that the collective policies consistently achieve significantly higher returns than the best individually trained policies."
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper proposes a dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that the proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,This paper proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS) to estimate the spatial impact of deep models by the feature perturbation inspired by light/shadows in nature. GLAS provides a useful coarseto-fine control benefiting from scalability of Gaussian mask. The authors also devised the ability to identify multiple instances through recursive GLAS. Experiments on the fine-grained classification dataset show the general applicability of the proposed method.
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"This paper proposes a method to remove both pixel-wise and channel-wise correlations before the data is fed into each layer of a convolutional neural network. The proposed method can be efficiently calculated at a fraction of the computational cost of the convolution layer. The authors also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Experiments are conducted on CIFAR-10, Cifar-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,This paper proposes a novel quantization method for GANs based on ExpectationMaximization (EM) algorithm and a novel multi-precision quantization algorithm. Experiments on CIFAR-10 and CelebA show that the proposed QGAN can quantize weights in GAN models into 1-bit or 2-bit representations with results of quality comparable to original models. Quantization is the most easy-to-use and scalable method that uses fewer bits for data representations than the 32-bit precision.
SP:58c4905f59f04a50b30d27c99521126a6455d38a,This paper studies the last-iterate convergence of the Hamiltonian gradient descent (HGD) algorithm for convex-concave min-max problems. The authors show that the HGD algorithm achieves linear convergence in a variety of more general settings than the simple bilinear setting. They also prove convergence rates for stochastic HGD and for some parameter settings of the Consensus Optimization algorithm of Mescheder et al. (2017).
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of learning ResNet. Specifically, the ResNet block hl = \phi(hl+1) + \tau(hl-1) where $\tau$ is a scalar scalar and $L$ is the number of residual blocks. The authors show that for standard initialization used in practice, $tau = 1/\sqrt{L}$, which is a sharp value in characterizing the forward/backward process of ResNet, where stability is guaranteed for $\tilde{\Omega}(L)$. Moreover, if ResNet is properly over-parameterized, the gradient descent is guaranteed to find the global minima 1, which significantly enlarges the range of $Tau$ that admits global convergence in previous work. "
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. The authors show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"This paper proposes a method to find meaningful directions in the latent space of generative models that can be used to control specific properties of the generated image such as the position or scale of the object in the image. The method does not require human annotations and is particularly well-suited for the search of directions encoding simple transformations such as translation, zoom or color variations. Experiments on GANs and variational auto-encoders demonstrate the effectiveness of the proposed method."
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a physics-as-inverse-graphics approach to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. The authors demonstrate the value of this framework by demonstrating the data-efficient learning of vision-actuated model-based control."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"This paper considers the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the “clean” probability is exploited as a relevance measure. The proposed method outperforms the method by Douze et al. (2018) using the same large-scale collection of data."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"This paper proposes a method to improve the performance of graph neural networks (GNNs) by maximizing the mutual information between edge features and message passing channels. The proposed method, called Edge Information Maximized Graph Neural Network (EIGNN), is reformulated as a differentiable objective via a variational approach. The authors theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper proposes a method for verifying the properties of generative models. The proposed method is based on EXACTLINE, which is a relaxation of the non-convexity of the generative model. The main contributions of the paper are: (1) a verification system APPROXLINE, capable of flexibly capturing the necessary properties; (2) a method to compute tight deterministic bounds on probabilities; (3) the first time that probabilistic abstract interpretation has been applied to neural networks."
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the problem of ""suspended animation"" in graph neural networks (GNNs), i.e., when the model depth reaches a certain limit, the model will not respond to the training data any more and become not learnable. The authors propose a GRESNET (Graph Residual Network) framework, which creates extensively connected highways to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. They prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node’s representations between sequential layers. Detailed studies about the GRESNet framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported with extensive empirical experiments on real-world benchmark datasets."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper proposes a method for 3D face reconstruction from a single image. The proposed method is based on linear 3D morphable models (3DMM) with convolutional neural networks (CNN) to regress the face shape and texture directly. The authors propose a semi-supervised approach to train their model with adversarial loss on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabelled face images from unconstrained photo collections. They also introduce a novel center loss to make sure that different facial images from the same person have the same identity shape and albedo. Besides, their proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper proposes a method for imitation learning in the presence of partial knowledge of the transition kernel. The transition kernel consists of two parts: (1) simulate the transition of state components for which transition kernel is known, and (2) extract from demonstrations the next state for which the kernel is unknown. The authors propose to use a policy gradient algorithm to solve the imitation learning problem in this setting. The proposed method is evaluated on a set of Mujoco tasks, where it is shown to outperform baselines."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self-supervised reinforcement learning approach for learning to control states of interest without any external reward function. The authors formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the state of interest. They evaluate their approach on two simulated robotic manipulation tasks from OpenAI Gym and a navigation task in the Gazebo simulator. They show that their method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards."
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a more powerful trojaning attack method for large models, which outperforms existing studies in capability, capability, generality, and stealthiness. First, the attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim’s deployment. Second, one trojaned model on a large-scale dataset can affect applications of different domains that reuses its general features. Third, the trojan shows no biased behavior for different target classes."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"This paper proposes a kernel-based few-shot regression (FSR) algorithm for drug discovery. The proposed algorithm is based on the idea of learning a deep network in combination with a kernel function and a differentiable kernel algorithm. The choice of kernel is critical, and the proposed algorithm learns to find the appropriate kernel for each task during inference. It outperforms the state-of-the-art algorithms on both toy and real-world drug discovery benchmarks."
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper proposes a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. The authors formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes the Fréchet Joint Distance (FJD) to evaluate conditional generative adversarial networks (cGANs). FJD is defined as the distance between joint distributions of images and conditioning, which allows it to implicitly capture the aforementioned properties in a single metric. The authors conduct proof-of-concept experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to existing metrics. Moreover, they use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper proposes a method to identify decision states, i.e., the parsimonious set of states where decisions meanfully affect the future states an agent can reach in an environment. The paper builds on the VIC framework (Gregor et al., 2016), which maximizes an agent’s ‘empowerment’, and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work (Goyal et al. 2019), the decision states are discovered without extrinsic rewards – simply by interacting with the world. The results show that the proposed method leads to better exploration on downstream goal-driven tasks in partially observable environments."
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a framework for classification of irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. The proposed method SEFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. The method is evaluated on multiple healthcare time series datasets and shows that it performs competitively while significantly reducing runtime."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper proposes a new convolution operation, called Harmonic convolution, to model inductive biases in audio signals by explicitly utilizing the harmonic structure. This is done by engineering the kernels to be supported by sets of harmonic series, instead of by local neighborhoods as convolutional kernels. The authors empirically show that networks using Harmonic Convolution can reliably model audio priors and achieve high performance on unsupervised audio restoration. They also achieve better generalization performance for supervised musical source separation."
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper proposes a data echoing technique to reduce the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline steps in order to reclaim idle capacity. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. In all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,This paper proposes a method for unsupervised pre-training in the Atari suite of games. The authors propose to use successor features to improve the generalization ability of variational intrinsic intrinsic motivation (VIC) and successor features (SIF) methods. The main idea is to learn a set of controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. The proposed method is evaluated on 12 Atari games and achieves human-level performance.
SP:83500230586a9134f910ad067b7233dc563dc1ba,"This paper studies the generalization properties of shallow and deep fully connected univariate ReLU networks from a functional point of view. Theoretical and empirical results are provided to show that the smoothness of the functional approximation, combined with a flat initial approximation, increases with the number of units, explaining why massively over-parameterized networks generalize well. The paper also shows that depth makes it easier for GD to optimize the resulting network, allowing for a greater flexibility in the movement of breakpoints, as well as the number breakpoints induced during training."
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper proposes a method for image-to-image translation using GANs. The proposed method is based on the attention mechanism, where the discriminator is equipped with an attention mechanism to estimate the probability that the input image is real, and also to create an attention map that highlights the critical features for the generator to produce more plausible and realistic images. Experiments on a number of image transfer tasks are conducted to demonstrate the superiority of the proposed approach."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper investigates the role of multiplicative interaction layers as primitive operations in a variety of neural network architectures, such as gating, hypernetworks, multiplicative RNNs, and dynamic convolutions. The authors show that such layers strictly enrich the representable function classes of neural networks and conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. They argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, they back up their claims and apply them in large-scale complex RL and sequence modelling tasks, where their use allows them to deliver state-of-the-art results."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper proposes a new algorithm for batch active learning with deep neural network models. The algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters, making it a useful option for real world active learning problems."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"This paper proposes a feature leveling network to improve the interpretability of deep neural networks (DNNs). The authors claim that DNNs are hard to interpret because each hidden layer carries a mix of low-level and high-level features. Based on this observation, the authors propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that the modified models achieve competitive results on standard datasets while being more self-explainable."
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper proposes a new approach for extreme classification, i.e. training a classifier over a large number of classes, known as ‘extreme classification’, by drawing negative samples from an adversarial model that mimics the data distribution. The proposed approach is based on the observation that the signal-to-noise ratio in negative sampling is poor since there is no association between input features and their artificial labels. The paper proves that the proposed approach minimizes the gradient variance while any bias due to non-uniform sampling can be removed. Experiments on large scale data sets show a reduction of the training time relative to several competitive baselines."
SP:29b52fee83309268d9864f3b1fc3617948577d41,"This paper proposes a new approach for efficient exploration in RL that leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. The proposed approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty and leverage these intrinsic rewards for sample-efficient exploration with planning routines. The authors test their approach on a number of maze tasks, as well as a control problem and show that their exploration approach is more sample efficient compared to strong baselines."
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper studies the problem of out-of-distribution detection in the few-shot setting. The authors propose two methods for this task and investigate their performance. They also establish benchmark datasets, based on four popular few shot classification datasets, and show improved results with the proposed methods."
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper proposes a generalized model of sequence generation that unifies decoding in directed and undirected sequence models. The proposed framework models the process of generation rather than a sequence, and under this framework, various neural sequence models such as autoregressive, semi-autoregressive and refinement-based models can be derived as special cases of the proposed model. This unification enables the authors to adapt decoding algorithms originally developed for directed sequence models to the unsupervised setting. The authors demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). "
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper proposes a two-stage method for the recognition of mathematical expressions (MEs) from images. The first stage is the detection of mathematical symbols and the structural analysis of mathematical formulas are carried out separately in two steps, which effectively improves the recognition accuracy and enhances the generalization ability. The second stage translates math symbols with position information into LaTeX sequences by a seq2seq model equipped with attention mechanism. The experiments demonstrate that the proposed method significantly outperforms the end-to-end method on the test data that doesn't come from the same source as training data."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,This paper proposes a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. They validate their approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x compression factor.
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper proposes a new transformer-based model that incorporates Tensor-Product Representations (TPR) into the transformer architecture to better support the explicit representation of relation structure. Specifically, the authors propose a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The proposed model sets a new state-of-the-art on the recently introduced Mathematics Dataset containing 56 categories of free-form math wordproblems. The paper shows that the proposed model is trained end-to-end and infers the correct answer for novel examples."
SP:d319df820c6630c409fab32097652a083e8f53ea,"This paper proposes a method to improve the generalization of deep neural networks (DNNs) by concatenating encodings of input features and then training the classifier on the extended features. The method is motivated by the idea that the training and test sets may not be sufficiently representative of the empirical sample set, which consists of real-world input samples. The authors derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity, and formulate an optimization problem to learn a more general classification function. Experiments demonstrate that a model trained on arbitrarily encoded input features is more robust to common corruptions and adversarial perturbations, and that using more encoded features may be beneficial to minimize the generalisation error."
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper proposes a new graph pooling method, called HaarPooling, which is based on the compressive Haar basis of the corresponding clustering. The pooling operation is computed by following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the input of the clusterings. The proposed method achieves state-of-the-art performance on diverse graph classification problems."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper proposes a point cloud decoder architecture that is differentiable, i.e., it can be used to represent variable-sized point clouds. The authors propose three differentiable point cloud encoders that map a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. They develop three sample-based decoder architectures and compare their performance to each other and show improved effectiveness over feedforward architectures. They also investigate the learned distributions to gain insight into the output transformation."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper presents a large-scale study on the generalization ability of deep neural networks (DNNs) trained on real-world noisy labels. The main contribution is the creation of a benchmark of real- world noisy labels at 10 controlled noise levels. The authors train DNNs across 10 noise levels, 7 network architectures, 6 existing robust learning methods, and 2 training settings (fine-tuning and random initialization). The study reveals several interesting findings: (1) Deep Neural Networks generalize much better on real world noise than synthetic noise. (2) Robust learning methods that work well on synthetic noise may not work as well on the real world data. (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data.  (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (5) Some robust methods that are well-performing on synthetic data may not be well performing on real data."
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a method to combine the efficiency of rules with the quality of instance labels. The authors propose a rule-exemplar method for collecting human supervision. The supervision is coupled such that it is both natural for humans and synergistic for learning. The training algorithm jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,This paper proposes a new memory layer for GNNs that can jointly learn node representations and coarsen the graph. It also introduces two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results show that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. The learned representations could correspond to chemical features in the molecule data.
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper studies the effect of initialization in deep linear networks and provides a proof that orthogonal initialization is superior to Gaussian initialization in terms of convergence time. The authors show that for deep networks, the width needed for efficient convergence to a global minimum is independent of the depth and scales linearly in the depth. In addition, the authors also show that Gaussian initializations have exponentially long convergence time if the width is too small."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"This paper studies the problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. The authors first explore the additivity of output error caused by quantization and find that additivity property holds for deep neural networks which are continuously differentiable in the layers. Based on this observation, the authors formulate the optimal bit allocation problem and propose a very efficient method to solve the optimization problem via Lagrangian Formulation. The proposed method obtains excellent results on deep neural network. It can compress deep CNN ResNet-50 down to 2 bits with only 0.7% accuracy loss."
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"This paper proposes a generative inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder network and a GAN network using an iterative primal dual optimization process. The authors also provide a rigorous probabilistic interpretation of our model under the framework of maximum likelihood estimation. The empirical experiments show that our model greatly mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample."
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper proposes a probabilistic approach for crowdsourcing anaphoric annotations. The authors extend the mention pair model (MPA) introduced by Paun et al. (2018b) by using a nonparametric partially pooled structure (based on a stick breaking process), fitting jointly with the ability of the annotators hierarchical community profiles. They show that the proposed model performs better than its unpooled counterpart in conditions of sparsity, and on par when enough observations are available."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes a hierarchical intrinsic drive (SID) that learns separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. It also introduces successor feature control (SFC), which is general and not task-specific. The experiments on VizDoom, DeepMind Lab and DeepMind Control Suite show a substantially improved exploration efficiency and the hierarchical usage of the intrinsic drives."
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper proposes a method for weakly-supervised video moment retrieval. The proposed method exploits a multi-level co-attention mechanism, frame-by-word interaction module and word-conditioned visual graph (WCVG) to learn richer multimodal representations. The method is evaluated on DiDeMo and Charades-STA datasets."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a method for image-guided re-rendering of reconstructed objects for virtual and augmented reality applications. The main contribution of this paper is the handling of view-dependent effects. Specifically, the authors directly train an object-specific deep neural network to synthesize the view-independent appearance of an object. The video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this proxy geometry, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, such as specular highlights, which can lead to artifacts. To this end, this paper proposes EffectsNet, a deep neural model that predicts view-dependency effects, which is able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, the pipeline reinserts the new view- dependent effects. To composite multiple reprojected images to a final output, a composition network that outputs photo-realistic results is learned in a self-supervised fashion."
SP:257d124367b1da9a595dc11a9df750d6bade298e," of Laplace Approximation (LAP) is proposed as a sparse representation of model uncertainty for deep neural networks (DNNs) that relies on an inverse formulation of Multivariate Normal Distribution (MND): an information form. The authors show that the model uncertainty can be estimated in this form using a scalable Laplace approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes the inversion of the information matrix intractable an operation that is required for a full Bayesian analysis, the authors devise a novel low-rank approximation of this eigendecomposition that exploits spectral sparsity of DNNs. "
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes a load-balanced hashing method for one-permutation hashing (OPH). The main idea is to balance the load of the bins (the number of elements in a bin) to reduce the number of empty bins in the min-wise hashing (MinHash) algorithm. The proposed method is based on the idea of amortization hashing (AHash), which can generate as few empty bins as possible. The experiments on real datasets validate the effectiveness of the proposed method."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,This paper proposes a feature extraction method for periodic signals. The authors propose a method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. Simulation and experimental results illustrate its effectiveness.
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"This paper proposes a method for data-to-text generation based on neural encoder-decoders. The authors propose a confidence-oriented decoder that assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data to text dataset (WikiBio) show that the proposed method is more faithful to the source than existing state-of-the-art approaches."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"This paper proposes a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. The authors show that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. The proposed method is easy-to-use and does not rely on the availability of training data except for the training phase."
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper proposes a decentralized stochastic gradient descent (SGD) algorithm that uses quantized communication. Theoretical results show that the proposed method can communicate a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Empirical results also show that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper studies the problem of learning partial models in model-based reinforcement learning (MBRL). In particular, the authors identify and clarify a fundamental problem of partial models: they are confounded by the observations they don’t model, and can therefore lead to incorrect planning. To address this problem, they introduce a general family of models that are provably causally correct, yet remain fast because they do not need to fully model future observations. They empirically investigate their effects on models learned in simple MDPs and 3D environments."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a new variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed model is based on two information theoretic problems—distributed simulation and channel synthesis, in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. The authors decompose a pair of data variables into their common representation and local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and the common representations as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images."
