paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"This paper proposes a new multi-agent multi-task RL algorithm that uses role-based learning to tackle the problem of decomposing complex tasks into roles. The authors propose to use a role selector to learn a role space and temporal resolution, and then use a bi-level learning hierarchy to learn the role selector for role discovery. The role policies are learned in primitive action-observation spaces, where joint action spaces are not available. The proposed method is evaluated on the StarCraft II micromanagement benchmark, and it is shown to outperform existing MARL algorithms in terms of learning efficiency and policy generalization in restricted role action spaces."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper proposes a stochastic subgradient descent (SSGD) method for solving over-parameterized nonsmooth optimization problems with an interpolation condition. The authors show that the composite structure of SSGD can be used to solve empirical risk minimization problems with convex and strongly-convex objectives. The paper also shows that the rates of the SSGD outperforms rates of SGD for smooth problems.   The authors also show that SGD and SSGD are equivalent to each other in the case of smooth and nonsmoothed machine learning models. The main contribution of the paper is that the subgradient method can be applied to both the smooth and interpolation setting, and that the interpolation is a special case of the convex case."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,This paper proposes to use non-linear “reservoir” layers instead of regular transformer layers in order to reduce the computational cost of transformers. The authors show that the proposed layers can be used to reduce wall-clock compute time and achieve state-of-the-art performance in machine learning. The paper is well-written and well-motivated. Experiments on machine translation are conducted to validate the effectiveness of the proposed method. 
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"This paper proposes a steerable CNN based on the group representation theory. The authors show that steerable convolutional kernels are invariant to transformation invariance and equivariant to equivariance. They also show that the steerable kernel function can be represented as a group representation based on filter transformed kernels.    Steerable CNN is an important topic in the field of neural network robustness to geometry transformation of data. Filter transform has been proposed as a simple and effective way to represent steerable neural networks. However, this paper shows that this theory is more general than the standard filter transform technique, and can be used to represent any kernel based on a filter transform. This paper also shows that the proposed approach can be applied to steerable convex and non-convex convolution operators to avoid overfitting."
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"Multimodal program synthesis is an important problem in program synthesis with user input, where the goal is to generate a program that satisfies a set of user-provided constraints. In this paper, the authors propose an optimal neural synthesis approach, where a program synthesis model is trained to generate programs that satisfy the user’s constraints. The authors propose to use a top-down recurrent neural model to learn the program synthesis score, and then use it to search for the optimal program in a search space with noisy signals (e.g., natural language, input-output examples). The authors show that the optimal neural model can be trained to learn to generate the program with a large number of different types of noisy signals, and that it can be used to synthesize programs with different user intent (i.e., the user intent for different multimodal synthesis tasks based on user intent, natural language (NL) and input-Output examples).  The authors also show that their method can be combined with existing automated program analysis techniques to search the search space, and it outperforms existing techniques in terms of accuracy and accuracy on the STRUCTUREDREGEX program synthesis dataset.    The main contribution of the paper is that the authors introduce abstract syntax trees, which is a way to represent abstract programs as abstract syntactically valid programs, and a neural model that learns to predict the program's score from the abstract syntax tree. The proposed model is also able to generate partial programs that are infeasible in the presence of the user's constraints, and is able to generalize to infeasibility of partial programs in the absence of a certain set of constraints. "
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper proposes a protein graph convolutional neural network (PGCN) based on the structure-based molecular interaction graph (SIREN) to predict the substrate specificity landscape of a specific type of proteins (protease enzymes). Previous methods for predicting protease specificity landscapes have been based on sequence patterns, but the authors show that the sequence motifs of a given protein are not necessarily the same across all proteins. The authors propose to use the structure of a protein to model the topology and energetic features of the protein, and then use the Rosetta energy function to map the protein's topology to the topological and energy features of a particular protein. The paper shows that the proposed PGCN (Protein Graph Convolutional Neural Network) is able to predict a protein’s specificity landscape, and that the specificity of NS3/4 protease on the Hepatitic C virus is similar to that of a typical NS3-type enzyme.   The authors also demonstrate that the sub-graph patterns learned by the proposed method are highly correlated with feature importance, which is a measure of the robustness of key life processes to mutational changes.  The paper also shows that, in classification tasks, the proposed model outperforms state-of-the-art machine learning models. The main contribution of the paper is that the authors propose a PGCNN model that can be applied to a wide range of proteins, including NS3 and NS4 proteases. The proposed model is based on physical interactions between two proteins, and is trained to predict whether a protein is active or not active, and the subgraphs of two proteins are active or inactive. The subgraph patterns are then used for molecular recognition based on feature importance for sub-based sub- graph patterns."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper proposes Double Q-learning, a method to mitigate the overestimation bias in Double Q - learning, which is a popular method to tackle the problem of value prediction. The authors propose to use the Bellman operation as a regularizer to avoid non-optimal stationary solutions, which has been a common problem in the recent years in the deep Q-Learning paradigm. They show that the approximate Bellman operator can be used to avoid the existence of non-optimistic fixed points, and that the underestimation bias is alleviated in the case of double Q- learning. The approach is based on approximate dynamic programming, and the authors show that their method outperforms several baseline algorithms on several Atari benchmark tasks. "
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"This paper proposes a two-step training framework for deep generative models (DGMs) trained on high-dimensional natural images. The authors propose two models for high-resolution image generation, BigGAN and VQVAE-2, which are GAN-based image super-resolution models (e.g., ESRGAN). The models are trained with different compute resources, and the authors propose to use end-to-end models.    The authors first train a sampler in the wavelet domain, and then a wavelet super-resolution decoder network is used to generate images in the low-frequency bands.  Wavelet-based down-sampling method is shown to improve the generative quality of a low-resolution sampler and to preserve structural information in the pixel-space. The paper also shows that the proposed Wavelet - based down-sampling method preserves the structural information of the images, and that the training cost is much lower than that of pixel-based methods.  The proposed model is evaluated on ImageNet, where the authors show that their model achieves better Fréchet Inception Distance (FID) compared to the original BigGAN model. They also show that the sampler, decoder, and decoder can be trained in parallel. "
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper studies self-supervised learning (SSL) algorithms for Fewshot learning(FSL) with large-scale labeled data. In particular, the authors propose to use SSL to improve the performance of FSL by using a pre-trained embedding network for downstream FSL tasks. The authors show that self -supervised training is able to learn a good pre-trainable pre-training set for FSL, and that SSL can be used to improve FSL performance when supervised training is not available.  The authors also show that, when the number of labeled data is large enough, the embedding networks can be trained in a supervised manner.   The main contribution of this paper is that the authors provide a theoretical analysis of the trade-off between self-trained loss and supervised loss in FSL. They show that the tradeoff between supervised training and self-substituted training improves the test accuracy, and the authors also provide empirical evidence that the self supervised FSL methods outperform the supervised ones. "
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper studies the problem of finding global minima of two-layer teacher-student networks. The authors propose to use first order methods to find ultra-wide neural networks with finite width. The key idea is to use the initialization of the teacher neurons to find the local minima, and then train the student neurons so that the student networks are as wide as possible. This is achieved by using the Angular Distance (AD) function. The methodology is shown to be computationally efficient. "
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper proposes a framework called Deep Adaptive Semantic Logic (DASL) to train deep neural networks with user-provided formal knowledge for learning from data. The key idea is to learn a knowledge representation for first order logic, where the formal semantics is learned in an adaptive way. The paper shows that vanishing gradients can be achieved by learning a deeper logical structure, and that finite sampling in infinite domains can be used to learn truth values. DASL’s representation is based on prior neuro-symbolic work, and is shown to be robust to data requirements, data scarcity, and lack of commonsense knowledge. Experiments show that the structure in an image classification task can be incorporated into a visual relationship detection task, and the results are shown to improve performance. "
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper proposes a new regularization approach for the learning of iterative solutions in feedforward residual neural networks (ResNets) for iterative recurrent computations. Iterative computations are an important problem in many computer vision tasks, and they have been used to train neural networks.    The authors argue that ResNets have the potential to learn iterative problems that are computationally expensive to compute, and that this inductive bias can be alleviated by introducing regularizations to the iterative convergent computation. The authors propose a method for recurrence regularization that combines the benefits of gradient coupling, Lipschitz constraint, and spectral normalization. They also propose a “recurrent” ResNet, which is a combination of a ResNet and a recurrent “resnet”, where the one is a recurrent network, and the other one is the one that is a residual network. They show that their method is more computationally efficient than existing methods, and shows that iterative convergence converges to a solution that is asymptotically similar to the original solution. They further show that this is a generalization of soft gradient coupling.  They also show that the Lipsichitz constraint on the residual functions is satisfied by spectral normalisation.  Experiments are conducted on several visual recognition tasks (MNIST, CIFAR-10, CifAR-100, and Digitclutter) where they show that recurrence normalization improves classification accuracy and improves the classification accuracy in terms of classification accuracy on a variety of tasks. In addition, the authors show that they are able to improve the performance of their method on a number of tasks that rely heavily on iterative converge (e.g., MNIST, MNIST).   Finally, they also show how their method can be applied to a range of existing methods and show that it can be used to improve their method. "
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques for deep neural networks are well-known to be effective for improving OOD generalization. However, they are typically applied to both independent and identically distributed (IID) data. This paper shows that standard normalization methods such as SelfNorm and CrossNorm can be used to improve the OOD performance of standard deep learning models. The main contribution of this paper is to show that SelfNorm, which is based on attention, is more robust to channel-wise mean and variance than CrossNorm. The authors also show that the performance of SelfNorm is comparable to that of CrossNorm in terms of out-of-distribution (OOD) generalization, and that CrossNorm is also more robust than SelfNorm when the feature maps are IID. The paper also shows that the self-attention used by SelfNorm for OOD classification and segmentation is more sensitive to the statistics usage. "
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper proposes a new approach to learn a neural network architecture for reinforcement learning (RL) using neural network architectures with high dimensional inputs (e.g. pixels). The authors propose to use inductive biases (i.e. Attention mechanisms) in reinforcement learning and control, where the goal is to learn multiple domains: vision, language, speech, and generative modeling, and unsupervised pre-training in multiple domains.    The key idea is that the attention module in a convolutional encoder in an RL agent can be seen as an inductive bias, and that they can be used to learn neural networks that are robust to different types of high-dimensional inputs. The authors show that the proposed approach can be combined with data augmentations and contrastive losses to improve the sample efficiency of the RL agent. The proposed module is evaluated on the DeepMind Control Suite environments and shows that the module is able to extract interpretable task-relevant information from the data. "
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"This paper proposes a gradient-based approach, called GradNorm, to train multitask networks. This extension to GradNorm is based on the observation that the learning is more efficient when the gradient magnitude of the task gradients is smaller than the gradient magnitudes of the other tasks. The authors propose Rotograd, an extension of the game theory to the problem of learning, and show that it converges to the optimal solution when the task magnitudes are small enough. They also show that the convergence is faster than the convergence of gradient-free gradient descent. The paper also shows that the proposed RotogRad outperforms existing approaches for multitask learning on several real-world datasets and network architectures. "
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper studies the geometry distortion problem of existing methods. The authors propose a new domain mapping function, called Minimal Geometry-Distortion Constraint (MGC), which is an I2I translation constraint. They show that existing methods suffer from geometry distortion issue in the case of paired data, which is caused by the randomness of color transformation in the translation process. They also show that the mapping function is invariant to the geometry structure of the source and target domains, and that the resulting translation suffers from unwanted distortions. To address this problem, the authors propose the Minimal geometry-distortion constraint, i.e., a function that minimizes the mutual information between a pair of translated images and the target images. The estimation and maximization of MGC is based on the approximate representation of mutual information. They demonstrate that MGC outperforms state-of-the-art methods on several benchmark datasets. "
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the problem of point sampling patterns in point cloud GANs. It shows that sampling-oversensitive discriminators (e.g., PointNet++, DGCNN, PointConv, PointNet-Max, and KPConv) are the most common sampling-sensitive discriminators for valid shape generation. The paper also shows that the sampling-insensitive discriminator design is sensitive to point clustering artifacts in shape point clouds, which is a common problem in generator design. The authors also show that the discriminators are sensitive to the sampling pattern of the point cloud generated by the generator, and propose a middle-point sampling-aware baseline discriminator (PointNet-Mix), which uses a sampling spectrum of metrics that includes perceptual metrics. They show that this sampling pattern is more robust to sampling-related metrics compared to geometry. "
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper studies the problem of white-box attacks on Convolutional Neural Networks (CNNs) trained on images with small quasi-imperceptible artificial perturbations. The authors show that Capsule Networks (CapsNets) are more robust than CNNs to small quasi imperceptible adversarial attacks. They also show that the robustness of Capsule networks can be improved by learning a computationally expensive routing mechanism, which can be used to attack the votes of CapsNets.    The paper also shows that the attack protocols for CNNs can be adapted to attack Capsule NNs.  The authors further show that multi-step attack methods can be applied to attack CNNs, and that the performance of Capsules is comparable to that of CNNs.  Finally, the authors propose a class-conditional reconstruction based detection method to detect the presence of vote attacks on Capsules. They show that a simple modification to the routing process of the vote attack in the detection-aware attack paradigm is sufficient to detect a vote attack on a Capsule Network.  They also demonstrate that Capsules can be trained to generate adversarial examples that are more likely to be detected by the adversarial robustness. "
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta-reinforcement learning is an important problem in reinforcement learning, where the goal is to learn a policy that can adapt to a new task in an online adaptation setting. The authors propose an algorithm that leverages privileged information from a task descriptor to improve the learning of recurrent policies. In particular, the authors propose to use recurrent neural networks to learn policies that are able to adapt to new tasks in a way that preserves the information about the previous tasks. The algorithm is based on the idea that the privileged information in the task descriptor is related to the behaviour of the RNNs. The proposed method learns an informed policy that is able to learn the task embeddings from the new task descriptors. This informed policy is then used to train a recurrent policy using parameters sharing and an auxiliary objective that encourages the learned policy to learn to perform well on new tasks. This approach is shown to reduce the learning sample complexity by a factor of $O(\sqrt{T})$. The authors show that the proposed method outperforms Thompson sampling, task-inference approaches for meta-rewarding, and other task-interference approaches. They also show that it is more efficient than the previous state-of-the-art meta-regression learning algorithms, and that it outperforms the previous work on meta-REINFORCE learning in the online setting with exploration/exploitation strategies. "
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper proposes a new approach to tackle the problem of trial-and-errors in RL for realworld applications. The authors propose a new paradigm to learn an RL policy from offline data using a model learning technique, where a simulator is used to learn optimal policies and a dataset is used for training a policy from an offline dataset. The RL policy is learned from this offline data, and the fidelity of the simulator is then used to train a new RL policy on the new dataset. In this way, the RL policy can be trained on offline data without the need for online sampling. The paper also proposes a method to learn robust recommendations that can be used for policy learning using existing models. In addition, the paper proposes an adaptive policy for real-world environments, which is able to adapt to the stochasticity of the dynamics. Experiments are conducted on two synthetic environments and a real-life ride-hailing platform. The method is shown to be able to overcome the distortion problem caused by online sampling, and to be more robust to changes in the dynamics during learning."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper studies the problem of learning goal-reaching behaviors with sparse rewards in reinforcement learning (RL) algorithms. The authors show that RL algorithms trained with imitation learning can learn goal reaching policies from expert demonstrations and a learned value function, but it is not guaranteed that the learned policy will reach the goal. To this end, the authors propose supervised imitation learning, where an agent is encouraged to learn a policy that maximizes the expected return from the demonstrations. The algorithm is shown to be able to learn to learn goal -reaching behaviors in the presence of sparse rewards. The paper also shows that the RL objective can be learned through an iterated supervised learning procedure, where the policy is trained to reach goals in a supervised manner, and that the algorithm is able to generalize well to new environments.  The authors also provide performance bounds for RL algorithms on several benchmark tasks, and show that their algorithm can generalize to a variety of environments and learn to reach new goals. They also show that the proposed algorithm can be used to improve the goal-reaching performance and robustness of RL algorithms. "
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper proposes a teacher-student distillation pipeline to improve the performance of autoregressive models for speech. The authors propose FastSpeech 2, which is a variant of the autorgressive teacher model for duration prediction. The key idea is to use knowledge distillation to tackle the one-to-many mapping problem in TTS, where the teacher model is trained on mel-spectrograms with information loss, and the student is trained to learn to predict the duration of the input signal. The teacher model learns to map the variation information of speech, such as pitch, energy, duration, etc., to conditional inputs for training.   The authors show that the proposed model, Fastspeech 2s, is able to learn a speech waveform from the conditional inputs to the predicted values for inference. They also demonstrate that the model can learn to learn the pitch and energy based on the learned conditional inputs, and that the predicted signal can be used for data simplification. Finally, the authors show the training speed-up in terms of the number of samples and training speed - up of the model compared to the original teacher model.  Experiments are conducted on a number of datasets, and show that, in general, the proposed method achieves better performance than the autore progressive models, and outperforms the previous state-of-the-art.  The main contribution of the paper is that the authors propose to use the predicted signals from the teacher and student models as conditional inputs in the training process, and to use end-to -to-end inference to train the speech waveforms from the two models. The paper also shows that the resulting model achieves better voice quality than the original model, and shows that it is more robust than the previous model."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"This paper studies the unsupervised dimension reduction problem (UDR) with tempered distributions in the k-dimensional subspace of a tempered distribution q(x) over the empirical probability density function. The authors propose an infinite-dimensional formulation, where the problem is formulated as the minimization of the distance between a nonnegative penalty function R(f, x) and the true distribution Q(f). The authors show that this problem can be solved using generalized functions, which is a well-studied problem in data science, such as the so-called sufficient dimension reduction task (SDR). They also propose an algorithm to solve this problem. The first algorithm solves the problem using a two-step iterative computation, and the second algorithm uses a single-step iteration to solve the problem. They show that the optimization problem of this optimization problem is equivalent to solving an optimization problem over distributions over ordinary functions. They also show that their algorithm converges to an algorithm that minimizes I(f) + λR(f), which is an algorithm based on the maximization of I(F(f)) +  (1) and (2) of a non-asymptotic minimizer of the objective function.  The authors evaluate their method on synthetic data, two synthetic data and two datasets, and two real data sets. They find that the proposed method outperforms existing methods on all three examples of UDR.   "
SP:93e54522e6c2b805905d21fc968fc40866f2898b,"This paper proposes two methods to improve the robustness of a model trained on rare or underrepresented patterns. The main idea is to use Feature Contrastive Learning (FCL) to learn a model that is robust to noise and sensitive to changes in the contextual utility of the features. Experiments on real-world applications show that models trained with FCL show improved robustness and sensitivity to noise, and improved generalization to noise."
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper proposes a new algorithm for learning autonomous agents from high dimensional observations. The algorithm is based on adversarial learning on the latent representation of a discriminator network. Imitation learning methods have been proposed in the literature, but they are usually based on the assumption that the optimal states are shared across all agents. This paper proposes to use mutual information constraints to learn a latent representation that maximizes the mutual information between the features of the agent and the discriminator. The authors show that this imitation learning method learns a shared feature space that is invariant to the constraints. The proposed algorithm is applied to several control problems, including balancing, manipulation and locomotive tasks, and is shown to be able to generalize to domain differences in terms of environment appearance and agent embodiment. "
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH), which claims that the generalization of a neural network trained with pruned weights is guaranteed to have zero generalization error, and that the test accuracy of an unpruned network is asymptotically the same as that of a pruned network. The authors propose to use the LTH in a deep neural network (DNN) for two applications: computer vision and natural language processing. They show that pruning multi-layer neural networks is equivalent to pruning the weights of a single hidden layer, and show that under certain assumptions on the objective function, sample complexity, and the number of weights, the pruned neural network has the same test accuracy as an unplunged network. They also provide an algorithm to prune the weights at each hidden layer to achieve the same guaranteed generalization. The algorithm is based on the (adversarial) stochastic gradient descent algorithm, where the algorithm first prunes the weights in the first hidden layer of the neural network model, and then uses the algorithm to train a pruning neural network that has a similar objective function and sample complexity to the one that was used to train the original neural network. Finally, the authors show that the model trained with the prune neural network is able to generalize as well as the original model with the same objective function.    The authors also provide a theoretical analysis that shows that the pruning of the winning ticket leads to a convex region where the non-pruned weights in a hidden layer have non-zero generalization performance. "
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes AutoLabel, a data augmentation method to improve the generalization performance of neural networks. The authors show that AutoLabel improves the accuracy and generalization of a number of existing methods on CIFAR-10/CIFAR100/ImageNet and ImageNet. AutoLabel is based on label smoothing, and the authors propose to use a hold-out validation set to improve calibration-performance of AutoLabel on augmented data, which is more robust to distributional shift between the clean distribution and the augmented distribution.  AutoLabel can be applied to several existing methods, such as AugMix, mixup, and adversarial training, and is shown to improve accuracy and calibration.  The authors also show that the models trained with AutoLabel are robust to adversarial attacks.   "
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"Self-supervised learning is an important problem in machine learning, especially in settings where there are no supervised signals and there is a large amount of unlabeled data. In this paper, the authors propose two methods to combine heuristic proxy classification tasks and data augmentations to improve the generalization performance of existing methods. The authors propose a causal framework for self-supervision in which the proxy classifiers are trained with invariance constraints on the augmentations used during pretraining. They propose a selfsupervised objective, Representation Learning based on Invariant Causal Mechanisms (RELIC) to learn an invariant prediction of proxy targets using an invariance regularizer to improve generalization guarantees. They also propose a contrastive method, called contrastive learning based on causality, which is an extension of a previous work (Zhang et al., 2017). Experiments show that RELIC outperforms existing methods in terms of robustness, out-of-distribution generalization on ImageNet, and human-level performance on Atari."
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"Object goal navigation is a challenging problem in which the goal is to navigate to a goal using only directional navigation signals. In this paper, the authors propose Visual Transformer Network (VTNet) to learn an informative visual representation in navigation. The visual representations of the observed scene are used to guide the navigation actions. The authors propose a new structure, called VTNet, to learn visual representations based on the structure of spatial locations of objects and image regions. A pre-training scheme is used to pre-train the visual representations for navigation policy learning. The proposed VTNet uses spatial-aware descriptors based on object and region features to learn the informative representation for navigation. These descriptors are learned using attention operations, and the visual representation is trained to capture the location cues in the object/region features. Experiments on the artificial environment AI2-Thor show that VTNet outperforms existing methods in terms of performance. "
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,This paper studies the problem of federated learning for neural network models. The authors propose a solution to privacy-preserving federated federatedlearning based on the secure aggregation primitive. They show that the communication-computation efficient secure aggregation saves communication/computational resources compared to the secure solution. They also propose a scheme to learn the topology of secret-sharing nodes based on sparse random graphs instead of a complete graph. They use the Erdős-Rényi graph to represent G. They evaluate the reliability/privacy of the proposed scheme on a number of standard federated learners and show that their scheme achieves state-of-the-art reliability and data privacy. 
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"This paper considers the problem of finding an incentive compatible auction in Auction Design, which is an extension of previous theoretical approaches to this problem. The approach is based on the observation that neural network architectures can be used to design optimal auctions, and the authors propose a theoretical auction design that allows for a time-independent Lagrangian. The optimization procedure is then based on an inner maximization loop that maximizes the optimal misreports. Auction Design is then formulated as a two-player game with stationary utility functions, where the hyper-parameter search is performed by a neural network. "
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper studies the problem of fine-tuning a pre-trained model for a downstream task on a large-scale dataset. The authors consider both supervised and unsupervised pre-training approaches for learning representations. The former has been widely used in fine tuning methods, while the latter has not. In this paper, the authors propose to use the discriminative knowledge of labels and the intrinsic structure of data to improve the performance of both. They propose a general learning approach, called Bi-tunning, to fine-tune both the classifier head and the projector head, which is based on a categorical contrastive learning loss. They show that the intrinsic structures of a classifier and a projector head can be used to boost the performance on a number of downstream tasks. They also show that a contrastive cross-entropy loss is used to enhance the label information of a particular classifier in an instancecontrast way, and that this improves the accuracy in the low-data regime.   The authors also propose a variant of their approach, which they call Bi-tuning, which can be applied to vanilla fine tuning, and can be seen as an extension of the recent work of [1].  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]"
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper proposes a new measure to measure the robustness of classifiers to adversarial accuracy and adversarial training. It shows that classifiers trained on adversarial perturbations are more robust to the perturbation than trained on clean data. The authors also show that adversarial robustness is correlated with generalization.    The main contribution of the paper is that the proposed measure is a measure that can be used to measure robustness for classifiers. It measures the distance between the predicted classes of a classifier trained on the clean and adversarially perturbed data, and it is shown that the distance metrics are invariance-based, which means that it is invariant to the class of the adversarial examples.  The authors further show that the invariance of adversarial attacks to the perceptual classes is a norm-based distance metric, and that the test accuracy of the classifier is correlated to the accuracy of perceptual classes.  Finally, the authors show that a model trained on invariant-based adversarial adversaries can be trained on a set of invariant examples, and the model can be further trained on more robust examples that are not invariant.  Experiments are conducted on a number of datasets, showing that the robust classifier can achieve a high level of accuracy on the adversary examples, but can also achieve high levels of accuracy for the original classifier on the original adversarial samples.  In addition, they show that test accuracy and test accuracy are correlated with test accuracy, and show that there is a trade-off between test accuracy on test and test adversarial instances."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"Disparate impact is an important problem in many machine learning applications. This paper studies the problem of disparate impact in graph-structured data. The authors introduce a new fairness concept called dyadic fairness, which is the notion of fairness in algorithmic designs. They show that the dyadic fairness of link predictive scores depends on the number of edges in a graph and the predictive relationship between the edges in the graph. In particular, the authors show that for a certain class of graph connections, the fair adjacency matrix of a pair of edges is equal to that of all edges in that graph. They then propose an algorithm called FairAdj to learn a fair admissible matrix for the fair link prediction under certain graph structural constraints. The proposed method is shown to achieve a fair-utility tradeoff between the predictive accuracy of the algorithm and the fairness of the final link prediction, mitigating discrimination.   "
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"Autoencoders are an important information compression framework, and it is well known that the generative ability of an autoencoder is highly dependent on the disentangledness of the latent code. This paper proposes a new method called Disentangled Exploration Autoencoding (DEAE) that combines disentangling representation and regularization to improve the exploration in latent space (i.e. interpolation). The key idea is to use GAN-based adversarial training to train a VAE with Gaussian prior knowledge for the synthesis, and then to use disentanglement and exploration in the positive loop of the DEAE. DEAE uses an encoder to disentangle a latent code space from the encoder and a decoder to learn a latent representation. The encoder is trained with directed interpolation, and the decoder is used to generate a disentanged latent code, which is then used as a positive loop for the positive loops. The authors show that DEAE is able to generate attribute-controllable augmented samples, and that disentangler and exploration are used to improve fairness problems, and DEAE can be used to mitigate dataset bias. The method is evaluated on a number of datasets, and is shown to be effective."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a Bayesian memory allocation scheme for the episodic and semantic memory in the human memory model, which is based on a hierarchical latent variable model. The authors propose to use a serial event (episodic memory) and a compressed representation (semantic memory) as complementary learning systems. The Kanerva Machine uses a locally contiguous memory to store the differentiable block allocated latent memory, and then uses it as part of a feed forward deterministic process. The allocation scheme is applied to the problem of memory conditional image generation. The conditional likelihood values of the conditional likelihood on binarized MNIST and binarised Omniglot are used, and the authors show that the allocation scheme works well for CIFAR10, DMLab Mazes, Celeb-A, and ImageNet32×32. They also show that memory writing is more efficient than previous work, and that the read key distribution is similar to the one used in previous work.  "
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,This paper studies the sample complexity and loss landscape of attention-based neural networks. Attention mechanisms and deep learning models are well-studied in machine learning tasks. This paper shows that attention mechanisms are more efficient than other deep learning approaches. The authors also show that attention models with self-attention have a lower sample complexity than other models. The paper also shows that the local minimum of the attention model has a lower prediction error.
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"Active inference is an important problem in Bayesian modeling based on a biologically plausible model. In this paper, the authors propose to use reinforcement learning (RL) algorithms to solve the problem of active inference using the expected free energy (EFE). The EFE is modeled as a negative value function, and the authors use the free energy principle, prior preference, and a theoretical connection between the EFE and the prior preference to propose a method to learn a prior preference for active inference in inverse RL. The authors show that active inference with EFE-based rewards is equivalent to prior preference learning for the inverse RL problem. "
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a new data augmentation method for improving generalization of neural networks. The authors argue that existing methods do not generalize well to out-of-distribution (OOD) data, which is a common problem in adversarial and standard learning. The paper proposes to use pseudo-labels as pseudo-label for OOD data to avoid undesirable features. Experiments are conducted on CIFAR-10, CifAR-100, and ImageNet, and show that the proposed method outperforms existing data augmentation methods, and is competitive with adversarial training. The main contribution of the paper is that the learning scenario is not limited to out of distribution data, but can be applied to any learning scenario where there is a large amount of OOD training data.   "
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,This paper proposes a metareinforcement learning (meta-RL) method called Fast Linearized Adaptive Policy (FLAP) that learns a shared linear representation of the policy for out-of-distribution tasks. The policy is learned using an adapter network that learns linear weights for each task and adapts the policy to the new task using gradient descent. The adaptation run-time of a separate feed-forward network is also improved. The authors show that FLAP outperforms prior meta-RL methods such as MAML in terms of average return and adaptation run time. FLAP is also shown to outperform prior methods on continuous-control meta-reward benchmarks.    The authors also compare FLAP to prior MetaRL methods on a number of tasks where the adaptation is performed in an unsupervised way. They also show FLAP achieves better average return as well as adaptation runtime speeds compared to prior methods. 
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a new federated kernel k-means algorithm. The algorithm is motivated by the communication efficiency of the algorithm. In particular, the authors consider the optimization problem of kernel k - means in federated settings, and propose a communication efficient mech anism (CEM) to reduce the communication cost. The authors show that feder ated kernelk - means can be used for privacy preservation, and provide an approximate solution. They also show that the local computational results can be obtained via matrix operations, which is similar to kernelk-mean. Finally, they show that DSPGD with CEM can achieve O(1/T) rate with a communication cost O(O(T) and O(T^T) times faster than the original federated Kernel k-Means algorithm, and that federated kerne l k-mean achieves better clustering quality compared to the original kernel k--means.   "
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"This paper proposes a new approach to train models with hardware & latency constraints to improve the accuracy of such architectures. The authors argue that CNNs have a constant training cost, but that the cost is highly correlated with the cost of the hardware and latency constraints. They propose an approach called Pareto-Pareto frontier (PPF) to reduce the combinatorial explosion of sub-optimal model configurations in the search space. They show that this approach can improve the performance of such models on resource-intensive tasks with different deployment targets. They also show that the model search/extraction time can be much faster than the state of the art in terms of the number of parameters and the training budget, and that their approach can be applied to a variety of models in the design space (e.g. CompOFA). They also demonstrate that their search space can be extended to a diversity of hardware and latencies targets and that the search can be done in a single step.    The authors show that they are able to find the accuracy-latency Pareta frontier in a search space with a search budget of $O(\sqrt{n\log n})$, where $n$ is the training cost and $n \log n$ is a training budget. The search space is defined as $n\times n$ where $N$ is $n^{-1/2}$ and $N\times d$ are the model dimensions, and $d$ is an input to the search. The paper also shows that models trained on a diverse set of hardware/latency targets can be found in this search space, and show that their model search time matches the state-of-the-art.  Finally, the paper shows that their method is able to achieve Paret-optimality on ImageNet, which is a dataset with a large number of configurations. "
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"Meta-learning aims to train a model on limited data with the goal of improving the performance of the model when the number of samples is limited. This paper proposes a meta-learning algorithm called ADML (ADversarial Meta-Learner) that uses adversarial samples to improve the performance when the learning model is trained on both clean and adversarial data. The authors show that ADML can improve the initialization of a learning model in an adversarial manner, and that it can also improve the robustness to adversarial attacks. The paper also shows that it is more robust to attack mechanisms that can be applied to the training data.   The authors also show that the proposed ADML algorithm is robust to both limited and even contaminated samples.  The paper evaluates ADML on two image datasets, MiniImageNet and CIFAR100, and shows that the performance improvement of ADML is comparable to the state-of-the-art meta-learners. In addition, it is shown that it outperforms the state of the art for adversarial training and robustness, and it is also shown to be robust to different attack mechanisms.  Finally, the paper shows that in the presence of limited data, ADML outperforms other representative meta learning algorithms in terms of accuracy, robustness and transferability. "
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"Error correction codes are of great importance in many communication applications. However, there are many suboptimal decoding algorithms, and the decoding of transmitted codewords using a maximum likelihood rule is one of the most commonly used algorithms. In this paper, the authors propose a data-driven framework for permutation selection that combines domain knowledge, machine learning concepts (e.g., node embedding, self-attention, etc.). The authors propose to use permutation decoding as a permutation for each permutation in the input sequence. The authors show that their simulated Bose Chaudhuri Hocquenghem (BCH) code achieves the best bit error rate compared to several baseline decoders. They also show that the proposed algorithms can be applied to physical layer communication systems using self-pay attention networks."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper studies fine-tuning BERT for the text classification task. The task is an unsupervised classification task, and finetuning for this task is a well-studied topic in the literature. The authors propose to fine-tune BERT on top of an existing unstructured clustering approach for the intermediate task. They show that this classification step is effective for topical classification tasks, and that the labeled examples from the classification step can be used to improve the performance of the downstream task. In addition, the authors show that the classification performance of BERT can be improved when the labeled data is more diverse. "
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper studies the problem of training generative models with a fixed (random shooting) control agent, where multimodal posterior predictives are used. The authors show that the sample complexity of MBRL with a certain training schedule is O(1/\sqrt{T}^T) when the multimodality is high, and O(T/T^2) when multimodalities are low. They also show that MBRL can be trained with the same training schedule as a deterministic model, but with a regularizer that encourages heteroscedasticity. They show that this regularizer can be applied to any probabilistic posterior predictive, and that it can be used to train a generative model that is asymptotically similar to deterministic models as well as to their more generalised counterparts. Finally, they show that, when trained with mixture density nets instead of models that are deterministic, they can achieve a sample complexity that is O(\sqrt{\log T}^2/\log T) times faster than their more standard (probabilistic counterparts). They also demonstrate that this framework can also be used for MBRL on Acrobot, where they train them on a control problem and then use them to solve the control problem."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes Affine Disentangled GAN (ADIS-GAN), a variant of the Generative Adversarial Network (GAN). The key idea is to use an affine regularizer to mitigate the inductive bias of the affine transformation properties of images. The affine transformations are defined as the transformation matrices of the input image. The authors propose a new affine matrix that is composed of the transformed part of the original image and the transformed parts of the target image, and then use maximum likelihood estimation to optimize the transformation parameters. Experiments are conducted on MNIST, CelebA, and dSprites datasets, and show that ADIS-GAN can learn disentangled representations that are more robust to different types of transformations (rotation, horizontal and vertical zoom, horizontal, vertical skew, horizontal & vertical translation, etc.). The authors also show that the features learned by ADIS - GAN outperform existing approaches on disentangling representations across different transformations. "
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"Representation learning is an important problem in many applications, and contrastive learning methods have been shown to be superior to their supervised learning counterparts in terms of generalizability. Representation learning with data augmentations can be problematic when there are distortions in the input image structures. This paper proposes a novel method for instance discrimination-based contrastive loss based on augmentations to the representation bank. The main idea is to use a fully supervised upper bound for unsupervised learning, which is based on an overoptimistic assumption that the distribution of strongly augmented queries is the same as that of the distribution divergence for the retrieval of weakly augmented queries. The augmentations are used to improve the performance of the contrastive losses. The proposed method is evaluated on ImageNet on top-1 accuracy on fully supervised ResNet-50 architecture and a single-layer classifier fine-tuned, and it is shown to outperform self-supervised and supervised methods on transfer learning and object detection tasks. The authors also show that the augmented images are more likely to have distorted visual structures, and that the distributions of the augmented examples are similar to those of their weakly augmentations."
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"De-identification from magnetic resonance imagery (MRI) is one of the most commonly used methods for identifying a patient’s MRI scan from a database. However, there are several issues with existing de-identifying methods for this task: they are expensive to train, they are sensitive to privacy-sensitive facial features (e.g. 3D volume), and they do not generalize well to medical analyses of the brain. This paper proposes to use MRI de-identified techniques to identify a patient's MRI scan and train a deep learning framework that can be used for medical analyses such as segmentation and age prediction.   "
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"Graph neural networks are an important tool for modeling graph data. Graph neural networks have been widely used in node classification and link prediction tasks, but they are not well-suited for the task of graph pooling. In this paper, the authors focus on the problem of learning a representation of a graph that is invariant to changes in task relevance and structural dependencies. The authors propose a new representation for graphs that is based on hierarchical graph partitioning methods, where each pooling function is used to learn a compact form of the representation for each node in the graph.    The paper proposes a multi-head attention based global pooling layer, called Graph Multiset Transformer (GMT), which is a multiset encoding problem with auxiliary information related to the graph structure. The paper shows that the multiiset encoder and decoder of the pooling problem can be decomposed into a multiscale encoding problem, where the auxiliary information is related to graph structure and task relevance.  The authors show that the proposed GMT achieves better injectiveness, permutation invariance, and memory and time efficiency on several graph classification benchmarks compared to existing graph pooled methods. They also compare the proposed methods to existing node clustering approaches for hierarchical graph poolings, and show that GMT achieves the best performance in terms of injectiveness (i.e., the number of node features per layer) compared to the existing methods. "
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper studies the problem of over-squashing in graph neural network (GNN) models. In particular, the authors show that GNNs with long-range interaction can suffer from the problem when the prediction task is based on a single node in a graph with exponentially growing information. They show that the problem can be alleviated by training GNN models with fixed-size vectors that are more robust to long range signals. The paper also shows that the long-term interaction between two nodes in the graph is a bottleneck for GNN training, and that the bottleneck can be removed by tuning and/or changing the weights of the GNN.   The authors also show that existing studies have shown that existing GNN-based models (GCN, GIN, GAT, GGNN) suffer from over-Squashing, which is a phenomenon that happens when the number of nodes in a GNN is large enough that the input to the network is dominated by incoming edges. The authors then propose a new GNN model, called Long-range GNN (LGNN), that is able to overcome this issue. The LGNN model is trained to be robust to this problem."
SP:90d8fa381446923902e42b259392e5e975e6caa1,"Sentiment analysis is an important problem in marketing and advertising, and there are many existing cross-domain sentiment analysis methods. However, these methods rely on cross-distribution generalizable classifiers, which can be problematic when the data annotation is not available. This paper proposes a novel domain adaptation method, where methods are trained to learn domain-agnostic representations. The key idea is to train a classifier on the annotated data, and then adapt the classifier to the target domain by learning a prototypical distribution over the data distributions. The authors show that this method can be applied to any embedding space, and that the proposed method is transferable to other domains."
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"Gender-biased stereotypes are prevalent in natural language processing. This paper studies the evaluation of genderbias in the context of natural language understanding through inference. The authors propose a challenge task to evaluate NLI models for gender stereotypes based on occupations. They show that a gender neutral premise is more powerful than a gender-specific hypothesis. They also show that models trained on MNLI and SNLI data-sets (BERT, RoBERTa, BART) are more sensitive to genderinduced prediction errors than models trained using other debiasing techniques. The paper also proposes a new evaluation methodology to evaluate the gender-balanced dataset."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper proposes a new unsupervised reinforcement learning method called variational intrinsic control (VIC). Two VIC algorithms are proposed: one is based on the notion of intrinsic options, and the other is a combination of the two. The latter uses an intrinsic reward to encourage the agent to explore the intrinsic options. Both methods are based on a transitional probability model and a Gaussian mixture model. Experiments are conducted on a number of stochastic environments."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the problem of sample efficiency in the low data regime of training deep neural networks for image classification. The authors propose to use an ensemble of relatively small deep networks to solve the image classification problems. They show that neural ensembling is effective for small data domains and that they can be used to improve sample efficiency on small datasets. They also show that the proposed technique, called deep ensembleling, outperforms state-of-the-art approaches on a variety of datasets and ensemble configurations. The paper also shows that the losses can be tuned to make the ensemble more robust to changes in the number of samples. "
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"Quantized neural networks are a popular technique for reducing the computational power and storage requirements and processing speed of DNN-based applications for InternetOf-Things (IoT) devices. However, they have a fixed and limited compression factor, which limits them to be useful for many DNN -based applications with limited resources. This paper shows that quantized networks are more efficient than Binary Neural Networks (BNNs) in terms of speed-up, and that they can be much more powerful than BNNs.   The authors propose a new model and training scheme, called Sparse Binary neural Networks (SBNNs), which is based on the observation that -1/+1 weights are much more sparser than standard binary networks. The authors show that sparsity is achieved by using positive 0/1 binary weights instead of the standard -1/-1 weights. The proposed method is evaluated on linear and convolutional networks on MNIST and CIFAR-10 datasets, and it is shown to outperform the state-of-the-art compression rates and generalization performance of SBNNs with the same compression factor. "
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a post hoc calibration method for model calibration on OOD data. The proposed method is based on outlier exposure to model probabilities on the corrupted data. Predictive uncertainty is defined as the difference between the class membership probabilities of the model outputs and the true model outputs. The paper proposes two measures to measure this uncertainty: (1) the Brier score, which is a measure of the distance between the predicted error rates and the actual error rates of a machine learning model, and (2) the Pareto frontier, which measures the gap between the true and predicted uncertainty of the models.   The paper shows that under certain conditions, a baseline method for measuring predictive uncertainty based on the softmax probabilities of a model can be used to calibrate the model. Uncertainty estimates are obtained by comparing the estimates of class membership probability for a given class to the model predictions. The authors also show that the calibration error of the proposed methods is a function of the number of times that the model has been trained on a corrupted data, and that models trained on the same corrupted data tend to have higher calibration error than models trained with a different number of classes.  Experiments are conducted on three different approaches: model ensembles, Stochastic Variational Bayesian Inference (SVB) for deep learning, and temperature scaling. The results show that PREDICTIVE UNCERTAINTY is the best measure of calibration error, and the authors show that their methods achieve better calibration error."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"Graph neural networks and graph kernels are commonly used in machine learning problems on graphs. Graph neural networks, graph kernels, and non-isomorphic graphs can be seen as special cases of these two approaches to graph properties.   This paper studies the expressive power of graph neural networks (and graph kernels) on graphs and shows that graph representations for the similarity/distance of graphs are more expressive than those of graph kernels. The paper also shows that existing approaches can be extended to non-Isomorphic graphs.  The authors also show that algorithms for learning graph representations and similarities are more powerful than those for graph kernels and show that models trained on node attributes are able to learn more expressive models and kernels. "
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a new method for self-supervised image-to-image (i.e., i.e. i.i.d. inpainting) image generation. The idea is to regularize the image generation process to avoid warping artifacts.   Image animation is an important problem in many real-world applications where the goal is to capture the motion of a driving video in a way that is consistent with the pose references of the driving vehicle.  Self-supervision is often used to train a generative model that can be used for this purpose.  This paper proposes to use data augmentation techniques such as CutMix for regularizing non-warp-based image generation, and then apply them to the task of image animation. CutMix is an augmentation approach for image animation that augments the original image with an image that has been warped by warping.  The main contribution of this paper is to propose a method called PriorityCut, which augments an image with a warped version of the input image. The key idea of the proposed method is to use the information from the inpainted regions of the image to guide the generation of a new image from the warped image.  In order to achieve this, the authors propose to use a combination of two existing data augmentations: (1) a modification of CutMix, which is based on the idea of using the information of the inpainted regions as guidance, and (2) the use of a modification to CutMix to make it more robust to warping, which can be seen as a way to mitigate the difficulty of inpainter of a warped image due to the warping artifact.  Results show that the proposed PriorityCut outperforms vanilla CutMix and other image animation models in terms of pixel-wise difference, low-level similarity, keypoint distance, feature embedding distance, and the ability to learn smooth transitions. The paper also shows that inpainteing is able to mitigate warping by using occlusion information for regularize discriminator predictions.  Overall, the paper is well-written, well-motivated, and easy to follow. The learning is thorough and the experiments are well-designed. The results are interesting.  However, there are a few concerns:   1. The authors do not provide sufficient analysis of their method.  2. They do not compare to other self supervised image animation approaches.  3. They only compare their method to a number of state-of-"
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper proposes a new approach to learn disentangled representations. The authors propose a self-supervised approach to disentangle independent causal mechanisms (ICM) from disentanglement. The key idea is to learn mechanisms that are independent of the data generation process and independent of independent latent variables in the model.   The authors show that existing approaches for learning disentangling representations do not work well because of the coarse granularity of data generation processes (mechanisms). The authors also show that the groundtruth mechanisms can be learned from observational data.  The proposed approach is based on an unconventional mixture prior that is used to train a self -supervised generative model to learn such mechanisms in a self supervised generative scenario.  Experiments are conducted on several downstream tasks and show that this approach is able to learn an intervention, a covariant shift, and noise to improve the performance of the proposed approach on downstream tasks compared to the state-of-the-art methods. "
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper proposes a self-labeling approach for chemical compound graph representation learning. The authors propose a graph aligning approach to generate rich or detailed labels for each molecule in a molecular graph structure (W) from a 2D image, which is then used to train a mediating representation V, where f is the number of molecules in the molecule, and W is the molecule's chemical graph structure.  The authors show that the proposed approach is able to learn chemical compound graphs from 2D images with normal labels W, and that domain adaptation outperforms a pretrained model. The self-labelling approach is evaluated on the Maybridge data set, and is shown to be able to achieve state-of-the-art performance.    The main contribution of the paper is that the authors propose to use a fully mediating layer, where each molecule is partitioned into a set of molecules, and each molecule’s molecule is represented by a fully-connected layer, and a machine learning model is trained to predict the label of each molecule. "
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper proposes a domain knowledge based deep learning framework for solving chiller plants energy optimization problems. The authors show that a deep network trained on realworld physical systems (e.g. image classification, NLP, etc.) can be used to learn a nonlinear model based on domain knowledge in the redundancy function space. They show that the proposed methods can be applied to complex systems, where previous methods can only learn a linear model for complex systems. They also show that domain knowledge can be leveraged to solve a small sample size problem, and that the energy consumption estimation of chillers can be solved as an input-output monotonic problem.    The paper also shows that a Neural Network with monotony constraints can be trained to solve the input-outline monotonicity problem. The proposed method is evaluated on a cooling system in a data center, and compared to existing ones for energy optimization, and the proposed framework outperforms existing ones. "
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a new method for spatio-temporal forecasting for large-scale forecasting models. The authors propose a new model called CausalTrans, which is based on the idea of causal attention. The idea is to use a graph-based attention mechanism to capture the causal effects between two nodes in a graph, and then use a multi-head attention with Taylor’s expansion and softmax to reduce the time complexity of the attention. Experiments show that the proposed method outperforms existing baselines in terms of accuracy and time efficiency."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes an unsupervised framework called the coupled mixture VAE (cpl-mixVAE) which is an extension of the existing VAE framework to the problem of learning a mixture of discrete and continuous factors of variability. The authors propose to use interacting autoencoding agents to solve this problem. The idea is that the mixture of continuous factors can be represented as a set of mixture representations, and that it can be treated as a variational inference problem.  The proposed approach is evaluated on categorical assignments on MNIST and dSprites, and it is shown that the proposed approach can identify type-specific, activity-regulated genes in a single-cell gene expression dataset with cortical neuron types. The multi-agent framework is also shown to be effective. "
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper studies Group equivariant convolutional networks (GCNNs) for learning convolutions with symmetry priors. The authors show that the equivariance constraint on the kernels of G-steerable kernels of convolutions can be used to define models that are G-stakeable, i.e. a compact group of groups. The paper also shows that there exist constraints on steerable kernels and spherical tensor operators based on quantum mechanics.   The authors also show that steerable kernel spaces can be obtained from generalized reduced matrix elements, ClebschGordan coefficients, and harmonic basis functions in homogeneous spaces via the Wigner-Eckart theorem.  The main contribution of this paper is to show that GCNNs satisfy the G-Steerability constraint, and that it can be seen as a special case of Gsteerability."
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"Selective classification is a popular technique to avoid abstentions due to spurious correlations between classes. However, it has been shown that it can be problematic due to accuracy disparities. This paper studies the issue of average accuracies of selective classification, which is a measure of the accuracy of a classifier on a subset of classes.    Selective classification can be seen as a way to reduce the gap between the average accuracy of the classifier and the average classifier of the entire class. The paper shows that selective classification can reduce the full-covering accuracy disparities between classes in both vision and NLP datasets.  The paper also shows that, under certain conditions, selective classification on models that are distributionally-robust (i.e. have symmetric margin distributions) can achieve better accuracies than models that do not have this distribution. In particular, the authors show that the margin distribution is symmetric if and only if there is a left-log-concavity between the margin and the true margin distribution. The authors also show that such a distribution can be used to obtain better full-coverage accuracies for distributionally -robust models."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper studies the problem of topic modeling on large-scale data with complex multi-modal structure in applications such as multi-layer network analysis, temporal document classification, and video data analysis. The authors propose a new training method called Neural NCPD, which is a training method for hierarchical topic modeling with multi- modal tensor data with a latent hierarchical structure.    The authors show that the neural network architecture and backpropagation for error propagation in hierarchical NCPD is similar to the one used in previous work. "
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,This paper proposes a new adversarial robustness certificate for graph neural networks (GNNs) based on the idea that adversarial perturbations can be applied to any node in the graph. The authors show that the proposed certificate can be used to certify that a GNN classifier is robust to adversarial attacks. They also show that this certificate is more robust than existing ones.   
SP:cc93dd2f68e415e2457166e78627865dc1b44697,"Generative Adversarial Networks (GANs) are a class of generative models that can be applied to complex real-world data. Learning high-dimensional probability distributions is an important problem for both generative and discriminative neural networks. However, existing GANs (Least Squares GAN (LSGANs), WGANs, WGAN) suffer from the non-convergence problem, mode collapse, gradient explosion or vanishing. This paper proposes a modification methodology of loss functions to alleviate these issues. Specifically, the authors propose to use quantile regression to approximate the 1-Wasserstein distance between the discriminator and the gradients of the generated and target samples. The authors show that LSGANs avoid mode collapse and LSGAN can avoid local minima, but that the mode collapse problem of LSGAN is not alleviated by the proposed modification methodology. They also show that the proposed approach to modify loss functions of GAN's can be used to improve the robustness of QRGAN (Quantile Regression GAN, QRGAN). QRGAN is a modification of the original QRGAN, which is a GAN with a Wasserstein-based loss function. QRGAN can be seen as an efficient way to mitigate the nonconvexity of the Wassersteins, which allows QRGAN to be more robust to mode collapse without inefficient computation. The paper also shows that QRGAN improves the generation performance assessment using the Frechet Inception Distance (FID) of the real and generated data distribution.    The main contribution of this paper is the modification of loss function of WGAN and QRGAN. The modification methodology is based on WGAN, and the authors provide a theoretical analysis that shows that the WGAN is more robust than QRGAN in terms of mode collapse. "
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,This paper studies the relevance metrics for similarity-based explanation for machine learning models. The authors show that the cosine similarity between gradients of two samples of the same class is a relevant metric. They also show that these metrics can be used as a proxy for other relevant metrics.  
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"Graph Neural Networks (GNNs) have been shown to have good generalization power. However, their generalization properties are not well-studied. In this paper, the authors propose a new graph isomorphism test based on algorithmic alignment. Specifically, they introduce an LRGA module in GNNs, and show that it can be used to improve the sample complexity of the kernel’s feature map. The authors also show that the 2-FWL update step of an RGNN with polynomial kernels with LRGA is equivalent to a randomly initialized two-layer MLP, and that the LRGA can be applied to any GNN layers.   The authors show that LRGA improves the generalization performance of existing GNN architectures. In particular, they show that dot-product attention improves the sample efficiency of a kernel, which improves the performance of GNN benchmarks. They also demonstrate that the expressive performance of LRGA does not depend on the number of layers, but rather on the size of the network. "
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper proposes to use objectness measures to improve the calibration performance of Convolutional Neural Networks (CNNs) on a variety of objectness-based classification tasks. The authors propose two methods to do so. The first method, called adaptive label smoothing, is based on the observation that the class activation maps of classifiers trained on CNNs trained on ImageNet-1K are highly sensitive to changes in the objectness of the training images. The second approach, called MS COCO, uses objectness to adjust the loss functions of classification CNNs. Both approaches are based on random crops, but they differ in the way that they are applied. Context dependence in safety-critical applications is a key factor in the two approaches. The proposed approach is applied to both classification and transfer learning tasks. In classification, the authors use the relative object size as a smoothing factor, and in transfer learning, they use the classifier to adaptively adjust the smoothing of the loss function. They show that the proposed approach outperforms the baselines on the classification task and outperforms a hard label approach on transfer learning. They also show that their approach can improve the confidences of CNNs on classification and learning on the transfer learning task.    The paper is well-written and well-motivated. However, there is a lack of comparison between the proposed methods. In particular, the contribution of this paper is limited to the context only images. "
SP:5254658923e594294b69d124a8d004166852822a,"This paper studies the duality of neural networks for inverse problems in medical imaging. In particular, the authors consider the convex duality framework and show that neural networks with weight decay regularization can be seen as convex solvers for solving the dual network optimization problem. The authors also show that the neural networks trained with path sparsity can be used to solve the inverse problems. Finally, the paper shows that the dual dual network can be viewed as a convex optimization of a conveX dual network for interpreting training and prediction. The main contribution of the paper is the application of piecewise linear filtering to the problem of prediction. Experiments are conducted on MNIST and fastMRI datasets to validate the theoretical findings."
SP:085cad6bc143c8713580bddfaa71f06496dac314,This paper proposes a novel approach to improve the performance of text-to-speech (TTS) models. The authors propose to use a differentiable alignment scheme to align the output of the generator during training and inference. They also propose a soft dynamic time warping to improve spectrogram-based prediction loss. Experiments show that the proposed approach outperforms the state-of-the-art in terms of mean opinion score. 
SP:01148cea55db606aa78d27e900818684a8bce9ab,"This paper proposes a non-parametric representation learning method for node attributes in real-world graphs. The proposed method is based on the Wasserstein distance between nodes in a graph. The authors show that the distance between two nodes in the graph can be approximated by the WASSERstein distance, which is a lower-dimensional space that can be used to represent discrete distributions. The paper also shows that the proposed method can be applied to node classification and matrix completion tasks. "
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes a new method for reinforcement learning (RL) in the presence of sparse extrinsic rewards. The authors propose an Adversarially Motivated Intrinsic GOals to learn a goal-conditioned “student” policy. The agent, called AMIGO, is trained using meta-learning, where the agent is trained with a “goal-generating teacher” that is trained to maximize the likelihood of the agent achieving the goal. The teacher is trained in a similar way to the way that the agent was trained in prior work, but with an additional “constructively adversarial” objective that encourages the agent to achieve goals that are more likely to be generated by the agent than the environment reward. Experiments are conducted on procedurally-generated tasks, and show that the proposed method outperforms prior work in terms of intrinsic motivation and RL methods."
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"This paper considers the problem of information retrieval from a dataset of files with a certain privacy level, where the goal is to minimize the rate-distortion-leakage tradeoff between the user distortion and the user privacy constraint. The paper proposes a data-driven framework based on generative adversarial models, where a model is trained with private information retrieval and the retrieval process is constrained to be free of distortion. The authors show that the perfect privacy requirement is satisfied if the mutual information of the information-theoretical formulation of mutual information between the data and the distortion constraint is equal to the true mutual information. They also show that there exist schemes that achieve a trade-off between a good download rate, distortion, and user privacy leakage. The proposed scheme is based on a constrained minimax game, and is evaluated on a synthetic Gaussian dataset, the MNIST and CIFAR-10 datasets. They show that their scheme achieves better trade-offs in terms of both the number of samples required to achieve a good tradeoff, and also shows that the rate -distortion tradeoff curve converges to the optimal tradeoff. Finally, they show that an achievable scheme based on source coding is also shown to achieve better tradeoffs between the desired trade-of-download rate and distortion.    The authors also demonstrate that the proposed data -driven approach outperforms the achievable scheme on the synthetic gaussian dataset and MNIST dataset, and outperforms an achievable approach based on the data from the source coding. They do not evaluate their scheme on Cifar-10, but show that they are able to match the performance of the achievable approach."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks (GNNs) have been widely used in graph-related applications, but they are not well suited for large scale settings. This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) that decouples the node embeddings of the GNN into two modules. The first module uses greedy auxiliary objectives, and the second module uses a lazy-update scheme. The authors show that the proposed method can be used for time or memory limited applications, and that the fidelity of the model is improved over sampling-based methods in terms of fidelity. The paper also shows that the sampling in GNN training (i.e. the sampling of a node embedding) can be decomposed into two parts: (1) the sampling that is performed at each layer of GNN layers, and (2) the number of layers that are updated at each iteration.   The authors also show that their method is able to achieve better efficiency and accuracy compared to previous methods. The main contribution of the paper is the decoupling of the two modules and the use of a lazy update scheme. They also show how the proposed approach can be applied to parallel GNN learning, and show that it can be combined with sampling in order to improve the efficiency of the DGL- GNN model. "
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"Neural link predictors are used to predict missing edges in large scale Knowledge Graphs. The paper proposes a framework to solve complex queries in incomplete Knowledge Graph with missing edges. The authors consider logical conjunctions (√ll), disjunctions, and existential quantifiers. The proposed models are trained end-to-end, and the authors propose two solutions to the optimisation problem: gradient-based and combinatorial search. The approach is compared to state-of-the-art methods and neural models, and is shown to outperform the neural models on Hits@3. The model is trained with intermediate solutions to complex query atoms, which is an end- to-end differentiable objective. The neural link predictor is trained to predict the existence of missing edges, and then the model is fine-tuned on the intermediate solutions. "
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper proposes a new procedure for verifying local robustness of feed-forward neural networks with piecewise-linear activation functions. Local robustness is defined as the ability of a model to be robust to adversarial perturbations to a `p-ball consistently (i.e., the number of times an input is perturbed in a certain region of the input space). The paper proposes to use decision boundaries as a metric for assessing robustness. The paper shows that the `2 norm of the decision boundary can be computed in a highly-parallel GPU implementation, and that the robustness can be verified in the presence of adversarial inputs that lie in convex polyhedral regions. The proposed approach is compared to other approximate verification approaches and verifiers, and is shown to be more robust than existing verifiers. The authors also show that their algorithm is computationally tractable. "
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper proposes an approach to learn the embedding of objects in the affordance space, i.e., the knowledge of an object’s “affordance”. This embedding is different from existing approaches in that they do not rely on human judgements of object affordance, but rather on text corpora. The authors show that this embedding can be learned in a similar way as existing approaches, and that it can be used to learn a mental representation of objects across different dimensions. They also show that their embedding performs well on a number of datasets, and can be combined with existing methods to learn representations that are more interpretable to humans. "
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). Individuality is important for human society. It improves efficiency and productivity. It can be used for the division of labor, and it can also be used in the context of multi-agents cooperation. The authors propose a probabilistic classifier based on EOI to predict the probability distribution of the agent’s individualities. They also propose two regularizers to improve the performance of the classifier. Finally, they propose to use intrinsic reward as an intrinsic reward to encourage emergence of individualism in MARL algorithms. Experiments show that the proposed methods outperform existing methods in several multi- agent cooperative scenarios.   "
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"Randomized smoothing has been shown to improve certified robustness against l2-norm adversarial attacks. However, randomized smoothing requires the base classifier to be trained on the entire training set. The authors propose the Smoothed WEighted ENsembling (SWEEN) scheme to train randomized smoothed classifiers. SWEEN is able to achieve optimal certified robustess in the presence of ensembling generality. The main contribution of the paper is the adaptive prediction algorithm to reduce the prediction and certification cost of the SWEen models. Experiments show that the SweEN models with small models can achieve competitive performance in terms of training time and certification performance.    The paper is well-written and well-motivated, and the authors have done a good job of explaining the benefits of the proposed adaptive prediction method. "
SP:ea892e3d199ed6121279b20061a87f43afae8796,"This paper proposes to use hierarchical structures in the learning process to improve generalization to complex real-world tasks. Specifically, the authors propose Ordered Memory Policy Network (OMPN) to learn a subtask hierarchy for task decomposition. The subtask boundaries are learned in an unstructured demonstration. The model is evaluated on two tasks (Craft and Dial) and compared with several baselines. The authors show that OMPN performs well in both unsupervised and weakly supervised settings. They also show that the model is able to generalize to partially observable environments where the inductive bias is less strong. Finally, they show that their model can generalize well to a task-agnostic setting, and that the subtask hierarchies learned in the model can be used to improve the performance of the model. "
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) generalization based on causal reasoning. The CSG is a generative model that learns a semantic factor and a variation factor that is invariant to domain-specific correlation. The authors propose two methods for OOD prediction based on the causal invariance principle. The first method is based on variational Bayes for learning and prediction. The second method uses semantic-identification for adaptation. Experiments show that the proposed baselines outperform the baselines on OOD generalization error. 
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of online learning algorithms with adversarially corrupted rewards. The authors consider the setting where an online algorithm is trained with a stochastic reward, and the goal is to learn an algorithm that is robust to corrupted rewards and has a small regret. In this setting, the authors show that for any algorithm with uncorrupted reward distribution, the regret with respect to the original reward is a function of the noise rate of the algorithm. They also show that robust estimation for unsupervised learning problems can be used to obtain robust online algorithms that have near optimal regret in these scenarios, including the standard scenarios of the so-called ""stochastic multi-armed bandits"", the linear contextual bandits, and Markov Decision Processes (MDPs) with stochedastic rewards and transitions. The algorithms are tested on both synthetic and real datasets, and are shown to be robust to a wide range of noise rates. "
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes RewriterEvaluator, an encoder-decoder architecture for neural machine translation (NMT). It consists of a rewriter, an evaluator and a termination policy. The rewriter uses a prioritized gradient descent (PGD) method to guide the rewriter through the rewriting process, and the evaluation policy is based on the termination policy of the Rewriter. The authors show that this framework can improve the performance of existing NMT models (e.g., Transformer) on two translation tasks (Chinese-English and English-German). They also show that the proposed framework outperforms several baselines. "
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"This paper proposes a black-box segmentation framework that incorporates a model for the learning of calibrated stochastic mappings between Ambiguities, images, and unsystematic annotation. The authors propose a two-stage, cascaded strategy for calibrated adversarial refinement, where a distribution over predictions is sampled from a multimodal predictive distribution, and the empirical frequency of the sampled predictions is used as a metric to measure the quality of the calibrated mappings. The proposed approach is evaluated on the multigrader LIDC dataset, the Cityscapes dataset, and on a toy regression dataset. The core design of the proposed core design is to learn a calibrated predictive distribution for tasks where the categorical likelihood is low. The adversarial network is trained to produce coherent predictions, and these are then used to train a model that can be used to refine the calibrated predictions. The framework is applied to the task of semantic segmentation, and is shown to outperform existing methods. "
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new algorithm for distributed stochastic optimization (DSO) for large-scale machine learning problems. The proposed algorithm is based on the idea of error feedback (EF), which is used to train an unbiased compressor to compress the gradient of the algorithm. The authors show that EF can be used to improve the communication efficiency of existing algorithms. They also show that the proposed algorithm can be applied to federated learning with partial participation. "
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper studies hyperparameter optimization (HPO) in the context of a machine learning (ML) algorithm. HPO is a well-studied problem in ML development, and the authors propose a new baseline for HPO, which is based on the observation that the performance of existing ML algorithms is highly dependent on the hyper parameter settings. The authors propose two approaches to this problem: (1) finding the optimal hyper parameter search space, and (2) learning neural architectures that are well-suited to this search space.  The authors show that the proposed baseline outperforms the existing HPO algorithm on a number of benchmark datasets. The paper also shows that the baselines can be found in python packages that are compatible with existing baselines and benchmarks. Finally, the authors also provide a theoretical analysis of the proposed research framework.   "
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper studies the robustness of high-dimensional, high-entropy label representations. The authors show that audio labels are more robust to adversarial attacks than numerical probabilities and text labels. They also show that the adversarial examples are more likely to be low-dimensional than text labels, and that they are more sensitive to the error signal.    The authors further show that high-dimensionality of the labels is more important for robustness than low dimensionality for classification performance. "
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes two approaches to train ensemble neural networks. Both methods rely on forward passes for prediction, where a single model’s capacity is used to partition the network into subnetworks, and each subnetwork is trained independently. The authors show that the forward pass improves the negative log-likelihood, accuracy, and calibration error. They also show that their out-of-distribution variants outperform existing methods in terms of model robustness. Experiments are conducted on CIFAR10, CIFar100, and ImageNet."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a novel method for Knowledge Distillation, where the intermediate knowledge of a Convolutional Neural Network (CNN) is shared between a teacher network and a student network. The proposed method, Sparse Representation Matching (SRM), is based on sparse representation learning. The intermediate feature maps of the teacher and student networks are learned using pixellevel and image-level labels. SRM uses sparse representations of the hidden features in the teacher CNN and a neural processing block based on stochastic gradient descent. Experiments show that SRM outperforms other KD techniques. "
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a novel approach to learn policies that have a sequential structure in the representation learning process. In contrast to existing reinforcement learning methods that focus on learning policies with a fixed structure, this paper proposes to learn a theoretically motivated policy similarity metric (PSM) that encourages behavioral similarity between the optimal policies. The authors propose a contrastive representation learning procedure to learn policy similarity embeddings (PSEs1) based on the PSM, which can be used as a state similarity metric. Experiments on three benchmarks (LQR, jumping task, and Distracting DM Control Suite) show that PSEs improve the generalization performance of these benchmarks in the presence of spurious correlations in LQR. "
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"This paper proposes a new approach to disentanglement in Machine Learning. The approach is based on distributed equivariant operators, where the model is trained on a latent representation of the input object shape. The key idea is to learn disentangled transformations (i.e., affine transformations) that are invariant to topological defects in the input images, e.g., rotations, translations, etc.). The authors show that this approach can disentangle the disentangling effects of a number of different transformations on the input image, including images with different transformations with different deformations. The authors also show that their approach disentangles the effects of disentangler and disentange the encoder and decoder in the latent space.  The authors further show that the proposed approach can be used to train models with distributed operators that can be applied to a large number of transformations, and that the approach can also be used in order to train a model with disentanged representations. Finally, the authors provide a theoretical analysis of the factors that lead to the group representation theory.   "
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes a statistical framework based on the Hawkes process to study the timedependent interaction of neuronal spiking activities. The authors propose to use nonlinear Hawkes processes to model the influence pattern induced by excitatory or inhibitory interactions. They propose an iterative algorithm to estimate the functional connection weights using auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes, and sparsity variables in Gaussian form). They also propose an expectationmaximization (EM) algorithm to obtain a maximum a posteriori (MAP) estimate. They evaluate their algorithm on synthetic and real data and show that their algorithm is able to capture the temporal dynamics of interaction. They also show that the algorithm can capture the interpretable functional connectivity of neural spike trains. "
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the gradient descent (GD) algorithm for two-layer neural network models. In the under-parameterized regime, the authors show that the GD dynamics in the parameter regimes where the number of quenched “neural network-like” neurons is small (i.e., in the case of a random feature model) exhibits a neural network - like behavior. The authors also show that GD can be seen as a quenching-activation process, where the “quenched” neuron’s continued activation and deactivation process is the result of the ‘implicit regularization’ of the inner-layer parameters. The paper also shows that GD dynamics with “mean-field” scaling can be explained as a result of this ‘quenching process’, and that it exhibits a “random featurelike behavior”. "
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper studies constrained Markov decision process (CMDP) problems for reinforcement learning problems with prescribed safety constraints. The authors propose a new model for the CMDP problem, which is based on the observation that there are two types of MDPs in a CMDP: reconnaissance MDP (R-MDP) and planning MDP(P- MDP). The authors show that the reward-seeking policy in P-MDEs can be approximated by a fixed threat function, and that the threat function is a combination of a threat function and a Q-function analogue of danger. They also show that a baseline policy can be learned from the threat functions.  The authors also propose a generative model to learn the risk function for the R-DP, and show that an approximation method can be used to approximate this threat function for R-MODE.  Finally, the authors demonstrate that the proposed method outperforms existing approaches on a benchmark dataset and complex collision-free navigation tasks. "
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper proposes a new crossentropy loss for neural architectures for classification tasks, which is more powerful than the square loss used in previous work. The authors show that crossentropies of neural architectures and benchmark datasets for NLP, NLP with automatic speech recognition (ASR) and computer vision tasks, as well as for non-vision tasks, can be obtained with hyper-parameter settings. Cross-entropy can be used for computer vision and NLP tasks, while square loss is used in NLP for classification. The paper also shows that deep learning on equal footing with respect to the cross-entropy of deep learning can be achieved with square loss. "
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes a self-supervised representation learning method for reinforcement learning. The proposed method, called Self-Predictive Representations (SPR), is based on the idea of self-predictive representations (SPRs), which is a generalization of the Self-Predictive Representation Learning (SPL) framework. The authors show that the proposed method is sample efficient and can be applied to a wide range of RL tasks, including reinforcement learning, reinforcement learning with limited interaction, and reinforcement learning without interaction."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper proposes a new method called InstantEmbedding for generating single-node representations. The proposed method is based on local PageRank computations, and the authors claim that the proposed approach is able to learn globally consistent representations that can be used for unsupervised representation learning for a wide range of machine learning tasks, including node classification, link prediction, and link prediction. The authors show that the instantiation of the proposed method, which is a simple extension of existing methods such as DeepWalk, node2vec, DeepWalk and VERSE, can achieve comparable performance to existing methods (DeepWalk, FastRP, and FastRP) in terms of computation time and memory.   The authors also show that their approach can be applied to any graphs, and that the single node’s embedding can be learned using Instant Embedding. In particular, the authors propose to learn a d-dimensional embedding vector for each node in a graph, and then use this d-dimensionality to learn the embedding of a single node in the graph. Unsupervised embeddings are then used for a range of different kinds of tasks, such as visualization, node classification (for node classification), link prediction (for link prediction), and node classification for node classification.  The proposed approach can also be used to learn compact representations of graphs. The paper is well-written and easy to follow. "
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"Graph coarsening is an important problem for large-scale graphs. In this paper, the authors propose to apply deep learning on graphs to improve the performance of graph coarsens. In particular, they propose a new framework for learning a coarsened version of the Laplace operator and the projection/lift operators for a coarse graph with edge weight. This framework can be applied to any existing coarsense algorithm, and it can be used to control the quality of the resulting graph. The authors also propose to use graph neural networks to learn the weight assignment map. The proposed method is evaluated on both synthetic and real networks, and compared with several existing graph-coarsening methods on various metrics, reduction ratios, graph sizes, graph types, and graph sizes. It is shown that the proposed method outperforms existing graphcoarsens methods on all of these metrics and graphs. The paper also shows that the method can be trained on large graph data, and that it can learn the essential properties of the graph, which is a nice contribution to existing data-driven methods."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper proposes a geometric deep learning algorithm for learning acoustic properties of 3D objects. Acoustic properties such as scattering characteristics, localization, and acoustic scene analysis are important for 3D audio content creation, environmental acoustic effects, and 3D acoustic content creation. The paper proposes to use numeric solvers to learn the acoustic properties for these three interactive applications.   The paper uses discrete-laplacian and implicit encoders to learn these characteristics, and then uses a multi-layer network on a NVIDIA GeForce GeForce RTX 2080 Ti GPU to learn acoustic properties on arbitrary topologies. The authors show that these characteristics can be used to learn 3D object properties at interactive rates, and that the learned characteristics are robust to point cloud approximation.  The authors also show that the learning method can achieve high accuracy in terms of accuracy in generating environmental acoustic effect in dynamic environments, and can be applied to any high-dimensional latent space."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"Distributional shift is an important problem in machine learning prediction systems, where the model’s sensitivity to extreme distributional shifts can be highly sensitive to the causal and anti-causal elements. This paper proposes Risk Extrapolation (REx), a new approach for robust optimization based on the perturbation set of extrapolated domains (MMREx). The authors show that REx is robust to both causal mechanisms and to covariate shifts. The authors also show that the proposed REx outperforms existing methods such as Invariant Risk Minimization in terms of robustness to causally induced distributional shift and covariate shift. "
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper proposes to use neural networks to learn mappings between finite-dimensional Euclidean spaces. The idea is that neural operators can be used to learn the mapping between function spaces, and that this can be applied to neural operators for solving partial differential equations (PDEs). The neural operator is learned by learning an integral kernel in the Fourier space, and the functional parametric dependence between the input and the output of the neural operators is used to define the mapping. The authors show that they are able to solve PDEs in this way, and demonstrate that they can solve Burgers’ equation, the Darcy flow, and Navier-Stokes equation. They also propose a new ML-based method called Fourier neural operator, which is a zero-shot super-resolution for turbulent flows. It outperforms existing PDE solvers in terms of accuracy, and it outperforms other learning-based solvers with fixed resolution.   "
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the convergence of gradient flow in linear neural network training with infinitesimal step size. In particular, the authors focus on the tensor formulation of neural networks and show that gradient flow (i.e., gradient descent) converges to a global minimum under the assumption that the convergence direction of the network parameters are orthogonally decomposable. The authors show that under this formulation, the gradient flow for separable classification is orthogonal. They also show that for L-layer linear tensor networks, the convergence assumptions are also orthogonsistent. Finally, they show that the global minimum of a gradient flow is a norm-like function in the transformed input space of a network with a “transformed” input space. "
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"Slimmable neural networks have been a popular topic of interest in recent years. They are particularly useful in resource-constrained settings (e.g. mobile devices) where the storage cost of the model can be prohibitively expensive and the FLOP requirements are high. This paper proposes to use slimmable neural networks with a width-multiplier to learn sub-networks with different performance profiles.    The approach proposed in this paper is to learn the width multipliers for each sub-network based on a multi-objective optimization lens. The authors propose an algorithm to learn shared weights and width-multiplyers for all sub-nets, and then apply the algorithm to optimize the shared weights, width- multipliers, and the number of layers.  The main contribution of the paper is that the authors propose to use heterogeneous width-pl multipliers across sub-nets, which is an interesting idea.  They show that the proposed method outperforms existing alternatives in terms of top-1 accuracy, prediction accuracy, FLOPs, and memory footprint. The experiments on the ImageNet dataset show that MobileNetV2 achieves state-of-the-art performance on MobileNetv2 with lower channel counts. The paper also shows that the method can be applied to a variety of network and dataset combinations, cost objectives (FLOPs or memory footprint) and different cost objectives, and achieves competitive performance. "
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper proposes a new federated semi-supervised learning problem called Federated SemiSupervised Learning (FSSL), which is a variant of the federated learning problem known as Federated Matching (FedMatch). In FedMatch, the goal is to minimize the labeling cost for each client, which is motivated by the fact that the expert knowledge of each client is not available to the other clients. The authors propose two scenarios in FSSL: (1) disjoint learning with both labeled and unlabeled data, and (2) federated matching with private data. The proposed method is applicable to both of these problems. In the first scenario, the authors propose FedMatch with an inter-client consistency loss, which penalizes the difference between the label of the private data and the labeled data of the public data.  In the second scenario, FedMatch is based on both the standard Federated Learning and Semi-Supervised learning approaches, where the annotation is provided to each client.  The authors compare the proposed method with baselines based on federated training and local semi supervised learning, and show that their method outperforms the baselines in both cases. "
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"Self-supervised learning with discrete event sequences for real-world users requires low-dimensional fixed-length vector representations for downstream machine learning tasks. This paper proposes CoLES, which extends contrastive learning from audio and computer vision domains to the discrete event sequence domain under the self-supervision setting. CoLES uses a novel augmentation method to augment the representation of discrete events in order to improve the performance of the discrete events. Experiments on public datasets show that CoLES representations outperform existing methods on several downstream tasks."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper proposes a new model, StructFormer, to learn the dependency and constituency structure of natural language grammars such as dependency grammar and constituency grammar for assembly of one or several corresponded words. The authors propose a new parsing framework to learn a constituency tree and a dependency graph. The proposed model is based on StructFormer which is a model that learns the dependency tree and the dependency graph in a transformer with a dependency-constrained self-attention mechanism. The induced dependency relations are incorporated into the transformer and the authors show that the proposed model can learn unsupervised dependency parsing and masked language modeling. Experiments are conducted on several datasets to demonstrate the effectiveness of the model.    The paper is well-written and well-motivated. However, there is a lack of comparison with other unsupervisory parsing methods."
SP:078966ff62775bba6031e47d374bda95f4a7dde3,"This paper proposes a method for learning structured representations from images. The proposed methods are based on an annotated mapping between nodes of scene graphs and object bounding boxes. The idea is to combine scene graph nodes, visual objects, and scene graphs with object features and relational features. Experiments are conducted on the Visual Genome (VG) and Visual Relation Detection (VRD) datasets. Results show that the proposed model outperforms state-of-the-art approaches on the scene graph grounding task, and the proposed method also outperforms the existing method on a scene graph parsing task with weak supervision."
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"This paper proposes a new framework called Relational regularized autoencoder (RAE) to learn the distribution of data with the help of reconstruction loss and relational regularization in the latent space. The approach is based on the observation that the inner discrepancy between the von Mises-Fisher distribution and the vMF distribution is a mixture of two distributions, which is a variant of sliced fused GromovWasserstein (SFG). The paper shows that the discrepancy is a product of the relational discrepancy, i.e., the difference between the two distributions. The paper proposes two variants of this relational discrepancy: spherical sliced fused fused gromov Wasserstein(SSFG) and SSFG. The SSFG uses the power spherical distribution to reduce the sampling time in high dimension settings, and the SSFG is a variation of the mixture of mixture of von Mise and Fisher distributions. Both variants of the RAE framework are based on these two discrepancies.   The paper also shows how to use autoencoders for learning latent manifold structure, image generation and reconstruction, and how to train autoenconders for both reconstruction and learning the discriminative task. "
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes a new approach to reduce the computational costs and training time of training for deep networks. The approach is based on the observation that deep learning model sizes tend to increase as the number of layers increases, and that the repeated structures (e.g., the transformer module) in deep networks tend to have repeated layers. The authors propose an adaptive untying criterion based on a theoretic analysis of deep linear networks. They also propose a new method to reduce BERT's training time by removing the need for weight sharing and monitoring gradient statistics. Experiments on natural language processing and computer vision tasks demonstrate the effectiveness of the proposed method."
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper studies the relationship between adversarial transferability and interaction inside adversarial perturbations. It shows that the negative correlation between DNNs can be used as a metric to measure the transferability of a perturbation, and that transferability-boosting methods rely on negative correlation. The paper also shows that existing methods for improving transferability are not robust to interactions in the attacking process. "
SP:f1565319075c1442c2cb52d96443facb492c06c2,"Catastrophic forgetting is a well-studied problem in deep learning models. This paper studies the relationship between neural network (hidden) representations and task semantics. The authors show that deeper layers are responsible for forgetting, and that sequential training can be used to learn task representational subspaces. Methods are proposed to improve the forgetting of deeper layers, but some of the contributions are limited to feature reuse (e.g. removing interference), while others are not. The paper also shows that the maximal forgetting of task sequences with intermediate similarity to the original task representations is not a result of interference, but of learning neural representations. "
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes a new training algorithm, called EarlyBERT, for pre-training and fine-tuning large-scale language models such as BERT, XLNet, and T5, which are Deep, heavily overparameterized language models for natural language processing (NLP) tasks. The authors argue that the inference time of BERT and XLNet is too slow due to the large batch sizes used during the training process, and that large NLP models should be pre-trained with model compression to reduce the training time. To achieve this, the authors propose to use Early-Bird Lottery Tickets for computer vision tasks, where tickets are randomly selected from a lottery ticket pool. The tickets are used during BERT training, and the authors show that their training algorithm can be used to speed up the training of large language models in terms of computation resources, training time, and model complexity.  The authors also show that the structured winning tickets can be incorporated into the transformer in order to reduce computational resource demands. They further show that this can be combined with self-attention and fully-connected sub-layers in the transformer, which can further reduce the computational cost.  Experiments are conducted on GLUE and SQuAD downstream tasks, and show that pre-trains with the proposed training algorithm (earlyBERT) outperforms BERT in training time and performance.   "
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper proposes a new f-divergence measure for learning with label noise. The authors show that f-Divergence measures have the decoupling property in the form of a variational form, and that they are robust to label noise in the sense that they decouple the classifier’s predictions and the supervised labels. They also show that this decoupled property holds for any divergence that includes the variational difference between the clean distribution and the noisy distribution.    The paper also shows that the robustness of f-derivergence functions is a function of the number of labels, the noise rate, and the labels’ noise rate. Robust-f-distivergence-measure has been shown to be robust to noisy labels in UCSC-REAL.  The authors also propose two new metrics that are based on f-disturbance functions. "
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes an ensemble-based weighted Bellman backups based on uncertainty estimates from Q-ensemble. The method is evaluated on a number of continuous and discrete control benchmarks and shows that the method is able to improve the diversity of learning in challenging domains. The authors also show that the Q-learning is more robust to noisy rewards, and that the uncertainty of Q-estimates is more sensitive to the signal-to-noise aspect.   The paper also shows that, in contrast to the standard Bellman backup, the ensemble of the ensemble is more diverse than the traditional Bellman Backup, and UCB Exploration. Bootstrap is also used to encourage diversity.  The authors show that, when using off-policy RL algorithms (such as Soft Actor-Critic and Rainbow DQN), the proposed method outperforms the state-of-the-art in both lowdimensional and high-dimensional environments, and the performance is comparable to that of the standard weights of the Bellman Backbone. "
SP:afc08f203562b841180811aef943bfb63a1659ea,"This paper proposes a new few-shot classification framework for modeling uncertainty in the meta-learning algorithms for fewshot classification problems. The prediction of uncertainty is based on class-wise similarities between the training data and the test data to avoid distributional mismatch due to the random sampling of tasks. The proposed method combines meta-training with a few existing methods to improve the performance of a model trained with meta-testing. The method combines several existing meta learning models, and the authors also propose a new training strategy to train the model for calibrated classification. Experiments show that the proposed training strategy improves the accuracy of the model, especially when the dataset shift is small. "
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a new paradigm for learning video-text representations that are dissimilar to the dominant paradigm in the literature. The proposed method is based on noise contrastive learning, where the representations are learned from visually similar videos. The method uses a generative model to learn the dissimilar representations. The authors show that their method outperforms others on MSR-VTT, VATEX, ActivityNet, MSVD, and MSVD. They also show that the learned representations are more dissimilar than the ones learned from the original video. "
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper presents a method for pre-trained language models (PLMs) for Chinese natural language processing (NLP) tasks. The proposed method is based on Chinese word segmentation (CWS) and subword tokenization. The idea is to pre-train a masked language model pre-training on a single vocabulary, and then use seg tok for Chinese BERT. The authors also propose multi-vocabulary pretraining (MVP) to improve the models expressiveness. They show that using MVP improves the expressiveness of Chinese PLMs, and that it improves the performance of seg-tok on sentence level tasks. They also show that the char based vocabulary is more expressive than segtok, and Chinese PLM can be trained on Chinese characters.   "
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks (GCNs) have been widely used for graph-based learning tasks, but there are many issues with GCNs, especially in real-world large graphs. In this paper, the authors propose a method called BDS-GCN, a method for distributed GCN training that can handle both graph partition and distributed training. In particular, the proposed method is based on an unbiased boundary sampling strategy, where each partitioned subgraph is partitioned into a set of boundary nodes, and the boundary nodes of each subgraph are sampled from the set of GCN structures. The authors show that this method can achieve full-graph accuracy with a fraction of the memory and communication costs of previous state-of-the-art methods. They also show that the performance of the method is comparable to the state of the art in terms of accuracy, throughput, and memory usage. In addition, they show that their method can be applied to GCN architectures that are more robust to changes in the size of the graph. "
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper proposes a graph neural network (GNN) architecture for large-scale physics-based quantum chemistry simulations. The proposed model, called ForceNet, is based on graph neural networks, which is able to learn the per-atom forces of atoms in 3D space. The authors demonstrate that the proposed model can be applied to a number of complex quantum chemistry problems, and can be used to perform quantum chemistry simulation on a large molecule-by-molecule basis. The paper also shows that the model can also be used for the discovery of new molecules. "
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper studies the generalization properties of deep neural networks trained with regularisation for fine-tuning. In particular, the authors consider two approaches to regularisation in the context of fine-tuning. The authors derive a neural network generalisation bound based on the Rademacher complexity of the algorithm. This bound matches the existing bounds for convolutional networks, and is a generalization of the previous bounds for the case that the radius of the search space is larger than the number of training samples. They also show that this bound can be extended to the case where the network is trained with transfer learning for initialisation of the network.   The authors also provide a theoretical analysis of the generalisation performance of transfer learning in terms of generalisation. They show that transfer learning improves generalisation when the initialisation is a function of the learning rate, and that this learning is more robust to the size of the training set.  Finally, they propose a new fine tuning algorithm for learning a hypothesis class. It is shown to outperform the state-of-the-art penalty-based competitors, and it is also shown to be more robust than penalty-free alternatives."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"This paper studies the problem of model pruning in the lottery ticket framework, where the goal is to find a mask that maximizes the likelihood of the model to be pruned. The paper proposes a lottery ticket-based method for mask discovery (Hfind) and mask evaluation (Heval) for model evaluation, where a mask is found based on the training configuration and the number of layers in the model. Two different hyperparameters are used for these two stages: mask discovery and Heval. The decoupled find-evaluation phenomenon is attributed to the use of unstructured magnitude pruning for vision classification tasks, which decouples the decoupling of mask discovery from mask evaluation.   The paper shows that masks are found using different layerwise pruning ratios, and that these masks can be found using the same Hfind values. The authors also show that these ratios can be used to decouple masks in a one-shot structured pruning, where models are pruned sequentially.  The authors further propose a lottery-ticket-based lottery ticket for the lottery-prioritization of hyperparameter values, which is based on decouplings hyperparametry from the mask values. "
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper studies the evolution of alignment of per-example gradients during training. The authors consider the Coherent Gradients (CG) theory and propose two metrics, m-coherence and O(m). They show that m-cherence is more robust to label noise than other metrics, and that the gradient diversity is more important than the gradient coherence. They also show that over-parameterized neural networks are more likely to memorize than non-overparametrized neural networks. Finally, they empirically show that ResNet and EfficientNet models trained on ImageNet with m-Coherence outperform real labels in terms of memorization and generalization. The main contribution of this paper is that the authors propose two variants of Coherent CG, one with label noise, and one with gradient diversity. "
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper considers the problem of constructing sufficient statistics for implicit generative models based on summary statistics. The authors propose a new approach to approximate Bayesian computation and neural likelihood methods. The evaluation of the likelihood function is based on the assumption that the true likelihood function can be decomposed into sufficient statistics and sufficient statistics that are not sufficient statistics, i.e. that the density or density ratio of the model is equal to the sum of the density of the true distribution and the sum over all the labels of the training data points.   The authors show that this approach can be applied to a variety of tasks, and that their approach is able to improve the performance of existing neural likelihood algorithms. They also show that the infomax learning procedure can be extended to deep neural networks, which is an interesting direction. "
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper proposes a new image-to-image translation model, called TUNIT, for the task of image to image translation. The key idea is to use set-level supervision for data collection, where the goal is to ensure that the paired images and the domain labels are consistent across all image domains. The proposed model is shown to outperform a standard set -level supervised model with full labels, and is also shown to be more robust to hyperparameters (e.g., pseudo domains). The authors also show that the proposed model can be used in a semi-supervised scenario where the estimated domains are not fully labeled, and that the model is able to generalize well to unseen domains. Experiments are conducted on a few datasets to validate the effectiveness of the proposed method.  "
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper considers wide neural networks with implicit bias in function space. The authors propose a curvature penalty function based on the probability distribution of the network parameters, and show that it can be applied to different initialization procedures. They show that for a width-n shallow ReLU network, the solution function is a natural cubic spline interpolation. They also show that an asymmetric initialization with uniform distribution can be used to compute a constant curvature, and that it is equivalent to a weighted second derivative. They then show that a uniform distribution is sufficient to compute the symmetric initialization for the same solution function. Finally, they show that spatially adaptive smoothing splines with a certain regularization strength are sufficient for training trajectories for multivariate regression and activation functions.  "
SP:8b885142facbb3b8db41ec9d83822cee81324694,"Weight decay is a popular regularization technique for training deep neural networks. In this paper, the authors show that L2 regularization for weight decay is equivalent to weight decay for adaptive gradient methods such as Adaptive Momentum Estimation (Adam) and decoupled weight decay (Decoupled Weight Decay (AdamW) in deep learning libraries). They also show that unstable weight decay in these optimizers such as Momentum (e.g. stochastic gradient descent (SGD) can cause the optimizers to overfit to the hyperparameter. The authors then propose a Stable Weight decay (SWD) method to solve the unstable weight decaying problem. They show that the SWD method is more stable than the L2 and the decoupling weight decay used in adaptive gradient algorithms. They also demonstrate that SWD can be used to improve the performance of Adam by incorporating weight decay into the training process. Finally, they show that Adam variants with different hyperparameters can be trained with SWD. "
SP:a3206dc71e32ba1830895bf442d3840f3331a532,This paper proposes a new method to improve the translation quality of neural machine translation (NMT) using Translation Memory (TM) for improving the performance of NMT. The proposed method is based on the observation that the encoder of a pre-trained language model (PLM) that is used to encode the semantic relationship between two sentences is more likely to encode information about the similarity score between the two sentences. The authors propose a sentence level retrieval approach to retrieve the information from this encoder for the purpose of improving the quality of the TM and NMT performance. They also propose an n-gram retrieval method that uses a sentence-level retrieval method. Experiments show that the proposed methods can improve the information flow between the TM encoder and the NMT decoder while maintaining the same translation quality.  
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper studies the connection between rate reduction and (shift) invariant classification in deep (convolutional) networks. The authors propose an iterative gradient ascent scheme for rate reduction of learned features in a deep network. They show that linear operators in multi-channel convolutions can be reduced to linear operators, and that the resulting architectures are invariant to back propagation training. They also show that a convolutional network in the spectral domain can be transformed into a discriminative deep representation by removing components of the network that are not relevant to the classification."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit acceleration of gradient flow for over-parameterized two-layer linear models. The authors prove a conservation law for implicit acceleration under the spectrum of the gradient flow. They also prove that the acceleration depends on the spectrum, which is a result of the matrix factorization problem and Riccati type differential equations. They show that the weights with small, balanced or spectral initialization have small, but non-negative, gradient flow, and that the corresponding Gramian matrices are non-degenerate."
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper proposes a new approach to explain the opaque model’s behavior in machine learning techniques. The approach is called explainable AI. The framework is based on the uniform sampling of user-defined subspaces, which is known as LIME. The authors propose a surrogate interpretable model called CLIME, which can be seen as an extension of the LIME framework. CLIME can be used to train an ML model with a standard ML model (e.g., LIME) and then explain the model to the user.    The main contribution of the paper is to propose a new OOD sampling problem that is motivated by the observation that LIME “explains” the opaque behavior of the model. The main idea is to use adversarial attacks to fool the model by perturbing it with a perturbation procedure that has logical constraints. The paper also proposes an estimation algorithm that can be applied to any OOD sample from the model, and the authors show that their approach is more robust to adversarial perturbations.  The authors also show that CLIME is able to be used for real-world problems where the model is not fully explainable, and that the fidelity of the explanation is lower than that of the original model."
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"Pre-trained language models such as BERT and AMBERT have been shown to be effective for natural language understanding (NLU) in languages such as Chinese, English, and French. Multi-word expressions are used to represent natural lexical units, and the authors show that fine-grained and coarse-rigorous tokenizations can be used to improve the performance of the pre-trained language models.   Multi-granular BERT (AMBERT) is a well-known pre-trained BERT model that uses fine-rigid tokenization. AMBERt is trained on English and French, and is a pre-training language model.  The authors use three benchmark datasets (CLUE, GLUE, SQuAD, and RACE) to evaluate the performance on different languages. They show that the performance improves when the encoder and encoder are fine-tuned, and when the contextualized representations of the words are fine tuned, and that the inference time is reduced. They also show that AMBERTs are able to outperform models trained on Chinese and English. "
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"Semantic parsing is an important task for learning machine-understandable information representation from natural language utterance. In this paper, the authors propose a PhraseTransformer architecture that learns a meaning representation based on phrase dependencies using the phrase dependencies. The Self-Attention mechanism in the Transformer is replaced by a Long Short-Term Memory (LSTM) to capture the local context of phrases, which is used to incorporate long-range word dependencies. Experiments on the Geo, MSParS datasets show that the proposed model outperforms the original Transformer for semantic parsing. The model is also able to capture local context awareness through the use of a Neural Network on the Atis dataset.    The paper is well-written and well-motivated. The idea of Neural Machine Translation is interesting, and the model is able to learn detailed meaning from a single sentence. "
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes a new training method called composite adversarial training (CAT) to improve the robustness of deep neural networks (DNNs) against combinations of multiple adversarial perturbations. The authors claim that individual perturbation models are not robust enough to achieve the same robustness as individual adversarial losses, so they propose a training method that combines multiple perturbational losses to achieve better robustness. They show that the proposed CAT outperforms other state-of-the-art adversarial robustness methods on a number of benchmark datasets. They also show that CAT can be applied to adversarial models that are more robust to different types of adversarial attacks, such as pixel-based and spatial-based."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"This paper proposes a new recurrent network called the Emergent Symbol Binding Network (ESBN), which is a recurrent network with an external memory for variable-bounding and a binding mechanism to learn symbol-like representations during the learning process. The authors show that the ESBN is able to learn abstract rules that can be used to guide the induction of abstract rules from high-dimensional sensory data. They also show that ESBN can learn rules that are invariant to changes in the input representation of the input, which is an important property of the symbol-processing machinery. Finally, the authors demonstrate that the proposed architecture outperforms competitive neural network architectures on a variety of tasks, and that the learned rules are more interpretable. Deep neural network algorithms have been shown to be interpretable in the past. However, this paper shows that this is not always the case when the input is high dimensional. The paper also shows that the learning of abstract rule induction is not tractable when high-dimensionality is used.    The paper is well-written and well-motivated, and the authors have done a good job of providing a theoretical analysis of the inductive bias of existing symbol-processing mechanisms.  However, the paper is lacking in novelty."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a new framework for structured prediction language tasks such as joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The authors propose a new translation task based on augmented natural languages and use it as a translation task. They also propose task-specific discriminative classifiers to solve the problem. The proposed approach is evaluated on a variety of tasks and shows superior performance over the task -specific models on most of the tasks. The paper also shows that the proposed architecture can be applied to other tasks with different hyperparameters.   The authors also show that their approach outperforms the state-of-the-art models on the tasks of joint entity (JE), relation classification (RCT), FewRel, and TACRED."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper studies the unlabeled entity recognition (NER) problem. The authors propose a novel approach to address the issue of misguidance in NER models, which they call “negative instances”. Specifically, the authors propose to use a pre-trained NER model to sample negative instances from the training data, and then use the negative instances to improve the performance of the model. They show that the proposed approach outperforms prior work on both synthetic datasets and two real-world datasets. "
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper introduces Acoustic Neighbor Embeddings, an acoustic word embedding (i.e., Acoustic Neighbour Embedding) that is based on stochastic neighbor embedding for sequential inputs.    The authors propose to embed the input sequence into a vector space of fixed, reduced dimensions, and use an encoder neural network to map the input to the embedding space.  The acoustic encoder and text encoder are used to encode speech signals into frame-wise subword posterior probabilities, which are then used to train an acoustic model to predict the subword transcriptions of the input.  In order to achieve phonetic confusability, the authors use the Euclidean distance between the embeddings of the subwords and their corresponding embedding vectors. The authors show that the triplet loss criterion is more robust than the proposed method for neural network training in terms of gradients, and that it can be applied to any encoder networks for word (name) recognition task, as long-range embedding can be used.  Experiments are performed on the isolated name recognition task using Euclideans, where the encoder network is trained on the test data of an approximate phonetic matching task, and the text network is used to learn a text-to-speech embedding of a given word.  Experimental results show that this method is able to improve the recognition accuracy of finite state transducer(FST)-based decoding on test data, and it is also able to learn low-dimensional embedding to achieve better performance.  Empirical results are also presented for Euclideal nearest-neighbor search on the problem of isolated name retrieval from a given input, using Euclidesan nearest-nodes. "
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper proposes a new reinforcement learning algorithm for stationary mean-field games, where the optimum is a stationary point of the game. The authors show that the mean- field state and the policy converge to a Nash equilibrium when the number of agents is large enough. They then propose a fictitious play algorithm based on gradient-descent and proximal policy optimization to optimize the mean - field state, policy, and policy. The algorithm is then applied to the single-agent reinforcement learning problem, and is shown to converge to the Nash equilibrium."
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a normalizing flow model for probabilistic inference with a joint distribution over the joint distribution generated by a pair of flow models for a given task. The authors propose a framework for approximate probabilistically inference, where the distribution is learned by a flow model, and the model is conditioned on the distribution. The method learns a generative model by conditioning the distribution on a set of data points, and then uses variational inference to learn a conditional distribution over this conditional distribution. This conditioning is based on arbitrary differentiable transformations, and it can be used for likelihood evaluation, inversion, and sampling. The proposed method is evaluated on a number of inference tasks for inverse problems, where it is shown to outperform MCMC baselines in terms of likelihood evaluation and inversion. "
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper presents a new ultra-high resolution image segmentation dataset, U-RISC, for cell membrane segmentation. Cell membrane is segmented using an annotated Electron Microscopy (EM) dataset, which is a variant of the recently proposed U-RCS dataset. The paper also introduces a new evaluation criterion, the Perceptual Hausdorff Distance (PHD), which is used to compare the performance of different segmentation methods and iterative manual annotation with uncompressed high-resolution raw data. Experiments show that the proposed segmentation evaluation criteria outperforms human perception and is competitive with existing evaluation criteria and PHD."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper studies the problem of catastrophic forgetting in the context of Continual Learning (CL) and proposes two benchmarks for CL algorithms. The benchmarks are designed to address the issue of forgetting in short streams of tasks, where a CL system is trained on a small number of tasks. The authors propose a modular architecture that consists of modules for atomic skills, and a learning algorithm that uses a task-driven prior in the exponential search space to guide the learning. The learning algorithm is evaluated on a number of standard CL benchmarks, and is shown to outperform the state-of-the-art on each task. Benchmark results show that the proposed modular architecture and learning algorithm outperforms the state of the art on all benchmarks."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,This paper proposes a meta-learning approach for few-shot learning of synthetic meta-tasks. The proposed approach is based on a generative model that is trained on a set of synthetic tasks. The authors show that the proposed approach outperforms a number of baselines on several benchmark datasets.  
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies the injectivity of generative priors for fully connected and convolutional ReLU layers and networks. Injectivity is an important property for generative models, and it is particularly important for inference, especially for inverse problems and compressed sensing. In this paper, the authors show that injectivity is a property that is related to the expansivity of the weight matrices, and that it depends on the well posedness of the weights. The global injectivity depends on expansivity, i.e., the number of iid Gaussian matrices. The authors also show that the worst-case Lipschitz constants of the stability of the injective ReLU network can be derived from the weights of an injective network.   The authors provide a layerwise analysis of injectivity, and show that a tractable model can be learned that is well-posed. They also provide arguments to explain why injectivity exists in deep networks. The arguments are based on the differential topology, and the authors also provide an argument to explain injectivity in the case of random projections.  The paper concludes with a discussion of nonlinear inverse and inference problems that can be solved by neural networks. "
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes a continuous conditional generative adversarial network (CcGAN) which is a generative model for image generation. CcGAN is a generalization of conditional GANs (cGANs) to categorical conditions (i.e., class labels) that are continuous, scalar, and continuous, i.e. categorical. The authors propose a method to learn a discriminator that is able to discriminate between continuous and categorical labels. The generator/discriminator consists of a hidden map and a one-hot encoded label, which are two different label input methods: the hidden map is used to train the generator and the label is used for training the discriminator. The empirical cGAN losses (e.g., empirical generator loss, empirical discriminator loss) are used in the continuous scenario, while the empirical losses in the categorical scenario are used for the continuous case. The proposed method can be applied to any continuous, continuous, categorical, or categorical continuous GAN.  The generator and discriminator are trained in an unsupervised way. The discriminator is trained with regression labels from the regression labels in the generator.   The authors show that the proposed method learns the generator by minimizing the difference between the true and the regression label of the image distribution. They also provide error bounds for their discriminator trained with HVDL and SVDL.  Experiments are conducted on a benchmark dataset for generative image modeling, called RC-49, which is an extension of the standard benchmark dataset Circular 2-D Gaussians. The results show that CdGAN outperforms cGAN in terms of accuracy. "
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper studies the sample complexity of deep learning (DL) under the setting of Semisupervised learning (SSL) with unlabeled instances for a given task. Active learning (AL) has been shown to have a similar sample complexity to SSL, but the difference between SSL and AL is that SSL is used for fully-supervised learning with labeled samples, while AL is used to train a classification network. The authors show that DL-based AL algorithms trained with SSL improve the annotation efficiency and diversity of the training data, and that the rate of convergence of AL algorithms is faster than SSL. They also show that the convergence rate of the classification network trained with the AL algorithm converges faster than that of the SSL algorithm.    The authors propose a new algorithm, called pool-based SSL (CRC), which is based on the idea of convergence rate control (CCR). The authors also propose a variant of the AL and SSL (ASSL) algorithms. The main contribution of the paper is that the deep neural network trained using the proposed CRC and SSL algorithm is able to converge faster than SL, and the authors also provide a theoretical analysis of the convergence of the algorithm. Finally, the authors demonstrate that their method can be applied to ASSL, and show that their algorithm can be used to improve the performance of AL, SSL, and AL + SSL. "
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes a federated learning method for distributively training neural network models across devices for parallelizing gradient computation. The authors consider the Federated Learning problem where each device has multiple clients and each client has its own local-device level empirical loss, and the goal is to minimize the global empirical loss across all devices.   The authors propose a scheme for training that minimizes the inexact minimization of the global loss, which is a dynamic regularizer that is sensitive to device level computations. This scheme is applicable to convex and non-convex settings, and can be applied to both devices with partial participation and unbalanced data. Experiments on both real and synthetic data show that the proposed scheme is able to achieve state-of-the-art performance in both cases. "
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,This paper proposes a self-supervised contrastive learning approach to learn representations that are more dissimilar to their supervised counterparts for a variety of computer vision tasks. The key idea is to use the contrastive loss in contrastive contrastive training to improve the performance of the learned representations. The authors show that the proposed approach is able to achieve better performance on ImageNet linear classification and image classification tasks compared to supervised methods. 
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies the global convergence and global optimality of actor-critic, one of the most commonly used reinforcement learning algorithms. In the single-timescale setting, the authors consider bi-level or two- timescale updates to the policy gradient direction of the actor and the critic, respectively. They show that under certain function approximation settings, linear or deep neural networks can be used to train an actor that converges to a globally optimal policy in a sublinear O(K−1/2) rate. They also show that the sublinear rate can be improved to O(1/\sqrt{T}^T) if the critic update is based on the Bellman evaluation operator. Finally, they show that in the linear function approximation case, the rate of convergence of single-timely actor - critic is O(O(T)^T, where T is the number of timescales.   "
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes to use Log files in computer systems to represent events, messages, and transactions. Log files are a common resource for structured textual and numerical data, and they can be used to represent both structured and non-structured data. Logs can be seen as sequential forms of data, such as natural languages and temporal signals, and Log files can represent both events and messages. Transformer Networks (TNs) are used to encode both numerical and textual information into log embeddings, and the authors show that they are able to capture both the field level, log level, and log sequence level. The representations are encoded in a vector format, and a representation for each level is learned. The authors also show that this representation can be applied to various log processing applications. "
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper proposes a wavelet decomposition-based architecture for deep convolutional neural networks (CNNs). The approach is based on wavelet packet decompositions with freely-trained mixture weights, which is a formalism that allows to decompose CNN kernels and oriented Gabor filters into CNN kernels. The authors propose a variant of the AlexNet architecture for image classification based on this approach. The two main feature extraction properties of the two are the directional selectivity and shift invariance, and the authors show that both of them are satisfied. The paper also proposes a variant based on a separable wavelet transfer, which can be applied to any existing variant. The accuracy rate of the proposed AlexNet is shown to be competitive with the state-of-the-art on several datasets, and it is also shown that the proposed variant of AlexNet can achieve a higher accuracy rate than the state of the art in terms of accuracy rate.    The paper is well-written and well-motivated. The mathematical theory behind the proposed network is clear and the experimental results are convincing.  However, there are a few concerns that need to be addressed."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper studies the problem of training real-world multi-agent teams. Coordinating teams with dynamic composition is an important problem. The authors propose a coach-player framework where the optimal team strategy is learned in an adaptive communication method. The learning is based on a variational objective, where the goal is to maximize the mutual information between the agent and the coach. The proposed method is evaluated on resource collection tasks in a multiagent particle environment, and the authors show that the proposed methods achieve zero-shot generalization to new team compositions with heterogeneous agents using an attention mechanism for dynamic team composition. They also demonstrate that the adaptive communication strategy is more effective than using a single coach for training dynamic teams.  "
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper proposes a post-hoc method to estimate influence functions for machine learning interpretability and uncertainty estimation. Influence functions are commonly used in deep learning with non-convex loss functions, but the accuracy of influence functions has not been studied before. This paper considers the problem of estimating group influences, i.e., the influence of a group on test-time predictions. The authors propose to use gradients and the Hessian of the underlying model to estimate the influence functions of a model.   The authors show that influence functions are useful for linear models, but not for deep learning. They also show that for deep networks, influence functions can be used to improve the performance of neural network models on standard datasets such as Iris, MNIST, CIFAR-10, and ImageNet.  The paper also shows that influence estimates of deep learning can be improved by using model parameterization and regularization techniques. The paper shows that the influence estimates are more sensitive to the network architecture and the number of layers, and that deep networks are more robust to changes in network architectures.  Finally, the paper shows how to improve influence estimates for shallow networks by using weight-decay regularization. In addition, the authors show how to use influence estimation methods to improve performance in non-consvex setups, and how to ensure that the convexity of a loss function is preserved. "
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"Autoregressive language models trained on large text corpora are shown to perform well on next word prediction and text classification tasks with zero-shot usage. The authors show that language models with high crossentropy (log-perturbity) are able to learn features that are useful for classification tasks of interest, and that language modeling, a pretraining task, can be used as an objective function to improve the performance of classification tasks on sentence completion tasks. "
SP:a67da438e9821010284416170c3699ae7ff96c99,"Membership inference attacks (MIA) are attacks where a neural network model is trained to infer the membership of a group of training samples. Membership inference attacks are an important problem in machine learning, and there are several MIA approaches to attack classification models. This paper proposes a new approach for membership attacks based on the reconstruction error of the training set. The authors show that this approach can be applied to a wide range of conditional image generation models (e.g., image translation). Reconstruction error is used as a measure of membership error, and the difficulty score is used to compare membership error to the membership error in terms of MIA accuracy. The paper also shows that reconstruction errors can be used as an indicator of overfitting, which is a common problem in MIA."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes a differentiable architecture search method to solve the distribution learning problem in differentiable NAS. Specifically, it proposes a new approach to solving the distribution of random variables in a continuously relaxed architecture mixing weight, where the random variables are drawn from a Dirichlet distribution. The authors propose a gradient-based optimizer to optimize the Dirichlett parameters of the pathwise derivatives of the weights, which is a well-studied problem in neural architecture search algorithms. The formulation is shown to preserve stochasticity and improve the generalization ability of the proposed method. The proposed progressive learning scheme is also shown to improve the performance of searching on large-scale tasks. Experiments are conducted on CIFAR-10 and ImageNet, and the authors show that the proposed algorithm achieves the best test error on NASBench-201 and NAS-201 datasets."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper studies the use of neural networks as function approximators for low-dimensional-but-complex functions. The authors consider the problem of training deep networks with high dimensional inputs, where the underlying differential equations are non-convex and non-linear, and the input is a set of images with pixel coordinates.   The authors propose to use signed distance functions and neural radiance fields as input, and show that these two elements can be used to learn positional encodings that are more robust to sinusoidal nonlinearities and Fourier features compared to ReLU networks. They also show that the compositional depth of the learned representations of the two networks is a function of the number of layers and compositional depths of the representation.  The main contribution of the paper is that the authors show that two types of functions can be approximated by neural networks that are trained with neural networks, and that the learned representation is robust to these functions.  In addition, the authors demonstrate that these functions are more computationally efficient than existing functions approximated with functions based on Fourier or Gabor basis functions, and can be applied to a wide range of problems such as multiplicative filter networks, which can be seen as a generalization of existing work.  Finally, they show that their approaches can be combined with existing approaches based on the Fourier feature and sinusoidal nonlinearity, Fourier networks, sinusolic activation networks, etc., and compare their performance to the state-of-the-art approaches using ReLU and ReLu networks."
SP:f5be855300f63c185a006834302bd4b033b56258,"Gradient-based meta-learning involves the use of task-specific models and a meta-model that uses gradients from all the task specific models in the loop. In this paper, the authors propose an algorithm that uses the meta-gradients from the inner loop to train a meta - model. The algorithm is based on the teacherstudent scheme, where a student network is used to learn the gradient-based gradient of the inner-loop optimization steps. The authors show that this approach can be applied to a wide range of tasks, including few-shot learning, long-tailed classification, meta-attack, and more. They also show that meta- gradients can be computed on a lightweight computation graph, and that it can be used to improve the performance of meta-learners on a variety of tasks. "
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"Offline Reinforcement Learning (RL) is an important problem in RL, where policies are learned offline, and the goal is to learn policies that are robust to out-of-distribution (less explored) actions. Offline RL is a challenging problem, and many off-policy RL algorithms have been proposed to address this problem. Behavior regularization has been proposed as a way to improve the performance of existing offline RL algorithms. In this paper, the authors propose behavior regularized offline reinforcement learning (BRAC+) that penalizes the behavior of an offline Reinforcement learning agent if it takes out of distribution (out of distribution) actions during training. The authors propose a novel analytical upper bound for the KL divergence between the policy and the behavior regularizor, and propose a new regularization term based on state-dependent Lagrange multipliers for the distribution of the distribution over the sampled batch.    The authors show that the distributing KL divergence penalty can be computed using the standard distribution over all sampled samples, and that the distribution can be estimated using the analytical upper bounds.  They also propose a regularization for the policy evaluation objective based on the gradient penalty term for the gradient of the Q value in the case of out- of distribution actions, which is based on sample based estimations.  The main contribution of the paper is that the authors show how to compute the freedom of deviation between the Q values in the out ofdistribution actions and in the in-distributions using Lagrange multiplier.  BRAC+ is evaluated on several offline RL benchmarks, and compared with model-free and model-based approaches, and is shown to outperform BRAC+. The authors also show that BRAC can be used to mitigate the catastrophic performance degradation due to the catastrophic behavior regularization, which can be alleviated by adding a gradient penalty to the policy to prevent the Q-value from being negatively affected by rare out-out-distributed actions."
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"This paper studies the problem of compressing deep neural networks. Adjoined networks are the most commonly used architecture for compressing neural networks, but they are expensive to train. Smaller networks are more suitable for edge-devices, and the one-shot learning paradigm is used to compress the networks. The authors show that the regularization behavior of the adjoint training paradigm can be explained by a simple CNN-based neural architecture. They also show that a resnet-50 trained on Imagenet can be reduced to the same architecture on CIFAR-100. The paper also shows that a network trained on the same datasets can achieve similar top-1 accuracy as a previous network, but with a much smaller number of parameters.    The authors also provide a theoretical analysis of the performance of different architectures and the inference time. The main finding is that adjoint networks can be much more efficient in terms of inference time and accuracy. "
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper studies the problem of exploration in reinforcement learning (RL) and proposes a new exploration algorithm called -greedy, which is a generalization of existing exploration methods (e.g. -greeder) that are more general than their counterparts. The main idea of the paper is to use duration distributions for exploration, which are based on ecological models of animal foraging behaviour, to reduce the complexity and improve the generality of the problem. The authors show that dithering (i.e. temporal persistence) and greedy exploration are not sufficient to achieve local optima, and propose to use random duration.   "
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper studies the implicit regularization of gradient descent with Matrix factorization with respect to the nuclear norm. In particular, the authors show that gradient flow with infinitesimal initialization is equivalent to a heuristic rank minimization algorithm called Greedy Low-Rank Learning, which is an extension of the standard norm minimization. The authors also show that the gradient flow converges to a depth-2 matrix factorization, and that this gradient flow can be viewed as a variant of a gradient flow that is more efficient than the standard gradient flow in the sense that it does not rely on the nuclear regularization.    The main contribution of this paper is that the authors provide a new view of the rank maximization view from the perspective of implicit regularized gradient descent. They show that this view is a generalization of the previous view that norm minimisation is a special case of the generalization view, and they show that under certain assumptions on the initialization magnitude, the convergence of gradient flow is guaranteed. "
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper proposes a two-stage framework to improve robustness of classifiers in machine learning. The first step is to patch the classifier using data augmentations to subgroup features. The second stage is to train the models to be robust to a spurious bandage.    The paper studies the problem of skin cancer classification, where the subgroup differences between two groups are known. The authors propose a theoretically-motivated subgroup consistency regularizer and a robust objective. CycleGAN is used to learn intra-class, inter-subgroup augmentations. Model patching is performed on top of CAMEL. CAMEL is shown to improve the robust error of the model for model patching on several benchmark datasets. The paper also presents a real-world skin cancer dataset and shows that CAMEL improves robust error over a baseline that does not patch the subgroups. The main contribution of the paper is that the authors show that the model is robust to spurious bandages and that the class information is preserved in the patching process.  The authors also show that semantic transformations can be used to remove spurious features. "
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"Rule-based models (e.g., decision trees, ensembles, etc.) have been shown to perform well on large data sets. However, there is a trade-off between transparent inner structures and model expressivity in terms of model interpretability. Ensemble methods and fuzzy/soft rules can be seen as a tradeoff between interpretable nonfuzzy rules for the data representation. In this paper, the authors propose a new classifier called Rulebased Representation Learner (RRL), which is a non-differentiable RRL. RRL learns a discrete model by gradient descent, and it can be applied to any continuous space. In particular, RRL uses logical activation functions to learn the discrete model, and a new training method called Gradient Grafting is proposed. The authors show that RRL can achieve better scalability than decision trees in the sense that it is able to learn continuous features, and RRL outperforms existing approaches on both small and 4 large datasets. "
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a novel regret minimization (RGM) algorithm for learning in structured environments, where the goal is to learn models that can be applied to a wide range of biochemical applications such as molecular property prediction and protein homology prediction. The authors propose to use natural environments with complex descriptors (molecular scaffolds and protein families) to learn the tasks, and then apply RGM to learn a representation of these environments that maximizes the predictive regret of the learned predictor.  The authors show that under a simultaneous optimality condition, the proposed RGM can achieve a regret of $O(1/\sqrt{T})$ under the held-out environments with hindsight access. They also show that RGM is invariant to invariant risk minimization and can be extended to more complex environments with specialized domain perturbations. The proposed method is evaluated on a variety of applications such that the proposed method outperforms the baselines in terms of RGM regret, and achieves state-of-the-art performance on two applications (Molecular property prediction, and proteinhomology and stability prediction)."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes to use cross-modal attentions to train text-vision BERT models based on BERT for NLP tasks. It uses both text and vision probes to learn crossmodal attention. The authors show that cross-imagenet attention can reduce the computation cost for language-vision tasks (e.g., text-image retrieval, text-video retrieval, and text-text classification). They also show that the cross-mutual attentions can be used for textvision retrieval for large-scale search. The proposed architecture is called cross-probe BERT, and the method is simple and effective."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new Actor-Critic algorithms based on the Actor called FORK, which is a forward-looking Actor. The proposed model-free ActorCritic algorithm, called BipedalWalkerHardcore, incorporates FORK into the core of its model. Experiments on the Box2D and MuJoCo environments with continuous state and action spaces show that the proposed algorithms outperform the baselines. The authors also show that for the case where the GPU is not available, the proposed FORK can be trained on the GPU and can be used to improve the performance of the model. "
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper studies the problem of federated learning for non-i.i.d. data, where the goal is to learn a global model from a set of local models. The authors propose a new aggregation algorithm based on the Bayesian inference perspective, called FEDBE, which aggregates the local models of the global model and aggregates them via a Bayesian model Ensemble. They show that the model distribution is approximated by a Gaussian or Dirichlet distribution, and that the aggregation method can be used to regularize the training of neural networks. In particular, they show that by regularizing users’ model training with FED BE, they are able to improve the performance of the federated learners. They also show that FEDbe can be applied to regularizing the model training of the local learners.   The authors also provide a theoretical analysis of the proposed aggregation method and show that it is a generalization of the standard federated training algorithm. "
SP:3ac5f437fc349a33810d0645664d1c448528af74,"This paper studies the problem of how to improve the generalization performance of deep neural networks. The authors propose a new method to improve generalization ability of deep learning models. The paper proposes to add a regularization term to the loss function to encourage the model to learn to generalize better. Experiments are conducted on CIFAR-10, Cifar-100, and ImageNet."
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper studies the settling time of deep neural networks with a priori upper bound on the number of training epochs. The authors derive a Lyapunov based analysis for the loss function under input perturbations, and show that the prior a finite time convergence analysis converges to a stationary point. They also provide a theoretical analysis of the priori guarantees of finite-time convergence in the deterministic control theoretic setting.    The authors extend the control theory framework for deep learning to the setting of deep learning in which the tracking problem for learning is solved as a control problem. In this setting, a supervised learning framework is introduced, and the authors provide an analytical formula for computing the a prior-time upper bound in terms of its settling time. They show that under certain assumptions, the network will converge to the stationary point in a finite number of epochs if the control inputs are sufficiently diverse. "
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"Disentanglement of representations is an important problem in machine learning. This paper proposes a novel method based on nonlinear independent component analysis theory to learn representations that are disentangled. The authors propose to use incompressible-flow networks (GIN) to learn the latent variables and the auxiliary variable, and use GIN to learn a compact and disentangling representation. The proposed method uses GIN for informative latent variables selection based on mutual information between the latent variable and auxiliary variable. Experiments on synthetic data show the effectiveness of the proposed method on several downstream tasks including classification, outlier detection, adversarial attack defence on synthetic and real data."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"Pooling in convolutional neural networks is a lossy process, where the pooling operations on feature maps (i.e., the feature maps that are shared across different receptive fields) can be seen as a downsampling. This paper proposes two variants of LiftPool, namely LiftDownPool and LiftUpPool, which are bidirectional pooling layers. LiftPool uses the Lifting Scheme for the signal processing, which is a well-known technique to reduce the size of the receptive fields. The authors show that LiftPool can be used to downsize two types of Bidirectional Pooling layers (LiftUpPool and LiftingDownPool) and that they are both robust to input variations. They also show that the downscaled sub-bands can also be downsized using LiftDownpool. Finally, the authors propose a new up-pooling layer LiftUppool, which downsamples the feature map of each sub-band to a single pooling function. The proposed methods are evaluated on image classification and semantic segmentation using two different backbones, and show improved robustness to input corruptions and perturbations. The paper also studies image-to-image translation challenges and shows that the proposed methods can be applied to a variety of datasets. "
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a new embedding method based on a stable noise-shaping quantization scheme. The authors consider a high-dimensional dataset where the input is a sequence of binary sequences, and the goal is to learn an embedding of each sequence.    The authors propose to use a fast linear transformation on the `1 norm of the Euclidean distances between the binary codes of a pair of sequences T and T, where T is a sparse Gaussian random matrix, and T is the Hamming distance between the two sequences.  The paper shows that this fast transformation can be used to compute the ` 1 norm of a sequence, and that the resulting embedding can be applied to any continuous valued Johnson-Lindenstrauss embedding and quantization error with polynomial decay. The paper also shows that the proposed method has a time complexity of O(1/\sqrt{T}) and space complexity O(T/T) for well-spread data, which is much faster than existing binary embedding methods.  In addition, the authors show that the embedding dimension of the proposed approach is polynomially differentiable, which means that it does not depend on the number of sequences, but rather on the input sequence. They also show that their method is able to achieve high accuracy on natural images, and they show that it can also be used on synthetic data. "
SP:f65e229bca3904095743e7a501b1083cc60f1e22,"This paper studies the plasticity rules of artificial neural nets (ANNs) in the context of learning tasks where the goal is to improve robustness and generalization. The authors propose a genetic setting where the training process of ANNs is based on a process called ""Synaptic Plasticity Rules"" (SPRs). In this setting, the authors use Gradient Descent (GD) to learn a set of plasticity rule parameters that are used to guide the learning of backpropagation of a classification network. They show that GD can learn rules that are robust to adversarial perturbations and generalize well to new data.    The authors also show that, in the genetic setting, GD can be used to learn rules on MNIST/Fashion MNIST using synthetic data, and that GD is able to learn such rules in a similar way to the way that learning rules are learned in a biological setting.  The paper also shows that, under certain assumptions on the rule parameters of GD, it is possible to train a classifier with a certain level of tolerance to the classifiers trained with different levels of tolerance.  In addition, the paper shows that GD also learns a perceptron algorithm based on GD as well as a multiplicative weights method.  Finally, it shows that the learning rules learned by GD are adaptive and that it can be learned in an evolutionary time. "
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new method for visual question generation (VQG) based on a graph-to-sequence model. VQG is an important problem in the context of human-like questions, where the goal is to generate questions that are referential and meaningful to humans. The authors propose a new learning paradigm to generate visual questions with answer-awareness and region-reference, where visual objects in an image are augmented with human annotations. The key idea is to use side information from the side information of the question and the image, as well as the image itself, to guide the learning of a graph to sequence model. Double Hints textual answers and visual regions of interests are used to guide visual questions, and a methodology is also proposed to extract visual hints from the visual hints and use them as part of a dynamic graph. Experiments on VQA2.0 and COCO-QA datasets show that the proposed model outperforms several baselines in this setting, and is able to capture the implicit topology of the visual questions. The paper also shows that the model can also be used to solve a one- to-many mapping issue, and that double hints can be used as additional information.  "
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper studies the double-descent phenomenon of learning algorithms such as linear regression and neural networks, where optimal regularization is used to mitigate the double descent phenomenon. The authors consider linear regression models with isotropic data distribution, where the sample size, model size, and the number of training epochs, as well as the size of the training set, affect the generalization performance. They show that the ‘optimally-tuned `2 regularization can prevent double descent in the case of linear regression, and that the same phenomenon holds for neural networks. They also show that algorithms with tuned regularization improve the test risk scalings."
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"This paper introduces spatial dependency networks (SDNs), a new type of generative modeling based on spatial regularities in images. The idea is to use a neural network for building image generators (decoders) and then apply it to variational autoencoders (VAEs). The feature maps in a deep neural net are modelled by a sequential gating-based mechanism to capture contextual information. The spatial dependency layers of the decoder are used for density estimation in a hierarchical VAE, while the baseline convolutional architectures are used to model density estimation. The authors show that the spatial dependency of the SDN decoder can be used for learning disentangled representations, and that the performance of SDN for large images is comparable to that of a standard VAE decoder in the vanilla VAE setting. They also show that neural architectures for this task can be trained with different spatial dependencies.    The main contribution of this paper is the introduction of the concept of spatial dependency in 2-D space, and the development of a new class of models. The paper also shows that spatial dependency is more powerful than the conventional convolutions in VAE settings. "
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper proposes EMaQ, a novel approach to offline RL that extends a prior approach, BCQ. Off-policy reinforcement learning (RL) is an important problem in which the goal is to learn policies from a dataset of interactions, but offline RL methods are expensive to train. The authors propose a heuristic design choice, where the algorithm uses a backup operator to ensure that the behavior policy does not change too much in the absence of a distribution support. The algorithm is based on BCQ, which is a prior work that uses a function approximator to estimate the optimal policy from the proposal distribution. The paper shows that EMAQ can achieve sub-optimality bounds on the complexity of offline RL problems in the offline RL setting, and outperforms BCQ on the D4RL benchmarks. EMaq also outperforms Soft Actor Critic (SAC) in the online RL setting. The main contribution of the paper is the generative model design for estimating behavior policies. "
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper studies the problem of fairness in batch selection in machine learning systems. Existing techniques for model fairness in data preprocessing, model training, and fine-tuning are limited due to the demographic disparity between training and test data. The authors propose a new fairness machine learning model, called FairBatch, which aims to address this issue. The basic idea is to use bilevel optimization to optimize a training algorithm, where the outer optimizer optimizes the inner problem, and the inner optimizer is used to optimize the training algorithm. The main contribution of the paper is to propose a batch selection algorithm based on optimization that incorporates fairness measures (e.g., equal opportunity, equalized odds, and demographic parity). The authors show that fair batch selection is possible in both synthetic and benchmark real data. They also show that minibatch sizes can be used to improve model fairness.  The authors also provide a PyTorch code for batch selection, which is used in the paper. They show that FairB batch can be applied to a variety of datasets, and that it can be combined with existing batch selection techniques such as faster convergence. The paper also shows that batch selection can be incorporated into model training and that the fairBatch can improve fairness by fine-tuning. "
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the robustness guarantees and generalization bounds of deep networks with Lipschitz constants. The authors consider two models, the deep equilibrium (DEQ) model and monotone DEQs, where the monotonicity parameter is fixed. They provide bounds for both of these models, and show that the bounds are tight for both models. In particular, they show that for monotonic DEQ models with multiscale convolutional structure, the bounds for the input-output mapping and the weight-outward mapping of the networks are tight. They also provide simple-yet-tight bounds for a simple form of input-outlet mapping and a simple yet tight one for a specific type of weight- output mapping. Finally, the authors show that these bounds can be used to derive PAC-Bayes generalization bound for infinitely-deep network.    The main contribution of this paper is that the authors provide bounds that are tight (in terms of the Lipsichitz constant) for monotonically monotonical DNNs, and that are robust to the exponential depth-dependent of comparable DNN bounds. "
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper considers two settings: imitation learning and goal-conditioned reinforcement learning, where the goal is to learn a probabilistic long-term dynamics and a desired value function. The authors propose a novel approach based on density estimation, and show that it alleviates the hindsight bias in stochastic domains, and that it is more robust to sparse rewards. They also show that the proposed approach can be applied to expert data.    The paper is well-written, well-motivated, and easy to follow. However, there are a few issues that need to be addressed, and the proposed solutions are not very novel.  I have a few questions about the details of the paper, and I am happy to increase my score if the authors provide more details. "
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"Multi-task learning is an important problem in machine learning. This paper proposes a new probabilistic inference framework for simultaneously learning multiple related tasks, called variational multi-tasks learning VMTL. Multi-task learning is a variational Bayesian inference problem, and the authors propose to use Gumbel-softmax priors to model the task relatedness. The prior is a mixture of variational posteriors, where the priors are learned by mixing weights. The authors show that the learned priors can be shared across tasks, and that the shared inductive bias can be used to learn representations and classifiers that are more robust to changes in the tasks. The paper also shows that the performance of V MTL with limited training data can be improved when the number of tasks is limited, and it is shown to perform well on several benchmark datasets."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper presents a systematic and unified benchmark for evaluating the model quality of vanilla Transformer models on Long-Range Arena, which is designed to evaluate model quality in long-context scenarios. The authors show that Transformers have quadratic self-attention complexity in the long-range domain, and that fast Transformers can be used to tackle this problem. They also show that long sequence lengths can be efficiently handled by fast Transformers. The paper also shows that models trained on the benchmark suite are able to generalize well to long sequences of text, natural, synthetic images, and mathematical expressions that share a similarity to the input sequence.   The authors also provide a comprehensive evaluation of various long range Transformer architectures (Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, Longformers) on the Long-range Arena, and show that all of them perform well on the standard benchmark suite. "
SP:e12e410c3335b76133ceda4c865b244fbbab8580,This paper proposes a multilingual code summarization model that can be applied to non-parallel data. Structure of source code (Structure) and Context are used to train machine learning models. Source code (Context) is the representation of a computer program. The authors show that language-agnostic features such as source code and features from AST can be used to improve the performance of the model. The paper also shows that monolingual code summation can be performed in a variety of programming languages. Structure and Context help representation learning on code.  
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper proposes a reinforcement learning approach for audio-visual navigation based on two elements: (1) waypoints that are recurrent aggregations of the audio observations and (2) an acoustic memory that is used to guide the agent motion. The sound source is sampled from a set of sights and sounds, and the navigation policy is conditioned on the acoustic memory. The key idea is to learn the geometry of an unmapped space from both audio and visual data. The proposed approach is evaluated on two real-world 3D scenes and Replica. The results show that the proposed model is able to learn a good trade-off between sounds, sights, and space.  "
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper studies the learning abilities of neural networks with different architectures and optimization methods compared to weight initializations. The authors first show that a small CNN can learn the update rules of a small convolutional networks. Then, the authors show that networks trained with the same architecture on the same task can learn networks that are asymptotically similar to lottery tickets. They also show that the lottery ticket hypothesis holds for a number of small perturbations (e.g., single sign change). Finally, they show that minimal networks can be trained with a different initialization parameters for gradient descent.   "
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"Semi-supervised learning (SSL) for unlabeled data is a popular technique for improving the performance of a model when the number of labeled data is limited. However, existing SSL approaches rely on a consistency regularization, which can be problematic when the labeled data amounts are limited. This paper proposes a new method called RankingMatch, a method that tries to improve the computational efficiency of the objective function, namely the BatchMean Triplet loss. The authors show that the accuracy of SVHN can be improved by using the proposed objective function. They also show that RankingMatch can improve the accuracy on CIFAR-10, CifAR-100, and Cifar-100. Finally, they show that rankingMatch outperforms the state-of-the-art on the SSL benchmarks on both accuracy and accuracy with respect to the original objective function (which is based on the consistency of the model).   The authors also provide an ablation study that shows that the proposed batchMeans triplet loss performs better than the original Triplet losses. "
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper studies the problem of few-shot meta-learning, where the goal is to learn a learning algorithm that performs well on a small number of tasks in a sequential learning setting. The authors propose a new formulation of the sequential learning problem, which they call Few-Shot Meta-Learning (FSL). They show that the sample complexity of meta-training and adaptation in the online setting is the same as that of empirical risk minimization methods in the offline setting. They also show that meta-learners with bi-level optimization can achieve better sample complexity and regret in the meta-learner setting.    The authors also provide a theoretical analysis of the problem and provide a few experiments to support their claims.  They first show that there exists a connection between meta-train and meta-adaptation, and show that if we use meta-trained data to optimize the meta -learning algorithms, we can achieve similar sample complexity or better regret as empirical risk maximization methods. Then, they show that bi-levels of optimization can be applied to the meta learning algorithms, and that the performance is similar to that of metalearning. Finally, they provide some experimental results on variable-shot settings in sequential learning, including many-shot learning, zero-shot classification, and many-task learning. They find that meta learning outperforms supervised methods in terms of cumulative performance on all sequential learning problems, and they also provide some experiments on learning systems where meta learning is used to train learning systems. "
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper investigates the representation invariance of Transformer-based language models to syntactic perturbations. The authors show that the representations learned by Transformer models are invariant to the syntactic distance between the input sentence and its syntactic context. They also show that this invariance is not only observed in the context of sentence-level syntax, but also in the global phrase-level structure of the sentence. The paper also shows that the invariance to syntactical distance is also observed for the local phrase structure.  "
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper proposes a new way to train Generative Adversarial Networks (GAN) on large-scale GPU-clusters with high-fidelity images. Specifically, the authors propose a few-shot image synthesis task with minimum computing cost to train a GAN on a single GPU. The light-weight GAN structure is evaluated on a 1024 × 1024 resolution on the Nvidia RTX-2080 GPU, and the authors show that the proposed skip-layer channel-wise excitation module and self-supervised discriminator can be trained with a single feature-encoder. The proposed model outperforms the previous state-of-the-art GAN, StyleGAN2, on two datasets with different image domains and different computing budget. "
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper studies the problem of neural network bounding for neural network verification systems. The authors propose specialised dual solvers for learning neural network bounds that are computationally efficient. They show that they can be obtained by learning a relaxation of a linear program, i.e., a linear relaxation for piecewise linear activations. The relaxation is defined in the dual space, and the authors propose a dual algorithm for learning this relaxation. They also propose a method for learning such a relaxation in the case that the dual variables are non-linear.  The authors show that the proposed method can be applied to any relaxation in dual space (e.g., tightness, linear separation oracle). They also show that it can be combined with existing dual approaches to learn weaker relaxations.   The main contribution of the paper is that the authors introduce a customised solver that can be used in combination with off-the-shelf solvers to achieve faster running time. This is achieved through the use of massive parallelism and a GPU implementation. The paper also shows that the bounds obtained by the proposed bounds outperform the off-sheeter solvers in terms of speed-accuracy trade-offs, and that the computational budget can be significantly reduced.  Finally, the paper shows that formal verification speed-ups can be achieved when the number of clients is small and the dual approaches are used to train the weaker relaxation. "
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"Pre-trained language models (PTLM) have been shown to perform well on many tasks, but they are not well suited for tasks that require relational commonsense knowledge (e.g. everyday concepts). This paper proposes two pre-training objectives (masked token prediction and masked span infilling) for BERT-style PTLMs, which are designed to improve the performance of existing PTLM. The authors propose a method called concept-aware language model (CALM)1, which is a text-to-text transformer that learns a concept-centric language model. The key idea is that the generative and contrastive objectives are useful for common sense, and the authors propose to use them for intermediate self-supervised learning tasks to improve performance of PTLNs.  The authors also propose a new task-specific fine-tuning to further improve the generalization performance of the PTLm. The main contribution of the paper is the introduction of a new pretraining framework that uses the pre-trained framework to learn generative (and contrastive) objectives to improve common sense performance. The paper shows that the proposed method, called CALM, outperforms existing baseline methods on NLU and NLG tasks, and is able to learn concepts that are common across different tasks. "
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper tackles the problem of unsupervised physical object discovery, where the goal is to discover objects in a scene that share the same 3D geometry as the scene. The authors propose two new frameworks for learning 2D segments of a scene from video, which are then used to learn 2D segmentation and object interactions (e.g. object interactions between objects). The proposed model is evaluated on both synthetic and real scenes, and the results show that the model is able to learn both multi-scale pixel cues and physical motion cues. It is also shown that the learned object properties are able to capture physical events that are observable and partially occluded in the scene, which is an interesting finding in the context of developmental psychology."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes a new training method for improving DNN robustness to white-box adversarial attacks. The authors propose a new method called Increasing Margin Adversarial (IMA) Training, which is based on the observation that adversarial samples generated by existing attack algorithms tend to have large margins in the decision boundaries of convolutional neural networks.  The authors show that the IMA method increases the margins of a DNN model trained on clean data and then fine-tunes it on white noises. The method is shown to improve the accuracy of the method on both clean data as well as classification accuracy on noisy data. The proposed approach is also applied to robust DNN applications such as COVID-19 diagnosis on CT images.    The paper is well-written and well-motivated, and the authors provide a thorough analysis of their approach. The paper also provides a detailed analysis of the robustness of the proposed training method. The experiments are conducted on the COVID19 CT image dataset and show that their method is able to achieve state-of-the-art robustness on the clean data. They also demonstrate that their approach can be applied to a variety of life-critical applications where white noises are present, and demonstrate that the method is effective in the presence of 100-PGD white-Box adversarial attack. "
SP:276ffd59fbf49e3ee02756da8920218102214917,"Knowledge distillation is a popular technique for training compact models. Deep neural networks are typically trained with a large number of local optima, which limits the computation capacity. In this paper, the authors propose ProKT, a model-agnostic method where a teacher model is used to train a compact student network. The main idea is to use supervision signals from the teacher model in the student’s parameter space to guide the optimization to poor local optimas. To achieve this, the proposed ProKT uses an approximate mirror descent technique to learn local intermediate targets for the training objective. The authors show that ProKT outperforms existing knowledge distillation methods on both image and text datasets. They also show that the proposed method does not rely on any particular set of quirks in the optimization, and that the projection is based on a simple approximation of the local intermediate target. "
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a new channel pruning method for compression of Convolutional Neural Networks (CNNs). The authors propose to use a hyper-structure network as a proxy for the architecture of the main network, where the hypernet is trained with regular backpropagation. The authors also propose a regularization term to reduce the computational resource of the compact network using fewer FLOPs. The proposed method is evaluated on CIFAR-10 and ImageNet and compared with state-of-the-art methods. The main contribution of the paper is that the proposed method can be applied to any channel-pruning methods. In particular, it does not require any additional layer-wise scaling factors for the gradients, and it only requires a small number of FLOPS to compress a single layer. The paper also shows that the hyper-gradient descent can be used to prune the layers, and that they can be pruned in a single step.  "
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper tackles the problem of automated higher-order logic theorem proving. The authors propose a novel exploration mechanism to train a theorem prover. It uses imitation and reinforcement learning to guide the prover through the process of learning. The exploration mechanism is based on the observation that human proofs tend to be more likely to be proved in a deep reinforcement learning scenario, and the authors propose an exploration approach called DeepHOL Zero, which encourages the exploration of premises.  "
SP:88209417a8ad07e6103084e41709be900303ce5f,"Data augmentation is a popular topic in machine learning, and models trained with data augmentation have been shown to improve the performance of models on many machine learning tasks. However, existing augmentation methods can only be applied to a single data modality, which can be problematic in the presence of artificial data. This paper proposes MODALS, an automated data augmentation approach that augments any modality with a set of universal data transformation operations in the latent space. The idea is to use image processing functions and word-replacing rules to transform image data into text data. MODALS is able to perform universal data transformations across modalities and modalities. "
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper studies the nonlinear and nontrivial dynamical limit of learning dynamics, i.e., the mean field limit, for large-width neural networks. The authors provide global convergence guarantees for two-layer and three-layer neural networks, and extend this analysis to the case of two-layered networks. They provide a global convergence result for unregularized feedforward 3-layer networks in a mean field regime, and show that this analysis holds for multilayer ones as well. They also provide a rigorous framework to prove the global convergence limit for mean-field limit for stochastic gradient descent training for three-layering networks. Finally, they show that for neural networks with neural networks in the probability space of the neuronal embedding (i.e. neural networks that share the same probability space as the weights of the neural networks), optimization efficiency converges to the mean of the mean in a given time step. The global convergence guarantee is based on the global approximation property of neural networks under certain regularity and convergence mode assumptions, and is shown to be universal for any convexity. The universal approximation property is obtained through an algebraic topology argument that shows that neural networks are invariant to the regularity of the weights and the convergence mode."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper presents a method for learning counterfactual explanations of expert behavior in the presence of an unknown reward function. The authors propose to use interpretable parameterizations for real-world decision-making, which can be used for introspecting and auditing policies. In particular, the authors propose a method that uses counterfactually reasoning to learn costbenefit tradeoffs between the expert’s actions and the reward function, and then uses the counterfactuality of the expert behavior to guide the policy evaluation in the batch setting. They show that counterfactulational reasoning can be applied to the case where the reward functions for expert behavior are not known. They also show that learning explanations can be useful for policy evaluation when the expert policies are not available.  The authors evaluate the effectiveness of their method on a number of real and simulated medical environments, and show that their method outperforms the state-of-the-art in terms of policy evaluation performance and active experimentation in healthcare. "
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper proposes a new graph neural network architecture, called AMORPHEUS, for graph-based continuous control tasks. The proposed architecture is based on graph neural networks (GNNs), which are a generalization of Graph Neural Networks (GANs). The authors show that GNNs are able to learn a graph structure that allows them to generalize across different state and action space dimensions. They also show that they can be used to learn graphs that are more robust to changes in physical morphology (e.g. limb features) and generalize better than existing models based on Multitask Reinforcement Learning (MRL) in terms of generalisation, data efficiency, and robustness. The authors also propose a transformer-based approach, AMOR PHEUS (AMOR-EUS), to incorporate morphological information into the graph. The idea is to use edges in the nodes to encode information about the physical morphology of the graph, which is then used to guide the message passing. The paper shows that it outperforms existing GNN-based methods that do not use morphology information in the message-passing scheme, and that AMOR-US is more robust than existing methods. "
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper addresses the problem of visual counting, where the goal is to count objects in a natural image. The authors propose an alternative based on modulated convolutions, called MoVie, which uses a forward-pass to speed up inference. This module is applied to generic VQA models and can be applied to number’related questions in a number of existing generic and explicit, symbolic models.   The authors show that the proposed module can be used to solve the VQa challenge in a single forward pass, without the need for a residual bottleneck. They also demonstrate that the modulated conVolutional bottlenecks can be alleviated. MoVIE is evaluated on three benchmarks (common object counting, COCO, and counting) and shows superior performance compared to prior-art on all three benchmarks.  The paper also shows that the module can also be used for the counting-specific VQE tasks.  Overall, the paper is well-written and well-motivated, and the authors have done a good job of providing a comprehensive analysis of modulated CNNs.  I have a few questions:    1. Is it possible to use the proposed Modulated ConVolution (MoVie) module for reasoning tasks (e.g. counting)?   2. Is there a way to remove the residual bottleneck?   3. Can the module be used as an additional module to solve a specific type of question? "
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,"This paper proposes a new poisoning attack on a model. The poisoning attack is based on online convex optimization. Unlike previous model-targeted poisoning attacks, this attack has provable convergence to the target classifier. The authors show that this attack, called the attack, can be seen as an extension of the existing attack on the classifier, and that it has a better attack success rate compared to previous attacks. They also show that the proposed attack can be used as an online attack. "
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper proposes a new model binarization approach, BiPointNet, for deep learning on point clouds under resource constraint on edge devices. The authors propose to use binarized models for point clouds to address two challenges: (1) scale distortion in optimization, and (2) aggregation-induced feature homogenization. To achieve this, the authors use Entropy-Maximizing Aggregation (EMA) to learn a distribution that maximizes the maximum information entropy. Layer-wise Scale Recovery (LSR) is also incorporated to improve the feature representation capacity. Experiments are conducted on three different tasks, and the authors show that the proposed techniques achieve speedup and storage saving on real-world resource-constrained devices. In addition, the paper shows that the performance of biPointNet is comparable to other state-of-the-art methods and outperforms its full precision counterpart. "
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper proposes a new architecture for Transformer-based models for natural language processing tasks. The key idea is to use the self-attention architecture in a transformer to learn context-aware representations. The Transformer model is trained with a trainable memory, and the authors propose Memory-augmented neural networks (MANNs), which use neural architectures with a general-purpose memory to learn representations that can be used across different neural architectures. The authors show that MANNs outperform existing algorithms using backpropagation on three tasks: question answering, language modeling, and machine translation. They also show that the complexity of RNNs and LSTMs trained with MANNs is comparable to that of the Transformer baseline.    The paper also shows that the memory of a masked language model can be augmented with memory tokens that encode non-local representations, and that it is able to capture global context. The memory bottleneck is used to store global information, and a dedicated layer is used for each memory update. The model is tested on machine translation and language modelling tasks, and on the GLUE benchmark, where it is shown that the model can capture the global context of the input language. The paper shows that memory augmented Transformers are more robust to changes in the amount of information in the memory, as well as to the number of memory tokens. "
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes Prototype Contrastive Learning (PCL), an unsupervised representation learning method. PCL uses contrastive learning and clustering to learn semantic structures in the embedding space, and then uses these semantic structures to improve instance discrimination using low-level features. The maximum-likelihood estimation of the network parameters is performed using prototypes for each of the latent variables, using the Expectation-Maximization framework. The E-step is based on the distribution of prototypes, and the M-step uses the contrastive loss from the InfoNCE loss. Experiments show that PCL outperforms other instance-wise contrastive methods in low-resource transfer learning. "
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"This paper proposes a new orthogonal multi-path (OMP) block in a neural network, which is a block of paths that is orthogonality-invariant. The authors show that this block can be used to learn robust features that are invariant to invisible perturbations to clean images. They also show that the OMP block can improve the robustness of neural networks to adversarial attacks.    The authors propose to use the Orthogonal Multi-Path (OPM) block as a regularizer in the forward learning and backward correction of the neural network. They show that by using this block, the neural networks are able to learn features that can be robust to white-box and black-box PGD attacks. The paper also shows that the orthogons of the paths can be learned in a similar way as in orthogonic constraint.  The paper shows that by applying this block to vanilla neural networks, they can increase the variety and accuracy of VGG16 on CIFAR10, and show that neural networks trained with this OMP blocks can achieve better accuracy and robustness to white box and black box attacks. In addition, they show that for white box PGD attack, there is an l∞ bound on the number of times that the neural net can be attacked.  Finally, the paper shows how the proposed neural networks can be trained with the proposed block to improve the accuracy of white box attacks, and demonstrate that their neural networks have better robustness against white- box attacks than adversarial defenders. "
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a new graph attention mechanism for graph neural networks. The authors propose a graph attention model for noisy graphs. The key idea is to use a self-supervised task to learn attention forms for edges in the graph, and then apply expressive attention to the mislinked neighbors. SuperGAT is used to learn an expressive attention for the edges. The proposed graph attention is based on graph characteristics such as homophily, average degree, and other graph characteristics. The paper also proposes a recipe for improving the attention design based on these graph characteristics, and shows that the proposed models outperform the baselines on several real-world datasets. "
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a dialogue system for medical automatic diagnosis (DSMAD) based on the Dialogue system. The agent is trained using reinforcement learning methods. DSMAD is a Markov decisionmaking process where the agent is given a set of symptoms and is asked to decide whether to investigate the symptoms or not. The authors show that DSMAD agents can achieve better diagnostic accuracy and robustness to interventions. They also show that the agent can be used for any medical application.   The DSMAD agent is composed of two cooperative modules: an inquiry module for symptom-inquiries, and an introspective module that considers the medical rationality of the inquiring process. In the inquiry module, the agent decides whether or not to investigate whether a patient should be treated or not, and the agent uses this decision to guide the diagnosis of the patient.  The agent can also be used to guide other diagnosing processes. The paper also shows that the evaluation metrics of DSMAD methods are more robust to interventions than existing evaluation metrics, and that INS-DS outperforms other methods in terms of reliability, robustness, and accuracy. "
SP:10ae09d90d465125433a9b4f15b1405ab017920d,"This paper proposes an adaptive batch-wise regularization based on Batch Confusion Norm (BCN) to learn a natural world distribution with fine-grained and long-tailed properties. The authors argue that existing approaches do not account for the fact that the FGVC classifier trained on the same task may not be well-suited for the task at hand. They argue that the long-tail distribution of visual classification is responsible for the class imbalance problem, and that the attention mechanism is not well suited for the discriminative parts of the image features of fine details.   The authors propose to use BCN to learn the distribution of confusion strength between the tailed and head categories, which is based on inter-class similarity and intra-class variations. They also propose class-balancing strategies, classifier normalization, and negative gradient of tailed categories. The BCN term is designed to prevent overfitting to the long tailed category, and the cross-entropy loss is optimized using network learning.  The paper also proposes a confusion energy-based framework to tackle the problem of long-tailed scenario, where BCN is used as a regularizer for the distribution.  Experiments on FGVC datasets show that the BCN technique improves the performance of FGVC model trained with an extra attention mechanism. The approach is tested on iNaturalist2018, a popular natural world dataset, and is shown to be effective on a variety of tasks.  In addition, the authors also propose an adaptive confusion concept, which can be applied to other problems, and show that BCN and BCN can be combined to improve performance. "
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"This paper considers the problem of Bayesian inference for the inverse reinforcement learning problem of ill-posed nature, where the reward of an inner-loop MDP solver is unknown. The authors propose a new method called Approximate Variational Reward Imitation Learning (AVIL), which is a generalization of existing methods in the small tabular setting. The key idea is to use a variational approach to learn the latent reward of the policy, which is then used to approximate the approximate posterior distribution of the reward. The proposed method is shown to outperform existing methods for Bayesian reward inference on real medical data and control simulations. The paper also shows that non-Bayesian methods can be used to improve the performance of the proposed method.    The paper is well-written and well-motivated. The idea of the paper is interesting and the motivation of the work is clear. However, there are some issues in the paper, which I will discuss below.  1. The main contribution of this paper is that the paper proposes a novel method for learning the posterior of a policy in the absence of offline imitation learning algorithms. This is an important problem in healthcare.  2. It is not clear to me that this is a novel idea. "
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper proposes a new search procedure called Learned Belief Search (LBS) to learn policies in both single and multiagent environments. Prior search approaches are limited to partially observable environments due to the computational cost of computing the hidden information. LBS uses a supervised task to learn an approximate auto-regressive counterfactual belief, and then uses a public-private model architecture to train policies with rollouts. The paper shows that LBS outperforms prior work in Hanabi and Hanabi-like environments, and LBS can be applied to multi-agent settings as well. The main contribution of the paper is that the exact belief distribution is learned in LBS, and that it can be used to improve the performance of existing search methods. The empirical results show that Lbs outperforms the state-of-the-art on Hanabi in terms of exact search and compute requirements."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,Planning in large state spaces is an important problem. This paper proposes a new algorithm called Shoot Tree Search (STS). It aims to address the bias-variance trade-off between MCTS and random shooting. The algorithm is based on the observation that TD(n) is biased in the tree search context. The authors show that STS can be seen as a special case of TD(N) and show that it can be viewed as a generalization of the previous work (MCTS).
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper investigates the inductive bias of transformer networks for a variety of tasks that are commonly used in architecture engineering. The authors propose two datasets that are designed to measure inductive biases in transformer networks on a wide range of tasks, including deduction, induction, abduction, and reasoning primitives. They show that a model trained on these synthetic tasks is able to learn reasoning biases on all of these tasks.   Mathematical rEasoning is one of the most commonly used applications of inductively biased models. Inductive bias is a common problem in this field, but it is not well-studied. This paper proposes a new pre-training methodology called LIME, which is based on LIME. Models trained with LIME outperform vanilla transformers on large mathematical reasoning benchmarks. LIME is also able to reduce the computation cost of the downstream task in comparison to other recent state-of-the-art pretraining approaches. The main contribution of this paper is that the authors show that LIME can be used to reduce computation cost in order to improve the performance of a pre-trained model, and that the reasoning biases learned by LIME are more robust to changes in the number of tasks or in the amount of mathematical knowledge used. "
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper studies the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets trained with exponential or cross-entropy loss. The authors propose exponential weight normalization (EWN) and show that gradient descent with adaptive learning rate converges to a gradient flow path that has a radial direction. They show that the gradient flow of the networks trained with EWN converges in the same direction as that of gradient flow in the original gradient flow. They also provide an asymptotic convergence rate for learning prunable neural networks. Finally, the authors show that under certain assumptions, the gradients of EWN converge in the radial direction, which is a result of the exponential or Cross-Entropy loss, and they show that this is the case even if the gradient descent is trained with an adaptive learning rates.    The authors also show that for simple data sets and unnormalized architectures trained on synthetic data sets, the gradient flows of EWN and SGD converge in a similar direction to that of standard gradient flow, and the authors also provide a theoretical analysis that shows that gradient flow is more likely to converge along the gradient direction of the EWN in the case of weight normalisation (SWN) as well as unnormalised architectures. They further show that SGD is able to converge to sparse EWN solutions in the presence of a certain amount of relative sparsity in the training data.  Finally, they also provide some experimental results on learning pruned neural networks, showing that simple datasets and architectures trained with SGD are able to learn sparse solutions."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,"Generative Adversarial Networks (GANs) are one of the most popular generative models, but they suffer from the mode collapse problem. Catastrophic Forgetting is an important problem in continual learning, and this paper proposes a training procedure to train discriminators that are more robust to mode collapse in the generator. The authors show that their training scheme is able to prevent mode collapse and improve the classification accuracy of the discriminator. They also show that the training scheme improves the metrics for GAN evaluation. Finally, they show that GAN frameworks trained with their proposed training scheme are able to avoid mode collapse.  "
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes AUBER, a regularization method based on reinforcement learning that prunes attention heads in BERT by using a proxy score to measure head importance. The authors argue that it is more efficient than existing heuristic-based methods as it does not require any additional regularization. They also show that AUBer can be combined with existing heuristics and rule-based policies to produce a pruning policy that can prune attention heads efficiently. Experiments are conducted on several datasets and show that the pruning performance is comparable to existing pruning methods, but they are much more efficient. "
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"This paper proposes a new method for imitation learning in the context of robotics problems. In this setting, physics simulators are used, and the goal is to learn a correspondence between a representation, physics parameters, and morphology. The correspondences are learned using both unpaired and randomly collected data. The authors propose to use dynamics cycles to model dynamic robot behavior, and use a cycle-consistency constraint to ensure that the dynamics cycles are consistent.   The authors show that the correspondence between simulation and real robot can be learned in two problem domains: (1) simulation and (2) real robot. They show that imitation learning can be transferred from sim-to-real in both cases, and that fine-tuning can be done in both settings. They also show that their framework can be applied to uncalibrated monocular video from a real robot arm with dynamic state-action trajectories from a simulated arm, and transfer learning is possible in both robotics environments.  The main contribution of the paper is that the authors propose a framework to learn the dynamics of a robot from paired data, and then fine-tune the policy on the paired data. "
SP:006434d56992836ab9420d7d4215bc70664de304,"Explainability is an important topic in model development. Explainability in AI is important because of its ability to capture operational nuance and explain the model’s features. This paper proposes two solutions to improve Shapley explainability on the data manifold. The first is based on the Shapley framework, where the “off-manifold” Shapley values are defined to capture the implicit model dependence between the model and the underlying data. The other is a Shapley value-function, which captures the relationship between the data imputations and the model's features. The authors propose a solution based on generative modelling, and show that their solution is more robust to unintelligible explanations on higher-dimensional data. They also show that the on-manit manifold explainability is more sensitive to sensitive attributes.   "
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"Existing methods for opponent modelling are based on local observations from the opponent. This paper proposes a new modelling technique based on variational autoencoders that uses local observations (observed world state, chosen actions, and received rewards) to learn embeddings for embedding the agent’s decision policy using deep reinforcement learning. Experiments show that the proposed method outperforms a baseline method that does not use opponent”s information and learns from the embedding of the policy. The paper also shows that the learned embedding is robust to opponent observations."
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"This paper proposes Consistent Contrastive learning for unsupervised visual representation learning. Consistent contrast (CO2) is a contrastive learning framework that incorporates a consistency regularization term to semi-supervised learning with unlabeled data. The paper shows that CO2 improves the top-1 accuracy of Momentum Contrast (MoCo) on the ImageNet linear protocol. CO2 can be applied to image classification, object detection, and semantic segmentation using PASCAL VOC, and is shown to improve top-5 accuracy on the instance discrimination task. The authors also show that the consistency term can be used to improve the performance of the semantic classifier.   The paper also shows that the visual representations learned by CO2 are transferable to other tasks, and that the label assignment strategy is more effective than the one used in MoCo. "
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies the convergence rates of saddle-point optimization with Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-points. The authors make two assumptions: (1) uniqueness of the optimal solution and (2) the existence of an exponentially small learning rate. They show that under these assumptions, OGDA and OMWU converge linearly to an optimal solution with a constant learning rate, and that OGDA converges with last-iterate convergence in a constrained setting. They also show that in bilinear games with probability simplex, OMWu converges linearly in a convex-strongly-convex setting. Finally, the authors show that for strongly convex and strongly-concave functions, the learning rate of OGDA is a universal constant. "
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes a federated setup to train User Verification (UV) models in a secure manner. The proposed framework, Federated UserVerification (FedUV), is designed to provide private and secure training of UV models. The key idea of FedUV is to train a set of loss functions for each user, which are then shared across all the users. Each user embeddings are obtained by learning linear combinations of the embedding vectors of all the instances of the same user. Each instance embedding is obtained by a secret user-defined linear combination of all instances of a given user. The model is then trained in an end-to-end fashion. The paper shows that FedUV outperforms existing approaches on user verification on voice, face, and handwriting data."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"Deep neural network classifiers have been shown to be invariant to margin and geometry of class manifolds (CMs). However, the geometry of a model depends on the architecture of the model and the number of layers. This paper proposes a technique to learn boundaries between CMs. The technique is based on the observation that real neural networks tend to form random affine subspaces. The paper shows that this technique can be applied to CMs to improve generalization, robustness, and ensembling. The method is tested on a variety of settings, including architecture, random initialization, stage of training, class size, ensemble size, label randomization, training set size, and model robustness to data corruption. The results show that the CM dimension of a CM is invariant when the dataset is large enough to cover the entire CM dimension.   "
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper considers the problem of reinforcement learning (RL) in which the goal is to learn a policy that maximizes the entropy of the environment states. This problem is well-motivated by the observation that RL methods often fail to learn policies that are robust to action noise due to the lack of exploration and exploitation. To address this problem, the authors propose two policies, namely Soft Actor-Critic (SAC) and Curiosity-Aware entropy temperature (CAT-SAC).    The authors propose to use the entropy temperature of the policy as a measure of the difficulty of a policy to learn. The authors also propose a curiosity mechanism to measure the instance-level entropy temperature, which is used to train two policies.   They show that the proposed SAC (SAT-CAT) improves the sample efficiency by using the Curiosity- Aware entropy Temperature, which uses the state prediction error as a proxy for the entropy. They show empirically that the prediction error is lower when the entropy is higher, and higher when the temperature is lower. They also show that CAT-SAT improves sample efficiency on the MuJoCo benchmark, where they show that they can learn to explore in unfamiliar states. "
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta-reinforcement learning algorithm for out-of-distribution (OOD) tasks, where the goal is to learn policies that generalize well to unseen tasks. The authors propose two approaches: (1) model identification and (2) experience relabeling (MIER). The authors show that the proposed approach is able to generalize better than the baselines in both cases. "
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper studies the meta-learning techniques for the few-shot learning (FSL) problem with label noise in the context of FSL. The authors show that meta-overfit happens when the meta learner is sensitive to label noise, and that the gradient noise problem is the main cause of the meta -overfitting problem. To address this problem, the authors propose Eigen-Reptile (ER) that uses historical taskspecific parameters to mitigate gradient noise in meta-parameters. They also propose prior models based on Introspective Self-Paced Learning (ISPL) to mitigate the issue of sampling noise. Experiments are conducted on several tasks to show the effectiveness of the proposed methods. The proposed methods are compared with state-of-the-art methods with noisy labels on a number of tasks, and show that Eigen - Reptile and ISPL are able to avoid overfitting to noisy labels, and the gradient step is more efficient."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper proposes Adversarial batch normalization (Adversarial Batch Normalization, AdvBN) to improve the generalization performance of models trained with adversarial training under distributional shifts. The authors argue that the reason for this is that models trained on adversarially crafted distributions are more robust to small adversarial perturbations, and feature statistics are more similar to image pixels. They show that AdvBN improves generalization for semantic segmentation on Stylized-ImageNet, ImageNetInstagram, and ImageNet variants. They also show that the mean and variance of deep image features can be used as a metric to measure the robustness of a ResNet-50 model to classification scores.   The authors conduct experiments on a variety of datasets, including gong, bolete, fox squirrel, and hen-of-the-woodens. They use Ibizan hound, a hummingbird, a goldfinch, a sulphur butterfly, a house finch, goldfinches, a bulbul, and a brambling and a guillotine as examples, and show that their method outperforms other methods on all of them.  They also demonstrate that the ResNet50 model can be trained to produce classification scores that are robust to adversarial attacks.  The paper also shows that the Adversary Batch normalization module is effective at reducing the variance of feature statistics. "
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"This paper proposes a new proxy metric called Variance of Gradients (VoG) to detect outliers in the data distribution, which is an important problem in machine learning. The paper also proposes a human-in-the-loop auditing method to identify outliers that can be used to improve the performance of the model. The authors show that models trained with VoG are more robust to outliers than models trained without VoG. They also show that VoG scores are more sensitive to the number of outliers."
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"Deep generative modeling has been a hot topic in recent years, and this paper proposes a new technique called Discriminator Gradient f low (DGf low) to improve the generation quality of a model. The technique is based on the non-linear Fokker-Plank equation of the gradient flow, which is used to define the entropy-regularized f-divergences between the real and the generated data distributions. The authors propose a refinement approach to GANs and deep generative models such as VAEs, Normalizing Flows, and vector-valued critics. The main contribution of the paper is to propose a technique to improve generated samples from simulated samples from real-world data (e.g. images). The authors show that the proposed method, Discriminators Gradient F low (GDF low) is a generalization of the McKean-Vlasov process, and can be applied to a wide range of GAN variants.  The authors also show that existing methods such as DRS, DDLS, and wasteful sample rejection can be improved by using DGf low. Experiments are conducted on synthetic, image, and text datasets, and the authors demonstrate that the quality of generated samples generated by DGF low is comparable to or better than the quality produced by previous generative methods, e.g., the discriminator Optimal Transport (DOT) and the discretization of GGANs, and discretized discretizations of vector-valued critics, and Discrimator-discriminator (DDR)."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper proposes VECO, a multilingual Transformer-based model for cross-lingual understanding and generation tasks. The authors propose to combine the advantages of both encoder-only Transformer and encoder -decoder Transformer for understanding tasks and for generation tasks in order to achieve better performance on a variety of tasks and frameworks. The paper proposes two sub-modules in the Transformer block, which are based on innersequence and cross-sequence masked language modeling, to learn multilingual representations. The proposed model architectures and pre-training tasks are evaluated on the XTREME benchmark for a number of cross-linguistic understanding tasks (text classification, sequence labeling, question answering, sentence retrieval, etc.). The authors show that the proposed sub-modules are able to perform well on understanding and generating tasks without any inference, and can be used for other downstream cross-latent tasks as well.  The authors also show that VECE outperforms the state-of-the-art on the X-Lingual X-Text-X-Text (X-TXT) benchmark on several tasks. They also demonstrate that the sub- modules are also able to generalize well to other tasks. Finally, the authors compare the performance of the proposed model with the state of the art on a few other generation tasks, and show the performance gain on the downstream X-Txt-X task.   The main contribution of the paper is the introduction of VECo, which is an extension of Transformer to the task of multilingual understanding. The experiments are conducted on a subset of X-XT-X tasks, where the proposed by the authors. The results show that, in terms of performance, the proposed VECCOE outperform the other cross-lifelong models and other Transformer variants on the generation tasks on X-X, and that the performance gains are more pronounced on downstream X tasks. "
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper proposes a new intrinsic motivation for Reinforcement Learning (RL) based on a causal understanding of the physical world. The authors propose to use auditory event prediction as an intrinsic reward for RL exploration. They use K-means to predict auditory event clusters and use a neural network to predict the auditory events. They show that the prediction errors are correlated with the intrinsic rewards. They also propose a module that uses Atari games to train the model for audio-visual exploration using Habitat simulator and active learning on the ThreeDWorld (TDW) simulator. They demonstrate that the audio signals are more informative for intrinsic rewards than vision-based models for RL explorations, and that the model is able to learn from audio signals more efficiently compared to the state of the art."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper proposes an end-to-end framework for learning a representation for singleand multimodal data for the task of category discovery. The key idea is to learn a shared representation space, and then use it for both labelled and unlabelled data for self-supervised representation learning based on noise-contrastive estimation. The idea is that instance discrimination based on category discrimination and cross-modal discrimination can be used to improve contrastive learning approaches. The proposed framework is evaluated on several image benchmarks (CIFAR10, CIFAR100, and ImageNet), including Kinetics-400 and VGG-Sound, and the results show that the proposed framework achieves state-of-the-art performance.    The main contribution of the paper is to use pairwise pseudo labels for cluster assignments based on the cluster assignments of the unlabeled data, and to use category discrimination for the labelled data, as well as the cross modal discrimination for multi-modual data. "
SP:4df640f502e88ddba2d7e183625231d70b083e82,"Weakly supervised segmentation is an important problem in image classification. Existing methods for weak supervision rely on the assumption that coarse annotations are available for each pixel in the image. However, this task can be seen as a semi-supervised metric learning problem, where the sparse annotations have a broad region coverage. The paper proposes to use partial annotations (e.g., image-level tags and object bounding boxes) for the task. Class activation maps for coarse labels and conditional random fields for the sparse labels are used to train a segmentation model. The authors show that the proposed method is able to achieve state-of-the-art performance on Pascal VOC.    The paper also proposes a new way of pixel localization based on partial annotations. The idea is to use contrastive relationships between low-level image similarity, semantic annotation, co-occurrence, and feature affinity. This is done by learning a pixel-wise feature from the partial annotations, and then using data-driven grouping and discriminative feature learning. The code is well-written and easy to follow, and the experiments show that this universal weakly supervised classifier can be trained on any dataset (i.e., any feature space).  "
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper studies the problem of self-supervised learning in the context of slow convergence of the pretext task. The authors propose a distillation strategy for unsupervised training that aims to improve the performance of small scale models compared to their supervised counterpart. The proposed method, called BINGO, is based on contrastive learning based methods, where the optimization is performed on a Bag of InstaNces aGgregatiOn, which is a bag of instances that are selected to be used for distillation. Experiments on top-1 accuracies on linear evaluation on ImageNet with ResNet-18 and ResNet+34 are conducted, showing that the proposed method outperforms the baselines in terms of the number of samples and the top 1% of top accuracy. "
SP:328866aad6544c81ded8980934df31dc4472435f,"Simulation-based inference (SBI) is a special case of statistical inference in stochastic models where the likelihoods of the posterior distribution of the data are assumed to be high-dimensional. Previous SBI algorithms have been compared with generative adversarial networks (GANs) and GANs, and GATSBI is an adversarial approach to SBI. Inference with implicit priors has been shown to be more efficient than using explicit likelihoods. In this paper, the authors show that the variational objective for implicit posterior distributions in the adversarial setting is the same as that of SBI and GGANs.    Inference in high-dimensionality is achieved by using the implicit posterior of a model of a wave propagation on the surface of a shallow water body.  The authors show empirically that the GATSBI is able to obtain well-calibrated posterior estimates in high dimensions, and that GATs are able to achieve similar performance as SBI on standard SBI benchmark problems as well as high-dimensions simulators.  In addition, they also show that Gatsby can be used for sequential posterior estimation in the presence of a high-divergence between the model and the true posterior.  Finally, they show that using high- dimensional simulation-based models for Bayesian inference with GAN is also possible, and show that it can be applied to the case where the model of camera optics is used as an implicit prior, and it can approximate the true one. "
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"This paper considers the problem of causal inference in the setting where there is limited overlap between treatment effects and individualized treatment effects. The authors propose a generative prognostic model, where a prognostic score is computed from a latent variable for each treatment, and a variational autoencoder (VAE) is used to learn the features of the treatment and the individualized features. They show that the prognostic scores of TEs can be used as a surrogate for the true TEs. They also provide TE error bounds for the generative model. Finally, the authors show that their model can also be used to estimate individualised treatment effects, and that their method outperforms existing methods on (semi-)synthetic datasets. "
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a new benchmark task for RL, called Embedding Reinforcement learning (EARL1), which is a collection of tasks that require human supervision. The authors present a set of RL algorithms for learning episodic simulated environments and evaluate them on a number of real-world platforms (including robots). The framework is built on top of the simulated benchmark EARL1, which consists of several simulated tasks where the goal is to learn without extrinsic intervention, and to learn with interventions. The paper shows that the proposed algorithms for reinforcement learning outperform existing approaches for episodic RL in terms of autonomy, and outperform other approaches that do not require human intervention. This is an interesting contribution to the field, and it is a good contribution to a growing body of work in the field that aims to address the problem of real world embodied learning."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"This paper investigates the use of Graph Neural Networks (GNNs) for the task of QA, a popular topic in AI and NLP fields such as Question Answering (QA) and QA systems with human-level reasoning capability. The authors propose two modules to improve the reasoning capability of existing state-of-the-art QA system by using knowledge graphs (KGs). The modules are built on top of existing GNNs and are called ""knowledge-aware GNN"" (KG-GNN).  The authors show that GNN-based modules have reasoning functionality and that they can be used to improve QA performance. They also show that pre-trained language models (LMs) are able to learn to reason about the reasoning capabilities of the GNN.   The main contribution of the paper is that the authors propose GNN modules for QA that have reasoning capability that is comparable to that of LMs, and they show that the reasoning process of a GNN can be understood as a graph neural counter.  The paper also shows that the graph-based reasoning modules can be applied to existing QA benchmark datasets such as CommonsenseQA and OpenBookQA, and show that they improve the performance of GNN for reasoning in QA.  Finally, the paper shows that knowledge-aware graph neural modules can also be used in the context of knowledge-powered QA and demonstrate that they perform better than the existing state of the art in reasoning (e.g. counting)."
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes Succinct Compression, a three-stage framework for Deep Neural Networks (DNN) compression (pruning, quantization, and quantization) that aims to compress them on low-cost devices and reduce the performance and space consumption. The proposed method is based on the use of the recently proposed Succint Data Structures (SDS) to perform fast queries in the compressed representation. The authors propose a method to compress DNN models based on Suffinct Data Structured (Succinct) formulations for DNNs in an Element-wise or Block-wise manner. The method uses execution pipelines to train the model formulations and then applies the method to transform the transformed DNN into a single DNN model. Experiments on AlexNet/VGG-16 inference show that the proposed method achieves near-optimal compression compared to Quantization, Pruning, and Quantization. The paper also shows that the method outperforms Huffman Coding in terms of inference runtime.  "
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes a new training method called Network Augmentation (NetAug) to train tiny neural networks with limited capacity. Previous regularization techniques, such as data augmentation and dropout, are commonly used to train large neural networks, but the authors argue that these techniques are not effective for training tiny networks due to the over-fitting caused by noise in the training data. They argue that the reason is that the tiny models trained with these techniques tend to underfit and over-fit compared to large models. To address this issue, the authors propose to augment the network (reverse dropout) trained with NetAug.    The idea is that a tiny model is trained with the same number of layers as a large model, but with a different number of weights. It is assumed that the network is trained in a self-supervised manner, where each layer of the network receives no supervision. The authors show that NetAug can be used to augment any network (up to a constant factor) and that it can be applied to any tiny model in larger models. They also show that the inference of the tiny model can be performed in parallel with the inference overhead of the larger model.  The authors evaluate the performance of NetAug on image classification and object detection on ImageNet, Cars, and Pascal VOC. NetAug is shown to outperform existing methods in terms of performance and computational cost for training these tiny models, and to be effective for under-fitting, over-fitting, and inference overhead."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a new data representation called Point cloud sequence, which can capture flexible shape and motion information. The authors propose a model to learn temporally coherent feature spaces, where scene flow information is encoded in the form of dynamic point cloud sequences. The point correspondence information in real-world environments can be represented as a sequence of point correspondence annotations. The generator is trained to produce a temporalally coherent output for each point cloud sequence. A learnable masking module is used to learn the upsampling ratio based on the point distribution. The proposed method is evaluated on three different domains: particles in a fluid dynamical system, human action scanned data, and a series of environments with dynamic point correspondence annotation. The quantitative and qualitative evaluation shows that the proposed method achieves state-of-the-art performance on both the downsampling task and learning temporal coherence on irregular point cloud sequential sequences."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,"This paper studies the problem of robustness and generalization in pre-training for detection transformers. The authors propose a new 12-layer transformer, called FP-DETR, and show that it can be used for object detection, and it can also be used as a task adapter for NLP with textual prompts. Specifically, query positional embeddings from visual prompts are used to train the model with the help of visual prompts, and the task adapter is trained with self-attention. The proposed method is evaluated on the COCO dataset and shows that it outperforms the state-of-the-art robustness, generalization, and robustness on small-size datasets. The paper also shows that the separation of upstream and downstream tasks in the separated training paradigm is beneficial, and that the encoder-only transformer is more robust to common corruptions."
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper proposes a new federated learning algorithm, FedPAGE, which is a variant of Federated Averaging (FedAvg) that is based on the optimal PAGE method. In the convex setting, the authors show that the communication cost of the optimal algorithm (called Local-SGD) is $O(\sqrt{T}/T}$, where $T$ is the number of local SGD steps, $T=T$ the communication round, and $T(T) = T$. In the nonconvex setting (where $T = T$, $T+T$), the authors prove that the optimal Page method is optimal for the federated convex learning algorithm. The authors also show that in the case of convex case, the communication complexity of FedPAAGE is O(1/T) times smaller than that of FedAvg.  The authors then show that FedPAEG can achieve the same communication complexity as FedAvg with fewer communication rounds than FedAvg in a convex federated setting and O(O(T^2) times less communication rounds in a nonconvergent setting.  Finally, they show that under certain assumptions on the communication costs of the clients (i.e., that the clients do not have to share the same number of communication rounds as the clients), FedPAERGE can achieve a communication cost as low as $O(1 / T)$, which is the optimal for convex cases.   The main contribution of the paper is that the authors provide a theoretical analysis of the communication efficiency of the FedPAEg and FedPAG. They show that, under some assumptions, the optimal FL algorithm (FedPAGE) can achieve communication complexity O(T/T). They also provide an empirical analysis that shows that FedGE and FedPGD are both competitive with local methods for federated concave convex and nonconformity, and that FedPGA and FedSGD are competitive with the optimal optimal FL algorithms. Finally, in the non-conformist setting, they provide an experimental analysis that compares FedGE to FedAvg, FedAvg and SCAFFOLD and show that their algorithm is more efficient. "
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper studies the decision boundary geometry of ANN classifiers in the presence of adversarial input perturbations.   Artificial neural networks (ANNs) are trained using mathematical operations, and it is known that these networks are robust to adversarial inputs.  This paper shows that adversarial subspaces can be found in the training process, and that the adversarial perturbation can be used to perturb a decision boundary of the network.  The analysis is based on the observation that the minimal perturbance is the one that is close to the boundary of a subspace.  It is shown that the boundary is a function of the dimensionality of the subspace and the number of samples.  In addition, the authors show that if the dimension of this subspace is large enough, then the boundary can be reached.  They also show that the test-time adversarial attacks can be incorporated into the training procedure.  Finally, they show that under certain conditions, adversarial training is more robust to test time attacks. "
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,This paper proposes a weakly-supervised contrastive learning approach for unsupervised representation learning based on data clustering information. The idea is to use auxiliary information from hashtags (e.g. Instagram image) as a way to augment the representation of a cluster with similar representations and dissimilar representations. The authors show that self-supervision representations with direct downstream labels are more robust to auxiliary-information-infused representations compared to supervised representations that are trained with only indirect supervision signals. They also show that the proposed approach outperforms baseline representation learning methods that do not use auxiliary data information in their approach. The paper also shows that the approach can also be used to learn more robustly to the presence or absence of auxiliary data in an unsuper supervised representation learning approach.   
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"Recovering sparse parameters in machine learning from observational data is an important problem. The authors propose two algorithms to solve this problem. One is the Provable Learning-based Iterative Sparse recovery Algorithm (PLISA) and the other is a path-following algorithm. PLISA improves the recovery accuracy and generalization ability of the empirical Rademacher complexity of recovering sparse parameters. The main contribution of the paper is that PLISA is a provable learning-based iterative iterative recovery algorithm.   The authors provide theoretical analysis of the generalization performance of PLISA for sparse estimation problems where the hyperparameters are provable and the problem distribution of interest is known. The generalization analysis is based on the assumption that the recovery of the parameters is provable, and the authors show that the recovered parameters are provably non-trivial, and that they can be recovered with high probability. They also provide theoretical analyses of the stability and convergence of the unrolled algorithm. Finally, the authors propose techniques to improve the generalizability of the learned algorithms and the stability of the algorithms. "
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes Hybrid Action Representation (HyAR), a novel approach to learn a compact and decodable latent representation space for discrete-continuous hybrid action spaces. The authors propose a discretization that allows for a unified homogeneous action space, which can be used for robot control and game AI.    The paper proposes to use an embedding table and a conditional Variational Auto-Encoder (VAE) to learn the latent space of a discrete or continuous action space. The action representation is learned via unsupervised environmental dynamics prediction. Reinforcement Learning (RL) algorithms are used to learn this representation space, and DRL algorithms are also used to train the representation space.  The main contribution of the paper is that HyAR learns a latent space that is decoupled from the discrete action space and decouples it from the approximation difficulties of learning a discrete action from a continuous parameter. The paper shows that the learned action space of the hybrid action embeddings in the action space can be decomposed into a discrete and continuous one, and that RL algorithms can be trained to learn such decoupling.  Experiments show that the proposed HyAR outperforms baselines for learning high-dimensional action spaces, and is able to generalize well to hybrid action RL. "
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper studies the general non-convex stochastic gradient descent (SGEM) problem in the online convex setting. In particular, the authors consider the setting where the gradient of SGEM converges to the stationary point of a non-smooth convex function. In this setting, SGEM can be viewed as an extension of the Adaptive Gradient Descent (AGD) algorithm. The authors show that SGEM has the same unconditional energy stability property as AEGD, and provide a regret bound for the online setting.  "
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a new non-autoregressive (NAR) approaches for inference and training. The authors show that NAR models outperform their AR counterparts in terms of human-level accuracy on multiple datasets. The main contribution of the paper is that the authors propose a new AR framework, called CMLMC, which is able to learn from raw data without distillation. Experiments show that the proposed method outperforms the state-of-the-art NAR methods on multiple benchmarks.   "
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper studies the problem of ultra-low power local signal processing for edge applications on always-on devices with limited power budget. Neuromorphic processors with spiking neural networks have limited computational power, and the authors propose WaveSense, which is a new neuromorphic implementation based on the WaveNet architecture. WaveSense uses neural dynamics and fixed time-constants instead of dilated temporal convolutions. The authors show that WaveSense is able to achieve state-of-the-art performance on standard datasets for keyword-spuriousing and keyword-spotting. They also show that the proposed network outperforms spiking networks and other artificial neural networks, such as CNNs and LSTMs. "
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper studies the problem of fairness in machine learning for social applications. The authors propose two algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees for machine learning algorithms that are sensitive to unfair behavior in the face of demographic shift. They show that these algorithms are effective at addressing demographic shift’s challenges, and that their technique, Shifty, can be applied to a wide range of social applications where fairness assurances are not available. They evaluate Shifty on a real-world dataset of university entrance exams, and show that Shifty outperforms existing approaches. They also show that the proposed algorithm can be used to train models that have high-confident fairness guarantees. "
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes Stochastic dual dynamic programming (SDDP) for multi-stage stochastic optimization, which is a general framework for modeling real-world process optimization tasks. SDDP has been shown to have the worst-case complexity of $O(\sqrt{T})$ for low dimensional problems, and the authors show that SDDP can be used to reduce the problem solving cost to $O(1/T)$. SDDP uses a trainable neural model to learn a piece-wise linear value function in an intrinsic low-dimension space, which can then be used as a training objective for the SDDP solver. The authors also show that the training objective can be combined with existing reinforcement learning algorithms to improve the performance of SDDP and improve the solution quality compared to competitors such as SDDP.  The authors evaluate the effectiveness of the proposed ν-SDDP on both synthetic and real world process optimization problems, showing that the proposed SDDP is able to achieve the best performance in terms of the number of decision variables, and that the optimization is more efficient.   "
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a new protocol for private next-token prediction based on the relaxation of group differentially private prediction. The protocol, called SUBMIX, aims to prevent privacy violations in language models trained on a private corpus. Specifically, it uses a data-dependent privacy accounting mechanism to make it more robust to data-extraction attacks. Experiments are conducted on transformer-based models (e.g., GPT-2) for next-Token predictions and show that the proposed model is more robust than existing methods."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate from a classification model. The proposed k-NN density estimator is based on finite-sample high-probability statistical results, and the authors show that their k-NCD density estimation outperforms the state of the art OOD detection method by a large margin. The authors also propose a new model based on label smoothing, the so-called Label Smoothed Embedding Hypothesis. The proposal is empirically shown to outperform OOD baselines in the presence of indistribution samples."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"Diffusion-based methods are based on stochastic differential equations in the continuous time domain. The authors propose a denoising score matching objective for diffusion-based representation learning, where the goal is to learn a non-adversarial generative model that is robust to the noise in the supervised signal. The paper proposes a new approach to learn an infinite-dimensional latent code, which can be used to improve the sample quality of the representation learned by GANs and VAEs.   The paper also proposes a novel denoizing score matching framework for representation learning. The proposed approach can be applied to multi-scale denoised autoencoders, and the authors show that the proposed approach is able to achieve state-of-the-art performance on semi-supervised image classification.  The authors also show that adversarial training for diffusionbased models can improve sample quality through the use of an approximation of the prior at smaller noise scales, which improves the sampling speed. The main contribution of the paper is that the authors propose an approach to denoise the latent codes, which is more computationally efficient. "
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"Goal-conditioned reinforcement learning (RL) is a popular approach for tasks that require offline data, expert demonstrations, and reward shaping. The authors propose a new algorithm called Classifier-Plans (C-Planning) that uses planning to learn a curriculum of intermediate states to guide the algorithm to a distant goal-reaching task. The algorithm is based on the idea of graph planning, where the policy is trained to maximize the expected return of the next state. The E-step and M-step of the algorithm are used to learn the goal-conditioning policy, and the expectation maximization is used to train goal- conditioned policies. Experiments show that the proposed method outperforms prior methods that do not use graph search, and that it can be applied to a wide range of tasks, including navigation, manipulation, and exploration.    The paper is well-written, well-motivated, and easy to follow. The idea of using graph planning is interesting. However, there is a lack of comparison with prior methods based on prior methods using goal-conditional RL and graph search. "
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper proposes a new regularization technique for deep neural networks called Mixup. Mixup is a popular regularization method for improving generalization and adversarial robustness. It is based on the observation that the local distributional structure of the training data is highly correlated with the clusters and the data manifold. The authors propose to use mixup to regularize the k-mixup case, which is a variant of displacement interpolation (i.e. interpolation on the Wasserstein metric).   The authors show that k-Mixup can be applied to both cluster and manifold structures. Perturbations induced by mixup are robust to perturbations caused by a data augmentation approach (e.g., mixup). Averaging weights from the beta distribution is used as a regularizer. The paper also shows that the optimal transport is obtained by using displacement interpolations.  The paper further shows that a fully-connected network trained on synthetic datasets for binary classification is robust to blur and contrast, and robustness to adversarial attacks is also improved.    Finally, the paper shows that mixup regularization can be used to improve the generalization performance of models trained on different network architectures and different benchmark datasets. The main contribution of the paper is that the authors propose a new procedure based on α. The idea is to mix the two distributions on the embedded manifold, and then apply k-mixtureup to each of the two discrete distributions.  Experiments are conducted on a few synthetic datasets, and on a fully connected networks trained on the synthetic datasets.  Results show that the benefits of mixup on perturbed training datasets are that have a global cluster or a manifold support structure, and that the benefit from k- mixup is not due to the local structure. "
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,"This paper proposes a novel nonlinear classification layer for a lightweight network to learn embeddings. The nonlinearity in representation (embedding) learning in deep networks can be seen as a result of deep networks with a nonlinear kernelized classification layer. The authors show that nonlinear classifiers in the embedding vector space can be learned by a single network with a limited-capacity backbone, and that they can be trained with a linear classifier in the same way as a deep classifier with the same number of layers.   The authors also show that a classification layer with radial kernel functions that are non-linear in their embedding and can be used to learn nonlinearclassifiers.  The proposed layer can be applied to a wide range of computer vision and natural language processing tasks, and the authors demonstrate that the proposed layer leads to model-efficient classifiers."
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"Graph Neural Networks (GNNs) are used to learn node representations for graphs. Node representation learning has been a hot topic in recent years, and the authors show that existing fairness-aware data augmentation frameworks rely on nodal features and graph structure to mitigate intrinsic bias. The authors propose two schemes to improve the fairness of existing GNN-based learning mechanisms. The first is based on real networks for graph contrastive learning. The second uses real networks to improve node classification and link prediction. They show that the proposed augmentation strategies outperform existing contrastive methods for fairness in terms of statistical parity and equal opportunity. "
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating treatment effects from observational data with unmeasured confounders. The authors propose a two-stage regression with instrumental variable (IV) and confounder-free (influential variable-free) regression, which is a variant of 2SLS, a well-known two-step regression algorithm for two-armed bandits.   The authors show that nonlinear IV regression variants of the two stages are biased towards confounding bias due to the additive separability of noise in the IVs, and that the confounding bias can be alleviated by balancing the treatment effect estimation using a balanced representation of confoundering variables.  They also show that the bias-variance trade-off between imbalanced treatment distributions and treatment effects can be reduced by balancing.  The main contribution of the paper is the development of a new algorithm, the CB-IV algorithm, which consists of two modules: treatment regression and outcome regression. In the treatment regression module, the authors propose to use the two-stages of two-SLS to mitigate the confounding.  In the first stage, the treatment is treated as a linear setting, and the unbalanced treatment distribution is used as an instrumental variable in the second stage.  Experiments are conducted to show that, under the assumption that the treatment distributions are balanced, the proposed CB- IV algorithm outperforms state-of-the-art methods (i.e., IV regression, confoundER balancing methods) on IV regression with IV regression under the multiplicative assumption, and on the outcome regression, and also shows that the proposed by the authors outperforms the state- of-the art of the art methods in terms of treatment effects estimation. "
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper studies the problem of few-shot learning in the setting of two-layer neural networks. In particular, the authors consider a linear regression setting, where the objective function is a linear function of the number of training samples and the size of the training set. The authors show that the MAML algorithm is a generalization of the non-adaptive learning (NAL) algorithm. They also show that NAL can be seen as a special case of NAL.   "
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes a new method for semi-blind source separation based on the Proximal Alternating Linearized Minimization (PALM) algorithm. The authors propose a new algorithm called Learned PALM (LPALM), which learns the hyperparameters of the proximal alternating linearized minimization algorithm. They show that the learned algorithm outperforms the original PALM algorithm in terms of separation quality. They also show that LPALM can be applied to the semi-blind setting. "
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper studies the power-law relationship between transformers and LSTMs in the context of language modeling. The authors propose a Legendre Memory Unit based model with an attention module (implicit self-attention, which is a variant of the Legendre memory unit based model proposed in [1] and [2]). The authors show that transformers with the proposed loss outperform LSTM in terms of loss, model size, and computational and memory requirements. They also show that the proposed model outperforms transformers without memory or computation.    The authors also propose a new architecture based on global self attention, which can be used to reduce model size and reduce the number of layers. They show that this architecture can be applied to any architecture with the same input sequence length, and that it can be combined with existing transformers."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"Generative adversarial networks (GANs) have recently been shown to achieve photo-realistic quality for image generation. This paper proposes a two-stage GAN called LatentKeypointGAN, which aims to improve the quality of the generated images. The key idea is to learn a set of space keypoints with internal conditioning on the image content. These keypoints are represented as appearance embeddings (e.g. generating portraits) that are shared across different spatial and appearance factors. The authors show that the latent space generated by LatentkeypointGAN can be used to learn an interpretable latent space, which is then used to train a GAN-based method for unsupervised keypoint detection. The paper also shows that the re-positioning of keypoints can be further improved by the use of different network architectures and training schemes. "
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper proposes a non-perturbative analysis of signal propagation in deep fully-connected neural networks with layer normalization based on the mean field formalism. The authors show that the depth of the network is not the cause of gradient explosion, but of representation shrinkage, which is caused by the initialization scheme and the activation function. They also show that normalization techniques can be used to alleviate these problems. Finally, the authors propose a method to regularize the initialization variances of residual networks, and show that this method can also be applied to residual networks. "
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper studies the problem of finding optimal step sizes for stochastic gradient descent (SGD) in the presence of inherent noise. The authors propose a line-search method for finding the optimal full-batch loss with different step sizes. They show that line searches can be used to find step sizes that are close to the optimal in the absence of noisy update step directions. Learning rates are approximated by parabolas, and the authors show that the optimal update step size can be found by using a parabola. They also show that their approach outperforms SGD with a piece-wise constant learning rate schedule in terms of validation and test accuracy for different batch sizes.   The authors also compare their approach with other line search approaches for Deep Learning across models and show that SGD is more robust to noisy update steps than other recent state-of-the-art line search methods."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper proposes Noise-contrastive estimation (NCE), a statistically consistent method for unnormalized probabilistic models. NCE is based on the assumption that the noise distribution of the NCE depends on the underlying noise distribution, and that the (flat) loss landscape of the exponential family is well-studied. The authors show that eNCE, a variant of NCE based on an exponential loss, is a special case of the proposed NCE. They also show that normalized gradient descent converges to an exponential family. "
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"Privacy is an important problem in distributed machine learning, especially when the model is trained with a distributed SGD algorithm with noisy information. This paper studies the problem of differential privacy (DP) under the assumption that the learning algorithm is not affected by the parameter-server architecture. The authors show that under certain assumptions, the convergence of the distributedSGD with Byzantine faults is guaranteed. They also provide an approximate convergence guarantee based on the (alpha, f)-Byzantine resilience of the algorithm, which is similar to those of previous works. They further show that hyperparameter optimization can be used to further improve the guarantee. Finally, they show that existing approaches for DP and BR can be extended to DP, BR, and DP.   "
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper tackles the problem of computer source code editing, where both support and query code snippets are available. The authors propose a deep learning approach to solve the code editing problem by learning a common pattern between the support exemplars of a query code snippet and the edit exemplars for a given common pattern for code editing. Each editing exemplar corresponds to an editorial pattern, and the authors use them to learn edit representations from a set of support exemplar and query exemplars, and use them for the task of code editing for queries. The proposed learning approach is a learning approach that combines edit representations learned from the edit representations of the query and support code snippets, and a multi-extent similarities ensemble for the problem. In particular, the authors propose to learn abstract syntax trees with a language-specific grammar, and to use a similarity-ranking error estimator. The similarity measurement is used to learn the collective tree representations of all the support examples and the query examples, which are then used to compute the similarity between queries and support examples. The query and query sample matching is then performed by using the proposed method on C# and Python datasets, and compared to non-compositional baselines.  "
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a new approach to learn a high-level structure in deep generative models for realistic sequence data, such as text and music. The authors show that models trained with local coherence and global coherence are able to capture the local and global structures of the data. They also show that a model trained on realistic data can capture the relational constraints of relational constraints, which are measures of music and text coherence. The generative process can be seen as learning a model that captures this relational constraints through a program synthesis algorithm. The constraints are learned by training a generative model on the constraint data. The proposed approach is compared to state-of-the-art methods for capturing high level structure, and shows that the proposed approach outperforms them in capturing the low-level structures.  "
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper studies the problem of set-to-hypergraph prediction, which is an important problem in particle physics, biological systems, and combinatorial optimization. In particular, scaling problems such as run-time complexity and memory requirements are of high importance in set- to-hyperGraph tasks. This paper proposes a new training method for iterative refinement of hyperedges, where the hyperedge is the set of nodes that have the most positive edges. The authors show that the proposed training method can achieve asymptotic memory scaling, which leads to a trade-off between efficiency and constant memory usage. In addition, the authors also show that contributions of the proposed model can be incorporated into the training of a standard set to hypergraph model. The proposed model is shown to outperform the state-of-the-art.   "
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a post-processing method to improve the performance of models that are sensitive to biases in deep learning algorithms. It is based on the idea that deep embeddings in a pre-trained model can be biased. The authors propose a shallow neural network, called the Ethical Module, to mitigate this issue. The representation power of the deep embedding is reduced by minimizing the von Mises-Fisher loss. The paper shows that the proposed methodology outperforms the state-of-the-art bias mitigation on a number of datasets, including gender bias in facial recognition. "
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper tackles the problem of knowledge distillation (KD) in class-incremental learning (CIL), where the goal is to distill knowledge from old-class data to new-class samples without any class overlap. The paper proposes a novel evaluation function, called knowledge distillation (DK), which is based on a reinforcement learning algorithm. The KD loss is trained on the new class data, while the old class knowledge is used to train a phase model to learn from the new data. The authors propose to use a placebo data from a free image stream (e.g., Google Images) as the KD loss. The idea is that the evaluation function can be seen as a function of the amount of overlap between the old and new classes, and that the learning of new classes can be regarded as a learning of the old ones.    The paper shows that KD loss can be trained on placebos (i.e., a set of placebos that are not used in the training of the new classes) and on pseudo CIL tasks, where the placebos are generated from the same image stream as in the original CIL task. The proposed method is evaluated on top-performing CIL methods on two higher-resolution benchmarks (ImageNet-1k and ImageNet-Subset) and is shown to outperform a number of existing methods. It is also shown that the performance of old class exemplars trained with supervision and a memory budget can be improved. "
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes a new approach to modeling discrete structure using Energy-based Models (EBMs). The key idea of the approach is to combine inference and learning of EBM with sampling from discrete distributions. The sampling is done using Markov Chain Monte Carlo (MCMC) with an informed proposal that is based on local updates of the energy function. The authors propose a path auxiliary algorithm based on the composition of local moves, where the energy changes are computed as a function of the evaluation of energy function and the number of local updates. The algorithm is shown to be computationally efficient. The paper also shows that path auxiliary algorithms are more efficient than generic samplers for sampling and inference, and that deep EBMs can be trained on high dimensional discrete data. Finally, the authors show that the sampling of discrete models from discrete models using discrete models with path auxiliary methods outperforms generic sampler and inference. "
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper proposes Variational Predictive Routing (VPR), a method for learning a spatiotemporal hierarchy in a sequential data. The idea is to learn a hierarchical generative model for a sequence of video frames, where each frame is represented as a layer-wise latent variable. This latent variable is then used to train a neural network to predict the next layer of the latent variable, which is used to learn an event detection mechanism. The proposed method is evaluated on a number of video datasets.  "
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,"Image retrieval is a challenging problem, and existing methods rely on global features. However, the re-ranking process in such methods is expensive and time-consuming. This paper proposes a local feature learning approach to tackle this problem. The key idea is to use convolutional neural networks instead of the standard RANSAC algorithm, and use it to improve UGALR by using spatial and channel attention and intermediate supervision to capture accurate and semantic local information. Experiments on Oxford and Paris datasets show that the proposed approach achieves state-of-the-art performance. The paper is well-written and well-motivated, and the experiments are well-designed. The results are interesting and interesting, and show that local feature matching can be achieved with convolution neural networks without memory consumption.   "
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"Multitask learning is an important problem in many applications domains such as computer vision and reinforcement learning. In this paper, the authors propose an algorithm called RotoGrad, which aims to address the problem of negative transfer between different tasks with shared network parameters. The algorithm is based on the Pytorch implementation of the gradient magnitude, and the authors show that it is able to adaptively adjust the gradient magnitudes for different tasks in order to improve the gradient directions. The authors also show that the proposed algorithm can be applied to complex problems such as multi-label classification and computer vision tasks on the NYUv2 dataset. They show that under certain conditions, the proposed method can outperform existing methods on complex problems including multi-labels classification, multi-class classification, and several other complex problems.   "
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"Layer-wise model fusion is based on soft neuron association between pre-trained networks and their weights via optimal transport. The authors propose a new model fusion framework called CLAFusion to fuse neural networks with heterogeneous data. OTFusion is used to fuse two networks with the same number of layers. The proposed framework uses cross-layer alignment to fuse heterogeneous neural networks by solving the cross-layton alignment problem, which is an unbalanced assignment problem. This is solved using dynamic programming. The framework can be applied to layer-wide model fusion for any number of neural networks, and it can be used to apply the finetuning process to any residual networks. The fused network can be trained with CLAFusions.  The authors evaluate it on the CIFAR10 dataset, and show that it achieves state-of-the-art performance in terms of model compression and knowledge distillation in the teacher-student setting. They also show that the proposed framework is robust to the number of neurons and the amount of retraining.  "
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper studies the implicit regularization of overparameterized deep networks in the context of generalization in deep networks. In particular, the authors show that stochastic gradient descent (SGD) can be used as an implicit regularizer for the overparametrized deep networks, and that this regularizer is more robust to degenerate solutions than SGD in supervised learning. The authors also show that the degenerate feature representations learned by SGD can be replaced by an explicit regularizer called DR3, which is a regularizer that encourages parsimonious solutions to be learned in the presence of degenerate features. This regularizer has the advantage of being more robust than the supervised learning case, as the representations learned for state-action pairs in the offline deep RL setting are more likely to be degenerate.    The authors further show that DR3 is able to learn feature representations with bootstrapping, where the deep network value function is approximated by the bootstrapped version of the feature representations. DR3 improves performance on unlearning in Atari 2600 games, D4RL domains, robotic manipulation on images, and deep reinforcement learning (RL) methods in general. The performance and stability of DR3 are improved over offline RL methods that do not use DR3. "
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes HyperDQN, an algorithm for deep RL that uses RLSVI to generate posterior samples from a non-linear neural network (i.e., a base model) to approximate Q-values. The proposed method is based on a probabilistic hypermodel, called a meta model, which is a nonlinear combination of a base and a hypermodel. The hypermodel is used to generate approximate posterior samples for the Q-value function, which are then used to train a hyper model for the exploration method.  The authors show that the proposed method outperforms DQN on the Atari suite and achieves the maximum human-normalized score. The authors also show that RLSVi can be used for exploration, and that the posterior samples generated from posterior samples can be efficiently used to compute Q-valued functions for the exploratory action sequences.   The paper also shows that HyperQQN outperforms the exploration bonus and randomized exploration methods for SuperMarioBros, and outperforms a number of existing exploration bonus methods. The main contribution of the paper is that the authors propose to use probabilistically trained models for exploration. The paper is well-written and easy to follow."
SP:b428383660928374c953f659ea1e05852dbdcd6e,This paper proposes a new representation learning method for learning causal representations of the causal structure of the data. The proposed method is based on the idea that causal representations can be learned from observational data and can be used to improve the generalization performance of the model. The authors propose a new counterfactual loss function that is designed to encourage the model to learn a causal representation that is more robust to spurious correlations in the observational data. Experiments on image classification and recommender systems demonstrate the effectiveness of the proposed method.  
SP:1258c05a80a17949b50e6dae13deea1d2235f456,"This paper proposes a federated learning scheme, called ProgFed, which is a distributed learning scheme where the model is trained on edge devices with limited network bandwidth. The authors propose a progressive training framework, called Progressive Fed, to accelerate the training of edge devices during training. It outperforms existing models in terms of asymptotic rate, and is able to learn compact formats (e.g. gradient compression and distillation). The authors also show that their training approach can be used to train converged models with lower computation and communication costs. The training approach is evaluated on three different architectures (VGG, ResNet, ConvNets and U-nets) and three different tasks (simple classification, medical image segmentation). The results show that the proposed approach outperforms compression in all three cases, and can be applied to full models."
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the Rademacher complexity of deep neural networks under adversarial attacks. Deep neural networks have been shown to be robust to a variety of types of attacks. Adversarial training is a common technique to improve the robustness of a model. This paper studies two-layer neural networks, and shows that adversarial training has the same adversarial Rademachacher complexity as the standard adversarial robust training. The authors also show that two-layered neural networks can achieve a similar adversarial complexity to the standard robust training in adversarial settings. The main contribution of this paper is to show that the robust performance of neural nets can be improved by reducing the Radmacher complexity.   The authors propose a novel method to do so. They show that adversarially trained weight norms are asymptotically similar to the trained weight norm of the original layer. The bound is based on the product of weight norms between the original and adversarial layers. "
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a new approach to estimate the differential entropy and mutual information between two sets of data points. The proposed method, called Knife, is based on the idea of estimating the (differential) entropy and the mutual information. The authors show that the proposed approach can be used to improve the performance of existing KNIFE-based estimators for differential entropy. They also show that it can be applied to train neural networks for real-world tasks. The method is evaluated on high-dimensional synthetic data and is shown to perform well on a number of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning."
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"Soft-greedy operators such as resmax and softmax have been widely used for exploration in action-value methods in reinforcement learning. However, these operators suffer from the suboptimality gap, which is a result of overemphasizing sub-optimal actions. This paper proposes a new soft-rewarder, called mellowmax, that is a variant of resmax. It allows for better coverage of the state-space, and it is more robust to non-expansion (e.g., mellowmin) than softmax. The authors also propose an exploration hyperparameter that encourages non-excess expansion. The paper also proposes a softmax policy with a state-action specific temperature. Experiments on tabular and deep RL show that the proposed resmax outperforms softmax, and the authors also show that it can be used in combination with softmax to improve the performance. "
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper studies the problem of learnability of deep learning models. The authors propose a new concept called learnability lock, which is the concept that a model with a certain class-wise perturbation will not be able to learn a given dataset. They show that this learnability is a result of a class-level transformation function that is invertible. They also show that a universal transformation function can be used as a universal function to enforce the learnability on any dataset. Finally, they show that the inverse transformation can also be used to enforce learnability. "
SP:9af10703605e620e563241e2602a50b629f3d37a,"Graph Neural Networks (GNNs) are an important technique for modeling relational data, but they can be problematic when there are missing node or edge features in the graph. In real-world applications, these features are often missing in the form of missing features. This paper proposes an approach to learning missing features in graph machine learning applications by solving a diffusion-type differential equation on the graph, based on the minimization of the Dirichlet energy. The authors propose an algorithm called Feature Propagation, which is a simple and effective algorithm. The proposed approach outperforms existing methods on common node-classification benchmarks with missing features on a number of common graph-level tasks.    The main contribution of this paper is that the authors propose a new approach to solving the problem of learning the diffusion of a graph with nodes and edges in a GPU, which can be used to solve a diffusion equation on a graph. This is an important problem in social networks, where the graph is a collection of nodes or edges, and the nodes are connected to each other. "
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"Active learning is an important problem in deep learning, where the goal is to learn a model with limited labeled data. The paper proposes two heuristics to improve sample selection strategies. The first is to use an integer optimization problem to find a core set of samples from an unlabeled data pool, and then to use the Generalized Benders Decomposition algorithm to solve the problem. The second strategy uses the learned latent features from the unsupervised learning to select samples from the unlabeling pool based on the discrete Wasserstein distance. The optimization approach is evaluated on two data sets, and compared with several baselines and shows that the optimization approach outperforms them in the low budget regime. "
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a method for assembling genomes using a method based on geometric deep learning for genome assembly. The idea is to train a graph convolutional network on a dataset based on human genomic data, where a human DNA sequence is represented as a graph, and a genomic sequence is used to construct an assembly graph from the genomic sequence. This graph is then used for manual inspection for genome reconstruction. The authors propose a greedy search algorithm to find the best graph topology, which is faster than the traditional greedy search. They also propose a dataset for training the graph neural network on which the dataset is constructed from human genomic DNA. They show that the proposed graph machine learning algorithms for the de novo genome assembly problem outperform human handcrafted techniques, and that the model can be trained using a single sample from the dataset. The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from a lack of comparison with the state-of-the-art in terms of assembly speed, which makes it difficult to judge the quality of the paper.  The paper also suffers from the lack of comparisons with the best of the best in the field, which may be a limitation of the work.  In addition to the issues with the paper, there are also questions about the performance of the proposed model, and whether the proposed method can be applied to a more complex task (e.g. de-novo assembly of a telomere).    The authors also have questions about how to train the graph network and how to select the best de novevo assemblers.  "
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"Continual learning suffers from catastrophic forgetting. Continual learning is an important problem, and meta-learning is a popular approach to improve the performance of metacontinual learning algorithms. In this paper, the authors propose an online aware meta - learning (OML) approach to address this problem. The authors propose to incorporate experience replay (ER) into meta-testing, and use ER in meta-training for continual learning representations. They show that ER can be used for metatraining, and show that the model learns to sample from the replay buffer based on ER. They also show that meta-learned Predictive Sample Selection is more effective at selecting a good replay buffer than reservoir sampling, and that the online-aware nature of OML is beneficial. Finally, they show that their method outperforms the state-of-the-art by a large margin, and they also demonstrate that the learned representations exhibit clustering structures that are more robust to randomness. "
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"Multi-agent joint Q-learning for multi-agent cooperation based on Centralized Training with Decentralized Execution (CTDE) is proposed. The authors propose two methods to solve the multi-agents credit assignment problem, which is an extension of centralized training with centralized execution. The paper considers the problem of learning the Bellman optimality equation of the joint Q - value of the agents, which can be formulated as the explicit credit assignment function. The main contribution of the paper is that the authors propose a gradient ascent solution to solve this problem.   The authors show that under certain assumptions on the Q-values of the agent and the agents’ Q-value, the authors prove that the optimal solution of the problem can be found by solving a simple optimization problem. They also show that the solution of this problem is non-trivial for deep neural networks. Finally, they show that ECAQ can be used to solve credit assignment more efficiently than other baselines.  The paper is well-written and well-motivated, and the authors provide a detailed analysis of the time horizon of their method, and provide a theoretical analysis of their results. They show that their method can be applied to multi-manual cooperation in complex problems, and show that it can solve the problem efficiently. "
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper studies the problem of adversarial robustness in the context of transductive learning. In particular, the authors propose a new attack framework called Greedy Model Space Attack (GMSA), which is motivated by the observation that existing defenses against adversarial attacks can be viewed as a bilevel optimization problem, where the model is trained with test-time input, and the defense mechanisms are evaluated from a threat analysis perspective. The authors show that the existing defenses are more robust to GMSA than existing defense mechanisms. They also show that GMSA can be used as a weak instantiation of AutoAttack, which is an existing attack framework.    The authors also provide a theoretical analysis of GMSA, and show that existing adversarial learning based defenses are not robust to the GMSA. The main contribution of the paper is that the authors provide an analysis of the robustness of existing threat models against transductively-learning based defenses, and demonstrate that adaptive attacks are more powerful than AutoAttack. The paper also shows that the attacking model space in GMSA is more robust than the original one, and that the adversarial perturbations are more effective in the case that the attacker has access to the original model space.  Finally, the paper shows that adversarial training with GMSA leads to better robustness than without it. "
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies the effect of batch normalization on the training of neural networks. The authors show that the gradient of the normalization of the weights of a neural network can be approximated as a function of the function class of the training data, and that this approximation is more efficient than the original normalization. They also show that batch renormalization can be applied to small minibatches and that the per-example training procedure can be reduced to a single per-instance training procedure.   The authors also provide a theoretical analysis that shows that the training step computation can be significantly reduced when the inference model is of the same function class as the training dataset and that identity shortcuts can be used to reduce the number of steps needed to compute the gradient.  Finally, the authors provide empirical evidence that normalization can reduce the amount of training steps and improve the model accuracy."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper studies the problem of large-scale pretraining and adaptation on general domain data, which is a common problem in natural language processing, where fine-tuning is a popular way to fine-tune models. The authors propose a new Low-Rank Adaptation, called LoRA, which extends the Transformer architecture with trainable rank decomposition matrices to reduce the training throughput and inference latency. They show that the GPT-3 175B trained with Adam can be trained with LoRA with the same GPU memory requirement, and that the trainable parameters can be used for other downstream tasks. They also show that LoRA is more efficient than finetuning on RoBERTa, and can improve the model quality in terms of model quality and performance on language model adaptation.   "
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper considers the problem of structured prediction in the presence of non-local constraints. In particular, the authors consider distributions with nonlocal dependencies, i.e., distributions with local dependencies that are constrained by CRFs. The nonlocal constraints in CRFs are called global arity constraints, and the authors show that CRFs satisfy these constraints, including nonlocal ones. The authors propose a regular-constrained CRF (RegCCRF) that replaces the CRF with a CRF that satisfies the global constraints. RegCCRFs are shown to outperform existing models on a variety of tasks, and it is shown that RegCC RFs are more robust to constraints in training than models that do not require local dependencies.   The authors also show that the constraints in the decoding can be reduced to the constraints of the RegccRFs, and show that constrained training is more effective than constrained decoding. Finally, the paper proposes a deep neural model that incorporates RegCCRRF into the training process, and uses it to perform semantic role labeling on a dataset where the constraints are not present in the original CRF’s Markov assumption, and where the output structures are non-convex. The paper shows that the RegCCRF is able to perform well on this dataset, and can be used for downstream tasks. "
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"The paper addresses the problem of camera-based physiological measurement, where the goal is to capture a person’s physiological measurements in a video.  Camera-based biomedical measurements are typically performed on a large number of sensors, and there is a large amount of data available.    The paper proposes two methods to reduce the number of preprocessing steps required in order to achieve a good performance.  Both methods are based on the observation that “endto-end” models (i.e., “core” network with a computational budget that scales linearly with the size of the dataset) can be easily replicated across multiple sensors.  The authors propose two neural models, called EfficientPhys, that are able to replicate the performance of existing neural models for camera-related physiological measurement (face detection, segmentation, normalization, color space transformation, and preprocessing).  Both models are trained on raw video frames, and the authors show that their models achieve comparable accuracy and latency to existing models.  They also show that the efficiency of a light weight network can be improved by reducing the amount of operations required to achieve the same accuracy.  In addition, the authors propose a “transfer learning” method that can be used to learn a transformer or convolutional backbone, which can be applied to any neural models that have pre processing steps that are commonly used in neural models (e.g., normalization or colorspace transformation)."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper proposes Hardware-Aware Latency Pruning (HALP) to improve the efficiency of network architecture pruning and inference speed. Structural pruning aims to reduce the latency of the network architecture while preserving the accuracy. The paper proposes a global resource allocation optimization problem for structural pruning, where the latency reduction potential and the global saliency score are computed from a latency lookup table. HALP uses HALP to compute the latency reduce potential of the latency and the saliency of each layer, and then uses a modified augmented knapsack solver to solve the problem.  The paper also proposes two metrics to evaluate the effectiveness of the proposed pruning. The first metric is the latency-aware latency-reduction potential, and HALP is shown to be able to achieve the accuracy drop when the latency is low. The second metric, the latency lookup score, is used for filter importance ranking, which can be computed from the latency table.   The authors also propose a reward maximization problem for global structural Pruning, and show that HALP outperforms prior work in terms of pruning efficacy and accuracy-efficiency trade-off.  HALP achieves state-of-the-art performance on ResNet-50/-101 pruning on ImageNet and VOC datasets for classification and detection tasks, and outperforms the prior art on SSD pruning (on VOC). HALP also improves the network throughput on VOC, and is able to perform well on classification and detections.  In addition, HALP improves the accuracy of the top-1 accuracy changes, and can be applied to improve network performance."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes a molecular graph generation method based on energy-based models (EBMs). Specifically, the authors propose GraphEBM, which is an extension of the Langevin dynamics to permutation invariance and multi-objective generation. The authors show that GraphE BM can be used for both permutation-invariant and molecule-level molecular generation.    The authors propose a novel learning strategy for learning the permutation of the energy function of a molecule, which allows for flexible degrees of permutation.  The proposed method is based on the recent development of the contrastive divergence between the learned energy function and the true energy function, which has been used in previous work.  In particular, the proposed graphEBM is able to learn a permutation invariant distribution on molecular graphs, which can be applied to compositional generation for drug discovery. "
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"This paper proposes a new approach to program synthesis, which builds upon existing approaches to combinatorial search algorithms using neural models. The key idea is to use a neural model to learn a hands-on search policy for bottom-up synthesis, where the search history and partial program executions are used to guide the search space. The search space blowup is caused by the fact that there is no structured prediction in the original search space, and the search policy is trained to find a solution that minimizes the loss on the search. The paper proposes CROSSBEAM, an approach that uses the learned neural model as a guide to learn the hands-onset search policy, and then uses this guide to find the optimal solution to the search problem.   The paper shows that the state-of-the-art is the state of the art on two domains: string manipulation and logic programming.  The authors also show that the learned-policy can be used to generate new data from the previous top-up searches, which can be then used to train a new-policy on the new data. The proposed approach is evaluated on two different problems, where it is shown to outperform the previous state- of-the art."
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper studies the problem of minimizing the squared Bellman error in Deep Reinforcement Learning. The authors propose a functional regularizer to reduce the squared bellman error. They show that target networks with lagging parameters are more efficient for training. They also show that the target networks trained with this regularization are more robust to fast-changing target Q-values. They further show that regularization with up-to-date parameters is more effective than regularization based on the current training method. Finally, they demonstrate that target-network based methods improve both sample efficiency and performance on Atari environments.    The authors also demonstrate that the proposed approach is more efficient than the standard approach of minimizing the squared Bernoulli error. "
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"This paper proposes a new type of graph neural network, called GraphSNN, which is based on the Weisfeiler-Lehman test for graph structure. This test is used to measure the similarity between the graph structure of a node and its neighborhood subgraphs. The authors show that this test can be applied to a number of existing graph neural networks (GNNs) and shows that it can be used to evaluate the structural properties of a graph. The paper also shows that GNNs trained with this test outperform existing ones on several graph classification tasks."
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a new prediction interval (PI) method for uncertainty quantification based on the retraining of neural networks (NNs) to improve the confidence level. Previous PI methods rely on retraining NNs with the standard mean squared error loss, but over-confident PIs are not well-calibrated due to the fact that they use customized loss functions with sensitive hyperparameters. The authors propose a PI3NN method to learn PIs that are well calibrated and robust to cross-entropy between confidence levels. They show that the PI methods can be improved by fine tuning to achieve a more robust and robust PIs.    The main contribution of the paper is to propose a new initialization scheme for the OOD identification challenge, which is based on an initialization scheme that is more robust to OOD samples with PIs than in-distribution samples.  The authors also propose to use root-finding algorithms to find PIs for linear combinations of PIs, and show that it can avoid the crossing issue.  Finally, the authors show that their method outperforms previous state-of-the-art approaches in terms of predictive uncertainty quality, robustness, and OLD samples identification. They also demonstrate that the method does not suffer from the issue of over-confidence in PIs and that it is robust to hyperparameter changes. "
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes a meta-learning algorithm for online learning of complex and high-dimensional problems where the input distributions are discrete. The authors propose the Fully Online MetaLearning (FOML) algorithm, which learns a model of an intelligent system that is able to adapt to new environments and environmental conditions in an online setting. This is achieved by using deep networks to learn functions such as classifiers, detectors, and trackers, and applying the learned models to a variety of applications. Meta-learning is used to guide the adaptation of a model to a new environment, and learning algorithms such as gradient descent are used to prevent slow adaptation. The paper also proposes metalearning for online problems, which is an extension of existing methods. FOML outperforms existing online learning methods on the Rainbow-MNIST, and CIFAR100 datasets. The main contribution of the paper is that they use a discrete notion of tasks that has known ground-truth task boundaries, which can be used to learn a model that is robust to changes in discrete boundaries in real-world settings. They also show that the task boundaries have ground truth knowledge.  "
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"This paper tackles the problem of molecular optimization, which is an important chemical science and engineering task for applications such as drug discovery and structural design of functional molecules. Deep generative models and combinatorial optimization methods have been widely used in recent years, but there has been a lack of work on gradient-based molecular optimizations. This paper proposes to use a graph neural network (GNN) to learn molecule structures that are locally differentiable ones. The authors propose a differentiable scaffolding tree (DST) that uses a knowledge network to learn discrete chemical structures, which are then used to guide the gradient of a molecule to be optimized. The DST can be used to learn a chemical graph structure, which can then be used as a guide to guide gradient -based optimization.    The authors show that the knowledge network is able to learn the derivatives of the derivatives and the graph parameters, and that the derivatives can be learned locally and globally. The paper also shows that the DST is robust to domain experts, and can be trained without brute-force enumeration. "
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"Personalized medical systems are becoming more and more popular in the medical community. However, there is a lack of understanding of the relationship between diseases and lab tests. This paper proposes a knowledge-augmented approach to address this problem. The idea is to use patient representation to capture sequential information from both the patient and the lab test responses. The proposed solution is evaluated on two real-world datasets, and the results show that the proposed solution achieves better prediction errors. The authors also show that graphs can be used to capture drug-lab interactions and diagnosis-lab interaction. "
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper studies the problem of domain generalization (SDG) in the setting where the target domain is not the same as the label space, but the source label space is different from the target domains. The authors propose the CrossMatch approach to tackle this problem. In particular, the authors propose to use the diversity of source domain to train a robust model that can generalize to unseen classes. They also propose a novel adversarial data augmentation strategy to improve the performance of existing SDG methods for identifying unknown classes.    The authors show that CrossMatch can be applied to any SDG method that uses a multi-binary classifier. In the OS-SDG setting, they show that the model trained with CrossMatch is able to generalize better than a model trained without CrossMatch. They further show that a consistency regularization is applied to the auxiliary samples generated by CrossMatch to encourage the model to generalise to unknown class identification.  They also show that using CrossMatch improves the performance on standard benchmark datasets and real-world applications.  Finally, they demonstrate that the proposed CrossMatch generalizes better than other SDG algorithms that do not use CrossMatch on the same benchmark datasets."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper proposes two Trust-region methods based on the Kullback-Leibler divergence for policy optimization in reinforcement learning. The authors extend the Wasserstein and Sinkhorn trust regions to policy optimization to the case where the policy distribution is a parametric distribution class and the entropic regularizer is a Lagrangian duality. They show that the proposed approaches, called Wassersteins policy optimization (WPO) and Sinkshorn policy optimization(SPO), achieve monotonic performance improvement over WPO. They also show that SPO achieves better sample insufficiency than WPO in the case of close-form policy updates due to the use of Lagrangians. They evaluate their approaches on tabular domains and robotic locomotion tasks and show that their approaches outperform existing policy gradient methods. "
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper studies the problem of forgetting in the context of learning trajectories of artificial neural networks. The authors propose a forget-and-reluar approach to tackle the issue of forgetting that is prevalent in both human and machine learning. They show that the forgetting of undesirable information during the relearning step can lead to disproportionate forgetting of desirable information, and propose a new forgetting step to remove the undesirable information from the features. They also show that iterative training in neural networks can be seen as a form of iterative learning in which the forgetting happens during the iterative stage of the training process.    The authors also provide a theoretical analysis of the forgetting-and--relearn framework for a number of popular iterative pretraining algorithms for image classification, and show that these algorithms can be understood as a combination of different forgetting operations. The paper also shows that the model is able to learn a good balance between forgetting and relearn. "
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper proposes a new setting called Batch RL, where the data-collecting process is offline-online and the agent is trained in an offline-offline setting. The authors show that RL agents trained in this offline-augmented manner outperform existing batch RL agents. They also show that in this setting, RL agents can be trained in a more efficient way than existing agents in the online setting, and that the agent can learn a sufficient coverage and a good policy.   "
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper studies the problem of domain generalization (DG) in the context of meta-learning, where the goal is to generalize well across different domains. The authors propose a novel method to tackle this problem, which is based on the Y-divergence between source-domain and target-domain samples. They show that the proposed method, Y-Divergence, is a generalization bound that is more robust to domain shift than existing generalization bounds. They also provide a theoretical analysis of the generalization performance of the proposed algorithm. "
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper considers the problem of solving PSPACE-hard planning problems, which is a special case of the NP-complete domains, where the goal is to find a solution to a set of problems that can be solved in a single step. The authors consider combinatorial search methods based on deep reinforcement learning to solve two-player games (Go and Go-like problems) that have been popularly used in the literature, such as SAT and CSP.    The authors show that existing search methods such as best-first search, Monte Carlo tree search, and other methods can be used to solve hard planning instances in an exponential combinatorially search space.  They also show that DNN-based best search on the Sokoban domain, using policy and value networks, is able to find solutions to hard instances in a polynomial number of steps.  The main contribution of the paper is that the authors provide a theoretical analysis of the cost distribution of the search algorithms in the case of heavy-tailed runtime distributions for Sokoben planning instances.  In particular, they show that the cost of a search is proportional to the number of times that the search is performed on the tail of the problem, and that the tails are exponentially sized sub-trees.  This is an interesting finding, and the authors also provide an empirical analysis that shows that a policy network trained with a value network that is trained to predict the return of the policy network during the search can be more robust to heavy tails.  Finally, the authors propose a new abstract tree model to model the tails, and show that this model is more robust than previous approaches, and can be applied to both left and right heavy tails, which are more difficult to solve due to polynomials in the search space, and to the fact that search algorithms with heavy-tail runtime distributions tend to have a similar cost distribution. They also propose random restart strategies to improve the performance of DNN -based search for the left and left heavy tails using the proposed model, and demonstrate that this improves performance over existing combinatorical solvers. "
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"This paper proposes a meta-imitation learning method, Meta-Imitation Learning (MIL), where the goal is to learn to imitate human demonstrations from robot demonstrations. The approach is based on human videos, and the authors propose a novel approach to learn a new meta-policy based on an adaptive loss. The key idea is that the robot demonstrations are trained to imitate the human demonstrations, and then the robot demonstration is used to train a new policy based on the learned human imitation behavior. The proposed method is evaluated on vision-based tasks, and is shown to outperform a baseline, and it is also shown that the meta-training phase can be extended to the case where there is no data collection."
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper studies the over-parameterized deep networks trained with gradient-based optimizers for classification and ranking problems. Adaptive optimizers, such as Adam, have been shown to improve the generalization performance of these networks when the weights of the network are tuned regularization. This paper shows that the adaptivity in the weight space is a function of the training loss, and that adaptive optimizers trained with weight decay (WD) and normal hyper-parameters tuning are more robust than SGD on the image classification domain. The paper also shows that adaptive optimization performance improves when the network weights are not tuned. "
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper proposes a new model for learning equivariance for group equivariant architectures. The authors propose to use symmetries between low and high-level features (e.g., edge orientations of the face, face poses of the camera, etc.) to learn a distribution over the group. They show that equivarient networks are able to learn partial and full equivariances, and show that Partial G-CNNs (which they call Partial Equivariant CNNs) are equivariants of the distribution. They also show that their method works for discrete groups as well as continuous groups.    The authors also demonstrate that their model is able to generalize to rotated MNIST, which is an important task for generalization, and they show that natural image classification can be learned in this way.  They also provide a theoretical analysis that shows that their Partial G - CNNs have full equivariances, and that they can be trained to learn them. "
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes a new deep latent variable model called Langevin autoencoder (LAE) based on Langevin dynamics, which is a variant of Markov chain Monte Carlo (MCMC) for approximating intractable distributions. The authors show that ALD can be used for scalable inference on large-scale datasets without datapoint-wise iterations and slow convergence, and that its can be applied to deep generative models as well. In particular, ALD is applied to generative modeling in the sense that it uses MCMC to approximate the prior distribution of the latent variable, and then uses the inference model to learn the latent variables. In addition, it can also be used to learn an unconditional distribution (an energy-based model). The authors propose to use ALD to learn a stationary distribution for each latent variable in the latent space, and use it as a prior distribution for the next latent variable.   The authors also propose a deep latent variational auto-encoder (LD) model based on ALD, and show that the ALD-based LAE is able to be used in conjunction with ALD for autoencoders, and can be trained on toy datasets. They also show that LAE outperforms non-amortized MCMC methods on three datasets (SVHN, CIFAR-10, and CelebA-HQ) for an image generation task, and on a Fréchet et al. (2018) dataset for which ALD performs better than LD.  The main contribution of this paper is the introduction of ALD as an autoencing method for generative modelling, and the development of an ALD model for deep latent variables, as well-tuned to the target distributions in both conditional and unconditional cases. The paper also shows that the proposed LAE can be combined with a latent space EBM. "
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper proposes a structured neural network for hypergraph reasoning in large domains, where the goal is to learn logical rules that can be applied to large domains. The authors propose to use hypergraph neural networks to learn the grounding of relationships using sparse tensors, and then apply a sparsification loss to the intermediate layers of the proposed SpaLoc model. SpaLoc is trained with neural networks and finite-domain quantification operations, and the authors propose a training and inference-time sub-sampling to improve the accuracy and efficiency of SpaLoc.  The authors also propose a sampling and label calibration paradigm based on an information-theoretic measure information sufficiency for the sampled sub-graphs, which is based on the information loss of sampled subgraphs.    The paper is well-written and well-motivated. The idea is interesting and the paper is clearly written.  However, the paper suffers from a lack of experiments on large-scale graphs (e.g., real-world knowledge graphs, i.e. graphs that contain a large number of entities and a large amount of information). The paper does not provide sufficient experiments on synthetic datasets, and does not compare with state-of-the-art baselines.  In addition, the authors do not provide any experimental evidence that the proposed model is able to generalize well to new domains.  Finally, the proposed method is evaluated on a number of real-life knowledge graph reasoning benchmarks, and it is shown that SpaLoc can generalize to a variety of domains, and that it generalizes well to a new domain. "
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper studies the problem of differentiable sorting and ranking. The goal is to improve the top-k classification accuracy in machine learning. The authors propose to use a probability distribution over the top k points of the probability distribution for k, and then compute the difference between top-1 and top-5 accuracies. The paper shows that models trained on ImageNet with the same number of training samples achieve similar or slightly better top 1/k accuracy than models trained with a large number of samples.  The paper also shows that for ImageNet models trained from scratch, the top 1 accuracy matches the top 5 accuracy."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a new method for solving the OT problem with sparse transport plans. The proposed method is based on the Douglas-Rachford splitting technique. The method is able to achieve speed and accuracy on both computation and accuracy. The authors show that the method can solve an approximate regularized problem with entropic regularization. The algorithm is shown to have a linear convergence rate to the solution of OT problem, and the proposed method has the same iteration complexity as the Sinkhorn method, but the method has a primal-dual stopping criterion. The paper also discusses numerical issues with the method, and shows that the algorithm can achieve faster computation times and robustness.  "
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper studies the distribution of distributions for Federated learning data. The authors propose a meta-distribution for federated learning, where the local data distributions are partitioned according to the out-of-sample gap and the participation gap. They also propose a new dataset synthesis strategy to generate realistic simulations of generalization. They show that the proposed framework can reduce performance gaps between natural and synthetic federated datasets. They further propose a semantic synthesis strategy, which can be applied to naturally-partitioned data. "
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper studies the generalization performance of pre-trained language models (PLMs) in the few-shot setting, where the goal is to improve the performance of LMs trained with self-supervision objectives. In this setting, the authors show that PLMs trained on manually/automatically created prompts are able to generalize better than those trained with pre-training techniques that do not rely on manually created prompts. The authors also show that the performance improvement is due to the use of supervised finetuning, which is a common technique in deep learning.    The authors first show that pre-trainable language models can generalize well to a wide range of tasks, and that LMs can be trained on a wide variety of tasks with different levels of model capacity and data size. They then show that, in the zero-shot learning setting, PLMs can generalise well to language understanding tasks on a few language understanding benchmarks (IMDB dataset, Amazon dataset, and GLUE) using PLMs that are trained with different pre-learning techniques. The models are based on the BERT family, and the authors further show that models trained with prompt-based learning outperform models that are not trained with prompts at all.  Finally, they show that training PLMs on a set of PLMs with prompts is beneficial in a single-task setting, in that the accuracy of the model is not affected by the number of prompts, but rather by the amount of prompts."
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,This paper proposes a new attention mechanism for tasks where the attention mechanism is trained with a sharpener module. The key idea is that it can be used to improve the alignment between the representation of the input and the output of the attention. Experiments on real-world scene text recognition datasets show that the proposed approach outperforms existing ones such as soft and hard attention. 
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper proposes a new deep reinforcement learning approach for solving combinatorial optimization problems. The authors consider the vehicle routing problem, where the goal is to find a route that maximizes the number of available vehicles in a given set of vehicles. This is a well-studied problem that has been studied in the literature for a number of years, and the authors propose a novel approach to solve it. The proposed approach is based on a supervised deep learning framework, where a learning algorithm is trained to solve the problem.  "
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a link prediction method for graph learning based on counterfactual inference. Existing methods rely on two variables: the observed graph structure (clustering effect) and the causal relationship between two variables, which is not suitable for many graph-based applications. The authors propose to combine these two variables and propose a new graph learning method. It learns representations based on both the observed and counterfactually links, and then uses it to train a model that predicts the link between two nodes in a graph. This is a natural extension of the link prediction work of [1].    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   The authors show that the proposed link learning method achieves state-of-the-art link prediction performance on several benchmark datasets. They also show that their method is able to learn causal models that are robust to global graph structural properties, and that the graph representations learned by the proposed method can be used to learn to predict links between nodes."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper studies the problem of unsupervised feature selection methods for ranking features in the presence of external guidance information. The authors propose a novel knowledge contrastive disTillation (SOFT) model, which aims to improve the performance of existing state-of-the-art features selection methods. Unsupervised features selection is an important problem in many real-world applications, and the authors propose to use a sparse attention matrix to capture the second-order relations between features, which is then used for feature selection via graph segmentation. The attention matrix is used to represent a relational graph, where each node in the graph is represented as a pair of features, and each feature is associated with a second order covariance matrix and a first-order data matrix. Experiments show that the proposed SOFT outperforms the current state of the art methods in terms of performance, and that the method is more robust to external guidance than existing methods."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"This paper proposes a multimodal variational autoencoders (VAEs) for learning a joint distribution over heterogeneous data (e.g., vision and language). The authors propose an alternative to the traditional multi-modal VAE, called Mutually supErvised MultimodAL VAE (MEME). The key idea is to learn a representation of the modalities that is invariant to the relatedness between data. This formulation is based on semisupervised VAEs. The authors show that this representation can be learned from partiallyobserved data. They also show that the resulting recognition model is robust to idiosyncratic representations from the heterogeneous representations of vision, language, and mixtures of mixtures and factorisations. Experiments show that MEME outperforms the baselines on a variety of metrics for both partial and complete observation schemes. The representations learned with mutual supervision are also shown to be more robust than approaches that do not rely on mutual supervision. "
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes an approach to explore options in deep reinforcement learning (DRL) that is motivated by intrinsic motivation. The approach is based on the idea of exploring options in a tabular setting, where the agent is given a set of options and the goal is to maximize the return of the options. The authors propose a method to select the best options from the set of available options, which they call Deep Explore Options. They show that the proposed method outperforms baselines on a number of Atari games."
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,"This paper proposes a new method for learning Hamiltonian dynamical systems. The proposed method, stiffness-aware neural network (SANN), is based on the idea that the stiffness of a dynamical system should be considered. SANN is trained to distinguish between stiff and nonstiff portions of a Hamiltonian system by using a stiff-aware index. The authors propose two time integration strategies based on classification and resampling technique to learn the dynamical characteristics of the Hamiltonian vector fields. Experiments on complex physical systems such as the three-body problem and the billiard model show that SANN outperforms state-of-the-art methods in terms of energy and accuracy. "
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper proposes to use language models to tackle two tasks: generating realistic text and synthesizing computer programs. The authors show that language models trained on scratchpads are able to perform multi-step computations, and that they can be used for both tasks with unbounded multi-stage computation. Transformers are used to perform multistep computations in this paper, and the paper shows that the models can be trained to solve multisteps computations. The paper also shows that a “scratchpad” (i.e., a set of intermediate computation steps in a sequence of integers) can be decomposed into “Scratchpads”, where each scratchpad consists of multiple intermediate computations for each program. The programs are decomposed as follows: (1) a program is decomposed in a series of programs, (2) each program is represented as a sequence, (3) all programs are represented as strings, and (4) each string is represented by an integer."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"This paper studies the problem of adversarial attacks on deep networks. The authors propose two methods to generate adversarial perturbations that can be used to fool feature-class associations. The main idea is that featurefool attacks can be seen as “copy/paste” attacks, where a natural image is replaced by a copy of the same image, but the target image is not the same as the original image. The paper shows that these attacks are effective in the presence of natural objects, and that they can also be used for targeted feature-level attacks on ImageNet scale.  The paper also shows that deep image generators and an optimization objective can be combined to generate feature-levels adversarial examples, which can be exploited to fool the featurefools.  Finally, the paper shows how to use them to generate “Copy/paste ” adversaries, and how to apply them to target targeted features of a target image.  "
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper proposes Simulated annealing (SA) which is a stochastic global optimisation technique for discrete and continuous variable problems. In particular, the proposed SA optimiser is based on two handpicked components: a neighbour proposal distribution and a temperature annealed schedule. The proposal distribution is learned using a policy that optimizes the solution quality. The proposed Neural SA is evaluated on three problems: Rosenbrock’s function, Knapsack problem, Bin Packing problem, and the Travelling Salesperson problem. The paper shows that Neural SA with hand-selected parameters outperforms the state-of-the-art SA baselines on all three problems and achieves better solution quality and wall clock time compared to existing solvers. "
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"Non-stationarity is one of the most important measurement indicators for measuring the non-stability of policy changes of agents during the learning process. This paper proposes a new notion called the δ-stigma measurement, which is a notion that measures the nonstationarity of a policy sequence. The authors propose two algorithms to compute the KL-divergence between the joint policy and a joint policy that satisfies the trust-region constraint.   The first algorithm, MAMT, is a Multi-Agent Mirror descent policy algorithm that is based on Trust region decomposition. The key idea of the algorithm is to factorize joint policy into a pairwise Markov Markov random field, and then use the policy factorization (i.e., mean-field approximation) to compute a policy divergence between two factorized policies that satisfy trust-regions constraints.  The authors show that the proposed algorithm, which they refer to as ‘trust-region decomposition dilemma’, is computationally efficient as it reduces the computational complexity of learning the joint policies’ divergence.  In addition, the authors propose an end-to-end manner to decompose the trust region of the local policies into a set of sub-policies that are shared across agents. The proposed method, called ‘MAMT’ (multi-agent mirror descent), is a multi-agent variant of the Multi-agent Mirror descent (MARS) algorithm.  MARS is a variant of MARS, which uses the message passing between two agents to learn a single-agent policies. MARS solves a non-stationary problem by using MAMMARS to compute joint policy divergence. The paper shows that MAMARS outperforms several baselines in terms of complexity on cooperative tasks. The method also outperforms baselines that do not use message passing."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper proposes a self-supervised representation learning framework for audio-visual speech. Video recordings of speech contain correlated audio and visual information. The authors propose AV-HuBERT to learn an audio - visual speech representation that can be used for lip-reading and automatic speech recognition.    The authors show that AV-huBERT achieves better WER than the state-of-the-art approach that uses transcribed video data.  The main contribution of the paper is that the speech representation is learned from multi-stream video input, and that the multimodal hidden units are learned in an end-to-end manner. The self-training is done in an unsupervised manner, and the authors show the performance of AV-uBERT on LRS3 with labeled data. The paper also shows that audio-only speech recognition with audio-video representation achieves better relative WER reduction than the standard benchmark for the task. The results are particularly strong for lip reading, where the lip-readers achieve a lip-texting WER of 0.5. "
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"This paper proposes a new RL algorithm, called ECORD, for combinatorial optimisation in real-world applications in logistics and natural sciences. Reinforcement learning (RL) is an important problem in many applications, but it can be slow and expensive due to wall-clock time. In this paper, the authors propose to use graph neural networks (GNNs) to speed up the decision step by using a pre-processing step to train a GNN, and then use a recurrent unit to perform a fast-acting exploratory phase. The authors show that using graphs is a natural choice to perform combinatorially optimisation, and can be applied to a variety of problems with pre-solved instances. ECORD is shown to outperform existing RL algorithms on the Maximum Cut problem using SOTA, and it is shown that ECORD can also outperform SOTA in terms of speed and scalability. In addition, ECORD shows that it is faster than the nearest competitor and has a smaller optimality gap. "
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper proposes a discrete variational autoencoder (VAE) method for learning discrete latent variables for the generation process of real world data. The authors propose Variational Autoencoders (VAEs) with discrete latents, which they call discrete VAEs. They show that discrete VAE training is more computationally efficient than previous ones, and that direct discrete optimization of the encoding model can be more efficient than directly optimizing the discrete nature of the latents. Discrete optimization in the variational setting is based on truncated posteriors and evolutionary algorithms, and the authors propose an approach based on gradient ascent to optimize the network weights. They also show that VAEs with binary latents can be trained with a discrete variant of VAE, and they show that the decoder network is able to learn the latent states of the VAE. They further show that direct optimization of VAEs using direct optimization can be used to train smaller networks more efficiently than amortized training for large neural networks, and direct optimization for denoising can be performed on smaller networks. Finally, the authors show that their approach can be applied to the training of large VAEs, and show that they outperform non-generative approaches in terms of performance.    The authors also provide a theoretical analysis that shows that the VAEs trained with the discrete variable can be denoised using a discrete VAational method, which is a generalization of gradient ascent. The paper also shows that a VAE trained with direct optimization performs better than VAEs when the number of latent variables is small, and shows that their sampling-based approximation, reparameterization, amortization, and reparametrization are effective for training VAEs in general. "
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"Identifying controllable aspects of the environment is an important problem for reinforcement learning agents. Previous methods rely on action-prediction, which can be problematic when the reward-based task is difficult to control. This paper proposes an unsupervised method called Controlled Effect Network (CEN), which uses counterfactual measures of blame to guide the exploration of an environment. The approach is evaluated on a reward-free and reward-constrained reinforcement learning task. The authors show that CEN can be used as an intrinsic motivator and that it can be integrated into an exploration method to learn controlled effects. They also show that it outperforms existing action-predictive models when CEN is used in combination with other exploration methods. "
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a new model compression technique called network pruning, which is an extension of existing model compression techniques such as neural architecture search and knowledge distillation, where they prune the networks to reduce the size of the networks. The authors propose structure-regularized pruning (SRP), which uses a regularization on the pruned structure to encourage the network to have a moderate model size. They show that it can be applied to SR networks and that it works well for both lightweight and larger image SR networks. They also show that filter pruning can be used to prune residual blocks in the residual blocks, and that filters that are pruned to remove unimportant filters are more likely to be used as residual for the remaining layers.  The authors show that SRP is able to improve the performance of existing imageSR networks on lightweight network SRPN-L, SRPN, and SRPN with L2 regularization.  They also demonstrate that pruned filters are able to achieve better performance when the number of filters is small.  "
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a novel model for few-shot learning from sparsely labeled novel category data. The model is based on the assumption that there is a large amount of abundantly labeled base category data in the target domain, and that the model is trained on a large number of examples from the source domain. The authors propose a novel framework to tackle the problem of large domain shift, which is common in existing methods. The proposed framework is built on top of a feature extracting backbone that is trained with contrastive loss on the source and target domains. The backbone is trained to extract features from distant domain categories, and a masking module is used for target domain classification. This backbone is then used to train a classifier on the feature extracted from the backbone. The framework is evaluated on the cross-domain few-set learning benchmark, and it is shown to outperform existing meta-learning approaches, and is also shown to generalize well to unseen domains.  "
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"This paper studies the effect of gradient descent on the generalisation performance of neural networks. The authors show that gradient descent converges to a function that depends on the implicit bias of architecture and on the number of layers in the network. They show that for networks with infinite width, the average test error is the same as for infinite width networks with Bayesian inference, and for finite width networks, the error is lower than that of chance. They also show that the generalization performance depends on both the implicit biases of architecture as well as gradient descent.    The paper also shows that for functions that depend on the function, gradient descent converge to the minimum a posteriori function. This is a result that is consistent with the observation that the NNGP posterior of a function is a function of the network architecture, and that the error of the function is the sum of the gradient of the weights of the architecture and the weight of the functions of the inputs. "
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper studies the problem of large-scale pre-trained multilingual representations for cross-lingual transfer methods with the goal of improving the transfer performance of existing cross-limbilingual transfer methods. The authors propose X-Mixup, which aims to reduce the cross-linguistic representation discrepancy between the source and target languages. X-mixup is evaluated on the XTREME benchmark, where it is shown to outperform several baselines on text understanding tasks. The paper also shows that the representation discrepancy can be reduced to zero when the number of languages is small.   The authors also show that the representations learned by X-Mixedup are transferable to other languages. "
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of Byzantine robust distributed or federated learning, where the goal is to defend against attacks on the machine learning model. The authors propose a new algorithm, called Byzantine robust bucketing, to address this problem. They show that under some realistic assumptions, robust algorithms for heterogeneous (non-iid) datasets are guaranteed to be robust to attacks under the proposed bucketing scheme. They also show that the bucketing and robust algorithms can be applied to heterogeneous datasets, and that the robust algorithms are robust to such attacks under bucketing. Finally, they prove a guaranteed convergence of the non-iID Byzantine robust problem under realistic assumptions. "
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper studies the relationship between disentanglement and multi-task learning in the context of hard parameter sharing in multi-tasks. The authors show that the disentangled representations learned by neural networks trained on automatically generated supervised tasks can be used to learn representations that are transferable across tasks. They also show that disentangling representations can be learned using neural networks that have been trained on a large number of tasks, and that they can be shared across different tasks. The paper also shows that the performance of disentangler is improved when the number of task-specific parameters is shared across tasks, which is a common practice in the multi-target learning setting.   The authors also provide a set of metrics to evaluate the performance on disentangle the representations learned during multi- task neural network training, and show that hard parameters sharing can improve the performance. "
SP:9851adb72e2918780f661f83f7da06eb866787be,"Certifying Robust Policies (CROP) is a framework for state level robustness certification for reinforcement learning under adversarial state perturbations. The paper proposes three robustness certified criteria: robustness to cumulative rewards, per-state actions, and lower bound of cumulative rewards. The robustness is certified by a policy trained with Gaussian noise on the Q-functions of the policy, and the policy is then used to train a local smoothing algorithm to improve the robustness of a finite-horizon cumulative reward against adversarial states perturbation. The authors also propose a global smoothing method to improve robustness in the presence of adversarial attacks.  The authors show that the proposed local smooth approach can achieve tight certification bounds on the reward using an adaptive search. The proposed methods are evaluated on a number of Atari games, and compared with other methods for empirically robust RL (adversarial training, regularization, and adversarial regularization). RegPGD, RegCVX, and RadialRL are shown to achieve certified robustness on the Atari games.    The paper is well-written and well-motivated. "
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a novel approach to conformal prediction for model uncertainty based on a calibrated candidate set. Conformal prediction is defined as the prediction of a set of candidate sets that satisfy a coverage property, i.e., the set of candidates that cover the entire training set. The paper shows that conformal sets are robust to noisy candidates, and that the coverage property of conformal predictions is robust to noise in the training data.  The paper also shows that in large-scale settings, the proposed approach is able to achieve a true positive rate of 0.5% in the presence of noisy candidates. This is achieved by imposing a constraint on the number of candidate set that are allowed to be in the conformal set, which is a constraint that ensures that the number (number of) conformal pairs is equal to the coverage and the precision of the candidate sets. This constraint is enforced in a user-specified tolerance. The proposed algorithm is tested on classification tasks in natural language processing, computer vision, and computational chemistry, and is applied to drug discovery through in-silico screening. "
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper studies the complexity of functions defined by a neural network in terms of the curve of outputs. The authors show that a network with a unit-length curve is equivalent to a ReLU network with random initialization. They also provide upper bounds on the expected length distortion of the network, and show that ReLU networks with a random initialization have higher moments of the length distortion and higher moments for distortion of higher-dimensional volumes. "
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes a safety-aware policy learning algorithm called SAFEty skill pRiors (SAFER) that learns a set of safety primitive skills that can be used to guide policy learning in the presence of unsafe environments. The safety primitive skill is learned by contrastive learning, where the safety skill is used as a prior over the policy. The paper shows that SAFER is able to learn safe primitive skills in the absence of offline data. The authors also show that the proposed algorithm can be combined with existing RL algorithms to improve the safety of the learned policies."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a multi-branch restoration model inspired by the Human Visual System (i.e., Retinal Ganglion Cells) to tackle restoration tasks in autonomous cars. Image restoration is an important problem that is often overlooked in learning based restoration methods. The authors propose a new multi- branch architecture called CMFNet, which is evaluated on three datasets: image dehazing, deraindrop and deblurring. The results show that pretrained models are able to achieve state-of-the-art performance on the restoration tasks, and that the generalization is robust to the number of branches. "
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper studies the problem of federated learning (FL) and personalized learning (PFL) in the context of data heterogeneity. In FL and PFL, the model is trained on unlabeled clients, and the goal is to train personalized models that are robust to data heterogeneity between clients. However, in FL, there is a large amount of labeled data, and there are many clients with large domain shift. The paper proposes an approach called IT-PFLHN, which extends the approach of FL to Personalized Federated learning with data heterogeneity by introducing a hypernetwork module and an encoder module. The hypernetwork is used to learn a personalized model for each client, which is then used to train the prediction service. The encoder network is used for the task-specific case, while the hypernetwork consists of a client representation and the client representation of the other clients. The proposed approach is evaluated on several benchmark datasets, and compared to FL and standard PFL methods. The results show that the proposed approach outperforms the state-of-the-art on multi-task learning and domain adaptation, and that it is more robust to multi-tasks than other FL methods. In addition, the paper shows that the learning setup does not suffer from generalization error and that data privacy is preserved.   "
SP:960d0a63a82593f6e72275b65f0501f0469d1924,"This paper proposes a conditional diffusion based generative model (RCDM) to learn representations that can be used for classification in selfsupervised learning. The authors show that the proposed model outperforms other generative models in terms of generation quality. The paper also proposes a tool to train self-supervised models to learn such representations. The key idea is to learn a SSL (backbone) representation of the input image, and then use the SSL projector embedding for various tasks (e.g. classifications, image manipulation, etc.). The SSL model learns the inherent structure of the image manipulation using the SSL model, and the representation is then used as a representation for the downstream tasks.  The authors demonstrate that the learned SSL representations are more interpretable than the supervised representation and the SSL representation, and that the generated images are more likely to have inherent structure.   The paper is well-written and easy to follow. "
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"This paper proposes a new streaming algorithm for frequency moments estimation, called Fp sketch, which is a well-celebrated streaming algorithm. Compared to DP baselines, the proposed non-private baseline is based on a logarithmic factor, and the authors show that Fp sketched in polylogarithmically space has a better differential privacy guarantee on the accuracy. The authors also provide an evaluation code for the proposed algorithm. "
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes a new paradigm called Reward-Switching Policy Optimization (RSPO) to learn diverse strategies in complex RL environments. RSPO optimizes both extrinsic and intrinsic rewards based on a trajectory-based novelty measurement that is used to guide the optimization process. The intrinsic diversity reward encourages exploration, while the extrinsiative reward encourages the learning policy to explore the state-action pairs that are more likely to be explored by the sampled trajectory. The authors show that by using policy optimization to optimize the intrinsic reward, RSPOP can learn trajectories that are diverse across different policies. They demonstrate the effectiveness of the proposed strategies using experiments on single-agent particle-world tasks, MuJoCo continuous control, multi-agent stag-hunt games, and StarCraftII challenges. "
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models are one of the most commonly used generative models in the literature, and the authors show that GANs have better sample quality than autoregressive models in terms of likelihood scores. They also show that diffusion models can be trained with flexible non-Markovian samplers, such as Generalized Gaussian Diffusion Models (GGDM). The authors propose a method to train fast sampler of the diffusion model with fast sampliers. They show that the GGDM sampler has higher degrees of freedom than the DDPM/DDIM baselines, and that the sample quality scores obtained by gradient descent can be used to train GGDM Samplers. The authors also propose a sampling process to speed up the optimization procedure by using the reparametrization trick and gradient rematerialization. They demonstrate that DDSS is able to achieve state-of-the-art performance for unconditional image generation on two datasets (FID scores and LSUN). They also demonstrate that the proposed method can be combined with a pre-trained diffusion model, and fine-tuning and re-training can be performed in parallel. "
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"This paper proposes P-Adapters, lightweight models that replace the embedding layer in standard LLMs with continuous ones. They leverage the LLM embeddings of a given LLM with continuous prompts from continuous prompts generated by Mixture of Experts (MoE) models. They also use a classifier trained on human-annotated data to generate natural language prompts. The authors show that P-adapters outperform MoE models for factual information from Large Language Models (LLMs) in terms of precision and consistency, and outperform a baseline trained on natural language queries. The paper also shows that PAdapters can be used to generate a natural language prompt from LLM’s embeddeddings. "
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes a new continual learning method for continuous classification of time series. The proposed method, called Adaptive Importance-based Continual Classification of Time Series (ACCTS), is based on the idea of multi-distribution multi-class learning, which is an important problem in real-world applications. In particular, the authors consider the problem of catastrophic forgetting and overfitting in the continual learning setting. The authors propose a new method that adapts the model training policy to adapt to the changing distribution of the time series in a continual learning fashion.  "
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes to use internal representations of past inputs to improve the performance of language models. Language models have been shown to perform well on many benchmarks and tasks (e.g., generic webtext (C4) and books (PG-19)) using language modeling based on approximate kNN lookup in memory. This paper proposes a new model based on these theorems and shows that the proposed model performs well on these tasks."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,This paper proposes a new masked language modeling (MLM) objective for training models with a masked language modelling (MLMs) objective. The authors propose to use energy-based sequence models to train MLMs to learn the probability distribution over the energy parametrizations of a sequence of words. They show that the MLMs trained with such MLMs are able to learn to learn such the energy parameters of the sequence. They also propose a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm.    The authors also show that using masked conditionals to train the masked language models can be used to learn more generalizable and tractable parametrized language models. They demonstrate that the proposed approach is more tractable than existing undirected generation approaches and can be applied to open-ended unconditional generation based on energybased models. The paper also shows that the stationary distribution of a Markov chain with respect to a stationary distribution can be learned using the proposed sampling algorithm. They further show that their approach can be extended to machine translation. 
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper studies the problem of data augmentation in deep neural networks for NLP tasks. It focuses on the problem where the number of labeled samples (in low-data or classimbalanced regimes) is limited (i.e., there are not enough informative training signals) and the data augmentation policy is biased. The authors propose a sample re-weighting scheme to improve the performance of the model. They show that learning-based augmentation outperforms other augmentation schemes that rely on parameter tuning and inherent randomness. They also show that the augmentation strategy for a given task can be learned from a learning data augmentmentation policy. Finally, they propose an augmentation policy that uses a reward function that encourages the policy to learn a good augmentation for a specific task. They evaluate their method on text classification tasks and show that their method outperforms existing augmentation techniques on a number of different augmentations for the same task.  "
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"Meta-learning is a popular technique for offline reinforcement learning (OMRL) where the task identity is learned from an augmented state. However, existing RL algorithms do not take into account the fact that the context-based encoder may not be well-suited for the task at hand. In this paper, the authors propose to use an intra-task attention mechanism and inter-task contrastive learning objectives to improve the performance of offline RL algorithms. The authors show that sparse reward and distribution shift can improve the task representation learning in the presence of sparse reward. They also show that the SOTA OMRL algorithms (e.g., FOCAL) outperform prior algorithms on meta-RL benchmarks."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"This paper proposes an inference-time improvement framework called belief fine-tuning (BFT) to improve the performance of parametric sequential generative modeling methods for the problem setting of learning a belief state in a partially observable Markov system. Previous methods for belief state modeling in multi-agent settings are limited to the case where the dynamics model is learned in a single agent and the policies in the belief model are learned over multiple agents. BFT uses approximate dynamic programming to fine-tune the model parameters during training, and it can be applied to any model. It improves the accuracy of a belief model trained with the same number of agents. The authors show that BFT can improve the accuracy in the case of approximate public belief state search in imperfect-information games using BFT.   "
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper studies the problem of overparameterized neural networks. Sparse model training has been a hot topic in recent years, but the computational cost has been prohibitively expensive due to the large number of sparse matrices required to achieve a good accuracy loss and slow training runtime. The authors propose two methods to improve the accuracy loss of these methods. The first method, Pixelated Butterfly, uses a fixed sparsity mask over a fixed structure (product of products of butterfly matrices) of the matrices, which is the hardware for butterfly (block and flat) matrices. The second method uses the same method to sparsify the network layers (attention, MLP, etc.) by using a flat block butterfly and low-rank matrices to obtain a more efficient method.   The authors show that the proposed method is able to achieve comparable accuracy to the original butterfly, but with much less computational cost. They also show that their method can be applied to other network layers such as attention and MLP as well. They show that dense MLP-Mixer, Vision Transformer, and GPT-2 medium outperform sparse models on ImageNet classification and WikiText-103 language modeling tasks. The paper also shows that the method can also be used to speed up training when the number of model components is small.  The paper is well-written and well-motivated, and the paper is clearly written. However, there is a lack of discussion about the accuracy – efficiency tradeoffs, and it is not clear to me whether the paper has made any contributions to the community. I would appreciate it if the authors could provide more discussion about why their method works, and how the fixed structure of the dense matrices is a good choice, and if the paper can provide a more detailed analysis of their method. I think the paper could benefit from a more in-depth discussion about how the flat and low rank matrices are used, and whether the flat matrices can be used as a basis for the method. Also, it would be good to see more discussion of the effect of the size of the network on the accuracy of the sparse models."
SP:136e31054a55abca840f6478491972023c2296cb,This paper proposes a new generative model for conditional diffusion probabilistic models. Score-based generative models are a general class of models that can be seen as a Markov chain. The authors propose a new formulation for controllable generation based on score matching between the data distribution and the class center of a conditional diffusion model. The class center in the forward and reverse process is the class of the data. The proposed method is based on faster sampling and is able to avoid the class clustering phenomenon. Experiments on CIFAR-10 show that the proposed framework outperforms state-of-the-art methods in terms of inception score and FID score for conditional image generation.
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper studies the generalization performance of domain generalization (DG) approaches. The authors propose a new method, called LASSO, to improve the performance of existing DG approaches by learning a set of latent sub-spaces with label-informative features for each label prediction task. They show that the proposed method outperforms existing methods in terms of generalization capacity on a number of benchmark datasets."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper proposes a novel kernel thinning (KT) algorithm for estimating the probability distribution of a kernel in the reproducing kernel Hilbert space (RKHS). Unlike independent sampling in reproducing kernels, the kernel of a square-root kernel can be represented as a fractional power kernel. The authors show that KT can approximate the RKHS using a fraction of the square-roots of the kernel and the corresponding distribution. They show that using KT yields dimension-free guarantees for the kernel. They also show that for analytic kernels (Gaussian, inverse multiquadric, sinc) and non-smooth kernels (Laplace, Matérn, etc.), the proposed target KT achieves maximum mean discrepancy (MMD) guarantees, which is better than square-regret KT, which achieves MMD guarantees for all analytic kernels. Finally, they show that the integration error of target KT and KT+ is the same as that of square-resolutions and square-versus, and KT can be used to approximate both target and power kernels.   The authors also provide individual function guarantees for target KT, showing that KT is able to approximate the differential equation posteriors of both the target and the power kernels, and that KT+ achieves a lower integration error than square root KT."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"Combinatorial optimization is an important problem in real-world problems, and graph neural networks (GNNs) have recently become popular as machine learning-based solvers. This paper presents an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, with both weighted and unweighted variants of the benchmark suite. The authors propose a guided tree search algorithm based on this benchmark suite, and show that a graph convolution network can be used as a guide for tree search to learn a solution structure from a set of random values. The algorithm is shown to improve the code quality and extensibility of the algorithm. The paper also shows that tree search with algorithmic techniques (e.g. graph kernelization) can be more efficient than classical algorithmic solvers, and that the tree search implementations can be competitive with existing solvers when the problem-specific solution structure is well defined. Finally, the authors show that the competitive solution quality of a GNN trained with reinforcement learning can be achieved when the solver is trained with the GNN. "
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"Quantization is a popular technique for compressing Convolutional Neural Networks (CNNs) to save computational resources. However, quantization can be expensive and can be prohibitively expensive for image-to-image tasks such as semantic segmentation and depth prediction. This paper proposes a new approach to activation maps compression for 1 × 1 convolutions in CNNs. The authors show that the proposed approach, called WCC, can achieve better compression ratios, computational savings, and low bit quantization rates. WCC is based on the hardware-friendly Haar-wavelet transform for image compression, which can be applied to any network architecture that includes 1×1 convolution in the network architecture. The paper also shows that WCC can be combined with light quantization to achieve similar compression rates, while achieving comparable accuracy and lower compression ratios. "
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper considers the problem of learning in games with correlated and coarse correlated equilibria in normal-form games with accelerated learning dynamics. In particular, the authors consider the case where there are sequential and simultaneous moves and imperfect information, and extensive-forms games with sequential and concurrent moves. They show that the no-regret learning dynamics converges to an extensive-form correlated equilibrium (EFCE) with O(T 3/4)-approximate EFCE for a correlated distribution of play. They also provide a refined perturbation analysis based on a structured Markov chain to show the stability of certain fixed point strategies. The authors also provide an analysis of the prior rate of the accelerated dynamics, which is based on the framework of -regret."
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces from the priors of demonstrations. Reinforcement Learning (RL) and RL with demonstrations are two of the most popular exploration problems in the RL literature. However, discrete actions are more expensive to learn than continuous actions, and the maximum of the action-value function can be prohibitively expensive for dynamic programmingbased methods. This paper proposes a discrete action deep RL algorithm to solve the continuous control problem. The exploration problem is formulated as an exploration problem, where the goal is to find the optimal action in the action space. The proposed method is evaluated on three different setups: (1) Imitation Learning, (2) RL, (3) RL with play data, and (4) RL using demonstrations. In all three setups, human data is used instead of synthetic data. The experiments show that AQuADem outperforms existing continuous control methods on hard manipulation tasks, and achieves better sample efficiency. "
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper proposes a new adversarial style augmentation (AdvStyle) approach to improve domain generalization for semantic segmentation by improving the robust model performance on labeled synthetic (source) data with image style variation. AdvStyle augments hard stylized images with a style feature and uses channelwise mean to learn the style features. Then, it uses adversarial training to improve the robustness of the model trained on the adversarial image generated by the style feature. Experiments show that AdvStyle improves the robust performance of models trained on synthetic-to-real semantically segmentation benchmarks. In addition, AdvStyle is also applied to domain generalized image classification, where it improves the performance of a model trained with AdvStyle on two datasets. "
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper proposes a neuromorphic gesture analysis system with high temporal resolution for event-based gesture data from a Dynamic Vision Sensor (DVS) with rigid, dramatic gestures for recognition. The authors propose a latent space representation to capture the similarity of mid-air gesture data, which is a common problem in mid-range gesture recognition systems. The proposed approach, Hybrid GuidedVAE, is trained on the DVSGesture dataset and it learns an interpretable latent space space representation from T-SNE plots, and it is able to handle sparse, noisy inputs. The model is trained using neuromorphic hardware, and the authors show that the proposed algorithm achieves state-of-the-art classification accuracy. "
SP:2e66468a6b94177e54b0052b97713ee63902c278,"Deep learning on tabular data has been a hot topic in recent years, but the computational capacity has been limited. This paper aims to improve the accuracy of neuron-based networks. The authors propose a new annealing mechanism for S-HTE inference based on ferns, and show that it improves the accuracy on a classification and regression benchmark. They also show that S-HETE can be applied to the Internet of Things (IoT), drone, and a Natural User Interface (NUI) application.    The paper is well-written and well-motivated, and the PyTorch implementation is intuitive and easy to follow. However, there are several issues with the paper: (1) deep learning capabilities are limited, (2) the paper does not compare with other deep learning methods, and (3) the authors do not provide a clear explanation of the computational complexity.  The authors also do not explain how S-ETE is able to learn the internal representations of neurons, and whether it can be used as a way to reduce the number of neurons."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper proposes a new offline RL method called Latent Action Q-learning (LAQ) for learning value functions from undirected stateonly experience. The paper proposes tabular Q-learning to learn a value function from discrete Markov decision processes (MDPs) using a refinement of the action space. The main idea of LAQ is to use a latent-variable future prediction model to learn discrete latent actions, and then to use Q - learning to learn value functions based on discrete actions.    The paper shows that LAQ learns value functions using state-only experience, and that the value functions can be learned from ground truth actions. Value functions are learned for domain-specific low-level controllers, and LAQ can be used for the acquisition of goal-directed behavior.  In experiments on two environments, a 2D grid world and 3D visual navigation, LAQ outperforms existing imitation learning oracles and competing methods. "
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper proposes a new model parallelism method for large-scale training of deep learning models. The proposed method, called SWARM Parallelism1, is an extension of the existing SWARM algorithm, which is used to train a model on a large number of GPUs in a distributed fashion. The authors show that the proposed method is able to achieve competitive performance on a variety of tasks, and that it can be applied to a wide range of hardware architectures.   "
SP:91d2f094d5481651b554f58aecc2a6207057a47c," Offline reinforcement learning with policies on a fixed dataset is an important problem in real-world applications, but offline reinforcement learning in decentralized multi-agent reinforcement learning is challenging due to the lack of online execution and the high cost of offline experiences. The authors propose online transition correction (OTC) to address the issue of biased transition dynamics in offline experiences, where the behavior policy is more likely to be biased than the policy learned from online experiences. To address this issue, the authors propose an adaptive rank-based prioritization based on the transition dynamics of the offline and online experiences, which uses the sampling probabilities of the online and offline experiences to compute the transition similarity between transitions. They also propose two new distances, the embedding-based and valuebased distance, to measure the similarity between transition dynamics and value estimates. The paper shows that OTC improves deployment efficiency and sample efficiency in offline training and online tuning. OTC is shown to improve the performance of agent policies trained with OTC in online tuning, and to improve data efficiency in the presence of uncoordinated and suboptimal policies. The transition bias is also alleviated by the use of adaptive rank based prioritization. "
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper studies the computational footprint of Deep Neural Networks (DNNs) training with 4-bit quantization. The authors propose two methods for quantizing intermediate neural layers. The first method, called unbiased quantization (LUQ), aims to reduce the computational cost of the training process by reducing the number of multiplications in the forward and backward phase of training. The second method, dubbed unbiased quantized neural network training (UNQ), is based on the logarithmic biased quantization of neural gradients (i.e., the loss gradients of the weights of the intermediate layers). The authors show empirically that the proposed method can be applied to any low precision format and that it can be combined with existing methods for high precision fine-tuning and variance reduction method. Experiments on ResNet50 on ImageNet show that the method is able to achieve state-of-the-art performance on the low-precision format.    The authors also show that UNQ can also be used for 4- bit training. "
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"Polythetic classifications based on shared patterns of features are more robust to misclassification than monothetic classifications that rely on features that are not relevant to the task at hand. Polythetic classification is a special case of meta-learning problems where a set of functions are learned by threshold meta-learners, such as Prototypical Networks.    The paper proposes to use the embedding dimension of these functions as a proxy for the number of features in the natural world.  The main idea is to use attentional classifiers (e.g., Matching Networks) to solve these problems with linear embedding dimensions.  This approach is shown to be effective for meta-training Boolean functions.  In addition, the paper shows that attentional models can be used to address the problem of ""misclassification"", i.e., the problem that happens when task-relevant features are not shared across tasks, but task-irrelevant features can be shared across all tasks.  A selfattention feature-selection mechanism is used to select non-discriminative features, which are more likely to be useful for the task."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes a multi-agent reinforcement learning for emergent communication via reinforcement learning on a continuous acoustic channel. Human communication is typically done on a discrete acoustic channel with discrete symbols, but this paper proposes to use reinforcement learning to learn a continuous communication channel to learn emergent language. The paper proposes an environment and training methodology where the communication channel is a lossy continuous channel, and each agent is trained to communicate with the other agents in a messaging environment. The emerging language is learned based on channel characteristics, e.g., the number of channels, the amount of noise, and how the emerging language changes when the communication signal changes.   The paper also proposes a vocoder that generates a continuous waveform from the input signal, and a platform for continuous signalling for language learning based on deep reinforcement learning. The platform is trained in a similar way as deep Q-learning, but the communication is done in a continuous signal, with the difference being that the vocoder is trained on the entire communication channel rather than just a small subset of it. The communication channel has to be able to handle noise, which can be controlled through the use of a communication channel that has been tuned to a certain level of noise. The authors also propose a way to train the platform in a way that allows for continuous signals to be more robust to changes in the noise in the communication environment.  The authors show that the platform is able to learn to communicate in a language that is robust to noise in a communication environment, and that it can be trained in an unsupervised way. They also show that their platform can learn concept combinations that are useful for communication."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"This paper studies the problem of backdoor attacks on NLP models. The authors propose a new backdoor attack, called BadPre, that can be applied to any pre-trained NLP model. They show that BadPre can be used to attack any language models on any downstream language tasks. They also show that NLP backdoor attacks are transferrable to other tasks. "
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper studies the problem of reward-free, unsupervised discovery of skills in an evolving or expanding environment. Reward-free is defined as a setting where the agent does not have access to hand-designing rewards, and the goal is to learn a set of skills that can be used to solve a new task. The paper shows that skill pre-training methods are superior to existing RL techniques in this setting, and that skill discovery in this evolving environment is similar to that in stationary environments.   The paper proposes a framework for skill discovery that is based on the observation that skill learning can be applied to either a single or an evolving/expanding environment. The authors show that in the evolving environment, skill discovery can be performed without any task supervision. They also show that the agent dynamics of the agent is able to adapt to the environment and learn new learned skills.  They show that incremental skills learned in both evolving and static environments outperform state-of-the-art skill discovery methods in terms of skill quality, and they show that their methods are more robust to changes in the environment dynamics.  The authors also provide a theoretical analysis of their framework, showing that their method is more robust than existing methods and that it is more stable than them.  Finally, the authors provide some empirical evidence that their framework is more resilient to the change in environment dynamics and that the learned skills are more transferable to new tasks. "
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"Convolutional neural networks with regular quadrilateral convolution kernels are able to learn features that are similar to the features of small convolution kernel, but with small local receptive fields. The authors show that models with small convolutions kernels can learn features similar to those of models with large convolutions. They show that LPSC is able to capture local spatial structures such as relative directions, logarithmic distances, etc.   The authors also show that a single-layer receptive field can be learned with LPSc, and that the network architecture can be decomposed into a set of convolutions that share the same local receptive field. They further show that the convolution can be trained with a convolution with log-polar space pooling, which is a simple modification of the convolutions in the original network architecture.  The paper is well-written and well-motivated, and the experiments on a variety of tasks demonstrate the effectiveness of the proposed method. "
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper provides non-vacuous bounds for the generalization performance of NNs under the PAC-Bayes theorem. In particular, the authors show that under the IIW-based information bottleneck, NNs’ training converges to a stationary point. The authors also provide an algorithm for the approximation of IIW.   The paper is well-written and well-motivated, and the results are interesting. However, there are a few issues that need to be addressed in ML research. First, IIW’s property in deep learning is not well-studied. Second, there is no connection between IIW compression and generalization. Third, the information complexity of the NNs (e.g., PIB) is not known. Finally, the paper does not consider the effect of varying batch sizes, overparameterization, and noisy labels.  The authors propose a MCMC-based algorithm to approximate the optimal weight posterior of the PIB, and show that compressing phase transition is the best way to do so."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"Indiscriminate data poisoning attacks are imperceptible perturbations that can be used to fool pre-trained feature extractors. These attacks are motivated by the fact that the perturbation is linear separable, and that they can be applied on an imperceptibly scale. This paper studies the shortcut learning problem, and shows that these attacks can be made with linear separability. The paper also shows that the synthetic perturbed data is more powerful than the deliberately crafted attacks.    The main contribution of this paper is that the authors show that shortcuts to deep models can be learned in a way that is imperceptable to the attacker. The authors also show that the imperceptibility of these perturbative attacks is not due to the normal features of the model, but rather that they are caused by the use of a shortcut learning algorithm."
SP:7b50be406138ad01db3ee112899f622637896fe9,"Offline policy optimization is an important problem in real-world decisionmaking problems. In this paper, the authors propose a new estimator, Importance sampling, for offline policy evaluation based on function approximations for value functions and process models. The authors also propose an algorithm that alleviates the overfitting phenomenon of the importance weighted return under the per-state-neighborhood normalization condition. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset, and compared with other batch reinforcement learning algorithms. The results show that the proposed method outperforms the baselines in terms of overfitting.    The paper is well-written, well-motivated, and easy to follow. However, there are a few issues with the presentation and presentation of the paper that prevent me from recommending the paper be accepted as a must-read for online learning. I have a few questions about the details of the approach and the experimental results. "
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper proposes a new model called CoLLIE for continual learning, which is a model that learns a multimodal embedding model across multiple languages and images. The key idea is to use a transformation function to learn language embeddings for both language and images, and then use this transformation function for new language use. The authors show that this model can be used for few-shot learning as well as continual learning. They also show that the images in the semantic space can be transformed into similar language use, and that the model is able to learn from zero-shot to continual learning better than a previous model. "
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper proposes a new benchmark for object captioning (NOC), which is based on the observation that existing image captioning models (e.g., BERT and CLIP) are not general enough to generalize well to visual data. The authors propose VLAF2, which combines Visual-Linguistic Adequacy, Fidelity, Fluency, and Fluency with Linguistic Linguistics. The proposed framework is evaluated on the nocaps dataset, and the authors show that models trained with intrinsic language knowledge can generalize better than models trained without any caption annotations. They also show that the proposed method outperforms the SPICE scores of human baseline on a variety of caption evaluation metrics, and that their method is more robust to changes in fluency, fidelity, and fidelity to object captions.   The authors also provide a quantitative and qualitative analysis that shows that their model performs better in terms of fluency and fidelity in the presence of changes to the caption annotations, and better in the absence of changes in the captioning annotations. The paper also shows that the model is able to learn object captionings that generalize to unseen objects in the scene, which is a significant contribution to the current state of the art in the field.  The paper is well-written and well-motivated, and it is clear that the authors have done a lot of work to improve the state-of-the-art visual/language models."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies few-shot learning problems, where the goal is to learn representations that are transferable to a new task. The authors propose two special-purpose algorithms to solve these problems. First, the authors show that the representations learned by a classifier can be transferred to new tasks. Second, they show that a clustering property (i.e., neural collapse) of the features learned by overparameterized classification networks can be used as a proxy for the transferability of the learned representations to a different task.  The authors also show that transfer learning can be performed using the learned feature maps from foundation models that are trained to learn the representations for classification.   "
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"Point cloud reconstruction is an important problem in 3D scanning. This paper proposes two tasks for point cloud reconstruction: densification and denoising. Point cloud can be represented as discrete voxels, and densification can be modeled as densities of 3D points. The authors propose a deep pointcloud reconstruction network that consists of two stages: 1) a 3D sparse stacked-hourglass network for densification, and 2) a refinement network that uses transformers for refinement. They also introduce a module called amplified positional encoding, which is a module that can be applied to any transformer. The module is used to learn positional encoding vectors based on the points’ distances for adaptive refinements. The network is evaluated on ScanNet, ICL-NUIM, and ShapeNetPart datasets, and the network is shown to be able to reconstruct real-world and unmet scenes."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"Graph Convolutional Networks (GCNs) are an important architecture for graph-structured data. However, large-scale GCNs are expensive to train due to the communication overhead between each partitioned subgraph and each GCN layer. In this paper, the authors consider the problem of communicating node features and feature gradients across different partitions of the graph, which can be problematic for training efficiency and model scalability. The authors propose a theoretical convergence guarantee for GCN training under the assumption that each partition is partitioned into multiple subgraphs and that each subgraph has a different number of nodes. They show that the convergence rate of GCN learning with stale features and stale feature gradientients is O(1/\sqrt{T}^T) when the intra-partition computation is intractable, and O(T^T log T) for large-sized partitions.  The authors then propose a new algorithm, called PipeGCN, to address the issue of stale features in GCN and propose a smoothing method to improve the convergence of distributed GCN during training. They also provide a theoretical analysis of their algorithm and show the convergence. Finally, they show that with the proposed smoothing algorithm, Pipe GCN achieves a convergence rate that matches or outperforms the convergence result of vanilla distributed GCNs. They further show that their algorithm can achieve better training throughput and improve the accuracy compared to other full-graph training methods. "
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper studies the problem of test time robustification, i.e., the problem that deep neural networks trained on in-distribution test points are more robust to distribution shift than trained on test points outside of the training distribution. The authors propose two methods for test time adaptation: (1) augmenting the model during the model training process with data augmentations, and (2) adapting the model parameters at test time to be robust to test time distribution shift.  The authors show that their approach outperforms prior augmentation and adaptation strategies in terms of accuracy and robustness in a number of applications. They also show that under certain assumptions, their approach is more robust than prior model evaluation, and that the model robustness does not depend on the number of augmentations used.  Experiments are conducted on ImageNet-C, ImageNet -R, and Image-A distribution shift benchmarks, comparing against baseline ResNet models, ResNet-50 models, and a robust vision transformer model."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper studies the problem of learning a model for RL and planning from a template. The authors propose a new algorithm (MnM) that learns a model and a policy from a single objective. They show that the model and the policy can be learned from the single objective, and that they can be trained in a way that improves the accuracy of the learned models. The lower bound on the expected return of the lower bound is based on the global lower bound of the objective of the two models, and it is shown that this bound depends on the number of transitions that the RL agent is allowed to explore. The paper also shows that the objective mismatch between the learned model and policy is alleviated by joint optimization, and the authors show that this algorithm (MNM) outperforms the GAN in terms of MSE. They also show that a classifier can be used to distinguish between real and fake transitions, and show that their algorithm can be applied to any RL agent."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a model combination approach for learning a control policy from a single observation. The approach is based on the observation history of the agent, which is used to learn a set of behavioral cloning policies from single observations and observation histories. The authors show that this approach is more robust than baselines that rely on instantaneous observations. They also show that the learned policy can be fine-tuned using human decision making.    The authors evaluate this approach on CARLA autonomous driving on images and MuJoCo continuous control tasks and show that it outperforms baselines.  Behavioral cloning with single observation (BC-SO) and observation history (BC -OH) is the main contribution of this paper. Attention maps of baseline imitation methods and the proposed method are used to train a “coupled coarse-to-fine” imitator, which learns a coarse action from an instantaneous observation and fine-tunes it using historical information.  The paper also shows that the method can be used to fine-train a coarse-tune a ‘fine’ imitation-learned policies. The method is tested on the CARLA driving task and shows that it can learn a policy that is robust to visual cues. "
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a meta-learning approach to improve the generalization performance of deep learning models for dynamics forecasting. The authors propose a model-based meta learning method called DyAd, which is based on the idea that systems that are subject to external forces and/or boundary conditions should be able to learn from them and generalize to new tasks. DyAd consists of two parts: (1) an encoder that learns time-invariant hidden features, and (2) a forecaster that learns a shared dynamics between the encoder and the forecaster. The encoder is trained with weak supervision, and the two parts of DyAd share the same encoder for each task. The forecaster uses adaptive instance normalization and adaptive padding to make the encode and forecaster more robust to changes in the environment during inference. The proposed procedure is shown to have a lower generalization error than existing approaches. They also show that the model is able to generalize better than previous approaches."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"Monocular 3D object detection is an important problem in 3D scene understanding due to the ill-posed nature of monocular imagery. In this paper, the authors propose to use manually annotated 3D box labels for LiDAR point clouds to improve the performance of existing 3D detection methods. The authors argue that the annotation process can be seen as a weakly supervised monocular3D detection, where only a small number of 2D boxes in the image are annotated. To solve the learning problem, a network is trained to predict 3D boxes based on the 3D alignment loss between the original image and the trained network, and the corresponding 3Dbox labels. The proposed method is evaluated on KITTI, and compared with fully supervised methods.   "
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper proposes a new model, called CHARFORMER, to tackle the problem of rigid subword tokenization algorithms for models for natural language processing. The key idea is to introduce a model inductive bias towards subwordtokenization, where the latent subword representations at the byte level are encoded as characters, and the block scoring network (GBST) of a deep Transformer model is used to compute GBST at the subword level. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed model outperforms other byte-level baselines as well as subword-based models. The competitive quality is also improved."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper studies the problem of detecting backdoor attacks on deep neural networks (DNNs) from the perspective of adversarial attacks. The authors propose a novel adversarial objective for backdoor detection based on extreme value analysis (AEVA) and show that AEVA can be used to identify backdoors in black-box neural networks. AEVA is based on the observation that the singularity of the adversarial map of a backdoorinfected example is correlated with the predictive confidence of the DNN. The paper also shows that adversarial singularity phenomenon is also observed in real-world applications such as on-device deployed DNNs.   The paper proposes an optimization perspective to tackle the problem and proposes a novel solution based on an adversarial loss that maximizes the loss of the solution under a skewed distribution. The proposed approach is tested on a variety of backdoor attacks and shows that the proposed approach outperforms existing backdoor detection methods. In addition, the paper also proposes an alternative approach to detect backdoor attacks in the case that the poisoned training data is not available. The main contribution of the paper is the introduction of an approach based on adversarial extreme values analysis to detect backdoors. The approach relies on monte-carlo gradient estimation and the use of the extreme values of a DNN to find a solution that minimizes the risk of a given backdoor infected example. Experiments show that the approach is effective in detecting backdoors even in black box hard-label scenarios. "
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new class-wise uncertainty measure for out-of-distribution (OOD) data, called Kullback-Leibler divergence (KLoS). KLoS is based on the idea that the class-probability simplex of a classifier can be approximated by a second-order uncertainty measure. The authors show that KLoLS is more robust to class confusion than other second order uncertainty measures, and that it can be used to improve the class confusion of OOD training data.   The authors also show that the evidential training objective can be further improved by incorporating distributional information into the training objective.  The main contribution of the paper is that the authors propose to use evidential models to learn a secondorder uncertainty representation for OOD data, and then use the uncertainty estimation for classifier training.  In addition, the authors introduce an auxiliary neural network called KloSNet, which is a refined version of the original K LoSNet and is trained on the same OOD dataset.  Experiments are conducted to validate the effectiveness of the proposed method. In particular, they show that under certain conditions, KLoNet is able to achieve a class-wide divergence measure that is more sensitive to class-wise divergence measure on in-distributions samples. They also show empirically that the proposed KLoNet can be applied to improve class confusion and class confusion.  Finally, they demonstrate that the KLoSNet can improve the performance of misclassifications and OOD samples on misclassified and out of distributional OOD datasets. "
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper proposes a semi-supervised algorithm to learn a linear classifier with datadependent features from unlabeled data. The authors consider natural image data as natural distributional assumptions for training CNNs. They show that the algorithm can be used to train CNNs with natural distributionals under natural distribution. The algorithm is based on the dimension of the patch distribution, and it is shown that it can be applied to any class of convolutional networks (CNN). They also provide a lower bound on the number of patches that are needed to train the algorithm. The patches are constructed on a low-dimensional manifold, and the authors show that this is possible due to the low-dimension structure of the distribution of patches."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"Face clustering on face images is an important problem, and many existing methods rely on graph convolutional networks (GCNs). However, the representation capacity of Graph Convolutional Networks (GCN) is limited due to the structure space, which limits the expressive power of GCN-based methods for face graphs. This paper proposes a new algorithm called Ada-NETS, which uses clean graphs to train GCNs. The key idea is to learn kNN relations in the feature space, and then use these relations to learn a graph with clean yet rich edges. This graph can then be used to train a GCN. The paper also proposes an adaptive neighbour discovery strategy to discover the edges that are not present in the original face image. It is shown that the noise edges are removed from the graph, and the face features are used to learn robust features from face features. Experiments on public clustering datasets are conducted to show the effectiveness of the proposed algorithm, and show that Ada-NTS outperforms state-of-the-art methods in terms of generalization.  "
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a new method for cross-data evaluation based on cross-domain representations. It leverages demographics information such as domain information and camera IDs to learn features that are robust to distribution shifts. The authors propose a new approach called distributionally robust optimization (DRO) for learning robust models. The idea is to learn a set of protected demographic features that can be used to train robust models in the presence of distribution shifts in the target domain. The proposed approach, Unit DRO, is based on a reweighted dataset where the data distributions (e.g., the uncertainty set) are re-weighted according to the protected demographics. The paper shows that the convex condition of KL DRO holds for overparameterized neural networks and that the proposed approach can be applied to any change-of-measure technique. Experiments are conducted on large-scale DG ReID and the cross-domains ReID benchmarks, and the proposed method outperforms other baselines."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks (GNNs) for molecular property prediction have been a popular topic of interest in recent years. However, there are many issues with GNNs, such as oversmoothing, overfitting, and overfitting to noise. In this paper, the authors propose a noise correction loss that aims to mitigate these issues. The noise correcting node-level loss is based on the observation that the noise in node latents can cause overfitting. The authors also propose a regulariser based on existing methods to mitigate the overfitting issue. Experiments on the Open Graph Benchmark (OGB) datasets show that Noisy Nodes and non-spatial architectures outperform existing methods. The paper also shows that generic architectures can be used for quantum chemistry. Finally, a GNN toolkit is proposed for 3D molecule property prediction. "
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper considers the set2vec problem, where the set is composed of a variable number of feature vectors, and the goal is to learn a vector representation of the set. Two approaches to learn the complex interaction between set elements, (Set)Transformers and (Set2vec) are proposed. Both approaches are based on self attention (e.g. (Set1vec, Set2vec), (SetTransformers), etc.). The authors propose a set embedding feed-forward network, which is based on ExpectationMaximization (EM) steps. The authors also propose inducing-point attention and optimal transport kernel embedding (OTKE), which is a generalization of the two approaches.   The authors show that the proposed framework, OTKE, is a single-step EM with balanced assignment constraints for the E-step, and MAP-EM steps for the MAP-E-step. They also show that a mixture distribution of i.i.d. samples from the mixture distribution can be learned in a single step, and that the marginal likelihood maximization and empirical Bayes are satisfied.  The proposed approach is evaluated on three tasks, where it is shown that OTKE outperforms the proposed approach in terms of both the computational overhead (i.e. the number of MAP-MAP steps) and the reduced computational cost (due to the use of auto-diff backpropagation).    Finally, the authors also demonstrate that the mixture set data fitting framework can be extended to the case where the mixture parameters are unknown, and show that their approach can be used to learn set embedd and prior-induced model regularization. "
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes an unsupervised feature selection for learning informative features for unknown downstream tasks in the contrastive analysis (CA) setting. In the CA setting, feature selection is performed in the presence of a background dataset, where the goal is to learn features that are informative for a given set of genes. The paper proposes a method for feature selection in this setting that is well-motivated from the machine learning community. The proposed method is evaluated on a semi-synthetic dataset and two real-world biomedical datasets, and it is shown to outperform state-of-the-art methods in both the two standard and two special cases of unsuper supervised feature selection scenarios, where genomic data is available. "
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the problem of early stopping for over-training neural networks. In particular, the authors consider linear regression models and show that linear models can be over-stopped in the presence of double-descent. They also show that the optimal early stopping time and the model dimension are the only two factors that prevent double descent. The authors further show that early stopping is beneficial for generalization in deep learning tasks."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"Policy gradient algorithms for reinforcement learning (RL) problems are known to suffer from instability due to the Shannon entropy. Regularization on entropy functions to improve stability and encourage exploration has been shown to be effective. In this paper, the authors propose a quasi-Newton method to improve the stability of the policy gradient algorithm by using entropy regularization. The proposed algorithm is a natural policy gradient (NPG) algorithm. The authors show that the proposed method can be used to regularize the entropy functions of existing policy gradient algorithms, which leads to Newton-type quadratic convergence of the algorithms. The experimental results on synthetic and industrial-scale examples demonstrate that the quasi-newton method achieves state-of-the-art performance with single-digit iterations, and is able to learn an optimal policy. "
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper studies the problem of grounded language understanding in Text-based games (TBG). Two problems are considered: generalization and sample efficiency. The authors propose to use deep reinforcement learning (RL) methods to solve TBGs using deep RL approaches. They propose a general method based on case-based reasoning, where an on-policy neural agent is trained to solve a set of TBGs with different distributional shifts. The proposed method is a combination of an existing method and a new method that learns a case-by reasoner. The approach is shown to outperform existing methods on out-of-distribution generalization. "
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"This paper proposes a two-stage method for learning multi-sense embeddings for resource-constrained systems. Pre-trained contextual language models are commonly used for language understanding tasks, and the authors propose to use the sense information to learn multi-ensembles, which is a skip-gram-like framework. Noncontextual word embedding is also considered.    The authors propose a simple two-step method based on a pre-trained language model (BERT). The first step is to learn a distribution over word senses from the output layer embedding of BERT. The second step is an approach to train a sense disambiguation mechanism in the model.  The proposed method is evaluated on contextual word similarity and sense induction tasks, where the proposed method outperforms the state-of-the-art multi-semmas embedding on multiple benchmark data sets. The authors also evaluate the performance of the proposed approach on the embedding-based topic model (ETM) and show that the proposed methods outperform existing methods in terms of polysemy."
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper investigates the transferability of computer vision models for 2D image and 3D point-cloud understanding. The authors show that a neural net model trained on the same architecture is able to transfer well to different visual representations of the physical world (e.g. point clouds and 2D images). They also show that the representations learned by human vision are transferable across different 2D model architectures, and can be used to transfer between different types of point clouds.    The authors propose a method called inflated imagepretrained models (FIP) that can transfer the transfer from a single image-pretrained model to a single point cloud model. FIP is based on the observation that the transfer between a 2D convolutional filters (2D filters) and a 3D filters (3D filters). It is shown that FIP can transfer well between point cloud models trained on different task-specific architectures. The paper also shows that models trained with different finetuning efforts are able to generalize well across different tasks, and that models with different batch normalization layers can generalize to different tasks. Finally, the paper shows that the models trained using FIP achieve state-of-the-art performance on 3Dpoint-cloud classification and few-shot classification. "
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"Autoregressive generative models have been shown to perform well on a variety of tasks on sequential data. However, they suffer from two issues: exposure bias and long-range coherence. This paper proposes a method to train an autoregressive model with an energy-based learning objective. The proposed method addresses the exposure bias problem and the temporal coherence by introducing a constraint on the joint distributions of the two distributions. The authors propose to use chain-style conditional modeling, where the energy scores of the joint distribution is computed by an autorgressive network. The energy scores are then used to train a second network to predict the energy of the first network. This second network is trained using importance sampling, which is a variant of the MCMC process.  The authors evaluate the proposed approach on three benchmarks: language modeling, neural machine translation, and image generation. "
SP:51e748c55bd4134047098559577fa3f37aa7433a,"This paper studies the problem of adversarial attacks on deep neural networks (DNNs) and proposes a new adversarial training (AT) method to improve the robustness of a DNN-based classifier against adversarial examples.   The authors propose two new AT-based methods, PGD-AT and TRADES, which use a pointwise adversary to generate a worst-case adversarial example for each classifier. The authors also propose a unified framework for Wasserstein distributional robustness, which is a generalization of existing AT methods.  In this framework, the authors show that under certain assumptions on the WESSERstein cost function and the risk functions, the proposed distributional AT algorithms are more robust than their AT counterparts. They also show that the adversarial effects of a point-wise adversary on the classifier are independent of the number of samples and the size of the class.  Finally, they provide a theoretical analysis of the performance of the distributional adversarial robustness AT - which shows that the proposed method is more robust to adversarial perturbations. "
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a new representation learning framework for multivariate time series. Unsupervised representation learning is a challenging problem in multdimensional time series due to complex dynamics and sparse annotations. The authors propose a novel contrastive learning framework, called Bilinear Temporal-Spectral Fusion (BTSF), which is a general framework that can be applied to any data augmentation techniques used in contrastive training.    The authors show that it is more efficient than the original problem when the augmentation methods are well-suited to the feature representation. For instance-level augmentation of time series using dropout in a global context and capturing long-term dependencies, the authors propose to use segment level augmentation based on time slicing, which is similar to previous work, but with a sampling bias.  The main difference is that the authors use dropout for the global context, while previous work used instance-levels.  BTSF is able to learn bilinear feature representations by using an iterative bilinearly temporal-spectral fusion module, which learns affinities between the feature representations of different time series based on cross-domain interactions.  Experiments are performed on a variety of tasks for time series, including classification, forecasting, anomaly detection, and forecasting.  Results show that the proposed method outperforms other state-of-the-art methods and outperforms them in most of the cases. The paper also shows that dropout can be used for instance-wise augmentation, which improves the performance. "
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper studies the problem of learning rate for deep neural networks. The authors propose a new algorithm to optimize the learning rate in the context of both the model architecture and the batch size. The proposed approach is based on the observation that gradient descent can be used to optimize both learning rate and the model weights. Learning rate can be optimized by the gradient descent step, but line-search can only be used if the weight gradients of the first and second order are close to each other. This paper proposes a new scheme to optimize learning rates for both first- and second-order gradients, and it is shown that it can also be used as an optimizing scheme. The method is tested on a number of datasets, and the results show that the proposed method outperforms existing methods."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a continual meta-learning method for sequential multi-task learning. Prior meta-reinforcement learning algorithms have been shown to outperform existing continual reinforcement learning algorithms, but they do not perform well when the number of tasks is limited. The authors propose a new method called continual meta meta-policy search (CoMPS), which is based on the observation that RL can be used for offline meta-training. They show that CoMPS outperforms prior continual learning and other off-policy meta-regime methods on a number of continuous control tasks."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a new “backdoor” poisoning attack against classification models. The idea is that a threat model trained on a poisoned classifier can be used to fool a different threat model to fool the poisoned classification model. The authors propose a procedure called Denoised Smoothing, where a classifier is first trained on adversarial examples, and then the smoothed adversarial images with human interaction are used as triggers to fool backdoored classifiers. Experiments on two high-resolution datasets (ImageNet and TrojAI) show that the attack is effective on both high-resolution datasets. The paper also shows that the proposed approach is more robust to backdoors in poisoned classifiers that have a secret backdoor. Finally, the authors show that their method is able to learn triggers that are more robust than a previous method that does not rely on modeling trigger distributions. "
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"Generative adversarial networks (GANs) are commonly used for content generation tasks. However, the computation is expensive to run on resource-constrained devices, and conditional GANs have been widely used as GAN compression methods. This paper studies the output discrepancy issue of unconditional GAN distillation, and proposes a novel distilling approach for the StyleGAN2 architecture. The authors propose a novel knowledge distillation losses in the heterogeneous distillation scenario, where a teacher and student model are trained with a different initialization strategy, and the student model is trained with an initialization strategy that encourages the output consistency of the teacher model. To achieve this, the authors introduce a latent-direction-based distillation loss that encourages semantic relations in the latent space to preserve the semantic consistency between the latent code and the teacher code. The style module is used to encode semantic information from the teacher to the style module. The proposed approach is evaluated on the task of distilling the original GAN from the original latent code, and is shown to outperform the state-of-the-art GANdistillation methods.   "
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a new methodology for learning offline algorithms in online settings. The authors propose a multi-task learning model to learn behavioral structures from graphs. The methodology is evaluated on synthetic data and historical stock market data.    The paper is well-written, well-motivated, and well-organized. "
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper introduces a new family of Bayesian models, called Gaussian Processes (GPs), which is a family of uncertainty estimates that can be used to improve the training and prediction performance of Bayes. Sparse GPs and variational inference for inferring q are used to approximate GPs. They can be seen as a generalization of sparse GP approximations. The paper shows that it is more efficient than other sparse variational GP approaches, and that it can be trained with a neural network to learn inducing points locations for each latent function. The authors also show that the proposed method can reduce the number of parameters of the model by a factor of at most 1.5. The proposed method is shown to be able to reduce the training time and improve the learning and prediction times for a number of learning tasks, and it is shown that the prediction performance can be improved when the latent function is sparse. "
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"This paper studies the problem of Byzantine tolerance in distributed training algorithms, which is one of the hardest problems in deep learning due to the lack of communication between the server and the clients. The authors propose a new protocol for secure (Bithmidinetolerant) decentralized training that improves the communication efficiency of the protocol. They also provide theoretical bounds on the resistance to Byzantine and Sybil attacks. They show that under certain conditions, the systems are robust to these attacks. The paper also shows that Byzantine attackers can attack both image classification and language modeling.   The authors also show that the algorithms for large-scale distributed deep learning are Byzantine-tolerant, and that it is due to a communication overhead that is proportional to the number of clients and the amount of redundant communication between them. The main contribution of the paper is that the models are robust against Byzantine attackers and that the protocol is robust to redundant communication. "
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper proposes Smoothed particle hydrodynamics (SPH) which is a mesh-free Lagrangian method for weaklyand strongly compressible turbulence in astrophysics and engineering applications. SPH informed fluid simulators based on physics based parameters and Neural Networks are universal function approximators. Neural Network parameters are used to learn the physics-based parameters of the system. The authors propose a learning algorithm for the mixed mode approach, which is based on the forward and adjoint based sensitivity analyses for gradient based optimization.  The authors also propose a physics informed learning method to solve inverse problems in a physically interpretable parameter space. The main contribution of the paper is that the authors provide a theoretical analysis of the training data, which shows that Neural Networks can be used as universal functions for the training of SPH informativeness and generalizability. The paper also shows that the learning algorithm can be applied to any learning algorithm.    The main contributions of this paper are as follows:  1. A theoretical analysis that shows that a Neural Network can be universal for the learning of the physics in SPH. 2. A new learning algorithm is proposed for the mixing of physics based and neural network parameters. 3. A physics-informed learning method is proposed to solve the inverse problems on time scales and the physical symmetries of turbulence. 4. A hierarchy of models is used to model the physical structure of the data. 5. Experiments show that the proposed method is able to achieve better interpretability and better generalization in terms (in terms of time scales, Reynolds numbers, etc.)."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"This paper proposes Mix-MaxEnt, an approach to train a deterministic neural network with an entropy maximization regularizer to improve both accuracy and uncertainty estimates. The approach is based on the cross-entropy loss, which encourages the class clusters to share the same embedding space, and the out-of-distribution samples to be clustered in high entropy regions. The authors propose synthetically generating between-cluster samples using a convex combination of images from different clusters. The solution is built upon data-dependent regularization for maximum likelihood estimation, where the entropy of the samples is proportional to the number of samples in the cluster. Mix-maxEnt is evaluated on two real-world datasets with ResNet and Wide-ResNet architectures, CIFAR-10 and CIFar-100, and is shown to improve classification accuracy. The main contribution of the paper is that the solution is able to avoid the entropy barrier in the high-density regions, and that it is robust to superficial input perturbations. It is also shown that Mix-MAXEnt can be used to improve uncertainty estimates by using calibrated probabilities for in-distributions data. "
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a new approach to learn a structure representation of a driving video, which can then be used to train an auto-encoder to generate an animator that can be applied to any driving video. The proposed approach, called Latent Image Animator (LIA), is a self-supervised autoencoder that learns a latent representation of the driving video using a GAN-based approach. The latent representation is learned by a linear combination of two modules, one that learns the structure representation and the other that learns to predict the motion of the vehicle. The authors show that the proposed approach outperforms the state-of-the-art methods on the TED-TALK and VoxCeleb datasets. "
SP:86f9f89f84e117c86478b9afaf087f65524f5472,"This paper proposes a meta-learning approach to improve the generalization performance of meta-training algorithms. Meta-learning is an important problem in many real-world scenarios, where the goal is to learn a good representation of the training data. The authors propose a new approach called data-adaptive meta-regularization (MLTI) to regularize the meta-learners to perform well on a variety of tasks. They show that MLTI improves generalization on a wide range of datasets, including image recognition, pose prediction, molecule property prediction, and medical image classification. They also show that the proposed MLTI framework outperforms state-of-the-art strategies when the number of tasks is small and the interpolation is limited.    The authors also provide a theoretical analysis of the performance of the proposed approach.  The paper is well-written and well-motivated, and the paper is easy to follow. The paper has a good amount of experiments on datasets such as image recognition and pose prediction. It also shows that the presented approach can be used to train representative meta -learning algorithms. "
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"Fair representation learning is an important problem in machine learning. This paper proposes a novel approach to improve the fairness of downstream predictors by encoding sensitive data. The authors propose Fair Normalizing Flows (FNF), which aims to reduce the unfairness of adversarial predictors. The key idea is to learn representations for sensitive attributes, which are then used to enforce fairness guarantees on the learned representations. To achieve this, the authors propose to use a normalizing flow to compute the statistical distance between the latent representations of the sensitive group and the non-sensitive group. The probability density for sensitive groups is then used as a proxy for the likelihood computation for the sensitive groups, and the encoder is trained with the normalising flow. Experiments on real-world datasets show that FNF improves group fairness notions on three properties: interpretability, transfer learning, and maximum unfairness. "
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a graph neural networks (GNNs) for learning a low-dimensional representation of subgraph patterns on graphs for graph-based tasks. The authors propose a node-centric message passing mechanism for GNNs, where nodes are represented as subgraph isomorphisms, and query graphs are treated as structured query graphs. They show that they can be used for complex structure matching, which is an important problem for isomorphism counting.   The authors introduce COUNT-GNN, a GNN which is a variant of GNN that uses an edge-centric message passing scheme at the edge level to encode information about the subgraph structure of the graph.  The key idea is that each edge in a query graph can be represented as a subgraph of the query graph, and each edge can be encoded as an edge in the original graph. The edge is then used for encoding graph structures, which are then passed through the GNN.  In the backtracking framework, each edge is encoded as the edge adjacency between two nodes in the query graphs, and then the edge is added to the graph representation at the graph level, and the matching is performed between the node and the edge.  COUNT - GNN is shown to be able to encode fine-grained structural information, and is able to reduce the computational cost for node-oriented tasks.  Experiments on several benchmark datasets show that Count- GNN outperforms several baselines, and that it can be applied to a variety of graph-level tasks. It is also shown that it is more robust to node-level changes in the graph structure, and can be trained to encode first-class citizens."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper proposes Agnostic Personalized Federated Learning (APFL) which is a variant of loosely constrained federated learning, where each client only has access to a small subset of the local data, and the goal is to train a model that is agnostic to the personalized labels of all the clients. The authors propose two labeling schemes: Label Heterogeneity (LH) and Domain Heterogeneous (DH). They show that they are agnostic when the labels are not personalized, but agnostic if they are personalized. They also propose a method called agnostic personalized Federated learning (APHFL) based on two existing ones for personalized knowledge reflection: Similarity Matching and Kernel Factorization (SimFed). The proposed method uses locally learned knowledge to compute the task-level similarity between the local labels and the labels of the clients, and then uses the locally learned labels to train the model parameters. The paper also shows that the knowledge collapse and information loss of heterogeneous knowledge can be reduced due to the dimensionlaity of parameter space, which can be alleviated by replacing the basis vectors in model parameters with local data. Experiments on both singleand multi-domain datasets show that the proposed method outperforms existing federated training methods.   "
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper proposes Object Dynamics Distillation Network (ODDN) to learn object dynamic representations from raw video input (e.g. velocity, occlusion, collision) and abstract entities. The key idea is to distill object-centric representations of scenes from static images and use it to learn dynamic representations of objects using a relation module to model object-pair interactions. The approach is evaluated on video events reasoning and video prediction tasks. The representaions are trained using existing scene representation methods. The model is trained with object dynamic clues (occlusion and objects collision) as well as object dynamics. Experiments show that the model achieves state-of-the-art performance on segmentation, reconstruction and scene decomposition quality. The paper also shows that ODDN is able to capture object-centric interactions between objects in a scene, which is important for video understanding. "
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks (GNN) have been widely used in graph-based learning tasks, but they are computationally expensive and slow to train. This paper studies the problem of link/motif prediction, which is a task where nodes in a graph are connected to each other and the task is to predict the next node in the graph. The authors propose to use positional encoding (PE) techniques, such as Laplacian Eigenmap and Deepwalk, to encode positional features in GNNs using PE. The paper provides a mathematical analysis of GNN layers, and proposes a solution called PEG, which combines node features, positional features, and rotation equivariance. They show that PEG is permutation equivariant to node features and node distance features. They also show that the complexity of PEG decreases as the number of nodes increases. They demonstrate that the proposed PEG improves link prediction performance on real-world networks, and shows that the generalization performance is improved."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes LaMer, a text style transfer framework based on large-scale language models with weak supervision. The authors show that models trained on non-parallel datasets can achieve similar transfer performance as models trained with non - parallel datasets, but without style-independent information. To achieve this, LaMer uses MLE training and imitation learning refinement to encourage intrinsic parallelism in the data. The parallel expressions of the non-parametric datasets are represented as scene graphs. The model is evaluated on the task of sentiment & formality transfer and political stance transfer, and shows that the model achieves better transfer accuracy, content preservation, and fluency. "
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"Multi-hop logical reasoning is an important problem in representation learning on knowledge graphs (KGs). It combines one-hop link prediction and logical queries. The paper proposes a hyper-relational modeling paradigm for KGs, where the algorithms are based on classical, triple-based graphs. The authors show that existing approaches for approximate query answering (QA) can be improved by using more qualifier pairs. Hyper-rel relational queries are commonly used in real-world KG applications, but this paradigm is not well suited for fine-grained context. In this paper, the authors propose a method to convert queries into hyper-ridiculous conjunctive queries, where each query is represented as a set of key-value pairs between two typed edges. The proposed method is based on Graph Neural Networks and query embedding techniques, and the authors demonstrate that the proposed method can be used to generate queries that are more likely to be answered by a given query. They also show that the query patterns generated by the proposed QA using the proposed query patterns are more similar to those generated by previous methods.   The authors also demonstrate that their method is able to solve the multi-hop reasoning problem for complex queries in hyper-referential KGs. "
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper proposes a new method called DYHPO, which is a generalization of existing Gray-box hyperparameter optimization techniques. The technique is based on Bayesian optimization for the gray-box setup. The authors propose a surrogate for Gaussian Processes that incorporates multi-budget information into the acquisition function. The learning curve dynamics of the surrogate is also incorporated into the surrogate. Experiments are conducted to show that the proposed method outperforms existing hyperparameters optimization baselines. The paper also shows that the surrogate can be used as a surrogate to improve the performance of existing multibudget search mechanisms. "
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper studies the rate-distortion of learned image compression compared to standard image coding techniques. The authors argue that the non-deterministic calculation of the quantization of the image compression is the main reason for the performance degradation in decoding and post-training quantization. They propose two training and fine-tuning based approaches to address this issue. The main contribution of the paper is to use deterministic inference to train Gaussian mixture models. The paper also proposes two methods to train image compression models in a cross-platform consistent manner, where the entropy parameters are shared across platforms. "
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images at a nanoscale resolution for imaging and analysis of cellular ultrastructure. The proposed architecture is based on gated recurrent units to remove noise from sequential data. The network is trained using fully un supervised training, where labels and/or noise-free data sets are not available and time consuming manual annotations are required. Experiments are conducted on 3D electron microscopeopy data sets, and show that the proposed network outperforms other supervised approaches in terms of empirical metrics. The paper also shows that the network is robust to imaging artifacts. "
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"Graph neural networks (GNNs) and label propagation have been a popular topic of interest in recent years, but the latter has not been studied as much as the former due to the lack of graph structure in many tasks such as node property prediction. The former uses stacked message-passing layers with neighborhood information to learn predictive embeddings, while the latter uses the latter to learn node features and labels. This paper proposes to combine the two by spreading label information between unlabeled nodes via a parameter-free diffusion process. The paper also proposes a new training pipeline that incorporates the label trick as a deterministic training objective. The main contribution of the paper is that the training pipeline incorporates the statistical properties of the features and the labels into the training objective, and that the stochastic label trick can be used as a data-fitting term to avoid label leakage issues. The regularization factor is based on the graph structure of the training data, and the paper also introduces a regularization term that encourages the GNN inputs to have the same graph structure. Experiments are conducted on the Open Graph Benchmark (OGB) leaderboard, and label trick use cases are also discussed. "
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind (ToM) is a core component of human intelligence, and machines can be seen as machines with a machine theory of mind. ToM agents are trained to solve tasks with predefined roles (e.g. speaker-listener scenarios). This paper proposes a new multiagent environment SymmToM, where agents are given a grid world, and are asked to solve a series of tasks that require them to perform well. The authors propose a strategy to learn a strategy based on the theory ofmind, and show that the strategy can be used to learn to solve the tasks in a multi-agent environment. The paper also shows that the proposed strategy can also be used in a single-agent setting.    The authors also show that in a grid-world setting, the proposed method can be combined with existing multiagent deep reinforcement learning models to learn the mental states of multiple agents. They show that this can be done by training a ToM model on the grid world and then fine-tune it on a single agent.  Finally, the authors show how the modeling of theory ofMind can be applied to multi-manual scenarios. "
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper proposes a robot vision system that can detect objects in a manufacturing environment. The key idea is to use visual data collecting and processing technology to train robots in an unsupervised learning setting. Zero-shot object detection is an important problem in unsuper supervised learning, but it is not well-studied in the context of smart manufacturing and high-mix-low-volume production. This paper proposes to train a vision system to detect objects that are not seen during the entire production process. This is a challenging problem because the production process is very different from the manufacturing setup, and there are many differences in orientations, illumination, etc.    The paper presents a zero-shot detection of daily objects in indoor scenes and indoor scenes, and shows that it is possible to detect daily objects even in the presence of multiple objects in the scene. The paper also shows that zero-shoot detection can be applied to any object size level, and that the detection of every object is possible even in a single scene.  The authors also show that the performance of the proposed method on the YCB Video Dataset is comparable to the state-of-the-art on the same dataset, and they show that it can also be used in the case where the object size is much smaller.  Finally, the paper shows that the proposed model is able to identify objects in an indoor scene from a single image, and it is also able to detect them in an outdoor scene from multiple images. "
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper tackles the problem of video prediction, i. Video prediction is a very important problem in the field of machine learning, both in terms of generating future frames and learning environment dynamics, but also in the sense of predicting high-fidelity future frames. The paper proposes a video prediction tool based on autoregressive latent video models, where the image generator model is trained to generate high-resolution (256x256) videos, and the autorespective latent video prediction model is used to predict high- fidelity future frames from the latent space of the image generation model. The authors propose to use a causal transformer model, which is a generalization of prior models, to learn a sequence of models that can be used to generate future video frames. They also propose to train a top-k sampling and data augmentation to improve the video prediction quality. The proposed method outperforms state-of-the-art approaches on several video prediction benchmarks on complex and large-scale datasets. "
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"Vision Transformers (ViTs) are well-known for their vision-specific inductive biases in image recognition. However, the ViT architecture can be seen as a special case of generative adversarial networks (GANs), where the goal is to improve the image generation performance. The authors propose a new approach, called ViT-GAN, which is based on the observation that ViT discriminators can be trained with different regularization methods than standard GANs, and that the self-attention of ViT generators is biased towards the latent and pixel mapping layers of the image. They show that existing regularization techniques can be applied to regularize the discriminators of ViTs to be more robust to the discriminator of the ViTs. The proposed approach, ViTGAN, is evaluated on two datasets (CIFAR-10, CelebA, and LSUN bedroom) and compared with standard CNNbased GAN models on all three datasets."
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper studies the problem of image generative modeling with variational autoencoders. The authors show that the sample quality of models trained with competitive likelihoods (good likelihoods) is a function of the entropy of natural image distributions with visually imperceptible information. Good likelihoods can be obtained from high-dimensional image data distributions. The paper also shows that models trained to learn competitively with these models can be trained with good sample quality. The main contribution of the paper is the observation that the modeling of visually perceptible information in the likelihood signal can lead to a good trade-off between good and bad sample quality, and that this tradeoff is more pronounced in the case of high-dimensionality of the data.    The paper is well-written, well-motivated, and well-structured. The motivation is clear. The work is well motivated, and the results are interesting. However, there are a few concerns that need to be addressed in order for the paper to be accepted as a contribution to the field of generative modelling."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models (DPMs) are a class of generative models in which the inference of DPMs is performed in an unsupervised way, and the variance of the variance in the reverse process during inference depends on the score function. This paper studies the optimal reverse variance and optimal KL divergence of a DPM. The authors propose a training-free inference framework, called Analytic-DPM, to obtain analytic forms of these two analytic forms, which is a combination of a Monte Carlo method and a pretrained score-based model. They show that the analytic form of the DPM can be derived from the training data, and that this analytic form can be used to derive lower and upper bounds on the variance. They also show that, in practice, the analytic DPM is able to recover the log-likelihood of the optimal DPM, and can also be used as an estimate of the log likelihood of the underlying score function, which can be then used to improve the efficiency of the inference.   "
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper investigates the performance of vision transformers (ViTs) over Convolutional Neural Networks (CNNs) in the context of automated medical image diagnosis. In particular, the authors consider classification, detection and segmentation tasks. They show that ViTs outperform CNNs and transformers on a number of medical imaging tasks. The authors also show that transformer-based models can be used for medical image classification in a supervised and self-supervised setting.   The authors conduct experiments on several medical image benchmark datasets and tasks, and show that the results are consistent across different medical imaging benchmark datasets. They also find that the performance on the natural image domain is comparable to CNNs on ImageNet. "
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper studies the problem of pretraining Neural Language Models (NLM) in the context of natural language understanding (NLI) tasks. In particular, the authors consider the setting where the neural architecture of a pretrained NLM is different from that of a fully-trained NLM, and show that it can be trained in a similar way as a fully trained NLM. The authors propose two NLM training heuristics for both the pretraining and fine-tuning stages: (1) pretraining example design, where semantically related non-neighboring sentences are used to guide the training, and (2) self-improving representations, where self-improvement is used to improve the quality of the learned representations. Experiments are conducted on NLM pretraining on Natural Language Understanding tasks and show improvements in sentence representations and open domain question answering abilities. "
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"This paper introduces Learning to Optimize (L2O), a method for learning to optimize a set of optimization rules. L2O models use neural networks to learn optimization rules using neural networks. The authors propose a holistic symbolic representation and analysis framework to learn the optimization rule for a given optimization procedure. They show that meta-training can be used to learn numerical rules from neural networks, and that the optimization rules learned by the neural networks can be applied to any optimization rule. They also show that the learned optimization rule can be interpreted as an optimization rule, which is useful for scalability, interpretability, and memory overhead.  The authors show that their L2o model outperforms human-designed and tuned optimizers on a number of large-scale problems, and is able to generalize well to new optimization rules that are not learned during training.    The paper is well-written, well-motivated, and well-structured. The motivation of the work is clear, and the contributions of the paper are solid. The contributions of this paper are:  1. Introducing a new way of learning a symbolic regression to learn a rule, 2. A holistic symbolic symbolic representation of the optimization process, 3. A new optimization rule is learned, and 4. A thorough analysis of the benefits and drawbacks of the proposed method, and a thorough comparison with the state-of-the-art.  This paper is a good contribution to the field of learning to optimizers, and it is an interesting contribution to improve the state of the art in the field. However, there are a few concerns that need to be addressed in the paper:  - 1. It is not clear what the contributions are in this paper.  2. There is a lack of comparison with previous work in this area.  3. There are some questions about the benefits of L2Os in terms of scalability and interpretability.  4. It would be good to see a more detailed discussion of the limitations of the previous work.  Overall, the paper is clearly written and well written. The paper has a good amount of interesting contributions. However there are some concerns about the lack of comparisons with prior work in the literature. "
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper studies the problem of provable adversarial robustness of deep neural networks (DNNs) in the setting of static supervised learning tasks (e.g., image classification). The authors consider two real-world adaptive tasks where DNNs are trained to be robust to adversarial attacks, i.e., reinforcement learning (RL) and policy optimization (policy smoothing). In RL, the goal is to train systems that are provably robust to a non-adaptive adversary. In the static setting, the authors propose two methods: (1) randomized smoothing based defenses, and (2) smoothingbased certificates. The authors show that these two methods are provable in the case of RL. In addition, they show that a defense strategy against an RL adversary can be formulated as a defense against an adaptive RL adversary, where the non-adversarial adversary is the policy.    The main contribution of the paper is the introduction of the Neyman-Pearson Lemma (Neyman et al., 2018), which states that under certain conditions, a policy can be robust against an adversarial perturbation (i.e. that the policy function is provably non-smoothed). The paper then proposes a procedure to obtain such provable robustness certificates. In order to obtain these certificates, the paper proposes a worst-case scenario in which the policy smoothing is used. The paper also shows that the robustness guarantees of the proposed method can be obtained in three environments: Cartpole, Pong, and Freeway.  The authors also provide a theoretical analysis of their method. "
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"Real-world machine learning deployments are becoming more challenging due to distribution shifts in the source and target domain. The authors propose a new method called Average Thresholded Confidence (ATC) to improve the model’s confidence in predicting the target domain accuracy. Previous methods for the problem rely on the use of both labeled source data and unlabeled target data, while ATC focuses on the problem of only using one of the two. The proposed method, called ATC, is based on the idea that the threshold for predicting accuracy is a function of the model confidence on the source domain. ATC is evaluated on a number of datasets (WILDS, ImageNet, BREEDS, CIFAR, MNIST) and model architectures, and shows that ATC outperforms prior methods on most of the datasets. The method is also tested on toy distributions, and ATC shows better performance than prior methods when the distribution shifts are due to synthetic corruptions, dataset reproduction, etc."
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper considers the problem of registration for a given transformation, which is an important task in the context of robustness against outliers and unknown non-rigid deformations. The registration problem is formulated as a partial distribution matching (PDM) problem, and the authors propose a method to solve the large scale PDM problem based on the partial Wasserstein-1 (PW) discrepancy. The proposed method is based on a neural network that is trained to minimize the PW discrepancy using the Kantorovich–Rubinstein duality. It also introduces a coherence regularizer to prevent unrealistic deformations, and a non-rigid transformations. The authors show that the proposed partial WASSERSTEIN adversarial network (WPAN) is able to minimize this PW discrepancy with the help of the neural network. The network is trained by minimizing the gradient of the loss on discrete distributions. Experiments on point set registration tasks show that PWAN outperforms state-of-the-art methods. "
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"Hyperparameter optimization (HPO) is an important problem in machine learning models, and it is important for transfer learning to be considered in the context of HPO. Previous approaches have been based on the Deep Kernel Gaussian Process surrogate, which incorporates the Landmark Meta-features (DKLM) in the hyperparameter evaluations. This paper proposes to use DKLM to capture the similarity between hyperparameters configurations across different datasets, and to use contextualized dataset-specific similarity representations based on DKLM. Experiments on three HPO meta-datasets from OpenML show that the proposed method outperforms stateof-the-art baselines in terms of performance on evaluated configurations."
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"This paper proposes a new method for detecting fakes in deep generative models. Generated data is more likely to contain fakes than real data, and the authors propose a new technology to detect deep fakes. The proposed work is based on the observation that deep fake detection methods often fail to distinguish between real and fake data. The authors propose to use a 128-bit fingerprint to identify identifiable models. They also propose a method for both deepfake detection and attribution based on this fingerprinting mechanism. They show that their technique can detect fakes even when the number of fakes is small, and that their method can be applied to a wide range of generative methods."
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"Post-hoc explanations for black box models are a common topic in classification and regression settings, and this paper studies the problem of model agnostic local explanations for similarity learners on tabular and text data. The authors propose a method to learn feature attributions for analogies in machine learning, which are then used to train a black box similarity learner. The main idea is to learn an analogy objective function that is submodular, i.e., the similarity between a sentence and a sentence encoder is equal to the complementarity between the two sentences. The model is then trained on a set of (latent) factors, and the proposed method is shown to be able to learn analogies that can be used as an explanation for any explanation in the context of machine learning. Experiments are conducted on a healthcare utilization application, where a model is trained on multiple datasets, and a healthcare provider is asked to provide an explanation based on a single dataset. Results show that the proposed approaches are able to achieve competitive performance."
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper studies the robustness of deep neural networks (DNNs) against adversarial examples. The authors propose empirical and theoretical defense approaches to certify a single ML model is robust to perturbations in the input space. They show that certified robustness for ensemble protocols can be obtained by comparing ensemble models trained with diversified gradient and confidence margin. They also provide a bounded model-smootness analysis based on the Ensemble-before-Smoothing strategy.    The authors also show that ensemble models are more robust to adversarial attacks than a single model. They further show that an ensemble model trained with DRT is more robust than the single base model. Finally, they propose a lightweight Diversity Regularized Training (DRT) for certifiably robust ensemble ML models. DRT enhanced ensembles are shown to be more robust in terms of certified L2-robustness on the ImageNet datasets, and they show that a single ensemble model can be as robust as the single model under the same model-moothness assumption. "
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper studies the expressive power of message passing Graph Neural Networks (GNNs) for learning with graphs. The authors consider the recursive pooling technique of local neighborhoods, where a model is trained to find subgraphs of the original graph. They provide strategies and lower bounds on the computational cost and expressive power for higherorder GNNs. They show that the computational complexity of the proposed method is much lower than that of existing low-order graph neural networks. They also provide a (near) matching information-theoretic lower bound on the number of subgraph to be found, which is based on the sparsity of the subgraph.   "
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper proposes two new knowledge integration (KI) methods to integrate external knowledge into pretrained language models (LMs) for factual knowledge. The authors show that KI methods are more effective than vanilla LMs in preventing catastrophic forgetting of already learned knowledge.  The authors propose two new methods, K-Adapters and ERNIE, which integrate knowledge of relations into the training of LMs. KI is a graph convolution operation, and the authors propose to use a probe model called Graph Convolution Simulator (GCS) to investigate the performance of knowledge-enhanced LMs trained with KI. The GCS model is used in the KI process and it is shown that it can be used to train two types of knowledge enhanced LMs: K-Adapter for simple relational knowledge, and K-Env for time-related knowledge. Both models are shown to be able to transfer factual knowledge across different time steps. The KI corpus is also shown to contain more complex relational knowledge than the vanilla ones.  "
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the adaptation (inner loop) learning rate for fast adaptation in MAML. The authors show that the adaptation learning rate in meta-learning with mixed linear regression is optimal for the adaptation error. They also show that optimal adaptation learning rates are optimal for minimizing the population risk of MamL. They further show that empirical risk minimization (ERM) is more effective than standard adaptation in the same number of iterations, and that the optimal adaptation loss is a function of the number of times that the adaptive learning rate is used in the inner loop. Finally, they show that in the case of initialization with an average distance of $O(1/\sqrt{n})$ to the training data, MAMM is more robust to the initialization of the initialization than ERM. "
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper studies the problem of adaptation from source-domain data to unlabelled data. The authors propose Source-free domain adaptation (SFDA) aims to train a model on unlabelling data and then adapt the model to unlabeled data in the source domain. The main contribution of this paper is to propose two methods for SFDA based on entropy-minimization techniques. The first method, Feature Restoration (BUFR), is based on the observation that the measurement system is biased in the sense that the source features of the unlabeling data are more likely to be similar to the target domain than the target data. To mitigate this issue, the authors propose a bottom-up training scheme, where the source model is trained to achieve feature-space class-separation from the target model on the labelled data, and the target network is trained on the unllabeled data. They also propose a new domain shift, called measurement shift, which is a measure of the domain shift between the source and target domains. They show that the model calibration can be improved if the target and source models share the same approximate feature distribution, and that the feature-extractor is trained so that it is able to recover the features from the source data.  The authors show that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency on both real and synthetic data. In addition, BUFR is shown to outperform the state-of-the-art of the art on both accuracy and calibration. "
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper studies the problem of adversarial training (AT) in federated learning (FL) where the goal is to train a model in a distributed learning schema, i.e., Federated learning with raw data. In FL, there are multiple FL users, each of which has a different number of resources, and each of these FL users has its own dataset. The authors show that adversarial robustness in the learning setting for non-iid users suffers from the presence of highresource users, which is a common problem in centralized learning due to adversarial learning (AT). To address this problem, the authors propose a propagation approach based on batch-normalization statistics, where the high-resource users are the ones with the highest model robustness, while the low-resource (i.e. low) users are those with the lowest. The propagation approach is based on the observation that the FL process is more robust when the number of users is small, and that it is less robust when there are more than two users. The proposed method is shown to achieve FL remarkable robustness against adversarial attacks, and it is shown that the method is robust to attacks that are more powerful than the ones that are aimed at the high resource users.   The authors also show that FL techniques can be used to improve the robustness of the low resource users to attacks. The paper also shows that the adversarial attack is more powerful when low-resourced users are not the only ones in the network.  The paper is well-written and well-motivated, and the results are interesting. However, the paper suffers from a lack of clarity about the problem that the authors are trying to address, which makes it difficult to understand the impact of the proposed method and the contribution of the paper to the community. "
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper studies the problem of learning a utility function for a game with strategic interactions. Existing methods are based on the utility function of the game. However, in this paper, the goal is to learn a mapping between the game's utility function and the network structure. To achieve this, a transformer-like architecture is proposed to learn the mapping between symmetries in the game and the equilibrium actions. The proposed method is evaluated on both synthetic and real-world data for network games and shows that the proposed method outperforms existing methods in terms of network structure inference. The paper is well-written and well-motivated, and the experiments show that the method is able to learn equilibrium actions that are robust to asymmetries and can be applied to a wide range of real-life scenarios. The authors also show that their method can be used in a variety of applications in economics and social sciences.    The main contribution of this paper is that it proposes a method for learning a mapping from equilibrium actions to equilibrium actions in a game. The idea is to use a transformer to map the equilibrium action of a game to the corresponding equilibrium actions of the other players. This is a simple idea, but it seems to work well in practice. In addition, the authors show that this method can learn to learn symmetric and non-symmetric actions in the same way as existing methods.  The paper also shows that their proposed method can also be used to learn in a more general way, and that it can be combined with existing methods to improve the performance of the proposed in the paper.  Finally, the paper shows that in some cases, their method performs better than existing methods when the game structure is not symmetric.  This is an interesting contribution to the literature, and I think this is an important contribution that can be useful to the community. "
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes a novel relation prediction framework called ANalogy SubGraph Embedding Learning (GraphANGEL) for heterogeneous graphs for relation prediction. The key idea of GraphANGEL is to learn the embeddings of subgraphs of the original graph, and then use this graph pattern as a logical rule to learn explainable predictive models. This inductive bias is used to improve generalization. The proposed model outperforms existing models on heterogeneous graph based recommendation and knowledge graph completion. The model also learns explainable heat maps of attention scores for the transductive setting. "
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper studies few-shot learning in histology images, which is a setting where there is a large number of well-labeled datasets and rare abnormal samples, but there are few natural images available (fewer than 10% of the images have manual labels). Few-shot classification is an important problem in many real clinics problems, especially cross-domain tasks. This paper proposes to use contrastive learning (CL) and latent augmentation (LA) to improve the few-shooting performance of a few shot system. The key idea of CL and LA is that CL is able to capture semantic variations between images, while LA can capture the semantic variations across different semantic variations. The authors show that CL outperforms LA and LA in label-hungry problems with unlabeled training data, and that these two components can be combined to improve label-efficient learning and generalizability. Experiments are conducted on ImageNet-like images for self-supervised learning, and show that models trained with CL outperform supervised learning on histological images, and CL improves the generalization performance on this data compared to supervised learning. The paper also shows that a model trained with representation learning and histological image analysis can be used as a model for representation learning. Histology images are also used to improve generalization.   "
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"Recurrent neural networks (RNNs) for irregularly-sampled time series with continuous-time hidden states can be seen as a special case of RNNs, where the hidden state is a timecontinuous state, and the training of the gradient can be regarded as an ODE solver.   This paper proposes Mixed-Memory-RNN (mmRNN) which extends the memory compartment of existing continuous time-continuous RNN models to the case where the RNN is a RNN with a continuous time dynamical flow.  In particular, the authors show that the memory of an RNN can be decomposed into two parts: (1) a memory compartment that stores time-lags in the past, and (2) the memory path with constant error propagation.  The authors also show that it is possible to decompose the memory in a mixed memory RNN into two separate parts.  Experiments on non-uniform sampled data with long-term dependencies show that mixed-MemoryRNN's outperform their RNN-based counterparts on a variety of non-randomized data. "
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper proposes BiBERT, a pre-trained BERT-like model that can be used to reduce the computational and memory footprint of BERT for natural language processing (NLP) tasks. Previous compression approaches, such as binarization, have been shown to be effective in reducing the computation and memory consumption, but they are expensive. This paper proposes a new binarized BERT model, BiBERt, that is able to achieve competitive performance on the standard NLP benchmark. The key idea is to use 1-bit parameters (1-bit weight, embedding, and bitwise operations) of the BERT to reduce its computation. The authors also propose a Bi-Attention structure to capture the representation information statistically, and a DirectionMatching Distillation (DMD) scheme to further reduce the memory and computation cost of the full binarised BERT. The experiments show that the proposed method can achieve the state-of-the-art FLOPs and model size of a fully-banned BERT, while achieving competitive performance in terms of information degradation and optimization direction mismatch in forward and backward propagation. The paper also shows that the BiBERTs outperform a baseline and a baseline with ultra-low bit activations. The proposed method is also able to address performance bottlenecks in real-world resource-constrained scenarios.   "
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper proposes a new method for keypoint detection and instance association using Transformer. The proposed method is based on the observation that naive attention patterns in Transformer can be problematic for these problems due to the lack of associative information. To address this problem, the authors propose to leverage the association information for keypoints grouping and propose to replace the self-attention in the Transformer with pairwise attention scores. This approach is then applied to supervising self attention for multi-person keypoint recognition, instance association, and instance segmentation tasks.    The proposed approach firstly learns the pixel assignment pipeline for each pair of keypoints in the input image. The pixel assignment is done by learning a pixel mask for each keypoint in the image. Then, the self attention is applied on the instance masks of the pair of keys. The self attention behavior is then evaluated by comparing the performance of the proposed approach to the previous approaches. The method is evaluated on the COCO multi-posteriority of multi-perspective keypoints detection and multi-people keypoint segmentation task, and is shown to outperform the previous methods. The authors also show that the supervised attention matrix is also useful for instance segmentations. The paper also shows that pre-defined offset vector fields, embedding, and CNN-based bottom-up models can be used to further improve the performance. "
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper considers reinforcement learning (RL) for sequential decision making, where existing methods suffer from the variance term. The authors propose a new approach, called Pareto efficient RL (Pareto-efficient RL) to address this issue. In particular, the authors show that the MV trade-off between the expected quadratic utility function and the expected variance of the policy is a Pareta efficient policy, and that existing methods do not satisfy it. The paper also shows that the Paretto efficiency of existing MV-efficient policies can be improved by considering the gradient estimation of the variance.   "
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper proposes a new approach for end-to-end learning of a communication system using an autoencoder. The approach is based on the idea of a mixture density network (MDN) where the encoder and decoder neural networks are trained on unlabeled data from the source domain. The authors propose a test-time domain adaptation to improve the performance of the autoencoders. The key idea is to use a wireless link between a fully-trained channel model and a fully trained autoen coder, and then use the resulting communication system as a testbed for a test domain adaptation.   The authors show that the proposed method improves the error rate of the trained method for the auteno-coder and the decoder by using feature transformations to adjust the channel distribution of the source and the target domain. They also show that this method can be applied to any MDN channel, and that the method is able to learn the MDN model in an unsupervised way.  The proposed method is evaluated on simulated datasets and real mmWave wireless channels, and shows that their method achieves a lower error rate than the state-of-the-art method. The main contribution of the paper is that the authors propose to use the test data from a source distribution that is not available to the source distribution, and to use test time domain adaptation as a way to train the test domain adaptor."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper studies the abductive natural language inference task (αNLI), where the inference network is trained to infer the intent of an interactive language model. The authors propose an interactive model with structural loss (i.e., joint softmax focal loss, which is a structural loss that minimizes the difference between the output of the interactive model and the input of the model) for the αNLI task. They call it the Interactive Model with Structural Loss (IMSL). They show that IMSL outperforms the RoBERTa-large pretrained model in terms of ACC and AUC. They also show that the reasoning abilities of their model are more robust to changes in the context of the context."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper studies the problem of overconfident OOD detection in machine learning for safety-critical systems. The authors propose a method that combines a certifiable OOD detector and an OOD aware classifier, which is a classifier that is certified to be OOD-robust on OOD data. They show that deep neural networks have an asymptotic overconfidence problem for OOD samples, and that certifiably adversarially robust OOD detections are more likely to be certified if the in-distribution is also certified as OOD. They also show that the assessment of uncertainy for machine learning can be a useful tool for improving the performance of machine learning.    The method proposed in this paper combines the idea of certifiable adversarial robust Ood detection with a method for certifiable overconfidence detection. The proposed method is based on the observation that the classifier of a certified OOD classifier is more robust to OOD attacks than neural networks, and the authors show that this classifier can be used to mitigate the so-called “overconfident overconfidence” problem of neural networks. The paper also shows that the detection performance of non-manifold OOD test data can be improved by using the certified classifier.  Finally, the authors provide a theoretical analysis of the relationship between prediction accuracy and detection performance. "
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper studies transfer attacks on Deep Neural Networks (DNNs) in the query-free black-box setting. The authors propose a new transfer attack, called the Generalized Transferable Attack (GTA) problem, where the target dataset is not available to the attacker, but is available to a victim model. They show that transfer attacks can be performed in a query-less, query-based, and query-driven setting. They also show that the transfer attack can be applied to both white-box surrogate models as well as black -box victim models, and that the surrogate models can be trained on the same datasets as the victim models. They propose a method called Image Classification Eraser (ICE) to remove classification information from the target and victim models in order to prevent them from being transferred. ICE is tested on Cifar-10, CIFar-100, and TieredImageNet, and shows that ICE is able to solve the GTA problem more effectively than existing transfer attack methods. "
SP:2e0447c741a3f09be1095633d870200355211260,"This paper proposes a discriminative PrLM to learn a contextualized representation that can be used to improve robustness of PrLMs. The authors show that existing pre-training methods can lead to false negative predictions in the context of language models. The paper also shows that the false negative issue is a common issue in the training process of discriminatively PrLM-based language models, and the authors propose two ways to mitigate this issue. The first is to use gradient updates with respect to the true label of the model, while the second is to train a model that is robust to false negatives. Experiments are conducted on GLUE and SQuAD benchmarks, and show that the proposed method is effective in reducing false negative errors.  "
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper studies semi-supervised learning in real-world settings, where unlabeled test data is available but labeled training data is not. The authors propose an end-to-end approach called ORCA, which is based on the assumption that the class distribution mismatch problem can be alleviated. The paper proposes to use an uncertainty adaptive margin to improve the discriminability of the model.    The paper shows that ORCA improves the class-distribution mismatch problem in the open-world semi-distributed learning setting, where the class is not known prior knowledge.  The authors also show that under certain assumptions, ORCA can achieve better discriminable performance on image classification datasets and a single-cell dataset.  In addition, the authors show ORCA outperforms several baselines on the ImageNet dataset in terms of seen and novel classes, and ORCA is able to learn novel classes that are more likely to be seen in the training data. "
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes SLIM-QN, a light stochastic quasi-Newton optimizer that is able to reduce the computational cost of second-order methods for training large-scale DNNs.    The main contribution of this paper is to extend the work of L-BFGS, which is a popular method for stabilizing the Hessian matrix of a stochtastic training (L-BFBSGS), to the case where the gradient of Hessian inverse of the gradient is non-asymptotic.  The authors show that the BFGS update rule of SLIM - QN can be viewed as a simple modification of BFGS, and that the authors propose to use momentum and an adaptive damping mechanism to mitigate the instability in Hessian updates caused by the convergence instability of KFAC.  In addition, the authors provide a theoretical analysis of the stability of the convergence of the proposed SLIM QN in the stochastastic setting, showing that the momentum is responsible for stable convergence.  Finally, they show empirically that the convergence performance of the SLIMQN outperforms SGD in terms of wall-clock time on large datasets (e.g., ImageNet) and compute and memory overhead compared to second- order methods.  They also show that SLIMqn can be applied to non-convolutional architectures such as Transformers, where the compute resources of SGD can be reduced significantly. "
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"Graph Neural Networks (GNNs) have been widely used for graph-related tasks, and for many real-world problems. However, there are many redundant components in large graphs due to the fact that GNNs are trained to solve these problems. In this paper, the authors propose a systematic method called LocalitySensitive Pruning (LSP) based on Locality-Sensitive Hashing to perform graph pruning. In particular, a pre-processing step is used to sparsify the graph, and then node or edge removals are performed during inference. The authors show that LSP is able to prune edges from large graphs, and that the pruning is more efficient than other pruning strategies based on locality properties. They also show that the LSP can be applied to both synthetic and real world datasets.    The main contribution of this paper is that the authors provide a systematic approach to pruning large graphs based on local graph properties. The main idea is to first prune a sparsified graph and then to apply LSP to the edges that are most relevant to the task at hand. The paper also shows that the number of edges pruned by LSP does not depend on the size of the graph.  The authors also provide a theoretical analysis of the effect of pruning on the final performance of the pruned edges, and show that in some cases, LSP prunes the edges more efficiently than existing pruning methods. "
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper studies the problem of data augmentation in contrastive self-supervised learning. The authors propose a novel adversarial augmentation method, IDAA, which augments samples with hard positives/negative. The key idea of IDAA is to use a variational auto-encoder (VAE) reconstruction of the data and augmentations to prevent the VAE from picking up hard negatives/negative samples, which can lead to ineffective learning. To achieve this, the authors propose to disentangle the identitydistinctive information of R(x) and G(x), which is a VAE objective. They show that IDAA can be used to learn a sample identity that is disentangled from other augmentations, and that it can be incorporated into the original dataset. They also show that this augmentation can be applied to any VAE’s bottleneck space, which is the case for contrastive learning. Experiments on several benchmark datasets show that the proposed IDAA improves efficiency and generalization performance.   "
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper proposes a new method to detect harmful shifts in machine learning models. The idea is to train a model on the data distribution, and then perform a series of tests to detect distribution shifts, and these can be applied to arbitrary shifts in the data. The authors argue that existing non-sequential methods do not work well for these, and propose a method to identify harmful shifts.  The authors propose two sequential tools to identify the risk function of interest: accuracy and calibration. The tracking process is based on the aggregation of statistical evidence by constructing time-uniform confidence sequences. The proposed framework is tested on both simulated and real datasets, and the results show that the proposed method is able to detect benign shifts with a lower false alarm rate. "
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"Neural networks have been shown to capture the dynamics of diverse physical systems, and they can be applied to the problem of high-resolution videos. In this paper, the authors combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) to learn interpretable physical models from visual observations. The authors show that neural implicit representation can be used for the processing of high resolution videos, synthesis of photo-realistic imagery, and to learn physical parameters for the synthesis of scenes from scenes. The proposed method is able to identify physical parameters from real-world videos (e.g. pendulum motion). The proposed model outperforms existing approaches that rely on large training datasets. The key contribution of the paper is the identification of an embedded neural ODE with a known parametric form that allows for identification and long-term prediction in state space. The paper also shows that the proposed method can identify the physical parameters of a pendulum in a monocular video, and that the model can be trained to predict the metric length of the pendulum from a single frame of the monocular version of the video. The method can also be used to reconstruct physical parameters in a scene from a video.   The authors also show that physical parameters can be learned for the task of reconstruction from physical parameters. The relative error of the learned physical parameters is measured in terms of the number of times that the state space is changed. "
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper considers the problem of learning a policy in a Markov process with abrupt (discontinuous) context changes and Markovian context evolution. In this case, it uses a Bayesian approach, variational inference, and a sticky Hierarchical Dirichlet Process (HDP) prior for model learning. The authors show that under the context cardinality assumption, the optimal policy for policy learning can be learned by RL algorithms. They also show that a context distillation procedure can be used to remove spurious contexts. The paper also shows that this framework can be combined with existing state-of-the-art methods to improve the performance of these frameworks.    The paper is well-written and well-motivated. The approach is well motivated. However, there are a few issues:"
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"Knowledge enriched language representation learning is an important topic in knowledge-intensive NLP tasks. This paper proposes a framework for knowledge based multilingual language models (KMLMs) trained on monolingual knowledge graph data. The authors show that the pretraining tasks for knowledge learning are dependent on the intraand inter-sentence structures of the data. They show that language models trained on Wikidata knowledge graphs are able to capture logical patterns that can be used by language models to encode factual knowledge. They also show that pretrained KMLMs can be applied to a variety of knowledge-intrinsic cross-lingual NLI tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task called logic reasoning. "
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"This paper studies the problem of training artificial agents to be altruistic in the presence of external supervision. In particular, the authors consider reinforcement learning agents. They show that agents trained in a task-agnostic manner can learn to exhibit altruistic behaviour in a multi-agent environment. The authors also show that, under certain conditions, agents trained with external supervision are able to learn to behave in a similar way as humans.    The authors propose a new concept called “altruistic agent’s behaviour”, which is defined as the behavior of an agent that behaves in a way that is similar to that of an altruistic agent that is trained in an unsupervised manner. The paper shows that this concept can be used to train agents that are more like human agents. The approach is tested in a number of multi-agents environments, and the authors show that their approach is able to outperform a number (but not all) of existing unsupervisory agents, and outperform them in some cases. "
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the double descent of neural networks with Neural Tangent Kernel. Double descent is a well-studied phenomenon in linear and kernel regression models, and the authors show that for finite-width neural networks, double descent can be observed in the limit of infinite width. The authors also show that double descent is also observed for models with infinite width, and that the interpolation threshold of double descent behaviour depends on the size of the population. The main contribution of the paper is that the authors provide a new loss function for double descent, which is based on the observation that the Hessian of the optimum of a Hessian can be approximated as a function of the number of influence functions, and this can be used as a parametric model. The paper also shows that neural networks and Hessian spectra can be related to neural networks. "
SP:b485114712055f39a7afb951dbc3db482ff523fd,"Graph convolutional networks (GCNs) for graph-structured data suffer from the over-smoothing problem. This paper studies the expressive power and trainability of deep GCNs in the context of node representations. The authors show that wide GCNs can be trained with gradient descent based on the Graph Neural Tangent Kernel (GNTK) of the optimization trajectory using gradient descent, and that the expressivity is better than trainability. They also provide asymptotic behaviors of the GNTK in terms of dropping trainability at large depth and exponential rate in the optimization process.   The authors also provide a theoretical framework for residual connection-based techniques to explain the exponential decay of trainability, and propose a gradient descentbased optimizer called DropEdge. The proposed method is shown to outperform its counterparts in both infinite-width and finite-width settings. The paper also provides a theoretical analysis of the expressive space and shows that DropEdge can be used to solve an exponential decay problem. "
SP:25a92b3583afdc6892e59f1e769125d52c8011af,"This paper presents a method for learning a second-order dynamics of a cardiac signal. The method is based on the observation that current computer vision methods are unable to capture the first-order dynamic of the signal, which is the optical flow. The authors propose a method to learn a second order dynamics from the first order dynamics. They show that this method is able to capture higher-order changes (e.g. acceleration) in the second order, which can be used to improve the accuracy of cardiac measurements from videos. "
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper studies the problem of emergent communication in a multi-agent setting, where agents are trained to communicate through simple interactions that are similar to human language. The authors propose a new architecture, called symbolic mapping, to learn a communication system that includes a symbolic mapping between agents. The symbolic mapping is learned through referential games, and the authors show that the symbolic mapping can be used for language learning in language learning. They also show that this process can be applied to multi-agents language learning, and that the learned language is a compositional and symmetric language.  The paper also shows that the process of vocabulary expansion can be seen as an extension of dialog games, where the goal is to increase the simplicity and complexity of the communication.  "
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"Robotic agents are trained to perform domestic chores using natural language directives. The authors propose a hierarchical modular approach to train such agents, where each policy has a hierarchy of subgoals that are executed in a divide-and-conquer manner. Each subgoal can be decomposed into a set of language instructions, and the agent’s navigation is guided by a master policy. The navigation policy and the independent interaction policies are trained separately. The interaction policy is used to generate object masks that are used to guide the manipulation actions. The hierarchical agent is evaluated on the ALFRED benchmark, where it is shown to be able to achieve better performance than Compositional Reasoning. "
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes Nuisance-Randomized Distillation (NURD) to improve the performance of predictive models on prediction problems where the nuisance variable is variable-dependent. The authors consider the nuisance-label relationship between two distributions (i.e., nuisance-randomized distribution and nuisance-unlabeled distribution) and show that this relationship can be used to train a model. NURD learns a representation of the distribution of the nuisance in a nuisance-varying family that is more robust to spurious correlations between the covariates of the two distributions. The paper shows that such representations are more robust than the representations learned from natural images, and that models trained with spurious correlations are able to learn models that are robust to pneumonia in the presence of non-lung patches, which is a common nuisance in real-world datasets. The results on two tasks (chest X-ray classification and pneumonia classification) demonstrate the effectiveness of NURd on both tasks."
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper proposes OTTER, a method for zero-shot classification that uses contrastive contrastive learning and InfoNCE loss to train a model to match images and text captions. The idea is to use computer vision models to learn predefined categories, and then use natural language as supervision to guide the learning of visual concepts. The authors propose to use CLIP with image-text pairs to train CLIP, and use Optimal TransporT distillation to train the model to learn to match two sets of images and a set of texts. OTTER uses online entropic optimal transport to learn a soft image - text match. The models are trained with both pretrained image and text encoders, and the models are fine-tuned on image text pairs. Experiments show that OTTER outperforms several baselines on zero-shoot evaluation on Google Open Images and multi-labeled ImageNet, outperforming baselines that do not use supervised ""gold"" labels. The experiments also show that the models trained with OTTER are more robust to label smoothing, label distillation, and knowledge distillation. The results are also shown in different dataset/architecture settings. "
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper proposes Pix2Seq, a language modeling task for object detection. Object descriptions such as bounding boxes, class labels, and class labels are used to train a neural net to generate an image from an image, which is then used as a prior knowledge for the task. This approach is evaluated on the COCO dataset, where it outperforms existing detection algorithms, and it is shown to be more robust to task-specific data augmentations.    The paper is well-written, well-motivated, and well-structured. It is a clear contribution to the community.  The authors propose a Pix2Segment framework that can be applied to any object detection task, and show that it can be used in conjunction with the Pix2Sq framework. "
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"Deep vision models in visual reinforcement learning (RL) are often used to train policy networks. However, these policies often lack interpretability due to input distribution shifts. To address this issue, this paper proposes a stage-wise approach that incorporates hierarchical reasoning into the training process. The approach first trains a policy network to learn an interpretable symbolic policy that combines geometric and numerical symbols and operators. Then, a policy regression algorithm (called RoundTourMix) is used to learn the symbolic rules. Finally, a symbolic distillation approach is applied to distill the CNN policy from the learned symbolic relations into a CNN policy network knowledge.    The paper shows that the distilled symbolic policy outperforms other CNN based RL agents on Airstriker-Genesis, Pong, CircusCharlie, and Seaquest. Detected bounding box Velocity is also shown to be interpretable. The paper also proposes an end-to-end learning pipeline, where the teacher-student is trained in a self-supervised manner, and the student is trained to learn a symbolic policy based on the teacher's knowledge of the symbolic relations. The authors also show that policy distillation can be applied to learn geometric relations in a more interpretable way."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a new image-to-image translation model, called PIVQGAN, for the task of disentangling the pose and identity of an image from a set of training images. The proposed model is based on StyleGAN2, and is able to disentangle coarse-level object arrangements (pose) and fine-grained level styling (identity) from exemplar sources.    The proposed method is built on top of techniques from StyleGAN, and the authors propose two techniques to achieve this. The first is a joint-training scheme to train a GANInversion encoder and a generator to achieve pose-identity disentanglement. The second is a VQSN module to capture shaping and composition information from training-set images, and a training scheme to improve the performance of the proposed model.  The authors also propose a latent-space reducing feature to further improve the quality of the learned pose-related representations.  Experiments are conducted on three model applications, and show that the proposed PivQGAN achieves state-of-the-art synthesis image quality and disentangled scores for all model applications. Unsupervised learning of a segmentation-like “pose-like” masks is also performed on the image, and it is shown that the learned segmentation masks can be used for segmentation in the context of a training image. The paper also shows that the model can also be used to learn a “identity” mask, which is a mask that can be applied to any training images to achieve a better segmentation performance.  Finally, the authors also show that PIVGAN can achieve good performance on the task for the case where the pose images are not available for training (e.g. in the presence of referential identity images), and that it can also perform well on the “unsupervised” setting where the training images are available. The key contribution of the paper is that the authors have proposed a novel way to learn the disentanged posture and identity control, and that they have proposed two models, one for pose and the other for identity. "
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a multi-layer perceptron (MLP) architecture, called speech-MLP, for the task of multi-modal speech processing. Transformers have been widely used in speech processing tasks such as speech recognition, speech synthesis, speech enhancement, and speech enhancement. However, these models have not been applied to other speech related tasks. The paper proposes to use multi-layered MLP to capture the multiscale local temporal dependency in the speech signals. The proposed model is evaluated on two tasks, namely, keyword spotting (using the dataset (VoiceBank) of Google speech command V2-35 and LibriWords), speech enhancement (using two benchmark datasets for keyword spotting and for speech enhancement), and speech synthesis (using a dataset (SpeechNet) for speech synthesis). The paper shows that the proposed speech- MLP achieves better performance than transformer-based solutions on all three tasks.    The paper also shows that transformers can be used to split the input speech signals into chunks, and that the feature channels can be shared across different chunks. The authors also show that the GFLOPS of the proposed model are comparable to the state-of-the-art transformers. The main contribution of the paper is that the model is able to handle contextual window sizes, which is useful in resource-constrained scenarios."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"This paper studies the problem of knowledge transfer in machine learning, where the goal is to transfer knowledge from a source domain to a target domain. Transfer learning has been a popular topic of interest in recent years, especially in the context of massive models trained on labeled data. However, there are many challenges in this setting, such as data collection, labeling, and the computational complexity of the transfer learning algorithm. This paper proposes a new lower bound on the generalization error of a transfer learning method, which is based on the observation that there is a trade-off between the cost of data collection and labeling. The authors show that the lower bound is tight, and that it depends on the source/target data distributions. The paper also shows that the bounds are tight in the setting where the source domains are different and the target domains are the same.   The authors also show that their bounds for binary classification problems are tight. They show that for real world data sets with labeled training data, their lower bounds are tighter than existing upper-bounds based on transfer learning base-lines based on weighted empirical risk minimization on both source(s) and target data sets. They also show bounds for real image classification and action recognition data sets, showing that the bound for the generalisation error is tight. "
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic shape completion method for the continuous geometry of large-scale 3D scenes. The formulation is based on the idea of Generative Cellular Automata (GCA) to learn a multi-modal distribution over the latent code of a sparse voxel embedding, which is then used to generate a local continuous shape from the sparse embedding. A progressive generation is used to train the generative model, and the training objective is a variational lower bound for the complete shape distribution.    The paper is well-written and well-motivated. Real-world scans of 3D scene are available, and shape completion is a very important problem. This paper shows that the proposed approach outperforms existing deterministic models, and is able to handle missing data well. The paper also shows that this approach can be applied to geometry completion, and that the probablistic formulation is more robust to missing data. The model is also shown to be able to generalize well to unseen objects."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper studies the problem of exploration in deep reinforcement learning. Behavioral priors are used to tackle this problem. The authors propose to use temporal priors that are state-independent and state-dependent. They show that this leads to reduced generality and restricted transferability in reinforcement learning, and propose a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning based on temporal consistency. Experiments on long-horizon continuous control tasks show that the proposed approach outperforms baselines in sparse reward settings. "
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a new stochastic optimization for deep neural networks, called Learning rate scheduling, which is a generalization of Adam. The main idea is to use pre-defined rules to guide the scheduling, and then use GNS to learn the dynamics of the dynamics using a graph message passing network. The agent learns the learning rate using reinforcement learning using a directed graph, and the scheduling mechanism is based on a scheduler that takes as input the intermediate layer information from the scheduler. The authors also propose a reward collection procedure to speed up training. The proposed framework is evaluated on three benchmarking datasets: Fashion-MNIST, CIFAR10, and GLUE for language understanding, image classification, and image classification. The results show that GNS outperforms the baselines for CNN and Transformer models with different network structures. "
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper proposes a new object-specification scheme, called SPAIR3D, which uses a point cloud for deep object-centric learning. The point cloud is used for both high-level relational reasoning and scalable machine intelligence. The authors propose a new framework for 3D point cloud, which is based on Chamfer Mixture Loss in the variational training pipeline. The proposed spatial mixture model is trained on point clouds, where each object is represented as a local voxel grid cell. The paper shows that the proposed scheme can be used to train a 3D 3D object model in an unsupervised manner. The method is evaluated on the task of scene decomposition, and the results show that the method achieves state-of-the-art results. "
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper proposes to use large language models (LLMs) to leverage world knowledge from natural language for interactive environments, where they are trained with LMs to solve high-level tasks in natural language. The proposed method is evaluated on the VirtualHome environment, where it is shown to outperform the LLM baseline in terms of executability and correctness.   The key idea of the paper is to learn a set of actionable steps that can be executed in the environment, and then use these actions to guide the agent to the next state. The actionable knowledge is learned by using language models1 and 2, where the low-level plans are learned in the same way as in LLMs, but the procedure is differentiable and can be applied to a large number of environments. The paper also proposes a human evaluation to evaluate the quality of the learned actions. The evaluation shows that the agent is able to learn admissible actions that are more likely to be executed by the agent."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a Variational Autoencoder framework based on a geometrical interpretation of the Riemannian structure of the learned latent space of VAEs. The authors show that a vanilla VAE can be viewed as a special case of a VAE that takes into account the geometric considerations of the latent space, and that VAE models trained on standard benchmark datasets can be seen as a generalization of the results obtained by VAEs on a uniform distribution over the learned space of the VAE. They also show that the uniform distribution can be expressed as a function of a Riemanian manifold. The proposed method can be applied to deep generative models in a low data regime, and is shown to be able to generalize to a complex neuroimaging dataset with high dimensional data and low sample sizes.   "
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper proposes Multi-head attention (MGK) to replace redundant heads in transformers in two applications: natural language processing (NLP) and computer vision tasks. The authors propose to use a mixture of keys in the transformer architecture, where the mixtures of keys are modeled as a Gaussian mixture model. The proposed Transformer-MGK is shown to be more efficient than its transformer counterpart in terms of FLOPs and training time. In addition, the authors show that the proposed model is more robust to redundant embedding and that the accuracy of the Transformer - MGK is better than the original Transformer with linear attentions. Experiments are conducted on three applications: language modeling, tasks in computer vision, and the Wikitext-103 and Long Range Arena benchmark, and show the effectiveness of the proposed method. "
SP:82731dcce233e748f63382e09b6224a513fe9689,"Spatial navigation is an important problem for biological agents. Spatial navigation has been a focus of interest in recent years, but most of the existing models are based on LSTM. In this paper, the authors propose a minimalistic recurrent architecture called the Resetting Path Integrator (RPI), which is a direct-inverse model of environment dynamics for both image and action related signals. The authors propose to use proprioception and linear and angular velocity to model the two-dimensional continuous environment, and use a directly inverse model of the environment dynamics to learn the reconstruction of the action from the image signal. The proposed RPI learns the internal state of a minimal model by resetting, and it learns the cognitive map of the minimal model, which is the integration of past movement with the current state. The paper shows that the proposed architecture is able to capture the internal dynamics better than previous state-of-the-art methods on a number of tasks, and outperforms LSTMs.   "
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the problem of feature learning for neural networks trained with gradient descent. The authors consider learning problems on practical data with class relevant patterns and background patterns. They show that linear models with polynomial sizes are able to learn data-independent features that are robust to changes in the structure of the input distribution. They also show that the features learned during feature learning can be used to improve the prediction performance.    The main contribution of the paper is to show that neural networks can be trained to solve these problems using gradient descent, and that the feature learning performed by neural networks is robust to change in the data structure.  The authors also propose a new Statistical Query model that incorporates the polynometric algorithm. Experiments are conducted on both synthetic and real data. "
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper studies the robustness of machine learning models to adversarial examples from test-time adversaries. The authors propose a methodology to improve robustness for a classifier by using lower bounds on the model's ability to distinguish between data distribution and attacker constraints. They show that the bounds are tight for arbitrary classification functions and can be extended to arbitrary architectures and models (e.g., neural networks). They also show that their methodology improves robustness to collision attacks for fixed feature extractors, and that it can be applied to any classifier. They also propose a bespoke algorithm that uses closed-form expressions for collision finding, which is based on a convex program. The method is shown to be robust to collisions in the presence of adversarial attacks.   "
SP:874b5fa51924cbcceed490d98a0ea80f74586b32," Offline reinforcement learning (RL) is an important problem in RL to solve real-world problems. However, existing offline RL algorithms rely on regularization or constraints to reduce the extrapolation error. This paper proposes a new offline method called Value-based Episodic Memory (VEM), which is an extension of the Expectile V-Learning (EVL) framework to learn a V-function. The framework is based on the observation that the learning procedure in an offline dataset is conservative in the sense that the Q-function tends to converge to a stationary point. The paper then proposes a framework that combines optimal value learning and behavior cloning to improve the conservatism of offline learning. The proposed method is evaluated on the D4RL benchmark and is shown to outperform the state-of-the-art method on a number of tasks, including sparse-reward tasks. It is also shown that implicit planning on the V-values obtained from offline trajectories can be used to perform implicit planning in the future."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper studies the robustness of neural network classifiers against adversarial perturbations in order to achieve a trade-off between robust accuracy and standard accuracy. The authors propose a new adversarial training framework for robust generalization based on the MAML-based approaches. In particular, the authors show that adversarial robustness can be achieved by minimizing the class-conditioned margin between the sample’s multi-class margin and the upper-level margin of the robust classifier. To achieve this, they propose a parametric function based on importance weight. They also propose a bilevel optimization problem for weighted adversarial learning. Experiments show that the proposed approach outperforms existing techniques and state-of-the-art baselines in terms of clean and robust accuracy on a number of datasets."
SP:3ad36be6b6900aabe43da043461cf178ce977082,"This paper proposes steerable message passing neural networks (SEGNNs) that are equivariant to node and edge attributes. The proposed model is based on steerable MLPs, which incorporate geometric and physical information to the message and update functions. The covariant information is covariant in the form of position, force, velocity, and spin.   The authors propose two activation functions for steerable feature fields based on MLPs. These activation functions can be seen as invariant scalars. The authors show that equivariance to steerable node attributes can be achieved by equivariancing the node attributes.  The paper also shows that steerable messages can be obtained by equivarient graph networks with invariant messages, which is an extension of the work on invariant non-linear convolutions [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,20].  The proposed method is evaluated on computational physics and chemistry tasks, and the results show that the proposed method outperforms the state-of-the-art methods.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]"
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"Differentiable physics modeling combines physics models and gradient-based learning to achieve model explicability and data efficiency. It can model dynamics, inverse problems, and design. Fine-grained models are used to model material structures and force interactions, and physics models such as rigid bodies and deformable sheets are used. The paper proposes a differentiable fabrics model for composite materials such as cloths, where the differentiable forces in empirical physics can be used to train a model to capture subtle dynamics. The authors show that these forces can be applied to cloths to capture complex physical phenomena such as individual yarn physics and yarn-to-yoke interactions. They also show that the granularity of yarns can be controlled by the model, and that the model is able to capture physical parameters in a way that allows for data-efficiency and high-fidelity.   "
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a new framework for reinforcement learning based on logical composition. In this framework, a task-specific skill is represented as a Boolean expression, and the goal is to learn an optimal policy that maximizes the expected return of the learned skill. The authors propose an algorithm for learning the optimal policy, and show that this algorithm can be seen as a generalization of a previous work (Zhang et al., 2017). The authors also show that the distribution of the optimal policies learned by the algorithm is similar to that of the distribution learned by Zhang et al. (2017).    The key idea of the proposed approach is to use an unknown distribution as a surrogate for the task distribution, and then learn a transferred policy that minimizes this distribution. This approach is called transfer learning, and it is shown that this transfer learning approach can be applied to a variety of tasks.  "
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper proposes a new method for multivariate time series classification (MTSC) that combines machine and deep learning solutions for the task. The authors show that the proposed method, called ROCKET, is able to achieve state-of-the-art performance in terms of prediction accuracy for complex models. The main contribution of the paper is the introduction of a novel MTSC solution based on random convolutional kernels.    The authors also show that their solution is more computationally efficient than existing deep learning approaches for the same problem, and that it can be applied to any MTSc problem.  The proposed method is based on the wavelet scattering transformation and distributed feature selection, which is a well-studied technique in real-world environments. In particular, LightWaveS is a distributed solution for the MtsC problem, where the time series is partitioned into multiple time series and each time series has a different number of nodes and channels. The proposed solution uses the same amount of nodes, channels, and features, but uses a different amount of time to process the different time series. The solution is trained on the same number of time series, and uses the ROCKET features as input, and the solution is evaluated on both training time and accuracy. The paper shows that the algorithm achieves a significant speedup in training time, accuracy, inference speedup, and accuracy with respect to the training time. In addition, the speedup of inference is further improved when the inference is performed on an edge device. "
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a new adversarial learning curriculum called AMOS for text encoders based on Mixture Of Signals for auxiliary generators. The idea is to train a discriminator to distinguish between replaced tokens generated by auxiliary masked language models (MLMs) and the ones generated by the original MLM. The discriminator is then used to train an encoder to generate the replaced tokens. The authors propose a unified auxiliary model consisting of two MLMs, one for each token, and the other for the training signals. The auxiliary MLMs’ outputs are conditioned on the mixture weights used in the discriminator loss, which is a combination of the gradient of the encoder’s output and the Gumbel-Softmax. AMOS is evaluated on the GLUE and SQuAD benchmarks for BERT base-sized models, and compared to ELECTRA, a previous method for ELECTRA-style pretraining, and is shown to improve pretraining efficiency. "
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper studies the relational fact extraction task, where the goal is to extract knowledge graph facts from a pre-trained language model. The authors propose to use a clozestyle sentence to extract relational knowledge from a set of pre-trainable language models. The paper shows that language models trained on clozestyles outperform existing knowledge graphs in terms of precision, and that the relational knowledge can be extracted from a small subset of the knowledge graph.  The paper also shows that adaptive fine-tuning of the fill-mask task using a pre - trained language model can improve the performance of the model in the presence of complex prompting techniques, and also improves the transfer learning capabilities of the language model when the model is trained on a smaller subset of facts. The approach is evaluated on a number of different datasets, and the results show that the proposed approach improves the knowledge extraction quality, and is more effective than baselines.   "
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"Knowledge bases with multi-relations are a very popular topic in machine learning. Knowledge bases are typically represented in a Euclidean space, and the authors propose a representation learning framework to learn relation properties (e.g., symmetry, inversion, composition, etc.).    The key idea is to learn these properties in a hyperbolic space, where the properties can be represented as Euclideans.   This is achieved by learning a set of properties that are invariant to transitivity in the hyper bolic space and then aligning the properties of the properties with those of the underlying geometry of the space.  The authors show that the properties learned in Euclideen embedding models are invariants to a variety of properties, including those that have tree-like properties.  They also demonstrate that the geometric spaces of the knowledge base embeddings can be mapped to geometric spaces, and that the resulting aligned embedding can be used in an out-of-taxonomy entity typing task, where entities are represented as a knowledge graph.  Finally, the approach is evaluated on two datasets (YAGO3 and YAGO2) and is shown to be effective on low dimensions and small training rates. "
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a new one-shot learning framework for link prediction on temporal knowledge graphs with a frequency distribution similar to real-world knowledge graphs. Previous approaches for static knowledge graphs have been shown to be effective for low-shot performance, but not for temporal settings. The proposed method uses a self-attention mechanism to capture temporal interactions between entities and a network to compute a similarity score. The authors show that the proposed algorithm is more robust to sparse relations than baselines and can be used in situations of data scarcity."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes a neural module that learns a solver to solve a given task using only a single neural module. This module can be used to solve any task, and can be applied to a variety of tasks. Lower modules are used to reduce the forgetting. The modules are trained using a single module. The proposed model is evaluated on visual reasoning tasks, and is compared with an attention-based baseline. The model is also compared with human judges for the reasoning process. "
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper proposes a new architectural unit for deep convolutional neural networks (CNN) called Selective Convolutional Unit (SCU), which is an architectural unit that aims to improve the parameter efficiency of CNNs with bottlenecks. The SCU is designed to improve channel-selectivity of the convolutionic layer, which is defined as the identity (e.g., residual) connection between the output layer and the identity connection. The authors show that SCU can be used to improve parameter efficiency by pruning unimportant channels during training. The pruned parameters are then rewired and the rewired parameters are used to train a new model. Experiments show that the SCU-based models achieve better model compression compared to the baselines that do not use postprocessing.   "
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper studies the problem of learning a binary classifier from both positive data and unlabeled data. The authors show that the selection bias in the labeling process is more severe in the case of labeled positive data than the case for unlabeling positive data. They also show that it is possible to learn a Bayes optimal classifier by PU learning. They propose an algorithm to learn the scoring function that maximizes the probability that the class posterior of the positive data is the same as the one of the unlabelled positive data, and that it minimizes a selection bias. They show that this method can learn a classifier with a lower threshold than the threshold of the classifier. The proposed method is evaluated on two real-world datasets and compared with other methods for PU learning, and is shown to outperform other methods. "
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes an approach to reduce the statistical inefficiency of neural networks using auxiliary losses to learn representations. The main loss is the auxiliary loss, and the auxiliary task is the main loss. The proposed algorithm is evaluated on three domains: multi-task supervised learning on ImageNet, reinforcement learning on a gridworld, and reinforcement learning in Atari games. The results show that the proposed algorithm can achieve better performance than existing methods. The authors also demonstrate that the adaptive weight can be used as an additional auxiliary task."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric framework based on the manifold reconstruction literature to study the high-dimensional geometry of adversarial examples in machine learning models. Adversarial examples are defined as misclassifications where the codimension of the data manifold is a function of the decision boundary between a low-dimensional data manifold and a high-dimensionality space. The authors show that adversarial instances can be seen as low-dimensioned versions of the original data manifold. They also show that a decision boundary can be defined on the low-dimensions of the high dimensional space, which can be used to define a robustness to misclassification.   The authors also provide sufficient sampling conditions for nearest neighbor classifiers and ball-based adversarial training, and show that the robustness of these norms can be improved when the number of samples is large enough.  Finally, the authors provide a theoretical analysis that shows that, under certain conditions, adversarial attacks on low dimensional data manifolds are more likely to be misclassified than on high dimensional ones. They further show that this phenomenon is also observed in the case where the data is on a low dimensional manifold. "
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"High-dimensional time series are of great interest in many applications where human cognition is interested in high-dimensional spaces. However, there is a lack of interpretable low-dimensional representations for these areas. This paper proposes a new representation learning algorithms for time series data. The proposed representation learning framework is based on interpretable discrete dimensionality reduction and deep generative modeling. The framework learns discrete representations of time series to learn smooth and interpretable embeddings. The authors propose a new way to deal with non-differentiability in discrete representation learning. They propose a self-organizing map algorithm that is more interpretable than the original. They also propose a probabilistic interpretation of our method based on a Markov model in the representation space. They show that their model learns a natural representation of uncertainty that captures the temporal transition structure of the data features. They demonstrate that the proposed model can be used for clustering, interpretability, and clustering on real world data. Finally, they show that the model is able to learn a model for a real world medical time series application on the eICU data set, which is a chaotic Lorenz attractor system with macro states. "
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper proposes to use multidimensional probability distributions as latent space prior distributions for implicit generative models. The authors show that linear interpolations in the latent space can be approximated by random latent vectors, and that decoding linear interpolation from a set of random latent latent vectors is equivalent to decoding a distribution mismatch between the latent distribution and the true distribution. They also show that non-linear interpolations can be used to mitigate the distribution mismatch by using the latent probability distribution of the prior distribution. The prior distribution is modeled as a multiddimensional Cauchy distribution, where the finite mean is a function of the number of latent distributions."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes a new geometry of embedding of object representations. The authors show that shallow networks can be embedded in hyperbolic space. They show that embeddings of ubiquitous attention mechanisms in modern neural networks architectures can be represented in this space. The paper also shows that the hyper-bolic geometry of the embeddens of modern embedding space is similar to that of Euclidean geometry.    The authors also show that existing approaches do not work well in this setting.  The proposed method is evaluated on learning on graphs on both synthetic and real-world graph tasks, and is shown to improve generalization in neural machine translation on WMT’14. The method is also tested on visual question answering (CLEVR) tasks, where it is shown that the proposed method performs better than existing methods. The main contribution of the paper is that the model is able to capture the geometry of objects in the scene, which is a key factor in the generalization performance of neural representations. In particular, the authors show how the semantic distance between two objects in a scene can be reduced to a single point in the space, which can be used to represent objects in graphs. "
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper studies the problem of DNN fingerprinting attacks on black-box networks. The authors propose two new attacks that exploit the architecture information of deep neural networks (DNN) to extract architecture information from the cache side-channels of the victim model. The attacks are based on the threat model of a co-attached process, where the victim and the attacker share a shared framework, and the co-location process is a deep learning (DL) system.    The authors show that the attack on the architecture of a DNN can be seen as a variant of the attack against a meta-model of a pretrained model in a transfer learning setting. The attack, called DeepRecon, is an attack that uses a cache side channel technique called Flush+Reload to extract internal information from a shared network architecture.  In the paper, the authors also show that for complex networks with forward propagation (e.g., VGG19 and ResNet50), the attack can be viewed as an extension of the existing framework-level defense techniques.  The paper also provides an empirical security analysis that shows that DNNs’ vulnerability to cache-side-channel attacks is highly correlated with the number of layers and the architecture attributes of the network.  Finally, the paper also shows that the fingerprinting process can be used to transfer the information of a pre-trained DNN to a different model. "
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a new representational hierarchy for predicting future video frames, which is composed of spatiotemporal memories in a hierarchical hierarchy. The proposed Hierarchical Prediction Network (HPNet) is a hierarchical network model that learns to encode and store spatiotmporal memories through recurrent connections. The model is based on feedforward, feedback and lateral recurrent circuits from the mammalian hierarchical visual system, and the model consists of a feed-forward path to encode spatiotamporal features and a feedback path to predict future frames. The authors show that the hierarchy is able to learn internal memory states that are invariant to prediction errors on a frame-to-frame basis, and that this level of representation learning can be applied to long range video sequence predictions on standard benchmark datasets. The paper also shows that the model can learn representations of global movement patterns, which are similar to the memories of early visual cortex. Finally, the paper shows that predictive self-supervised learning for representational learning in the visual cortex can be learned through hierarchical interaction between layers of the network."
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper proposes a method to learn continuous embeddings for kmers from raw RNA-seq data. The idea is to use a model to learn DNA sequence similarity and DNA sequence abundance in the embedding latent space, and then use the learned vectors to predict the location of exon information in the exon embedding. The authors show that the learned embedding of exons in the latent space can be used for the detection of genomic abnormalities (e.g. translocations and patient-specific mutations) and identify genomic abnormalities in the representation space for visualization and analysis. They also show that their method is able to identify exons that are not present in the original RNA-Seq data for acute myeloid leukemia patients, and compare them to known gene sub-structure."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes a new approach for model compression in the architecture space, called Architecture Compression. The key idea is to learn a mapping from a 1-D CNN encoder/decoder to a network in the weight or filter space of the network. The mapping is learned by gradient descent in the continuous space, and the mapping from the continuous embedding to the back is used to compute a compression objective function that maximizes both the accuracy and the parameter count. This embedding is then used to reduce the parameter number of the compression phase. The authors show that this approach outperforms existing model compression methods on several visual recognition tasks (CIFAR-10/100, FMNIST, SVHN). The authors also show that the discrete architecture can be decomposed into a discrete feature and a discrete architecture in the decoder, and that this discrete architecture space can be used to compress a dataset."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper proposes a “plan online and learn offline” framework that combines local model-based control, global value function learning, and exploration via local trajectory optimization. The key idea is to use approximate value functions to guide the planning horizon and learn policies based on the approximate value function. The value function approximation is done by temporally coordinated exploration via trajectory optimization and estimating uncertainty for value function approximations. The paper also proposes to learn an internal model that can be used to predict the value function of the current state of the system.  The paper demonstrates the effectiveness of the proposed components on a variety of control tasks, including humanoid locomotion and dexterous in-hand manipulation.  "
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper proposes a new approach to parallelize Neural Machine Translation (NMT) systems using parallel data. The proposed approach combines zero-shot and dual learning, where the former is based on unsupervised and semi-supervised methods and the latter uses reinforcement learning to learn the duality of the machine translation task. The authors show that the zero-shoot dual system outperforms the standard NMT system on the UN corpus of English-French and English-Spanish, and outperforms an LSTM-based UMT system in the en−–fr task. They also show that their zero- shot dual system can outperform the standard UMT on SpanishFrench and SpanishFrench.    The authors also demonstrate that the proposed approach can be applied to other low-resource languages where monolingual data is not available (e.g., French-German, Spanish-English).  The paper also shows that their proposed zero-shots dual method outperforms a LSTMs-based, LMT-based and L2L-based NMT method on en− →fr task, which is a machine translation problem where the goal is to translate from one language to another in a single pass. The paper shows that this is an important problem that is often overlooked in the literature, especially in the context of unsupervision.  The results are reported in the paper in the form of newstest2014, which shows that for the en–fr–fr (French-German) task, the proposed zero–unsupervised method performs better than the standard LMT system. For the proposed method performs slightly worse than the LSTm-based one, but performs slightly better than a standard L1-based method. On the other hand, the authors report that their method performs much better on the en de–fr−f–en task, outperforming the LMT method.  Finally, the paper also reports results on the Fr−–en–fr + en−f task, where they show that they outperform their proposed method in terms of performance. "
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper proposes Generative Adversarial Networks (GANs), a framework for Information-Retrieval (IR) based on the framework of IRGAN. The idea is to train multiple models in an adversarial fashion on multiple domains, each of which is a different task, and each task is modeled as a conditional probability distribution. The generator of the distribution is trained in a minimax loss function, and the objective is to minimize the loss of the generator on the target domain. The adversarial formulation of the loss curves is similar to that of the original IRGAN, except that the loss functions are learned in a co-training like setup, where the model is trained on the source domain and target domain separately.   "
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"This paper studies the problem of approximate inference for variational auto-encoders (VAEs) for approximate inference in intractable generative models. The authors consider the problem that VAEs learn latent codes that are highly sparsified, and that these representations can be used for various auxiliary tasks (e.g. human interpretation, classification, etc.). The authors propose a new approach to approximate inference based on the observation that the sparsity in the latent space of a VAE is related to the Spike and Slab prior distribution over the latent dimensions of the input data. They derive an evidence lower bound for approximate posterior inference using a discrete mixture recognition function, and show that this approach is able to learn sparse representations that are more interpretable than VAE representations. They also show that the sparse representations learned by the proposed approach are more robust to noise in the input, and are more likely to be subjectively understandable sources of variation. Finally, the authors show that sparse elements are also able to be used to learn subjectively intuitively intuitive sources of variations that are not present in the original latent space.    The authors demonstrate that their approach can be applied to learning sparse representations in the context of classification accuracy and robustness in the presence of noise in MNIST, and shows that the learned sparse representations are able to generalize better than the original VAE representation, and can also generalize to other types of data. "
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper proposes a non-adversarial feature matching-based approach for training generative models. The proposed approach, Generative Feature Matching Networks (GFMN), uses pretrained neural networks (autoencoders, ConvNet classifiers, etc.) for feature extraction. The approach is based on first order statistics, and is evaluated on challenging datasets (CIFAR10, STL10, and ImageNet). The results show that the features extracted by the pretrained ImageNet classifier are more robust to adversarial attacks. "
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"Graph Neural Networks (GNNs) are widely used in the representation learning of graphs, but they are not well-studied. This paper proposes a neighborhood aggregation scheme for GNNs, where each node is represented as a representation vector, and the goal is to learn the representation vector for each node. The paper shows that GNN variants of GNN are able to achieve state-of-the-art performance on both node and graph classification tasks.  The paper also proposes a theoretical framework to study the graph structures learned by GNN for graph representation learning, and shows that the GNN can learn graph structures that are invariant to the WeisfeilerLehman graph isomorphism test.  Experiments are conducted on two popular GNN architectures, Graph Convolutional Networks and GraphSAGE, and show that the proposed architecture is able to learn more complex graph structures than previous GNN-based methods. The model is also shown to perform well on several graph classification benchmarks. "
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"This paper proposes a new framework for interpretable continual learning (ICL) based on the variational continual learning framework. The authors show that the ICL idea can be applied to several existing continual learning approaches and show that this improves the performance of ICL. They also propose a new metric based on saliency maps to evaluate the overall continual learning performance, which shows that ICL improves the average classification accuracy and reduces the risk of catastrophic forgetting. "
SP:27a565b3e5442b93d208652784051e640b0c1bfe,This paper proposes a new evaluation framework for evaluating the robustness of machine translation (MT) models to adversarial attacks. Adversarial examples are generated by perturbations to the model that are meant to preserve the semantics of the input word. The authors show that seq2seq models can be robust to a variety of adversarial examples. They also show that adversarial training under meaning-preserving attacks improves adversarial robustness when the model is trained on a large number of examples.    The authors also propose a way to enforce the meaning preservation in the training process. They show that existing methods do not work well because they require human and automatic evaluation. They then propose a set of constraints for word-based MT systems to ensure that their models are robust to such attacks. 
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper studies the problem of learning policies that are robust to mis-actions in reinforcement learning algorithms. The authors show that policies trained with rewards and inverse (negative) rewards are more robust than policies trained using only inverse rewards. They also show that such policies are more sensitive to the inverse rewards than standard policies. Finally, they show that hybrid polices trained with deep Q-learning, double Q-networks, and on-policy actor-critic outperform algorithms trained with only inverse policies.   "
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper proposes a new formulation for learning a hierarchical, disentangled object representation and a dynamics model from unlabeled videos with object parts, hierarchical structure, and system dynamics. The hierarchical structure is learned by learning a structural descriptor for low-level concepts, and the hierarchy is learned using the structural descriptor. The object parts are represented as a layered image representation. The PSD model is evaluated on both real and synthetic datasets, and is shown to perform well on two tasks: (1) segmenting object parts and (2) learning motion distributions."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"Large-scale datasets with noisy (inaccurate) class labels are often used to train deep neural networks on noisy training datasets. The authors propose a new inference method called Deep Determinantal Generative Classifier (DDGC), which uses a softmax neural classifier trained on noisy datasets to learn the decision boundary. The generative classifier is trained on the hidden feature spaces of a discriminative deep model with a minimum covariance determinant estimator. DDGC is shown to be robust to adversarial perturbations on noisy labels and adversarial samples.  The authors show that DDGC can be trained with different training techniques to handle noisy labels or adversarial data, and that the learning models trained with these different learning models are able to deal with noisy labels. They also show that the noise-handling training method is able to improve the test accuracy of the deep model on the CIFAR10 dataset with noisy training labels.   "
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes two approaches for Reinforcement Learning (RL) that are suitable for large-scale applications with huge state spaces and sparse delayed reward feedback. Specifically, the authors propose two action selection policies with temporal abstraction for Hierarchical Reinforcement learning (HRL) methods, where skill policies are learned to achieve subgoals. Abstraction is achieved by using an internal reward signal for each of the skills to guide the subgoal attainment. The approaches for subgoal discovery in HRL are based on two approaches: (1) a model-free method that uses incremental unsupervised learning, and (2) an intrinsic motivation learning mechanism. The proposed approach is evaluated on a number of RL problems with sparse delayed feedback (e.g. Montezuma’s Revenge in the rooms environment and the ATARI 2600 game), and shows that the proposed method is able to discover subgo goals that are more likely to be achieved by the model. The authors also show that the intrinsic motivation for the skills learned in the proposed approach improves the performance of HRL. "
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a neural framework for solving the Circuit Satisfiability problem in Representation Learning and Formal Methods (Neuro-Symbolic Methods). The model is trained to solve the SAT problem, and a rich embedding architecture is used to capture the problem structure. The proposed framework is based on the rich neural architectures and the end-to-end differentiable training procedure for Reinforcement Learning. The authors show that the proposed framework can achieve better out-of-sample generalization compared to the NeuroSAT method. "
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes two approaches to policy search: Deep neuroevolution and deep reinforcement learning (deep RL) algorithms. The former is based on the observation that in the hyper-parameter setting, the off-policy deep RL algorithm is more sample efficient than the one based on DDPG. The authors propose to combine them into a single approach, which they call CEM-RL. They propose an ad hoc evolutionary algorithm, a goal exploration process, and a Deep Deterministic Policy Gradient (DDPG) algorithm, which is a sample efficient version of the D.D.P.G. algorithm. They also propose a combination scheme based on cross-entropy method (CEM) to improve the sample efficiency of the proposed combination scheme. Experiments show that the proposed method outperforms the state-of-the-art method in terms of sample efficiency."
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes a new predictive model for multivariate time series. The proposed model, called IMV-LSTM, aims to improve the performance of forecasting and forecasting performance of a model that is trained on a set of target and exogenous variables. The authors propose to use a latent variable-wise LSTM to model the variableswise hidden states of the target and the exogenous variable, and then use a hidden state matrix and an update process to update the latent state of the latent variable. They also propose a mixture attention mechanism and two summarization methods to capture temporal and variable importance. The performance on real datasets is compared with several baselines, and it shows better performance. It also proposes an end-to-end framework for forecasting and knowledge extraction from multi-variate data. "
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes a new data augmentation method called feature smoothing, which aims to reduce the computational overhead and improve the robustness of defenses against adversarial attacks. The key idea is to use the interpolation of features from virtual training data to train a neural network on the virtual data points, and then use the learned features to improve the adversarial examples that can be used as defenses. The paper shows that feature smooting can improve both adversarial and clean accuracy on MNIST and CIFAR10 datasets. The authors also show that the proposed methods, weight decay, logit squeezing, mix up, label smoothing and weight decay can all be used to improve unbiased estimation of the decision boundary with lower estimated variance.   The paper is well-written, well-motivated, and well-organized. The idea is interesting, and the experiments are well-designed. However, there are a few issues in the paper, and there is a lack of comparison with existing data augmentation methods, and it would be better if the authors could provide a unified framework to explain their findings. "
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for disentangling networks with ReLU nonlinearity, such as a deep convolutional neural network (DCNN) which is a deep and locally connected nonlinear network with a projection nature. The framework is based on the teacher-student setting, where the teacher’s computational graph is shared with the student, and the goal is to learn disentangled representations in deep networks. The authors show that the proposed framework is able to disentangle the data distribution under certain gradient descent rules and regularization techniques such as Batch Norm. They also show that unrealistic assumptions such as Gaussian inputs and independence of activation can be avoided. "
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper proposes a modular architecture of neural networks that includes a Behavioral Module (BM) and an end-to-end training strategy. The authors claim that this approach is able to improve the learning of behaviors, preferences representation, and the human behavior formation process in the Pre-Preferential cortex (PFC) to learn a behavior repertoire. They show that this property is useful for both user modeling and recommendation tasks. The method is evaluated on video games playing, where dialog agents are asked to learn personalized representations of different user states. They also show that the proposed strategy can be applied to transfer of newly learned BMs from one task to another. The paper also shows that the independent learning of new behavior patterns can improve network extendability. "
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper studies the role of plastic changes in synaptic connectivity in lifelong learning. It shows that neuromodulation is responsible for these changes and that the self-modifying abilities of the brain in biological reinforcement learning are related to these changes. Based on this observation, the authors propose a differentiable formulation of the neurmodulation of plasticity that can be used to train artificial neural networks via gradient descent. The authors show that the neural networks trained with neuromodeated plasticity outperform LSTMs on reinforcement learning and supervised learning tasks. They also show that neural networks with differentiable Hebbian plasticity are able to learn more efficiently than neural networks that do not use the differentiable neurmodes. Finally, they show that a benchmark language modeling task on which they show the superiority of neuromoderated plastic LSTM over LSTm on this task.   "
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper proposes a non-intrusive quantization technique based on a full precision model trained with a binary quantization. The main idea is that existing quantization techniques can reduce the inference latency/memory consumption by reducing the number of parameters in Deep Neural Networks. The authors propose two techniques to achieve this. First, the authors propose a binary model that can be used to reduce the size of the quantization training process compared to the original training process. Second, they propose a new loss function to achieve a reduced quantization error.   The authors show that binary quantisation can achieve full precision accuracy with 2 bit quantization, and that the proposed techniques can be applied to the CIFAR dataset. They also show that a 1.5 bits hybrid model outperforms the original TWN LSTM model on WikiText-2 and ImageNet."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a method for learning content embedding using a deep metric-learning technique. The method is based on a variational autoencoder (VAE) with a content encoder and a to-be-trained style encoder, where the style is modelled as a set of class-irrelevant properties, and the VAE reconstruction loss is used to capture the relationship between content and style.   The method combines an auxiliary loss, leakage filtering, and a novel method for data-set augmentation based on Recombinations for data set augmentation.  This is a domain-independent method that can be applied to few-shot learning tasks.  The paper proposes an approach for content-style decomposition and recombination. Decompositions are used for classification, and decompositions of the content and the style are used to augment the content representation of the style representation with the style information from the content.  For the face-recognition task, the paper shows that decomposing the content into within-class variation and content-based decomposition results in better performance. The paper also shows that the proposed approach STOC (Stochastic Open-Ended Content) can be used to learn content and content based on supervised training to learn both style and content representations. STOC is an extension of the original objective of STOC, which is leakage filtering.  Experiments are performed on a few standard few shot learning tasks, including lighting, lighting, pose, expression, and hairstyle, and on an emotion-reconstruction task. The results show that STOCD and STOC are able to generalize to a range of different types of content, and that the approach can generalize well to different domains. The approach also generalizes well to other approaches that rely on specific domain knowledge (e.g., human body pose, music composition).  "
SP:d37e15cde7765fca87595a242f0a4511b3346d46,This paper proposes a method for deep reinforcement learning (deep RL) training to tackle problems with state-action permissibility (SAP). The authors show that there is a strong correlation between the performance of deep RL algorithms with respect to the SAP property and the state of the art on a number of problems. The authors also show that the permutation of the SAP is a property that can be used to guide the training process. The paper also shows that the SAP guidance can be applied to the training of existing RL algorithms to improve the performance.
SP:20015d8b60e13300586b67c281858cbe28825c48," autoencoder models have been shown to suffer from phase transition in large dimensions. This paper proposes a random deep weight-tied multilayer vanilla autoencoders with random weights. The authors show that the phase transition phenomena is due to reversibility of the Lipschitz activations of the weights. They then propose a training initialization practice where the weights are randomly permuted. They also show that this random deep loss can be used for approximate inference.   The authors also propose a few analytical techniques to explain the phenomenon. They show that under certain assumptions on the weights, deep autoencopers are more robust than their shallow counterparts. They further propose two techniques: layer-wise pre-training and batch normalization. Finally, they show that a deep Autoencoder trained with tanh activation is more robust. "
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper considers the search problem for the construction of adversarial images, which is a search problem in which the goal is to construct an adversarial image that can be used to fool a model. Model evaluations are used as sporadic feedback, where the model evaluations are based on model evaluations and the adversarial loss is based on the loss of a low frequency component in the discrete cosine transform (DCT). The authors propose an algorithm that uses an iterative principle to find the best search strategy. The proposed method is shown to be effective for both targeted and untargeted attacks. The method is evaluated on median queries for Google Cloud Vision, and the method is also shown to improve query efficiency. The algorithm is also applied to adversarial black-box attacks, where it is trained using PyTorch code. Experiments are performed on ResNet-50 and ResNet50-50, and it is shown that the method can be applied to a single adversarial ImageNet image. "
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"This paper proposes a method for learning task-agnostic transferable transferable skills that can be used to tackle the curse of dimensionality in learning temporal abstractions in Hierarchical Reinforcement Learning. The authors propose a hierarchical framework, the options framework, where the goal is to discover a set of options (or “bottleneck states”) that are transferable across different tasks. Option discovery is performed using heuristics that are based on previous work on discovering bottlenecks. The proposed method, called Successor options (successor options), is a model that uses Successor representations learned from primitive actions as options, and then uses a pseudo-reward to learn intra-option policies. The options are learned using an Incremental Successor Options model, which is trained to discover options that are “stabilized” in the bottleneck states (i.e., in which the “landmark” sub-goals are found). The authors show that the proposed method outperforms existing methods for discovering bottleneck states in well connected regions, and outperforms methods that do not consider sub-goal states. The approach is evaluated on grid worlds and complex high dimensional environments (e.g. Deepmind-Lab). "
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper considers the problem of domain division, i. In this problem, recognition tasks such as Open Set Learning (OSL) are partitioned into known, unknown and uncertain domains, and the goal is to train classifiers that generalize well across all domains. The authors propose a probabilistic way to compute the decision boundary between the known and unknown domains, based on the notion of ""probabilistic distributions"". They propose a domain division algorithm that can be applied to a wide range of recognition tasks in a single domain. The proposed framework is based on WSVM, and they use a number of statistical tools such as bootstrapping and KolmogorovSmirnov (K-S) Test to compute a decision boundary. They evaluate their approach on the OSL and G-ZSL benchmarks, and show that their framework can generalize to an uncertain domain."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper considers the problem of training a neural network for classification and regression with different layout structures. The authors propose three solutions: softmax cross-entropy, mean squared error, and simplicity (Occam’s Razor). These networks are called polar prototype networks, and the structure is based on polar prototypes. The layout of a polar prototype is defined as a hypersphere, where the layout of each layer corresponds to a set of class prototypes. In this way, the authors show that polar prototypes can be used to learn the structure of the network, and that they have a maximal separation between the output dimension and the input dimension. The paper also shows that minimizing angular distances between two prototypes is equivalent to minimizing the distance between the outputs of the two classes.  The authors also show that training for regression with higher-dimensional outputs can be improved by training with polar interpolation. The main contribution of the paper is that the polar prototype network is able to learn a mapping from the input dimensions to the output dimensions, which is an important property for classification.  Experiments are conducted on regression and classification, and show that the classification performance of the proposed network methods is comparable to state-of-the-art network methods.  "
SP:d1034342785d133cf8372b8624897963cc2ee83a,"This paper studies the problem of learning features that are useful for RL agents to learn. In particular, the authors focus on the problem that RL agents can learn features that can be used to learn preferences. The authors propose a method to learn such features by learning a reward function that maximizes the mutual information between the reward function and the features learned by the agent and the robot. The idea is motivated by the observation that implicit preference information is not always useful to learn, and that there are often side effects of learning such features. The proposed algorithm is based on Maximum Causal Entropy IRL, and it is tested on several proof-of-concept environments."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed modeling and inference framework combines the advantages of both deep generative models and probabilistic graphical models. Specifically, a variational autoencoder (VAE) with a Bayesian network with flexible dependency structure is learned in the latent variable space. Inference is performed using top-down and bottom-up reasoning over the latent variables values, and a single objective is used to learn the network parameters, variational parameters, and latent topology. In addition, a sampling procedure is proposed to improve the quality of the learned latent variable structures. Experiments on MNIST, Omniglot, and CIFAR-10 show that the proposed framework outperforms structured variational auto-encoder baselines. "
SP:976dedab53e69610692a563382ada1dbb82c1e9d,"This paper proposes a novel dynamical neural network with interconnected neurons that can be used to learn numerical solutions to mathematical optimization or learning problems. It has a number of interesting computational properties: it has a massively parallel computer architecture that allows for power and throughput efficiency, and it can store local information (e.g. local memory) in the state space. The authors show that a dynamical network trained with spiking neurons is able to learn gradients for the `1-minimizing dictionary learning problem with top-down feedback and contrastive learning. They also show that the gradients used for learning are non-trivial to compute, and that the computational system can be decomposed into two parts: (1) a computational system that takes as input the current state of the system, and (2) a learning process that takes the current and previous state as input, and outputs the next state.   The authors also provide a theoretical analysis of dictionary learning problems, showing that the learned gradients can be efficiently computed. "
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"This paper proposes a new class of Convolutional neural networks (CNNs) for the task of semantic image segmentation and lane detection. The authors show that such nets have a spatial pyramid structure and an encoder-decoder structure, and that these nets can be used for both semantic images segmentation as well as for lane detection tasks. In particular, they show that a network trained on the semantic segmentation task can be applied to the lane detection task as well.    The authors also show that, in the case of model-based lane detection, a network can be trained on a single image and then used to train a network for multiple images. They show that this network can also be trained to be able to be used in a multi-scale context and to achieve pixel-level accuracy.  Finally, the authors also propose a new evaluation methods to evaluate the performance of lane detection based on the encoder - decoders module.  The paper is well-written and well-motivated, and the proposed methods are interesting. However, there are a few issues in the paper that need to be addressed (weak visual appearance and prior information).  "
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper proposes a novel approach to batch contextual bandit learning based on Maximum Likelihood Inverse Propensity Scoring (MLIPS) based on logged bandit feedback. The authors show that the proposed approach outperforms existing approaches based on inverse propensity weights (Inverse Proposition Scoring and Policy Optimizer for Exponential Models (POEM) in both supervised learning and online learning settings, and outperforms the state-of-the-art results in the online and interactive systems in the batch learning setting (e.g., ad platforms and recommendation systems). The authors also show that a maximum likelihood surrogate policy is learned from logged action-contextual pairs, and that this surrogate policy performs better than a historical policy trained on the same logged feedback. In addition, they show that MLIPS achieves a nonasymptotic mean squared error that is at least as good as that of IPS, which is a lower bound of the mean square error of the log-likelihood of the historical policy. They further show that their surrogate policy technique outperforms other error reduction techniques, and is competitive with existing approaches that do not use the surrogate policy. Finally, they evaluate MLIPS on multi-label classification problems and a large-scale ad placement dataset, and show that they outperform previous work. "
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"Meta-learning is a learning framework for few-shot classification tasks. In meta-learning methods, a meta-learner is used to guide model optimization, parameter initialization, and similarity metric. In this paper, the authors propose to use individualized feature embedding for classifying, and use a kernel generator to learn the feature embeddings for each query image, and then fine-tune the query images based on the individualized features. The authors show that the meta-knowledge of the convolutional kernels learned during training improves the performance of the proposed method on standard few-set classification data sets such as Omniglot and miniImageNet. "
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"Deep artificial neural networks (DNNs) have been the focus of a lot of work in recent years, but gradient-based learning algorithms (e.g., backpropagation) have not yet been considered. Evolution strategies (ES) have recently been shown to outperform backprop-based algorithms such as Q-learning and policy gradients in a variety of deep reinforcement learning (RL) problems, and it is well-known that Evolution strategies are more efficient than backprop - based algorithms in many deep RL problems such as Atari and humanoid locomotion.    This paper studies the evolution of DNN scales using non-gradient-based evolutionary algorithms. The authors propose Deep GA, which is an evolutionary algorithm for training neural networks with free parameters. They show that it can be used to accelerate stochastic gradient descent by using an operation called the finite-difference approximation of the gradient. They also show that Deep GA is able to learn networks with networks that have free parameters, and that it performs well in a range of environments.  The paper also shows that reward-maximizing algorithms (such as A3C, DQN, ES, and GA) can be adapted to solve a high-dimensional problem using DNNs and novelty search. "
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a novel approach to learning from sparsity in reinforcement learning algorithms. Rewards in RL algorithms are often sparsified to encourage the agent to learn from the sparsity. The authors propose a curiosity method based on an episodic memory that is used as a novelty bonus for learning. The approach is tested on visually rich 3D environments such as VizDoom, DMLab, and MuJoCo, and is shown to outperform existing RL algorithms. The agent outperforms the previous curiosity method, ICM, on a number of navigational tasks.    The authors also show that the agent is able to learn to explore more efficiently than the previous agent, and that the behavior of the agent can be explained as a result of the use of a curiosity module in ant, which is a variant of an ant that is trained on the same environment dynamics as the real task reward. The paper also shows that first-person-view curiosity can be used as an additional reward for the agent."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper proposes a new representation for transition models in complex uncertain domains based on relational rules. Feed-forward neural networks are used to model the transition distribution, and an iterative greedy algorithm is used to learn deictic references. The proposed strategy is shown to outperform a monolithic transition model in a simulated domain. "
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a new instance-wise feature selection method called INVASE, which is a new method for selecting the best feature from big data. The idea of INVASE is to use a combination of neural networks, a selector network, a predictor network, and a baseline network. The selector network is trained using an actor-critic methodology, and the baseline network is used to train the selector network and the predictor network.    The authors show that the proposed methodology is able to outperform existing state-of-the-art methods on both synthetic and real data experiments. They also show that INVASE outperforms other state- of-the art benchmarks in terms of global feature selection, and outperforms existing methods for instance-wide feature selection. "
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes to use per-pixel annotations to improve the performance of supervised models such as convolutional neural networks. Predicting structured outputs (i.e., semantic segmentation) from per-pixels annotations is an important problem in the context of model finetuning. The authors propose to learn discriminative feature representations of patches in a disentangled space based on label histograms. They also propose an adversarial learning scheme to learn feature representations that can be used to guide the training of the models. The proposed framework is based on a global alignment process and patch-level alignment. The paper also proposes a domain adaptation method to learn representations that are useful for guidance. Experiments on benchmark datasets show that the proposed framework achieves state-of-the-art performance on semantic segmentations. "
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper studies the problem of online learning of optimization algorithms. In particular, the authors consider the case where a mini-batch of stochastic gradients is available for training deep neural nets. They show that optimistic algorithms such as AMSGrad and Adam can be learned from the predictability of gradients. They also show that algorithms based on the momentum method, adaptive gradient method, and algorithms inspired by these algorithms can be used for OPTIMISTIC ONLINE LEARNING. The paper is well-written and well-motivated. However, there is a lack of comparison with the online learning literature."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper introduces a new benchmark, IMAGENET-C, to evaluate the robustness of image classifier robustness against common perturbations. The benchmark is designed to address the corruption robustness topic in the context of safety-critical applications, where classifiers are used to protect the privacy of users in the presence of adversarial attacks. The authors also introduce a new dataset called IMAGENTET-P, which is a generalization of the original dataset IMAGenet-C.  The authors show that on this new dataset, the classifier’s robustness to common adversarial perturbation is robust to a range of perturbings, including common corruptions as well as adversarial examples.  They also show that the new benchmark is more robust to perturbational examples than the worst-case adversarial robustness, and that adversarial samples are more likely to be perturbed in this new benchmark.  Finally, they show that AlexNet classifiers and ResNet classifier achieve relative corruption robustity, and bypassed adversarial defense achieves a similar level of robustness.    Overall, the paper is well-written, well-motivated, and well-designed. However, there are a few issues that need to be addressed, and the paper suffers from a lack of clarity, and it is not clear to me that the authors have made any significant contributions to the field. "
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper studies the problem of dropout in conditional models. The authors propose a new MAP estimation for dropout training, which they call deterministic subvariant’s bound. They show that the power mean of a family of models with sampled dropout masks can be approximated by lower bounds on stochastic subvariants of the dropout objective. They also show that deterministic dropout can be used for MC averaging.  It is interesting to see that this is a generalization of previous work on regularisation-heavy language modelling.  "
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a robust pruning method called GSFP, which aims to remove redundant filters in Convolutional Neural Networks (CNNs). The authors argue that global redundancy is the main cause of excessive pruning rate and propose a soft pruning strategy for GSFP. The authors also propose a cumulative saliency strategy to improve the accuracy of pruning in the model recovery process. The saliency of a filter depends on the number of layers in the network, and the authors show that pruning based on saliency is more effective than local pruning.  The authors further propose a normalization formula for the layers of filters in a network that can be applied to any pre-trained CNN model. They evaluate GSFP on various CNN architectures and data sets, and show that it improves the test accuracy on MNIST and CIFAR10. They also show that GSFP improves the compression ratio of the model and that it can be used as a pruning guidance.  Finally, the authors propose to use GSFP for both global and soft prune strategies. "
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a novel transfer learning scheme for cross-lingual subword similarity with limited training data based on a character-based embedder and a word-based classifier. Text classification is done by learning a joint character representation of the input word and its subword. The embedder is used to learn vector representations in written forms, and the classifier is trained on word vectors. The model is trained with a multi-task objective. The authors show that CACO models trained in low-resource settings outperform state-of-the-art cross-langual word embedding models trained with related language pairs in high-resource setting. The paper also shows that the model can be trained to transfer from one language to another without access to cross-limiting or monolingual resources. "
SP:544e421f9c747640d949f433e3091763508b7237,"This paper proposes the marginalized average attentional network (MAAN), which is a generalization of marginalized average aggregation (MAAC) to the case of dense and integral regions. MAAN is based on the marginalization of the latent discriminative probabilities in the MAA module. The authors show that MAAN can be used to learn dense and Integral action regions by maximizing the overestimation of the most salient regions.  The authors also propose a new algorithm for MAA to reduce the complexity of the algorithm.   The main contribution of the paper is the proposed MAAN, which uses the marginalized average aggregation(MAAN) module in MAAC to learn the class activation sequences of the input video snippet features, and then uses the averaged subset features from the input clips to train MAAN.  Experiments are conducted on large-scale video datasets to demonstrate the effectiveness of MAAN for weakly-supervised temporal action localization on a variety of tasks. "
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper studies the problem of training neural models for Natural Language Processing with structureless distributed representations. The authors argue that existing language models do not take into account the linguistic structures that are present in natural language, and propose Holographic Reduced Representation (HRR) to address this problem. They show that models trained with HRR are able to learn more complex structures in wordlevel and chunk-level representations than models trained without this representational form. They also show that HRR can be used to train models to learn crude linguistic roles, and that models can be trained to learn a structured compositional representation. "
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"This paper considers the problem of decision-making in partially observable Markov decision processes (POMDPs) where the stochastic outcome of the decision process is unknown. The authors propose a point-based value iteration algorithm that incorporates a greedy strategy in the form of a greedy algorithm for observation selection to achieve near-optimal uncertainty reduction from sampled belief points. The solver is trained to find a reachable subspace of belief simplex that minimizes the uncertainty of the known distribution. The paper shows that this solver can be applied to several real-world scenarios, including active perception, planning, and decision making in robotic scenarios. The main contribution of the paper is that the selection process can be more efficient than existing computations for both perception decision and planning decision.    The paper is well-written and well-motivated, and the authors provide a thorough analysis of the POMDP models and the results are interesting. "
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"Deep neural networks have been shown to be sensitive to distribution changes in weight of top layers during training.  Deep neural networks can be seen as a function of the number of weights of the top layers.  Curriculum learning is used to train a network in the presence of such distribution shifts.    In this paper, the authors show that the internal covariate shift in the network forward pass is related to the data complexity of the network training, i.e., the amount of data that the network needs to store.  The authors propose a new curriculum loss that combines adaptive weight and representation loss to encourage low weighted samples to be used in the backward pass.  In the forward pass, the distribution shifts are caused by the fluctuation of the weights in the embedding space during training, which causes the hard examples to be hard to learn due to noisy gradients.  Low-weighted data is then used as hard samples, and the representation loss is used for low-weighting the hard samples.  This curriculum loss is then combined with standard stochastic algorithms such as SGD.  Experiments on benchmark datasets are conducted to show the effectiveness of the proposed curriculum loss, which is based on random sampling. "
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper studies the use of heuristics for combinatorial optimization problems. The authors propose a model that uses REINFORCE to learn a baseline for a deterministic greedy rollout, where the baseline is learned as a value function. The model is trained with attention layers, and the authors show that it can be trained to solve the Travelling Salesman Problem (TSP) and the Orienteering Problem (OP). The authors also show that heuristic heuristic can be applied to the Vehicle Routing Problem (VRP) and a variant of the Traveller Problem (OTP).   The authors further show that the model can also be trained with REinFORCE for the Pointer Network (PCTSP) problem.    Finally, the authors demonstrate that the proposed models can be used to solve a number of existing problems. They show that their heuristic outperforms baselines on the Prize Collecting TSP (PTCTSP), and they show that they can also outperform the baselines for other heuristic heuristic methods for the Travellers Problem (OTT).  The paper also shows that their model is able to outperform baselines when the number of hyperparameters is small."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper studies the time and space complexity of neural network inference with network quantization. The authors propose a differentiable neural architecture search (DNAS) framework for the exponential search space based on gradient-based optimization. The problem is formulated as a neural network search problem, where the goal is to find the optimal design space that maximizes the performance of the network. This is an important problem with limited computational and memory resources in embedded and mobile devices, and the authors show that existing quantization methods do not scale well when the design space is limited. The paper also shows that quantized models with larger bit-widths perform better than full precision models in terms of model size and computational cost on CIFAR-10 and ImageNet. "
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"This paper studies the effect of attention on neural architectures. The authors show that the attention used in the decoding stage of a neural architecture is related to the posterior attention distribution. They also show that posterior attention models have better BLEU score and alignment accuracy compared to other attention models on translation, morphological inflection tasks.    The authors also propose two new attention architectures, called Posterior Attention Models (PAMs), which are based on the idea that the architecture can be seen as a weighted sum of attention distributions across layers. "
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper tackles the problem of unpaired image-to-image translation, which is a manifold view of the problem where artifacts and degenerated transformations are present in the input images. The authors propose a new method called ""HarmonicGAN"" for bi-directional translations, where the goal is to learn consistent mappings between the input and the target image.    The key idea is to add a smoothness term to the harmonic functions in the sample graph to ensure that the resulting mapping is self-consistent and that it does not rely on pixel- to-pixel supervision.  The authors show that the inherent selfconsistency property of the proposed method, which they refer to as similarity - consistency, is a result of the fact that the smoothness of harmonic functions can be decomposed into two terms: (1) the mean-squared error of the resulting translation, and (2) the training-time cost.  Distance metrics are defined based on features such as histogram, CNN, etc.  Experiments are conducted on three different applications: medical imaging, object transfiguration and semantic labeling. The results show that compared to CycleGAN, the proposed HarmonicGAN achieves better interpretability and outperforms the state of the art on all three tasks. The method is also applied to a medical imaging task, where it is shown that the method can be applied to the case where the manual inputs are not available. "
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"Recurrent neural networks have been shown to suffer from the exploding and vanishing gradient problem (EVGP). This paper proposes a new stochastic algorithm (h-detach) for LSTM optimization to solve this problem. EVGP is the problem that arises when the gradient components of LSTMs have long term dependencies. The authors propose to solve the problem by introducing a linear path (cell state) in the LSTT of the LMT, which is then decomposed into two components: (1) the gradient of the current layer and (2) the gradients of the previous layer.   The authors show that this path can be decomposed as follows:  (a) The first two components are the ones that depend on the current state of the network, and (b) the second one is the one that depends on the previous state.  The main contribution of the paper is that the authors propose a new algorithm h-detached, which computes the paths of the two components in the path, and then computes a solution to the problem.  (b). The authors also show that h-Detach converges to the optimal solution for all the components of the path.  In addition, the authors show how to compute the path of the first component, and how to calculate the path for the second component.  This paper also shows that the paths can be computed in a single step.  Finally, the paper shows how to find the path that minimizes the sum of the paths in the cell state, and the paths that minimise the sum over all the layers in the network.  It is shown that the path is non-trivial to compute, and that it is a linear function of the number of layers and the size of the cell.  Experiments are conducted on several benchmark datasets to demonstrate the generalization performance of the proposed algorithm. The results show that the convergence speed and robustness of the algorithm is better than the vanilla L STM gradient based training. The generalization speed is also improved when the number or seed, robustness, and learning rate is increased."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a new deep learning method for binary weight neural networks. The authors propose to learn the posterior distribution of the binary weights of a neural network using a Bayesian deep learning approach. The proposed method, called SnapQuant, is based on a reinforcement learning framework, where a policy network is trained to optimize the posterior of binary weights. The paper shows that the proposed method is able to achieve state-of-the-art performance on ImageNet classification tasks."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The authors propose a new inference approach for learning a global network, where the local neural network weights are shared across all clients. The proposed approach is evaluated on a number of federated classification and image classification problems, and the results show that the proposed approach outperforms the state-of-the-art baselines in terms of both supervision and data pooling.   "
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper proposes a new algorithm, Opponent-Learning Awareness (LOLA), which aims to improve the generalization ability of learning methods in differentiable games. The authors consider GANs, intrinsic curiosity, multi-agent RL, and other types of games, and show that their approach ( Opponent shaping) can improve the learning dynamics of these games. They also provide theoretical guarantees for their algorithms. Finally, they propose a method called LOLA and a stable variant called Stable Opponent Shaping (SOSP) which is a variant of LOLA. LOLA agents are shown to be able to learn to avoid the ‘arrogant’ behaviour of their opponents, and SOS is shown to learn a ‘strict saddle’ that prevents the learning of opponents that are ‘archaic’ in their behavior. LookAhead is also shown to find equilibria that avoid the Iterated Prisoner’s Dilemma. "
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes a learning system to detect rare events in the feature space of classifiers/regressors. The key idea is to learn a low dimensional feature space, where the prior information, such as shape feature, can be used to guide the learning of the classifiers / regressors. To do so, the authors propose a Variational Auto-Encoder(VAE) that learns a segmentation result by using the shape feature as a feature space. The shape feature is learned as a loss function, and the VAE is trained using ground truth masks. The VAE learns shapes that are more likely to be in the low dimensional space. This representation is learned in a one-dimensional feature space and the authors show that the learned representation is able to capture the qualities of segmentation results. The authors also show that bad shapes can be classified as a rare event. The proposed alarm system is tested on standard segmentation algorithms on a medical segmentation task, and it outperforms the state-of-the-art segmentation methods. The paper also shows that the proposed system is robust to the presence of bad shapes.   "
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"Deep neural networks, such as convolutional neural networks and deep decoders, are commonly used to solve inverse problems such as denoising, inpainting, and reconstruction with few and noisy measurements. Deep neural networks have been shown to be effective at compressing images and solving inverse problems with large datasets. However, these tools (e.g., wavelets) are computationally expensive compared to imagegenerating deep neural networks.   This paper proposes to use a deep neural network to generate natural images from an untrained simple image model such as a deep decoder. The authors show that the output dimension of the decoder is the same as that of the original image, and that the network weights are asymptotically similar to the wavelet-based thresholding.  The authors also show that underparameterization of the deepdecoder can lead to overfitting to the weight parameters of the image, which is a common problem in many inverse problems. The main contribution of the paper is that the authors propose a theoretical analysis that shows that the weights of the network can be learned to be similar to those of a wavelet. The paper also shows that this is the case for any upsampling unit, any pixel-wise linear combination of channels, any ReLU activation, and any channelwise normalization.  Finally, the paper shows that it is possible to train neural networks to solve these inverse problems and apply them to the signal representations learned by neural networks in order to improve their performance. The idea is to learn a set of convolutions that are similar to each other, but with a different output dimensionality."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"Program synthesis in natural language (NL) is an important problem in software development. This paper proposes SAPS, an end-to-end neural network for multi-sentence NL specifications. The architecture is based on abstract syntax trees, pretrained word embedding, and bi-directional multi-layer LSTM for processing of word sequences.  The architecture consists of three neural components: (1) a doubly-recurrent LSTMs, (2) signal propagation schemes, and (3) a soft attention mechanism for the decoder.  Experiments show that SAPS outperforms the state-of-the-art method by a large margin. The paper also shows that it can be combined with existing methods without post-processing, and that it is able to learn a fixed-dimensional latent representation for both the NL analyzer and the source code generator, which is useful for end-user programming."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper studies the problem of adversarial robustness of deep neural networks to tiny input perturbations in computer vision, such as MNIST. The authors show that a neural network model trained with L0 robustness (i.e. the L0 perturbation of the input) can be attacked by a decision-based adversarial attack. They also show that undefended networks can be robust to L0 attacks, but not to L2 attacks.    The authors propose a simple L∞ defense, which is based on input binarization. The idea is that if the input is binarized, the adversarial perturbated input will be classified as L0, L2 and L√2, and the L2 perturbings will be considered Lp norms. They show that under certain conditions, the Lp norm of L0 and L2 is equal to that of Lp-norm of the original input, and that L0 adversarial examples will be class-conditional on the input. They further show that if Lp and L0 norms are equal to Lp, the attack will not be able to classify the input correctly.  The paper also shows that a robust classification model trained on class-conditioned data distributions can be trained to be robust against adversarial attacks. The proposed approach is tested on MNIST, and is shown to improve the robustness to adversarial instances of MNIST in the presence of unrecognizable images, and to be more robust against L0 (and L2) attacks. In addition, the paper shows that the attack can be used to train a model that is robustly robust to a variety of types of attacks, including L0. "
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a new framework to control the spectra of weight matrices in the discriminator of Generative Adversarial Networks (GANs). The authors propose a reparameterization approach to train GANs by controlling the spectral of the weight matrix of a discriminator. The authors show that the slow singular value decays of the spectral matrices of the slow GAN discriminator are caused by two things: (1) the use of regularizers and (2) constraints on the spectrum of the light matrices. They show that these two factors can be combined to improve the performance of the GAN. They also show that spectrum control can be applied to the spectral normalization of the discriminators of GAN's to improve their performance.  The authors compare their method with existing methods on CIFAR-10, STL-10, and ImgaeNet datasets and show that their method outperforms the previous methods.   "
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,This paper proposes a new accelerated value iteration algorithm called Anderson Accelerated Value Iteration (A2VI) based on the Anderson acceleration technique for value iteration in reinforcement learning methods. The proposed method can be applied to any Deep Q-learning algorithm. The authors show that the proposed approach can be used for the approximation of the policy evaluation. They show that A2VI outperforms policy iteration in a number of toy problems and several Atari games. They also show that their algorithm is more robust to historical data. 
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,This paper proposes a method called SupportNet to tackle the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines deep learning and support vector machine (SVM) in the form of SupportNet. The authors propose two consolidation regularizers to improve the performance of the proposed model. Experimental results show that SupportNet outperforms the state-of-the-art deep learning model and other incremental learning methods on a number of tasks. 
SP:d228d213f79716774043cea253305fecece659ec,"This paper proposes two methods to measure the unit selectivity of representations learned by neural networks (NNs) from activation maximization (AM) images. The proposed measure, called top-class selectivity, is based on the observation that the top-ranked units in an RNN are the ones with the highest selectivity. The paper also proposes two other measures, i.e., precision and class-conditional mean activity selectivity (CCMAS), which are also used in AlexNet. The precision and CCMAS measures are used to measure selectivity in fc6 and conv5, and fc8, respectively.  The paper shows that RNNs trained with AlexNet are able to learn localist representations similar to the ones learned by RNN with fc2, fc4, and AlexNet, and that these representations are more selective than those learned by other methods. The authors also show that these selective units are more likely to be active in the same hidden layers, which is a common phenomenon in NNs. "
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper considers the problem of community detection using graph neural networks. Community detection is an important problem in many real-world applications, where the goal is to detect the presence of a group of nodes in a node-wise classification problem on graphs, which can be solved using spectral methods and posterior inference using probabilistic graphical models. Community detectors are typically based on the stochastic block model, which is a family of random graph families where the computational threshold is a function of the signal-to-noise ratio between the statistical and computational detection thresholds.   The paper proposes two approaches to train generative models for community detection. The first is based on Graph Neural Networks (GNNs) that are trained to solve community detection problems in a supervised learning setting, where it is assumed that the community detection problem is solved on graphs.  The second approach uses a belief propagation algorithm to learn a belief for binary and multiclass Stochastic block models, and then uses it from a learning perspective to find a local minimum. The authors show that (linear) GNNs trained with a non-backtracking operator on the line graph of edge adjacencies are able to find the global minimum/minima, and that they are more robust than the standard (non-linear) belief propagation method.  Experiments are conducted on several real-life datasets, and show that the optimization landscape can be learned by (linear ) GNNS, and they outperform the state of the art. "
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper considers the dictionary learning problem, where the goal is to learn a linear combination of sparse weights in a linear model. The dictionary is constructed by computing the coefficients of the linear combination, and the goal of the optimization is to recover the coefficients. The authors propose two provable algorithms for the problem of dictionary learning. The first provable dictionary learning methods for coefficient recovery is based on previous work, and it combines linear and non-linear operations. The second provable algorithm, called NoODL, is a generalization of the previous work. The main contribution of the paper is that the recovery of the dictionary is provably provable, and that the geometric rate of the recovery is also provable. Experiments show that the proposed algorithm outperforms state-of-the-art techniques on a variety of datasets and neural architectures.   "
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"This paper proposes a new loss function for binary hash codes that combines a differentiable model and a similarity function. The loss function is similar to prior methods based on the log likelihood loss, but instead of using the Hamming distance target, the authors propose to use the multi-indexing for the hashes. The authors also propose a training scheme where the loss terms are optimized by minimizing the difference between the MAP of the query and the query of the original hash. The proposed techniques are applied to similarity search tasks on ImageNet and SIFT 1M for information retrieval tasks. The results show that the proposed training scheme outperforms prior work in terms of query cost. "
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"Neural architecture search (NAS) aims to learn a task-specific neural network topology that can be used to improve the performance of existing manual architecture designs. The authors propose a Graph HyperNetwork (GHN) to reduce the search cost by using a graph neural network for inference. They show that existing networks can be trained to perform well in search. They also show that GHNs are more efficient than regular hypernetworks and premature early stopping for learning an architecture.    The authors show that the speed-accuracy tradeoff between two networks trained with and without GHNs is a trade-off between validation accuracy and the surrogate search signal, and they show that they outperform other random search methods on CIFAR-10 and ImageNet. They further show that these networks outperform manual designs in the anytime prediction setting.  The paper is well-written, well-motivated, and well-structured. However, it is not clear to me that it is a new direction for NAS. "
SP:65ccf43cd4e033d22239069057f5200d49f33724,This paper proposes a new method for generative adversarial imitation learning. Imitation learning uses expert demonstrations to learn an optimal policy from expert demonstrations for deep learning. The proposed method is based on multiclass classification to learn discriminator functions that can be used to train a discriminator for non-expert demonstrations. Experiments on continuous control tasks show that the proposed method outperforms the state-of-the-art generative discriminator imitation learning baseline and is able to learn policies that are robust to non-adversarial perturbations.
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper proposes Invertible Neural Networks (INNs), a class of neural networks that can be used to solve the ambiguous inverse problem, i.e., the problem that arises when neural networks are trained to solve a particular task, but the hidden system parameters are not invertible. This is an important problem in many applications, such as natural science, medicine, astrophysics, etc. In this paper, the authors propose INNs, which are neural networks where the posterior parameter distribution over the latent variables of the forward process of the neural networks is invariant to changes in the posterior distribution of the latent output variables. They show that INNs can solve the inverse problem in a similar way as other neural networks, but INNs are different from INNs in the sense that they are able to learn a model for the inverse process with invertibility. In particular, INNs for multi-modalities (in the parameter space) can learn to recover unrecoverable parameters, and INNs that can recover parameter correlations across multiple modalities.  The main contribution of this paper is that the authors show that an INN can be trained to recover the parameters of a system from a single observation, and that this can be done even if the observed measurement is ambiguous. The ambiguity is caused by the fact that the INN is trained on artificial data, where the distribution of latent variables is not invariant.   "
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes an ensemble of deep neural networks (NNs) based on a mixture model approach. The authors show that the ensemble of NNs based on this scoring rule is more robust to prediction uncertainty than Bayesian NNs. The ensemble method relies on a finite mixture model with uniform mixing weights, which is similar to a mixture density networks. The main difference is that instead of using mixture components, the authors propose to use an adaptive, input-dependent distribution over the parameters of the NN to replace the fixed mixing weights. The proposed model is tested on adversarial examples and is shown to outperform existing approaches in terms of uncertainty estimates. "
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper studies the problem of model size reduction in deep learning, i.e., reducing the memory footprint, energy consumption, communication bandwidth, and storage requirements of deep neural networks. The authors propose two techniques to compress Shannon-style coding schemes (pruning and quantization) that encode the empirical weight distribution of a neural network into deterministic weights. They show that the compression rates of these coding schemes can be improved by using a full variational distribution of the weights, and that the KullbackLeibler divergence between the sampled variational density of the network weights and that of the encoding distribution is lower bounded by a bits-back argument. They also show that a constraint on the compression rate is imposed on this Kullbacks-Leiblers divergence, which is a result of weight determinism.  The authors show that their encoding scheme is a variational family and that it outperforms the information-theoretical lower bound. They demonstrate that their approach can be applied to VGG-16/CIFAR-10 with a fixed memory budget and achieves better compression rates than it can be compared to existing approaches. They further show that this method can be used for neural network compression in the presence of a random sample for network weights.   "
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper proposes ProxylessNAS, a new architecture search method for neural architecture search (NAS) for large-scale neural networks. The method is based on the Differentiable NAS (DNAS) framework, which uses a continuous representation of the architecture of a neural network to search for the best architecture for a given task. The authors show that the proposed method is able to find a good architecture for large scale NAS tasks, and that it can be applied to existing NAS algorithms. The paper also shows that the method is transferable to different hardware architectures."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper considers the problem of learning a Lagrangian dual of the problem under additive linear penalties. In non-convex settings, this problem can be seen as a special case of the fairness problem under linear constraints, and the authors propose a new training procedure for this problem. The main contribution of this paper is to extend the training procedure to non-consvex, large-data settings. The authors show that under the constrained objective, a deterministic saddle-point equilibrium can be found, and that under a constrained objective the Lagrangians of the two-player min-max games can also be found.   The authors also show that the problem is non-trivial in the setting of linear penalties with second-order ones. They show that a penalized objective under secondorder penalties can be derived from the penalty coefficient of the second order penalty. They also provide a method to compute the gradients of the classifier under the proposed method.  Finally, the authors provide an algorithm for training a classifier in the case of stochastic mini-batch settings. "
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"This paper studies the problem of sampling discrete latent variables for highvariance gradient estimators. Discrete latent-variable models are a special case of continuous latent variable models, where the pathwise derivative is a function of the number of branch paths in the model. The authors propose two control-variate schemes for the former and two continuous-relaxation methods for the latter. They compare the performance of state-of-the-art methods for discrete latent-variance models using both control-viate schemes and continuous-lossy methods. They show that RWS outperforms other models and inference networks in terms of importance weighted autoencoder, and that it outperforms state- of-the art methods for deep generative models. "
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper studies the problem of learning a scalar reward function from human knowledge and non-differentiable pipelines for structured output prediction tasks. In particular, the authors consider the case where the task is structured prediction, where the goal is to learn a function in the output space that maximizes the expected return on the labeled training data. For this setting, the paper proposes to use a truncated randomized search to train structured prediction energy networks (SPENs) based on gradient-based search for test-time inference. The authors show that the scalar function for these tasks can be learned from both human knowledge as well as from non - differentiable pipelines. They also show that SPENs can be trained with supervision via truncated random search for unknown local improvements, and that the reward function can be optimized using the learned reward function via the use of truncated Randomized Search (SR)."
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper considers the problem of learning policies that are robust to perturbations in the environment model parameters. It focuses on transferring policies from a simulation environment to a real-world environment. The authors propose an active learning based framework, EffAcTS, to learn the model parameters and then use these to learn robust policies. The proposed method, EPOpt, is based on the framework of Multi-Task Learning and is shown to improve sample efficiency. The approach is evaluated on a number of continuous control tasks and is compared with existing approaches. The paper also proposes a Robust Policy Search from a Multi-task Learning perspective. "
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper considers the problem of learning a value function for policy evaluation and control in reinforcement learning agents. The authors propose two extensions to existing methods for nonlinear function approximation, namely temporal difference learning and Q-learning. The main idea is to use a two-timescale network (TTN) architecture to learn a linear function approximation on a fixed basis and a fixed representation, and then to use nonlinear gradient temporal difference (nonlinear gradient) learning to learn the nonlinear representation. The proposed approach is based on two existing algorithms: data-efficient least-squares methods (e.g., eligibility traces) and linear policy evaluation algorithms for learning nonlinear value estimates in the linear setting. The paper shows that the proposed algorithms can be used to learn nonlinear values for any linear component with dependent features, and that linear methods can be learned using the two-timescale network (which can be trained in parallel). The paper also shows that TTNs are more efficient than existing state-of-the-art linear value function approximation algorithms for both policy evaluation  and control. Finally, the paper shows how the proposed approach can be combined with existing algorithms, and shows that it outperforms existing algorithms in the case of nonlinearity. "
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes a method for learning a semantic regularizer for deep reinforcement learning agents. The key idea is to learn a semantic model of the environment and use it to train a multi-target sub-policy and a Bayesian model to predict the performance of a sub-sub-policy from visual inputs. The semantic structures of the environments are learned via the Bayesian network. Experiments are conducted on a 3D environment called House3D, which consists of human-designed indoor scenes with real-world objects. The paper shows that the proposed method LEAPS outperforms baselines that do not consider the semantic content of the scene. The authors also conduct experiments on visual navigation tasks on House3d."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a method to improve the generalization ability of deep learning driving models in the presence of unobserved driving environment. Specifically, it uses multi-task perception-related basic knowledge (segmentation map, depth map, driving knowledge) and driving knowledge to train a driving model that combines a perception module and a driving module. The segmentation map and depth map are used for easier drivingrelated perception problems, while the driving module is used for difficult driving task with control commands. Experiments show that the proposed method improves the average sucess rate on navigation tasks, generalization and accident explanation ability using multitask perception knowledge. The method is also shown to outperform a benchmark method that does not use depth map and pixel level understanding of images. The authors also show that trained weather and untrained weathers are more diverse than trained weather, suggesting that the diversity of training driving dataset is important. "
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the relationship between adversarial robustness and generalization. The authors show that robustness to adversarial perturbations is correlated with the accuracy of the model, and that robust classifiers are able to learn feature representations that are more robust than standard classifiers. They also show that features learned by robust models are more sensitive to salient data characteristics and human perception. "
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"Deep neural networks are trained using reverse-mode automatic differentiation (backpropagation). This paper proposes a method based on the Equilibrium Propagation to learn local learning rules for gradient-based training of neural networks. The key idea is to use the iterative optimization of neural activations during inference as an iterative inference procedure similar to that of Equilibrium propagation. The main difference is that a feedforward network is used to learn the local learning rule for the iteratively iterated iterations of the feed-forward network.    The authors show that the feedforward neural network can be trained to learn Initialized Equilibrium Proposition (i.e., a network that learns a local solution to the gradient of the gradients of all neurons in the network).   They also show that this network is able to learn a local version of the local solution that is more robust to perturbations in the weights of the network. The authors also demonstrate that the initializing network that is used for inference can be learned to be more robust than the one that was used for training the feed forward network, and that the network can learn a more robust local solution than the original one.  Finally, the authors demonstrate that for deep networks trained with backpropagating, the network trained with the proposed network outperforms the one trained with Equ equilibrium propagation in terms of the error gradient."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper proposes ZO-signSGD, which combines gradient-free operations with signSGD to improve the convergence speed of SGD-type algorithms. The latter uses the sign information of gradient estimates to guide the convergence rate. The authors show that the convergence is faster than the previous state-of-the-art result (ZO-SGD) when the optimization variables are sign-free. They also show that using gradient estimators that are more sensitive to the sign of the gradient is also beneficial to the convergence. The paper also shows that ZO - sign SGD is more robust to black-box adversarial attacks for the generation of adversarial examples in black- box neural networks. Experiments on two image classification datasets (MNIST and CIFAR-10) show that on MNIST, the authors demonstrate that the gradient estimator used for the convergence improves the performance.   "
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper proposes a new optimization method for training convolutional neural networks.   Deep learning is an important problem in many artificial intelligence applications, especially in the context of energy-limited edge device, where a complex neural network model is required to be trained on a small amount of data.  This paper proposes to add multiply-accommodate (MAC) operations to the convolution filter, which is a simple optimization method that can be applied to any convolution filters.  The idea is to add a checkpoint to each layer of the filter during the optimization of the MAC process, so that the number of operations in the filter does not increase too much during the fine-tuning process.  In order to do so, the authors propose a simple modification to the original convolution operations, where the activation or pooling layers are replaced by a filter that has a fixed number of layers.  Experiments on the CIFAR-10 example model and the Network in Network are conducted on both the standard and the more complex CIFar-10 and CIFARS-100 datasets.  Results show that the proposed method is able to achieve a small accuracy drop in terms of MAC operations compared to the previous method, and that the accuracy drop is even larger when the filter is trained with multiple checkpoints. "
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper studies the problem of generating adversarial examples for neural network models that are robust to adversarial inputs. The authors propose two learning principles based on unique data properties. First, they show that the discriminative power depends on the temporal dependency of the audio data. Second, they propose an image adversarial defense for input transformation that is based on the observation that temporal dependency on the input transformation can be used to improve the discriminate power. They demonstrate the effectiveness of their approach on both automatic speech recognition (ASR) tasks and audio adversarial attacks. They also show that adversarial robustness of ASR systems can be improved by incorporating domain-specific data properties to generate more robust examples. Finally, the authors show that audio robustness can be enhanced by generating more robust and adaptive attacks.  "
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"Deep generative models are an important problem in many applications. They are able to learn representations that are highly interpretable and interpretable. However, existing approaches rely on images that are generated from core inductive biases. This paper proposes to learn a generative model from real-world images. The key idea is that the generator of a GAN should be able to generate images from a composition that is interpretable at a representational level. The approach is evaluated on multi-object image datasets, and the results show that the generated images are interpretable in terms of their representation. The authors also show that images generated from the generative process are invariant to changes in the reference distribution, and that the resulting images are similar to the original images. "
SP:fb59990b8da0e95d8202383478a456667de60449,"Learning disentangled representations for computer vision tasks on visual data is an important problem. In this paper, the authors propose a novel learning setting called referencebased disentangling, where the high-level generative factors are not available, but the target factors are. Supervised approaches to disentangle these factors have been shown to perform well in the literature.  This paper proposes a deep generative model, called reference-based variational autoencoders, to learn a weak supervisory signal from a reference set. During training, a variational inference framework based on adversarial learning is used to learn the objective function. The authors show that the proposed model is able to learn disentanged representations with minimal supervision on three tasks: feature learning, conditional image generation, and attribute transfer. They also show that this representation can be learned without any additional supervision. "
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper proposes a method for continual online learning based on deep neural network models for rapid online adaptation. Specifically, a mixture of models is used to approximate non-stationary task distributions using an expectation maximization algorithm based on the Chinese restaurant process. The online learning procedure is based on stochastic gradient descent to update the model parameters. The authors show that the proposed online learning (MOLe) approach uses meta-learning to adapt the model to a new task using SGD. They also show that their MOLe outperforms prior methods for continuous adaptation to non-stochastic task distributions (e.g. varying terrains, motor failures, and unexpected disturbances). They also demonstrate that the predictive model can be used for control, which is a useful tool for real-world phenomena. "
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper studies the problem of distributed prioritized experience replay for RNN-based RL agents. The authors propose a new training strategy, called Recurrent Replay Distributed DQN, to address the issue of representational drift and recurrent state staleness in distributed training of RL agents due to parameter lag. The agent is trained with a single network architecture and hyperparameters. It is shown to achieve human-level performance on two Atari games, Atari-57 and DMLab-30. "
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper proposes a hierarchical generative models for coordinated multi-agent trajectory behavior (e.g., offensive basketball gameplay). The authors propose a hierarchical framework for learning sequential generative model for long-term coordination between agents. They show that hierarchical models for these settings can be trained with intermediate variables that capture high-level behavioral semantics. The proposed approach is based on programmatically produced weak labels for the spatiotemporal regime. The authors show that the framework can capture complex interactions between basketball players and demonstrate that the proposed framework is able to capture realistic multimodal trajectories of basketball gameplay with quantitative and qualitative evaluations on synthetic settings. "
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper proposes a new vision model for interacting agents. The proposed method learns a dynamics model from ambiguous visual information and uses this method to capture temporal information. The method is evaluated on two sports datasets, one using a soccer game engine and one using real basketball trajectories, and shows that the proposed method outperforms the baselines on both sports datasets."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"Deep neural networks are well known for approximating complicated functions using gradient descent methods. Deep neural networks can be trained end-to-end, but training can be expensive due to the large number of black-box functions. This paper proposes a novel method to perform a base neural network with a differentiable neural network that can be used to learn a specific functionality from the input to the output of the neural network. The proposed method can be applied to any end- to-end training where the input function is not available in the input space.    The authors propose a method to train a base network that is differentiable for a specific task, and then replace the input with a sequence of blackbox functions that are present in the blackbox function interface. The idea is that a neural network trained with the “Estimate and Replace” paradigm is able to learn an input to blackbox functionality from a single neural network to an input with multiple functions. The differentiable estimator is trained with an external blackbox non-differentiable counterpart. The authors show that the integrated model trained with a black-board function is more efficient than a fully differentiable model that uses a black -box function for inference. The integrated model is also able to be trained in a way that avoids the need for an end-of-end optimization process, which is common in RL-based methods.  The main contribution of the paper is that the authors propose to replace the base network with an input function that is present in a specific function in a particular function interface, and that the differentiable network is trained to be able to adapt to this input to a specific input function. The paper also proposes a way to handle the intermediate labels. "
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,"This paper proposes a data-driven inductive bias for learning for a new task. The authors combine gradient-based meta-learning and hierarchical Bayes, where a function approximator (i.e., a neural network) is used to approximate a mixture of hierarchical Bayesian models, and a stochastic expectation maximization procedure is used for parameter initializations for gradient descent. These initializations are used for the latent assignment of tasks, and the authors show that the proposed approach is able to improve the diversity of training tasks by incorporating inductive biases into the hyperparameters. Experiments on the miniImageNet benchmark for 1-shot classification show improved generalization on the transfer from one task to another. A non-parametric variant of the proposed method is also applied to the task distribution for a few-shot regression tasks. "
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes Meta Auxiliary Learning (MAXL) for image classification. Auxiliary learning is used to augment the principal task with a set of auxiliary tasks that are based on domain knowledge. The auxiliary task is hierarchical sub-class image classification, where the auxiliary tasks are manually-defined. A meta learner is trained to learn the sub-classes of the auxiliary task, and a multi-task evaluator is trained using the learned meta learners. The proposed MAXL outperforms several baseline auxiliary learning methods on CIFAR datasets, and is shown to generalize better than a simple method that does not rely on human-defined auxiliary tasks. The authors also show that the proposed method can be applied to automated generalisation in the presence of human knowledge.   "
SP:76248e1c914c60ce69de244fe7ec62488d01e161,This paper proposes a neural network based representation for the open set recognition problem. The key idea of the paper is to learn a representation of the set of open sets. The paper shows that the proposed approaches outperform existing approaches on two datasets.    The paper is well-written and well-motivated. 
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of energy and area efficiency in embedded deep network inference. The authors consider the case of 4-bit models where the cosine similarity between the input and the output of the network is a function of the energy and the number of bits used to compute the solution. In this setting, the authors show that the accuracy of full-precision baseline networks trained with finetuning (ResNet-34, ResNet-50, Resnet-152, ResNets-152), Inception-v3, densenet161, and VGG-16bn networks on the ImageNet classification benchmark is comparable to that of pretrained models trained with fp32 precision baseline networks. They also show that training error is bounded by stochastic gradient descent, and that the solution distance between pretrained fp2 precision baseline and fine-tuned networks is bounded.    The authors also propose two techniques to improve the performance of the baseline networks: (1) matching learning rate annealing to combat combat noise, (2) gradient noise quantization, and (3) quantization to reduce the maximum variance of the gradient estimates. They show that these techniques can be used to calibrate the accuracy and energy of the low-probability baseline networks, and they are shown to be effective in practice."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"This paper proposes a new approach to learn surface properties for bouncing trajectories. The authors propose a model that predicts post-bounce trajectories based on sensor inputs and physical properties such as bouncing restitution and effective collision normals. The model is composed of two modules: Physics Inference Module (PIM) and Visual Inference module (VIM). PIM learns physical interactions between physical parameters and observed pre-collision 3D trajectories for the prediction task, and VIM learns to infer physical parameters from the physical parameters of PIM and observed trajectories of the bouncing trajectory. The proposed model is evaluated on a dataset called the Bounce Dataset and compared with several baselines including trajectory fitting based on Newtonian physics. The results show that the proposed model outperforms the baselines in terms of predicting post-batches trajectories and the physical properties.   "
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper studies the vulnerability of neural networks to adversarial images. The authors show that the adversarial vulnerability is related to the gradients of the `1-norm of the network’s weight distribution. They also show that adversarial examples are imperceptible in the presence of targeted but imperceptibly image perturbations. The paper also shows that the image size, the number of layers, and the network architectures are the most important factors that affect the vulnerability.  "
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper proposes a new interactive agent modeling scheme that uses probing to learn a mind model for agent modeling. The proposed framework is composed of two learning processes: (1) pure curiosity-driven reinforcement learning that learns a probing policy, and (2) imitation learning to learn an approximated agent model. The approach is evaluated on a variety of tasks, including distilling optimal planning, collaboration, and competition. The agent model trained with the proposed approach outperforms existing ones based on passive observation, random probing, and curiositydriven approaches. "
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to Artificial Neural Networks (ANNs) that is designed to improve the performance of ANNs. The main idea is to modify the firing modes of a biological neuron by modifying the peripheral factors (i.e., neuromodulators) that control the firing of the biological neurons. Biological neurons are modulated by a set of modulators, and the authors show that the modification to ANN nodes can improve the activation sensitivities of biological neurons, and that the slope of the activation function is similar to that of the modulators. The authors also show that this modification is more effective than existing ANN nodes for Convolutional Neural Networks and Long Short-Term Memory networks. "
SP:287a577834fd2820a939a1113b39146a22727491,"This paper proposes a neural analysis and synthesis (NANSY) framework for voice, pitch, and frequency. The key idea of NANSY is to use the information bottleneck to learn the analysis features for controllable synthesis. The training strategy is based on information perturbation, where the synthesis networks are trained on a pre-trained multilingual dataset, and the bottleneck structures are learned on the speech data. The paper shows that it can improve the reconstruction quality and controllability of voice and pitch. The analysis features used in NANSy are the wav2vec feature, pitch feature, and Yingram, which is used for self-supervised training. NansY is also applied to the multilingual setting, and it is trained on multilingual datasets. NANsY is applied to three applications: zero-shot voice conversion, pitch shift, and time-scale modification."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"This paper studies hyperparameter optimization in the (gradient-based) bilevel programming framework. The authors show that gradient-based algorithms suffer from the same overfitting problem as cross-validation, and that the overfitting of the validation set can be explained by the optimization properties of the hyperparameters. They provide an expectation bound for the cross-validation algorithm, and show that regularization terms on the outer and inner levels of the gradient -based algorithms can help alleviate the problem. They also show that feature learning and data reweighting for noisy labels can be used to mitigate the effect of noisy labels.    The authors also provide a theoretical analysis of the generalization performance of gradient based algorithms. They show that uniform stability is achieved when the number of parameters is small, but not when the parameters are large. They further show that this is due to the fact that the regularization term on the inner and outer layers of the algorithm has uniform stability, which is not the case for blevel programming. "
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper proposes a new knowledge distillation approach for the transfer of dark knowledge, i.e. knowledge transfer between two different models. The authors propose an algorithm to learn student-friendly representations by distilling the knowledge of a teacher model into the student branches of the same teacher model. They show that this approach can be used to train teacher models to transfer knowledge from one branch to the other. They also show that the proposed algorithm can be applied to existing knowledge distillations methods to train student models to achieve better accuracy and convergence speed. Finally, the authors show that their technique can be combined with other existing student distillation methods to improve the performance of student models. They evaluate their algorithm on a variety of knowledge distilling techniques based on both teacher and student models and show that it can improve the accuracy of the student models in terms of the distance between the teacher networks.   "
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies the problem of extracting invariant features for out-of-distribution (OOD) generalization in machine learning. The authors show that invariance is necessary for OOD generalization, and propose two algorithms based on this idea. The main contribution of the paper is to propose an expansion function that is invariant to OOD and can be used to improve the generalization performance of OOD learning algorithms. The paper also introduces a model selection module that is used to select the OLD learning algorithm. The theory of the model selection criterion in the theory shows that the OOD problem can be formulated as a model-selection problem, and the authors provide a theoretical analysis of this problem. Experiments on several benchmark OOD datasets show that the proposed model selection criteria outperforms the baselines. Finally, the paper also shows that their model selection algorithm outperforms baselines in terms of generalization error bounds.  "
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper studies the problem of meta-learning in the online setting, where the meta-parameters are drawn from a stationary distribution. The authors propose VC-BML, which uses a Dynamic Gaussian Mixture Model to model the number of component distributions in a meta-learned model. The Chinese Restaurant Process is used to estimate the number and number of components in a non-stationary distribution. Dynamic mixtures are used to model a multi-task learning problem, which can be seen as a negative knowledge transfer problem, where each task in a task family has a different number of parameters. Dynamic mixture models can be used for diverse and dissimilar tasks.    The authors show that VC-BML is able to avoid catastrophic forgetting by using a point estimation method similar to the posterior approximation method in structured variational inference for avoiding forgetting knowledge in the parameter space. They also show that the proposed VC-MBL can be applied to a variety of tasks with non-stochastic distributions.  The paper is well-written and well-motivated. "
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"Boundary value problems are a fundamental problem in machine learning. Boundary conditions of ordinary differential equations can be expressed as a function of a set of computational pipelines, and the goal is to find the optimal solution to a first-order boundary value problem.    This paper proposes a new algorithm, called probabilistic BVP solver, which is a generalization of existing non-probabilistic methods.  The key idea is to use a Gauss–Markov prior on the vector field and use it to solve BVPs using manifold learning.  This is a well-motivated idea, as it is well known that it is possible to learn the posterior distribution in linear time, and that it can be used to learn a model for uncertainty quantification, mesh refinement, hyperparameter adaptation, etc.  In this paper, the authors propose a statistical modelling tool-chain, where the leftand right-hand side boundary conditions are defined as the vector fields of a vector field.  They show that the probabilism of the proposed algorithm is equivalent to solving a first order problem, and they show that it converges to the solution of the first order BVP in a similar way to the non-Probababilistic algorithms. They also show that their probabilist BVP solution is more efficient than non-observational algorithms for solving ODE boundary value problems.  Finally, they demonstrate that their algorithms are more computationally efficient for ODEs that are more general than for higher-order problems, and show that they can be applied to any machine learning problem. The paper also shows that their algorithm is more tractable than existing algorithms for first order problems, which are computationally expensive to compute.  Experiments are conducted in numerical simulation, probabilistically inference, and in the integration domain, where numerical simulation is used for numerical simulation.  Probabilistic numerical algorithms are shown to be able to solve the lineartime complexity, adaptive step-size selection, and polynomial convergence rates. The authors also provide a theoretical analysis of their scheme, which shows that probabilists are able to find a solution with a polynomially optimal solution, and can also solve the initial value problems with a smaller number of samples. "
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper studies the problem of reward mixing in reinforcement learning. The authors consider reward-mixingMarkov decision process (MDP) in an episodic reinforcement learning setting, where the reward model is not available but the dynamics of the environment is, and the goal is to learn a near optimal policy in a partially observable system. In this setting, the authors consider switching reward models for the reward function can be difficult because the observation space is not the same as the latent state space, and switching reward-models is expensive due to the time-horizon. To address this problem, they propose a polynomial-time algorithm for learning a -optimal policy in the partially observed environments. They show that under certain assumptions, the proposed algorithm converges to the optimal policy. They also provide algorithmic and analysis techniques to solve the problem. "
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes two new methods for conditional average treatment effect estimation, which is an important problem in many applications. The authors focus on multi-cause treatment effect problems, where there is a confounding bias between the treatment effect of a single treatment and the treatment of a multi-causal combination of treatments.   The authors propose a two-step procedure called Single-causality perturbation (SCP) to address this problem. First, the authors propose to augment the observational dataset by augmenting it with a cause combination of the treatments. Then, they propose a causal inference procedure to identify the cause of the confounding bias.  Second, the proposed procedure is based on the idea of causal inference. It uses a covariate adjustment to improve the estimator of the augmented dataset.  They evaluate the performance of the proposed SCP on both synthetic and semi-synthetic experiments."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"This paper proposes a multi-wavelet-based neural operator learning scheme based on fine-grained wavelets to learn an operator’s kernel from a large amount of data. The projection of the kernel is based on multiwavelet polynomial bases, and the inverse operator map is learned by learning inverse multiwavelets filters. The proposed method is tested on the Korteweg-de Vries (KdV) equation, Burgers’ equation, Darcy Flow, and Navier-Stokes equation, and shows better accuracy than state-of-the-art neural operator approaches. The method is also applied to time-varying equations, and is shown to have a lower relative L2 error than the previous state of the art.    The method uses mappings between function spaces to learn the projection of a projected kernel, which is a resolution-independent scheme. The authors also show that the proposed method can be applied to lower-resolution data, and that the method is robust to complex dependencies. "
SP:1153785e6a016cfee2644952a772aa08927299b6,"Binary neural networks (BNNs) with 1-bit with sign function can be used to compute full-precision weights, but the gradient of sign function in the Fourier frequency domain is non-convex and cannot be computed efficiently. Binary neural networks with sign functions are a special case of BNNs with sine functions, e.g., frequency domain approximation (FDA). The authors propose two approximations to this problem, one based on back-propagation, the other based on a noise adaptation module. The authors show that the approximate gradient of the sign function is the best way to reduce the optimization difficulty, and that this approximation error depends on the low-frequency information in the gradient. They also show that this method can be applied to any binary network with a sign function. They show that their method is more robust to noise in the factual gradient, and more robust when high-frequency coefficients are used. They evaluate their approach on several benchmark datasets and neural architectures. "
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"This paper studies the role of the coordination of multiple brain areas in neuroscience-based tasks. Recurrent neural networks (RNNs) have been shown to be able to coordinate different cortical areas for different tasks, and the authors show that multi-area RNNs with neuroscience-inspired architecture constraints are able to perform well at multi- area computation. The authors also show that the networks satisfy Dale’s Law, which states that networks with full observability of the output-relevant information will perform well on a variety of tasks.  The authors further show that a modular computation in the cortex is able to capture minimal sufficient representations of task information, and that this modular computation can be used to constrain the number of computations in a single RNN.   The paper concludes with a discussion of the benefits of distributed computation in neural systems, and shows that, in the case of constrained multi-region RNN, these computations can be performed efficiently. "
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"Saliency maps for convolutional neural networks (CNNs) for image classification are an important and important problem.   In this paper, the authors propose a novel beam search algorithm, where the saliency map for an image of interest is used to search for a set of image regions in the image. The maps are used for classification, and the confidence of the classifier is used as a metric to measure the quality of the maps. The attention maps are constructed using structured attention graphs (SAGs).   The authors propose an approach to learn a compact and representative SAG for visualization based on diverse sampling.  They show that SAGs outperform saliency maps on the comparative counterfactual questions on the user study for the task of comparing the performance of different image classifications.  The paper also shows that the proposed approach outperforms the state-of-the-art on user accuracy on the question of whether saliencymap baselines. "
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies the transferability of the hidden representations of convolutional neural networks trained on ImageNet with different loss functions and regularizers on different image classification tasks. The authors show that the representations learned with different types of loss functions can achieve similar test accuracy on different downstream tasks. They also show that networks trained with the same objective on different tasks can learn representations that are transferable across different tasks.    The paper also shows that networks with fixed feature extractors can learn to transfer the representations of different tasks using the same loss functions. Representations learned on the same task with different objectives are more transferable than vanilla softmax cross-entropy on the ImageNet accuracy. The main contribution of the paper is that the authors propose to use centered kernel alignment as a regularizer to improve the performance of hidden representations learned by networks. The paper shows that the proposed objectives and hyperparameter combinations are able to achieve better class separation across different objectives and different hyperparameters, and that learning invariant features and features learned on a single task can be used to learn features for multiple transfer tasks."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper proposes selective backpropagation through time (SBTT) as a neural network training strategy for deep generative models of latent dynamics. SBTT is an extension of sequential autoencoders, and is designed for neural interfaces where spatial sampling and the temporal frequency of sampling is limited. The authors show that SBTT improves the inference of neuronal population dynamics from electrophysiological and calcium imaging data to infer neural population dynamics in the context of neural interfaces.    The main contribution of this paper is that the authors demonstrate that the neural population activity of a population of neurons in the brain is highly dependent on the interface bandwidths of the brain circuits, and SBTT can be used to learn a latent low-dimensional population dynamics that is more robust to bandwidth limits.  The authors also show that the latent dynamics of a neural population can be learned using SBTT, and that the learned latent dynamics can capture the high-frequency temporal structure of the neural activity.  In addition, the authors show the effectiveness of SBTT on two-photon calcium imaging, where SBTT shows that neural populations of neurons with high frequency temporal structure are more likely to share the same interface bandwidth than neural populations with low frequency.  Finally, they show that models trained with SBTT are able to learn sparsely-sampled data with limited, highbandwidth sampling to pretrain dynamics models.  This is an interesting contribution to the field of Neural Information Processing Systems (NIS).   This paper is well-written and well-motivated, and the results are interesting. However, there is a lack of comparison with previous work, and there is not a clear connection between SBTT and neuroscience and neuroscience.  I have a few comments:"
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"Sequence-to-sequence learning for sequence prediction tasks with neural networks is an important problem. The current state-of-the-art models are based on a hierarchical approach to sequence-to sequence learning using neural networks. This approach learns a local distribution through a neural network, which is based on quasi-synchronous grammars. The authors propose a new approach, called compositional generalization (SCAN), which learns a neural parameterization of the grammar, which can be applied to any combinatorial space of derivation rules. They show that it outperforms the baselines in three different domains, including a diagnostic language navigation task with a latent neural grammar, a compositional learning task with style transfer, and a small-scale machine translation task. They also show that their approach can be used for manual feature engineering."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a new algorithm for function-on-scalar feature selection in Feature Selection and Functional Data Analysis. The authors propose an algorithm for Group Elastic Net, which is a general algorithm for the analysis of large and complex data sets. The algorithm is based on the Functional Principal Components of the functional response of scalar predictors. The paper shows that the proposed algorithm is able to recover the sparsity structure of the Augmented Lagrangian of the Group Elastic net in ultrahigh dimensional settings. The proposed algorithm can also be applied to the function-onscalar regression framework, where the computational burden is much lower. Experiments on simulations show that this approach outperforms competitors and is applicable to the application of the Genome Wide Association Study. "
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"This paper considers the problem of model estimation using functional principal component analysis (FPCA) for Structured point process data. The authors propose a new matrix, called FPCA, which is a generalization of the log-Gaussian Cox processes. The proposed framework is validated on several real data analyses. "
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper proposes an online multi-task learning approach for adaptive nonlinear control, called Online Meta-Adaptive Control (OMAC). The key idea is to learn a nonlinear system with adversarial disturbance and unknown environmentdependent nonlinear dynamics, and then use a shared representation to model the environmentdependent dynamics. The approach is applied to robot control, where the goal is to control a robotic system in an online setting. The authors provide a unified framework for both control-theoretic and learning-thruthic guarantees. They also provide a non-asymptotic endto-end convergence guarantee for multi-tasks non-linear control. They show that OMAC can be combined with deep representation learning, and show that the performance of OMAC is comparable to other adaptive control approaches.   "
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper studies bound propagation based certified robust training methods for training neural networks with certifiable robustness guarantees. The authors propose two (SOTA) methods: interval bound propagation (IBP) and CROWN-IBP, and show that they can be fast-certified robust (in terms of per-batch training complexity) and robust to exploding bounds. They also show that a weight initialization method can be used for IBP training.   The authors also propose Fast-Certified-Robust-Training (FCTR), which uses Batch Normalization (BN) to regularize the model during training. They show that this regularization can be applied to the ReLU activation states of the BN to improve the certified bounds.  They also demonstrate that the proposed regularization is effective for certified training, and that FCTR achieves wamrup on CIFAR-10 and TinyImageNet with verified error and verified error w.r.t. the network architecture.  Finally, the authors show that their methods are robust to long warmup schedules, long training schedules, and overfitting. "
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection, i. The authors consider the setting where the model violations are caused by a heavy-tailed noise distribution and the goal is to identify the source of the change point. They show that theoretically-probabilistic methods can be shown to be robust to the presence of systematic contamination. They also show that a change-point detection method is computationally-feasible under certain conditions. "
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the problem of learning a differentiable model and a neural network in the presence of a population loss. The authors show that batch Gradient Descent (GD) can be used to approximate the empirical loss, and that SGD and GD can improve the learning power of learning with statistical queries (SQ) in PAC learning. They also show that GD and SGD improve the sample-based learning algorithm with fine enough precision compared to GD for PAC learning, and SQ learning with SGD for SQ learning. Finally, they show that the precision ρ of the gradient calculations is the same as the precision of the population loss, which means that GD can learn with bρ, SGD cannot learn with ρ.    The paper also shows that GD does not improve the performance of SQ learning when the mini-batch size is smaller than the population size, but SGD does. "
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"This paper studies the minimization problem of the Wasserstein distance between the model probability distribution for discrete data in machine learning and inverse problems. The authors consider a variant of Lloyd’s algorithm, where the Voronoi cells are replaced by Power cells, and the goal is to minimise the model distribution of the data.    In this paper, the authors consider the case where the model is a uniform probability distribution and the data is discrete.  They show that if the model has a uniform distribution over the data, then they can prove that the minimisation problem can be solved exactly.  In particular, they prove that for any uniform distribution $p(x) = p(y|x)$, if $y$ is a point cloud in the ambient space, then the problem is minimised exactly. This is a special case of the problem studied in [1].   The authors also prove that if $x$ is discrete, then there are no spurious critical points.  [1] [2] [3] [4] [5] [6] [7] [8] [9]   [10] [11]  [12] [13] [14] "
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"This paper proposes a novel relational self-attention (RSA) feature transform for video understanding. In contrast to traditional convolutional kernels, RSA uses relational kernels to capture the rich structures of spatio-temporal relations in the input video. The authors show that RSA can be used to improve the performance of CNNs and transformers on motion-centric benchmarks.  "
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the mean field theory of multilayer neural networks. The authors prove a mean field limit on the learning dynamics of the multilayers in the infinite-width limit of the neural network, which is a result of the neuronal embedding framework. In particular, the authors prove that the random fluctuation of the network depth is a function of the number of layers and of the amount of network depth. They show that in the large-width regime, the random fluctuations of the training trajectory converge to a global optimum. They also prove that this limit theorem holds in the case of infinite-wide networks.    The authors also prove a limit theorem for the random-flutter case, which shows that there exists an infinite- width limit on random fluctuations in the multilevel setting, and that it holds for any network with a large width. They then show that this infinite-widething limit depends on the number and depth of the layers and on the complexity of the complex interaction among the layers.  Finally, they show that the limit theorem also holds for multillayer networks with fluctuation in the limit.  The main contribution of this paper is the formulation of the stochastic dependency of the fluctuation to the number, and the analysis of the gradient descent mean field training. They prove a system of dynamical equations for the limiting fluctuation distribution, which they call the second-order means field limit. This formulation allows them to prove that any large-wider-than-the-training-plane network has a second order mean field.  In addition, they prove that for any loss function that is close to the limit, there exists a limit on how much the loss function changes in the training process.  They also show that for a certain loss function, the limit is tight. "
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"Forecasting of time-series data based on inductive biases is an important problem in predictive extrapolation. Forecasting of physical systems using data-driven modeling and machine learning (ML) tasks is challenging due to the lack of learnable dynamics of the dynamical system. This paper proposes a new parameterization of dissipative brackets in metriplectic dynamical systems, which is based on the metrization of the Hamiltonian/Lagrangian form of the structure of the system. The authors show that this process is equivalent to generalized Casimirs, i.e., entropy. The dynamics of this dynamics is more stable than penalty-based approaches, and the authors propose a framework for modeling reversible systems. They show that existing approaches to learn inductive bias for reversible systems can be used to learn a structure preserving models of reversible dynamics, and that the physics-based structure of these architectures allows to learn architectures with minimal bias in a black-box model form.  The authors propose to use the fluctuation dissipation theorem (FTD) to model closed stochastic systems, and show that the flow map of a system with the algebraic structure of Hamiltonian / Lagrangian dynamics has a flow map with respect to the energy, and a symplectic structure. Physical systems with this framework can be seen as a generalization of the framework for dissipative chaotic systems. The proposed training strategy for NODEs is similar to the training strategy used to model algebraic objects in the literature, but for a more general system with a different latent dimension. In particular, the authors use the same training strategy to model the system as a system that has non-observicially observable states and non-uniform non-localities, and to learn the system's dynamics. This system is then used to solve multiscale problems with time history, and is shown to be able to learn to predict a system's internal entropy, temperature, and other non-inverse properties. The system is also shown to exhibit thermodynamic consistency in terms of mimetic properties such as the first and second laws of thermodynamics. The paper also shows that the system is able to model non-irreversible and irreversible components of the dynamics in null-spaces, and it is shown that the learned model can be applied to a variety of science and engineering problems where the system can be viewed as an algebraic chaotic system. "
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection-based algorithm for fair and robust training in Trustworthy AI. Fairness and robustness are two of the most important issues in the training of an unbiased model. In this paper, the authors propose an unbiased selection of samples is formulated as a combinatorial optimization problem, and the optimization problem is solved by a greedy algorithm. The authors show that their algorithm outperforms the state-of-the-art technique in terms of fairness as well as robustness on synthetic and benchmark real datasets. They also show that the algorithm is more robust to data corruption than the state of the art fairness and robust robust training baselines.  The authors also propose a new sampling step for batch selection, where the algorithm only uses clean data for the training algorithm. "
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"Neural network models are known to have hidden data biases. However, it is not known whether these models are invariant to inductive biases in the function space. This paper proposes to use periodic activation functions for Bayesian neural networks and shows that they are. The authors also show that the network weights are translation-invariant, stationary Gaussian process priors, and that sinusoidal (Fourier) activations are not invariant. The paper also shows that the triangular wave and periodic ReLU activation functions can be used to train deep neural networks for out-of-domain detection on in-domain data with perturbed inputs."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"This paper proposes a method to train an automatic feedback system for interactive programming. The idea is to train a classifier to identify bugs in a program, and then train an autoregressive agent to learn to solve the problem. The authors show that the proposed method is able to learn a good classifier that can be applied to a wide range of interactive programming tasks. The paper also shows that the learned classifier can be used as a tool to improve the quality of the feedback provided by an agent."
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"This paper tackles the problem of Interpreting Deep Reinforcement Learning (DRL) models in the presence of transparency regulations. The approach is based on high-level latent object features extracted from superpixels and low-level input features (superpixels, attentions, saliency maps). The authors propose a Represent And Mimic (RAMi) framework, which uses an identifiable latent representation to capture independent factors of variation in the latent features, and a disentangled representation for the high level latent object feature. A mimic tree of DRL action values is proposed, which is trained to maximize the fidelity of the mimic tree under the Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. The authors show that the IB-optimal mimic tree can achieve the same fidelity as a DRL model trained on the same number of nodes. The mimic tree is shown to be more robust to decision rules, decision rules and causal impacts, as well as human evaluation results."
SP:84560de78af979354fff83d1370d8675c1e9191f,"This paper proposes a Bayesian framework for learning the structure of dynamic predictions based on Gaussian latent information martingale (GLIM).    The authors consider the problem of learning probability estimates of future binary outcomes, which is an important problem in time series analysis.  They consider weather forecasts, political prognostications, and financial projections, where the latent process of information flow is modeled using historical data.  The proposed approach is able to learn probability paths that can be interpreted as a combination of two types of probability paths: the Martingale structure and the volatility.  In the first case, the probability of the first path depends on the history of the data, while in the second, the former depends on whether the trajectory of the information flow has changed significantly in the past few years.  Experiments show that GLIM outperforms baseline methods on several metrics for estimating the estimated posterior probability path distributions, and that the dynamic structure of predictions is more robust to changes in trajectories.  "
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration with fixed confidence in generic stochastic bandit environments, where the sampling budget is fixed and the structural properties of the environment are unknown. The authors provide instance-specific lower bounds on the expected sample complexity of this problem, which are based on the Oracle algorithm. They show that under certain proportions, the optimization problem has tractability. They then propose a new algorithm, called FWS, for solving pure exploration problems, and extend it to the Frank-Wolfe algorithm for solving the lower-bound optimization problem. FWS is shown to outperform existing state-of-the-art algorithms on two pure exploration tasks, namely, arm identification and finding the best arm, and achieves sample complexity asymptotically tight as the number of learning algorithms increases. "
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper considers the problem of optimizing combinatorial spaces (e.g., sequences, trees, graphs, etc.) with black-box function evaluations. The authors extend Bayesian optimization (BO) to these problems and propose a new framework, called LADDER, which extends the BO approach to the case where the function evaluation is based on a discrete structure rather than on continuous spaces.   The authors propose to use deep generative models (DGMs) to learn a latent representation of structures, which is then used to train a surrogate model in the latent space. The surrogate model is trained using a DGM. The latent space representation is used for surrogate modeling, and the structural information in the decoded structures is incorporated into a structure-coupled kernel.  Experiments on real-world benchmarks on drug design are conducted to show the effectiveness of the proposed approach, LADder, compared to BO and other state-of-the-art methods.  The paper also shows that the inductive bias is not present in the case of continuous space, and that the surrogate model can be learned from the learned latent space without the use of a black box function."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper considers the problem of regret minimization over the representation of state-action value functions in an MDP with linear structure. The authors consider the setting of universally spanning optimal features (UNISOFT) where the MDP has a linear reward function and the Bellman closure assumption is satisfied. They prove a constant regret bound for any MDP that satisfies the UNISOFT condition. This condition holds for both low-rank MDPs with zero inherent Bellman error and for problems with linear reward functions. They also provide two optimistic algorithms (LSVI-UCB and ELEANOR) that achieve the constant regret under this condition. Finally, the authors propose an algorithm for representation selection, and show that it achieves the same constant regret with respect to the representations (or at least one of them) that satisfy the UN ISOFT condition, and that it does not require any additional information about the representation. "
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper proposes a differentiable contact model for learning contact mechanics, frictionless/frictional, elastic/elastic, and elastic/inelastic contact models for physical systems with contacts and collisions. The authors propose a neural network architecture that incorporates Lagrangian or Hamiltonian dynamics into the dynamics to ensure energy conservation. Previous approaches are based on differential equations, which are computationally expensive to compute. This paper proposes to learn the inductive bias by learning the joint angles between two points in the system. The proposed contact model allows for simultaneous learning of contact and system properties, and the authors show that the proposed model can be used to enforce inequality constraints. The framework is evaluated on 2D and 3D physical systems that have coefficients of restitution, and on legged robots and robotic manipulators. The results show that this differentiable physics simulator is able to learn dynamics for downstream gradient-based optimization tasks such as planning and control. The paper also shows that the contact model can also be used in the training of Lagrangians and Hamiltonian neural networks."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,This paper studies the Benevolent training hypothesis (BTH) for deep neural networks. The authors show that the Lipschitz constant of a deep neural network (DNN) is bounded in the presence of a 1st-layer bias. They also show that this is the case for any stochastic gradient descent algorithm.    The authors also provide a generalization bound for DNNs. 
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper studies the anticoncentration properties of the Forster transform, an operation that can be used to learn a distribution over a disjoint mixture of few distributions. The authors propose a polynomial-time algorithm for the distribution-independent PAC learning of halfspaces in the Massart noise model, which has polynomials sample complexity. They also provide algorithms for the learning problem that achieve sample complexity of polynoms in the order of the bit complexity. "
SP:e5229305af00067ae2dbabd903e585964aec8928,"Graph neural networks are one of the most popular models for graph-based learning tasks. However, adversarial attacks against graph-level classification are not well-studied in real-life applications (e.g. biochemistry and social network analysis). This paper proposes a Bayesian optimisation-based attack method to attack graph classification models. The authors show that the adversarial robustness of node-level classifiers can be improved by perturbing the underlying graph properties, constraints, and modes of attack. The proposed method is evaluated on several graph classification tasks and shows that the proposed method can improve the robustness against adversarial perturbation. "
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of online label shift adaptation in the online setting. Machine learning models are sensitive to distribution shifts in the test-time label distribution, and adaptation to label distribution shift is an important problem in online learning. The authors propose two adaptation algorithms based on existing online learning techniques, namely Follow The Leader (FTL) and Online Gradient Descent (OGD), to adapt the model to the label distribution of the test distribution. They provide regret bounds for both simulated and real world label distribution shifts. The regret bounds are based on the estimation of the expected test loss. They also show that OGD can be used to adapt to different label shift scenarios. "
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper proposes a method for detecting and localizing gradual changes in the distribution of time-ordered observations for the detection and localization of gradual changes from time-order observations. The authors consider the abrupt setting where there is a discontinuity jump in distribution, but the data generating model has not been trained. The proposed method is able to learn the features of the distribution with prior knowledge. The method is shown to be effective for both detection and for localization. "
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper studies the blind source separation (BSS) problems in the brain. The authors propose to use Independent Component Analysis (ICA) to solve the linear BSS problems in signal processing. They show that a biologically plausible NN with neural architecture and synaptic learning rules is biologically plausible under the objective function of ICA. They also show that the ICA neural network (NN) is able to learn the parameters of a biological circuit. The algorithm is based on the observation that synaptic plasticity plays a key role in the success of the algorithm, and that biophysical variables (e.g. neuromodulators, extracellular calcium, local field potential, and nitric oxide) are important for the success. The paper also shows that the performance of the NN is highly correlated with the synaptic weight update. "
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper investigates the neural activity of Recurrent Neural Networks (RNNs) in order to understand the task-related behavior of RNNs. The authors show that RNN's activity is similar to neural data, and that the space of solutions for a given task can be represented by a two-neuron network. They also show that the diversity of the solution space in discrete dynamical regimes can be modeled as a function of the number of neurons in a RNN.    The authors also propose a tool to study the reduced dynamics of networks based on a compact directed graph, which is a tool that can be applied to a wide range of machine learning algorithms. They show that a number of neuroscience-inspired tasks (Delayed discrimination, Interval discrimination, Time Reproduction, etc.) and machine learning (e.g., neuroscience, machine learning, and neuroscience) can be understood as a result of this representation. They further show that extrapolation patterns for dynamical objects are similar to those of the hidden structure of a neural network, which suggests that the neural features of a network can be used as a proxy for the underlying dynamics of the environment."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper considers the problem of density estimation in unsupervised learning, i.e., modeling distributions of covariates in the context of density estimations. The authors propose a new problem, called arbitrary conditional density estimation, which is an extension of the previous problem of learning one-dimensional conditionals for the conditional distribution over the covariates. The proposed method, called Arbitrary Conditioning with Energy (ACE), is based on the observation that the density of the joint distribution can be approximated by the energy function of the densities, and that prior knowledge can be used for inference. The paper shows that ACE is able to reduce the complexity of the problem to a single-dimensional case by learning one of the one-dimensionality of the density. The approach is compared to prior methods and is shown to outperform the state-of-the-art in terms of performance in both arbitrary conditional likelihood estimation and data imputation on standard benchmarks.   "
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new loss function for low-level vision, called single image super-resolution (SISR), which is a variant of the MSE or L1 loss function. SISR uses adaptive weighted loss to train deep networks in two situations: (1) when the visual information is sparse on a pixel-by-pixel basis, and (2) when high-resolution image (mean) and low-resolution images (variance) are not available. The paper shows that adaptive weights are effective for both situations, and that the adaptive weights improve the visual quality of the trained deep networks.  The paper also shows that texture and edge areas are more important than smooth areas in the original photographic images.    The main contribution of the paper is the use of a sparsity prior for regularizing the loss function, which is based on the assumption that the high-resolution image (measured by the mean) and the low-res (measurement of the variance) are close to each other. The variance estimation is used to regularize the regularization of the loss for the SisR solutions based on uncertainty estimation.  Experiments show that the uncertainty-driven loss outperforms the standard MSE/L1 loss and other loss functions in terms of computation and performance. The authors also show that adaptive weighting can be used to improve the performance of trained SISRs in two other situations (textured and edge pixels).   In addition, the paper shows how the spatial adaptation can be performed in the case of different network architectures. "
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"This paper provides PAC-Bayesian generalization bounds for adversarial robustness based on the PACBayesian framework. The authors provide a worst-case analysis of the generalization error of the model, which is based on a theoretically founded analysis. The paper also provides a PAC Bayesian framework for estimating the averaged risk under perturbations with majority votes. The main contribution of the paper is that the authors show that the robust model can be robust to several types of attacks, including adversarial attacks, and that a robust model is robust to such attacks in the learning phase.    The paper is well-written and well-motivated, and the authors have provided a theoretical analysis of their results. "
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"Logical reasoning in Knowledge Graphs (KGs) is an important problem for querying mechanism in large and incomplete databases. Logical reasoning is often performed by considering the logical operations of projection and intersection, but previous approaches rely on spatial geometries (e.g. boxes) to represent query representations. In this paper, the authors propose a Probabilistic Entity Representation Model (PERM) that represents entities as a Multivariate Gaussian density with mean and covariance parameters for semantic position and a smooth decision boundary. The proposed PERM uses transformation tricks to transform unions into closed logical operations (projection, intersection, and union). The authors also propose an end-to-end objective function for each union, which can be used to learn closed logical operators. The authors evaluate PERM on three public benchmark KG datasets and show that PERM outperforms state-of-the-art methods in terms of F1 and other evaluation metrics on the public benchmark datasets. PERM’s competence is also demonstrated on a COVID-19 drugrepurposing case study, where the query answering process is based on a low-dimensional visualization of the Gaussian representations. The results show that the proposed work outperforms previous methods on F1, and is competitive on the logical query reasoning problem."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"Gradient-based hyperparameter optimization is a popular technique for few-shot meta-learning. This paper proposes a new algorithm that addresses memory scaling and gradient degradation issues by forward-mode differentiation. The authors provide theoretical guarantees on the noise reduction properties of the proposed algorithm. They show that greedy gradientbased alternatives outperform existing black-box methods on a number of tasks. They also show that the hyperparameters of the algorithm are not greedy, and that the greediness is not due to unrolled optimization, but rather to the fact that the algorithm is able to deal with memory scaling issues. Finally, the authors provide experiments on CIFAR-10 with a variety of tasks, and show that their algorithm outperforms existing algorithms in terms of noise reduction."
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper proposes a neural sequence models for structured tasks. Neural sequence models can be seen as systems in the context of Human reasoning, and the authors propose to learn candidate generations from a single neural sequence model. The idea is to learn a neural System 1 and a logical System 2, where System 1-like sequence models are used to generate a sequence of candidate generations, and a symbolic reasoning module is used to enforce logical consistency. The approach is based on neural inference, where neural inference is applied to the neural system 1 and neural System 2. Experiments on story generation and grounded instruction-following show that the proposed approach improves coherence and accuracy of neurally-based generations. The authors also show that System 2-inspired logical reasoning can be used to improve the coherence of neural systems, and that they are able to generalize to unseen tasks."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper studies the problem of off-policy evaluation (OPE) in continuous treatment settings, such as personalized dose-finding, where the treatment decision rule is not available for historical data. The authors propose a new estimation method for OPE in discrete treatment settings based on deep jump learning. The proposed method is based on the observation that deep learning and multiscale change point detection can be combined with deep discretization to improve the performance of OPE methods for continuous treatments. Experiments on Warfarin Dosing show that the proposed method outperforms existing methods. "
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a new modeling framework for inference in time-series data from Switching dynamical systems. The model is based on a Markov jump process with a subordinated diffusion process, which is used to model the prior and posterior marginal densities of the evolution equations. This is an important problem in both natural sciences and engineering applications where inference is important in both the natural sciences (biology and discrete-event systems) and in engineering applications (machine learning).  The authors propose a continuous-time variational inference algorithm that combines Gaussian process approximation for the diffusion level and posterior inference for Markov jumping processes. The authors use a path-wise Kullback-Leibler divergence for Bayesian latent state estimates and variational expectation maximization for point estimates of unknown system parameters.  The algorithm is evaluated on several real-world examples and is shown to perform well.   "
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper considers the nonlinear inverse problem, which is a componentwise nonlinear transformation of a linear mapping (e.g., A) to a nonlinear mapping f. The authors propose a new model for two signal processing problems: compressed sensing and phase retrieval. The model is based on a previously proposed nonlinear processing function, and the authors show that f can be decomposed into a spectrum of sensing matrices (i.e., the spectrum of the mapping from f to f). The authors also show that for phase-retrieval problems, the spikier spectrums of f are more likely to be the optimal sensing systems.  The authors then propose two recovery methods based on the expectation propagation algorithm (EP) and show that EP can be recovered from the EP based on spikiness of the spectrum. They also provide a new measure for EP that can be used for recovery.  Finally, the authors provide some experiments on 1-bit compressed sensing problems, showing that the EP can recover the optimal solution in the case of sub-Gaussian and orthogonal matrices.   "
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper proposes a new approach for zero-shot learning based on auxiliary semantic information (i.e., category attributes) for Generalized Zero-Shot Learning (GZSL). The proposed approach, called Dual Progressive Prototype Network (DPPN), aims to improve cross-domain transferability and category discriminability of visual representations. To achieve this, DPPN uses attribute prototypes as prototypical visual patterns and uses these prototypes to learn the attribute-related local regions. The attribute-region correspondence between the attribute prototypes and the corresponding category prototypes is learned by DPPP. Experiments on semantic-visual alignment and representation transferability show that the proposed approach achieves state-of-the-art performance. The paper also shows that the attribute localization ability of the visual representations learned by the proposed visual representations can be improved by combining progressive attribute localization and DPPn. Finally, the paper proposes an unifed framework for learning the attribute and category prototypes for visual representations, which can be used to improve the performance of the learned visual representations in GZSL. "
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"Defocus blur is one of the most common blur effects in images, and this paper proposes an end-to-end deep learning approach for removing defocus blur from an all-in-focus image for consequent vision tasks. The paper shows that models trained with GKM have a linear parametric form of the spatially variant blur kernels, and that the accuracy of the models is highly correlated with the amount of blur in the input image. The authors propose a deep neural network called GKKMNet, which uses a fixed-point iteration to perform the standard GkM-based deblurring. They also propose a lightweight scale-recurrent architecture and a scale-reward attention module to learn the mixing coefficients of the GKMs, which is used to further reduce the model complexity and improve the computational efficiency. Experiments show that the proposed GKMCNet outperforms the state-of-the-art in terms of model complexity, computational efficiency, and accuracy.   "
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper proposes a new contrastive contrastive learning method for low-resolution video. The proposed method is based on the idea of mutual information between RGB frames and motion vectors, which can be used to train models for SSVRL. Specifically, the authors propose to use motion vectors from compressed videos with low-resolution optical flows, where the visual content of the videos is not available, but only the RGB frames are available. The authors propose a cross-guided learning algorithm based on a multi-instance InfoNCE loss to learn the motion vectors with respect to supervision signals. Experiments on several downstream tasks show that MVCGC outperforms competitors in terms of both storage and computation efficiency. "
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"This paper studies the overconfidence of ReLU Bayesian neural networks (BNNs) in the infinite-width limit. The authors propose a Bayesian treatment for overconfidence in ReLU nets, which is based on the Gaussian process (GP). The authors show that under certain assumptions on the features of the BNN, Bayesian linear models with ReLU features can be approximated by ReLU linear models. They also show that the infinite ReLU BNNs with infinite GP are equivalent to finite ReLU neural networks with infinite width. Finally, they show that it is possible to approximate the GP posterior of the model with the same number of parameters as the Bnns, and that it can be used to approximate any BNN with finite width.  "
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper proposes two tools to improve the finite-sample confidence bounds for the estimation of potentially complex nuisance functions. The main contribution of the paper is to extend the bestarm-identification bandit framework to the case where the data collection mechanism is non-convex. The authors show that the asymptotic variance of the best-arm-id identification algorithms based on LUCB and Successive Elimination can be approximated by finite-sampling confidence bounds. They also show that these bounds can be used to derive the upper bounds for best-armed-invalidation algorithms. Finally, they show that their method achieves a sample complexity of $O(\sqrt{n\log n})$ for artificially generated data. "
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper considers the problem of distributed learning with the goal of improving the scalability of the communication in distributed learning. Communication cost is a major factor in the communication cost. The authors propose a new algorithm called ErrorCompensatedX to address the issue of biased compression. The algorithm is based on the observation that the variance of stochastic gradient descent in training without compression can be reduced by training with compression without error compensation. To achieve this, the authors propose to use a moving average of the history gradients to reduce the variance in the gradient. The variance reduced algorithms are analyzed under a unified theoretical analysis framework, and the authors show that the convergence speed is O(1/\sqrt{n}) for training with and without compression, and O(O(n^{-1/2}) for learning with compression error. They also provide an asymptotic convergence rate that matches the best known one for error compensation, which is the one for the algorithm that uses the moving average.   "
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper proposes a new approach to explain the performance of graph neural network (GNN) models. The authors argue that existing GNN explainers tend to focus on local explainability (i.e., explainability of the model on a given graph) and ignore class-wise patterns. They argue that these approaches are not performant because they do not consider the local context and do not take into account the global context. They propose a performant paradigm for multi-grained explainability, i.e. the model is trained on a single graph, and then a pre-training phase is followed by a fine-tuning phase. They show that their explainer outperforms baselines in terms of AUC for explaining graph classification on both synthetic and real-world datasets. They also show that the proposed explainer is able to provide multi-granularity in explaining the performance, which is a result of the pre-train and fine-tune idea for the explainer.   "
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"This paper proposes a method to generate counterfactual explanations for Graph Neural Networks (GNNs). The methods are based on a subgraph, where the noise is added to each node in the subgraph. The authors show that GNNs are able to learn a common decision logic, and that they can be used to generate explanations that are consistent with human intuition. They also show that the explanations can be generated by using common decision boundaries in the GNN. "
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper tackles the problem of voice conversion, where the goal is to convert the converted voice to a speech that is similar to the original voice, but the converted speech containing source speech style is different from the original speech. The authors propose VoiceMixer, which uses an information bottleneck and adversarial feedback to improve the generalization ability of voice style. The idea is to use self-supervised representation learning to tackle the information bottleneck, which is achieved by training a discriminator that is trained with adversarial training. The discriminator is composed of a content and style discriminator, and the discriminator can be trained with self-submission. Experiments show that the proposed model achieves better generalization and transfer performance when the source speech content is not available. The model also shows that the content information helps improve the audio quality. "
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"This paper proposes a Siamese voxel-to-BEV tracker for 3D object tracking in point clouds. The tracking is performed on sparse 3D point clouds, which is challenging in dynamic environments due to the sparse point clouds and lack of training data. The proposed method is based on a template feature embedding, and it is composed of two components:  1. A Siamesa shape-aware feature learning network that learns the discriminative features between a 2D center and a z-axis center, and 2. A voxels-toBEV target localization network that uses a voxeled point cloud with max pooling to learn a dense BEV feature map.  2. The Siamesed Shape-aware Feature Learning Network (Siamese Shape-Aware) is used to extract 3D shape information from a dense 3Dshape.  Experiments on KITTI and nuScenes datasets show that the proposed method outperforms the state-of-the-art methods. "
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a positional encoding method based on learnable Fourier features for attention-based deep model architectures such as the Transformer. Attentional mechanisms are used to encode spatial multi-dimensional position (e.g. pixel positions in an image) and positional relationships between two points in the image. A trainable encoding is learned using a multi-layer perceptron, which uses a learnable feature mapping. The representation is then used to represent a spatial multi dimensional position, i.e. the L2 distances between the two points and their positional relationships. The paper shows that the learnability of the multi-dimensions of the representation is beneficial for multi-dimension positional encoding, and that the proposed positional encoding can be trained with a single multi-layered perceptron. Experiments are conducted to show that learning the multi dimensional positional encoding with learnable fourier feature representation leads to better accuracy and faster convergence than existing methods. "
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of conditional independence (CI) tests for large graphs, where there are latent variables and a selection bias between the system's causal MAG and observational data. Constraint-based methods have been proposed to solve this problem, but these methods are computationally expensive. This paper proposes a new technique, called a recursive constraint-based method, which is able to achieve completeness guarantees in the case of large graphs where the structure of the system is not known. The authors show that the resulting technique is computationally efficient.    The authors also propose a new lower bound for the computational complexity of the proposed constraint - based method based on existing CI tests, and show that this lower bound matches the upper bound of the existing lower bound. The latter is used to compute the CI tests and the former to compute a lower bound on the number of samples required to complete the test. The paper also shows that the proposed upper bound and approach outperform the state of the art on both synthetic and real-world structures. "
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"This paper studies online decision making problems with information parallelism. The authors propose the batch Thompson Sampling framework to solve two canonical online decision-making problems, the stochastic multi-arm bandit and the linear contextual bandit. They provide a (asymptotic) regret bound for the case of a dynamic batch allocation, and show that the batch policy is able to achieve an exponential reduction in the exploration-exploitation trade-off when the number of samples is large enough. They also show that dynamic batch allocations outperform natural baselines (e.g., static batch allocations). "
SP:653a519e3c799c25e0d0b4240322642040b121a3," Domain adaptation (DA) aims to learn a domain-invariant representation that is invariant to changes in the target domain. The paper considers the multiple source DA and domain generalization (DG) settings, and provides upper-bounds on the target general loss for learning domain- invariant representations. The authors show that these representations are invariant in the sense that they do not change when the source domain changes. They also show that learning these representations is equivalent to learning them from scratch. The theory is well-motivated."
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes an aligned structured sparsity learning (ASSL) method to improve the performance of SR methods with lightweight architectures. The authors argue that existing model compression techniques such as neural architecture search and knowledge distillation require large memory and computation resources and moderate model size. The paper proposes a new model compression technique called network pruning, and shows that it can be applied to SR networks with moderate size. Specifically, filter pruning is used to prune residual blocks and scale parameters, and L2 regularization is applied to encourage sparsity.   The paper also proposes an alignment of the weight normalization layer in the proposed method, which is used as a sparsity structure alignment penalty term to enforce the norm of soft mask gram matrix between the pruned filter locations at different layers.  The authors show that the aligned structured learning strategy is able to reduce the size of an image SR network (i.e., ASSLN, a modified version of the original ASSL network) by a large margin, and show that it outperforms state-of-the-art methods in terms of model size and computation. They also show that, when the model size, computation, and number of network parameters are small, the proposed AssLN outperforms existing methods. "
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper proposes Episodic Multi-agent reinforcement learning (EMC), which is an extension of EMC to factorized MARL algorithms. In EMC, agents are encouraged to learn embeddings of local actionobservation histories and use exploration to solve complex coordination problems. The authors propose to use reward backpropagation for centralized training, and use individual utility functions (i.e., ""induced"" individual Q-values) to guide the local execution. They also introduce an episodic memory for policy training to store the explored informative experience. The intrinsic rewards for coordinated exploration are modeled as a weighted sum of the individual reward for each agent. The proposed method is evaluated on a number of tasks from the StarCraft II micromanagement benchmark, and compared with several MARL baselines. The method is shown to outperform the baselines on most of the tasks, and the authors also show that the proposed method performs well on a few didactic examples. "
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"This paper studies the problem of list-decodable linear regression with Gaussian covariates, where the noise distribution of the data is known. The authors provide a Statistical Query (SQ) lower bound for this problem, which is based on the observation that the linear regression model can be decomposed into two parts: (1) the hypothesis vectors, and (2) the regression vector. They also provide upper bounds for this task. The SQ lower bound is shown to be tighter than existing algorithms. "
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper considers the problem of Modeling a system’s temporal behaviour, which is an important problem for many Machine Learning (ML) approaches. In this application, ML models are trained with expert domain knowledge (e.g. patient health status and disease progression) from pharmacology. Pharmacological models are typically based on systems of Ordinary Differential Equations (ODEs). This paper proposes a latent hybridisation model (LHM) that combines expert-designed ODEs with machine-learned Neural ODE. The LHM is trained in a small sample regime and is able to learn observable quantities from both expert and latent variables. Experiments on synthetic data show that the LHM achieves state-of-the-art performance in this application. The paper also shows that the models are able to generalize to unseen variables in the system."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"Representation learning for meta-learning is an important problem in rapid learning of new tasks. Previous works, such as MAML, have been shown to learn task-specific representations, but the “frozen representation” objective has not been studied. This paper proposes a new per-task adaptation, which is a fine-tuning-based objective, to learn a representation with a shared representation across different tasks. The paper also proposes a theoretical framework for a MAMML-like algorithm. The proposed method is based on a shared structure, and the authors show that finetuning with gradient descent improves the risk bounds of the predictors trained with finetuned gradient descent. The bounds are tested on logistic regression and neural network settings, and show that the proposed algorithm performs well in few-shot learning. "
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper proposes a lexicalist approach to learn a compositional and grounded meaning representation of language from grounded data (e.g., paired images, texts, etc.). The authors propose a neuro-symbolic semantic program in symbolic form, where each syntactic type is represented as a program, and a neural network embedding is used to map the program to a set of shiny objects. These meaning programs can then be used to train a classifier to predict the lexical meanings of the program given the syntax. The learning is based on joint parsing and an expected execution algorithm, where learning is performed in an exponentiallygrowing compositional space. The authors evaluate G2L2 on two domains: visual reasoning and language-driven navigation, and show that G2l2 can learn compositions of words that are compositional (i.e., that are more likely to be the same syntactically type of adjective) and that are grounded in the symbolic form. They also show that the training time can be reduced to a single step, and that local marginalization can be used. "
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper proposes a stochastic Newton algorithm for homogeneous distributed stoching convex optimization. The authors show that the population objective of the population is a convex function of the stochedastic gradients of the target function and of the Hessian-vector products. They also provide convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression). Finally, the authors demonstrate that the proposed method can reduce the number of communication rounds compared to existing methods. "
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes a new similarity measure called Density-aware Chamfer Distance (DDCD) that is based on Chamfer distance (CD) and Earth Mover’s Distance (EMD). The authors claim that the proposed DCD is more robust to mismatched local density and better captures the fidelity of detailed structures. The authors also propose a point discriminator module that can be applied to any point cloud completion task.    The paper also proposes a guided downsampling step to improve the performance of the model.  The authors show that it is more sensitive to disparity of density distributions than CD and EMD. They also show that DCD can be used as a training loss for a model trained with CD loss, and that it outperforms CD, EMD, and DCD on point cloud similarity evaluation.  They also propose to use DCD for the pointcloud completion task, and show that the model can be trained with different metrics. "
SP:e4b302009520770814ff2c096020b779a9fc38fe,"Knowledge distillation is a popular technique to train a small student network to improve the performance of a teacher model (e.g., an ensemble of networks). However, it can be problematic when the predictive distributions of the student and the teacher are very different. This paper studies the problem of knowledge distillation to improve student generalization. The authors propose a new dataset for distillation, and show that the optimization can be done in an unsupervised way. "
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper proposes a new algorithm for computing the (k, ε)-coreset of decision trees in machine learning. The coreset of a k-tree is defined as a set of axis-parallel rectangles, where the error parameter is the number of times that the regression or classification loss of the tree converges to a point in the coreset, and the loss is the sum of the log-likelihood of all points in the tree. The authors show that the optimal k-trees can be found by computing the log of the error of each point in a coreset. They also show that decision trees and partition trees with the same computational geometry can be computed in a similar way. The coresets are shown to be able to reduce the computation time of random forests and parameter tuning on two real-world data-sets (sklearn and lightGBM). "
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the problem of Top-m identification (i.e., fixed-confidence Top-M identification) in misspecified linear bandit models. The problem is important in both medicine and recommendation systems, where the linearity and the structure of the problem are important factors. The authors propose a tractable lower bound for the sample complexity of the δ-correct algorithm for the Top- m identification problem. They also provide an algorithm for this setting that is tractable in general.    The main contribution of the paper is to provide a tractible lower bound on the sample cost of the proposed algorithm. The lower bound is based on the observation that the upper bound of the sample efficiency of the algorithm in this setting depends on the number of samples, the structure, and the misspecification of the data. The algorithm is tested on both synthetic and real-world data, and is shown to outperform the baselines in both cases."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,This paper proposes a new self-supervised learning method for graph contrastive learning. The key idea is to learn disentangled representations of the graph structure of a real-world graph. This is achieved by learning a contrastive loss that encourages the disentanglement of the latent factors in the graph representation. The proposed method is evaluated on both synthetic and real world graph datasets. 
SP:0a7edbbdabab11273689c40c517001eb46491113,"This paper studies the robustness of a network trained with stochastic simulation in Statistical Reliability Engineering. The authors provide theoretical guarantees for robustness under the assumption that the sample size is small and the network function is well-behaved. They also propose a statistical hypothesis test to improve robustness assessment. The proposed procedure is based on the Importance Splitting simulation, which is a well-known technique in large scale networks. Experiments are conducted to validate the effectiveness of the proposed method."
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper proposes a new generative modeling for machine learning. Deep polynomial neural networks (PNNs) have been widely used for unsupervised image generation. However, PNNs are typically trained with single-variable polynemic expansions, which are not suitable for conditional generation tasks (e.g., super-resolution). This paper proposes to use two-variable inputs, i.e., noise variable and conditional variable, to train two-variance inputs: noise and conditional. The authors propose a framework for autoand cross-correlation between the input variables and the output of the PNN, which is then used to train a polynomorphic expansion of the input variable. The proposed CoPE is evaluated on three different tasks: class-conditional generation, inverse problems, and edges-to-image translation. CoPE achieves state-of-the-art performance on all three of these tasks, outperforming the previous best results. The paper also shows that CoPE can be applied to a variety of conditional generation task, including attributeguided generation, class-conditioned generation, edge-to image translation, and image-to -image translation, where the noise is added to the synthesized image.   "
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper studies the connection between neural tangent kernel (NTK) and MMD, and proposes a new approach to compute the MMD statistic with a connection between the two. The main contribution of this paper is that it proposes a two-sample test based on the NTK, which can be used to reduce the memory and computational complexity of computing the standard two sample test. The proposed approach is able to compute NTK based two sample tests with the connection, which is more efficient than the standard online implementation based on MMD. The authors also extend the theories to kernel MMD and show that the connection improves NTK test statistic properties such as Type-I error and testing power. Experiments on synthetic and real-world datasets validate the effectiveness of the theory and the proposed NTK-MMD statistic.  "
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"This paper studies the detection and defense of adversarial attacks. The authors propose to use the minimum necessary information to train a neural net D( ·) and a variational autoencoder G( ·). The former uses class-disentanglement, while the latter uses the class-dependent information from the classifier G(x). The classification in x-G(x) is then used to train the adversarial attack. The paper shows that it can detect both clean images and adversarial images, and it can also defend against adversarial perturbations in both cases. In addition, the paper also shows that the perturbation generated by the attack can be used as a regularizer for the classifying and attack models.   The paper also proposes a new approach to detect and defend against the class adversarial detection and the defense of G(X). The approach is based on the observation that adversarial examples are more likely to be perturbed in the class dependent part of the class, and that the class of the perturbed examples is the one that is most likely to have the most information.  The authors show that the proposed approach is effective for detection and robustness to adversarial defenses."
SP:2789874561620ba7894c4672f935056bb911e919,"This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) in the federated learning (FL) setting. The authors show that the FTS is differentially private under differential privacy (DP) for deep neural networks. They also show that FTS can be used to improve the privacy-utility trade-off in two applications: (1) federated hyperparameter tuning, and (2) the use of DP for iterative algorithms.    The authors propose the Federated Thompson Sampling (FT) algorithm, which is a variant of the popular Federated Bayesian Optimization (FBO) algorithm. In BO, the privacy guarantee of FL is the same as that of FTS. However, in FTS, the user-level privacy is guaranteed by DP. In this paper, the authors propose to use DP for FTS to achieve the same privacy guarantee as in BO. The main idea is to use local modeling in BO to learn the parameter vectors in the DP framework. The proposed algorithm is based on distributed exploration (DE) to optimize the utility of the algorithm. The theoretical guarantees of privacy and utility of DP-FTS-DE are provided. Experiments on several real-world experiments show that DP-FT-DE achieves better privacy and better utility than FTS-BO. "
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"Multi-label classification (MLC) is an important problem where there are many labels with correlated (non-exclusive) labels, and it is one of the most important real-world problems where it is important to train MLC models with data annotation. However, there is a lack of cost-effective annotation with informative samples, which can be problematic due to the sparse label space. To address this problem, this paper proposes a Bayesian Bernoulli mixture of label clusters (BM) to model the global pattern of label correlations, where each label is represented as a mixture component. The BM is used to learn the label correlations between the data features, and a predictive GP is applied to the feature-component-label mapping between the BM and the predictive GP to learn data features. The authors show that the BM can be used to model sparse labels, which is useful for AL to learn sparse labels with sparse labels. The paper also shows that the GP can learn the mixture components of the data, and that the feature uncertainty and the label covariance (i.e. the label covariances) of data sampling can be learned from the BM.   The authors also propose an auxiliary variable based variational inference algorithm to tackle the problem of non-conjugacy in the correlated label space, and propose an inductive bias to ensure that the predictive distribution of the label prediction is invariant to the mapping process, which allows for end-to-end posterior inference. The proposed model is evaluated on three real-life multi-label datasets, and achieves state-of-the-art performance on AL on all of them. The model is also shown to be able to learn a predictive distribution for label prediction, and is able to generalize well to unseen labels. In addition, the model is shown to learn feature uncertainty (measured by the GP) for data sampling, as well as label covariate (measurement of feature uncertainty) and label covariation (measuring the correlation between label covariates). "
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"This paper proposes a new streaming data source, namely lidar, to improve the end-to-end latency of lidar perception models. The key idea is to use wedge-shaped point cloud sectors instead of point cloud, which is more efficient than a point cloud. The proposed methods are based on cartesian coordinate systems, where the sectors are partitioned into rectangular regions, and a polar coordinate system is used to map each region to rectangular regions. A core polar convolutional architecture is proposed based on feature undistortion and range stratified convolutions, and multi-scale padding is applied to the spatial context. Experiments on nuScenes dataset show that the proposed streaming based methods outperform non-streaming methods."
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper studies the problem of learning a differentiable surrogate for structured latent variables in deep learning models that incorporate prior knowledge. The authors propose a learning approach to learn a latent variable that is invariant to changes in the underlying distribution of the underlying variables. They use the Gumbel-Max trick to learn distributions that are invariant in structured domains. They show that the differentiable surrogates can be learned using standard score function estimators for optimization. They also propose two differentiable algorithms, one called stochastic invariant and one called Stochastic Invariant, to learn these variables for efficient learning. They demonstrate that the gradient estimates of gradient estimates and control variates based on the learned feature are biased, and that the surrogate can be used to correct the biased gradients. Finally, they show that structured latent variable models outperform their relaxation-based counterparts. "
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"Deep convolutional neural networks (CNNs) are commonly used for image denoising on large datasets, but they suffer from overfitting due to the large amount of noise in the original image. In this paper, the authors propose to train denoisers on a noisy image and then apply a multiplicative scaling parameter to the weights of the denoiser to make it more robust to noise. The authors show that GainTuning, which is a variant of CNN models trained with large datasets and trained on a large variety of image-denoising benchmarks, outperforms standard CNNs on denoizing on a number of popular image-denoising benchmarks. They also show that the models are able to learn features that are robust to the noise level, image type, and noise level of the input image.    The authors also propose an adaptive variant of their method, GainTune, that adaptively scales the number of layers in a CNN to avoid overfitting. This is done by removing the convolutions of the convolution layers in the CNN and replacing them with a single layer that has the same noise level as the original one. They show that this method can be applied to any CNN trained on synthetic data, and that it is more robust than the standard CNN.  Finally, they show that their methodology outperforms the state-of-the-art GainTuned method on the same data with low signal-to-noise ratios.  They also demonstrate that the adaptive gainTuning is also applicable to transmission-electronmicroscope images, and they use this data to demonstrate the effectiveness of their methodology.  In addition, they use their data to study the structure of catalytic nanoparticles, and show that adaptive GainTunting is able to improve the performance on this data. "
SP:90afa1102683b456bc72a54abef466326827546a,This paper proposes a fully differentiable architecture for simultaneous semantic and instance segmentation that combines a convolutional neural network and an asymmetric multiway cut problem solver. The latter is used to solve a combinatorial optimization problem that combines semantic and boundary predictions for panoptic labeling. The paper also proposes a new formulation for learning a smooth surrogate of the Panoptic quality metric. Experiments on Cityscapes and COCO datasets show that the proposed approaches outperform existing approaches. The authors also show that their approach can be applied to the problem of combinatorially optimization in panoptIC segmentation (COPS) and that the optimization problem can be solved efficiently with a simple gradient. The proposed approach is a good combination of optimization and deep learning.   
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes two sequence models: Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs). PCFGs model nested hierarchical dependencies (tree structures) and DBNs model continuous latent variables.    The authors propose Recursive Bayesian Networks (RBNs) based on the use of PCFGS and the extension of DBNS to the case of discrete latent variables, where the latent variables are assumed to have a nested hierarchical dependency structure.  The joint distribution of the joint distribution over the discrete or continuous latent variable of two tree-structured bayesian networks can be expressed as a function of the number of possible structures in the nested structure of the network structures, which is a special case of the mixed discrete-continuous case.  In this case, the joint inference can be done by maximizing the maximum posterior estimates obtained by gradient descent between the inside and outside probabilities of RBNs, which are obtained by computing the marginal data likelihood (evidence) and the marginal posterior distribution over all possible structures. The authors show that the PCFG and the DBN can be combined to form a Gaussian RBN, which can be used as a generalization of the RBN.  Experiments are conducted on synthetic data and on real-world data, showing that the joint probability of the pair of latent variables can be approximated by the maximum likelihood of the inside or outside of the latent variable, and that PCFGFs and Dbns can approximate the joint probabilities of the two latent variables in a similar way.  Finally, experiments show that RBNB are able to learn to segmentate noisy sequences, and can be applied to both robust parameter optimisation and Bayesian inference, and perform better than either of these methods.  They also show how RBNS and RBNN perform well in terms of change point detection, hierarchical clustering, and segmentation on musical data at the raw note level, and show that their performance is comparable to the performance of previous work (e.g. [1]. "
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a novel constrained backpropagation (CBP) algorithm for training deep neural networks with different objective functions (backward propagation of errors) and loss functions (different loss functions). The authors propose a pseudo-Lagrange multiplier method to learn the objective function, and propose a new constraint called two-bit shift weight constraints (i.e., the weight precision is two bits larger than that of the original objective function). They also propose a posttraining method to improve the performance of the proposed CBP algorithm.    The authors show that the proposed constraints (binary and two-bits shift weight) are sufficient to train AlexNet with CBP, ResNet-18, and GoogLeNet. The authors also show that CBP can be used to train a deep neural network with a minimal performance loss.  The paper also shows that using CBP on ImageNet with a post-training method, the proposed algorithm achieves better top-1 accuracy than existing methods.  In addition, the authors demonstrate that the learning algorithm can be learned with the proposed constraint functions, and that the CBP is able to learn a learning algorithm with different constraint functions (e.g., binary vs two bit shift).   Finally, the paper shows that the resulting algorithm can also be used for training a deep network with binary weights. The proposed algorithm is shown to be able to achieve better performance than existing algorithms.  On ImageNet, it is shown that the learned algorithm achieves the best performance with the constraint functions and achieves a top-of-the-articulate performance. "
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"Active learning uses an acquisition function to improve data/label efficiency. Active learning can be seen as an extension of existing active learning scenarios for Gaussian Process Classification (GPC) where the goal is to reduce the amount of labeled data for labeling in an onestep-look-ahead manner.   This paper considers the discrete instance set (pool-based scenario) and the continuous instance space (query synthesis scenario).   The authors propose two active learning strategies for estimating the classification error, called Estimated Error Reduction (EER), based on two existing algorithms for EER-based active learning.  In particular, it is shown that it outperforms existing gradient-based optimization techniques for query synthesis in the continuously instance space, and it is also shown that the query synthesis active learning algorithm can be applied to existing EER strategies.  The main contribution of this paper is to propose two algorithms for estimating EER in the case of GPC.  First, the authors propose to use the gradient of the acquisition function as a gradient chain rule, which is a one-dimensional integral to the joint predictive distribution of label pairs.  Second, they show that the proposed algorithms outperform existing state-of-the-art algorithms in terms of sampling efficiency on both synthetic and real-world datasets.  They also show that their algorithms are more computationally efficient than the state of the art algorithms.  Finally, they provide theoretical analysis of the properties of EER based acquisition functions and show that they are more robust to computational overhead. "
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper considers continuous variational autoencoder (VAE) models, where the parameter gradients of the energy function of the model are bounded. The authors show that the energy functions of VAE models have bounded gradients, and that unbounded gradients can lead to posterior collapse. They also show that under- and over-regularization of the VAE energy function can cause numerical instabilities in the presence of large gradients. Finally, they show that such gradients are responsible for the numerical instability of autoencoders trained with regularization on data on a low-dimensional manifold (e.g. natural images).    The authors also provide a theoretical analysis of the over- and under-regularisation of VAEs. They show that autoencied-based architectures (i.e. VAE-based models with infinite gradients) suffer from over-and underregularization, and show that there is a trade-off between the number of samples and the amount of regularization. They further show that a model trained with infinite regularization can suffer from suboptimal feature selection, which can be attributed to the infinite gradient of the autoenced energy function."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the bandit problem with graph feedback in the setting of a directed graph, where the goal is to maximize the min-max regret of a given graph. In this setting, there are a finite number of bandit arms and each arm has a discrete number of vertices. The authors consider the problem of partitioning the graph into sub-graphs and solving them using a linear program, i.e., a fractional vertex packing set. They show that under certain assumptions, the regret of the algorithm is bounded by the fractional weak domination number and the k-packing independence number. The regret upper bound is based on the strong duality theorem, and the lower bound relies on the fact that the integrality gap of the dual linear program is bounded. They also show that for trees and graphs with bounded degree, the vertex packing problem with bounded integrality can be solved with bounded regret.    The paper is well-written and well-motivated. The notions are well-grounded. The bounds are tight. The proof of the bounds is clear."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper proposes to use Shapley values for model agnostic feature attributions based on neighbourhood reference distributions. The idea is to use the global population distribution to model feature absence, and then use the Shapley analysis to estimate feature absence from a global population. The authors use the Nadaraya-Watson estimator, which is a kernel regressor based on the self-normalised importance sampling estimator. Neighbourhood Shapley value is then used for sparse feature relevance attributions. They are shown to improve on-manifold explainability and robustness against adversarial classifiers. "
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes a novel approach to learning feature representations for deep reinforcement learning (RL) by learning virtual state-action sequences (i.e. state-actions) from un-experienced or less-experimental trajectories (e.g. from a limited number of demonstrations). The authors argue that RL is prone to data inefficiency due to the lack of cycle-consistent virtual trajectories. To address this issue, the authors propose PlayVirtual, which learns a backward dynamics model for each trajectory cycle, and uses them for feature learning to improve the data efficiency of RL feature representation learning. The dynamics model is trained to learn the dynamics of each trajectory in the latent space, and then the learned dynamics model can be used to learn a dynamics model of the entire trajectory. The authors also introduce a cycle consistency constraint to ensure that the trajectory is consistent across all possible actions. The proposed method is evaluated on the Atari and DeepMind Control Suite benchmarks, and the results show that the proposed designs outperform the baselines. The method is also shown to be robust to groudtruth state supervision.   "
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,"This paper studies the problem of robustness to noisy labels on large real-world datasets. The authors propose a framework to evaluate the robustness of a network’s architecture and the target/noise functions. They show that the predictive power of the representations of a linear model trained with clean labels is the same as that of the network trained with noisy labels. They also show that for neural network architectures that are more robust to noise, the architecture is more robust than the noise. Finally, they show that predictive power for representations trained with the same predictive power as methods that use clean labels outperform noisy-label-training methods on test accuracy."
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper considers the problem of learning a reward function in reinforcement learning (RL) algorithms. The authors propose a method to learn a value function that is invariant to transitions in RL algorithms. This is achieved by learning an RL algorithm that learns the reward function for a sequence of transitions. The proposed method is based on a data-driven Bellman equation, where the intermediate reward function is learned in a two-stage process. In the first stage, a control algorithm is used to control the transition, and in the second stage, the method learns the value function using a set of hyperparameters. The paper shows that the proposed approach outperforms prior methods in terms of the number of transitions required to learn the final reward function term. "
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization in both convex and non-convex settings. In the convex case, the authors show that the optimal excess population risk of an algorithm in near-linear time for general convex losses is nearly the same as that of the algorithm in the l2 setting, and the algorithm for the l1 setting has a dimension dependent lower bound. The authors also show that for general non-smooth convex loss, the algorithm has a nearly dimension independent rate in linear time.   In the non convex setting, the paper shows that the algorithm with nearly-optimal excess population loss is also nearly-linear in the number of samples.  The paper also shows that for certain special cases of differentiallyprivate algorithms in super-lineartime, the algorithms for the differentially public l1-case and for the polyhedral constraint l2-case are nearly-parametric.  Finally, the proposed algorithms are applied to the non-differentially private non-consistent l2 and lp-case, and are shown to have nearly-dimension independent rates.  For the constrained l2 - case, a linear-time algorithm is shown to be nearly-approximate for the smooth losses and the polyhedron constraint.  In addition, the method is applied to non-strongly convex weakly-concave optimization, where the proposed method outperforms a non-private algorithm by a factor of 2.5 to 3. The proposed method is also applied for non-sparse lp setting, which is a generalization of the proposed by the authors of the previous work. The main contribution of the paper is the polylogarithmical (in the dimension) overhead. "
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies the cooperative bandit problem for large-scale decision-making in the setting of stochastic networks with arbitrary corruptions and delays. The authors consider two real-world communication scenarios, i.e., cooperative bandits and message-passing, in which the communication is performed by a group of agents. In the first case, the authors show that the communication between agents can be done via a set of discrete stochastastic networks, and in the second case, agents are able to communicate with each other in a distributed setting.    The authors study the problem in the case where the agents have access to adversarially corrupted rewards. In particular, they consider two scenarios: (1) when the agent has access to instantaneous rewardsharing via a network with random delays, and (2) when there is a byzantine communication among agents.  In both cases, the agents are allowed to share information about the current state of the network, and the goal is to minimize the incurred group regret. In both scenarios, the algorithms are decentralized, and decentralized algorithms have near-optimal guarantees on the incurred regret.  The paper also considers two real world communication scenarios:  (a) when agents are in a stochastically time-varying networks, which is a common setting in real-real-world distributed settings (e.g., message- passing), and (b) where agents are not allowed to have perfect communication, but can only share information in a limited time window. In this setting, they show that a delayed-update algorithm is more efficient than the state-of-the-of the-art, and achieves tighter network-dependent minimax lower bounds on the group regret, and outperforms state-and-noise algorithms in both cases. They also show that their algorithm is able to achieve better network topologies compared to the previous state-only or delayed-updates-only algorithms, and show that it is also able to outperform a state- of-the art algorithm that is not decentralized."
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a post-training quantization algorithm for vision transformers for computer vision applications. The authors show that convolutional neural networks have similar architectures and feature representations on mobile devices, but their architectures are more suitable for feature representations that can be quantized by a transformer. They propose a mixedprecision quantization scheme based on the nuclear norm of the attention mechanism. They show that the optimal low-bit quantization intervals for the quantization task can be found by optimizing the self-attention. They also propose a ranking loss to further improve the performance of the proposed quantization objective. Finally, they show that quantization loss and feature diversity can be further improved by the proposed nuclear norm in the attention map. The proposed method is shown to outperform existing posttraining quantitative quantization algorithms on several benchmark models and achieves top-1 accuracy on the DeiT-B model on the ImageNet dataset."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the overestimation issue of Q-learning in the setting where the constant learning rate is polynomial in the state-action space. The authors show that double Q - learning can be seen as a special case of this problem, and propose a finite-time analysis of the convergence rate of double Q-learners. The main contribution of this paper is to show that the slower convergence rate is due to the exponential decay of the polynomical learning rate, and that the global optimum is a synchronous double-learning. They also propose a new sampling strategy for asynchronous double-Q-learning, which has a time complexity of O(1/\sqrt{n}) for a finite number of samples. The paper also provides a finite time analysis of their asynchronous algorithm, and shows that the time complexity is O(n^{-1/2}) for the case where the discount factor is constant. Finally, the authors provide some analytical tools to further improve their convergence rate.   "
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper studies semi-supervised learning (SSL) studies in the setting of out-of-distribution (OOD) detection. In this setting, the distribution of labeled data is unknown to the learner, and the goal is to detect OOD samples from the unknown distribution. The authors consider two OOD detection settings: unlabeled and test data, and a setting where the unlabeling data comes from the same distribution as the labeled data, but the test data is from a different distribution.    The authors propose two SSL algorithms for real-world applications. The first approach, called Structure-Keep Unzipping (Stephanie et al., 2018), is an optimization algorithm that tries to find a good trade-off between the quality of the unzipped representation of the labeled and test samples. The second approach, named STEP, is an extension of the technique proposed in [1], which is a technique that is applied to the problem of out of distribution detection. It tries to learn a representation space that can be used to distinguish between OOD and not-out of distribution (OD) samples in the training data. The paper shows that the proposed approach outperforms existing methods in detection on three benchmarks for detection.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]"
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper proposes a multi-task learning framework for visual reasoning tasks that require visual grounding in visual reasoning, such as phrase localization, referring expression comprehension/segmentation, and visual reasoning. The authors propose a two-stage setup, where a visual-lingual encoder is trained with two modalities in a transformer architecture, and a decoder is used to learn a model for contextualized lingual queries. The two approaches are applied to the tasks of phrase localization (REX) and segmentation (RES). The authors also propose a one-stage multi-tasking framework for the visual grounding tasks, where the model is trained on a pre-trained dataset and then fine-tuned on a post-training dataset. The decoder learns a bounding box for each task and a segmentation mask for the referred regions. The contextualized model is compared to state-of-the-art methods on the REC and RES tasks, and is shown to outperform complex task-specific one-staged architectures. The pre-training schedule is also evaluated on an external dataset, and the authors also show that the model can benefit from contextualized information and multi-target training."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper proposes a new algorithmic approach called Boosting. Boosting is an algorithm that can be applied to both weak and moderately inaccurate hypotheses. In particular, a weak learner (i.e., an agnostic PAC learner that is agnostic to the classification loss) is proposed, which is a variant of multiclass boosting. Multiclass boosting is an extension of boosting to the case of weak-learner calls. The authors propose a boosting algorithm for weak hypotheses, which they call AdaBoost. The idea is to combine the benefits of both the booster and the weak learners, where the booster is the one that learns the weak hypothesis and the learner is the agnostic one. The paper also shows that AdaBoost can be used to improve the performance of a weak learners in the sense that it can be shown to be more robust to changes in the number of weak learners and to the change in the weak learners’ accuracy parameter. "
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper proposes two methods for unsupervised object segmentation and object-centric scene generation. The proposed methods are evaluated on simulated and real-world datasets with limited visual complexity. The authors propose two paradigms: (1) an embedding-based approach where the object representations are learned using RNNs, and (2) an iterative refinement of randomly ordered object representations using a clustering procedure.  The proposed model, called GENESIS-V2, is an extension of the work of [1]. The authors show that, in the case of an unnatural ordering of objects, the iterative learning of object-representations results in a stochastic stick-breaking process, where the number of objects in the scene is not fixed, but changes over time.  Experiments show that the proposed model GENESis- V2 is able to learn a variable number of object representations with the help of a few different types of RNN, and that it is more robust to unnatural ordering than baselines.   The authors also compare the performance of the proposed baselines on synthetic datasets and real -world datasets, and demonstrate that the method outperforms the baselines in terms of performance for both image segmentation (unsupervised segmentation) and for object-centric scene generation (object-centric scenes).  "
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper proposes a new black box method for point predictions. The proposed framework is based on conformal inference, which is an extension of previous methods that consider the data generating distribution as a function of the underlying data generating process. The authors show that the performance of existing conformal inferences can be improved by adapting the learning problem to deal with distribution shift. They also show that their method, called adaptive conformal infer, outperforms existing adaptive approach in terms of coverage frequency. They evaluate their method on several real world datasets and show that it outperforms state-of-the-art methods.   "
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"Multi-person pose estimation in a crowded scenes is challenging due to overlapping and occlusions. This paper proposes a direct pose-level inference strategy that combines bounding box detection and keypoint grouping. The Pose-level Inference Network (PINet) learns complete pose cues based on visible body parts, and a Pose Fusion module is used to fuse the person bounding boxes and keypoints. PINet uses visual body cues to generate global pose cues, and then uses Part-based Pose Generation (PPG) to generate coarse poses using PINet. A Pose Refinement module is also used to refine the coarse poses based on the pose priors. The proposed PINet is evaluated on several crowded scenes pose estimation benchmarks, and it is shown to achieve state-of-the-art AP on the OCHuman dataset. The paper also shows that PINet can learn discriminative body parts that are more likely to be identified by PINet, which is a nice contribution. "
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,"Robust Markov decision processes (RMDPs) are an important component of robust reinforcement learning algorithms. This paper proposes an algorithm for learning the Bellman operator for S-rectangular robust Markov Decision Processes with L∞-constrained rectangular ambiguity sets. The proposed algorithm combines the homotopy continuation method and the bisection method to solve the S-triangular ambiguity in quasi-linear time. The algorithm achieves cubic time in comparison to leading general linear programming methods, and it outperforms the leading commercial optimization package. The method is well-motivated and well-written."
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper studies the online knapsack problem with machine-learned predictions for online algorithms. In particular, the authors consider generalized one-way trading and two-stage online knapack. The competitive ratio of the proposed online algorithms is shown to be upper and lower bound. The paper is well-written and easy to follow."
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"Episodic control is an important problem in reinforcement learning. This paper proposes a model-based episodic memory of trajectories for episodic control. This memory is used to train an agent to learn a complementary learning model that can leverage the memory. The proposed architecture is a combination of dynamic hybrid control, model- based, episodic and habitual learning. Experiments show that the proposed model outperforms existing reinforcement learning agents in both stochastic and non-Markovian settings. "
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"Semi-supervised learning (SSL) is a popular technique for learning from labeled data for deep learning. Semi-supervision learning (SSL) aims to learn from unlabeled data with pseudo labels. This paper proposes a new SSL method called DP-SSL, which is based on the data programming (DP) scheme to learn probabilistic labels for unlabeling data from the labeled data. In SSL, the initial labeling functions (LFs) are learned by human experts. In this paper, the authors propose to add LFs to the SSL style, where the LFs are learned using DP methods. The authors show that DP methods can learn the initial labels for the unlabelled data from noisy labels, which can be used to train a label model with noisy labels. They also show that learning from noisy labeled samples can be more efficient than learning from the original labeled samples.  The authors further propose a DP method based on DP methods to learn the LF for the labeled samples, which they call DP-SSL. They show that the proposed DP method is able to learn LFs that are robust to noisy labels and can be trained with DP methods without the need to use human experts to train the model. They demonstrate that their DP method outperforms the state-of-the-art SSL methods on a number of SSL benchmarks and test sets for classification and classification on CIFAR-10. The paper also shows that DP- SSL can learn to learn to classify unlabel data with unlabelable labels. Finally, they show that for classification on the test sets, DP-DSL outperforms other SSL methods.    On the test data, the paper shows that the DP method performs better on classification accuracy on the unlabelated data, and on test data on test sets. On the other hand, on the unlabeled data, it outperforms all the SSL methods in terms of classification accuracy. On test data that has been labeled correctly, it also outperforms them all.  Finally, on a subset of test sets that have been labeled incorrectly, it is shown that DP - SSL performs well on classification performance on the labelled data. On a subset that has not been labeled properly, it performs worse on classification. However, it does perform better on the labeled and unlabelized data. "
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"This paper proposes Multi-view Pose transformer (MVPT) for estimating multi-person 3D poses from multi-view images. MVPT learns a volumetric representation for estimating 3D joint locations from detected 2D poses, and then uses query embeddings of the skeleton joints from multiple views of the same joint to generate multi-perspective queries for intermediate tasks. The proposed pipeline achieves state-of-the-art accuracy on the Panoptic dataset.    MVP uses a hierarchical scheme to learn a query embedding of multi-people skeleton joints, and a geometrically guided attention mechanism, called projective attention, to capture cross-view information. The feature representations of the feature representations contain view-dependent camera geometry, which is used as input to the inputdependent query adaptation approach.  The MVP model is trained using the RayConv operation, where the query is extracted from multiple view pairs, and the volumetries of the multi- view information are concatenated together to produce a single query.  Experiments show that MVP is able to recover a human mesh from multiple images, and that it outperforms the previous approach on the AP25 dataset. In addition, MVP can also be used for recovering human mesh using the SMPL model, and is also able to perform well for modeling multiple-person body shapes, and for recovering multi-body shapes. "
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of learning problems with noisy responses. The authors propose a new family of sparse vectors, which is a family that includes sparse vectors with error-free responses. They show that these problems can be seen as support recovery and approximate recovery problems with 1-bit compressed sensing. They also propose learning algorithms to solve this problem. The main contribution of the paper is that the learning model is able to learn sparse vectors that have noisy responses, and that the query complexity is bounded by the number of unknown vectors."
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper considers the problem of detecting abrupt changes in temporal behavior patterns from sensors for industrial and security applications. The authors consider the bandit quickest changepoint detection problem and propose a novel online sensing scheme. The proposed scheme is based on the expected delay bounds, which are tighter than existing information-theoretic lower bound for finitely parameterized probability distributions. The main contribution of the paper is that the detection delay matches the information-thrighly lower bound in the case of sensing actions, and that the exploitation of querying informative actions can be avoided. The method is evaluated on both synthetic and real datasets, and the false alarm rates are shown to be very low. "
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper studies the problem of stochastic nested optimization (stochastic bilevel, min-max, and compositional optimization) in machine learning applications. Stochastic nested optimisation is a special case of the class of problem-specific algorithms and analyses where the goal is to find a solution to a nested problem that minimizes the total number of iterations needed to solve the original problem.    The authors consider three problems with a nested structure:   1.  2. 3. 4. 5.  They show that under certain regularity conditions, SGD-type updates to these nested problems have a better convergence rate than they do for non-nested problems.  The main contribution of the paper is the analysis of the sample complexity of solving the nested problem, and how it is related to the number of iterates required to find the optimal solution.  In particular, the authors show that for any SGD approach (e.g., the ALternating SGD gradient dEscenT (ALSET) method, which is an extension of the work of Zhang et al. (2020) and Liu et al., 2018), the number $O(\sqrt{T})$ of iterations required to solve a given nested problem is at most $O(T)$, where $T$ is the dimension of the problem. The authors also show that this sample complexity does not depend on the hidden smoothness of the underlying problem, but rather on the regularity of the SGD updates.  Finally, they show that SGD type algorithms can be used to solve these stochedastic nested problems, and that ALSET is a generalization of the previous work (Zhang et al, 2018). "
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper proposes a new transformer-based model for supervised learning on the multi-modal QA (VideoQA) task. The authors propose a new reasoning strategy called Siamese Sampling and Reasoning (SiaSamRea) approach that combines the siamese sampling mechanism to generate sparse and similar clips (i.e., siamesese clips, i.e. soft labels). The reasoning strategy is composed of two modules: (1) siamesed knowledge generation, where the inter-relationship between the input and output of a pair of modalities is considered, and (2) a soft label generation module, which uses siameser knowledge reasoning to generate the soft label for each modality.  The authors show that the proposed reasoning strategy improves network inference by incorporating the interdependent knowledge in the network. They also show that SiaSam rea can be applied to the multimodal reasoning paradigm and achieve state-of-the-art performance on the MSRVTT-QA, MSVD-qA, ActivityNet-QAs, How2QA and TGIF-Qa.   The paper is well-written, well-motivated, and well-structured. The experiments on the standard VideoQA benchmarks demonstrate the effectiveness of the proposed method. "
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a new approach to learn structured distributions for learning latent probabilistic representations from observed data. The approach aims to reduce the computational and memory complexity of learning such latent representations by reducing the number of hidden states in the combinatorial spaces. The authors propose two models: Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs). The authors show that the proposed approach can learn structured models with lower computational complexity than existing structured models in terms of the rank of the hidden states and speed. The central inference step is based on the matrix-vector product between the hidden state and the true hidden state, and the authors propose to use a low-rank constraint. The proposed approach is evaluated on language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling using neural parameterized structured models, and shows improved accuracy over existing models with large state spaces. "
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper considers the exploration-exploitation dilemma in Reinforcement Learning, where the goal is to learn an action-value function that maximizes the expected return of the current state-action pair, while minimizing the expected regret of the action taken by the agent. The authors propose two Bayesian exploration strategies: Sample Average Uncertainty (SAU) and Thompson Sampling, both of which are Bayes-based approaches for learning outcome models. The main contribution of this paper is to extend the technique to more complex environments where the probability distributions have computational intractability (e.g., deep neural network models and approximate posterior methods). The authors provide regret bounds for both exploration and exploitation trade-offs based on approximation techniques. They also propose a new uncertainty measure, Sample Average Ununcertainty, for contextual bandits, which is an extension of SAU, a frequentist approach to the uncertainty measure used by previous Bayesian approaches for outcomes uncertainty (such as Thompson Sampled). They show that SAU can be used as an uncertainty measure for deep contextual bandits as a drop-in replacement for epsilongreedy exploration, and show that the proposed SAU-based exploration outperforms existing deep Bayesian bandit methods on several real-world datasets with modest computation cost.    The authors also provide a theoretical analysis of the complexity of the SAU in the deep bandit scenario, which shows that the regret bounds are tight. "
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper proposes Disentangled Behavior Embedding (DBDE), a method for learning behavior embeddings from video. The authors propose to use a deep autoencoder with dynamic behavioral factors (pose) as a prior, and use an end-to-end approach to learn discrete behavior representations. They show that DBE can be combined with a stochastic temporal model to achieve robust behavior embedding. They also show that models trained on temporal structures of pose dynamics are able to learn consistent behavior representations that are robust to changes in the temporal structure of a motor task. They evaluate DBE and VDBE on two tasks: fine-grained behavioral motif generation and behavior decoding, and show that their approaches outperform previous approaches on both tasks."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"This paper proposes DMTET, a deep 3D conditional generative model based on user guides (e.g., coarse voxels). The authors propose a hybrid 3D representation for both implicit and explicit 3D representations, where the signed distance values are used to represent finer geometric details, and the reconstructed surface is represented as a mesh. Compared to previous implicit approaches, the authors show that the proposed model is able to generate complex 3D animal shapes with complex topology. The authors also show that their model is more robust to adversarial attacks compared to other deep generative models for explicit representations (i.e., meshes).    The authors introduce a deformable tetrahedral grid, a discretized signed distance function, a differentiable marching tetrahedra layer, and an implicit signed distance representation and an explicit surface mesh representation. The proposed approach is applied to the problem of conditional shape synthesis with coarse vauxel inputs, and is shown to be robust to reconstruction and adversarial losses on the surface mesh, and robust to the generation of the hierarchy of subdivisions, and to the arbitrary topology of the surface."
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper studies the estimation of highdimensional Mutual information (MI) in the context of information theory, statistics, and machine learning. The authors propose a surrogate measure of dependence, sliced MI (SMI), which is a measure of statistical dependence between two pairs of data points. They show that it captures the structural properties of MI and that it can be used for both scalable computation and estimation. They also show that SMI can be applied to deterministic transformations, which is an important property of MI.    The paper also shows that the SMI is more efficient than the original SMI in terms of statistical scalability, as it is based on the processing functions of raw data. The theory is tested on independence testing, feature extraction, and high-dimensional inference, where SMI outperforms the original MI in all cases, and is particularly useful for one-dimensional random projections. "
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper studies the query efficiency of non-myopic Bayesian optimization (i.e., expected improvement) compared to myopic methods on the problem of query efficiency. The authors consider the unconstrained setting, where there are no constraints on the sampled acquisition function surface, but there is a constraint on the number of feasible and infeasible regions. They propose a multi-step lookahead constrained BO method based on unreliable bruteforce derivative-free optimization for the Monte Carlo rollout acquisition function. They show that their method 2-OPT-C outperforms existing methods on query efficiency in both sequential and batch settings. They also show that the computational cost is much lower than that of unconstrain BO methods. Methods based on the reparameterization trick are also shown to outperform existing methods.  The authors also propose a likelihoodratio-based unbiased estimator for the acquisition function optimization, which is based on sample average approximation and infinitesimal perturbation analysis.  "
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper proposes a new joint distributional DQN (JDQN) model for RL, which is based on distributional RL and hybrid reward architectures (HRA) to learn source-specific value functions in the value network. The authors show that the return distribution of the joint distribution can be asymptotically different from the scalar value of the whole value network, and that this is due to the use of joint distribution modeling. They propose Multi-Dimensional Distributional Distributional Bellman operator (MD3QN), which is a variant of Distributional RL that learns a joint return distribution over the joint return and the Bellman target. They also show that this joint return can be approximated using Maximum Mean Discrepancy (MMD) as an empirical algorithm. They further show that MD3Qn is able to capture the rich reward correlation between the joint reward function and the reward function of the source. They show that in the control setting, the proposed method can be used to learn a joint reward distribution over richly correlated reward functions, and they show that their method outperforms existing RL methods using multi-dimensional reward functions in a control setting."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper proposes a geometric deep-learning model called CorticalFlow, which is based on the flow Ordinary Differential Equation (ODE) framework. The model is trained on diffeomorphic transformations of a 3-dimensional image, where the model is able to learn to generate a 3D surface from a template mesh’s topological properties. The topological errors are modeled as discrete resolution, and numeric conditions on the manifoldness of the manifold are used to ensure that the manifold is well-manifolded. The paper shows that the generation of surfaces from the template mesh is computationally efficient, with a GPU memory footprint much smaller than existing surface reconstruction methods, and its performance on brain cortical surface reconstruction is comparable to its state-of-the-art counterpart, but with much faster computation time.    The paper also shows that, in addition to the GPU memory savings, the authors also show that the use of numeric conditions to ensure manifoldness is also beneficial, and that this can be used to reduce the number of computationally expensive computations.  Finally, the paper shows how the proposed corticalFlow can be applied to the generation and generation of anatomically plausible surfaces, and how it can be combined with existing methods."
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"Data deletion algorithms have been a hot topic of interest in the recent years. However, the computational cost of these models is expensive due to the non-convex setting. This paper aims to address this issue by proposing a novel attack on the SISA algorithm.    The authors consider the setting of differential privacy and max information, where the update sequence is non-adaptive. They show that under certain assumptions on differential privacy, there are provable deletion guarantees for adaptive deletion sequences with provably provable deletions. They also show that there are deletion guarantees in non-consvex cases for adaptive sequences and non-autoregressive deletion guarantees. Finally, they show that non-vanishingly adaptive deletions are provably non-trivial under certain training methodologies.  The paper also shows that the attack can be applied to non-comprehensive models and can be used to attack the adaptive deleting sequences. Experiments are conducted on CIFAR-10, MNIST, and Fashion-MNIST. "
SP:7150006590e268ab732c9be6c9048f67a377f956,"This paper studies risk-averse Bayes-adaptive reinforcement learning in the setting where the epistemic uncertainty (i.e., the prior distribution of the MDPs) and aleatoric uncertainty (the inherent stochasticity of the prior) are known, and the conditional value at risk (CVaR) is known. In this setting, the authors consider policy optimising CVaR. They propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation to solve the problem in a two-player stochastically game. They show that their approach outperforms baseline approaches in this problem. "
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies deep ReLU networks trained on binary classification data. Deep networks have been shown to be universal approximators of arbitrary prediction problems when the data distribution is non-i.i.d. (i.e., when the joint distribution of the input and the output of the network is a finite sample from a joint distribution). Deep networks are typically trained using gradient descent with a constant step size.    This paper considers the problem of training shallow ReLU neural networks with logistic loss on a finite set of samples. The authors show that gradient descent can be used to reduce the population risk of the population misclassification rate of a classifier trained with gradient descent for restricted conditional models. This is done by using a sigmoid mapping from the conditional distribution to the true conditional distribution, which is defined as a function of the number of network nodes and gradient descent iterations.  The authors also show that the complexity measure of the conditional model can be expressed as the sum of the logistic and logistic losses of the classifier, and show that this complexity measure is a lower bound of the Bayes risk of a trained classifier.  In addition, they show that for any infinite-width random feature model, gradient descent on the inner (inputfacing) weights of a deep network with vanilla gradient descent converges to the optimal test error for a class of networks (shallow reLU networks, i.e. networks with ReLU activation functions).   The main contribution of this paper is that the authors show how gradient descent (with early stopping) is able to reduce population risk when the training time is large enough.  They show that, for any finite sample, the induced conditional model has the same local interpolation property as the original model, which means that the data simplicity of the training risk is the same as the optimality of the univariate classifier with respect to the data. They also show how this technique can be applied to large network width.  Finally, the authors prove that, under the assumption that the distribution of samples is a sphere around a point on the sphere, any univariate predictor with the same number of samples has a local interpolated property (which they call “universal interpolation”), and that this univariate predictors can be learned using deep networks.  This is achieved by gradient descent in the following way:   1. For any infinite set of data points, we can learn a Bayes (convex) risk for a conditional model  2. We can learn an infinite number of parameters of the infinite set, and we can use gradient descent to learn a class-conditional model.  3. For a finite number of training samples, we are able to learn the optimal classifier (with infinite width).  4. We are allowed to sample from an infinite set with infinite width and infinite depth.  We can also sample from this infinite set. "
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"This paper proposes a methodology for detecting coordinated group detection of misinformation on social media for social outcomes from misinformation campaigns. The authors argue that deep learning based coordination detectors are limited in their expressive power because they rely on prior knowledge (e.g., temporal logic and pre-defined filtering functions) that has limited expressive power, and propose a methodology to mitigate this issue. The proposed coordination detection framework incorporates the neural temporal point process, a prior knowledge of the Gibbs distribution of group assignment, into the detection framework. The method is evaluated on a real-world dataset where the sparsity of account activities in social media is observed, and is compared to a model that does not rely on account embedding space or prior knowledge. The model is also evaluated on the COVID-19 Vaccine Tweets dataset, where spreading misinformation on COVID19 vaccines is also observed. In addition, it uses a theoretically guaranteed variational inference approach to obtain a mean-field approximation to the distribution of the distribution, and it is shown to outperform a previous model on this dataset. "
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper studies the generalization guarantee of deep networks on nonlinear data with a low-dimensional nonlinear structure. The model problem is defined as a binary classification task over a set of smooth curves on a unit sphere, where the structure of the model problem depends on a deep fully-connected neural network. The authors show that under mild regularity conditions, the network depth is a fitting resource for the classification problem, while the geometric properties of the neural tangent kernel (NTK) regime of the reduction to dynamics are non-trivial. They show that the NTK is a translationally invariant operator on manifolds and that the decay properties of NTK can be controlled by fine-grained control on the network width and the intrinsic data properties. They also provide convergence and generalization results under randomly-initialized gradient descent. "
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes ReACGAN, an extension of Conditional Generative Adversarial Networks (cGAN) that incorporates class information into the training of a GAN. The authors argue that existing cGANs (e.g., auxiliary classifier GAN, softmax cross-entropy loss (ACGAN) and D2D-CE) do not capture the relational information of the class-labeled dataset, and that the diversity of a cGAN can be improved by incorporating this relational information. To achieve this, the authors propose a modification of ACGAN, where the classifier is trained on a subset of the training data, and the diversity is encouraged to be high on the unit hypersphere. The proposed re-parameterized ACGAN is evaluated on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets, and shows superior performance compared to ReacGAN with differentiable augmentations. Model weights and a software package are also proposed to train representative cGAN, which can be combined with any existing software package. Experiments are conducted on D2d-CE and StyleGAN2 architecture.  "
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper proposes a reinforcement learning (RL) algorithm called Policy Space Response Oracles (PSRO) which is an extension of the RL algorithm called Neural XDO (NXDO) to two-player zero-sum games. PSRO is an extensive-form double oracle algorithm, which is a variant of the Extensive-Form Double Oracle (XDO), a well-studied RL algorithm for large games. In this paper, the authors show that PSRO converges to approximate Nash equilibria in large games with infostates, while XDO converges directly to the Nash equilibrium with best responses. The authors also show that deep RL can be used to learn the best response in a similar way as XDO.   The authors evaluate PSRO on a sequential multidimensional continuous-action game, a Leduc poker game, and an Oshi-Zumo poker game. They show that XDO outperforms PSRO and NFSP in terms of exploitability, while PSRO outperforms XDO in the case of continuous actions. They also compare the exploitability of XDO to PSRO, NFSP, and a deep RL method called NXDO (Neural XDO) which uses deep RL to learn an approximate Nash equilibrium in a high-dimensional continuous action sequential games.  In addition, they show that the best-response in XDO can be learned using deep RL. "
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,This paper studies the problem of graph structured data for training deep neural networks. The authors consider node-level unsupervised learning (node-level clustering) and graph-level supervised learning (graph-level learning with adjacency matrices). The authors show that the representation complexity of graphs with the same number of nodes is the same as that of the graphs with different number of edges. They propose a permutation-invariant variational autoencoder to learn graph structured information. They also propose a model to learn the node order for graph reconstruction. The extracted representations are then used for downstream graph -level classification and regression.   
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"Graph Neural Networks (GNNs) have been shown to have limited scalability due to the limited graph and model sizes. This paper studies the issue of oversmoothing in GNNs. In particular, the paper shows that a GNN trained on a subgraph with bounded-size scope (i.e. bounded-width and bounded-depth) can only express a localized subgraph of bounded size, which can be decomposed into subgraphs with bounded number of critical neighbors. The paper proposes to decouple the informative representation learned by the GNN from the local neighborhood of the subgraph, which is then decoupled from the global graph. This decoupling is motivated by graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). The paper also shows that the model depth and the receptive field of GNN can be reduced to zero, which leads to degraded expressivity. The authors also show that the decouplings can be used to reduce the neighborhood explosion, which occurs when a node is added to the local graph, which results in expensive computation. Experiments on graphs and backbone GNN architectures demonstrate the effectiveness of the proposed design on several graphs."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"This paper studies the problem of learning a tractable likelihood for latent-variable generative models, such as Normalizing flows. Affine-coupling models have been shown to be universal approximators of the log-concave distribution of the Jacobian of the latent-to-observable-variable transformation, and the authors propose a new architecture, called ""affine coupling models"". The authors show that the likelihood of the likelihood in linear time can be approximated by affine couplings for regular distributions, and that networks with a nearly-singular Jacobian have representational power. The authors also show that well-conditioned affine-condensing flows can be used to approximate a log-consistent distribution, and show that underdamped Langevin dynamics and Hénon maps (a stochastic differential equation with symplectic diffeomorphisms) are universal approximation for affine coupling architectures.    The main contribution of the paper is that the authors provide a theoretical analysis of the generalization ability of affine couplings, and they show that for a certain class of networks with Gaussian padding, the likelihood-based training converges to the true distribution.  The authors further show that, for a class of normalizing flows that use iid Gaussians, a padded version of the input distribution is equivalent to a normalizing flow that is a function of ill-conditioning Jacobians.  Finally, the authors show empirically that, under certain conditions, affine fourier networks are able to approximate the true distributions of a structured dynamical system, and can generalize to a more general class of structures.  In addition, they also provide an empirical analysis of how well-conditional affine matching works for different types of networks. They show that in a few cases, well-constrained networks can be learned in a single step.  They also provide some theoretical analysis that shows that, in the presence of Gibbs measures that are non-trivial to compute, the learned representations are not universal, but can be learnt in an unsupervised way. "
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper considers the problem of coupon allocation in the online e-commerce market. Coupons allocation is an important problem in this setting, and the authors propose a method to learn a coupons allocation policy in an offline setting, where the coupons are available in an online fashion and the goal is to allocate the best coupon to the best buyer. The authors consider the Lagrangian problem of the coupons allocation, which is an interesting problem with a large computation overhead.    In this paper, the authors introduce the concept of the coupon allocation policy learning, and propose the λ-generalization (BCORLE(+)) framework. The idea is to learn the policy that maximizes the probability that the buyer will receive a coupon with the best price, and that the coupon will be allocated to the buyer.  The authors show that the policy learning process can be decomposed into two steps: (1) learning the policy space, and (2) learning a Lagrangeian multiplier variable $\lambda$.  The proposed method is evaluated on a simulation platform and a real-world e-commerchants market, and it is shown that the proposed method outperforms the existing offline reinforcement learning method and an off-policy evaluation algorithm for policy learning and policy evaluation. The paper also shows that the algorithm is able to improve the users’ retention rate. "
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601," Domain adaptation (DA) is an important problem in machine learning, and many existing DA methods rely on the assumption that a source pretrained model is invariant to label noise in the target domain. However, the affinity between source and target labels is not always the same. This paper proposes to use local affinity as a measure of label consistency to measure the intrinsic structure in the source domain. The authors propose a self regularization loss to mitigate the problem of noisy neighbors in a source domain classifier. The proposed method is based on the observation that the inherent structure in a domain adaptation is highly correlated with the affinity of the source and the target labels. To mitigate this issue, the authors propose to learn local neighbors, reciprocal neighbors, and an expanded neighborhood. The local structure is learned by learning local neighbors and reciprocal neighbors in the original domain, and expanded neighborhoods. Experiments are conducted on 2D image and 3D point cloud recognition datasets. "
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes to learn representations from sets for point cloud processing, graph learning, image/video recognition, object detection, and graph learning. The authors propose a geometrically-interpretable and generic pooling mechanism to learn a fixed-dimensional representation. The proposed pooling method is based on an end-to-end trainable Euclidean embedding for the sliced-Wasserstein distance between the probability distribution and the set-structured data (e.g., point-cloud). Experiments show that the proposed method outperforms existing set representation learning approaches. "
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper studies the problem of training stability of recurrent neural networks (RNNs) and proposes a new RNN called SBO-RNN, which is an extension of stochastic bilevel optimization (SBO) for RNNs. The main idea is to use feedforward and backpropagation for both the lower and upper-level optimization. The authors show that the SBO problem can be solved by stochedastic gradient descent (SGD) on an RNN, and that the hidden states of the RNN are invariant to the choice of hyperparameters. The paper also shows that the vanishing or exploding gradient can be avoided if the training data is sufficiently large. The approach is tested on several benchmark datasets and shows promising results. "
SP:d3a4300e21ca215334f256f0467a428470548fe4,This paper considers the online problem of minimizing power consumption in an online setting. The authors propose a learning-augmented online algorithm based on the predicted lengths of the idle periods. The algorithm is shown to be able to find power-saving states with low energy consumption and wake-up costs. The paper also provides a worst-case guarantee for the proposed algorithm for the online ski rental problem in the learning augmented setting. 
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper proposes a mathematical framework to quantify the transferability of learning models in terms of sample sizes for different tasks, task similarities, and sample complexity. The authors consider transferability for multi-source transfer learning problems, where the goal is to learn models that can transfer knowledge across different tasks.   The authors propose to measure transferability by optimal combining coefficients, which is a generalization of previous transfer learning algorithm designs.  The main contribution of the paper is to provide an analytical expression for the trade-off between sample sizes, model complexity, and similarities between two tasks, and to provide a quantifiable transferability measure based on a parameterized model.  In addition, the authors propose an alternating iterative algorithm for training deep neural networks to solve multi- source transfer learning tasks, which can be used as a knowledge transferring mechanism.  Experiments on image classification tasks show that the proposed approach outperforms existing transfer learning algorithms in both multi-distribution and few-shot scenarios, and is competitive with state-of-the-art methods on a number of practical learning tasks using similar analyses."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"This paper studies the problem of search asymmetry in the context of visual search. Visual search is an important problem in many real-world applications, and the asymmetry of search tasks is of great interest to the community. The authors propose a computational model that can be trained on a search image, and show that a model trained on ImageNet with eccentricity-dependent visual recognition and target-dependent top-down cues outperforms human behavior on a number of paradigmatic search tasks. The model is trained using ImageNet and a developmental diet, where the model is shown to be able to learn to find the optimal solution to a task-specific search problem.  The authors also show that neural network models with classical perceptual properties (e.g. eye movements) are more sensitive to the polarity of the input images, and that the model can learn to identify the optimal search solution for a particular search task from a single input image.   The paper is well-written, well-motivated, and well-structured. The idea of the paper is interesting, and it is interesting to see that the proposed method VisualSearchAsymmetry can be applied to a wide range of search problems. The paper also shows that the performance of the model on a model that is trained on natural images is comparable to human behavior.  However, there are some issues with the paper:   (1) The paper does not provide a thorough discussion of the role of polarity in visual search, and does not clearly state the contribution of the proposed model to the problem. (2) There is no discussion of how the model’s performance is related to the design of the search task, or the task specific training. "
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"Certifiable training is a popular technique to train robust models against adversarial examples. In this paper, the authors study the tightness of the upper bound on the worst-case loss for certifiable training. They show that the best-known Interval Bound Propagation (IBP) training with looser bounds outperforms existing models with tighter bounds. They also show that linear relaxation-based methods have similar loss landscapes to those of IBP, which is a state-of-the-arts method. Finally, they propose a new method based on tightness and smoothness.   "
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper studies the problem of online linear regression in the stochastic setting with bounded observations. In this setting, online regression algorithms have been shown to achieve high probability regret bounds for both the online ridge regression and the forward algorithm. The authors consider the case where the robustness of the regularization parameter is assumed to be bounded. They show that under this boundedness assumption, the ridge can be approximated by a linear function approximation, and they show that a forward algorithm with ridge is guaranteed to converge to the optimal solution with high probability. They also show that this modification holds for linear bandit settings. Finally, they extend their theoretical bounds to the case when the regularizer is not bounded. "
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper proposes an extension of the extragradient (EG) method to the nonconvex-nonconcave setting. The proposed method can be applied to this setting to solve minimax problems, such as generative adversarial network and adversarial training. The authors propose a two-time-scale variant of EG, called EG+, which is an O(1/k) rate for the squared gradient norm in the smooth structured nonconvolutional setting. They also propose an extra anchored gradient (EAG) method, which uses the anchoring technique in EG to anchor the problem parameters to the ground truth. They show that EG+ and EAG converge to a fast O(O(1 / k) rate on the squared gradients norm in a smooth convex-concentrated setting. Finally, they show that the two-scale EG, EG++ and FEG, can be used to solve two-times-scale versions of EG. In particular, FEG-A is a backtracking line-search version where the saddle-gradient operator has a negative comonotonicity condition, and the authors show that FEG+ and EG+ converges to a good solution at a faster rate than FEG."
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies the problem of uniformity testing on ranking data in statistical data such as rankings. The authors consider the setting where the alternative class is a large domain and the uniform distribution is a uniform distribution over pairwise statistics, and the Mallows model is trained on pairwise measurements of the ranking data. They show that the uniformity of the pairwise distribution of the two Mallows models is a function of the number of pairs of pairs in the dataset and the privacy budget parameter. They then propose a central DP algorithm for testing uniformity and show that it can be trained on binary statistics from ranking data, and they also provide a uniformity test algorithm for the local DP scenario.  Testing ranking data is a challenging problem, and this paper is an attempt to address this problem in a practical way. The main contribution of the paper is to provide a theoretical analysis of the privacy cost of uniforming algorithms. "
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"Greedy algorithms for learning graphical models with sparse structure have been shown to have the worst-case exponential runtime. This paper studies the problem of learning directed acyclic graphs (DAGs). The authors propose a greedy scorebased algorithm for learning DAGs. Compared to edge-greedy algorithms (e.g., GES and hill-climbing algorithms), the proposed approach is based on score evaluations. The algorithm consists of two polynomial-time algorithms, which are used in the previous work, to compute a score for each edge in the DAG. The score functions and optimality conditions are derived, and the authors prove sample and computational complexity bounds for the algorithm. The authors also show that the score-based algorithms can be used to improve the sample and complexity of order based algorithms.    The authors consider Bregman divergences and exponential families. "
SP:b60989706296b963b6671c01f22384978a334be1,This paper proposes a neural architecture dilation algorithm to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. The authors argue that adversarial training improves the accuracy and adversarial robustness for CNNs when they are trained with a dilation architecture. The proposed algorithm is evaluated on several real-world datasets and benchmark neural networks and shows that the proposed algorithm can improve accuracy and robustness with minimal computational overhead. 
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies the problem of model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). The authors propose an algorithm based on the Linear Mixture MDP assumption, where the transition probability kernel of the MDP is approximated by a linear function with feature mappings, and the planning phase is followed by an exploration phase, where a policy is learned that maximizes the probability of the next state given the current state and the current feature mapping. The authors show that under the linear Mixtures MDPs, UCRL-RFE is able to learn a ε-optimal policy with respect to the reward function. The main contribution of the paper is the introduction of a Bernstein-type bonus, which is used to improve the upper bound of the lower bound. The paper also shows that the reward of the reward-based algorithm is a function of the number of features and the number the feature mapping is.  "
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a Shifting Seasonal Matrix Factorization approach, called SSMF, to learn seasonal patterns in a data stream of events. The method is based on a lossless data compression scheme, and it can be applied to regime shifts in seasonal patterns. Experiments on real-world data streams show that the proposed algorithm outperforms several baseline methods without human intervention. "
SP:e4bb07033001be4d04695ef058f426d49fe440be,"Assignment is an important problem in informatics, and exact solvers for assignment problems with NP-hardness or incomplete input are of great interest. However, there are many existing approximation algorithms for this problem, which are computationally expensive. In this paper, the authors consider the combinatorial problem of assignment, and propose a learning-based 7 method that is computationally efficient. The authors show that their algorithms can be used to solve real problems in which the objective functions and prior assumptions are differentiable. They also propose a neural network architecture called WeaveNet, which is a core module that consists of a feature weaving layer and a learning module. They show that this model can solve non-linear assignment problems (e.g., stable matching in the strongly NP -hard settings) and generalize well to real-world assignment problems. The paper also shows that their algorithmic method can be applied to a variety of problems where there is a large gap between the true and approximation algorithms.    The authors also show that the learning module of their model can be seen as a special case of the learning-by-7 method. "
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper investigates the robustness of 3D deep learning models under adversarial attacks on point cloud data for safety-critical applications (e.g. autonomous driving). The authors show that existing threat models for 3D point clouds are robust to point-level input perturbations, and that self-supervised learning proxy tasks are also robust to adversarial training on self-surrogate learning of such threat models. The authors also show that self supervised learning of point cloud recognition with self-submission improves robustness against 3D adversarial robustness for MLP-based (PointNet), convolution-based, DGCNN, and transformer-based(PCT) 3D architectures. Finally, the authors also demonstrate that the adversarial perturbation of point clouds generated by local feature learning improves 3D robustness to point clouds produced by point clouds trained with self supervision.    The paper also shows that the robust performance of self-distributional self supervision improves when the number of points in the point cloud is small, and when it is used for adversarial propagation.  The authors further show that the performance improvement is due to a combination of self supervision and adversarial learning, which improves the performance of the self- supervision over the standard adversarial testing baseline. The paper further shows that 3D-adversarially robustness is improved when the point clouds in a point cloud are trained with DGCnn or a jigsaw proxy task. "
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"This paper studies the problem of online mirror descent in the setting where the computational bottleneck is high. Optimization algorithms such as projected Newton’s method, FISTA and mirror descent have been shown to have near-optimal regret bounds and convergence rates. This paper extends the results to linear optimization with conditional gradient variants. The main contribution of this paper is a toolkit for learning such projections from discrete and continuous perspectives. The toolkit is based on iterative projections from cardinality based submodular polytopes. The authors show that the runtime v/s convergence rates of these algorithms are suboptimal (in terms of O(T) regret), but that the O(S) regret of online Mirror descent can be improved to O(O(1/\sqrt{T}) with early termination. They also provide an away-step Frank-Wolfe algorithm with an early termination, and show that Bregman projections have a similar runtime. "
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"This paper considers the problem of estimating the natural parameters of the k-parameter minimal exponential family with i.i.d. samples. The natural parameters are assumed to be of the form $p(x,y) = \mathcal{O}(\log p(x|y)$, where $p$ is the number of samples and $y$ is a function of $p$. The paper proposes a method to estimate the natural parameter of the exponential family by using a maximum likelihood estimator. The main contribution of the paper is that the proposed method is based on the maximum likelihood estimation of a re-parametrized distribution of an exponential family, with finite sample guarantees for parameter estimation. The sample complexity of the method is shown to be polynomial in $O(\log n)$, and it is shown that the estimator is polynomially efficient. The paper also shows that the computational complexity is bounded by $O(n^2)$. "
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a new differentiable renderers for predicting intrinsic object properties, which is a natural extension of previous learning-based approaches for inverse graphics. The authors propose DIBR++, a hybrid differentiable rendering model that combines rasterization and ray-tracing to achieve photorealistic effects. The renderer is composed of environmental lighting and spatially-varying material models, where naive lighting and material models are replaced by non-Lambertian, specular reflections.  The authors show that the renderer can be used to model light transport by direct estimation and spherical basis functions, and that the learning frameworks for geometry, reflectance and lighting prediction can be applied to learn geometry and reflectance directly. They show that DIB-R++ is able to learn a compact and expressive shading model, which allows for speed and realism. They also show that physics-based differentiable rendererers that use path tracing are able to achieve better results than DIB -R++.  Finally, the authors demonstrate that the proposed approach outperforms previous state-of-the-art rasterisation and ray tracing based approaches in terms of material and lighting disentanglement in both synthetic and real data, and in artistic applications such as material editing, relighting and relighting."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a new differentiable training method called sampling-argmax, which is a variant of the popular soft-arg max operation in detection-based methods. In a differentiable manner, a probability map is learned for each pixel in the input image, and pixel-wise supervision is applied to this map. This map is then used to learn implicit constraints on the output of the model. The main idea is to learn the expectation of the localization error using a continuous formulation of the output distribution, and then use this expectation to guide the differentiable sampling process. Experiments show that the proposed method is able to reduce the average error of the neural network trained with soft -argmax to a neural network that is trained with no implicit constraints. In addition, the authors show that, in localization tasks, the proposed sampling-armmax outperforms the standard soft- argmax operation. "
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning (GCL) aims to learn generalizable representations from contrastive views using data augmentation. In contrast to the message passing scheme, GCL uses a graph changing action, i.e., a directed graph changing the graph structure of the input graph. The authors argue that existing models learning from incomplete structure information can be problematic due to the lack of intrinsic graph structural information, e.g., the directional structure of directed graphs. To address this issue, they propose GCL with hand-picking parameters for the predefined contrastive view in GCL, which is based on the observation that contrastive information is often lost due to poor performance of contrastive representation learning from data augmentations. The proposed directed graph contrastive learning framework (DGCL) leverages the use of the contrastive features of the directed graph structure to improve the performance of GCL. Specifically, the authors propose to use Laplacian perturbation (a directed graph data augmentation method), which is an extension of the work of (Zhang et al., 2017) to the context of directed graph representation learning, and it augments contrastive images from the directed graphs with different hand-picked parameters. They also propose a multi-task curriculum learning approach to train the proposed model, and show that it is able to learn contrastive representations from easy-to-difficult contrastive perspectives. They show that their model outperforms previous state-of-the-art GCL models in terms of performance on a number of benchmarks on three benchmarks, and that the model can learn to learn the structural features of a graph in a better than previous state of the art. "
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes SILG, a set of grounded language learning environments with a common interface. The SILG consists of three grid-world environments (RTFM, Messenger, and ALFWorld) and three symbolic counterparts of visual worlds, where the goal is to learn language by interpreting rich natural language for complex scenes. The authors propose to use a shared model architecture for RL across all environments, which is based on egocentric local convolution, recurrent state-tracking, entity-centric attention, and a pretrained LM.   The authors show that the shared architecture is more robust to the richness of observation space, richness of action space, language specification, and plan complexity, and that the unified models are able to generalize to unseen entities. They also show that SILG is able to learn a shared architecture for all environments better than other environment-specific architectures. The models are evaluated on the multi-environment benchmark, and SILG shows that the learned language grounding is more effective than existing methods. "
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper proposes Vision MoE (V-MoE), a variant of Vision Transformer, which can be seen as a generalization of Vision Transformers (MoE). The authors argue that Vision MoEs (MoEs) are more efficient than dense networks, and demonstrate that V-MoEs can be used to improve the performance of existing vision models. The authors also show that a 15B parameter model trained on ImageNet is able to outperform the state-of-the-art on a number of tasks, including image recognition and language processing.   The paper also proposes an extension to the adaptive per-image compute, which is based on a routing algorithm, and shows that the proposed extension can be applied to a wide range of vision models, and that the performance gain is significant.  Finally, the authors show that the results are consistent across a range of tasks and datasets, and show that V - MoE outperforms networks trained on a variety of tasks. "
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper considers the problem of training neural networks with 1-hidden-layer networks with n (sample size) neurons, where the loss function lies on a benign optimization landscape. The authors show that for narrow networks, there exists a global minimizer with zero training loss that is a KKT point of the network, which is a function of the size of the activation and the expressivity of the weights. They show that this point is a feasible region of the optimal solution of gradient descent. They also show that there exist local-min or saddle points in which a nice local region can be found, and they show that the local minimizer of the KKT points is a local-max or saddle point.   The authors also provide a constrained optimization formulation, which shows that projected gradient methods can be used to find KKT pointed out by SGD for training narrow neural nets, and that this constrained formulation can be extended to the case where the feasible region is not known. "
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper studies the problem of learning risk measures for risk-aware multi-armed bandit models, where the risk measures (variance, etc.) are correlated with the number of options available to the learner. The authors show that in real-world online decision making problems with correlated options, the optimal regret of a learner in a CMCB can be approximated by the concentration of the weight vectors of the options.    The authors provide matching lower bounds for algorithms with optimal regrets that depend on logarithmic factors.  They show that under certain feedback settings (e.g., full-information and full-bandit feedback), the optimal regrets of algorithms with random feedback are bounded by a constant factor that depends on the option covariance. They also provide analytical techniques to estimate the concentration based on the estimated covariance of the risk of selected actions.  Finally, the authors provide a theoretical analysis of the option correlation in risk- aware bandits, which shows that the concentration can be estimated based on analytical techniques based on sampling strategy properties of the bandit analysis.  The paper also shows that under some reward observation scenarios, there is a trade-off between the concentration and the covariance structures. "
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper studies the Nonnegative Matrix Factorization (NMF) problem in the context of the Positive Semidefinite (PSD) factorization, i.e., the r × r-dimensional PSD matrices in the original PSD factorization. In this problem, the objective is to factorize an arbitrary matrix into r–r-dimensional non-negative vectors, and the non-negativity of the matrix geometric mean of appropriate PSD matrix is assumed to be non-zero.    The authors consider the problem of nonnegative matrix factorization as a non-convex optimization problem, which is motivated by information theory and quantum resources.  In particular, the authors study the nonnegative NMF problem as the nonnegativity in the case of positive diagonal matrices.  The nonnegative problem is formulated as the NonNegative Matrix Factorisation (NNF) problem, where the NMF matrix is nonnegative. The authors propose an algorithm for solving NMFs that is based on the MajorizationMinimization framework, and show that the algorithm is non-asymptotically non-commutative. The non-computative extension to PSD is the Matrix Multiplicative Update (MMU) algorithm, which extends Lee-Seung’s algorithm to non-consistent PSD.  This algorithm is a simple extension of Lee-Segung's algorithm, and it uses a multiplicative update algorithm to compute the non negation of NMF. The update scheme has a simple squared loss objective, which can be solved efficiently.  in the sense that it does not require the use of any additional information.  Experiments are conducted on real and synthetic data, showing that the proposed MMU algorithm is able to compute a primitive for blockdiagonal PSD and tensor PSD factors, and that the primitive can be used as a primitive in the MMU method to compute blockdiagonality of the PSD function. The proposed method is shown to be computationally tractable, and can be combined with existing methods. The paper also provides a proof of the existence of Lieb's Concavity Theorem."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes Domain Generalization (DG) to improve the generalization capability of a model by leveraging domain-invariant information. Previous DG approaches do not consider domaininvariance information, and the authors propose a meta-learning framework to learn a domain-specific representation for better generalization.   The authors propose mDSDI, an extension of the invariance view to the case where features in the latent space are shared across all domains.  The paper shows that the proposed framework is able to generalize better than previous DG approaches that do not take into account the invariant view. The authors also show that mDS DI outperforms state-of-the-art techniques for DG on a dataset called Background-Colored-MNIST.  In addition, the authors also propose a unified framework that unifies the use of domain-agnostic and domainspecific features.  Experiments are conducted on a few datasets to show that the performance of the proposed DG is better than that of a single DG on the same dataset, and that the generalisation performance of a DG trained on a different dataset is also better than the performance on a single dataset."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"This paper proposes a new architecture for unconditional image synthesis based on the observation that diffusion models have better image sample quality than other generative models. The proposed method is based on classifier guidance to improve the sample quality for conditional image synthesis using gradients from a classifier. The authors show that the proposed method improves the diversity and fidelity of the generated images compared to ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512.  The authors also show that using the proposed architecture, BigGAN-deep can achieve FID of $O(\sqrt{n\log n})$, which is a significant improvement over the previous state-of-the-art FID achieved by the same architecture.   The paper also shows that the benefits of the proposed approach are not only in terms of FID, but also in the diversity of generated images, and that the method can be applied to improve diversity and improve the fidelity of existing diffusion models as well.  Finally, the authors show how to combine the benefits from the proposed model with upsampling diffusion models and show that their method can improve the diversity. "
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper studies few-shot learning with out-of-distribution samples, i.e. unlabeled samples that contain irrelevant features. The authors propose a novel approach to learn a classifier from such out- of distribution samples, which they call prototypes. The idea is that the classifier can be trained on out-out-of distribution samples (i.e., query data) and learn a good classifier on in-distributions. The proposed approach is applicable to both inductive and transductive settings. Experiments show that the proposed method is able to outperform existing pretrained networks with different architectures and architectures. The paper also shows that pre-training with prototypes can be more efficient than using feature extractors.  "
SP:b1f65724926f136979829b7a6c870bc31f38f591,"Prioritized sampling is a popular technique in experience replay for reinforcement learning. Prioritized sampling aims to generate samples that minimize the regret minimization objective. The paper proposes three criteria in prioritization: TD error, recentness, and corrective feedback. The authors propose an optimal prioritization strategy for the Bellman update, which minimizes the hindsight TD error. They also propose two methods to optimize the prioritization weight based on on-policiness and Q value. The proposed methods, ReMERN and ReMERT, are both methods that optimize the temporal ordering of states. They show that the proposed methods outperform other prioritized sampling algorithms on standard RL benchmarks such as MuJoCo, Atari, and Meta-World.   The authors also propose a new error network called ReMerner, which is a variant of ReMert, and show that ReMBERT can be used to improve the quality of sampling.  The paper is well-written and well-motivated. However, there are a few issues that prevent me from recommending acceptance of this paper. "
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,"This paper studies the problem of sequential prediction in a nonstationary environment with expert advice. The authors provide regret bounds for a linear-time algorithm with a relative entropy projection step, which is an algorithm that combines the benefits of long-term memory guarantees with the advantages of weight-sharing approaches. In particular, the authors show that the projection step in linear time is equivalent to weight-sharing approaches in terms of implicit costs of weight updates, and that the proposed algorithm can be used as an algorithm for any projection step with linear time. They also provide a theoretical analysis for portfolio optimization. "
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper proposes a variant of contextual linear bandits for routing applications in both navigational engines and recommendation systems, where the hidden d-dimensional value is a linear function of the input d. The authors propose two algorithms to solve this problem. The first algorithm has O(d log d) regret and a list size poly(d), while the second algorithm has a regret of O(log d) and a poly(D) regret.  The authors also propose two nearly tight algorithms for this problem, which are based on two algorithmic techniques: (1) Steiner’s formula for finding the centroid of a convex set, and (2) cutting-plane algorithms.  "
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"Automated machine learning (AutoML) is becoming a popular tool for data scientists to train data scientists. However, there are several issues with AutoML: (1) AutoML is not compatible with non-compositional code changes, and (2) the search spaces for AutoML optimizers are not well-suited for non-functional programming. To address these issues, the authors propose a sklearn-compatible AutoML library called Lale, which uses orthogonal combinators to transform compositional code into a set of pipelines that contain machinelearning operators. The authors also propose a translation scheme that uses hyperparameter schemas and search spaces from these pipelines. The paper also conducts a user study to show that Lale can be used to improve the performance of AutoML, and that it can be applied to a variety of tasks."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper studies the generalization performance of neural network weights on small datasets. The authors consider the problem of weight initialization for few-shot and continual learning, where the goal is to learn a good generalization error on a small number of training examples. They show that a pattern of sparsity on the problem-byproblem basis can be observed on the training data and the learning algorithm. They also show that, under certain assumptions on the learning rate and the number of examples, a learning algorithm will converge to a solution that minimizes generalization and interference.    The authors show that under certain conditions, there is a trade-off between generalization in generalization for selective sparsity and interference in both the generalisation and interference for both the problem setting and the setting where there is patterned sparsity. They further show that meta-learning is able to learn adaptable features that are more robust to the inductive bias of the learning rates, and that sparse gradient descent is more likely to converge to the optimal solution. "
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"This paper tackles the multi-view learning problem, i.e., shared response modeling, which is a multi-set learning problem where there are multiple views of the same data point, but the common components have shared independent components. The authors propose Shared Independent Component Analysis (ShICA), which is based on multi-sets canonical correlation analysis for unmixing matrices. Multiset CCA uses sampling noise for each view, and the proposed approach uses joint diagonalization. The proposed method, ShICA-J, uses a maximum-likelihood method called ShICA - ML, which uses the non-Gaussianity of the shared independent component and the additive Gaussian noise for the shared component, and uses second-order statistics for the joint component. Experiments on fMRI and MEG datasets show that ShICA outperforms other alternatives, and that the proposed method can be applied to the problem of shared components estimation using ShICA. "
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper proposes a new multi-agent reinforcement learning technique called Fictitious Co-Play (FCP) to learn a human-aware agent from human data. The proposed method is based on the observation that human data is highly correlated with human data, and that it can be used to train “human-aware” agents, i.e. agents that are able to generalize well to unseen environments. The authors propose to use two existing multi- agent reinforcement learning techniques, self-play (SP) and population play (PP), to train agents that can generalize to unseen domains. They also propose a new model called “Behavioral Cloning Play” (BCP) that is able to train two types of agents, namely “behavioral cloning play” and “bouncing ball play,” which can be trained using behavioral cloning to train a human model.  The authors demonstrate that their approach improves the generalization performance of the agents trained with human co-players in a two-player collaborative cooking simulator. They show that their FCP agents outperform SP, PP, and BCP in terms of generalization, and they also show that the agents can generalise to unseen tasks. The paper also shows that the proposed method can be applied to other multi-agents in competitive domains. Finally, the authors show that FCP can be combined with other multi agent approaches to improve the performance of agents trained to learn to play with an agent partner.    The main contribution of the paper is the introduction of FCP, a new method for learning to learn agents that generalize from unseen environments, which is a natural extension of the work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,19].   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] "
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes a novel method for cooperative multi-agent reinforcement learning in discrete and continuous action spaces. The approach is based on deep deterministic policy gradients to learn policies, which is similar to MADDPG, a multi-agents actor-critic method. The authors propose a novel approach called FACMAC to learn a centralised but factored critic that combines per-agent utilities and a joint action-value function. The centralised critic is a non-linear monotonic function, which allows for non-monotonic factorisation of the policy gradient. The paper also proposes a variant of QMIX, a well-studied and well-motivated multi-manual Q-learning algorithm, which can be used in the proposed approach.   The paper shows that FACMAC can learn the centralised policy gradient estimator in the joint action space, and it can be applied to a wide range of tasks that require monolithic, or monotonically factored critics. It also shows that it can learn a representational capacity that allows for coordinated policy changes across agents. The proposed FACMAC is evaluated on the multi- agent particle environments, the single-agent MuJoCo benchmark, and StarCraft II micromanagement tasks, and shows superior performance to other baselines. In addition, FACMAC shows that the proposed method is more robust to nonmonotonicity of the critic, and that it is able to learn better representations of the environment.  The authors also show that the performance of FACMAC matches that of MADDPGS and shows that, in some cases, it outperforms the state of the art. "
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper studies the use of memory-augmented neural networks (MAMNets) in the context of long-term memory in neuroscience. The authors propose a novel key-value mechanism for memory augmentation that is based on Hebbian plasticity and attractor dynamics. The key idea is to augment the memory of a MAMNet with an augmented version of the Hopfield network, which is a well-studied model in neuroscience and has been shown to be a good memory augmenter. The paper shows that the proposed method can be used to augment memory in a variety of tasks, including continual recall, heteroassociative memory, and sequence learning."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper considers the problem of learning tasks such as bipartite ranking, metric learning, and pairwise learning. It considers two machine learning tasks where the loss function is a function of the number of training samples and the amount of streaming data. The authors propose an approach for streaming data based on the online gradient descent (OGD) algorithm. They show that the scalability issue with OGD is that the gradient direction of the algorithm is non-differentially private, which causes the storage and computational complexity to be high. To address this issue, the authors propose to use stochastic and online gradients for the problem.    The authors provide stability results, optimization, and generalization error bounds for convex and nonsmooth problems, as well as generalization bounds for OGD with and without a buffering set. They also provide algorithms and stability analysis for differentially private SGD algorithms for both algorithms.  The paper is well-written and well-motivated. The techniques used for optimization and the generalization analysis are well-grounded, and the paper is clearly written."
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper proposes REDO, a class-agnostic framework for Dynamic Objects based on RGBD or calibrated videos. The core idea is to learn a canonical 4D implicit function based on aggregated temporal visual cues, which can be applied to any object dynamics, including rigid motion, non-rigid motion, and non-ruling motion (accommodating occlusion and camera settings). The paper proposes a unified framework for this problem setting, and introduces two modules. The first component is a 4D transformation module that transforms the object dynamics into a single object, and the second component is an implicit function that is learned from aggregated visual cues. Experiments on real-world video data 3DPW show that REDO outperforms existing dynamic reconstruction methods. "
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper studies the linear stochastic approximation (LSA) algorithms with fixed stepsize. The authors provide high probability bounds for LSA under the assumption that the products of matrices are non-convex. They also provide polynomial concentration bounds for random estimates. The main contribution of this paper is to extend the central limit theorems on the covariance matrices to the case where the stepsize is fixed.   The authors also provide Gaussian or exponential high probability bound for the case when the step size is large enough.  The paper is well-written and well-motivated, and the methods are well-suited for machine learning tasks. The bounds are also well-supported by experiments. "
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper proposes a new options framework for temporal abstraction in reinforcement learning. The authors consider discounted Markov decision processes (MDPs) and average-reward MDPs, where the option-interrupting behavior is discounted. They show that existing algorithms (i.e., intra-option algorithms and samplebased planning variants in learning algorithms) can be combined with existing algorithms and convergence proofs, and show that those algorithms can converge to the optimal solution in the Four-Room domain. They also show that the discounted MDP formulation is equivalent to the average reward formulation. "
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper presents a theoretical analysis of why Visual Transformers (VTs) perform better than Convolutional networks (CNNs) in terms of the convolutional inductive bias. In particular, the authors show that, unlike CNNs, VTs are able to capture global relations between image elements, and that they have a better representation capacity. The authors also show that the models are more robust than common CNNs when the local properties of the visual domain are incorporated into the CNN architectural design. The paper also shows that the robustness of VTs in the small training set regime is enhanced by the use of an auxiliary selfsupervised task that uses images from the training set as an auxiliary task. Finally, the paper shows that VTs can capture spatial relations more effectively than CNNs.   The paper further proposes a new task for VT training, called Drloc, which is a combination of the task from ImageNet and a variant of the auxiliary self-supervised learning task. The proposed method, called VTs-Drloc, combines the benefits of both the task and (supervised) training, and it is shown that the proposed method improves the accuracy of the VTs on ImageNet with a reduced computational overhead. The experiments show that Drloc outperforms the state-of-the-art."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"Label-free alignment of hierarchical datasets is a well-studied problem. Hyperbolic spaces provide informative representations of hierarchical data, and this paper proposes a geometric approach for label-free alignments. The authors propose the hyperbolic Procrustes analysis (HPA), which is based on the Riemannian geometry of the Lorentz model of the data.   The key idea of HPA is to decompose the data into three components: alignment, translation, scaling, and rotation.  The authors provide theoretical properties, stability, and computational efficiency for HPA, and evaluate its performance on several batch correction tasks on gene expression and mass cytometry data. The paper also proposes methods to perform label-based alignments in the same way as existing methods.  Experiments on unsupervised batch effect removal on the same data show that the proposed methods perform well."
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"Privacy-protected microdata is used to train a differentially private algorithm. The paper provides lower bounds on the accuracy of the accuracy (in terms of the logarithmic factor) of differential private query answering systems.   The paper is motivated by the uncertainty principle, which states that there is a trade-off between pure differential privacy and privacy with respect to the amount of microdata required to satisfy the microdata requirement.  The authors provide lower bounds for pure, approximate, and concentrated differential privacy, and show that under certain assumptions on the number of queries, the accuracy can be improved by a factor of O(log(d) for sum query and O(D) for point queries with noisy answers. The authors also provide mitigation strategies to mitigate the effect of the noise in the sum query, and provide lower bound on the O(d ) factor for the point queries under the assumption that the noisy answers are from the same set of queries.  Experiments are conducted on several benchmark datasets, and the results show that the lower bounds are tight."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"Goal-conditioned reinforcement learning (RL) and planning (RRT) are two popular approaches to tackle the problem of sparse reward and inefficient exploration in long-horizon tasks with dense reward/guidance. Planning is typically done by learning a curriculum of tree-structured sub-tasks, where dense feedback is used to guide a path-planner and an RL agent through the curriculum. In this paper, the authors propose CO-PILOT, which combines RL, planning and RL with dense feedback in order to address the issue of inefficient exploration and sparse reward in sparse reward environments.   Planning is done by training a planner that learns a planner for a given task from an easy-to-hard curriculum. The planner is trained in a way that encourages the RL agent to learn a dense feedback to guide the path-plunger and RL agent. The RL agent learns a bottom-up traversal of the tree, and the planner learns a top-down traversal from the top to the bottom. The goal is to learn an environment model that can be used as a guide for RL and planning.  The authors propose to train a RL agent and a planner in parallel, and then use mutual training between RL and planner to train the planner for the long-term goal.  In order to learn the environment model, the planner is first trained on a tree of sub-task, and RL is then trained to learn to explore the sub-trajectories of the planning policy.  Experiments on navigation and continuous control tasks show that CO-based RL outperforms RL (SAC and PPOE) in terms of success rate and sample efficiency, and RRT* in the case of RL (PPO). In the authors also show that the proposed CO-PUILOT outperform RL and RL when RL is used in combination with RRT*. The authors also compare the performance of CO-PIILOT with RL and RRR in navigation and continual control tasks, and show that it outperforms the combination (SoRB).    The paper is well-written and well-motivated. The authors have done a good job of combining RL with planning to improve the success rate, sample efficiency and improve the sample efficiency. "
SP:9911693a04a300b5a93634fb0267ef83e5489d77,"This paper studies the problem of black box explanations for model credibility. The authors propose a Bayesian framework for generating local explanations, and propose two techniques for generating explanations that are robust to hyper-parameter tuning. The framework is based on the idea that local explanations are a function of feature importance, and the authors show that LIME and KernelSHAP can be seen as credible intervals for feature importances. The paper also shows that the proposed framework is robust to uncertainty. Experiments are conducted on several real world datasets and user studies."
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper proposes Adder neural networks (ANNs) to reduce the energy cost of training energy-efficient neural networks and hardware accelerations. The authors show that ANNs have fatter tails than CNNs, which is due to the multiplications in convolutional neural networks, but also due to fatter weights in the feature space. They show that Adder ANNs (and CNNs) can achieve a low energy cost while maintaining the same accuracy as CNNs.   The authors propose two training tricks: (1) knowledge distillation, which distills training tricks into a single training step, and (2) Multivariate Skew Laplace distributions (MSP) which is based on the property difference between the similarity measurement of the filters and the features, and the properties of the feature distributions in the loss function.  They also show that unordered heavy tails in ANNs and CNNs are the main reason why ANNs perform better in classification.  The paper also proposes a method to remove heavy tails from ANNs by adding an angle-based constraint to encourage the diversity of tails. The proposed method is applied to the distribution parameters of each classifier. The paper shows that the proposed approach improves the performance of ANNs on a number of benchmarks, and that the distributions of the distributions are more diverse than those of CNNs and ANNs. Finally, the method is shown to be more robust to changes in the number of filters and features."
SP:cbccb65457564992d534504c0d060da44cafce8c,"Gradient Starvation: Gradient descent phenomenon is a well-studied phenomenon in the learning proclivity of over-parameterized neural networks. Gradient starvation happens when the cross-entropy loss of a neural network is large enough to cause the predictive features of the network to have a statistical structure that is non-trivial to compute. The authors show that gradient starvation happens in the presence of feature imbalances in neural networks, and that the learning dynamics of gradient descent is responsible for this imbalance. They also provide guarantees for a regularization method that is based on this formalism to regularize the feature learning dynamics and improve the accuracy of the regularized network.   "
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"Deep reinforcement learning is used to train superhuman AI to play competitive games (e.g., Go and StarCraft). The paper shows that AI teammates trained with learning techniques outperform those trained with subjective metrics of trust. The paper also shows that a rule-based AI teammate (SmartBot) outperforms the human-machine collaborative games when the game score is low. The authors also show that the AI teammates outperform AI teammates when the objective team performance is high.   The paper is well-written, well-motivated, and well-structured. The AI agents are trained in a cooperative card game Hanabi, where the game is played between humans and AI agents. The game is a cooperative cooperative game, and the AI agents can be rule- based and learning-based agents. There are three subjective measures of human-AI teaming: teamwork, interpretability, and trust. In this paper, the authors show that AI design and reinforcement learning benchmarking can be used to compare AI design to human design, and show that rule-free AI design outperforms AI design when the human design is good enough. They also find that the human AI team performance improves when the AI teammate is good at a task, and that the learning based AI teammate performs better when the learning- based agent is bad at the task. "
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper proposes a new method for visual question generation (VQG) that uses visual hints to guide the generation of human-like neural questions from an image and side information. The idea is to use visual hints for both uninformative and non-referential questions. The model is trained with double visual and answer hints, where the visual hints are generated from salient visual regions of interest and the answer is generated from the same region of interest. The authors propose a rule-based similarity matching method to identify candidate visual hints, and they also propose a learning approach for double-hints based VQG based on a weakly supervised learning problem. The proposed method is evaluated on three benchmark datasets and compared with several existing approaches on three different metrics: automatic machine metrics, human evaluation, and a human evaluation of the quality of the suggested visual hints. The results show that the proposed method outperforms the previous approaches in all three metrics. The paper also shows that the generation procedure is more robust to the number of visual hints and the amount of side information, and that the predicted visual regions are more likely to be in the predicted salient visual region. The quality of predicted visual hints is also improved."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"Label noise, class imbalance, and class imbalance are common problems in real-world datasets. The authors propose Generalized Data Weighting (GDW) to address the problem of class imbalance by using gradients at the class level. GDW uses the gradients of the class-level to compute the loss gradient for each class, and then applies a chain rule to learn the weights of each class.   The authors show that GDW outperforms existing instance weighting methods in terms of computational cost and performance on clean and unbiased data. They also show that the gradient descent step of GDW can be used to learn class-levels of the weights, and GDW is shown to be more efficient than existing methods in the sense that it does not require any additional training data.  In addition, GDW also shows that in the uniform noise setting of CIFAR10, in which GDW performs better than state-of-the-art methods, the authors also demonstrate that the proposed method is more robust to class imbalance and label noise.  The main contribution of the paper is that the authors propose to use class- level information to learn weights at the level of the classes, which is a natural way to incorporate class imbalance into existing methods. The paper also proposes to use a gradient-descent-based method to learn a set of intermediate gradients for the weights at each layer. "
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper proposes a new spatio-temporal language grounding task. Language grounding is the task of learning a language from sensorimotor modalities that can be used to guide embodied agents to learn a grounded language. The paper proposes to use language to guide an embodied agent to learn spatiotemporal descriptions of behavioral traces of the embodied agent. The descriptions are composed of time-extended predicates and spatio - temporal references. The authors propose two models, multimodal Transformer architectures, for this task. The architectural biases of the two models are that the former uses attention computations, while the latter uses the truth function. The generalization performance of the models is evaluated on generalization on randomly held-out sentences, and generalization using grammar primitives. The results show that Transformers with attention computation that depends on object identity are able to generalize better than models that do not rely on this information.    The paper is well-written, well-motivated, and well-structured. The idea of language-guided autonomous embodied agents is interesting, and the paper is clearly written. However, there are a few issues in the paper:   1. It is not clear to me that the paper addresses the problem of generalization in generalization.  2. There is a lack of code and pretrained models.  3. It does not seem to be clear that there is any connection between the code and the generalization results.  4. It would be good to see a more detailed analysis of the generalizability of the proposed generalization properties of the model. "
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper proposes a Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation based on rich spatio-temporal information, which is useful for detecting, tracking, and tracking with a single frame predictions. Previous approaches to the association problem are based on the temporal dimension of the objects in the space-time memory, while PCAN uses cross-attention to capture the rich information. PCAN also uses a prototypical appearance module to learn contrastive foreground and background prototypes, which are then used to learn the segmentation mask from the single frame to the next frame. Experiments on Youtube-VIS and BDD100K datasets show that PCAN outperforms the video instance tracking & segmentation competition winners on the Youtube-VIS, Youtube-BDD100K, and Youtube-DDD100k datasets. "
SP:1175ad16382b349ab1a39895150172d266abe571,"Gradient flow is a well-studied optimization technique for deep learning, and it is well-known that it can be used to solve the initial value problem of gradient flow. However, it is not well known that gradient descent can provide an approximate numerical solution to the initial values of the gradient flow trajectory with respect to the curvature of the input. This paper studies gradient flow trajectories with favorable curvature and shows that they can be approximated by gradient descent with homogeneous activations of deep neural networks. In particular, the authors show that gradient flow is approximated with deep linear neural networks, and they show that the gradient descent converges to a global minimum with a small step size. They also show that under certain conditions, gradient descent is equivalent to gradient flow with a random initialization, and that the step size of gradient descent in the case of deep networks can be much smaller than gradient descent for gradient flow in the presence of homogeneous activation functions.    The paper is well written and well-motivated. Gradient flow can be seen as a useful tool for improving the computational efficiency of gradient flows, and the authors have made a number of contributions. "
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper studies the problem of multi-armed bandits in which the delayed and long-term impact of actions are considered in the context of the multi-arm bandit setting. In this setting, there are multiple arms and each arm has a different delayed impact on the reward, and the goal is to minimize the gap between the delayed impact of the previous arm and the current arm rewards. The authors propose two techniques to address this problem. The first is to use action history to guide the learning, which is based on the fact that the delayed impacts of historical actions can be considered as a feedback loop. The second is to learn a matching regret lower bound, which matches the matching lower bound in the bandit literature. The regret of the proposed algorithm is shown to be $O(\sqrt{T})$-approx.   The authors also provide a theoretical analysis of the regret of their algorithm, which shows that their algorithm achieves a regret that matches the lower bound of the fair algorithms in the literature. "
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"This paper proposes Inter-frame Communication Transformers (IFC), an end-to-end solution for video instance segmentation (VIS) using transformers. The key idea is to use a per-clip pipeline instead of per-frame methods, which avoids the overhead of each frame and allows for concise memory tokens to be stored. The authors show that the per-clip models are able to handle the issue of frame to frame communications, and that the proposed method is able to perform near-online inference on standard benchmark sets, and offline inference on the YouTube-VIS 2019 val set. The paper also shows that the features learned by IFC are more robust to changes in the number of frames and the amount of data."
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"Graph embedding is a vector-space representation that is widely used in graph analysis and machine learning applications. Graph embedding can be used to represent any graph, but the structural properties of graphs are often not well-studied. For example, the sampling of context nodes in existing graph embedding methods is based on the biased sampler, which is called random walks. In this paper, the authors show that random walks (random walks with a certain degree) can be seen as a way to debiasing the structural biases in graphs. The authors propose a new graph embeddings method, called residual2vec, that debiases the random walks’ bias in the graph representation learning. They show that the debiased random walks have a degree that depends on the degree of the node. They also show that a variant of the proposed residual 2vec method, which uses random graphs, can be learned from random walks as well. Experiments on link prediction and clustering are conducted to demonstrate that the de-biasing of structural properties in a graph is beneficial.   "
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy. The authors consider the setting where the discrete distribution is non-interactive and the target distribution is a multinomial model. In this setting, the authors consider privacy mechanisms (PM) and show that the power sum functional can be estimated with quadratic risk, which is a non-trivial result in the non-intrusive case. In the sequentially interactive case, they show that plug-in type estimators are asymptotically optimal, while MLE is optimal under a two-step procedure. They also show that under a certain privacy constraint, they can estimate the power of the power under the multinomials in the case of a Gaussian model. Finally, they provide a connection between the α-LDP mechanisms and the proposed estimators with private samples. "
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper considers the problem of online multiclass classification, where the learner’s feedback is given as a directed graph. Two applications of feedback graphs are considered: filtering and label efficient classification. The authors propose GAPPLETRON, which is an efficient and scalable algorithm based on arbitrary feedback graphs, and provide surrogate regret bounds for the algorithm. The main idea is to use bandit feedback as a surrogate loss, and then use the surrogate losses as a regularizer for the prediction space. In particular, the authors consider a graph-theoretic parameter called the ""domination number"", which is a measure of the dominance of a node in a prediction space over a given time horizon. They show that under certain assumptions, GAPPLON achieves a surrogate regret of $O(\sqrt{T})$ for the full information case, and the bounds are tighter than the lower bound. The algorithm is evaluated on synthetic data, and compared with several baselines on different feedback graphs. The results show that the proposed algorithm performs better than the baselines. "
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of k-clustering, i.e., the problem where a threshold cut is applied to a single dimension (feature) of a cluster, and the goal is to find a cluster that minimizes the k-means objective. The authors propose an algorithm for explainable clustering based on the idea of `p-norms', which is a generalization of the previous work [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21].    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29]   The main contribution of this paper is to derive upper and lower bounds for k-medians. The upper bounds are O(k) and O(K), where k is the number of clusters in the cluster. The lower bound is O(O(k^k) for the case where the cluster size is small.   In particular, the authors show that the upper bounds depend on the size of the cluster and on the dimension of the data, and that the lower bound depends on the choice of the threshold cut. They also show that it is equivalent to a decision tree that is constructed from the decision tree.  In addition, they also provide a Ω(log k) lower bound on k-mean, and show that this is a lower bound of the original upper bound.  The authors also provide an upper bound on the number p of clusters that can be found by the algorithm, which depends on k.  Finally, they provide an algorithm that achieves the same upper bounds as [1]. "
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper proposes a multilingual pre-trained language model (PrLM) for downstream natural language processing tasks. The multilingual PrLM leverages language universality to leverage limited resources for low-resource languages. The authors show that multilingual prLMs can be trained on plain text with monolingual linguistic structure knowledge. Syntax has a universal dependency parse, and the authors propose to use explicit universal dependency parsing and implicit language modeling with multilingual PRLM. The model is trained with a learned representation, and then the model is evaluated on two linguistic structure parsing datasets. The proposed model outperforms multilingual-BERT, a previous model, in terms of universal linguistic structure clues. The paper also shows that the proposed approach is more interpretable than multilingual BERT.  "
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper proposes a deep architecture, called Dual-Aspect Collaborative Transformer (DACT), for solving vehicle routing problems (VRPs) using a Transformer. The authors propose a positional encoding (PE) method for representing VRP solutions, and use it to train learning improvement models for VRP. They also propose a Dual-aspect Collaboration Transformer to learn embeddings for node and positional features. The Transformer is able to preserve the symmetry of VRP problems (e.g. cyclic sequences) by using the cyclic positional encoder (CPE) method. They further propose a curriculum learning strategy to improve sample efficiency. DACT is evaluated on the traveling salesman problem (TSP) and the capacitated vehicle routing problem (CVRP). They show that DACT outperforms other Transformer based improvement models on both synthetic and benchmark instances.    The paper is well-written and well-motivated, and the paper is easy to follow. However, there are a few issues with the paper:   1. The paper does not provide a clear description of the proposed DACT.  2. It does not explain how DACT works.  3. It is not clear whether DACT can be used in conjunction with Proximal Policy Optimization (PPO).   4. There is a lack of experiments. "
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"This paper considers the data-driven classification problem, where the goal is to minimize the classification error of a model trained on a given data distribution. The authors consider generative models based on normalizing flows. They show that the Bayes error of generative model trained with normalizing flow can be approximated by an invertible transformation, and that it can be used to approximate Gaussian base distributions. They also show that this technique can be applied to any class of flow models, and show that it leads to a lower Bayes ratio for flow models trained with Holmes-Diaconis-Ross integration. The paper also shows that this approach can be extended to classification models trained on synthetic datasets that have a smaller Bayes rate than standard benchmark datasets.    The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from an intractable quantity, which makes it difficult to follow. It is not clear to me that the proposed method is particularly applicable to benchmark datasets, and it is unclear to me whether the proposed technique is applicable to real-world data. "
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper proposes an automated and architecture agnostic method, called GradInit, for initializing neural networks with different neural architectures for language modeling and computer vision. The authors argue that existing architecture-specific initialization schemes for different architectures can be problematic due to hyper-parameter choices and training instability. To address this issue, the authors propose Grad Init, a heuristic that can be applied to any convolutional architectures with skip connections. Grad Init is based on the observation that hyperparameters of different architectures tend to be highly correlated in terms of the norm of the network layer, which is a scalar multiplier variable.  The authors propose a numerical scheme to compute these variables, and show that GradInit can be used to compute the norm for any convolutionsal architectures, and that it can be combined with SGD, Adam, and SGD for learning rate warmup. It is also applied to the Transformer architecture for machine translation, and it is shown that it performs better than SGD and Adam for learning rates and momentum coefficients.   The main contribution of the paper is that the authors provide a theoretical analysis of architecture-agnostic initialization schemes, and the authors show that the hyperparameter of different neural networks is highly correlated with the number of normalization layers and the norm. "
SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed-effect models for disease progression are an important problem in longitudinal data, where the goal is to learn interpretable parameters for subject trajectories.  This paper proposes a new approach, called ADNI, to learn such interpretable models. The proposed metric is based on a diffeomorphism in the reproducible kernel Hilbert space of the Riemannian manifold, which is used to define a Euclidean metric. This approach is motivated by the observation that patient-specific trajectories tend to diverge on a central geodesic, which can be interpreted as a measure of interpretability. The authors propose to use radial basis functions in a reproducing way on the reproducing part of the reproducibly kernel space, which allows for a more interpretable surrogate of a patient's progression profiles.  The authors also propose a metric update for the forecasting of imaging and clinical biomarkers.  Experiments on the TADPOLE challenge show that the proposed methods outperform the state-of-the-art methods.    Contributions:  1. The paper proposes an interpretable model for longitudinal data.  2. The idea of Neural Information Processing Systems (NIPS) is interesting."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a routing-by-memory mechanism for training CNN architectures.   Convolutional Neural Networks (CNNs) are typically trained using parallel procedures, and the authors propose parallel Procedural Units (PUs). PU is a type of multi-head architecture that consists of a memory head, a procedure head, and a procedure sequence. The procedure sequence consists of two steps: (1) the memory head computes a sequence of specialized procedures, (2) the procedure sequence computes intermediate features, (3) and (4) the procedures are combined to produce a set of features. Networks trained with this mechanism are trained using a four-step training strategy.  The method is evaluated on VGGNet, ResNet, and EfficientNet’s accuracies on Tiny ImageNet, ImageNet and CIFAR-100 benchmarks. It is shown that the proposed method is able to improve the performance of the network. "
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper studies the relationship between neural networks with fundamental symmetries and coordinate freedoms of physical law and neural networks trained with irreducible representations. In particular, the authors consider the case of high-order tensor objects that are invariant to symmetry-enforcing constraints. The authors show that such frameworks can be seen as a special case of existing frameworks that can be applied to any set of polynomial functions.   The authors also show that classical physics can be viewed as an extension of these frameworks.  They show that, under certain conditions, physical laws can be expressed as a function of the fundamental symmetry of a set of scalars (e.g., scalar products, scalar contractions, etc.) and that these scalars can be represented as polynomials.  The paper also shows that, for translation, rotation, reflection (parity), and rotation with permutations, the symmetry of the set of permutations can be approximated by a scalar-based method.  Finally, the paper shows that Euclidean, Lorentz, and Poincaré groups can be considered as special cases of the theory. "
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper studies the problem of Bounding box (bbox) regression, an important problem in computer vision. The authors propose two new loss functions for bbox regression, the Intersection over Union (IoU) loss and the power IoU loss. The IoUbased losses are a generalization of IoU based losses.   The authors show that the power  IoU losses have the following properties:  1) order preservingness, 2) loss/gradient reweighting, and 3) a power regularization term that depends on the power parameter α.  The paper also shows that the proposed losses, called α-IiuU losses, are more efficient than the existing IoU-based losses in terms of performance margin.  Experiments are conducted on several object detection benchmarks and models, and on small datasets and noisy bboxes. The results show that for detectors with a certain level of noise, the proposed α-IIOU losses achieve better performance margin compared to the existing state-of-the-art performance margin of the previous state of the art."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper considers the imitation learning problem of learning a policy in a Markov Decision Process (MDP) setting in which the reward function is a function of the demonstrated behaviors. In this task, the authors propose Distributionally Robust Imitation Learning (DROIL), where the goal is to learn a policy that is robust to noisy demonstrations. The problem is formulated as imitation learning in which a policy is learned using an adversarial construction of the learned policy. DROIL is a generalization of DROIL and Maximum Entropy Inverse Reinforcement Learning. The authors propose a framework for learning a generalized concept of entropy for DROIL. They also propose an approach to learn an objective function that is a convex optimization problem over a polynomial number of variables over the state and action spaces of the loss functions. The proposed approach is applied to both stationary and non-stationary policies. The optimization method is evaluated on synthetic data and a highway driving environment, and the authors show that their optimization method outperforms DROIL on both synthetic data, and outperforms it when it is used as an inner reinforcement learning problem. The paper also shows that the proposed approach can be used to learn policies that are robust to noise in noisy demonstrations, and that it can learn optimistic generalizations. "
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper proposes a new approach to algorithmic fairness in ML systems. Post-processing is an approach that aims to ensure that postprocessing does not bias the model in a way that is harmful to individual fairness. In particular, it does not rely on retraining for individual fairness, but rather on the fact that the objective function is a weighted sum of a set of fairness constraints over a similarity graph. The authors propose two post-processing algorithms for individual fair (IF) that are based on the idea of individual fairness (IF). The IF post-processing problem is formulated as a graph smoothing problem, which is solved by a graph Laplacian regularization. Experiments are conducted to show that the proposed post- processing algorithms are able to mitigate individual biases in large-scale NLP models (e.g., BERT). The authors also show that post-postprocessing algorithms improve accuracy in terms of accuracy on a variety of datasets. "
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper tackles the Text-to-SQL task for SQL queries. The authors propose a unified encoding model for both natural language question and database schema, where the model is able to link two database schemas in a single model. The encoding method is based on SADGA, which uses the graph structure of the question-graph and the schema-graph as input. The paper proposes a structure-aware aggregation method based on the unified modeling, which combines Global Graph Linking, Local Graph Linked, and DualGraph Aggregation Mechanism. The proposal is evaluated on the well-known standard Text to-SQL benchmark Spider, and is shown to outperform the state-of-the-art on the cross-domains and is competitive with the best performing question-schema linking method. "
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper proposes a new approach to learning discrete-continuous computation graphs. The authors propose to use discrete and continuous model components to train models for supervised and reinforcement learning. The discrete component is used to model the graph’s execution paths, while the continuous component is applied to the computation graphs, which are then used to train neural networks with discrete probability distributions.  The authors show that the stochastic computations graphs can be decomposed into sequential discrete components, which can be used to learn models with small gradients and local minima.  They also show that neural networks trained with the same number of discrete components are able to learn with the standard set of discrete softmax tricks, and that the learning behavior can be controlled by adjusting the scale parameter of the Gumbel noise perturbations.  Finally, the authors propose two strategies to improve the performance of complex discrete-stochastic models compared to their continuous counterparts on several benchmark datasets.    The main contribution of the paper is that the authors introduce the use of dropout residual connections for learning stochedastic, discrete, and continuous computation graphs and show that their approach can be applied to a variety of existing approaches to learning such models. "
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0," of Bayesian neural networks (BNNs) trained with Hamiltonian Hamiltonian Monte Carlo (HMC) is shown to be robust to covariate shift in out-of-distribution data. Approximate Bayesian inference is used to train neural networks with linear dependencies on features, which is more robust than classical estimation. The authors also show that high-fidelity approximate inference can be achieved with full-batch Hamiltonian Monocoal (HBM) using a variant of classical classical estimation, and that the Bayesian model average is robust to the covariate shifted due to the posterior contraction. Finally, the authors show that approximate inference procedures and maximum a-posteri (MAP) training can be combined to improve the robustness of BNNs."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper considers two settings in meta-learning evaluation: in-distribution [ID] and out-of-distributions [OOD]. In the ID setting, the task distribution for both train and test tasks is the same. In the OOD setting, there is a large gap between the training and test distributions. The authors propose two methods to mitigate this issue. First, they use metalearning theory and FSL applications to train a model on the training data and then they use them for task generation. Second, they evaluate OOD evaluation on standard few-shot classification benchmarks.    The authors show that the ID vs. OOD performance of the proposed methods is comparable to that of the standard methods. They also show that meta-learners on OOD datasets are able to outperform the standard ones. Finally, they show that there is no significant difference between the performance of ID and OOD methods on standard benchmarks. The paper also shows that the proposed method can be used for model selection in a meta learning method.  The paper concludes with an analysis of the relationship between the ID evaluation and OLD evaluation on FSL benchmarks."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper studies the problem of learning rules from rules in a knowledge base (KB) using language model (LM)-based rule generation.    Rules are learned from a set of rules that have been annotated in the knowledge base.  The authors show that existing KB-based rule induction and LM-based rules generation can be learned from data commonalities.  They also show that these methods can be used to learn “canned” rules, i.e. rules that are not automatically inducted by inference systems, but are learned by rule induction systems.  Finally, they show that open rules can be generated by LMs with rich expressive power, and that LMs can be trained to generate free text.  In addition, the authors propose an open rule induction problem to learn open rules using LMs, and show that the Orion (open rule induction) system is able to generate open rules in this setting.  For relation extraction, open rules are also learned using open rules.  Experiments are conducted on a number of datasets, showing that automatically inducting rules is more powerful than manually annotated rules. The authors also find that the supervision of annotated rule is more important than the learning of new rules, as they are able to learn rules that do not need to be annotated.  Overall, the paper is well-written and well-motivated.  However, there are a few issues with the paper:   1. The paper does not provide sufficient discussion of the connection between the open rules and the rule induction system.  2. There is a lack of comparison with existing work on the topic.  3. It is not clear to me that there is a clear connection between open rules for relation extraction and automatically learned rules. 4. It would be better if the authors provide more discussion about the limitations of existing LMs for learning open rules, and how they can be improved."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,"This paper proposes a new offline RL algorithm called Implicit Constraint Q-learning (ICQ) for multi-agent reinforcement learning (MRL). ICQ is based on the observation that the extrapolation error of RL algorithms in offline multiagent RL is a function of the number of agents and the state and action space of the agents. The authors propose to use this observation as an implicit constraint to learn a joint-policy, which is then used to learn the value estimation of the state-action pairs for value estimation. The proposed algorithm is shown to outperform the single-agent counterpart and outperform offline RL algorithms for a number of real-world scenarios. "
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper studies the problem of training machine learning models for security related applications against real-world adversaries in the presence of uniform norm-bounded perturbations to neural network based detectors. In particular, the authors propose to use adversarial examples (AEs) that are imperceptible to uniform perturbation, but can be detected by the empirical data distribution. AEs can be classified in three different domains: malware, finance, and social networks. The authors show that the imperceptibility of AEs in these domains is due to semantically meaningful dependencies in the features of the features. They also show that non-uniform adversarial training can be performed in the absence of these features for these applications. The paper also shows that the robustness certification performance of the proposed approach can be improved by using non-utility for certification. Experiments are conducted on malware classification, credit risk prediction, and spam detection. "
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper studies spectral filtering in learning with kernels to improve the statistical properties of spectral filtering. The authors show that existing regularization schemes for least squares have faster convergence rates compared to Tikhonov regularization, and that the excess risk is also reduced. They also show that under certain source and capacity conditions, regularization strategies can be used to achieve faster convergence of the excess loss.    The authors propose two new estimators based on loss functions, the generalized self concordant loss functions (GSC) and the logistic loss, which are based on the results of Tikhonsky et al. (2017). They also propose an iterated version of the proximal point method for optimization. They show that the iterated Tikhonis regularization scheme is able to achieve fast and optimal rates for the GSC, and they also provide a theoretical analysis of the convergence rate of the learned estimators.  Finally, the authors provide some numerical experiments on the learning task. "
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper proposes a new linear transform, called Deformable butterfly (Deformable Butterfly), which is a linear transform over the input-output dimensions of butterfly matrices. It can be used to compress neural networks. The authors show that the sparsity of the DeBut layer can be leveraged for network compression. They also show that DeBut is more efficient than fully connected and convolutional layers in terms of the natural complexity-accuracy tradeoff between the light weight and the inference complexity. The paper also shows that a neural network can be trained with DeBut, and that it can also be trained to achieve better accuracy. "
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper addresses the problem of Catastrophic Forgetting (CF) in artificial neural networks. The authors propose a method called MetA Reusable Knowledge (MARK) to address the issue of forgetting of old information. The proposed method is based on the idea of weight reusability, i.e. the reusing of weights in a network that have been trained on the same task. To achieve this, Mark uses shared weights from a common Knowledge Base (KB) and trainable masks. A metalearning approach to the KB is used to find the KB relevant weights, and the model is trained to re-use these weights. Mark is evaluated on the 20-Split-MiniImageNet dataset, and is shown to outperform existing methods in terms of average accuracy, forgetting, and overwriting. The paper also shows that Mark can be applied to other benchmarks, and outperforms the state-of-the-art methods.  The authors also show that Mark is able to learn reusable knowledge that can be re-used across different tasks, and that it is more robust to forgetfulness."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper studies the problem of finding exact solvers for Mixed Integer Programming (MIP) using primal heuristics. In real-world applications, there is a large gap between the amount of work required to train primal solvers and the number of samples required to solve the problem. This paper proposes a data-driven framework to learn the exact solver for MIP using a set of hard-coded rules. The solver consists of two parts: (1) a solver that combines the MIP heuristic with the primal heuristic, and (2) a new problem-specific schedule for the primal solver. The main contribution of the paper is that the solver is built on top of the hard-coding rules for solving the problem, and that the rules for the problem are used to train the heuristic and the solvers. The paper also proposes a new algorithm for learning the new schedule. The algorithm is based on the idea that the optimal solution to a learning task should be the one that minimizes the average primal integral of the solution to the new learning task. The authors show that the exact MIP solver can be learned using the proposed algorithm, and they show that their algorithm is able to learn a scheduling heuristic that maximizes the expected return of the new problem. They also show that this algorithm can be applied to any problem with a problem- specific schedule.    The paper is well-written and well-motivated, and the idea of learning a problem specific schedule is interesting. However, there are a few issues that prevent the paper from being a clear contribution to the field. The problem is not well-grounded, and there are also a few questions that need to be answered. "
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper considers the problem of reinforcement learning (RL) in the setting where there is binary feedback, and the goal is to learn a policy that maximally maximizes the expected return of the agent. The authors consider an RL practice where the agent is given a reward signal, and it is trained in a similar way to real-world applications in reinforcement learning such as self-driving cars and robotics. The algorithm is shown to have a sublinear regret when the reward signal is binary and the agent has access to trajectory labels from an unknown parametric model. The paper also shows that this can be extended to the case where the learning is done in an unsupervised way."
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"Graph neural networks have been widely used for representing graph-structured data, but they have not been applied to the problem of graph representation learning. In this paper, the authors propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT) that learns holistic graph-level edge representations. Specifically, the edges in a graph are represented as a hypergraph, where nodes in the hypergraph are connected to each other via connectivity. The edges in the graph are used for discrimination, and message-passing techniques are used to learn node representations based on the dual hypergraph construction. Experiments are conducted on three tasks: graph reconstruction and generation, graph classification tasks, and graph representation and generation on three graph datasets. The edge representations learned using hypergraphs are shown to outperform the existing graph embedding and graph pooling methods, and the proposed method is shown to be competitive with the state-of-the-art edge representation and pooling method on three different graph datasets, and is also shown to perform well on graph classification. The authors also show that the edge representation learned by the proposed edge representation learns on three existing graph datasets outperforms the state of the art in terms of lossless compression of the nodes and removal of irrelevant edges.   "
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the problem of learning representations of data using Mutual information (MI) maximization. The authors show that existing representations for reinforcement learning (RL) can be learned using MI objectives that are insensitive to irrelevant and redundant information. They also show that MI objectives for learning representations that are sensitive to irrelevant information can be used to learn representations for RL that are more robust to the structure of the MDP. Finally, they show that using samples of high-dimensional observations from MI is sufficient to learn an optimal policy that maximizes the mutual information between the state representation of the agent and the environment.    The paper is well-written, well-motivated, and well-structured. The paper provides a detailed analysis of existing MI based objectives. They show that insufficient representations are learned using these objectives, and that the optimal policy is learned by learning a state representation that is invariant to the irrelevant information.  They also provide experiments on a simulated game environment with visual observations, where the agent is given a set of visual observations and the goal is to learn a control policy that minimizes the optimal state representation. The results show that the proposed methods outperform existing methods. "
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper proposes a steerable convolution for 3D semantic analysis based on sparse tensors. The authors propose to use SE(3)-equivariant deep feature learning to steer the processing of 3D data. The proposed pipeline, called Sparse Steerable Convolution (SS-Conv), is a pipeline for precise estimation of object poses. The Feature-Steering module is built on top of previous work that leverages SE(2) and (3) to achieve pose refinement. Experiments are conducted on 3D object semantic analysis (instance-level 6D pose estimation, category-level pose and size estimation, and categorylevel 6d pose tracking). The authors show that the proposed pipeline outperforms existing methods on all three metrics for the three tasks. In addition, the authors also show that SS-conv outperforms other convolutions in terms of accuracy and efficiency.    The authors also provide a detailed analysis of the proposed designs. The code is well-written and easy to follow."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a dynamic token sparsification framework to remove redundant tokens from self-attention in vision transformers. The authors argue that redundant tokens are important for informative tokens for image recognition, but unstructured sparse tokens are also important. To this end, the authors propose a lightweight prediction module that generates an importance score based on the features of each token. This module is then incorporated into other layers to remove the redundant tokens. The proposed method is based on hierarchically pruning, and the authors also propose an attention masking strategy to make the prediction module more interpretable. Experiments on ImageNet show that the proposed method can reduce FLOPs and improve throughput while maintaining the accuracy of Vision transformers in terms of complexity/accuracy trade-offs. The DynamicViT models trained with the proposed DynamicTokenSparsification (DTSP) framework outperform standard CNNs and vision transformerers in ImageNet.    The authors also provide a theoretical analysis of the proposed framework. "
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper studies the problem of data analysis problems in which the data is drawn from a distribution with distributional assumptions. The authors propose three methods for obtaining distribution-free guarantees for predictive inference: holdout methods, cross-validation methods, and conformal prediction. They show that inference for a regression function (i.e., inference for the conditional mean) with inference for E [Y |X] and E [X] can be obtained with inference guarantees that are non-vanishing in terms of the number of samples and the sample size. They also show that the confidence interval of the confidence intervals has a non-disappearing width.    The main contribution of this paper is that the authors provide a theoretical analysis of the vanishing-width confidence intervals obtained by inference in the finite setting and in the continuous setting. In particular, the authors show that under certain assumptions on the distributional assumption, the inference guarantees obtained by the proposed methods (e.g., cross-Validation methods and cross-validation methods) can be extended to the case of predictive inference. "
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper studies the problem of learning debiased encoders for bias mitigation methods for DNN models. The authors propose a new mitigation technique called Representation Neutralization for Fairness (RNF), which aims to mitigate the impact of biased representations on fairness. Specifically, the authors propose to remove the fairness sensitive information from the encoder and use instance-level annotations to identify sensitive attributes from the ground-truth label and sensitive attributes for sensitive attribute annotations. They show that this discrimination improves the discrimination performance of DNNs with a single task-specific classification head. They also show that the neutralized representations of the classification head of a DNN model trained with RNF can be used to improve the performance of a classification head trained with a bias-amplified version of RNF. Finally, they demonstrate that the bias-implified model can also be used for proxy annotations for sensitive attributes using proxy annotations from the biased-im amplified model in low-resource settings. The proposed RNF framework is evaluated on several benchmark datasets and shows that the proposed method is able to achieve better discrimination performance for different tasks. "
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper proposes a new class of convolutional neural networks, called Bessel-CNNs, for medical imaging. They are based on Bessel functions, which are a generalization of convolutions. The authors show that the Bessel function is invariant to rotations and can be used as a regularizer for convolutions, which can be applied to any convolution layer of a Bessel network. They also show that Bessels are invariant in terms of rotation angles.   "
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper proposes a large-scale solver for kernel ridge regression, called ParK, which is a general approach that combines partitioning, random projections, iterative optimization, and partitioning with space and time complexity. The authors show that the proposed approach can achieve state-of-the-art statistical accuracy while maintaining a high statistical accuracy in terms of both local effective dimension and bias. The paper also shows that the orthogonality of the local estimators is preserved, and that the feature space of the partitions is orthogonal to the input space. The proposed model achieves a statistical-computational tradeoff between the number of partitions and the number (number of iterations) of iterations. The method is tested on several large scale datasets, and is shown to perform well."
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper studies the problem of learning discrete tokens for Neural agents in reinforcement learning settings. Neural agents typically learn discrete tokens as discrete tokens in the form of one-hot vectors, which are discrete communication tokens that are generated from human communication. The authors propose to use natural language processing to learn word embedding techniques that can be used to encode discrete tokens into neural agent architectures. They show that the discrete tokens can be encoded in a continuous space, which is a natural extension of existing techniques. They also show that this method can be applied to the case of unlabeled emergent agent communication, which can be seen as a form of communication that is useful for zero-shot understanding. They demonstrate that this technique is able to learn communication in a way that is more efficient than using one- hot tokens, and they show that their technique is also able to be used in a decision theoretic framework.   "
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper studies the generalization performance of Transformers in computer vision, and shows that they outperform standard convolutional networks in terms of model capacity, generalization, and efficiency. The authors propose two hybrid models, called CoAtNets, which are “coat” nets that combine depthwise Convolution and self-Attention, and show that they achieve better generalization and efficiency compared to standard ConvNets. They also show that the inductive bias of these architectures is that the convolution layers and the attention layers are related, and that the relative attention is used to train the hybrid models. The paper also shows that the depthwise convolution can be trained with relative attention, and the self-attention can also be trained using relative attention. Finally, the paper shows that CoNets with “coat” networks (i.e. “capes”) achieve better capacity, efficiency, and generalization compared to the state-of-the-art, and it achieves top-1 accuracy on ImageNet-21K, and achieves a similar level of performance as CoAtNet with JFT-3B under resource constraints.   "
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper proposes a second-order oracle bound for the expected risk of a weighted majority vote in the presence of a large majority of voters. The bound is based on a parametric form of the ChebyshevCantelli inequality (one-sided Chebysherv’s) which is a generalization of the prior oracle bounds. The authors show that this form can be used to solve the optimization challenge of prior prior work, and that the prior work can be seen as an optimization challenge to prior work on the prior.    The prior work builds upon prior work that uses the previous work on prior work (Chen et al. 2017) to derive prior bounds for the prior on the concentration of measure inequality (PAC-Bayes-Bennett and PAC-Bernstein).   In this work, the authors extend the prior to the case of minimization and show that it is equivalent to the prior for the oracle upper bound of the second order Markov's inequality.  The authors also show that prior work also uses the prior and prior bounds in this setting.  In particular, the prior works in this paper use the prior Chebyshes inequality and prior C-bounds to obtain prior bounds that are equivalent to prior results in the case where the prior is non-asymptotic and non-convex.  Finally, the paper shows that it can be combined with prior work in the form of a PAC-Bayesian bounding and the prior, which combines the prior PAC-BENNETT inequality with the prior (and prior work).  The paper also shows that the proposed method can be applied to the empirical estimation of the oracles and that it converges to an empirical bound based on the previous bound.  This paper is well motivated and well-motivated, and the paper is clearly written. The paper is also well-written and well motivated. "
SP:5bac542a6532d43cf100e085398b4a4783719814,This paper tackles the audio-visual video parsing task for audio or visual event categories. The authors propose a new method that uses common and diverse event semantics to distinguish between audio and visual events. The idea is to use cross-modality co-occurrence as a metric to measure the amount of supervisory signals that are sent to the parsing model. The proposed method is evaluated on weakly-supervised audio-video video parsing and shows that the proposed method outperforms existing methods in the task. The paper also provides video-level annotations.
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper studies the problem of federated learning (FL) to compress a global model in the presence of heterogeneous data. The authors propose a quantized and personalized FL algorithm for collective (personalized model compression) training based on knowledge distillation (KD) and show that the proposed algorithm is able to compress both quantized models and personalized models with different quantization parameters and model dimensions/structures. The paper also shows that the compressed personalization problem can be decomposed into an alternating proximal gradient update, where the algorithm compresses the (federated) learning process and compresses personalization. The proposed algorithm, QuPeD, uses a relaxed optimization problem to compute the quantization values for each client, and then computes the local client objectives based on the knowledgedistillation loss on the global model. The compressed model dimension of the compressed model is then used as a metric to measure the model dimension for the local clients, and the compression loss is used to optimize the knowledge distilation loss for the compression of the global client objectives. Experiments are conducted to show the effectiveness of the proposed compressed personalisation framework in the context of the existing personalized FL algorithms. The results are shown to outperform the state-of-the-art personalized FL methods (FedAvg and FedAvg with local training of clients) as well as the state of the art personalization methods (QuPeD).   "
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"Constrained clustering is an important problem in machine learning, and it is often used to leverage prior information from partially labeled data. This paper proposes a new framework for constrained clustering based on deep generative models. The framework is based on stochastic gradient variational inference, and the model (DC-GMM) uses domain knowledge from probabilistic relations between the prior clustering preferences and the constraints. The authors show that the proposed DC-GMM outperforms state-of-the-art deep constrained clusters and robustness to perturbations in the clustering process. The proposed approach is evaluated on several real-world applications, and DC-GM is shown to be more robust to changes in the pairwise constraints. "
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper studies the Neural Tangent Kernel (NTK) for infinitely-wide neural networks trained with gradient descent on the least squares loss. The authors propose a convolutional counterpart of NTK (CNTK), which has a linear runtime of $O(1/\sqrt{n})$. The authors show that the NTK regression outperforms finitely-widest neural networks on small-scale datasets. They also show that kernel methods for large-scale learning tasks have the same computational complexity as NTK, but with a spectral approximation guarantee for the NTk matrix.  The authors also propose a near input-sparsity time approximation algorithm for NTK based on polynomial expansions of arc-cosine kernels, where the random features of the arc-sinine kernels are approximated by leverage score sampling, and a sketching algorithm is used to combine random features from different kernels. The proposed methods are evaluated on both large scale regression and classification tasks, and CNTK is shown to outperform NTK on the CIFAR-10 dataset. In addition, a linear regressor based on the linear function of the CNTk features is also shown to achieve speedup over the standard linear function.   "
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"This paper proposes a framework for multi-person 3D motion trajectory prediction. The proposed Multi-Range Transformers model consists of a local-range encoder for individual motion and a global-range decoder for social interactions. The human pose trajectory is used as input, and the prediction is made using a Transformer decoder. The model is evaluated on long-term 3D object detection, and is shown to outperform existing methods in terms of both the quality of the human pose and of the individual motion prediction. "
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a new approach for learning a program-guided RL agent that can be used to solve long-horizon planning problems. The proposed approach is based on the idea of predictive program synthesis (MPPS), which is a generative model that learns to generate programs that can guide the agent to solve a programming task. The paper shows that the proposed approach outperforms non-program-guided approaches on a number of benchmarks, including a 2D Minecraft-inspired environment. "
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper studies the problem of causal imitation learning in sequential settings. Imitation learning is an important problem in the context of naïve imitation, where the goal is to learn to imitate a demonstrator’s behavior (DO) given a set of sensors. In this setting, the imitator is trained in a single-stage decision-making. The authors propose a graphical criterion for causal imitation, which they call “imitability”, that is, the ability of the demonstrator to imitate the behavior of an imitators. They also propose an algorithm for learning the imitability. The theory is well-motivated and the theory is clearly stated. "
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,This paper proposes to use transition losses to learn an object-structured representation instead of pixels. The proposed model is based on two transition losses: object persistence and object identity. The model is trained with an alignment module. The transition models are two types of transition models: the objectlevel loss and the object alignment. The paper also proposes a slot-wise object memory. The experiments show that the proposed model outperforms a baseline and it is able to learn object occlusion and re-appearance in partially observable environments. 
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"This paper studies the problem of empirical risk minimization (ERM) in machine learning, specifically classification, regression, and off-policy policy learning. The authors propose a generic importance sampling weighted ERM algorithm based on adaptively collected data, and provide modelagnostic guarantees. They also propose a contextual bandit algorithm that achieves fast convergence rates. The main contribution of the paper is that the authors prove fast rates for classification based on the convexity of squared-error loss, and for regression based on maximal inequality of the rates with respect to the exploration rate.  The authors also provide regret guarantees for policy learning based on their theory.    The main idea is to learn a hypothesis class from bandit-constrained data that is invariant to the importance sampling structure, and then use this hypothesis class for exploration. "
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper proposes a reweighting strategy for kernel-reweighted regression, where the goal is to improve the predictive power of models that are re-weighted in the presence of covariate perturbations. The authors propose a mitigation strategy to tackle the problems of low sample sizes, covariate adversarial perturbation, and covariate noise in the weighting of a weighted regression model for machine learning tasks. They propose a doubly non-negative matrix to reweight the sample weights, and use the log-determinant divergence and the Bures-Wasserstein distance to reweigh the weighted matrix on the uncertainty set. They also propose an adversarially reweighted estimate based on first-order methods. "
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper studies the problem of unbiased gradient estimators for discrete latent variables in models. In particular, the authors consider the categorical setting, where it is assumed that the model is trained on binary random variables. The authors propose a continuous relaxation for low-variance reparameterization gradients, and show that it is a performant estimator. They also show that this estimator can be improved by importance sampling and statistical couplings.   The authors show that continuous relaxations for categorical variables can be used to improve the performance of categorical gradient estimations. They show that, for a set of sequences of binary variables and Rao-Blackwellization, the gradient estimator of a pair of pairs of pairs can be approximated by reparameters of the two sets of variables.  They also demonstrate that their estimators outperform REINFORCE, which uses a leave-one-out-baseline estimator, in terms of stick-breaking coupling. "
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a new approach to Neural Architecture Search (NAS) based on predictor-based NAS approaches. The idea is to train two predictors for top architectures, one for architecture-performance pairs, and one for the architecture space. The search path for the high-performance sub-space is based on the weaker predictors. The strong predictor is used to guide the search path to the best architecture space, while the proxy accuracy predictor is the best predictor for the well-performed architectures. The ranking of sampling space is done via coarse-to-fine iteration. Experiments on NAS-Bench-101 and NAS- Bench-201 show that the proposed WeakNAS outperforms SOTA on the ImageNet MobileNet Search Space. The paper also shows that the weak predictors can be used as a proxy for the top-performance architectures. "
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes Intrinsic ConTrol (EDDICT) based on Entropic Desired Dynamics to learn a globally consistent coordinate system with latent codes that are invariant to changes in the local objective. EDDICT’s globally consistent codes allow it to learn tractable learning and interpretable latent space. The paper shows that prior methods do not learn EDDCT’S globally consistent code, and that it can learn a fixed additive latent dynamics. Experiments on state coverage and unsupervised performance on hard exploration games (e.g. Montezuma's Revenge) are conducted.   "
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"This paper proposes a reinforcement learning (RL) for drug design using a reward scoring function, the molecular docking program, which is a physical simulation (i.e., molecular docking) of a molecule’s binding affinity to a protein-small molecule binding affinity in a molecular structure. The authors propose two models for designing chemically realistic and pharmacochemically acceptable molecules, and show that their models are able to generate molecules that are both biologically plausible and biologically reproducible. They also show that the RL framework can be used to design molecules that have good docking scores, and that the exploration problem (docking score optimization) can be reduced to a local optima and smooth surfaces. Fragment-based generative RL based on Explorative Experience replay for Drug design (FREED) is proposed, which combines a fragment-based generation method with error-prioritized experience replay (PER). The proposed model is evaluated on both de novo and scaffold-based schemes, and the proposed method, predictive error-PER (PE), is shown to outperform existing methods. "
SP:b938bca513e7de1231212064caf8877a78d8b612,"This paper studies the complexity of directed acyclic graphical models on observational data. The approach is based on a local Markov boundary search procedure to recover ancestral sets of the graphical model. The forward greedy search algorithm is used to recover the ancestral sets, and the backward pruning phase of the proposed approach is applied to graph ensembles. The authors provide finite-sample guarantees for recovering Markov boundaries under distributional assumptions. They also provide results for recovering polytrees in polynomial time. The sample complexity of the algorithm is shown to be O(1/\sqrt{n}) for discrete or continuous distributions, and O(O(n^2) for continuous distributions.   The authors also provide an identifiability condition on the graph, and show that under minimal assumptions on the structure of directed graphical models, their approach can be applied to discrete/continuous distributions, where the nodes are connected to each other and the graph is connected to a set of nodes connected to other nodes. "
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper studies the problem of learning with differential privacy (DP) in the setting where there is no privacy protection. The authors propose a new DP algorithm for learning a privately learnable class, which they call ""-DP algorithms"". They prove a nearly-matching lower bound for the probabilistic representation dimension of the local model. They also prove that the global stability is guaranteed by the use of public randomness. Finally, they propose a correlated sampling strategy to improve the performance of the algorithm."
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"Estimating the per-state expected cumulative rewards in reinforcement learning approaches is an important problem. This paper proposes to use deep neural-network function-approximation methods to address this problem. The authors propose to use a latent Markov decision process to model the transition and reward models, and use value iteration networks to learn the implicit representations of value functions. They show that gradient descent converges to global optima for the linear parametrization of the value predictions. They also provide convergence rates for two cases, where the implicit representation of stochastic gradient descent (SGD) is more powerful than its explicit counterpart. "
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes a new approach to KG-based question answering. The authors propose a new KG refinement task, called IterefinE, which is based on the idea that knowledge graph embeddings (KGs) are useful for inferring new facts from text sources. The paper proposes two techniques to improve the quality of the KGs produced by the proposed techniques in the context of knowledge graph refinement. The first one uses the ontological information in the KG embedding and inferences rules, while the second one uses a combination of the two.   The paper shows that the proposed IterefineE framework is able to improve on the performance of existing methods on KG based question answering tasks.  The authors also show that the ItereformE framework can be applied to the task of question answering in a more general KG setting, where the ontologies are more general.  Experiments are conducted on a number of KG benchmarks, and the authors show that their approach outperforms existing methods in terms of the overall weighted F1 score. They also show the effectiveness of their approach on the PSL-KGI task, where they show that using the proposed approach, they are able to obtain better embedding performance than existing methods. "
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"Knowledge base completion (KBC) methods are used to evaluate the quality of a knowledge base (KB) based on a likelihood ranking of all entities in the knowledge base. This paper proposes a new evaluation paradigm for model selection criteria, which is based on binary predictions of the KBC quality. The proposed method, called FB14k-QAQ, is evaluated on a new benchmark for KB embeddings models, where real-world entities are included in the KB, and a ranking task is used as well as a completion task. The authors show that a model trained on the new benchmark performs better than a baseline model trained only on the original KB. They also show that models trained on this new benchmark are able to achieve better prediction separability compared to existing KB embedding models. They further show that the proposed method TransE, which adds thresholding to the classification F1 score of the model, is able to outperform the baseline model on the ranking setting.    The authors also provide a detailed analysis of the evaluation data structure, showing that the ranking-based and classification-based evaluation are not the same, and that there is a trade-off between the ranking performance of a model and the performance of the final model. The paper also shows that the performance on the benchmark is highly correlated with the number of KB queries, which suggests that the models are not well-suited for the task. "
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes an extension of the Alternating Roles Dialog Model (ARDM) framework to dialog system models for tasks that require human annotations. The authors propose a framework called Alternating roles Dialog Models (ARDMs) that uses a large pre-trained language model (e.g. BERT, GPT-2) to generate dialog response generation from human-annotated language priors for down-stream NLP tasks. ARDM is based on a large pretrained language model. It generates dialog responses from human annotations (belief states, dialog acts, etc.). It is able to generate conversations with human annotations, and can be used as supervision for downstream tasks. The paper shows that ARDM outperforms state-of-the-art methods on two task-oriented dialog datasets (CamRest676 and MultiWOZ). ARDM can also generate human-like responses for non-collaborative tasks (persuasion and persuasion)."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"This paper considers the problem of classification in a deep neural network, where the softmax values of the network are known. The authors propose a new uncertainty measure, the implied loss, which is a measure of the uncertainty of the classifier. They also propose two confidence measures for Top k, which are based on binning values. The proposed method is tested on two networks, and the results show that the proposed values are more accurate than existing methods."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the generalization performance of neural networks with different architecture and hyperparameters. The authors show that wide neural networks have a wide network limit, and that gradient descent on wide networks is more likely to converge to a large depth limit. They also show that the spectrum of the NNGP kernel (Neural Network Gaussian Process (NNGP) kernel), a kernel similar to the Neural Tangent Kernel (NTK) kernel, is the limit of the spectrum for gradient descent. They show that NTK has the same spectrum as the NGGP kernel, but the spectrum is much wider than that of the NTK.    The authors further show that random networks are not the only ones with a wide limit, as wide networks also have a large limit. This is true for other architectures such as Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs).   Finally, they show that CNNs have a similar learning dynamics as FCNs, and CNNs with average pooling are similar to FCNs in terms of pooling, but CNNs also have similar pooling in the hyperparameter space.  They also demonstrate that training accuracy on real datasets is improved when the width of the network is larger. "
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper proposes a graph-based method called GRAPHQA, which aims to learn protein models that are able to fold a protein into a 3D structure. Proteins have been shown to fold in biological processes, and computational methods for protein folding have been proposed to learn a protein’s structure. The authors argue that there are a number of favorable properties (geometric invariance, computational efficiency, and representation learning) that can be exploited to improve the performance of protein models learned by GRAPH QA. The paper shows that the state-of-the-art hand-engineered and representation-learning approaches outperform the current state of the art in terms of performance on both sequential and 3D structures.    The paper also shows that there is a trade-off between the quality of the learned representation and the performance, and the authors show that the performance can be improved by removing the need to learn the sequential component of the representation.  The authors also show how to remove the need for a sequential component from the representation learning, and how to decompose the representation into the sequential and the 3D part of the protein.  Finally, the authors demonstrate that the proposed method is more computationally efficient than existing methods. They also show that their method can be combined with GRAPHQLA components. "
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper studies the loss function of a neural network in a space where the geometry of the functional space and the parameterization of this space depends on the network’s weights. The authors show that pure critical points are more likely to exist than spurious critical points for linear neural networks with the same loss function. They also show that the landscape of linear networks is a determinantal variety of a functional space, i.e., the determinant of the function of the network in this space. They show that this space can be seen as the space in which the function is of bounded rank.    The authors further show that linear networks with different loss functions and different parameterizations share a similar loss landscape. In particular, they show that for smooth convex losses, the landscape is dominated by linear maps with bounded rank, and for linear maps of a certain kind (linear maps of the form linear maps over a finite set of architectures) the landscape becomes more and more complex as the number of layers increases.  They also find that for certain types of linear maps, there exist determinantals of the landscape that are more complex than the ones in the literature.  Finally, the authors provide a number of experiments that show that filling architectures with a quadratic loss are more stable than non-filling architectures, and that for a certain class of networks, there exists a set of networks that lie in this functional space that lie on pure critical point (or in a set that lies on a set on pure point).  "
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper considers the problem of inductive and unsupervised graph learning for predictive or information retrieval tasks. In particular, the authors focus on the setting where there is no label information and the learning processes are based on graph similarity evaluation. The authors propose a SEED framework to learn subgraphs with reconstruction error based loss functions, where the subgraph samples are sampled from a set of subgraph vectors, and the output vector representation of the graph is learned from the embedding of a subgraph vector distribution.  The authors show that the reconstruction errors of the subGraphs are related to the number of reconstruction errors in the original graph, which is a common problem in graph learning. They also show that SEED and graph isomorphism can be used as a way to connect graph structured objects to each other. Experiments are conducted on several public benchmark datasets and the authors demonstrate that the proposed SEED approach outperforms competitive baseline methods. "
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper studies counterfactual regret minimization (CFRM) methods for the problem of playing twoplayer zero-sum extensive games with imperfect information. The authors propose a new CFR algorithm called Lazy-FRM, which is a variant of vanilla CFR for large-scale games with a game tree. The main idea is to use a lazy update strategy to improve the performance of the CFR algorithm. The regret of Lazy -FRM is shown to be much lower than the regret of the vanilla CFR, and the regret is also lower than that of the original CFR. "
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,This paper proposes a new unsupervised domain adaptation (UDA) method based on the Distribution Matching Prototypical Network (DMPN) framework. DMPN is an extension of the previous work that uses a Gaussian mixture distribution to model the feature distribution discrepancy between the source and target distributions. The authors show that the proposed method is able to learn discriminative and domain-invariant features that are transferable across different domains. They also show that their method is more robust to hyperparameter changes than previous methods.
SP:40be996e8bb86e887077b762b87c7c34a786ac98,"This paper proposes a new class of deep generative models, called Continuous Normalizing Flows (CNFs), for tasks that require exact likelihood estimation. CNFs are commonly used for both conditional image generation and downstream predictive tasks. In this paper, the authors propose a conditional CNF, called InfoCNF, that combines class-specific supervised code and unsupervised code in the latent space. The proposed model learns a highdimensional latent code, which is then partitioned into two parts: (1) a set of classes, and (2) a subset of classes that share the same labeled information. The gating networks are used to learn error tolerances for ordinary differential equation (ODE) solvers, and the partitioning strategy is used to improve the performance of InfoC NF on time-series data. The authors show that the proposed method improves the test accuracy of the baseline by a large margin, and improves the likelihood scores on CIFAR10 with high NFEs. They also show that a partitioned strategy is also effective for extrapolation. "
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper considers the problem of learning nonlinear functions for the approximation of the value function in the context of the Temporal-Difference (TD) learning algorithm. The problem is well-motivated and well-studied. In this regime, the approximating function is nonlinear and the learning process is non-asymptotically linear. The authors show that under the lazy training regime of the algorithm, the algorithm converges to the optimal solution of the problem. They also show that non-lazy TD learning can be used to train models that are non-over-parametrized.    The main contribution of this paper is that the authors provide a theoretical analysis of the convergence of non-linear functions in the regime of lazy training. They show that the scaling of neural networks in this regime is exponential in the number of training samples, and that the model is able to converge to a solution that minimizes the difference between the true solution and the solution obtained by the nonlinear function.  The authors also provide empirical evidence that the non-Lazy learning is more effective than non-slackled TD learning in terms of convergence.  Finally, the authors show empirically that their results are consistent with the theoretical analysis and show that neural networks trained with non-discriminative learning outperform underand over-parameterized frameworks. "
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper proposes a reinforcement learning problem for hypothesis verification, where the agent is given a pre-condition, an action sequence, and a post-condition. The problem is formulated as an optimization problem, and agents are trained to solve this problem. The agent is trained to verify the hypothesis of the pre-conditions and the action sequence. The authors show that they are able to achieve good performance in this setting.  "
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper studies the use of neural networks for approximate reasoning in a fixed dimensional latent space. The authors consider the problem of rewriting a sequence of mathematical formulas, where the goal is to find a set of transformations that are compatible with the current state of the art in the field. In this setting, the authors propose to use graph neural networks to model the rewrite-success of statements, and show that the embeddings of the rewrite steps in the formula space and in the latent space are very similar, and that the semantic features in the vector space are similar to those in the embedding space of the original statement. The paper also shows that the rewrite rule can be interpreted as a rewrite rule, and the authors show that this reasoning can be applied to a number of mathematical disciplines in a corpus of mathematicalulas.  "
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"Natural intelligent agents are a class of artificial systems that are able to learn to solve equations of projective geometry (e.g. geometry of a scene) using visual and haptic feedback. However, existing methods require multi-view geometry, which can be challenging for natural agents. This paper proposes an approach to learn depth from images and sparse depth measurements. The key idea is to use a global-local network architecture to mitigate the inductive bias of natural agents and to learn a model for monocular dense depth estimation based on sparse ground truth. The global parameters of the network are used to model the metric agent motion, and the network is trained with sparse supervision. "
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper studies the problem of learning word pieces for machine learning tasks with opaque ids. The authors propose to use natural language models with word pieces as word filters to tackle this problem. The idea is to use a Bloom filter, which is a variant of the Bloom filter proposed in [1]. The Bloom filter is a special case of hashing, where hash functions are used to encode the hash tokens.  The authors show that models trained with a multi-layer Transformer on Bloom filter digests can achieve higher accuracy than models trained using a single layer Transformer. They also show that such models can be trained with sampled softmax within the same computational budget.  Finally, the authors demonstrate that this method can be applied to other problems with large vocabulary size.  "
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper tackles the problem of discovering 3D parts. The authors propose a learningbased agglomerative clustering framework based on a grouping policy for small part proposals. The prior is a prior over a set of small parts, and the part-level features are learned in a local context. The proposed method is evaluated on a largescale fine-grained 3D part dataset called PartNet. Compared to existing shape segmentation baselines, the proposed approach is able to learn the knowledge of parts in a more general way. The paper also proposes a contextual bandit problem, which is a generalization of existing data-driven Shape Segmentation approaches. "
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a new way of training generative neural networks that can edit input/output datasets so that they are more robust to distribution shifts (e.g. gender-related characteristics, hair color, etc.). The authors propose to train a generative adversarial network (GAN) to edit images of black-haired men trained on images of blond-haired men, and then train a discriminator to distinguish between the source distribution and the target distribution. The discriminator is trained in a similar way to the way that generative models are trained.   The key difference is that the discriminator does not need to know the source and target distributions of the input and output data, but instead it only needs to be able to distinguish the source from the target on the manifold. The authors show that this edit can be used to produce a transformation in the latent space, and that this transformation can be applied to complex and non-linear transformations.  The authors also show that an autoencoder can be trained on the transformed data generated by the editing transformation. The transformation is then applied to the latent trained space. The technique is applied to a variety of data domains, modalities, and applications in biology, including removal of batch artifacts (i.e. the removal of unwanted noise from the input) and drug treatments. The main contribution of the paper is that it is able to learn image transformations that are robust to different distribution shifts in the input space, which is an important property of neuron’s activations. The paper also shows that the neuron editing can be performed in a way that is more robust than a single edit."
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper proposes a meta-learning approach for few-shot learning, where the goal is to learn a model that generalizes well to unseen tasks. The authors propose to use a meta learning approach to learn the initializations of the model, and then use the learned initializations to improve the generalization performance of the meta-learner. They show that this approach improves the performance of meta-learners on a few shot classification and reinforcement learning tasks.  "
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes Prototypical Random Walk Networks(PRWN), an extension of the SS-FSL approach based on Prototypial Networks (PN) to few-shot learning with unlabeled data. The authors propose a novel SS-FSL approach that builds on the idea of prototypical random walk (PRW) in the context of graph-NNs. PRWN uses a random walk semi-supervised loss to learn representations for each class using a network that is trained on a collective test set of all classes in a transductive setting.  The authors show that the proposed model outperforms the state-of-the-art art in the 1-shot mini-Imagenet case, and it outperforms it in the 5-shot setting on Omniglot. They also show that their model is more robust to distractors than fully supervised prototypical networks.    In general, the paper is well-written and well-motivated. The idea of using graph-based approaches as a baseline for few-task learning is interesting. However, there are several issues with the paper:  1. The paper does not provide sufficient analysis of the impact of distractors on the performance of AI models.  2. The proposed model is not well-suited for the mini-imagenet setting, as the authors do not provide a discriminative power test for their baseline.  3. They do not compare their model to the state of the art in terms of robustness against distractors.  4. They show that they are able to learn compact and well separated class representations that are more robust in the sense that human intelligence is.  5. Finally, the authors provide an analysis of their model and show that it is more sensitive to the distractors (i.e., distractors that are not present in the training set).   The paper also shows that their proposed model performs better in the case where there is no labelled/unlabeled class distribution mismatch, which is a common problem in the real world. "
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a new self-supervised learning framework for multi-sensor, multi-channel remote sensing. The proposed method is based on the Contrastive Sensitive Fusion (CSF) framework, which is an extension of Contrastive Convolutional Neural Networks (CCNN) to the remote sensing domain. The authors show that CSF is able to learn representations that are more robust to label noise, which can be used to improve the performance of the model in remote sensing applications. "
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper studies the problem of fine-tuning neural network pruning algorithms. In particular, the authors propose a new retraining technique called ""weight rewinding"", which is an extension of the classic retaining technique known as ""fine-tunning"" (Zhang et al., 2017). Weight rewiring is a simple way to prune a network, where the weights are re-weighted according to the learning rate, and the training schedule is re-set to a pre-specified learning rate.    The authors show that fine-tuning outperforms other retraining techniques in terms of accuracy and compression ratios. They also show that learning rate re-winding can be applied to unpruned weights as well. Learning rate reweighing can be done with a different learning rate schedule from the one used for weight rewowing.  The paper also shows that the performance of a network-agnostic pruning algorithm based on rewitching techniques is comparable to that of the original fine -tuning. "
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper studies the relationship between the (normalized) output margin and generalization of deep models. The authors show that the output margin of linear classifiers is a measure of generalization, and that the margin, called the all-layer margin, is a theoretically inspired training algorithm. They also show that neural nets with Jacobian and hidden layer norms are robust to adversarially robust setting, and show that deep networks with robust test error can be trained with a neural net with the proposed algorithm. "
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper tackles the problem of learning an intelligent conversational agent that is capable of generating knowledge-grounded dialogues. The authors propose a novel response generation model that learns a disentangled response decoder and a generation model. The model is trained on ungrounded dialogue, unstructured dialogues, and unsupervised documents. The proposed model is evaluated on three benchmarks, where it is shown to be able to generate dialogue with out-of-domain knowledge with limited training examples for small parameters.   "
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a new parallel corpus for training neural machine translation models (NMT) on non-parallel bilingual data. Existing approaches have been shown to benefit from the use of non-plausible bilingual data for training and decoding. The authors propose a single unified architecture called mirror-generative NMT (MGNMT), which consists of a source to target translation model, a target to source translation model and language models. The translation directions are generated from non-differentiable translation models in the latent semantic space, and the translation models are trained on the translation directions generated by translation models.  The authors show that MGNMT outperforms existing approaches in both resource-rich and low-resource situations. They also show that translation models trained on translation models, language models trained for decoding on the nonparallel data, as well as language models that are trained with translation directions from the source language. "
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper studies the sample efficiency of maximum entropy reinforcement learning algorithms for Deep Reinforcement Learning (DRL). The authors propose a new maximum entropy objective for the Mujoco benchmark. They show that the entropy term in the standard maximum entropy algorithms (e.g., Soft Actor Critic (SAC) and SAC with the same entropy term for the bounded nature of the action spaces) can be used to improve sample efficiency on these benchmarks. They also show that this entropy term can be incorporated into the standard SAC in order to improve the performance of SAC.  The authors also propose two streamlined algorithms for SAC based on entropy maximization. The first streamlined algorithm uses a non-uniform sampling scheme, where the transitions in the replay buffer are sampled from a different distribution. The second streamlined algorithm is based on a non - uniform sampling method, where transitions are sampled based on the number of times they have been visited during training. The authors demonstrate that the proposed streamlined algorithm outperforms SAC on continuous control tasks. "
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper studies the problem of adversarial attacks on machine learning models. The authors show that existing industrial copyright detection tools for the web are vulnerable to adversarial music, and that they can be easily exploited to fool existing copyright detection systems. Adversarial music can be used to fool industrial systems such as the AudioTag copyright detector and YouTube’s Content ID system.   The authors propose a new music identification method that can be applied to existing neural network based systems. The system uses a neural net to identify the source of the adversarial examples, and then uses gradient methods to fool the system. They show that the system can be trained to fool a classifier. "
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper proposes a new framework for deep metric learning with visual explanation. The authors propose a learning representation that is based on the overall activation map of each point in the image, and the point-to-point activation intensity. They show that the proposed framework can be applied to several metric learning applications, including cross-view pattern discovery and interactive retrieval, and show that their model outperforms classification. "
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper studies the problem of learning control in an online lifelong learning scenario, where the goal is to learn compact networks that do not suffer from performance degradation. The authors propose Adaptive Online Planning (AOP), which is an extension of model-free policy learning methods. AOP learns a continual learning agent that is able to adaptively adapt to new environments and environments that are more challenging to learn from. The setting is very similar to the one in which the authors propose the setting of learning-to-learn (L2L), but the authors argue that they are able to avoid some of the failure modes of L2L due to the fact that they do not require any computational resources that are typically required in model-based planning methods.  The authors show that AOP can be applied to a variety of settings, including a setting where there is no planning, a setting in which there is a planner, and a setting with constrained computation limits. They show that in this setting, AOP is more robust to the dynamics of the environment, and that it can learn to adapt to a new environment in a way that is more resilient to unpredictable changes in the world. They also show that the algorithm can be used to learn a planner that can be combined with any of a number of existing models, and can be seen as a generalization of existing reinforcement learning methods, which can be considered as a way to combine the benefits of model based planning with the benefits from model free learning.   "
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,Visual attention mechanisms for image captioning models have been a topic of interest for many years. This paper proposes two sparsity-promoting transformations to the softmax attention mechanism: sparsemax and Total-Variation Sparse Attention (TVMAX). The idea is that sparsemax encourages the sparse attention weights to be close to the relevant features in the image structure. The authors show that the TVMAX transformation improves interpretability and human-rated caption quality on the Microsoft COCO and Flickr30k datasets. They also show that TVMAX outperforms other attention mechanisms in terms of humanrated captions quality and attention relevance.   
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes a new model for predicting the evolution of dynamic graphs in graph mining. Neural networks are commonly used to model structured data (e.g. graphs). This paper proposes to use a graph neural network and a recurrent architecture to predict the temporal evolution patterns in dynamic graphs. The key idea is to learn a generative model that predicts the topology of a graph instance, and then use this graph instance to learn the underlying topology. The proposed model is evaluated on two artificial datasets with common network evolving dynamics, and on two real-world datasets with static graphs. It is shown that the proposed model outperforms the real world networks on both of these datasets. "
SP:ff722957a1765c0568426ed88dd910a6b74054ef,This paper proposes a new method for imputing missing features from incomplete datasets for machine learning applications. The authors extend previous missing data imputation techniques for filling missing values to the case where the distribution of missing values is unknown. The proposed method is able to generate a distribution of target assignments from incomplete data. The imputations are made by using a generator network to generate imputations from the incomplete data and a discriminator network to discriminate between the imputed features and the true missingness rates. The generator network is then used to train a predictor network that uses imputed samples to model classification uncertainties. The method is evaluated on the CIFAR-10 image dataset and real-world tabular classification datasets. The results show that the proposed method can achieve state-of-the-art performance for generating imputations with class uncertainties for a classification task.
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper considers the problem of off-policy estimation for long-horizon problems, where high-fidelity simulators are available but not available for on-policy evaluation. In real-life applications such as healthcare and robotics, there is a curse of horizon that existing importance-sampling-based methods suffer from. This paper proposes a new approach to address this problem. The proposed estimator is based on Reproducing Kernel Hilbert Hilbert Spaces (RKHSs) and is able to estimate the importance ratios of stationary distributions of a known behavior policy with respect to a stationary distribution of the data. The authors show that their estimator achieves asymptotic consistency and finite-sample generalization, and that it can be applied to any problem where the operator is a linear function of the number of times that the operator has been used in the past."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper proposes a new probabilistic framework called the Mixture Model (MM) to model a dataset with a Gaussian distribution. The dataset consists of a set of modes, each of which corresponds to a different conditional likelihood. The authors propose a new GMM paradigm, where each mode is associated with a distribution index, and the distribution index is a function of the number of modes in the dataset.    The authors show that under Gaussian MM, the distribution of the modes in a dataset can be approximated by a GAN, where the modes are assumed to be drawn from the same distribution.  The paper also shows that under Euclidean distances, a plausible method to estimate the probabilities of the probabilities can be derived from the Generative Adversarial Network (GAN) framework.  In particular, the paper shows that for large datasets with unlabelled modes (e.g., the paintings dataset and fashion images), the probability of the distributions in a GMM can be estimated from the latent representation of x, and that the distribution in the latent space of the GAN can be used to train a classification network.  Finally, the authors propose two techniques to sample from the unsupervised dataset. First, they use smooth linear interpolation to sample samples from the “outdistribution” data, which is defined as samples from a distribution that is independent of the dataset segments. Second, they apply GAN to learn the distribution over the samples, and use the learned distribution as a basis for the task of assigning a “responsibility” to each sample. "
SP:2da1608209058d214f8671062cc9eb0833ba4831,"This paper proposes a new method for training large capacity neural networks with a focus on accuracy and dynamic computational cost. The authors propose a deep-learning architecture at the fine-grained-level, where the convolutional maps of the network are aggregated to form the final output of the deep network. This is achieved by using a residual block architecture in a fine- grained manner, which is similar to the residual block that is used in the original deep learning architecture. The marginal aggregate posteriors of features in a neural network are obtained from a pre-specified prior distribution. The technique is applied to the gates of the gates, and the authors show that the technique can be applied to both image classification on CIFAR-10 and ImageNet datasets, and semantic segmentation on Cityscapes. The proposed method is shown to outperform the average computational cost of the original architecture as well as the architecture of a ResNet34 gated networks in terms of accuracy and top-1 accuracy on ImageNet. The complexity of the ResNet18 model is also shown to be much smaller than the original one.  The authors also show that their method can be extended to other architectures, and that the networks trained with the same number of features can be trained with different number of gates.  Finally, the authors provide a theoretical analysis of batch-shaping, which shows that the network is able to learn a good trade-off between the quality of the features and the number of parameters. "
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"Deep neural networks (DNNs) have been shown to be efficient in terms of energy and computational resources. However, pruning DNNs can be expensive due to the large number of parameters required to prune a DNN. This paper proposes a probabilistic importance inference approach to pruning the parameters of DNN, which is based on a nonparemetric scoring test. The paper shows that the proposed approach outperforms existing techniques in lossless compression rates. The main contribution of the paper is that the DNN’s outputs can be pruned in a way that does not require any additional parameters."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper presents a new perspective on hierarchical reinforcement learning. The authors show that existing approaches for hierarchical reinforcement learn can be suboptimal when sub-goal structure is not well defined. They propose a method for iteratively compressing action trajectories to learn nested behavioral hierarchies. The key idea is to use action primitives to encode deeper hierarchies, which are then used to learn a compact code of actions and sub-goals. They show that this approach can be used to speed up learning by learning a non-trivial hierarchical structure. They also show that the proposed approach is transfer-agnostic and can be applied to a variety of tasks."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"Autoencoders are one of the most popular generative models for complex data (e.g. images). However, existing models such as the variational autoencoder (VAE) are unimodal Gaussian decoders. This paper proposes a new probabilistic generative model called the Hierarchical Bayes Autoencoder (HBAE). HBAE consists of a multimodal decoder that uses an energybased model (EBM) and a variational inference to learn a latent code. The decoder is trained using an adversarial approximation to the EBM distribution, and the conditional generator is used to learn the EMC of the EMI of the decoder, which is then used for stochastic reconstruction from the code.   The paper shows that the proposed model, called Set-HBAEs, is able to learn complex image sets with semantic variations, where the sets are generated from a set of latent codes. The authors also show that the model can learn sets with multiple sampling steps, and that a decoder can be trained to generate realistic unconditional samples in the single image and set cases. "
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper studies the problem of normalization in reinforcement learning (RL) algorithms. In particular, the authors investigate the relationship between deep off-policy TD algorithms and feature normalization techniques. They show that normalization for target networks can improve optimization stability when normalization is applied to a mixture of on-policy transitions. It can be seen as a variant of batch normalization. The authors also show that cross-normalization improves the performance of DDPG and TD3 on the MuJoCo benchmark tasks.    The main contribution of the paper is that the authors provide a theoretical analysis of the effect of normalisation techniques on the stability of off-propagation learning."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper tackles the problem of bias and confounding effects in machine learning applications. The authors consider two challenges: (1) spurious associations of confounding variables, and (2) biases in medical studies. They propose two techniques to improve precomputed features for confounding variables: residualization and stratification. These techniques are commonly used in statistical methods, but not in end-to-end deep learning methods.   The authors propose a method to improve discriminative features by using an adversarial training strategy to train a discriminator on datasets where the confounder(s) are known. The method is tested on synthetic data and medical images, and is shown to improve the performance of face recognition systems. The adversarial loss function is used to train the discriminator, and it is shown that the method is more robust to bias or confounding variables than existing models. "
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"Character-level language modeling with a Transformer is a challenging problem due to limited resources and the limitation of recursive operation. In this paper, the authors propose a lightweight model called GroupTransformer, which uses grouped embedding operators to reduce the computational resources of existing Transformer-based models. The Group-Transformer uses inter-group linear operators, and the authors show that the proposed lightweight model is able to efficiently compute the calculation paths for different groups of groups. The authors also provide a qualitative analysis of the proposed group strategy. Experiments on two benchmark tasks, enwik8 and text8, show that Grouptransformer outperforms existing LSTM-based and Transformer -based models in terms of performance. "
SP:946c26d371297c88d0ac246257104099b4585edc," Probabilistic models with hierarchical-latent-variable structures can be seen as an extension of Variational Autoencoders. This paper proposes a new approach to train models with deep- latent hierarchies based on Optimal Transport. The authors show that this non-likelihood-based framework can be used to train generative models with bespoke models and inference networks. They also show that both inference and optimisation schemes can be applied to improve the performance of existing approaches. The method is shown to be able to learn a generative model with a deep-latency hierarchy, and it outperforms the Wasserstein Autoencoder with Maximum Mean Discrepancy divergence. "
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"This paper studies the problem of generating natural video. The authors consider the statistical complexity of video-specific neural network architectures, latent variable models, adversarial training, and other methods in video generation models. They show that the inherent stochasticity of natural video is a limitation of existing approaches, and propose a three-dimensional self-attention mechanism to improve the performance of autoregressive video generation methods on standard benchmark datasets. They also show that they are able to generate continuations that achieve high realism and fidelity, and that they do so in narrow domains. They evaluate their models on Kinetics, a large scale action recognition dataset based on YouTube videos, which contains phenomena such as camera movement, complex object interactions, and human movement. "
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"This paper proposes a framework for zero-shot learning of International Classification of Diseases (ICD) codes, which are the most commonly used classification codes in the medical community. Automatic ICD coding is a popular multi-label text classification task with noisy clinical document inputs and long-tailed label distribution. The authors propose a latent feature generation framework to learn generalized zero-Shot ICD codes from labeled data. They use frequent and zeroshot codes for fine-grained classification, and use frequent codes for classification with seen codes. They propose a framework that learns semantically meaningful features to generate zero-shots, and uses the ICD code hierarchical structure and a cycle architecture to generate keywords. The proposed approach is evaluated on the public MIMIC-III dataset, and shows that the proposed methods are able to learn zero- shot codes with high AUC score and low F1 score. They also propose an adversarial generative model to further improve the performance of the proposed framework in the context of multi-labels text classification.   "
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,"This paper proposes to use self-supervised representation learning to improve sample efficiency in reinforcement learning (RL) by using the forward prediction objective to learn embeddings of states and action sequences. The authors show that the embedding of the states and actions are useful for policy learning, and that the learned representations can capture the structure of the environment’s dynamics. They also show that learning action embedding improves sample efficiency of model-free RL in low-dimensional states. Finally, the authors demonstrate that learning of high-quality policies from pixel observations using state and action embeddens is possible using goal-conditioned continuous control. "
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes a meta-learning framework for tasks where the task-specific structure of the knowledge base is not shared across all tasks. The authors propose to use a hand-crafted structure design to learn a set of tasks that can be shared across tasks. They show that this framework is able to capture task heterogeneity and improve model interpretability. They also show that the knowledge organization in the knowledge bases can be learned through the meta-learner using the structure knowledge.    The authors show that ARML outperforms other baselines on tasks such as 2D toy regression and few-shot image classification. The paper also shows that the framework can be used to capture the task heterogeneity through the use of meta-knowledge graph.  The paper shows that there is no clear advantage of the proposed framework over existing ones. The main contribution of the paper is to propose a framework that can capture the cross-task relations between different tasks. This is an interesting direction to explore in the context of the literature. The idea is interesting and the paper has some interesting contributions. However, there is a lack of comparison with other globally shared meta - learning methods and there is not a lot of experimental evidence to show the effectiveness of the framework."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM and attribute classifiers (i.e., attribute models) for text generation and fine-tuning on attribute-specific data. The idea is to sample from a huge text corpora and then fine-tune the model architecture and fine -tuning based on the generated text. Sampling is done in an unsupervised way: no retraining is performed on the entire corpus, but only on a subset of it. Model samples are used to train an attribute model, and the attribute model is used to guide the generation. The authors show that the attribute models outperform the LM on both automated and human annotated evaluations of attribute alignment and fluency. They also show that differentiable attribute models can be used to improve the quality of text generation.    The authors also propose a new way of training PPLMs. The key idea is that the gradients of the attribute classifier are used as hidden activations, and that the output of the classifier is used as a proxy for the true input of the PPLM."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper studies the problem of learning robust representations on unlabeled data from deep neural networks for machine learning tasks. The authors propose an unsupervised learning framework where the goal is to learn representations that are robust to noisy input data from clean data in the gradient domain. They show that both un supervised and self-supervised approaches to learning visual data with domain knowledge can benefit from domain knowledge. They propose to learn a denoising autoencoder to learn the data representations from the clean data, and then use the learned representations to train an agent that is robust to single-scale corruption. They evaluate their representations on several visual benchmarks and show that their approach is able to learn robust representations for vision tasks."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper studies the problem of under-sensitivity in neural networks for Natural Language Processing. The authors propose a new technique for formal verification of this specification, which they call the decomposable attention mechanism. The technique is based on the interval bound propagation (IBP) approach, which is a technique for training models that satisfy this specification.  The authors show empirically that IBP training on the SNLI and MNLI datasets improves the verified accuracy of the proposed method. They also show that the training methods that are designed to alleviate the under-sensitiveness of natural language inference also improve the training performance.  Finally, the authors show that their proposed method can be applied to any model that is trained on a large number of training examples.    The main contribution of this paper is that the authors have proposed a new metric to measure the under sensitivity problem, which can be used to evaluate the performance of a model on a set of different metrics. The paper also shows that the proposed metric is more sensitive to the number of examples in the training set than the total training set. "
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper considers the problem of learning the Q-values of a Markov Decision Process (MDP) with state and action spaces. The authors propose QGRAPH, where transitions are represented as a data graph, and the transition in a continuous Q-learning problem is a Q-value of a transition in an MDP. The transition is defined as a subgraph that has a favorable structure (i.e., a soft divergence between the structure of the subgraph and that of the current state and the current action) and a replay memory for network updates.  The authors provide lower bounds on the soft divergence of their method, which is based on TD learning, and show that their method achieves a sample efficiency of $O(1/\sqrt{T})$ when the transition is a continuous MDP with favorable structure. They also provide a lower bound on the replay memory capacity of their algorithm, which shows that their algorithm achieves a trade-off between sample efficiency and replay memory.  Finally, the authors provide a theoretical analysis of the algorithm, showing that the algorithm converges to the optimal solution with high probability, and that the hyperparameters are well-behaved. The algorithm is shown to be computationally tractable, and can be applied to a wide range of Q-valued MDPs."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,This paper studies the problem of Unsupervised domain adaptation. The authors propose an approach to this problem based on domain-invariant embeddings. They show that the embedding complexity of the target domain is the limiting factor of the generalization performance. They also provide a theoretical framework for multilayer neural networks. The proposed strategy is shown to be more efficient than the layer-dependent complexity tradeoff.
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper proposes a new framework for algorithm-dependent generalization error bounds based on PAC-Bayesian theory and algorithmic stability. Generalization error is defined as the difference between the out-of-sample error and the true error of the algorithm. The authors propose a Bayes-Stability method to derive data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) for noisy gradient methods such as momentum, mini-batch and acceleration, and Entropy-SGD.    The authors show that under certain assumptions on the statistical learning theory, tight generalisation error bounds can be obtained for any continuous Langevin dynamic. They also show that the data-dependence of generalization bounds for randomly labelled data is tighter than for random labelled data with the same noise level.  The main contribution of the paper is that the authors derive generalization bound based on the Log-Sobolev inequality of the parameter distribution of the Langevin dynamical system, and show that for any total loss consisting of a bounded loss and a `2 regularization term, the bound is tight.  In addition, the authors provide a theoretical analysis of the generalization of the bounds. "
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper studies the continual learning of navigational strategies in the hippocampus. The authors show that the hippocampus plays a key role in spatial memory and goal-directed spatial navigation, and that the populationlevel activity of hippocampal CA1 neurons in continual learning is correlated with the population level activity of the hippocampus for spatial navigation strategies. They propose a new method, called Demixed Principal Component Analysis (dPCA), to identify which components of the neurons are important for continual learning. They use dPCA to measure the firing activity of a set of task variables (decisions, navigation strategies, reward location, etc.). The authors demonstrate that the hippocampal neurons are highly correlated with these task variables. They also show that a deep reinforcement learning model trained on the hippocampus is more robust to task switching than other reinforcement learning algorithms.   The authors also demonstrate that in both allocentric and egocentric spatial tasks, the hippocampus can be used as a good resource for continual continual learning, and show that it can be trained in a similar way as animal learning. Finally, they show that using biological and machine learning, spatial continual learning can be learned using the hippocampus as a resource for reinforcement learning. "
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"This paper proposes a new tree search based policy optimization method called TPO, which extends Monte Carlo Tree Search (MCTS) to discrete environments (e.g. Go) and extends it to continuous domains. The authors propose a hybrid approach to policy optimization based on TPO. The main idea is to use the off-policy MCTS trajectories to guide the policy gradient from a pre-trained policy to a bootstrapping tree search, where the limiting tree search branching factor is a function of the branching factor of the policy distribution and the number of times the policy has been visited by the current policy. This approach is shown to be effective in a variety of settings, and the authors show that TPO can be used to bootstrap a policy in a continuous action space. They also show that the policy bootstrapped in continuous MCTT can be trained using policy bootstraping, and that the optimal branching factor and the simulation count of the continuous policy are the limiting factors of the optimal policy.  The authors also provide a baseline algorithm based on PPO, and show that this baseline algorithm is robust to the limiting factor of branching factor. The paper also shows that in complex environments such as Humanoid, the policy trained with TPO is able to learn a policy that is more robust to limiting factors.    The main contribution of this paper is that the authors provide a theoretical analysis of the limiting branching factor, which shows that the limit of the limit can be controlled by the branching factors of a policy distribution, and also depends on the loss function. This analysis is also shown to hold for complex environments. "
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper investigates the lottery ticket hypothesis, which claims that sparse subnetworks in neural networks can lead to higher accuracy in training the network but lower accuracy in test time. The authors argue that this is due to neural network optimization, where the initializations of the network are sparse. They show that winning tickets on the ImageNet dataset outperform winning tickets without any supervision in the generating process. They also show that on ImageNet classification task, winning tickets outperform other winning tickets. "
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"This paper studies the problem of ""excessive prediction undersensitivity"" in Neural reading comprehension models. This is a complementary problem to the complementary problem of excessive prediction under-prediction, i.e., overfitting to adversarially selected input. The authors propose a noisy adversarial attack on NewsQA models, which is motivated by the observation that these models are sensitive to spurious surface patterns in the training data. They show that this attack is semantically invariant to the semantic invariant text perturbations. They also show that under this attack, the semantic variations of comprehension questions are semantically similar to each other. They then propose two defence strategies based on data augmentation and adversarial training to mitigate this problem. Finally, they demonstrate that under undersensitivity attacks on the model on held out evaluation data, they achieve a F1 that is on par with the best model trained on the train/evaluation distribution mismatch in the biased data setting, and that they outperform a model trained with no predictive cues. "
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model-based approach to improve the safety of reinforcement learning (RL) agents for real-world tasks where there are bad incentives or unsafe scenarios. Previous approaches to this problem have been evaluated on complex domains, but they do not consider the transition dynamics of the environment. In this paper, the authors propose to add a safety penalty to the reward function. The authors propose a directed graph, called the ""imagination module"", which is an extension of existing approaches. The idea is that the safety penalty is a function of the transition from a baseline state to the current state in a discrete action space, and the authors show that this graph can be used as a way to model the transition between states. The proposed method is evaluated on a gridworld environments and a self-driving car simulator, where the proposed approach is shown to outperform a baseline. The paper also shows that it can be combined with any RL algorithm, and that the proposed method can be applied to any task. "
SP:c2796f28fb067138303df8d424d646f4ada31558,"This paper studies the problem of learning dynamics of physical systems from sparse data. The authors propose to use deep learning models to learn physics-regulating observations on an unstructured grid, where the discretization error is proportional to the number of spatial and temporal differences between the sequential observations. They show that the finite differences are related to the numerical error of finite differences induced by neighboring information, and that the physics equations for these finite differences can be approximated using physics equations. They then propose PA-DGN to learn the dynamical relations between the spatial, temporal, and dynamical components of the system. PA-GND is applied to the approximation of directional derivatives, the prediction of graph signals from synthetic data and real-world climate observations from weather stations. The architecture is well-motivated and the experiments are well-designed."
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper studies the problem of training structured neural networks (NN) with nonsmooth regularization and other constraints (e.g., interval constraints). The authors propose a novel algorithm ProxSGD, which is based on the notion of `1-norm` (i.e., the ` 1-norm is the sum of all the regularizations of the weights of the NN). They show that the training is equivalent to solving a nonconvex optimization problem, where the learning rates are fixed and the constraints are non-uniform. They also provide a theoretical analysis that shows that the stationary point of the ProxSgd algorithm is a stationary point, which implies that the learning rate is non-unsmooth. Finally, they show that Prox SGD can be used to train sparse or binary neural networks with the same regularization function and constraint set. "
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper proposes a new framework for lossy image compression based on a non-deterministic compression codec. The idea is to encode an image into a discrete code and then use approximate methods to train an encoder to learn compression algorithms. The encoder and decoder are trained end-to-end in a continuous space, where the quantization step is in continuous space and the encoding distribution is in discrete space. The expected code length and relative entropy of the encoder are used to guide the algorithms, and the decoder is trained to be differentiable. The authors show that gradient-based optimizers can be used to learn an end- to-end differentiable compression framework. The paper shows that the rate-distortion curves on the Kodak dataset have better rate-drift curves with low bitrates compared to the state-of-the-art, and it can be applied to any method based on Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset. "
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper tackles the problem of Super Resolution (SR) in the context of image processing operations, i.e., SR, where the goal is to generate a compressed JPG (C-JPG) image from a given SR image. The authors consider the SR issue in the setting where SR models are trained on a large number of high-qualified SR images, but the storage space is limited. They propose a functional sub-model, called Super-Resolution (SR-S), to address this SR issue. The SR structure is composed of two components: the components of the functional submodel and the cycle loss. The functional submodular part of SR-S is used to generate high-quality SR images from the prevalent C-jPG images. The hybrid loss function for SR generation is also proposed, which combines the SR solver with a cycle loss to further improve the performance of SR models. Experiments show that the proposed approach outperforms state-of-the-art methods in terms of the quality of the generated C -JPG images, as well as the number of steps required to generate the final image."
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper proposes a fully convolutional network architecture for learning the surface of pass probabilities from single-location labels from professional soccer matches. The network is trained with a feature hierarchy, where low-level inputs are used to guide the network through the training process and high-level ones are used for sampling levels. The proposed approach is applied to weakly supervised learning and is shown to be effective for spatiotemporal decision-making analysis in the context of sports, where the goal is to select the best pass-selection likelihood from a set of players in a game. The paper also shows that the proposed deep learning architecture can be applied to a variety of applications in sports analytics, including the application of the proposed approach to the problem of learning a spatio-temporal decision making analysis in a setting where coarse and fine detail are available. The authors also show that the single pixel correspondence between the predicted probability map and the true probability map can be learned in this setting."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion model based on side information, i.e., user’s age, movie age, and movie genre, which is a combination of content (side information, e.g. movie rating) and a (rating) matrix. The authors propose a graph neural network (GNN) based on IGMC, which learns a low-dimensional latent embeddings of the input and its subgraphs, and uses them to learn a (ratio) completion matrix for each subgraph. It outperforms other transductive baselines on the MovieLens dataset, and the authors also show that their model can be applied to Douban movie ratings. Long-range dependencies are important for modeling recommender systems, and side information can be a useful tool for inductive Matrix completion models. However, existing matrix completion methods rely on embedding the embedding of the subgraph to the rating matrix, which can be computationally expensive. This paper proposes to use a graph GNN based on the IGMC to embed the sub-graphs to the matrix completion matrix, and shows that it is computationally efficient. The paper also shows that the proposed inductive methods are more robust to changes in local graph patterns.   "
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper proposes a stochastic zeroth-order method for unconstrained minimization of a smooth objective function. The authors propose to use heavy ball momentum, which is an extension of the popular Stochastic Zeroth Order Optimization (STO) algorithm. The proposed method is evaluated on learning to continuous control tasks, and compared to STP, derivative-free optimization algorithms, and policy gradient methods. SMTP is shown to be more efficient than STP and other methods, and the authors also show that SMTP with importance sampling is more robust to function evaluations. The complexity of SMTP_IS is also improved. "
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes a new deep learning architecture for multiagent coordination mechanisms based on the Action Semantics Network (ASN), which is a network architecture inspired by the action semantics of neural networks. In multiagent systems (MASs) there is a strong correlation between environmental stochasticity and uncertainties, and the authors show that the ASN is able to capture the dynamics of system evolution in MASs. The authors also show that ASN can be combined with existing deep reinforcement learning (DRL) algorithms to improve the performance of co-learning agents. The ASN has been tested on StarCraft II micromanagement and Neural MMO, and is shown to outperform existing DRL approaches and existing network architectures."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"Value-based methods for planning and deep reinforcement learning (RL) have been a popular topic of interest in recent years. This paper studies the problem of learning the global structures of the Q function, i.e., the state-action value function, which is a Q function that depends on the system dynamics. The authors propose a framework based on Matrix Estimation (ME) techniques to learn low-rank Q functions for both control and deep RL tasks. They show that the lowrank structure of big data matrices can be used to model the global structure of Q function. They also propose a scheme to learn a planning procedure for classical control that can be applied to “low-rank” tasks. The proposed approach is evaluated on a variety of control tasks and Atari games. "
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes a new algorithm called Best-Action Imitation Learning (BAIL) for batch reinforcement learning for sample-efficient learning in Deep Reinforcement Learning (DRL). The algorithm is a generalization of off-policy DRL algorithms to the batch DRL setting, where the goal is to maximize the maximizing Q functions in the action space. The paper shows that it can be used to train a policy network using only state-action pairs, and it can also be used with imitation learning to learn a new policy network. Experiments on the Mujoco benchmark show that BAIL achieves state-of-the-art performance in terms of performance on the Q functions and on the number of samples. "
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper proposes a new deep extreme multi-label learning algorithm, called DeepXML, for short text documents. Deep extreme classifiers are used to learn feature representations and classifiers for short texts. The authors propose to use word embeddings from the Slice algorithm to improve the performance of the Deep XML algorithm. Specifically, the authors use negative sub-sampling techniques to sample negative training data, and then use them to train a residual connection between the word embedding and the classifier.    The authors show that the proposed architecture is able to achieve state-of-the-art accuracy in terms of accuracy on search engine queries and advertiser bid phrases, and that it outperforms XML-CNN and AttentionXML. They also show that DeepXml architecture can be trained with pretrained embeddens from the previous work using the same amount of training data as well as with the same number of classes as the original training data. "
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper proposes a new hashing-based collaborative filtering based on binary vector representations (hash codes). The authors propose to use Hamming distance for recommendations based on a user hash code as a mask, and use a Boolean AND operation to compute the mask for each user. The proposed approach is evaluated on the NDCG, and compared with several baselines. The authors show that self-masking is more efficient than Hamming distances, and that Hamming is more computationally efficient than the binary user-level importance weighting. The paper also shows that the distance computation is computationally tractable. "
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"Generative adversarial networks (GANs) have been shown to generate images with low-level perceptual quality, but this paper investigates the phenomenon of mode collapse in the GAN’s learned distribution. The authors propose a new set of evaluation metrics for image synthesis that can be used to evaluate the mode collapse of a GAN learned distribution, and show that the evaluation metrics are sensitive to the low level perceptual quality of the generated images. They also propose a set of statistical tools to measure mode collapse for GANs using a new toolset. The paper also shows that mode collapse can be caused by the model parameters."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper studies over-parametrized neural networks and linearized models with Neural Tangent Kernels (NTKs) under the NTK theory. The authors show that neural networks with NTKs are more robust to the Taylor expansion of the network compared to neural networks without neural Tangent Kernel (NKT). The theory shows that under mild distributional assumptions, the optimization landscape of randomized two-layer networks with escaping-saddle algorithms converges to the optimal solution of a Taylor expansion on the network.    The authors also show that under a similar setting, under the quadratic case, the optimal solutions of NTK regime can be found in the limit of a certain dimension factor, which they call the dimension factor.  The main contribution of this paper is that it provides a theoretical analysis of the sample complexity bounds for these networks and shows that it can be obtained for networks with higher-order terms in the Taylor series, and it also provides a randomization technique that can be used to improve the sample efficiency of the randomization.  In particular, it shows that the sample-efficiency bounds are tight for quadratically models, and that it is tight for the case where the dimension is larger than the dimension of the neural network. It also shows that this is also the case for randomized networks. "
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"Graph Neural Networks (GNNs) are widely used for graph data for a variety of downstream tasks. However, the graph filter design in many GNN models is not well understood. This paper proposes a new assessment tool for evaluating the performance of graph convolutional filters on graphs. The paper proposes an adaptive filter design for graph filters that can be applied to any filter for any graph data. The authors propose a new model called Adaptive Filter Graph Neural Network (AFGNN), which is a model that adaptively selects data-specific filters for each filter for a given filter for the purpose of semi-supervised node classification task. The proposed AFGNN is based on the Graph Filter Discriminant Score (GFD) which measures the difference between the optimal filter and the one that is optimal for the given filter. The graph filter is constructed by considering the graph properties of the input graph and the number of filters applied to the graph, and the loss term is the sum of the difference of the average of the optimal and optimal filters across all the filters applied. The model is tested on both synthetic and real-world benchmark datasets and the authors show that AFGnn is able to learn a filter for each graph based on graph filter assessment, and that the model can learn the filter that is best suited for a particular filter for that filter. In addition, the authors also show that the base filters of AFGNet can be adapted to different filters for different graphs.  "
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies overparameterized neural networks on the i.i.d. test set. The authors consider Distributionally robust optimization (DRO) for models trained with group DRO. They show that the average training loss of a model with vanishing worst-case training loss can be approximated by a group of samples with spurious correlations. They also show that worst-group generalization can be achieved with regularization (e.g. early stopping). They also provide convergence guarantees for a stochastic optimization algorithm that optimizes the worst-groups of a set of samples, and show that it can be used to regularize the average generalization in the over parameterized regime. Finally, they show that this regularization, called ""group DRO models with early stopping"", can be applied to improve the performance of the worst group in a natural language inference task and several image tasks. "
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper studies the problem of local explanation methods for the decision of black-box classifiers. In particular, the authors propose two methods: (1) learning a mask predictor, and (2) using it and a neural network to learn the distribution of relevance scores based on ad hoc constraints on the classification loss of the predictor. The authors propose a strategy to learn discriminative scores using features from the mask predictor. They show that their method outperforms others in terms of faithfulness and explainability. They also show that the hyperparameters of the distribution controllers are not too different from the ones used in previous work."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"This paper tackles the problem of learning a deep network for image reconstruction and classification problems, where the detection of multiple object instances is difficult due to the lack of supervision. The authors propose to use a task-specific network (e.g., auto-encoder or classifier) that is trained to solve a domain specific problem using patches from a set of patches. The training optimization problem is formulated as a non-differentiable top-K selection process, where each patch is selected by a separate network. The top-k selection is based on a slack variable, which can be a function of the number of patches in the patch and the number (and the size) of the patches. It is shown that the proposed method is able to identify recurring structures in the training data, and that it outperforms the state-of-the-art by a large margin. The paper also shows that the method is more robust to multi-stage training, and can be applied to a variety of tasks."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,"This paper proposes AutoAssemblet, a neural program synthesis algorithm based on self-learning reinforcement learning for large code space.   Neural inductive program synthesis is an important problem in many real-world applications, where the state change in assembly code can affect the performance of the CPU in terms of memory and RAM. Policy networks and value networks are used for Monte Carlo Tree Search, and synthesis is performed using Policy networks. The authors propose a multi-entropy policy sampling technique to reduce the online update correlations between task generating instructions and the current state of the art.  Experiments are conducted on a variety of tasks, and AutoAssemblest achieves better success rates than the baselines, and is able to generalize to more complex tasks. "
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper considers the problem of learning a neural network architecture design space, where the goal is to design a model architecture that achieves high accuracy, speed of training, and resource requirements. The authors consider gradient descent optimization under a first-order ODE, and study the convergence rate of the ODE’s coefficient matrix H, which depends on the architecture of the network and the number of parameters. They show that the speed of convergence can be improved by modifying the model architecture parameters. The paper also proposes a new analysis technique for computing H for model architecture modifications. "
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper studies the problem of initialization for overparametrized neural networks (NNs) and kernel methods. In particular, the authors consider the case of wide NNs and interpolating kernel methods, where the overparameterized NNs are trained with gradient descent. The authors show that the interpolating kernels are a minimum-complexity interpolation method, and they provide a minimum complexity solution to the problem. They also provide generalization bounds for fully-connected wide ReLU-NNs with a squared loss.    The authors also provide a generalization bound for wide NN with a test error of $O(1/\sqrt{n})$ for the case where the test error is $\mathcal{O}(\log n)$, where $n$ is the number of training samples and $n^{-1/n}$ the initialization variance.  Finally, they show that for a certain initialization scheme, the generalization performance of wide and interpolated NNs can be improved by a factor of $\log n$, which is a factor that depends on the initialization strategies. "
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"Detecting objects in 3D for autonomous driving, i.e. cars, pedestrians, and vehicles, is a challenging problem due to insufficient information. Previous approaches have used LiDAR sensors to capture accurate depth information, but the depth map is not always available for 3D detection. This paper proposes a pseudo-LiDAR framework based on stereo depth estimation, which uses stereo images. The authors propose a new stereo network architecture, loss function, and depth estimation of faraway objects. The depthpropagation algorithm is based on the depth estimates obtained from the proposed depth estimates. Experiments on the KITTI object detection benchmark show that the proposed approach improves the depth estimation and stereo-based 3D object detection performance compared to the state-of-the-art detection accuracy. "
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a new adversarial example detection method for white-box adversarial attacks. The authors propose a generative adversarial training method to generate adversarial examples that can be used to improve the robustness of deep neural networks. The proposed method, GAT-Generative-Adversarial-Training (GAT-GAT), is based on the idea that adversarial samples can be generated from the training data, and the detection mechanism can be trained to distinguish between one-versus-the-rest classification and one-vs-all classification. The paper also proposes a defense against such attacks.  "
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"Exploration in model-free reinforcement learning is an important component in sparse reward environments. Exploration is typically done in sparse environments with no extrinsic rewards. However, intrinsic rewards can be used to encourage exploration in environments with high-dimensional observations.  This paper proposes a method for exploring procedurally-generated tasks on MiniGrid, where the learned state representation is sparse.  The proposed method is evaluated on a number of tasks with high dimensional observations. The proposed approach is shown to outperform existing exploration methods in procedurally generated MiniGrid environments, and it is shown that the agent with an intrinsic reward is able to explore more sparse environments than existing approaches.  "
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper tackles the large-scale query-document retrieval problem, where the goal is to retrieve documents from a large document corpus. The authors propose a retrieval algorithm that is based on the notion of ""recall"", i.e. the number of documents in the solution space that the retrieval algorithm is able to retrieve from a given document. The problem is formulated as the following: given a large corpus of documents, the retrieval problem is to find the most relevant document in the problem space.    The paper proposes to use cross-attention models for BERT-style pre-training tasks, where sparse handcrafted features are used to train the models.  The authors show that their retrieval algorithm outperforms existing Information Retrieval (IR) methods in terms of retrieval performance.  They also show that the score of the retrieval is more important in the scoring phase than in the retrieval phase, and that the TF-IDF weights are more sensitive to the amount of information in the documents.  Finally, they show that a pre-trained embedding-based Transformer model can be trained on a number of pre-train tasks that are similar to the ones used in previous work. They show that Transformer models trained with BM-25 and other embedding models outperform the previous state-of-the-art models on three paragraph-level pre -training tasks: Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP).  "
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"Graph neural networks have been widely used for several applications, including learning relational representations, modeling data on irregular domains (point clouds, social graphs, molecular structures, etc.), and modeling data from irregular domains. This paper proposes a new graph convolution operator that can be applied to existing graph neural network architectures. The authors show that the representational hierarchy can be learned from a set of graph convolutions operations and non-parameterized pooling or expansion layers, which can be used to learn a representational hierarchies. The paper also shows that the bipartitegraph convolution operation (a parameterized transformation of the graph) is a special case of the bipartisan convolution, which is a generalization of existing convolutional network architectures, including parameterized strided and transpose convolution operations, skip connections, and graph autoencoders. The proposed framework is able to learn multi-graph aggregation, and the authors demonstrate that the proposed framework can learn flexible and adaptable network architectures (e.g., BiGraphNet), which can learn hierarchical architectures with a single convolution and pooling, with different memory requirements, and with different number of layers. They also show that a BiGraphnet formalism (ii) allows to learn different architectures with different types of modeling flexibility, and can be combined with existing methods (i.e., graph convocation, single parametric bipartites, and skip connections).   "
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"This paper proposes a new metric function for feature embeddings to improve the performance of metric-based few-shot classification algorithms. Few-shot learning has been a hot topic in recent years, and there is a large body of work on the problem of domain generalization. This paper focuses on the domain shift problem, and proposes a learning-to-learn (L2L) framework to improve performance of existing few shot learning methods.    The paper proposes to use feature-wise transformation layers to transform image features using affine transforms, and then use the learned feature distribution as a metric function. The paper shows that the proposed learning to learn approach is able to learn the hyper-parameters of the feature-wide transformation layers, which can be used to improve feature distributions.  Experiments are conducted on three standard few shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. The results show that the use of feature-wise transformation layer improves performance of the metric- based models, and that the feature distribution of the learned features is more robust to domain shift. "
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper proposes a new approach for Lagrangian fluid simulation using a convolutional network. The approach is based on the observation that moving particles in fluids can be represented as moving particles with a graph structure, and the authors propose to use networks to model these moving particles. The authors use N-D convolutions to represent the continuous domain, which is a generalization of previous approaches. They also propose a network architecture that can handle arbitrary collision geometries and solve inverse problems. Experiments show that the proposed continuous convolutions outperform prior formulations in terms of accuracy and speed.  The authors also show that spatial convolutions can be used as well."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes a new ensemble method called BatchEnsemble1, which is a variant of Ensembles. The authors argue that the ensemble method can achieve better accuracy and predictive uncertainty than single neural networks, while having a fraction of the ensemble’s cost. The main idea is to use a mini-batch, where each batch is composed of a number of mini-batches, and the weight matrix is computed using the Hadamard product. Compared to ensembles, the authors show that Batchensemble achieves speedup and memory reduction on CIFAR-10 as well as on out-of-distribution tasks. They also show that the performance on lifelong learning on Split-ImageNet shows that the proposed Batch ensemble can be used for lifelong learning with sequential learning tasks, where the ensemble performs better than progressive neural networks in terms of computational and memory costs.  "
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network-based partial differential equations solver for solving forward and inverse problems. The proposed solver is mesh free and shape free, and the authors propose an unsupervised approach where the solution is learned by training a deep neural network. The solution is obtained by learning an explicit smooth differentiable function in an analytical form, which can be used as a strong PDE solution. Robust boundary conditions constraints and regularizers are used to ensure that the boundary conditions satisfy the desired boundary conditions. The authors also propose two numerical methods: finite differences and finite elements. The algorithm is able to solve both forward or inverse problems in a single pass, and can be applied to free shape 2D second order systems (e.g. for Electrical Impedance Tomography (EIT) and diffusion and wave equations).   The authors provide a theoretical analysis of the network and show that the proposed framework is a generalization of an existing framework. The paper also shows that the method can be extended to the case where the derivatives of the desired function are non-trivial and the loss function is non-convex.  The proposed method is applied to the problem of electrical impeding torsions and diffusion equations, and is shown to be able to learn free-shape 2D 2D systems, and to solve free shape two-order systems.   "
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper introduces a new class of networks called Binarized Neural Networks (BNNs), which are neural networks that can be seen as a special case of deep learning. The authors show that they can be used to train logic-based reasoners (e.g., SAT solvers) and explainers (i.e., Boolean logic) and that these tools can be applied to existential and probabilistic queries for explanation generation. They show that a BNN architecture, training procedure, and the number of parameters of the network are sufficient to train a network that can solve existential and Probabilistic Questions. They also show that their approach outperforms previous work on existential and probing queries in terms of accuracy.   "
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,This paper studies the expressive power of graph neural networks in the message-passing framework (GNNmp) of the node attributes and layer expressiveness. The authors provide lower bounds on the expressiveness of GNNmp. They also provide a technique for computing impossibility statements for certain types of problems. They provide an approximation for a number of tasks that require distributed computing. 
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper introduces localised generative flows (LGFs), a new family of flow-based density models that use continuous bijections to model target distributions with complicated topologies. LGFs are stacked continuous mixtures of bijection, and the authors propose a variational scheme for learning the bijection. The authors show that the proposed method outperforms existing flow -based methods for training the LGF model. They also show that normalising flows outperform LGFs on density estimation tasks, and that the log likelihoods of LGFs can be improved."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper proposes a method to train an agent to follow naturallanguage instructions in an environment where the goal is to follow step-by-step navigational instructions. The authors propose to use a combination of environment re-splitting and feature replacement to perform diagnosis experiments. The main idea is to use language and a navigational graph as input to the neural agent models, and use ResNet features to model the low-level visual appearance of the environment. The semantic representations of the semantic representations are then used to capture the low level visual information. The agent is trained on a set of datasets, including R2R, R4R, and CVDN, where it is shown that VLN is able to learn to follow the instructions in the environment, and that the agent can be trained on the features learned by the baseline agent model and the training method. The paper also shows that the state-of-the-art models are able to generalize well to unseen environments. "
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper proposes a system that uses implicit human feedback (i.e., error-related event potentials) to train a DRL algorithm in an Atari-type environment, where the agent’s learning for RL tasks is based on implicit feedback from non-expert humans. The system learns state-action pairs conditioned on the human feedback from the system, and then uses this system to train an RL agent that is able to generalize well to new environments.    The paper proposes to use the system as an auxiliary reward function for learning of the game, and uses the learned state and state action pairs from the human as an indicator of the state of the system. The paper also introduces a new definition of a game, which is defined as a game in which the agent is given the ability to learn from human feedback. In this paradigm, the RL agent is encouraged to learn a game that is similar to a game played by humans, but with the goal that the agent can generalize to a new environment.  In order to do so, the paper introduces two novelties: (1) the use of event-related electric potentials, and (2) using an electroencephalogram (EEG) cap to control the error-potentials and using them as an additional auxiliary reward to train the D RL algorithm for the learning of a new game.  Experiments on synthetic and real user experiments show that the proposed approach works well in a variety of complex environments (games) that require implicit human information, and that require expert labeling and demonstrations. In particular, it is shown that the RL algorithm trained with DRL can generalizes well to more complex environments, and generalizes better to new complex environments. It is also shown that this approach generalizes to new games, and is more robust to changes in the environment (e.g., changes in environment dynamics).  "
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper studies the problem of laconic classification for diverse image classifiers. The authors propose three reductions for classification: crop, colour reduction, and resolution reduction. They show that a classifier trained on the ILSVRC test-set can learn an approximate minimal-entropy positive image from a single classifier. They also show that complementary frameworks can be used to train both human and machine classifiers on minimal-Entropy positive images. The paper also shows that machines trained on cropping, reduced colour, and reduced resolution are more sensitive to cropping and reduced colour than human classifiers, and that there is a texture bias in the learned representations of machine classifying images. "
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619," adversarial examples can be generated by small perturbations in the decision space. The authors propose two new defenses for Convolutional Neural Networks, which are based on the instability assumption in existing defense techniques. Specifically, the authors propose deterministic lossy compression algorithms and randomized perturbation to improve the robustness of existing defenses. Experiments are conducted to validate the effectiveness of the proposed defenses.   "
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper proposes a new view prediction task for 3D visual recognition based on view prediction. Predictive coding theories have been applied to this task before, but this is the first paper to apply them to the case of view prediction, which is one of the most important prediction tasks in the field.    The paper proposes to use neural 3D mapping networks on 2.5D (color and depth) video streams from a moving camera, which are generated from a 2.2D (depth and color) moving camera.  The key idea is to use contrastive prediction losses instead of the standard color regression loss, which has been widely used in previous work. The proposed model is able to learn visual representations for semi-supervised learning of 3D object detectors, and for unsupervised training of a 3D moving object detectors. The paper also proposes a scalable self-supervision task called view prediction as a way to improve the performance of a view prediction for the task.  Experiments are performed on complex photorealistic data, where the motion of the inferred 3D feature maps is captured in videos of dynamic scenes, and retinas are used to map the scene to the 3D space. "
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of Unsupervised Domain Translation (UDT), where the goal is to learn a mapping from a source domain to a target domain without implicit biases. The authors propose an Optimal Transport (OT) framework for UDT, where the implicit bias is that the map between the source domain and target domain has a low energy transformation. They show that existing approaches to learn such mappings have theoretical guarantees, and propose a model that can be applied to such applications.    The key idea of the proposed approach to UDT is to use a dynamic formulation of OT, similar to CycleGAN. The key difference between CycleGAN model and the authors' approach is that CycleGAN models are trained to learn the mapping between source and target domains, while the authors propose to train a neural network that learns the mapping from source to target using hidden layers in the neural network.  The authors show that this approach can be used for a variety of UDT problems, including image captioning, natural language translation, and domain translation. They also show that the mapping learned in domain translation can be transferred to other problems that require the translation to be performed on the target domain.  In addition, they show that their model can be adapted to a new task by learning a dynamical formulation of the mapping, and that the resulting model is more robust to unwanted pairings in the input data, and can be trained with networks of minimal complexity.  Finally, the authors demonstrate that the proposed model outperforms CycleGAN-like models trained using Optimal transport theory, and is able to learn well-behaved mappings to the target domains. The paper also shows that the model is robust to the presence of unwanted pairs of pairings (i.e. pairings that have low energy transformations) in the target and source domains, and the model can learn to learn to map the source to the source domains without unwanted pairs.  They also provide a theoretical analysis of the UDT problem, showing that the objective function can be learned in a way that ensures that the learned mapping is invariant to pairings of the source and the target. They further show that, under certain conditions, the model learns to learn mappings that are invariant under the mapping. "
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a new regularization method called RotationOut to improve the performance of neural networks. Unlike Dropout, which regularizes each neuron/channel separately, RotationUp regularizes the whole network. The authors show that RotationThrough regularizes convolutional layers and recurrent layers, while Dropout only regularizes a single neuron or channel. They also show that the co-adaptation reduction can be performed with RotationOver and Dropout. The paper also shows that RotoOut/Dropout/RotationUp and Batch Normalization can be combined to improve performance on vision and language tasks. The main contribution of the paper is a noise analysis method to analyze the effect of RotationIn and RotoDropout regularization.   "
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a novel method for crafting Universal Adversarial Perturbations (UAP) for CNN. Data-free approaches have been shown to be effective at crafting adversaries, but the problem of adversary generation is challenging due to nonlinearity of the perturbation. This paper proposes to use a ReLU activation function to generate UAPs.    The key idea is to use sequential optimization to generate adversarial perturbations using dilate loss, which is based on Euclidean norm.  Dilate loss is defined as a function of the number of samples and the size of the data set.  Experiments show that the proposed method has a better fooling rate than existing data-free work, especially in limited data cases. "
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper studies the problem of Neural Architecture Search (NAS) in artificial intelligence areas, where the goal is to find the best architecture for a given task from a set of hand-designed networks. Previous NAS methods have been shown to be effective in this setting, but the searching cost can be prohibitively expensive. In this paper, the authors propose a new NAS method called T-NAS, which is a transferable neural architecture search method based on meta-learning. The idea of NAS and fast adaptation of neural architectures is interesting, and the proposed method is well motivated. The experiments on few-shot learning and supervised learning demonstrate the effectiveness of T- NAS in finding the best meta-architecture for a specific task."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes Stochastic neural networks (SNNs), a family of neural network variants that can be seen as a generalization of existing paradigms (e.g., dropout, Bayesian neural networks, variational information bottleneck (VIB), noise regularized learning, etc.). The authors show that these networks are robust to adversarial attack, robust to label noise, and generalize well. They also show that SNNs trained with pruning and adversarial defense outperform SE-SNN in terms of generalization, network compression, robustness, and learning with label noise. Finally, they show that pruning can be used to improve the performance of network compression through pruning, and that discriminative learning can also be used."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta-learning approach for learning curiosity mechanisms for reinforcement learning. The core idea is to train an agent’s reward signal using a set of curiosity mechanisms, and then use this reward signal to guide the inner loop of reinforcement learning by transferring neural network weights to the outer loop. The authors claim that this approach is useful for generating curious behavior that can be leveraged for meta-learning.   The authors propose a rich language of programs that includes neural networks, buffers, nearest-neighbor modules, and custom loss functions. They show experiments on grid navigation, grid navigation with image inputs, acrobot, and ant, and show that their curiosity algorithms outperform human-designed published curiosity algorithms. They also show that the evolution of these algorithms is similar to that of existing ML papers.  The paper is well-written and well-motivated, and the paper is clearly written.  However, there are a few issues with the paper:  1. The paper does not provide a clear definition of curiosity, which makes it difficult to understand the approach.  2. It is not clear how to compare the performance of different meta-learner algorithms.  3. There is a lack of discussion about the benefits and drawbacks of the approach, and it is unclear whether the approach can be applied to more complex problems.  4. The approach is not well-suited for transfer learning, as it does not seem to be able to transfer across different tasks. "
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"This paper proposes a new approach for AnyC2C, an approach for learning a code snippet from the strict syntax of programming languages using tree–structural language modeling (SLM). The problem is interesting because it can be seen as a special case of the task of learning a sequence of expressions that encode structural information in a programming language. The authors propose a neural model that uses AST paths to learn the conditional probabilities of the expressions, which are then used to train a SLM. They show that their model outperforms seq2seq and other structured approaches for Java and C# code, and that their approach is more interpretable than other structural techniques for learning expressions."
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper studies gradient descent methods for non-convex optimization problems in large-scale neural networks (NN). The authors consider the case where the objective functions of NNs are non-vanishingly convex. They show that the gradients of a pointwise linear transformation (i.e., the disparity matrix) between the NN model space and the canonical space is non-zero, which is a result of the fact that the canonical model space is not a full-rank space. They also show that under this setting, large NNs can be seen as over-parameterized NNs. The authors then show that gradient descent algorithms can achieve a global minimum of zero loss using gradient decent algorithms. The main contribution of the paper is that the authors prove that under the full rank condition on the disparity matrices, the learning of such NNs is asymptotically equivalent to normal convex optimization. "
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"This paper proposes a novel method for learning deep learning based segmentation models from large-scale ground truth data sets. The authors propose interactive graph-based segmentation algorithms that encourage connectivity between different nodes in the graph. They propose a discrete Potts model based on an instanceaware heuristic, where each node is represented as a global optimum. The algorithms are trained on RGB and use feature maps from a DCNN. They are evaluated on semantic (and panoptic) segmentation on PASCAL VOC 2012 and the Cityscapes dataset. The interactive approach is also evaluated on the VOC validation set for mIoU. They also propose a weakly supervised learning framework, where they use a few scribbles to guide the learning process. They demonstrate that interactive annotation can be used to improve the performance. "
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper proposes to use saliency tools to detect adversarial examples based on salient features of an image. Adversarial perturbations to the saliency map of the image can lead to misclassification. The authors show that adversarial defense can be achieved using gradient-based saliency tool, and that it outperforms a baseline based on the same model. They also show that real-time defense with learnt saliency models can reduce the computational cost of saliency.    The paper also shows that a real defense against adversarial attacks can be performed with a learnt salient model. The defense is based on a CNN trained on salient pixels of the adversarial images and natural images, and the authors demonstrate that the learned saliency model can be used to improve the performance of the defense. They evaluate the defense on MNIST, CIFAR-10, and ASSIRA, and show that the proposed defense outperforms the baseline trained with the salency map. They further show that weak defenses can be applied to adversarial image generated by attacks such as C&W and DeepFool. "
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper studies the global adversarial robustness guarantees of machine learning models. The authors consider the notion of local robustness properties, i.e., the measurability of a model to adversarial attacks, and derive concentration inequalities for global robustness. They show that the robustness/accuracy trade-off between neural networks architectures and training methods for MNIST, Fashion-MNIST, and CIFAR is highly correlated. They also show that networks trained with stochastic gradient descent and iterative pruning techniques can achieve robustness and accuracy trade-offs that are highly correlated with each other. They further show that these methods are Bayesian and that they can be combined with existing methods. Finally, they show that both of them work well in Bayesian settings and that the estimation error is low. "
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the problem of robustness to environmental dynamics in learning algorithms. In particular, the authors consider the robustness of system dynamics (e.g., transition probability) to perturbations in the environment. The authors propose a novel robust learning algorithm, called Wasserstein Robust, which is based on the observation that the optimal policy should be robust to environmental perturbation in the presence of a transition kernel disturbance. The disturbance is modeled as a Wassersteinsatz distance between the simulated environmental parameters and the reference transition kernel, and the authors prove that the Wasserststein distance is a function of the disturbance.  The authors then propose an infinite-dimensional optimization problem and a finite-dimensional risk-aware problem, and prove that optimal robust policies can be found by solving the risk-adversarial optimal Bellman equation. They also provide a sensitivity analysis of the perturbated state of the system, and propose a new algorithm, named WASSERSTEIN Robust. The proposed algorithm is tested on the Cart-Pole environment, and is shown to be robust against the transition kernel and state disturbance, as well as to be sensitive to state disturbance."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method for finding mixed strategy Nash equilibria in multi-player continuous games, which is an important problem in the context of deep learning based approaches for finding a pure strategy Nash equilibrium for multi-agent games. The proposed method is based on the observation that the mixed strategy in continuous strategy spaces can be approximated by a pushforward measure technique, and the authors propose a method that combines the pure ones with the mixed ones. The mixed strategy is approximated in continuous spaces, and a convexity assumption on the payoff functions is made. The authors also propose a gradient descent algorithm for the proposed approach, and show that the approach converges to a stationary Nash equilibrium. The method is tested on quadratic games, blotto games, and GAMUT games, where the proposed method outperforms previous works in finding a Nash equilibrium, and is shown to be more robust to the pure strategy weakness in the case of generative adversarial networks.   "
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"Natural language (NL) explanations can be used to augment deep neural networks for NLP tasks that require data annotation. However, the amount of labeled data for training data-hungry models can be prohibitively expensive, especially for tasks where sufficient domain knowledge is needed to obtain sufficient supervision for the labeled data. Natural language explanations (NL explanations) have been shown to be useful for augmenting model learning by augmenting them with additional supervision from NL explanations.   This paper proposes a Neural Execution Tree (NExT) framework1 for text classification that uses NL explanations from different linguistic variants to augment the model learning. The idea is to use a modularized model to learn the semantics of the NL explanations, and then use them to augment augmenting the training data. NExT is able to generate executable logical forms based on NL explanations using semantic parsing, which are then used to perform actions based on the logical forms. Experiments are conducted on a number of standard NLP benchmarks, including relation extraction, sentiment analysis, and multi-hop question answering, where NL explanation is used to improve the performance of the baseline methods. The results show that the proposed method can reduce the annotation time by a factor of at most 1.5. "
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"Formal verification of machine learning models is an important problem. Formal verification aims to verify two properties: (1) robustness to perturbations of the input features, and (2) misclassification in the presence of adversarial robustness. This paper proposes a new verification procedure, called verifiably robust models, which is based on the notion of desired specifications. Specifically, it can be applied to recurrent neural network architectures and complex specifications (i.e., specifications for temporal properties) and it allows for verified training of models that are robust to adversarial attacks. The paper also proposes a verified training method that can be used to train models that have been trained with adversarial training. Experiments show that the proposed training method is able to verify that models trained with this training are robust against adversarial examples.  "
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper studies the problem of domain randomization in reinforcement learning. The authors propose a novel method to regularize the randomness of the policy distribution of the learned state representations. The proposed method is based on the idea of visual domain randomisation, where the goal is to find a constant that maximizes the generalization performance of policies trained on the visual domain.  The authors show that the proposed regularization method can be applied to any randomization parameters. They also show that their technique achieves better generalization scores than the state-of-the-art method of standard randomization. "
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"Deep metric learning (DML) is an important problem in computer vision. Deep metric learning is a special case of deep learning in which complicated losses and hard example mining methods are considered. DML is a pairwise binary classification problem, and the authors consider the problem of imbalanced data pairs. In this paper, the authors propose a new framework to learn a model that is robust to imbalanced pairs. The authors propose to use distributionally robust optimization to learn the robust loss. They also propose two variants of complicated losses based on the uncertainty decision set of the dual variable. Experiments on benchmark data sets show that the proposed method outperforms the state of the art. "
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"This paper considers the problem of finding the local minimum in non-convex finite-sum minimization. The authors propose a trust region method based on inexact gradient and Hessian estimation, and show a convergence rate of O(1/\sqrt{T}^T) and O(T^T). They also show that the (,√)-approximate local minimum can be found using the stochastic trust region (STR) algorithm. The main contribution of this paper is that the authors show that Hessian-free STR algorithms have a runtime complexity of O(\sqrt{\log T}) and O(\log T) times faster than existing algorithms.   The authors also provide a theoretical analysis that shows that the differential estimations of the Hessian of the local minimizer can be approximated by a simple Hessian estimator, and that the convergence rate depends only on the number of times that the Hessians are estimated.  Finally, the authors provide some numerical experiments to show the effectiveness of the proposed algorithms."
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper proposes a geometrically motivated method to reduce the size of deep neural networks. The proposed method is motivated by the observation that batch normalization and weight initialization are the main causes of over-parameterization in training neural networks, and that Farkas layers can be decomposed into linear programming. The authors show that the ReLU activation of residual networks can be reduced to linear programming, which can be used to reduce network sizes on standard benchmark datasets. The paper also shows that the proposed method can reduce the training capacity by up to 1.5x while maintaining the same initialization.  "
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper proposes two computationally-efficient robustness certificates for deep classifiers based on differentiable activation functions. The idea is that the decision boundary of a classifier is a function of the eigenvalues of the Hessian of the parameters of the network, and the robustness certificate is defined as the minimum distance between the classifier’s decision boundary and the boundary of an adversarial examples of the same classifier. The authors prove a lower bound on the l2 norm of the robusts certificate, and show that it can be computed efficiently via nonconvex optimization. They also show that the curvature of a deep network can be used as a computationally efficient differentiable upper bound for the certified robustness of a network.    The authors also propose Curvature-based Robustness Certificate (CRC) as a regularization term for a network based on curvature, and CRT as an efficient regularizer for a trained deep network. They show that CRT achieves better certified accuracy on adversarial attacks compared to adversarial training, and that the proposed regularizer is more effective than CROWN’S certificate, which is based on the classification output. "
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a method for compressed sensing recovery from untrained deep generative models. The proposed method is based on Deep Image Prior (DIP) and uses a learned regularization technique to incorporate prior information into the network weights. The approach is formulated as a differentiable linear inverse problem, and the authors show that the reconstruction error can be reduced to zero when the convolutional weights of the network are learned during pre-training. Compared to existing approaches that use differentiable models, the proposed method achieves better results. The authors also propose a DIP optimization approach for overparameterized single-layer networks, which can be applied to unlearned methods. The paper also proposes an early stopping to prevent over-parametrized networks from overfitting to noisy measurements.   "
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes a hierarchical reinforcement learning (HRL) approach for long-horizon RL problems with long-range dependencies. The authors propose a new algorithm TAIC, which is based on the TAIC framework. TAIC is a hierarchical RL algorithm that learns a latent representation of the long-term history of a sequence of actions. The paper shows that the proposed algorithm achieves better sample efficiency and convergence rate compared to other HRL algorithms.  "
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"Graph Convolutional Network (GCN) is a popular technique for graph representation learning, especially in graph-based applications. However, the time complexity of shallow models on small graphs can be prohibitively slow due to the lack of local bi-directional influence (correlation) in the mini-batch of nodes. This paper proposes a layer-wise sampling strategy to speed up the training of GCNs by using acceleration methods. Specifically, GCN-like models are trained on larger graphs and deeper layers, and a self-attention mechanism is introduced to reduce the number of sampled nodes. The proposed model combines first-order and higher-order proximities in the single layer propagation process, which is achieved through recursive propagation and skip connection. The model is evaluated on large benchmark graphs and shows promising results. "
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper proposes STOVE, a state-space model for videos that is able to capture velocities and interactions in a physical system. It uses an image model and a dynamics model for inference. The authors claim that models trained with this model are more robust to changes in physical system dynamics, which can be useful for regularizing training. Experiments are conducted on a series of simulated and real-world robotic control tasks, where the model is trained for model-based control, and compared to supervised baselines. The results show that STOve outperforms other unsupervised models in most cases, and can capture physical behavior more accurately."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"This paper proposes an autoencoding model that combines the advantages of variational autoencoders (VAE) and generative adversarial networks (GAN). In particular, the authors argue that GANs are prone to the mode collapsing problem, where the model distribution of the training data converges to the same distribution as the target distribution. The authors propose a novel approach to address this issue by replacing the explicit likelihood (such as Gaussian or Laplace) in the VAE with an implicit likelihood, which is based on an adversarially trained discriminator. The model is trained to minimize the λ-Jeffreys divergence between the target and source distributions. It is shown that the proposed approach can be used to train a VAE model that achieves both mode-seeking and mass-covering behaviour on the CIFAR-10 and TinyImagent datasets.    The authors also propose a new objective for VAE training, which they call “adversarial training”, and show that it can be applied to any VAE loss. "
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"Adversarial attacks on CNN classifiers are known to be effective when the geometry of high dimensions (i.e., digits) of the data distribution is different from that of low dimensions (e.g., the optimal decision boundary of the classifier). This paper studies the problem of unreasonably linear extrapolation in CNNs. The authors propose a Bayes-Optimal classifier that is robust to attacks on the optimal classifier on two datasets. They show that under certain assumptions on the geometry and the class distributions, the optimal classesifier has a smooth decision surface. They also show that it can be trained on adversarial examples.    The authors show that adversarial vulnerability in machine learning is not due to CNN training, but rather to suboptimal training methods. In particular, they show that large-margin methods can be used to train a classifier which is more robust to adversarial attacks."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the effect of Neural network pruning techniques on the top-1 accuracy of sparse and non-sparse models. The authors show that the top1 test set accuracy can be improved by pruning identified exemplars (PIEs) from the network. They also show that pruning is beneficial to abstract representations, which is useful for fine-grained classification.   The authors also find that the sparsity of PIE images is correlated with the image quality, and that hard-to-generalize-to images are more likely to be pruned. "
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper studies the problem of text-based simulations (Interactive Fiction games) in which natural language is used as a resource for reinforcement learning agents to improve natural language understanding, partial observability, and action generation in combinatorially-large text -based action spaces. The paper proposes KG-A2C1, which extends the template-based action space of [1] to a dynamic knowledge graph, where the knowledge graph represents the game state, and natural language generation is based on a knowledge graph of the current state of the game. The authors show that KG - A2C outperforms existing IF agents in a number of IF games, and is able to generate combinatorial large natural language actions in games where the action space size is very large. They also show that they are able to generalize to more complex games. "
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper studies the problem of maximum likelihood estimation (MLE) in the context of sequence prediction problems such as language generation. The authors propose a novel Kullback–Leibler divergence term, which is based on a data-dependent Gaussian prior and detailed training prediction. They show that it can be used to mitigate the negative diversity ignorance, i.e., that the MLE loss is dominated by the prior topological order of tokens. The proposed method is evaluated on a variety of language generation tasks including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning."
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper proposes a new calibration approach, called Temperature scaling, to improve the calibration of a DNN. Miscalibration is a common problem in Deep Neural Networks (DNNs) and the authors propose to use temperature scaling to calibrate the DNN in order to improve its performance on downstream tasks.   The main idea is to use cross-entropy loss and focal loss for calibrating the models.  The authors show that temperature scaling, focal loss, and calibration can improve the accuracy and calibration of calibrated models. They also show that the confidence of a model trained with focal loss can be improved by temperature scaling.  Finally, the authors evaluate their approach on a variety of network architectures and show that their approach improves the accuracy on standard NLP (SST, 20 Newsgroup) datasets. The authors also demonstrate that the proposed approach can be used to improve both the accuracy as well as calibration. "
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper studies the Lipschitz constant of neural networks. The authors propose LiPopt, a polynomial optimization framework that uses sparse connectivity in the network to reduce the complexity of computation. They show that the proposed approach outperforms the baselines in terms of `∞-Lipschatz constant. They also show that networks trained with random weights and networks trained on MNIST can be pruned to achieve the same `*∞***’.    The paper is well-written and well-motivated. The optimization problems are well-structured, and the paper is clearly written. The results are interesting and convincing. The paper also shows that convolutional as well as pruned neural networks can benefit from the proposed method. "
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self-supervised learning approach to learn video features. The proposed self - supervised learning approach outperforms existing methods on three tasks: video classification, captioning and segmentation. The method uses sequences of real-valued feature vectors extracted from a BERT model trained on text sequences. The key idea is to combine the softmax loss with noise contrastive estimation (NCE) to improve the performance of cross-modal training. Experiments show that the proposed method can learn representations from sequences of visual features and sequences of words extracted from automatic speech recognition."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper studies the problem of data transfer during the inference phase of machine learning models. The authors propose a new framework, which is based on the observation that the data is not always available in the public storage server, and that the model may not be able to transfer well when the data transfer is limited. To address this problem, the authors propose to use selection masks and a neural network to share information between the model and the server. The paper also proposes a way to share the masks between the server and the model."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a new method for out-of-distribution (OOD) detection. The method is based on the Outlier Exposure (OE) technique, which is a technique for detecting OOD examples in the training of deep neural networks for classification tasks. The authors propose a methodology to train a neural network so that it is able to detect out of distribution (ODD) examples. They propose a loss function based on OE to improve the performance of OOD detection on image and text classification tasks, and show that the proposed loss function is more robust to novel class distributions than existing classification algorithms. The proposed method is tested on a simple OLD detection task and is shown to outperform a Mahalanobis distance-based classifier in terms of classification accuracy. "
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes E2Efold, an end-to-end deep learning model for RNA secondary structure prediction. The authors propose an unrolled algorithm for constrained programming, which learns deep architectures to satisfy the constraints of the RNA base-pairing matrix. They show that E1Efold outperforms SOTA for pseudoknotted structures, and it outperforms other algorithms in terms of inference time on several benchmark datasets. "
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper studies the problem of learning policies that are robust to biases in a virtual simulation. The authors show that agents’ simulations are biased in the sense that they learn biased representations of the environment. They then propose a collective policy that is robust to these biases. They show that such collective policies are more robust than individually trained policies. They also show that in a real-world environment, the bias is more pronounced in internal simulations. "
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper tackles the problem of open-domain dialog generation, i.e., Generic responses. The authors propose a dialog generation model that learns a semantic latent space, which is then used as a one-to-one task to solve the generic response problem. The model learns semantically related responses in the latent space. The features extracted from the prompt are used to train an autoencoder to generate the features of the response. A regression task is used to learn the pair relationship between the response and the prompt. The proposed model is compared with several baselines on the coherence of the generated responses, and is shown to outperform the baselines when the MLE loss is used."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper proposes Gaussian light and shadow (GLAS) which is a salient explanation method that uses feature perturbation to improve the performance of deep models. The key idea of GLAS is that the discriminative features of the input image can be used as salient explanation for a fine-grained classification task, and that it can be applied to life-affecting decisions. The authors show that GLAS improves coarseto-fine control through the scalability of Gaussian mask. They also show that the Gaussian masks are more robust to perturbations in the input images.  The authors evaluate GLAS on the fine-rigorous classification dataset on the ImageNet Large Scale Visual Recognition Challenge, and show that it improves the performance on the 224×224 image. GLAS also shows improved performance with high speed compared to the previous state-of-the-art method, recursive GLAS.   "
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"This paper proposes a novel method to remove redundant data from convolutional kernels in convolution neural networks. The authors propose a method called ""deconvolutional deconvolution"" that removes redundant information from the convolution kernel. The method is based on the observation that the center-surround structure of biological neurons in the visual regions of the brain is similar to that of convolution filters in a neural network. The paper also shows that the method can be applied to any convolution kernels, and that it can be combined with existing techniques to reduce the computational cost of a convolution layer."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"This paper proposes a new quantization method for generative adversarial neural networks (GANs) based on neural network quantization methods for GANs. Previous work has shown that convolutional neural networks [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21] can be quantized to 1-bit or 2-bit representations, but most of them require extreme low bits (e.g. 4 bits or less) which is not practical for edge devices such as smartphones. This paper shows that CNN quantization algorithms can be used to reduce the number of quantization bits required by GAN models.   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   This paper proposes QGAN, which is a quantization algorithm based on EM algorithms [1]. The authors show that QGAN is a generalization of previous work [1], and that it can be applied to a wide range of GAN methods. The authors also propose a multi-precision algorithm to improve the quantization precision of the GAN.  [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [34] [35] [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [49] [50] [51] [55] [56] [57] [58] [59] [60] [63] [64] [65]  The authors claim that the proposed QGAN outperforms existing models on CIFAR-10 and CelebA, and is competitive with them on image qualities requirements. "
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies the problem of convex-concave min-max optimization. In particular, the authors consider nonconvex applications such as Generative Adversarial Networks, where the goal is to obtain global last-iterate convergence rates in bilinear and convex - strongly concave settings.   The authors consider two natural algorithms, namely Simultaneous Gradient Descent/Ascent and Hamiltonian Gradient Decentraction (HGD).   They show that the HAMILTONIAN GRADIENT DESCENT (HDG) algorithm has a linear convergence in the convex case, which is a result of the “satisfying” “subsatisfied” condition on the number of steps in the Consensus Optimization algorithm.  They also show that for convex concave problems, the convergence rates of the stochastic HGD are linear.  The main contribution of this paper is that the authors provide the first generalization of the average iterate convergence results to the case where the last iterates of SGD and SGD-HGD are non-linear.  This is an interesting result, and it is an important contribution to the literature.  In addition, they also provide a theoretical analysis of the convergence rate of the last step of the SGD algorithm in the case that the last steps are nonlinear, and show that it converges to a stationary point.  Finally, they show that in the setting where the data is convex, the last-strongly concave, and non-convergentive, they provide last-th iterates for SGD.  For the case of training GANs and GAN, they give a theoretical explanation for the convergence of GAN-SGD to stationary points. They also provide an explanation for why training GNNs is a non-trivial problem, and provide a proof of convergence for the case when the data are convex.  As a result, they conclude that the convergence results are tight, and they also give a proof for the last iteration of the Hamiltonian gradient descent algorithm."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of ResNet under the over-parameterization requirement. The authors consider the ResNet structure, where each ResNet block hl is a ReLU activation, and each block is a residual block. They show that gradient descent converges to global minima when the number of residual blocks is large enough. They also show that if the residual blocks are large enough, then the forward process converges in a similar way.    The authors also show the global convergence of a ResNet with a normalization layer, which is similar to the vanilla feedforward network. The main contribution of this paper is that the authors show that the stability in ResNet can be improved by adding a regularization layer to deep ResNet. "
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with wall clock inference times. Sparse neural networks have been shown to be more efficient than dense networks, and they can be used to speed up wallclock inference times for inference. However, dense-to-sparse training methods tend to have a trade-off between accuracy and computational cost, and the authors propose a trainable sparse model. The proposed method uses a fixed parameter count and a fixed computational cost to train a sparse network. The method learns the topology of the network based on parameter magnitudes and infrequent gradient calculations. The authors show that the proposed approach can achieve better accuracy with fewer floating-point operations (FLOPs) compared to prior techniques. The paper also shows that sparse training can be performed on ResNet-50, MobileNet v1, Mobilenet v2, WideResNets, and RNNs on the ImageNet-2012 dataset, and on the CIFAR-10 dataset, as well as on the WikiText-103 dataset. In addition, the paper shows that the method is able to learn a topology based on the number of local minima, which is more suitable for optimization.  "
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"This paper proposes to use deep generative models to generate photo-realistic images and visual or textual content embeddings, which can be used in both computer vision and natural language processing. The key idea is to train a generative process that can be applied to a variety of transformations, including translation, zoom, color variations, etc. The authors show that their approach is able to achieve state-of-the-art performance on both GANs and variational auto-encoders. They also show that the proposed approach can be combined with a BigGAN model, and that the generated images can be annotated with human annotations. "
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a new model for unsupervised physical parameter estimation of systems in video from video using only a few labeled states. The model is based on a physics-as-inverse-graphic approach, where the scene dynamics are modeled as differential equations, and the goal is to learn a differentiable physics that can be applied to any physical scene understanding methods that rely on object state supervision. The authors propose a framework for long term extrapolative video prediction, vision-based model-predictive control, and long-term future frame prediction of systems with interacting objects. The proposed approach is evaluated on a pendulum system with a vision-actuated model-based control and is shown to outperform existing un supervised methods for long-range future-frame prediction of such systems. In particular, the authors show that the dynamics in the model can be learned with an inductive bias and that the controller’s interpretability can be used for goal-driven control and physical reasoning for zero-data adaptation.   The authors also show that their approach is more interpretable than previous work that relies on vision-image-to-image translation, which is similar to the previous work, and that their model is able to capture the dynamics of the system in a single frame.  The main difference is that the authors propose to use a physics -as-intrinsic-rendering (PIG) approach, which uses a physics model of the state and velocity of the pendulum to learn the physics of the scene. This is a generalization of previous work (e.g. [1] from [2] that uses the physics-based approach to learn dynamics of a single pendulum, and [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]  The paper is well-written and well-motivated. The idea of the approach is interesting and the results are convincing. However, there are a few issues in the paper:  (1) the authors do not discuss the relationship between their approach and prior work, (i.e. [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,19) and (12) the use of vision-physics integration is not discussed. "
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"Graph Convolutional Networks (GCN) are used to model class relevance, i.e., the ability of a classifier to distinguish clean from noisy examples. In this paper, the authors propose to learn the structure of clean and noisy data from a single graph per class. The relevance measure is based on the GCN-implicitly inferred “clean” probability, which is a measure of how well the classifier is able to distinguish the clean from the noisy data. The authors propose a novel GCN that uses a weighted binary cross-entropy loss function to train a binary classifier. The proposed method is evaluated on the few-shot learning problem, and the authors show that the proposed method improves the classification accuracy of a GCN -based cleaning process on the standard classification accuracy for few-task classification. The paper also shows that the performance of the proposed GCN is comparable to that of a transductive approach.  "
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"Graph Neural Networks (GNNs) learn a representation vector that maximizes the Mutual Information (MI) between edge features and message passing channels. The paper proposes a differentiable objective for maximizing the MI, which is based on a variational approach. The authors show that the proposed objective can be used to learn a model that maximises the edge information of a model, and that MI-maximized models can be applied to a variety of learning tasks, including regression on molecular graphs and relation prediction in knowledge graphs. "
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper studies the certification of generative models, i.e., models that are capable of specifying visual transformations (e.g. Generative networks). The authors propose two verification methods to certify generative networks. The first verifier, APPROXLINE, is a verifier that verifies the non-convexity of the network’s latent space, and the second verifier is a deterministic and probabilistic abstract interpretation of the latent space. Both verifiers are based on the assumption that the network's latent space is non-consistent. "
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper proposes a spectral graph convolutional operator for graph neural networks (GNNs) to solve the so-called ""suspended animation problem"", which is the problem that models trained with deep architectures are unable to perform well on graph data due to the lack of model depth. GNNs have been shown to solve this suspended animation problem in the past, but this paper proposes to extend the model to the case where the model depth is much smaller than the suspended animation limit. The authors propose a GRESNET (Graph Residual Network) framework to learn the connected highways between nodes’ raw features and intermediate representations in a graph, and then use these intermediate representations to train model layers.    The authors show that under certain learning settings, residual learning methods can be used to preserve the graph residual terms from a norm preservation perspective. They show that existing GNN’s such as GCN, GAT, and LOOPYNET can all be trained using the GRESnet framework. They also show that the learned residuals can be reused across different types of graph data and different learning settings. Finally, the authors provide some experiments on several real-world benchmark datasets to show the effectiveness of the proposed method. "
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper proposes a new method for face reconstruction based on linear 3D morphable models (3DMM). The key idea is to use the face prior knowledge of a 3D model to guide the reconstruction process. The authors propose a new dataset for this purpose, which is built on top of the existing 3D Morphable Model dataset. The paper also proposes a semi-supervised adversarial training method to improve the performance of the proposed method. "
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"Model-based imitation learning methods rely on the transition kernel with partial knowledge. Reinforcement Learning (RL) has been shown to be useful for imitation problems, but policy evaluation is expensive. This paper proposes to use an eMDP to model the transition of state components between two states. The transition kernel of the transition model is known to be non-trivial. The authors propose to use the unknown transition kernel and a synthetic kernel to learn the transition between the two state components. Experiments on imitation tasks in multiplayer games show that the proposed policy gradient algorithm and the proposed model outperform a simulation-free alternative."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self-supervised reinforcement learning approach to learn skills that are useful for reinforcement learning. Learning useful skills with a manually-designed reward function is an important problem in reinforcement learning, and the authors propose a selfsupervised Reinforcement Learning approach to address this problem. The proposed method is based on intrinsic mutual information rewards, where the intrinsic objective is to maximize the mutual information between the robot state and the external reward function. The authors demonstrate that the proposed method achieves state-of-the-art performance on two simulated robotic manipulation tasks from OpenAI Gym and a navigation task from the Gazebo simulator. The key idea is to learn a pre-trained policy and a mutual information discriminator to guide the learning of task rewards. The pre-training is done by sampling from a set of context states, where context states correspond to robot states of interest, and mutual information is used to train the discriminator. The paper also shows that learning with sparse rewards can be more effective than using sparse rewards for a variety of robotic manipulation environments.   "
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a new type of attack, called neural network (NN) trojaning attack, which aims to fool NN models to misclassify the weight parameters of the model. Unlike adversarial attacks, it does not require a fixed target classes, and it can be applied to any malicious functionality. Previous work has shown that NN trojans attacks on small datasets are effective. This paper shows that a trojaned model trained on a large-scale dataset with a large number of classes is able to fool a large model on a small domain. The authors show that the trojan has a biased behavior, and that the malicious misclassification target is not fixed. They also show that this trojaning attack method can be used to fool large models as well. The paper also shows that the proposed trojaned attack method outperforms previous studies in terms of capability, generality, and stealthiness. "
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"This paper studies the few-shot regression (FSR) problems (e.g., modelling of biological assays) and prediction tasks in drug 1 discovery, which are popular prediction tasks with real-world constraints. The authors propose a new FSR 6 algorithm based on deep kernel learning, which is a generalization of existing FSR methods for a variety of tasks that are often considered in the literature. The paper also proposes a few applications of FSR and reinforcement learning methods to these applications. The algorithm consists of a deep network, a kernel function, and a differentiable kernel 8 algorithm. It outperforms state-of-the-art algorithms on a number of real-life benchmarks with noisy and uncertain environments (drug discovery, drug discovery, and data generation). It is also able to learn complex task distributions, and is able to generalize well to unseen tasks.    The algorithm is based on a kernel that is learned for each task, and the kernel is used in the inference of each task. The kernel is learned in the same way as in previous work, but the algorithm is differentiable, which means that the algorithm can be used to learn the kernel for any task.  The authors also propose an algorithm that combines the kernel function with the differentiable Kernel 8 algorithm, and show that this algorithm can generalize to unseen task distributions.  Experiments are conducted on a set of benchmarks based on biological assay and drug discovery. "
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper proposes a new framework for studying the expressive power of GNNs for reasoning tasks. The authors show that GNN-based models can be seen as a special case of dynamic programming (DP), which is a generalization of DP. The paper also shows that a GNN can be viewed as an extension of DP, and that it can be used to solve a number of reasoning tasks, including intuitive physics, visual question answering, and shortest paths. "
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new benchmarking metric for Conditional Generative Adversarial Networks (cGANs) based on the Fréchet Joint Distance (FJD) between the joint distributions of the conditioned and unconditioned conditional distributions. The authors argue that existing metrics for evaluating models (e.g., image quality, conditional consistency, intra-conditioning diversity, etc.) do not take into account the joint distribution of the two conditional distributions, and propose a new metric that does so. The proposed metric is based on Frerchet distance between two joint distributions, which is a measure of the distance between the conditional consistency of a pair of data points and their joint distributions.  The authors show that the proposed FJD is more robust to conditioning modalities (i.e., class labels, object masks, bounding boxes, images, and text captions) than existing metrics, and that it can be used to measure the properties of the joint and non-joint distributions. They also show that FJD outperforms existing metrics on a controllable synthetic dataset.  Finally, the authors evaluate the proposed metric for cGAN-based models on a variety of conditioningmodalities (class labels, class masks, and images). The authors also demonstrate that the FJD improves the performance of cGAN benchmarking and model selection using the new metric.   "
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper proposes the VIC framework, in which the empowerment objective is replaced by the identification of decision states that can be used to guide exploration in the context of exploration in downstream goal-driven tasks in partially observable environments. The key idea of the paper is to use extrinsic rewards to encourage exploration in an environment that is partially observable. The paper is well-written and well-motivated.   "
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a new architecture for irregularly-sampled and asynchronous time series on real-world datasets (e.g. healthcare applications). The authors propose a new framework for classifying irregularly sampled time series with unaligned measurements, which is an important problem for deep neural networks. The framework is based on differentiable set function learning, and the authors show that the proposed framework improves data efficiency and data efficiency in online monitoring scenarios. The proposed method outperforms competitors on a number of healthcare time series datasets, and it also improves the runtime in some cases."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper studies the inductive biases of convolutional neural networks (CNNs) for image recognition. The authors show that deep networks have similar audio signals that share the same inductive bias as natural image priors, but that their priors are more complex and more inductive.   The authors propose two network architectures for audio processing. The first is based on the harmonic structure of the input audio signal, and the second is built on the local neighborhoods of the convolution kernel of the harmonic series.  They show that these two networks are able to learn audio priors and that they can be trained with Harmonic Convolution. They also show that the networks can be used for unsupervised audio restoration. Finally, they show that they improve the generalization performance of supervised musical source separation with the help of the use of the proposed method, and demonstrate that they also improve generalization in the presence of noise. "
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper studies the problem of speeding up the training of neural network training on GPUs and specialized hardware accelerators on GPUs. The authors propose a new training pipeline, called data echoing, where the accelerators are shared across all the pipeline stages. Data echoing is used to reduce the amount of data that needs to be shared across the entire network during training. The paper shows that the benefits of data echoing are demonstrated on a number of different workloads, including disk I/O, data preprocessing, and batch sizes. In addition, the authors show that the data echoing algorithms outperform baseline algorithms on a variety of workloads. They also show that their data echoing algorithm outperforms a baseline that does not use any upstream computation. Finally, they show that ResNet-50 with wall-clock time on ImageNet is able to achieve state-of-the-art results on a few datasets.   "
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes a new formulation of the successor features problem. Successor features is defined as a generalization problem where the goal is to learn behaviors in a controllable subspace of a Markov decision process, where a policy is trained to generalize better than other policies. The paper proposes two techniques to tackle this problem: (1) learn a set of controllative features, (2) learn the reward function in a grounded feature space, and (3) use these learned features to guide the generalization and task inference.  The paper shows that the proposed algorithm, Variational Intrinsic Successor FeatuRes (VISR), is able to improve generalization performance on a number of tasks with limited feedback. The algorithm is based on a successor features framework, and is a simple algorithm that can be applied to any task inference problem. The experimental results on the Atari suite show that VISR improves human-level performance on some of the tasks.  "
SP:83500230586a9134f910ad067b7233dc563dc1ba," the functional view of deep neural networks. The functional view allows us to view the networks from a functional view, and to understand them in terms of the smoothness of the functional approximation and the flat initial approximation. In this paper, the authors show that the generalization performance of massively overparameterized networks with smooth initializations is a function of the loss surface, and that smoothness is correlated with generalization.   "
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2," in supervised and unsupervised manner. This paper tackles the image-to-image translation problem using a Generative Adversarial Network (GAN) to address the imbalance problem in GAN-based methods. The authors propose a new approach, called GuideGAN, to tackle the issue of mode collapse and diminished gradients. The key idea of the proposed approach is to use an attention mechanism in the generator and discriminator to improve the relative model capacities of the generator to the discriminator. In particular, it learns an attention map for the generator, which is then used to train a discriminator that is more robust to mode collapse. Experiments on several image transfer tasks demonstrate the effectiveness of the GuideGAN framework. "
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper studies neural network architectural motifs such as gating, attention layers, hypernetworks, and dynamic convolutions based on multiplicative interaction. Multiplicative interaction layers are primitive operations that are common in modern neural networks. The authors show that these layers are inductive biased, meaning that they can be seen as a special case of conditional computation. They also show that multiplicative interactions in neural network architectures can be viewed as an inductive bias. They further show that the inductive biases of these primitive operations (e.g. gating and attention layers) can be understood as a result of the concatenation operation.   The authors also show how to use them to construct a set of neural network architecture that can be used to construct multiplicative interactions. "
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper proposes a new algorithm for batch active learning with deep neural network models. Batch Active learning with Diverse Gradient Embeddings (BADGE) is an algorithm that combines predictive uncertainty and sample diversity. The key idea is to learn a hallucinated gradient space, where the uncertainty and diversity of each sample are correlated. The authors show that BADGE is able to achieve a trade-off between uncertainty and the diversity of the samples. They also show that it can be applied to real world active learning problems and that it is more robust to hand-tuned hyperparameters. In addition, they show that existing approaches for batch sizes are more robust than BADGE."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"Self-explaining models have been a popular topic of interest in recent years due to their ability to explain decision making parameters. However, there are several issues with the current state-of-the-art Self-Explaining models: (1) complex architectures, obscure feature extraction and the transformation process, (2) non-linearity of activation functions, and (3) the model reasoning process.    This paper proposes a feature-leveling architecture based on General Linear Models (GLMs) to address these issues. Specifically, the authors propose a feature leveling architecture that combines the low level features of the hidden layer with the high level features from the previous layer. The architecture consists of a single GLM layer for each layer of the architecture, and a feature leveler layer for the low-level features and a GLM for the high-level ones. The authors show that the proposed feature levelers are able to explain the decision making of deep neural networks (DNNs) in a way that is consistent with the model weights. They also show that their models perform better than other main-stream architectures.  The authors also propose a per-layer basis for explaining the high levels of the model, which can be applied to all layers of the deep architectures."
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper studies the gradient cost of softmax regression with uniform negative sampling for scalable softmax approximation. The authors propose a new training method to reduce the gradient signal of the classifier by using an adversarial model to sample from the data distribution. The proposed training method uses negative samples generated by the proposed adversarial sampling mechanism as negative samples for the gradient variance. The paper shows competitive results on large scale data sets against competitive baselines in terms of training time and performance. The main contribution of the paper is that the proposed method is able to achieve a signal-to-noise ratio of O(1/\sqrt{T}^T) which is a significant improvement over the state-of-the-art.    The paper also shows that the bias of non-uniform sampling can be alleviated by removing bias due to the use of a non-differentiable classifier, which can be used to improve the performance of the proposed technology.  However, the authors also show that the non-discriminative bias is still present in extreme classification, which is not well-suited for this setting due to slow convergence.  The authors also propose a simple and effective way to mitigate the bias by using the adversarial samplers for gradient updates. "
SP:29b52fee83309268d9864f3b1fc3617948577d41,This paper proposes a novel approach to efficient exploration based on a lowdimensional encoding of the environment using both modelbased and model-free objectives. The proposed approach uses intrinsic rewards based on the weighted distance of nearest neighbors in a low dimensional representational space to capture novelty. The intrinsic rewards are used to guide sample-efficient exploration using planning routines in the representational spaces. Experiments on maze tasks and a control problem show that the proposed exploration approach outperforms the baselines in terms of model accuracy. 
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper proposes a new learning model for few-shot classification and out-of-distribution detection. The authors show that it is possible to learn a good classifier that can be used to detect out of distribution (OOD) samples in the few shot setting, and that it can also be used for few shot classification. The paper also shows that the performance of outof distribution detection in a few shot learning setting can be improved by using the proposed learning model. "
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"Undirected neural sequence models such as BERT have been widely used in discriminative natural language understanding tasks such as question-answering and natural language inference. The authors propose a framework for the generation of monotonic monotonicity in directed sequence models, which is a generalization of the work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21].   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]  The paper proposes a generalized model of sequence generation that combines decoding in both directed and undirected models.    The authors show that, in the case of directed models, monotony can be achieved by monotonically generating sequences that are monosyllabic, and in the sense that the decoding in a directed model is monotonical.  [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [34] [35] [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [49] [50] [55] [56] [57] [58] [59] [60] [63] [64] [65]  In this paper, the authors consider the problem of generating sequences from two models, one autoregressive and one non-autoregressive.  In particular, they focus on the case where the two models are trained on the same input sequence (i.e., BERT).   In order to achieve monotone generation, they propose a general framework for both the generation and decoding of a sequence, and they show that this framework can be applied to both neural sequences models (refinement-based non-auto-regressive models, and to the case when the input sequence is generated from a sequence that is generated by a neural sequence model that is non-iid.  The main contribution of this paper is that the authors propose decoding algorithms for directed sequences models that can be used to train undirect"
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper tackles the problem of mathematical expressions (MEs) recognition from real scenes. The authors propose a two-stage approach to generate a LaTeX sequence from a printed mathematical expression image, which is then used to train a neutral network to predict the presence of mathematical symbols. The proposed method first predicts math symbols using an object detection algorithm, and then trains a seq2seq model to generate LaTeX sequences using an attention mechanism. The seq2sequences are then used for the detection of math symbols based on the position information of the objects in the scene. The paper shows that the proposed two-step method outperforms an end-to-end method in terms of ExpRate(expression recognition rate) and recognition accuracy. The model is also shown to be able to generalize to unseen mathematical formulas.   "
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"This paper proposes a new vector quantization method to reduce the memory footprint of convolutional network architectures. The key idea is to compress the weights of a ResNet-50 model to reduce its memory size, and then use it to quantize in-domain inputs and reduce the loss reconstruction error of in-domains inputs. The proposed method can be used for inference on any CPU with a single CPU, and can be applied to any compression factor. The method uses bytealigned codebooks to compute the compressed weights, and the method can also be applied on unlabelled data to speed up the quantization time. Experiments on ImageNet object classification show that the proposed approach can achieve top-1 accuracy, and is competitive with Mask R-CNN."
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper proposes TensorProduct Transformer (TP-Transformer), a Transformer-based model that uses Tensor-Product Representations to learn an explicit representation of relation structure. TP-Attention is a new attention mechanism that is based on the idea that attention can be used to capture ambiguities in representations. The authors demonstrate the effectiveness of the proposed model on the Mathematics Dataset for solving free-form math wordproblems. Pretrained models have been shown to be effective at representation-building, but it is not clear whether the TP-transformer’s attention maps are as effective. "
SP:d319df820c6630c409fab32097652a083e8f53ea,"Deep artificial neural networks have been shown to have identically distributed training and test sets, but the empirical sample set of real-world input samples is very different from the training set, and training and inference accuracies can be very different. Deep artificial networks can be seen as a special case of this problem.    Deep artificial neural network can be viewed as a neural network that is trained on a training set that contains a large number of training examples and test examples that are very dissimilar to each other.  The paper proposes a new learning algorithm for learning the source code of a learning algorithm, which is based on a procedure called Kolmogorov complexity. The universal cognitive similarity metric, the information distance, is defined as the number of samples that are sufficiently dissimilar (in terms of their channel codes) to be a sufficient condition for the optimization problem of the classification function. The paper shows that under this condition, the classifier can be trained on features that are dissimilar enough to the ones that are present in the training data, but not dissimilar so far in the test data.  In addition to this, the paper also shows that in the presence of corruptions (e.g. Gaussian and shot noise), the training and testing sets of a training and a test set are very different, and that the training sets of training and the test sets of the empirical set of the training are very similar. In addition, it shows that a model trained on uncoded input features of a model with corrupted or perturbed input features is more robust to corruptions and adversarial perturbations, and is able to generalize better than a model based on encoded input features. Finally, it is shown that the model can generalize to unseen corruptions using projected gradient descent, and generalize well to unseen adversarial examples. "
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper proposes a novel graph pooling method, called HaarPooling, for graph classification and graph regression tasks. The proposed method is based on compressive Haar transforms. The authors show that the pooling operation can be applied to graph convolutional networks (GCNs) and graph neural networks (GNNs). The authors also show that it can be used to improve the performance of GNNs on graph classification tasks. "
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"Point clouds for 3D objects are of great interest to the 3D object recognition community. However, the precision of point clouds is not always high-precision. This paper proposes to use encoder networks to capture the semantics of their input point clouds. The paper shows that point-cloud decoders using fully-connected networks are able to capture shape representations, and that sample-based decoder architectures can capture semantics of variable sized point clouds as well. In addition, the paper also shows that the shape representation learned by the sample-by-sample decoder can be improved by using feedforward architectures, where the point feature distribution is sampled from a set of sampled features. "
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper studies the problem of deep learning with controlled synthetic noise in the presence of real-world noisy labels with controlled noise levels. The authors show that Deep Neural Networks (DNNs) can be robust to real world noisy labels, even when the noise levels are controlled. Robust learning methods are shown to be able to deal with synthetic noise, but not real world noise. They also show that robust DNNs are able to handle real world data with different noise levels, and that networks trained on real world labels are robust to synthetic noise. Finally, they show that the robustness of ImageNet architectures to noisy data on noisy data can be improved on ImageNet datasets.    The paper is well-written and well-motivated, and the results are interesting. However, the paper suffers from a lack of comparison to the benchmark of realworld noise labels, and there is no comparison to real-real world noisy data. Real-world noise is not controllable, and it is not clear to me that robust learning methods can handle synthetic noise with controlled levels. "
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a rule-exemplar method for collecting human supervision for the task ofpain-staking human supervision on labeled data. The training algorithm for learning rules is based on soft implication loss on coverage and label variables, and the training algorithm uses the latent coverage variables to train a model to learn a set of rules. Then, the denoised rules and the model are used for inference. The denoising rules are trained with coupled rule-exploitation, where the human supervision is used to train the model. The proposed algorithm is evaluated on a number of tasks, and compared with other methods that use clean and noisy supervision. The results show that the proposed algorithm outperforms the other methods on most of the tasks. "
SP:6f2c656dbb7629f652a4291d6971625184d8118b,"Graph neural networks (GNNs) are a class of deep models that operate on graphs. In this paper, the authors propose a new memory layer for GNNs to improve the performance of node representations. Specifically, they propose two networks: memory-based GNN (MemGNN) and graph memory network (GMCN), which are two networks that use the same layer to learn hierarchical graph representations. The proposed models are evaluated on several graph classification and regression benchmarks. The authors show that the representations learned by the two networks are able to capture chemical features in molecule data, and that the chemical features of the representations are preserved in the graph."
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper studies the convergence of gradient-based optimization of the initial parameter values of deep learning systems. The authors show that the convergence times and the model performance of deep neural networks with different initialization schemes are highly correlated. They show that for deep linear networks with orthogonal group, the convergence converges faster than Gaussian initialization with iid weights. For deep non-linear networks, the authors show the convergence is faster than for Gaussian initializations. They also show that deep networks are invariant to the choice of the orthogonality of the initialization, and that the global minimum is a global minimum. Finally, they show that in the case of deep nonlinear networks with dynamical isometry, the learning can be accelerated by using a different initialization for each layer. "
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"This paper studies quantization methods for deep neural networks. The authors propose two methods: 1) coarse quantization, which quantizes the weights of layers with equal bit rate, and 2) high rate compression, i.e., 2-bit quantization. They show that both methods are optimal for deep CNNs compression. The main contribution of this paper is that the authors show that the quantization of output error has an additivity property, meaning that the accuracy of a layer can be improved if the output of the layer is more sensitive to quantization than that of the previous layer.  2) The authors also show that a method based on Lagrangian Formulation can be used to solve the optimization problem of the optimal bit allocation problem in a joint framework.  The proposed method is evaluated on a number of standard deep CNN neural networks, and is shown to achieve state-of-the-art accuracy loss. It is also shown to be applicable to deep CNN ResNet-50.  "
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"This paper proposes a new metric for measuring the convergence of Generative Adversarial Networks (GANs) based on the Wasserstein distance between the output of an encoder network and a generative network. The authors propose a new framework for auto-encoders and WGANs based on this new metric, which they call the WASSERstein GAN (WGAN). The authors show that the minmax two-player training of GANs is unstable in the presence of unstable training, and propose an inference WGAN (iWGAN) model to address this issue. The iWGAN is based on an iterative primal dual optimization process, where the model is trained with maximum likelihood estimation, and the autoencoder and generative networks are trained in parallel.  The authors provide a generalization error bound for iWANs, which shows that the model converges to a mode collapse under certain stopping criteria. They also provide a measurement of quality check for the model, and show that their model is more robust to mode collapse.  Finally, the authors evaluate the performance of their model on a number of benchmark datasets against state-of-the-art GAN models, and demonstrate that iWANNs outperform the state of the art in terms of generalization performance. "
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper studies the problem of anaphoric annotation, i.e. the problem that an annotator should be able to annotate a document in a way that is consistent with its classification tasks. Crowdsourcing has been shown to outperform expert annotation in coreference and NLP, but it is not well-studied in the context of coreference or NLP. This paper proposes to use an adjudication to improve crowdsourcing. The authors propose a nonparametric partially pooled structure based on the stick breaking process. They show that the sparsity of crowdsourcing environments can be controlled by MPA. They evaluate their model on a large-scale crowdsourced anaphora dataset and show that their model outperforms the state-of-the-art crowdsourcing setups. They also demonstrate that the proposed model can be applied to other annotation tasks for classification and demonstrate that it can be used to improve the performance of existing coreference chains. "
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"Exploration in sparse reward reinforcement learning is an important problem in which the intrinsic motivation for a sparse extrinsic reward signal can be very different from that of the intrinsic reward signal itself. This is due to the fact that the signals are sparse and the bonus rewards are sparse. This paper proposes a new intrinsic reward, successor feature control (SFC), which is a combination of two types of intrinsic drives: one that encourages exploration and the other that encourages stabilize learning. It is shown to be more efficient than existing methods that rely on local information to learn intrinsic motivation. It uses statistics over complete trajectories to learn a mixture policy, where the mixture policy is the sum of the learned policies of the two intrinsic drives. Experiments on three environments with pure visual inputs (VizDoom, DeepMind Lab and DeepMind Control Suite) show that the proposed scheduled intrinsic drive (SID) agent achieves better exploration efficiency compared to a single intrinsic drive on VizDoom. The paper also shows that SFC can be used to learn both intrinsic and extrinsical task policies.  "
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper tackles the problem of weakly-supervised video moment retrieval, where the model is trained on video-sentence pairs that have been annotated with temporal annotations. The authors propose a mechanism that combines visual and language representations to capture the latent correspondence between the two, and a multi-level co-attention mechanism to learn multimodal representations. The proposed mechanism is composed of a Frame-By-Word interaction module and a Word-Conditioned Visual Graph (WCVG). The proposed approach combines positional encodings from Transformers with iterative message-passing to learn visual-semantic representations. Experiments on the DiDeMo and Charades-STA datasets show that the representations learned by the wMAN model outperform the state-of-the-art weakly -supervised method in terms of Recall@1 accuracy metric. "
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a learned image-guided rendering technique that combines image-based rendering and GAN-based image synthesis. The proposed method is applicable to both virtual and augmented reality applications such as virtual showrooms, virtual tours, and digital inspection of historical artifacts. The work includes the handling of view-dependent effects such as warping and warping on diffuse surfaces, and the use of object-specific deep neural network to learn a view-dependant appearance. The proxy geometry is learned from a video, and a multi-view stereo is used to generate a 3D proxy geometry from a single video. The paper also proposes a pipeline to learn view-independent effects (e.g. specular highlights) from diffuse surfaces. The main contribution of the paper is that the proposed pipeline is able to learn the view- dependent effects from diffuse images, and that it can be used to improve the appearance of captured images.    The proposed approach is evaluated on real data, and it is shown to be able to achieve state-of-the-art results. The authors also show that their proposed pipeline can be applied to a different view of the same object, which is called EffectsNet. The network is trained using an image-driven approach, where a composition network generates photo-realistic results for each object in the scene. The method is also shown to perform well on a synthetic dataset. "
SP:257d124367b1da9a595dc11a9df750d6bade298e,This paper proposes a sparse representation of model uncertainty for deep neural networks (DNNs) based on the diagonal correction of the Kronecker-factored eigenbasis in a scalable Laplace Approximation scheme. This operation allows for a full Bayesian analysis of the information form. The authors show that the low-rank approximation of the eigenbais of DNNs with spectral sparsity leads to spectral sparsification. They also show that memory-wise tractable sampling computations can be achieved using this approach.    The authors also propose a novel inversion of the Information matrix to make the information matrix sparsified. They show that this approach is more memory-efficient than existing methods for sparsifying. 
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes Minwise Hashing (MinHash) for set similarities and compact high-dimensional data for learning and searching. MinHash uses a permutation (hash function) to compute the MinHash values for each pair of bins. Permutation Hashing uses the same permutation as in [1] and [2]. The authors show that MinHash converges to the optimal solution when the number of bins is large enough, but not when there is an unbalanced load. The authors propose a remedial strategy, called densification, to mitigate the issue of false similarity computation. They also propose two strategies for densification. The first one, called Amortization Hashing, is a variant of load-balanced hashing, which is called AHash. The paper shows that AHash outperforms OPH and other densification strategies in terms of runtime efficiency on real datasets. "
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"This paper proposes a novel method for feature extraction for periodic signals, where the periodicity of the signal is a function of the shaft’s rotation. This is relevant for mechanized transportation vehicle, power generation, industrial machine, robotic system with rotating shafts, and more. Imprecise timing is critical for phase shifts, and existing methods (e.g., multi-layer perceptron) do not work well due to phase shift. The authors propose a robust method to extract features from phase shift data using a machine learning architecture based on cyclic permutation of the graph data. They show that the proposed robust method is robust to phase shifts in the graph structure. "
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"This paper proposes a new approach to train conditional text generation systems. The authors propose a variational Bayes objective to train the conditional generation system, and use them to train real world systems with high precision and controllability. They also propose a calibration technique to speed up the inference time for faithful generation. The approach is evaluated on a structured data-to-text dataset (e.g. WikiBio) and compares against state-of-the-art approaches on automatic metrics and human evaluation."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"Magnitude-based pruning is a popular technique for pruning neural networks. However, it has been shown that the Frobenius distortion of a linear operator on a single layer of a neural network can be much larger than that of a multi-layer neural network. This paper proposes a new pruning method, called lookahead pruning, to tackle the problem of pruning modern architectures. The main idea is to use magnitude - based pruning to address the Frobonius distortion in pruning the linear operator at each layer of the neural networks, which is known to exist in the high-sparsity regime. The authors propose a novel pruning algorithm based on single layer optimization, which can be used for multi-layered optimization. The proposed method is shown to outperform magnitude-based prior pruning on two networks, VGG and ResNet. "
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper proposes a new technique for decentralized SGD with quantized communication. Decentralized stochastic gradient descent (SGD) is a technique that involves multiple parallel workers in a graph, each with a different memory. The authors provide an asymptotic rate of $O(\log n)$ for the algorithm with full-precision communication, and show that the algorithm Moniqua achieves $O(1/\sqrt{n})$ convergence with a wall clock time. They also show that with 4-bits-per-parameter communication, the algorithm can be faster than the previous algorithm, Monique, which has a memory of $n$ bits. They show that Monique outperforms the previous quantized decentralized algorithms with biased or linear quantizers, and Monique is faster than Monique with non-convex objectives.  The authors also provide an analysis of Monique's performance on non-trivial bit-budgeted algorithms. They find that the VGG16 outperforms Monique on CIFAR10 with a bit budget of $1/n$ and that the performance improves when the number of workers increases.  "
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper studies the problem of reinforcement learning in the context of partial models. In particular, it considers the setting of jointly modeling future observations (i.e., jointly modeling the current state of the art) and future observations. The authors show that it is possible to learn partial models in this setting and that they can be trained in an unsupervised way. They also show that partial models can be learned in a supervised way.   "
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is motivated by two information theoretic problems: distributed simulation and channel synthesis. The authors show that the common representation (i.e., the shared concept) of the correlated data variable is a product of a common representation and a set of local representations, and that Wyner’s common information is the sum of the mutual information between the two data variables.  The authors further propose a regularization term that encourages the common information to be the same across all the data variables, and they show that this mutual information improves the performance of joint and conditional generation. The approach is tested on synthetic data and real images for style control for joint and unconditional generation, and it is shown that the proposed model outperforms other VAE variants and the variational information bottleneck method. The paper also shows that this model can be used to learn a more efficient generative model that is able to capture a more concise common representation.   "
