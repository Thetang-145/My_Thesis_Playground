paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"This paper proposes a method for multi-agent multi-task learning based on role-based learning. The main idea is to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Then, a role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. The proposed method is evaluated on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the behavior of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, the authors prove that SSGD converges, respectively, with rates O(1/ ) and O(log(\1/)) for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the smooth problems. The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsm smooth machine learning models."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper proposes a method to improve the efficiency of transformer-based NLP models by freezing some of the layers and replacing them with non-linear ""reservoir"" layers. These layers are randomly initialized and never updated. The authors propose a new metric, AUCC, to measure the performance-efficiency trade-off between the frozen layers and the full transformer layers. They show that freezing the reservoir layers improves the performance of the full transformers on a variety of tasks. "
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,This paper studies the connection between steerable convolutional filters and the commonly used filter transform technique. The authors show that the kernel constructed by filter transform can be interpreted in the group representation theory. They also show that filter transformed kernels can be used to convolve input/output features in different group representations. This interpretation help complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable kernels. 
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper proposes an optimal neural synthesis approach for multimodal program synthesis, where the goal is to find a program that satisfies user-provided constraints while also maximizing the program’s score with respect to a neural model. Specifically, the authors focus on multimodality synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of the method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model allows for efficient search over the space of syntactically valid programs, and it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs. The experimental results show that the method substantially outperforms prior state-of-the-art techniques in terms of accuracy."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper proposes a protein graph convolutional neural network (PGCN) to predict the landscape of protease specificity, i.e., the set of all substrate sequence motifs that are recognized (and not recognized) by a given enzyme. The proposed method is based on a structure-based molecular interaction graph generated using the Rosetta energy function that describes the topology and energetic features, to determine the substrate specificity. The authors use the PGCN to recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C virus, and show that its performance in classification tasks is equivalent or better."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the underestimation bias of double Q-learning, which is a classical method for reducing overestimation bias in Q learning. The authors show that double Q learning may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address this issue, the authors propose a simple but effective approach as a partial fix, which leverages an approximate dynamic programming to bound the target value. Experiments on the Atari benchmark tasks demonstrate its significant improvement over baseline algorithms."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"This paper proposes a two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, it generates images in low-frequency bands by training a sampler in the wavelet domain. Then, it super-resolves these images from the wavelets domain back to the pixel-space with a novel wavelet super-resolution decoder network. Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet, the proposed method achieves a FID of 10.59 – beating the baseline BigGAN model – at half the compute (256 TPU-v3 cores)."
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper provides a theoretical analysis of self-supervised learning (SSL) for few-shot learning (FSL). In particular, the authors analyze why SSL is suitable for FSL and the main difference between supervised training and self supervised training on FSL. They also provide a bound for the gap between self supervised loss and supervised loss. Finally, they propose potential ways to improve the test accuracy under the setting of self supervised learning."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper studies the convergence properties of two-layer teacher-student neural networks with ReLU activation and Gaussian inputs. The authors show that the weight vectors of the student neurons eventually align with the weight vector of one of the teacher neurons, suggesting that there might not be any local minima near the initialization. Then, the authors prove that under the most basic settings, all student neurons must be aligned with the teacher neuron at any global minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call angular distance (AD) function."
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper proposes a method for combining neuro-symbolic knowledge representation and deep learning. The authors propose to represent knowledge in the form of logical statements, which are then used to train a neural network to predict the truth of the statements. The neural network is trained to maximize the sum of the logical connectives, which is then used as the label of the neural network. The method is evaluated on a toy task and a visual relationship detection task, where it is shown that the proposed method outperforms baselines. "
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper studies the inductive bias of residual neural networks (ResNets) towards iterative computations. The authors define three indices of iterative convergence and show that, even though ResNets can express iterative solutions, they do not learn them when trained conventionally on computer vision tasks. Then, the authors introduce regularizations to encourage iterative convergent computation and test whether this provides a useful inductive biases. To make the networks more iterative, they manipulate the degree of weight sharing across layers using soft gradient coupling. They also impose a Lipschitz constraint on the residual functions using spectral normalization."
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"This paper proposes two normalization techniques, i.e., self-norm and cross-norm, to improve the robustness of deep neural networks to out-of-distribution (OOD) data. SelfNorm uses attention to recalibrate statistics (channel-wise mean and variance), while CrossNorm exchanges the statistics between feature maps. Extensive experiments on different domains (vision and language), tasks (classification and segmentation), and settings (supervised and semi-supervised) show their effectiveness."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper proposes R-LAtte: Reinforcement learning with Attention module, a simple yet effective architecture for encoding image pixels in vision-based RL from pixels. The authors use two-stream encoders to extract non-attentional and attentional features from the images. The proposed method is able to achieve similar performance compared against representation learning and data augmentation methods, which are currently the best-performing ones in the literature. The paper also demonstrates that the proposed module extracts interpretable task-relevant information."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"This paper proposes an extension to GradNorm, a gradient-based approach for training multi-task neural networks. The proposed method, called Rotograd, aims to homogenize not only the gradient magnitudes but also their directions across tasks. Specifically, it adds a layer of task-specific rotation matrices that aligns all the task gradients. Theoretical guarantees on the algorithm stability and convergence are provided. Experiments on several real-world datasets and network architectures show that it outperforms previous approaches for multitask learning."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper proposes a method for unsupervised image-to-image translation, which aims to learn a domain mapping function without paired data. The authors propose a novel I2I translation constraint, called Minimal Geometry-Distortion Constraint (MGC), which promotes the consistency of geometry structures and reduce the unwanted distortions in translation by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of MGC, the authors propose an approximate representation of mutual information called relative Squared-loss Mutual Information (rSMI) that can be efficiently estimated analytically. The experimental results demonstrate that MGC achieves high quality translation to maintain the geometry of images in original domain."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the effect of point sampling patterns in point cloud GANs. The authors propose a sampling spectrum to depict the different sampling sensitivities of discriminators. They further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics. They discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on sampling-related metrics."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper investigates the adversarial robustness of CapsNets (CapsNets), a recently proposed neural network architecture that is shown to be more robust to white-box attacks than CNNs under popular attack protocols. The authors propose a novel vote attack where they attack the output capsules directly and integrate it into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. The vote attack is not only effective but also efficient by circumventing the routing process."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"This paper proposes an approach for meta-reinforcement learning in the online adaptation setting, where a single policy is trained for a fixed family of tasks. In this setting, the goal is to balance exploration to reduce the uncertainty about the current task, and exploitation to maximize the cumulative reward of the task. The authors propose a new algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the tasks and by exploiting them efficiently. It is able to learn better strategies than Thompson Sampling approaches, and faster than recurrent neural network policies and task-inference approaches."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper proposes a new paradigm to learn an RL policy from offline data in the real-world sequential recommendation system (SRS). Instead of increasing the fidelity of models for policy learning, this paper proposes to learn to adapt to diverse simulators generated by the offline dataset. Experiments are conducted in synthetic environments and a large-scale ride-hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real world."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper proposes a method for learning goal-reaching policies from scratch, without the need for expert demonstrations or a value function. They leverage the property that any trajectory is a successful demonstration for reaching the final state in that trajectory. They propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal- reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. They formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy and empirically demonstrate improved performance and robustness over current RL algorithms in several benchmark tasks."
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper proposes FastSpeech 2, an extension of the Fastspeech, a non-autoregressive text-to-speech (TTS) model by Ren et al. (2019). The main contributions of the paper are: 1) the authors propose to directly train the model with ground-truth target instead of the simplified output from teacher, 2) introduce more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs, 3) improve the duration accuracy and introduce more variance information including pitch and energy, and 4) improve pitch prediction by introducing continuous wavelet transform. Experimental results show that the proposed method achieves a 3x training speed-up over the original Fastspeech and achieves faster inference speed."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"This paper proposes to reformulate the unsupervised dimension reduction problem (UDR) problem in the language of tempered distributions, i.e. approximating an empirical probability density function pemp(x) by another tempered distribution q(x), whose support is in a k-dimensional subspace. The problem is then reduced to the minimization of the distance between q and pemp, D(q, pemp) over a set of generalized functions. This formulation allows to establish a connection with another classical problem of data science — the sufficient dimension reduction (SDR). Thus, an algorithm for the first problem induces a method for the second and vice versa."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,This paper proposes a novel approach to balance robustness and sensitivity in deep neural network training by introducing two notions: contextual feature utility and contextual feature sensitivity. The authors propose Feature Contrastive Learning (FCL) that encourages the model to be more sensitive to the features that have higher contextual utility. The performance of FCL is validated on both synthetic and real image classification datasets.
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper proposes a method for imitation learning from observations of expert demonstrations. The proposed method is based on adversarial learning with a latent representation inside the discriminator network. The latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. The method is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides a theoretical analysis of learning one-hidden-layer pruned neural networks, which offers formal justification of the improved generalization of winning tickets observed from empirical findings in the Lottery Ticket Hypothesis (LTH). The authors characterize the impact of the number of remaining weights in a pruned network on the required number of samples for training, the convergence rate of the learning algorithm, and the accuracy of the learned model. They also provide extensive numerical validations of their theoretical findings."
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes AutoLabel to automatically learn the labels for augmented data, based on the distance between the clean distribution and augmented distribution. The proposed method is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. The authors show that AutoLabel is a generic framework that can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on Cifar-10, CIFAR-100 and ImageNet show that the proposed method can improve models’ accuracy and calibration performance."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper studies the problem of self-supervised representation learning from a causal perspective. The authors show that the representation should be invariant predictors of proxy targets under interventions on features that are only correlated, but not causally related to the downstream tasks. Based on this, they propose Representation Learning via Invariant Causal Mechanisms (RELIC) that enforces invariant prediction of proxy target across augmentations through an invariance regularizer which yields improved generalization guarantees. In addition, they generalize contrastive learning and provide an alternative theoretical explanation for the success of these methods. Empirically, RELIC significantly outperforms competing methods on ImageNet and Atari."
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper proposes a visual transformer network (VTNet) for object goal navigation. The key idea of the proposed method is to leverage the relationships among all the object instances in a scene and the spatial locations of objects and image regions so that directional navigation signals can be learned. The authors also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments."
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"This paper proposes a new secure federated learning method that is more efficient than the existing secure aggregation (SA) method. The main idea is to design the topology of the secret-sharing nodes (denoted by the assignment graph G) as sparse random graphs instead of the complete graph corresponding to the existing solution. Theoretical analysis shows that using O(n log n) resources is sufficient for guaranteeing reliability and privacy of the proposed system with n clients, which is much smaller than the conventional wisdom. Experiments on real datasets, measuring the test accuracy and the privacy leakage, show that the proposed method requires only 50% of resources than conventional wisdom, to achieve the same level of reliability."
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"This paper builds on the work of Duetting et al. (2019) which proposed RegretNet, a neural network architecture to find the optimal auction. The authors propose two conceptual deviations from their approach which result in enhanced performance. First, they use recent results in theoretical auction design to introduce a time-independent Lagrangian. Second, the optimization procedure in previous work uses an inner maximization loop to compute optimal misreports. They amortize this process through the introduction of an additional neural network."
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper proposes a bi-tuning approach to fine-tune both supervised and unsupervised pre-trained representations to downstream tasks. The main idea is to integrate two heads upon the backbone of the representations: a classifier head with an improved contrastive cross-entropy loss to better leverage the label information in an instance contrast way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. Comprehensive experiments confirm that the proposed approach achieves state-of-the-art results on CUB and CUB-C by large margins."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper introduces a new measure for the robustness of classifiers called “genuine adversarial accuracy”, which measures the adversarial robustness without trading off accuracy on clean data and accuracy on the adversarially perturbed samples. It does not favor a classifier with invariance-based adversarial examples, samples whose predicted classes are unchanged even if the perceptual classes are changed. The authors prove that a single nearest neighbor (1-NN) classifier is the most robust classifier according to the proposed measure."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper studies the problem of link prediction in homogeneous graphs, where the dyadic fairness criterion expects the predictions to be statistically independent of the sensitive attributes from the given two instances within a homogeneous graph. The authors theoretically analyzed how the connections in graph links affect the fairness of demographic parity when employing graph neural networks for representation learning. They proposed FairAdj to learn a fair adjacency matrix, and pursued the fairness and prediction utility simultaneously. Empirical validations demonstrated the achievement of fairness and a better fairness-utility tradeoff."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes a disentangled exploration autoencoder (DEAE), which uses disentanglement representation and regularization to guarantee the validity of exploration in latent space and achieve controllable synthesis. The encoder first turns the input sample into disentangling latent code, then explores the latent code space through directed interpolation. To aid the interpolated latent code in successfully outputting a meaningful sample, after the decoder, the authors regularize the output by ’reusing’ the encoder to force the obtained latent representation to maintain perfect disentangle. Experiments demonstrate that DEAE can improve the performance of downstream tasks by synthesizing attribute-controllable augmented samples. It also provides a solution for the fairness problems."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a new Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. The authors take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. They simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. The allocation scheme improves performance in memory conditional image generation."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper studies the sample complexity and loss landscape of attention-based neural networks. Theoretical results show that, under mild assumptions, every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Experiments on various datasets validate the theoretical findings."
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper extends the concept of expected free energy (EFE), which is a core quantity in active inference, and claims that EFE can be treated as a negative value function. Motivated by this connection, the authors propose a simple but novel method for learning a prior preference from experts. The proposed method, called prior preference learning (PPL), can be applied to an inverse RL problem. The experiments show that PPL can learn a local prior preference, which is more effective than the global preference."
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. The authors show how to improve the generalization theoretically using OOD data in each learning scenario and complement their theoretical analysis with experiments on CIFAR-10, CifAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper proposes a new meta-RL algorithm called Fast Linearized Adaptive Policy (FLAP) that learns a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods fail to generalize to."
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a federated learning algorithm for kernel k-means. The authors propose a distributed stochastic proximal gradient descent (DSPGD) algorithm to solve the optimization problem of kernel-k means in a distributed manner. They also propose a communication efficient mechanism (CEM) to reduce the communication cost. Theoretical analysis shows that DSPGD with CEM converges with an O(1/T) rate, where T is the number of iterations. The clustering quality of the proposed algorithm is evaluated on real-world datasets."
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"This paper proposes CompOFA, a design space for Once-For-All (OFA) networks using compound couplings between model dimensions, which speeds up the process of one-shot training and neural architecture search with hardware latency constraints. The authors show that intractably large architectural search spaces are unnecessary for both accuracy and diversity of models, and introduce a simple heuristic that vastly shrinked the search space without losing on Pareto optimality. This smaller cardinality reduces interference in weight-shared training, which allows CompoFA to reach the same accuracy in half the training budget."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper proposes a meta-learning algorithm, ADML (ADversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. The authors claim that it turns out to be very effective even in the cases with only clean samples; 2) it is robust to adversarial attacks; 3) it sheds light on tackling the case with limited and even contaminated samples. The proposed method is claimed to be model-agnostic and can be applied to any learning model that can be trained with gradient descent."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper proposes a data-driven framework for permutation selection in permutation decoding. The authors propose a self-attention mechanism to improve decoding of linear error correction codes. For every received noisy word, the proposed model selects a suitable permutation out of the code’s permutation group (PG) without actually trying all the permutation based decodings. The method pre-computes the permutations’ representations, thus allowing for fast and accurate permutations selection at the inference phase."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper proposes to use unsupervised text clustering as an intermediate task for fine-tuning BERT for text classification. Specifically, the authors propose to partition the unlabeled training data into relatively homogeneous clusters of text instances and treat these clusters as labeled data for an intermediate text classification task, and train BERT – with or without additional MLM pretraining – with respect to multi-class multi-labeling over the actual target task labeled data. Extensive experimental results demonstrate the practical value of this strategy on a variety of benchmark data sets."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper presents an empirical study of micro-data model-based reinforcement learning (MBRL) on Acrobot. The authors compare a set of probabilistic, deterministic, and mixture density nets (MDF) models in terms of the asymptotic performance on the control problem. They also propose a new metric to evaluate the performance of MBRL models. They show that MDF models outperform deterministic models when multimodality is required, and that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes an affine disentangled generative adversarial network (ADIS-GAN) that can explicitly disentangle affine transformations in a self-supervised and rigorous manner. The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. Unlike the disentanglement representation learned by existing approaches, the features learned by ADIS-AN are axis-aligned and scalable, where transformations such as rotation, horizontal, and vertical zoom, horizontal and vertical skew, and horizontal/Vertical translation can be explicitly selected and learned. The paper demonstrates the effectiveness of the proposed method on MNIST, CelebA, and dSprites datasets."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"This paper proposes CLSA (Contrastive Learning with Stronger Augmentations), a contrastive learning method that exploits the distributional divergence between the weakly and strongly augmented images over the representation bank to supervise the retrieval of strongly augmented queries from a pool of candidates. The proposed method achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned. Moreover, it outperforms the previous self-supervised and supervised methods on both the transfer learning and object detection tasks."
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"This paper proposes a method for de-identification of magnetic resonance imaging (MRI) scans. The proposed method is based on a multi-scale generative adversarial network (GAN) that generates a 3D volume of a patient’s MRI scan, in which the brain is not modified but the face has been de-identified. Compared to the classical removal-based techniques, the proposed method preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper proposes a multi-head attention based global pooling layer for hierarchical graph pooling. The proposed method is based on a multiset encoding problem with auxiliary information about the graph structure, and proposes a Graph Multiset Transformer (GMT) to capture the interaction between nodes according to their structural dependencies. The authors show that GMT satisfies both injectiveness and permutation invariance, that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. The experimental results show that the proposed method significantly outperforms state-of-the-art node pooling methods on graph classification benchmarks with high memory and time efficiency."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper proposes a new explanation for the over-smoothing problem in GNNs. The authors claim that there is a bottleneck when aggregating messages across a long path. This bottleneck causes the information to be squashed into fixed-size vectors, which causes the GNN to fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. They demonstrate the existence of the bottleneck in a controlled problem, provide lower bounds for the hidden size given the problem radius, and show that GCN and GIN are more susceptible to over-squashing than GAT and GGNN. They further show that prior models of chemical, biological and programmatical benchmarks suffer from over-splashing and can be dramatically improved using a simple FA layer."
SP:90d8fa381446923902e42b259392e5e975e6caa1,This paper proposes a method for cross-domain sentiment analysis based on aligning two domain-specific distributions in a shared embedding space. The authors propose a new domain adaptation method which induces large margins between different classes in an embeddings space based on the notion of prototypical distribution. The proposed method is trained to be domain-agnostic by matching the data distributions across the domains. The experiments demonstrate that the method is effective.
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper proposes an evaluation methodology to measure the presence of gender bias in NLI models by constructing a challenge task which involves pairing gender neutral premise against gender-specific hypothesis. The authors use the challenge task to investigate the state-of-the-art NLI systems on the existence of gender stereotypes using occupations. Their findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets are significantly prone to genderinduced prediction errors. They also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce bias in certain cases."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper revisits variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. The authors show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, they propose two methods respectively based on the transitional probability model and Gaussian mixture model."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the problem of using ensembles of deep neural networks (DNNs) for the task of few-shot image classification, where the goal is to improve sample efficiency in the low-data regime by using an ensemble of relatively small DNNs. The authors propose a simple yet effective technique that outperforms current state-of-the-art approaches for learning from small datasets. They compare different ensemble configurations to their deeper and wider competitors given a total fixed computational budget and provide empirical evidence of their advantage."
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"This paper proposes a method to further compress Binary Neural Networks (BNNs) by introducing sparsity, while reducing their required computations. The approach is based on quantization of weights in the 0/1 binary domain and a highly sparse initialization of the network. The method has been evaluated on feed-forward linear and convolutional network on MNIST and CIFAR-10 data sets, respectively. Experiments confirm that SBNNs can achieve high compression rates and good generalization."
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a simple post hoc calibration method for predictive uncertainty. The method uses outlier exposure to properly calibrate the model probabilities. The proposed method is model agnostic, so it can be applied to future models. Experiments on corrupted data show that the proposed method significantly improves on benchmark results."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, it compares the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. The authors also investigate the impact of node attributes on the performance of the different models and kernels. The results reveal interesting findings. For instance, theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph Neural networks."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a data augmentation method for self-supervised image animation. The main idea is to use the top-k percent occluded pixels of the foreground to regularize image generation. The proposed method is built on top of the First Order Motion Model (FOM) and the CutMix method (Yun et al., 2019). The paper claims that the proposed method preserves better identity than vanilla CutMix and outperforms state-of-the-art image animation models."
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper proposes a self-supervised method to learn independent causal mechanisms (ICM), which directly model multiple data generation processes (mechanisms) in a coarse granularity. In particular, the authors aim to learn a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. The authors outline sufficient conditions under which the mechanisms can be learned using a single self supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, they prove the identifiability of the model w.r.t. the mechanisms in the self supervised scenario. They compare their approach to disentangled representations on various downstream tasks, showing that their approach is more robust to intervention, covariant shift, and noise."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper proposes a graph aligning approach that generates rich or detailed labels given normal labels W for the task of predicting molecular graph structure (W) given a 2D image of a chemical compound (U). The proposed approach is based on the idea of fully mediating representation V that factors into U → V → W. However, observing V requires detailed and expensive labels. In this paper, the authors investigate the scenario of domain adaptation from the source domain where we have access to the expensive labels V to the target domain where only normal label W are available. The authors show that, using only 4000 data points, the proposed approach achieves up to 4x improvement of performance after domain adaptation to target domain compared to pretrained model only on the original source domain."
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper proposes a neural network-based method for chiller plant energy optimization. The authors propose a monotonic neural network (MNN), which constrain the input-output of the chiller power model to conform to physical laws and provide accurate function space about chiller plants. The proposed method is evaluated on a cooling system of a data center, and the results show the superiority of the proposed method over the existing ones."
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a new method for predicting the causal effects of predictors on multiple forecasting targets, such as supply and demand in two-sided ride-hailing platforms. The proposed method is based on the idea of causal attention, which is a variant of double machine learning (Chernhukov et al. 2018). The authors integrate the causal attention with the Conditional Average Treatment Effect (CATE) estimation method in causal inference. Moreover, they propose a novel and fast multi-head attention evolved from Taylor’s expansion instead of softmax, reducing time complexity from O(V) to $O(V), where V is the number of nodes in a graph. They further design a spatial graph fusion mechanism to significantly reduce the parameters’ scale."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes a variational approach to jointly learn a mixture of discrete and continuous factors of variability. The authors propose a multi-agent variational framework, where each agent learns a continuous latent representation for one of the data modalities, while being encouraged to reach consensus on the categorical assignments. The proposed method is evaluated on MNIST and dSprites datasets, achieving state-of-the-art categorical assignment while preserving interpretability of the continuous factors. It is also evaluated on a single-cell gene expression dataset, where it is shown that the proposed method can identify annotated neuronal types and differentiate between type-dependent and activity-regulated genes."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper provides a general characterization of steerable kernel spaces for group equivariant convolutional networks (GCNNs). In particular, it provides a characterization for the practically relevant case of G being any compact group. This work is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical Tensor operators, this paper proves that steerable Kernel spaces are fully understood and parameterized in terms of generalized reduced matrix elements, Clebsch-Gordan coefficients, and harmonic basis functions on homogeneous spaces."
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper studies the effect of selective classification on accuracy disparities between groups. It shows that while selective classification can improve average accuracy, it can simultaneously magnify existing accuracy disparities among groups, especially in the presence of spurious correlations. The authors analyze the margin distribution, which captures the model’s confidences across all predictions and determines which examples it abstains on at each threshold (Figure 1). They prove that selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Motivated by their analysis, they train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selective learning uniformly improves each group on these models."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper proposes a hierarchical nonnegative CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. The model reveals the hierarchy of topics learned at different NCPD ranks, which is not available to standard NCPD or NMF-based approaches. The authors empirically demonstrate the promise of this method on both real and synthetic datasets."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,This paper proposes a collective adversarial robustness certificate for node classification. The proposed approach is based on graph neural networks (GNNs) and leverage their locality property to fuse multiple single-node certificates into a drastically stronger collective certificate. The authors evaluate it on multiple semi-supervised node classification datasets with different classifier architectures and base certificates and show that the proposed approach yields much stronger certificates than existing methods.
SP:cc93dd2f68e415e2457166e78627865dc1b44697,This paper proposes Quantile Regression GAN (QRGAN) to improve the performance of GANs. The authors propose to minimize 1-Wasserstein distance between real and generated data distribution as a novel approach in modification of loss functions for improving the GAN performance. QRGAN obtains an apparent improvement in the evaluation and comparison of Frechet Inception Distance (FID) for generation performance assessment.
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"This paper investigates relevance metrics for similarity-based explanation. Specifically, the authors propose three tests to evaluate whether the relevance metrics satisfy the minimal requirements for similarity based explanation. The experiments reveal that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. The authors also show that some relevance metrics perform poorly in the tests and analyze the reasons of their failure."
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"This paper proposes a low-rank global attention (LRGA) module, which is a computation and memory efficient variant of the dot-product attention (Vaswani et al., 2017), to improve the generalization power of Graph Neural Networks (GNNs) for improving their generalisation power. The authors show that adding the LRGA module provides algorithmic alignment to a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In more detail, they: (i) consider the recent Random Graph Neural Network (RGNN) framework and prove that it is universal in probability; (ii) show that RGNN augmented with LRGA aligns with 2- FWL update step via polynomial kernels; and (iii) bound the sample complexity of the kernel’s feature map when learned with a randomly initialized two-layer MLP. "
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,This paper proposes a label smoothing method to improve the calibration performance of convolutional neural networks (CNNs) on ImageNet-1K. The proposed method uses bounding box information pertaining to objects to compute a smoothing factor that is adaptive based on relative object size within an image. Experiments on classification and transfer learning tasks demonstrate the effectiveness of the proposed method.
SP:5254658923e594294b69d124a8d004166852822a,"This paper proposes a convex duality framework for CNN-based denoising networks for medical image reconstruction. The authors propose a two-layer fully-convolutional ReLU network with weight decay regularization and piecewise linear filtering, and propose a dual program that offers optimal training using convex solvers, and gains more interpretability. Experiments on MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem."
SP:085cad6bc143c8713580bddfaa71f06496dac314,"This paper proposes an end-to-end approach for learning to synthesize speech from natural language text or phonemes. The authors propose a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. The proposed model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models."
SP:01148cea55db606aa78d27e900818684a8bce9ab,"This paper proposes a non-parametric approach to learn node representations for node classification and matrix completion. In particular, the authors propose a Wasserstein graph diffusion method to smooth the distribution representations of nodes with information from their local neighborhoods. The proposed method is evaluated on two applications: node classification (missing node attributes) and node completion. "
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes a meta-learning framework for generating a curriculum of goals that can be used as an intrinsic reward to train an agent as a form of meta-reinforcement learning. This is achieved by having a goal generator as a teacher and a policy that acts as a student conditioning on those goals. The teacher is rewarded to propose goals that are challenging but not impossible, and the policy is rewarded for maximizing the intrinsic reward. Experiments on procedurally generated tasks show that the proposed method outperforms state-of-the-art intrinsic motivation methods."
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"This paper studies the problem of private information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. Specifically, a user requesting a file from the dataset should be able to reconstruct the requested file with a prescribed distortion, and in addition, the identity of the file should be kept private from the server. The authors show that the optimal rate-distortion-leakage tradeoff is convex and that in the limit of large file sizes this allows for a concise information-theoretical formulation in terms of mutual information. Moreover, the authors propose a new data-driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in the terms of download rate from the data itself."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) that, instead of sampling the input graph, decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. This approach allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. The authors also propose a lazy-update scheme during training to further improve its efficiency."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper proposes a framework for answering complex queries on incomplete Knowledge Graphs. The authors translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. They then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. The experiments show that the proposed approach produces more accurate results than state-of-the-art methods."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper considers the problem of verifying the local robustness of neural networks with piecewise-linear activation functions for the l_2 norm, i.e., the robustness to adversarial perturbations in the input space. The authors propose a novel approach that relies on geometric projections instead of constraint solving or conservative overapproximations. They provide an efficient, highly parallel implementation to certify the l2-robustness. The proposed approach outperforms existing exact tools by multiple orders of magnitude, while empirically maintaining the same or better precision under a time constraint."
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper presents an approach for learning representations of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, better than a variety of alternative approaches. The authors also show that the dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects."
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for promoting the emergence of individualism in multi-agent reinforcement learning (MARL). The proposed method learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and the authors also propose two regularizers to increase the discriminability of the classifiers. The authors implement EOI on top of two MARL algorithms, MAAC and QMIX, and empirically show that the proposed method outperforms existing methods in a variety of MARL scenarios."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper proposes a smoothed weighted ensemble (SWEEN) scheme to improve the performance of randomized smoothed classifiers. Theoretical analysis shows that SWEEN can achieve optimal certified robustness w.r.t. the $\gamma$-robustness index. Moreover, the authors also provide an adaptive prediction algorithm to accelerate the prediction and certification process. Extensive experiments show the effectiveness of the proposed method."
SP:ea892e3d199ed6121279b20061a87f43afae8796,"This paper proposes a method to discover subtask hierarchy by learning from demonstration. Specifically, the authors propose a novel ordered memory policy network (OMPN) that can represent the subtask structure and leverage it to perform unsupervised task decomposition. The experiments show that OMPN learns to recover the sub-task boundary in both unsupervisioned and weakly supervised settings with behavior cloning."
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) prediction, which models the semantic and variation factors separately. Theoretically, the authors prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Experiments show the improved performance over prevailing baselines."
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of robust online learning in the presence of adversarial corruptions of the rewards. In particular, the authors consider stochastic multi-armed bandits, linear contextual bandits and Markov decision processes (MDPs) where the reward can be arbitrarily corrupted with probability $\mathcal{P}(0,12)$. The authors propose algorithms with small regret over a period of time steps, where the algorithm observes corrupted rewards, but its regret is small with respect to the true uncorrupted reward distribution. The authors build upon recent advances in robust estimation for unsupervised learning problems to design robust online algorithms with near optimal regret in three different scenarios: "
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes Rewriter-Evaluator, a framework for neural machine translation (NMT) that consists of a rewriter and an evaluator. The rewriter produces a new translation at every pass to improve the past translation and the evaluation of the quality of the translation is used to decide whether to terminate the rewriting process. A prioritized gradient descent (PGD) method is proposed to train the rewriter-evaluator jointly. Experiments on two translation tasks, Chinese-English and English-German, show that the proposed framework significantly improves the performance of NMT models and significantly outperforms previous baselines."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,This paper proposes a method for learning a calibrated multimodal predictive distribution for semantic segmentation. The proposed method is based on a two-stage approach. The first stage models the data with a categorical likelihood. The second stage trains an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. The authors demonstrate the utility and versatility of the approach by attaining state-of-the-art results on the multigrader LIDC dataset and a modified Cityscapes dataset.
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new method for dealing with compressed communication with error feedback (EF) in distributed optimization. The main idea is to use unbiased compressors instead of contractive compressors, which are not unbiased. Theoretical results show that the proposed method is better than EF in terms of convergence rate, memory requirements, and communication complexity. Empirical results are also provided. "
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper proposes a framework for hyperparameter transfer across adjustments (HT-AA) to improve efficiency during the development of machine learning (ML) algorithms. The authors propose four simple HT-AA baseline algorithms and eight benchmarks changing various aspects of ML algorithms, such as hyper-parameter search spaces, neural architectures, and neural architectures used. The best baseline, on average and depending on the budgets for the old and new HPO, reaches a given performance 1.2–3.6x faster than a prominent HPO algorithm without transfer. "
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper studies the role of label representation in training deep neural networks for image classification. The authors propose to replace standard categorical labels with high-dimensional, high-entropy matrices as speech labels. They show that high-dimension, high entropy labels achieve comparable performance to text labels on the standard image classification task, but features learned through the label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes a multi-input multi-output (MIMO) configuration for training multiple independent subnetworks within a network. The key benefits of MIMO are its simplicity, because it does not require significant modifications to the network architecture and it has few hyperparameters, and its computational efficiency, since it can be evaluated in a single forward pass. The empirical results confirm that MIMMO improves performance and robustness with minor changes to the number of parameters and the compute cost."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. The proposed method first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixel-level and image-level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plugand-play manner. The experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a new notion of state similarity based on behavior similarity between states. The authors also propose a contrastive representation learning procedure to embed any state similarity metric, which they instantiate with PSM to obtain policy similarity embeddings (PSEs1). They demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite."
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"This paper studies the problem of disentanglement, i.e. disentangling natural factors of variation in data (e.g. object shape vs pose) by learning to map each of these factors to distinct subspaces of a model’s latent representation. The authors show that for a very broad class of transformations acting on images —encompassing all affine transformations (translation, rotations, translations, etc) —an encoder that would map these transformations into dedicated latent subspace would necessarily be discontinuous. Based on this, the authors propose an alternative, more flexible approach to disentangle which relies on distributed equivariant operators, potentially acting on the entire latent space. They theoretically and empirically demonstrate the effectiveness of their approach to learn disentangled affine transformation. "
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes a point-estimation method (EM algorithm) to estimate the maximum a posteriori (MAP) estimate of neural spike trains. Three auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. Based on the augmented likelihood and prior, the conditional densities of latent variables and parameters in closed form can be obtained, which constitutes a Gibbs sampler with better efficiency than MCMC-Aug since the time-consuming MetropolisHasting sampling is not needed."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the dynamics of gradient descent (GD) for training two-layer neural network models in the under- and over-parameterized regimes. The authors show that there are two phases in the dynamics: the early phase is similar to the random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) “activated” neurons that dominate the dynamics and another group of quenched neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions. This is qualitatively different from the GD dynamics associated with the mean-field scaling where all neurons participate equally."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper proposes a method to solve constrained Markov decision process (CMDP) problems by decomposing the CMDP into a pair of MDPs; reconnaissance MDP and planning MDP. In R-MDP, the proposed method trains threat function, the Q-function analogue of danger that can determine whether a given state-action pair is safe or not, and in P-mDP, it trains a reward-seeking policy while using a fixed threat function to determine the safeness of each action. With the help of generative model, it can efficiently train the threat function by preferentially sampling rare dangerous events. The proposed method can solve other CMDP problems with different reward and different danger-constraint without the need to re-train the model."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper provides an empirical comparison between the cross-entropy loss and the square loss for training deep neural networks for classification tasks. The authors explore several modern deep learning models and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square losses, even after equalizing computational resources. They argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the Cross-Entropy loss. Their results indicate that the performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to square losses."
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes a self-supervised representation learning method for low-data RL. The proposed method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future using an exponential moving average of the agent’s parameters and makes predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. It further improves performance by adding data augmentation to the future prediction loss, which forces the representation to be consistent across multiple views of an observation."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper proposes an efficient method for generating local node embeddings using local PageRank computations. The proposed method is based on a high-order similarity matrix based on Personalized PageRank (PPR) that is computed via hashing. The authors provide theoretical guarantees on the locality of the computation, as well as the proof of the global consistency of the generated embeddeddings. They show empirically that their method is able to produce high-quality representations on par with state-of-the-art methods, with efficiency several orders of magnitude better in clock time and memory consumption."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"This paper proposes a framework to evaluate the quality of graph coarsening algorithms and shows that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight may be suboptimal, the authors parametrize the weight assignment map with graph neural networks and train it to improve the quality in an unsupervised way. Experiments on both synthetic and real-world networks demonstrate that the proposed method significantly improves the performance of existing graph-coarsening methods."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper proposes a geometric deep learning algorithm to predict the acoustic scattering properties of objects. The proposed method is based on discrete-laplacian and implicit encoders to compute these characteristics for general 3D objects at interactive rates. It uses a point cloud approximation of each object, and each point is encoded in a high-dimensional latent space. The multi-layer network can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce RTX 2080 Ti GPU. The authors also prove that the learning method is permutation and rotation invariant and demonstrate its application to generating environmental acoustic effects in dynamic environments."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper proposes Risk Extrapolation (REx) as a method for robust optimization over a perturbation set of extrapolated domains (MMREx), and proposes a penalty on the variance of training risks (V-REx). The authors prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (covariate shift). In particular, REx is able to trade-off covariate shift and causally induced distributional shifts and outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur."
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper proposes a method for learning partial differential equations (PDEs) using neural networks. The main idea is to parameterize the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. The method is evaluated on Burgers’ equation, Darcy flow, and Navier-Stokes equation. It is shown to be up to three orders of magnitude faster compared to traditional PDE solvers. "
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the implicit bias of gradient flow (GD) on linear neural network training. The authors propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. They show that gradient flow on separable classification finds a stationary point of the $\ell_2$/L max-margin problem in a “transformed” input space defined by the network. For underdetermined linear regression, they prove that GD finds a global minimum which minimizes a norm-like function that interpolates between weighted `1 and `2 norms in the transformed input space. They also provide experiments that corroborate their analysis."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper proposes a method to train slimmable neural networks by jointly learning both channel configurations and the shared weights. Specifically, the authors propose a training algorithm, PareCO, which jointly learns both channel configuration and the width-multipliers. The authors conduct experiments on 15 network and dataset combinations and two cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper studies the problem of federated semi-supervised learning (FSSL) where each client learns with only partly labeled data (labels-at-client scenario), or supervised labels are only available at the server, while clients work with completely unlabeled data. The authors propose a new method, called FedMatch, to tackle the problem. FedMatch improves upon naive combinations of local federated learning (FL) and semi-Supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning. Experiments are conducted to validate the effectiveness of FedMatch."
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"This paper proposes a contrastive learning-based self-supervised learning method for discrete event sequences. The method is based on a novel theoretically grounded data augmentation strategy, which adapts the ideas of Xing et al. (2002) and Hadsell et al (2006) to the discrete event sequence domain. The authors theoretically justify under mild conditions that the augmentation method underlying CoLES provides representative samples of discrete events sequences. Experiments on several public datasets show that CoLES representations consistently outperform other methods on downstream tasks."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,This paper proposes a method for unsupervised dependency parsing and masked language modeling. The main idea is to use a distance-based parsing mechanism that predicts syntactic distances and syntactic heights to represent dependency and constituency trees at the same time. The authors also introduce a dependency-constrained self-attention mechanism that allows each attention head to focus on a specific mixture of dependency relations. This brings Transformers closer to modeling a directed dependency graph. The experiments show that the proposed method can induce meaningful dependency structures and achieve better performance on masked language model task.
SP:078966ff62775bba6031e47d374bda95f4a7dde3,This paper proposes a method for learning the mapping between scene graph nodes and visual objects under weak supervision. The proposed method learns a metric among visual objects and scene graphs by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that the proposed method can post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on the scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method.
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"This paper proposes a new variant of sliced fused Gromov wasserstein (SFG) for relational regularized autoencoder (RAE), which is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the latent space. The authors propose a new relational discrepancy, named spherical sliced fused gromov-fusion (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, the authors introduce two variants of SSFG to improve its performance. The first one is called mixture spherical SSFG (MSSFG) that replaces the vMF distribution by a mixture of vMF distributions to capture multiple important areas of directions that are far from each other. The second one is named power spherical sliced fusion (PSSFG), which replaces the fusion distribution with a power spherical distribution to improve the sampling time in high-dimensional settings."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes a simple approach to speed up training for a particular kind of deep networks which contain repeated structures, such as the transformer module. In this paper, we first train a deep network with the weights shared across all the repeated layers till some point. We then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. The adaptive untying criterion is obtained from a theoretic analysis over deep linear networks. Empirical results show that our method is able to reduce the training time of BERT by 50%."
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper analyzes the transferability of adversarial perturbations from the perspective of interactions based on game theory. Theoretical results show that adversarial transferability is related to the Shapley value of the interaction between perturbation units, which is defined as the change of i’s importance when the j-th unit is perturbed w.r.t the case when the ith unit is not perturbed. The paper further shows that the lower the  the  transferability, the higher the  negative interaction between adversarial units. Based on this, the paper proposes to penalize interactions during the attacking process, which improves the adversarial  transferable. "
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper studies the role of higher layers in catastrophic forgetting in deep neural networks. The authors find that higher layers are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. They also investigate different methods for mitigating forgetting, finding that while all stabilize higher layer representations, some methods encourage greater feature reuse in higher layers, while others store task representations orthogonally, preventing interference. Finally, they show that maximal forgetting occurs for task sequences with intermediate similarity."
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes an efficient training algorithm for large language models. The proposed method is inspired by the Lottery Ticket Hypothesis (LTH) for efficient training of computer vision models. In particular, the authors propose to prune the self-attention and fully-connected sub-layers inside a transformer to identify structured winning tickets in the early stage of BERT training, and apply those tickets for efficient BERT pre-training and fine-tuning. Experimental results on GLUE and SQuAD demonstrate that the proposed method can achieve comparable performance to standard BERT with much less training time."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper studies the robustness of f-divergences to label noise in the presence of label noise. The authors derive a nice decoupling property for a family of f -divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. In addition to the analytical results, the authors also present thorough experimental evidence. "
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes ensemble-based weighted Bellman backups, which re-weights target Q-values based on uncertainty estimates from a Q-ensemble. The authors also propose a unified ensemble method, SUNRISE, which integrates the proposed weighted bellman backups with bootstrap with random initialization, and UCB exploration to handle various issues in off-policy RL algorithms. The experiments show that the proposed method improves the performance of existing off-Policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments."
SP:afc08f203562b841180811aef943bfb63a1659ea,This paper proposes a method for task calibration in few-shot learning. The authors propose to measure the distributional mismatch between support and query sets via class-wise similarities. The proposed method is algorithm-agnostic and readily expanded to include a range of meta-learning models. Experiments show that the proposed method helps the model avoid being indiscriminately confident.
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a method for video-text representation learning that leverages a generative model to naturally push these related samples together. Specifically, each sample’s caption must be reconstructed as a weighted combination of other support samples’ visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. The proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper proposes a method, seg tok, to re-build the vocabulary of Chinese BERT with the help of Chinese word segmentation (CWS) and subword tokenization. The authors also propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that seg-tok improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; and MVP improves PLMs’ downstream performance, especially it can improve the performance on sequence labeling tasks."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"This paper proposes a boundary sampling-based method for distributed training of graph convolutional networks (GCNs) on large full-graphs. The main idea is to partition a graph into subgraphs and train them in parallel with necessary communication. The proposed method, called BDS-GCN, adopts unbiased boundary sampling strategy to enable efficient and scalable distributed GCN training. The authors claim that the proposed method can boost the throughput by up to 500% and reduce the memory usage by up-to 58% while achieving the same accuracy as compared with the state-of-the-art methods."
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper proposes a graph neural network (GNN) based model for predicting the per-atom forces in quantum chemistry simulations. The proposed model, called ForceNet, is based on graph neural networks and uses surrounding 3D molecular structure to estimate per-atomic forces. The authors show that ForceNet reduces the estimation error of atomic forces by 30% compared to existing ML models, and generalizes well to out-of-distribution structures. Finally, the authors apply ForceNet to the large-scale catalyst dataset, OC20, where ForceNet is able to achieve 4x higher success rate than existing models."
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,This paper investigates the impact of different regularization strategies for fine-tuning deep learning networks. The authors provide two new bounds on the generalization performance of neural networks based on the distance of the final weights from their initial values. They show that MARS distance is a more appropriate metric in the parameter space of convolutional networks than Frobenius distance. They also show that enforcing a hard constraint throughout the entire training process on the distances the parameters can move is far more effective than the widely used strategy of adding a penalty term to the objective function.
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,This paper investigates the effect of different hyperparameters for mask discovery and mask evaluation in unstructured magnitude pruning. The authors propose to decouple the hyperparameter values for both mask discovery (Hfind) and evaluation (Heval) and show that the decoupled find-eval phenomenon occurs. They also show that different Hfind values yield masks with materially different layerwise pruning ratios. The results demonstrate the practical utility of decoupling Hfind and Heval.
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper proposes a new metric, called m-coherence, to study the evolution of alignment of per-example gradients in the course of training. The metric is based on the number of examples that benefit from a small step along the gradient of any one example on average. It is more interpretable, cheaper to compute, and mathematically cleaner than other commonly used metrics such as the expected pairwise dot products. The authors also show that the metric is closely connected to gradient diversity."
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper proposes a method for learning sufficient statistics for likelihood-free inference. The proposed method is based on the idea of mutual information maximization, where the goal is to maximize the mutual information between the representations of the data and the learned statistics. The authors claim that the proposed method can be applied to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks. "
SP:c5997bf2348e94949684f45fbd418661e85220c1,This paper proposes an unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translate input images into the estimated domains. TUNIT uses a GAN to learn the image translation functions across various domains. Experimental results show that the proposed model achieves comparable or better performance than the set-level supervised model trained with full labels.
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper studies the implicit bias of gradient descent training of wide neural networks and the corresponding implicit bias in function space. In particular, it shows that the solution of training a width-n shallow ReLU network is within n-1/2 of the function which fits the training data and whose difference from initialization has smallest 2-norm of the weighted second derivative with respect to the input. The curvature penalty function is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and we compute it explicitly for various common initialization procedures. The solution function is the natural cubic cubic spline interpolation of the data."
SP:8b885142facbb3b8db41ec9d83822cee81324694,"This paper studies the effect of weight decay on the performance of Adam and other adaptive gradient methods. The authors show that weight decay is not identical to weight decay for SGD and AdamW, and propose a stable weight decay (SWD) method to fix the unstable weight decay problem from a dynamical perspective. The proposed SWD method makes significant improvements over L2 regularization and decoupled weight decay in the experiments."
SP:a3206dc71e32ba1830895bf442d3840f3331a532,"This paper proposes a method to combine the strengths of both Translation Memory (TM) and Neural Machine Translation (NMT). The authors treat the matched sentence pair of TM as the additional signal and apply one encoder enhanced by the pre-trained language model (PLM) to encode the TM information and source sentence together. They extend the sentence level retrieval method to the n-gram retrieval method so that they don’t need to calculate the similarity score. Further, they explore new methods to manipulate the information flow from TM to the NMT decoder."
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper attempts to interpret modern deep (convolutional) networks from the principles of rate reduction and (shift) invariant classification. The authors show that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators (linear or nonlinear), and parameters of the network are all explicitly constructed layer-by-layer in a forward propagation fashion. All components of this “white box” network have precise optimization, statistical, and geometric interpretation."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. They show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and the amount of acceleration depends on both the magnitude of that difference (which is fixed at initialization) and the spectrum of the data."
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper proposes a method for generating interpretable explanations for black-box models. The proposed method builds on top of the Locally Interpretable Model-agnostic Explanation (LIME) framework (Ribeiro et al., 2016), which trains a surrogate interpretable classifier to be faithful to the original model in a small neighborhood around a user. The main contribution of this paper is that it proposes to sample the subspaces of input data using a constraint language, which allows the user to define the precise subspace of the input domain to be explained. Experiments on synthetic data and real-world datasets demonstrate the effectiveness of the proposed method. "
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"This paper proposes AMBERT (A Multi-grained BERT), a pre-trained language model for natural language understanding (NLU) based on BERT. The main idea of the paper is to use a two-layer encoder-decoder architecture, one for the sequence of words and the other for the sequences of phrases, with shared parameters between the two encoders. Experiments are conducted on CLUE, GLUE, SQuad, and RACE datasets for English and Chinese. The results show the effectiveness of the proposed model."
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper proposes a new Transformer-based model for semantic parsing. The main idea is to incorporate Long Short-Term Memory (LSTM) into the self-attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS and Atis datasets."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes a new adversarial training method for improving the robustness of deep neural networks (DNNs) against combinations of multiple adversarial perturbations. The authors propose a new class of composite adversarial attacks that combine multiple perturbation models and penetrate the state-of-the-art defenses. They then propose a training method that improves DNNs robustness not only against individual perturbational models but also against their compositions. The proposed method, called CAT, is able to flexibly integrate and optimize the adversarial robustness losses, which leads to improved robustness with respect to multiple individual adversarial models and compositions."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"This paper proposes a method for learning abstract rules from high-dimensional sensory data. The proposed method is based on a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the model to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a generative model for structured prediction tasks. The proposed approach is a translation between augmented natural languages (TANL), which is able to solve many structured prediction language tasks such as entity and relation extraction, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on several tasks."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper studies the problem of unlabeled entity problem in named entity recognition (NER), where the entities of a sentence may not be fully annotated. The authors propose a general approach, which can almost eliminate the misguidance brought by unlabelled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeling entities. Experiments on synthetic datasets and real-world datasets show that the proposed method is robust to unlabelED entity problem and surpasses prior baselines."
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel acoustic word embedding called Acoustic neighbor embedding where speech or text of arbitrary length are mapped to a vector space of fixed, reduced dimensions by adapting stochastic neighbour embedding (SNE) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained: an acoustic encoder that accepts speech signals in the form of frame-wise subword posterior probabilities obtained from an acoustic model and a text encoder network that accepts text as input. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper proposes a reinforcement learning algorithm for stationary mean-field games, where the goal is to learn a pair of mean field state and stationary policy that constitutes the Nash equilibrium. The authors propose a fictitious play algorithm that alternatively updates the mean field and the policy via gradient-descent and proximal policy optimization, respectively. The algorithm is in stark contrast with previous literature which solves each single-agent reinforcement learning problem induced by the iterates mean field states to the optimum. Theoretical results show that the proposed algorithm converges at a sublinear rate."
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a framework for approximate probabilistic inference on the joint distribution defined by a normalizing flow model. The authors show that this task is computationally hard for a large class of flow models. Motivated by this hardness result, this paper trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, the authors can efficiently train it using variational inference and also handle conditioning under arbitrary differentiable transformations. The resulting approximate posterior offers exact likelihood evaluation, inversion, and efficient sampling."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper proposes a new Electron Microscopy (EM) dataset, U-RISC, for cell membrane segmentation, which is the largest annotated EM dataset for the Cell membrane with multiple iterative annotations and uncompressed high-resolution raw data. The authors claim that the current popular segmentation evaluation criteria are inconsistent with human perception and propose a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of the segmentation results. In addition, the evaluation criteria of PHD and existing deep learning segmentation methods are re-examined."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper proposes a new benchmark for continual learning (CL) that aims to evaluate the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. The authors also propose a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. The learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks, and the experiments show that the proposed approach yields a desirable trade-off between accuracy and compute/memory usage."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper proposes a meta-learning approach that uses generative models to generate pairs of in-class and out-of-class samples from the latent space of a generative model. The proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), is evaluated on Omniglot and CIFAR-10 datasets. It is shown that the proposed approach outperforms or is competitive with current unsupervised learning baselines on few-shot classification tasks on the widely used benchmark datasets."
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies the injectivity of ReLU neural networks. The authors show that injectivity is necessary and sufficient for layer-wise and layer-to-layer injectivity. They show that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. They also show that any Lipschitz map can be approximated by an injective ReLU network."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes a continuous conditional generative adversarial network (CcGAN) for image generation conditional on continuous, scalar conditions (termed regression labels). The main contributions of this paper are: 1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; 2) proposing a novel method to incorporate regression labels into the generator and the discriminator; 3) a new benchmark dataset, RC-49, is also proposed for generative image modeling conditional on regression labels. Experiments on the Circular 2-D Gaussians (C2DGaussian) and the UTKFace datasets show that the proposed CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label."
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper proposes to combine active learning (AL) and semi-supervised learning (SSL) in order to reduce the sample complexity of supervised learning. In particular, the authors propose an active learning scheme called convergence rate control (CRC) that focuses on controlling the convergence rate of a classification network by actively querying instances to improve the rate of convergence upon inclusion to the labeled set. The proposed algorithm is developed under the assumption that a well-optimized network f(\infty) induced by the SSL objective is a good approximation of a model f^* that generalizes well. The authors show that a deep neural network trained using a combination of the proposed algorithm and a recently proposed SSL algorithm can achieve high performance using far less labeled samples than SL."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes FedDyn, a federated learning method for distributed training of neural network models. The authors point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss, and propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. Empirical results on real and synthetic data show that the proposed method leads to efficient training, in both convex and non-convex settings, while being agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper focuses on accelerating contrastive learning algorithms with little or even no loss of accuracy. The authors claim that the similarity on the intermediate layers is a good surrogate of the final similarity, and exploit this observation by introducing additional intermediate contrastive losses. In this way, they can truncate the back-propagation and update only a part of the parameters for each gradient descent update. They also do selection based on intermediate losses to filter easy regions for each image, which further reduces the computational cost. They apply their method to recently-proposed MOCO, SimCLR, SwAV, and VOC2."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies the global convergence and global optimality of actor-critic in the single-time-scale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, the authors consider two function approximation settings where both the agent and the critic are represented by linear or deep neural networks. For both cases, they prove that the actor sequence converges to a globally optimal policy at a sublinear O(K-1/2) rate, where K is the number of iterations."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes to represent logs at a few levels of abstraction including field level, log level, and log sequence level. The representations are in vector format and serve as interfaces to downstream applications. The authors use a version of Transformer Networks (TNs) to encode numerical and textual information that is suitable for log embeddings. They show how a number of log processing applications can be readily solved with the representation."
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper proposes a method to constrain the behavior of convolutional layers by splitting them into a succession of wavelet packet decompositions, which are modulated by freely-trained mixture weights. The authors evaluate their approach with three variants of wavelets with the AlexNet architecture for image classification as an example. The first variant relies on the separable wavelet transform while the other two implement the 2D dual-tree real and complex wavelet transforms, taking advantage of their feature extraction properties such as directional selectivity and shift invariance."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes an attention mechanism for both the players and the coach in multi-agent reinforcement learning (MARL) settings where the number of agents and the composition of the team can vary. The proposed approach is inspired by real-world team sports, where the players have a partial view of the environment, while the coach has a complete view. Specifically, the authors propose to use a multi-head attention mechanism, a variational objective to regularize the learning, and an adaptive communication method to let the coach decide when to communicate with different players. Experiments are conducted on resource collection tasks in the Multi-agent particle environment. "
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper provides an empirical study of the influence function in deep neural networks. The authors show that the network architecture, depth and width of the network, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, they show that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy can vary significantly depending on the examined test points."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"This paper investigates the connection between the pretraining task of next word prediction and text classification. The authors hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pre-training task. With a mathematical formalization of this hypothesis, they make progress towards (2) and show that language models that are -optimal in cross-entropy (log-perplexity) learn features that can linearly solve classification tasks with O(sqrt(log^2) error) error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. They experimentally verify various assumptions and theoretical findings, and also use insights from the analysis to design a new objective function that performs well on some classification tasks."
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper proposes a black-box membership inference attack (MIA) method for conditional image generation models (e.g. image translation). The proposed method uses pixel-wise reconstruction error to discriminate between easy and hard images. The authors also propose a novel difficulty score that can be computed for each image, and its computation does not require a training set. Experimental results show that the proposed method achieves high MIA accuracy."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes Dirichlet Neural Architecture Search (DrNAS), a differentiable neural architecture search method by formulating it into a distribution learning problem. The authors treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlets, and propose a progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. The proposed method can be optimized efficiently via gradient-based algorithm and possesses theoretical benefit to improve the generalization ability."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper proposes a multiplicative filter network (MFN) for representing low-dimensional but complex functions. In particular, the authors propose to multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier and Gabor basis functions, respectively. The authors compare their approach on networks with comparable numbers of parameters to the exact benchmarks proposed in the SIREN and Fourier features papers."
SP:f5be855300f63c185a006834302bd4b033b56258,"This paper proposes a teacher-student scheme for the gradient-based meta-learning algorithms to allow them to explore long horizons by the inner loop. The key idea is to employ a student network to adequately explore the search space of task-specific models (e.g., by more than ten steps), and a teacher then takes a “leap” toward the regions probed by the student. The teacher not only arrives at a high-quality model but also defines a lightweight computation graph for meta-gradients. The approach is generic; it performs well when applied to four meta- learning algorithms over three tasks: few-shot learning, long-tailed classification, and meta-attack."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"This paper proposes to improve the behavior regularized offline reinforcement learning (BRAC) algorithm by proposing a low-variance upper bound of the KL divergence estimator to reduce variance, state-dependent Lagrange multipliers to allow more freedom of deviation to high probability states while restricting low probability states, and a gradient penalty term to penalize the gradient of the Q value w.r.t. the out-of-distribution actions. The proposed method outperforms the baselines on challenging offline RL benchmarks."
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,This paper proposes an approach to compress and regularize deep neural networks. The authors propose a one-shot learning paradigm that trains both the original and the smaller networks together. The parameters of the smaller network are shared across both the architectures. They prove strong theoretical guarantees on the regularization behavior of the adjoint training paradigm. They complement their theoretical analysis by an extensive empirical evaluation of both the compression and regularization behaviour of adjoint networks.
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper proposes a simple extension of the popular greedy exploration method -greedy, which simply repeats the sampled action for a random duration. The authors claim that the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. To address this limitation, the authors propose a temporally extended form of greedy, called z-greedy. They show that z-Greedy improves exploration and performance in sparse-reward environments with minimal loss in performance on easier, dense-rewards environments."
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper shows that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to Greedy Low-Rank Learning (GLRL) under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from Gunasekar et al. (2017). The authors also extend the results to the case where depth > 3, and show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimisation is more likely to take effect for initialization with practical scale."
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper proposes CAMEL, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. CAMEL first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate sub group features. The authors demonstrate CAMEL’s effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline."
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper proposes a rule-based representation learner (RRL) that learns interpretable nonfuzzy rules for data representation. The RRL is formulated as a hierarchical model with supporting layers and disjunction operations. To train the non-differentiable RRL effectively, the authors project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. The experimental results show that RRL enjoys both high classification performance and low model complexity on data sets with different scales."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a regret minimization algorithm for generalization across structured biomedical domains such as molecular scaffolds or protein families. The proposed method builds from invariant risk minimization (IRM) by recasting simultaneous optimality condition in terms of predictive regret, finding a representation that enables the predictor to compete against an oracle with hindsight access to held-out environments. The structured extension adaptively highlights variation due to complex environments via specialized domain perturbations. The authors evaluate their method on multiple applications: molecular property prediction, protein homology and stability prediction and show that RGM significantly outperforms previous state-of-the-art baselines."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes a cross-probe BERT model for large-scale cross-modal text-to-image retrieval. The proposed model is built on top of BERT with cross-model attentions, where the cross model attentions are conducted on both text and vision probes. The cross model attention is conducted on a small number of probes, which is much more efficient than text-vision BERT. Experiments on two public datasets demonstrate the effectiveness and efficiency of the proposed model."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new type of Actor, named forward-looking Actor or FORK, for Actor-Critic algorithms. It can be easily integrated into a model-free actor-critic algorithm. The experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK can bring to the state-of-the-art algorithms."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper proposes a new method for model aggregation in federated learning (FL). The authors propose to use Bayesian inference to aggregate the outputs of a wide range of global models for a more robust prediction. In particular, they propose to sample higher-quality global models and combine them via Bayesian model Ensemble, leading to much robust aggregation. They show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. The empirical studies validate FEDBE's superior performance, especially when users’ data are not i.i.d. and when the neural networks go deeper."
SP:3ac5f437fc349a33810d0645664d1c448528af74,"This paper proposes a method for analyzing the contextualization of BERT models. Specifically, the authors introduce multi-partite patterns, abstractions of sets of paths through a neural model (a graph), which quantify and localize the effect of an input concept on an output concept to a collection of paths passing through a sequence of model nodes and/or edges. They describe guided pattern refinement, a search procedure for finding patterns representative of concept-critical paths that let us selectively selectively importance of chosen aspects of a model (e.g. in BERT) in terms of the criticality of certain paths. They further extend the framework to integrate impacts of multiple words towards a given concept (as opposed to impact of a single word). They show how BERT handles subject-verb number agreement (SVA) and reflexive anaphora (RA) and qualitatively and visually explore the BERT’s contextualization in the two tasks using their methodology."
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,This paper studies the training of a deep neural network from a control theory perspective. The authors formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. They provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting.
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper proposes to use the mutual information between each learned latent variables and the auxiliary variable to correctly identify informative latent variables. Theoretical analysis and extensive experiments on synthetic and real data show that for any GIN type of models, mutual information is a better criterion for informative latent variable selection than the previously proposed variance criterion. The authors also show that by selecting the correct features in the representation, the GIN model can perform better on the classification, outlier detection and adversarial defense tasks."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"This paper proposes a bidirectional pooling layer for convolutional neural networks, inspired by the classical lifting scheme in signal processing. It decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail sub-band, which is useful for image-to-image translation challenges. Experiments show the proposed methods achieve better results on image classification and semantic segmentation."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset T into binary sequences in the cube {±1}. When T consists of well-spread (i.e., non-sparse) vectors, the embedding method applies a stable noise-shaping quantization scheme to Ax where A is a sparse Gaussian random matrix. The proposed method is both fast and memory efficient, with time complexity $O(m)$ and space complexity of $O(\sqrt{m})$ on well spread data. Moreover, the authors show that Euclidean distances among the elements of T are approximated by the `1 norm on the images of {+1}$."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,"This paper proposes to use plasticity rules as a proxy for Gradient Descent (GD) in order to improve the generalization and robustness of RNNs. Plasticity rules are laws that govern the strength of a synapse based on the firing history as seen at the post-synaptic neuron. The authors argue that the plasticity rule is biologically plausible, in the sense that it can be learned over evolutionary time: they describe a genetic setting where natural selection of a numerical parameter over a sequence of generations provably simulates a simple variant of GD. In the special case of the last layer of a classification network, they show analytically that GD recovers (and improves upon) the perceptron algorithm and the multiplicative weights method."
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new method for visual question generation (VQG) based on the idea of visual hints, i.e., visual hints that can be used to guide the generation of visual questions. The authors propose a graph-to-sequence learning framework that first models the visual hints as a dynamic graph and learns the implicit topology, and then utilize a graph to sequence model to generate the questions with double hints. The experiments on VQA2.0 and COCO-QA datasets demonstrate that the proposed model can significantly outperform existing state-of-the-art baselines."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper studies the phenomenon of ""double descent"", i.e., non-monotonic test performance as the sample size and the model size increase. The authors show that optimally-tuned l2 regularization can mitigate the double descent phenomenon. Theoretically, they prove that for certain linear regression models with isotropic data distribution, l2-regularization achieves monotonic performance. Empirically, the authors also demonstrate empirically that the same idea can be applied to more general models, including neural networks."
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"This paper proposes a spatial dependency networks (SDN) to improve generative modeling by better exploiting spatial regularities and coherence in images. The proposed SDN is tailored to image generators that operate in a non-autoregressive way, i.e. synthesize all pixels ‘at once’. Spatial dependency layers improve upon convolutional layers by modeling spatial coherence and long-range spatial dependencies. SDN was shown to improve the performance of VAEs immensely."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper proposes a new offline RL algorithm called EMaQ, which is a simplified version of BCQ (Fujimoto et al., 2018a) by removing a heuristic design choice and restricting extracted policies to remain exactly within the support of a given behavior policy. The authors derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, it explicitly considers the number of samples and proposal distribution, allowing them to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting – the main focus of this work – EMAQ matches and outperforms prior state-of-the-art in the D4RL benchmarks (Fu et al. 2020a). In the online RL setting, the authors demonstrate that EMaq is competitive with Soft Actor Critic (SAC)."
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper proposes a bilevel optimization-based batch selection method to improve model fairness. The main idea is to use SGD as the inner optimizer, and the outer optimizer performs batch selection based on the fairness measure of the intermediate model, measured in the current epoch. The authors show that the proposed method is compatible with existing batch selection techniques, thus gracefully achieving multiple purposes. Experiments are conducted on both synthetic and real-world datasets."
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the Lipschitz constants of monotone deep equilibrium (DEQ) networks, a recently proposed class of networks that is a subclass of DEQs. In particular, the authors show that the monotonicity of the network can be bounded as a function of the strong-monotonicity parameter $m$, which is defined as the sum of the input-output mapping $x(y)$ and the weight-output map $y(x)$. The authors also show how to use these bounds to develop PAC-Bayesian generalization bounds that do not depend on any depth of the networks and which avoid the exponential depth-dependence of comparable DNN bounds."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper proposes a unified approach for imitation learning and goal-conditioned reinforcement learning. In the imitation learning setting, the authors propose to use density estimation to estimate the likelihood of reaching a given state, which is then used to learn to reach a given goal state. The authors show that the proposed approach is able to match the distribution of demonstrated states in the goal domain, and outperform a simple stochastic variation of the same domain. The paper also shows that the approach can be applied to the imitation setting as well. "
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"This paper proposes a variational multi-task learning method, which is a general probabilistic inference framework for simultaneously learning multiple related tasks. The proposed method is based on variational Bayesian inference, which enables task relatedness to be explored in a principled way by specifying priors. The authors introduce Gumbel-softmax priors to condition the prior of each task on related tasks, and the mixing weights are learned in a data-driven manner for each individual task. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on four benchmark datasets."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. They systematically evaluate ten well-established long-established Transformer models (Reformers, Linformers,. Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper proposes a language-agnostic Transformer-based code summarization model that jointly learns on Context and Structure of source code. The main idea is to combine Structure and Context, which are complementary representations of the same computer program. The proposed model achieves state-of-the-art performance on monolingual code summarisation on all five programming languages considered in this work. The authors also propose a multilingual code summary model, which is trained on non-parallel data from multiple programming languages and improves results on all individual languages."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper introduces a reinforcement learning approach to audio-visual navigation with two key novel elements: (1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and (2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. The authors demonstrate their approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper studies the lottery ticket hypothesis, which suggests that neural networks may rely on lucky random initial weights of subnetworks called “lottery tickets” that converge quickly to a solution (Frankle & Carbin, 2018). To investigate how weight initializations affect performance, the authors examine small convolutional networks that are trained to predict n steps of the two-dimensional cellular automaton Conway’s Game of Life, the update rules of which can be implemented efficiently in a small CNN. They find that networks of this architecture trained on this task rarely converge. The number of weights necessary for networks to reliably converge on a solution increases quickly with n. Additionally, they show that the probability of convergence is highly sensitive to small perturbations of initial weights. Finally, they explore properties of the training data that significantly increase the probability that a network will converge to a correct solution."
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper proposes a method for semi-supervised learning based on consistency regularization and metric learning. The proposed method considers not only the perturbed inputs but also the similarity among the inputs having the same label. The authors also introduce a new objective function, dubbed BatchMean Triplet loss, which has the advantage of computational efficiency while taking into account all input samples. The experimental results show that the proposed method achieves state-of-the-art results across many standard SSL benchmarks with various labeled data amounts."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper studies the problem of online meta-learning in the setting of sequential learning, where the tasks are presented in a sequence and the goal is to learn the structure that underlies the data coming from a set of related tasks, and use this structure to learn new tasks with only a few datapoints. The authors propose a variable-shot approach, which is able to handle the variable settings that naturally arise in sequential learning: from many-shot learning at the start, to zero shot learning towards the end. The proposed approach introduces a scaling rule for the learning rate that scales with the number of shots. This approach strongly outperforms a strictly more expressive approach of learning individual learning rates for each number of shot."
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper presents a series of probes designed to test the sensitivity of Transformer representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. They experiment with three different perturbations: (1) random permutations of n-grams of varying width, (2) swapping of two spans which do or do not form a syntactic phrase, (3) swapping two adjacent words which do/do not break apart a syntactical phrase, to test sensitivity to local phrase structure. Results show that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process."
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper proposes a light-weight GAN structure that gains superior quality on 1024x1024 resolution. The model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute their work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With 13 datasets covering a wide variety of image domains, they show their model’s superior performance compared to the state-of-the-art StyleGAN2."
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper proposes a specialised dual solver for neural network bounding. The main idea is to use a linear program of size linear in the number of neurons to solve the LP relaxation of Anderson et al. (2020), which is a weaker relaxation of the LP. The authors propose a unified dual treatment that includes both the weaker LP relaxation (Ehlers, 2017) and the tighter one (Anderson et al., 2020). The main contributions of the paper are: (1) A novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. (2) A linear separation oracle that recovers the strength of the relaxation in the dual space: tightness and a linear separation. (3) It shares the benefits of previous dual approaches: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time."
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper proposes a self-supervised approach to augmenting language models with concept-centric commonsense knowledge. Specifically, the authors propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). The authors also develop a joint pretraining framework to unify the generative-contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that the proposed method, concept-aware language model (CALM) can pack more commonsens knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper proposes a self-supervised method for object segmentation and object discovery from videos. The proposed method is based on the idea that objects exist and move smoothly, and that coherent object motion constrains expectations about future object states. The method learns to extract object-based scene representations from videos using motion cues. The visual generative model factors the scene decompositions across local patches, then aggregates those local patches into a global segmentation. The link between the visual model and the dynamics model is used to constrain the discovered representations to be usable to predict future world states."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes a novel adversarial training method, Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The proposed method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data."
SP:276ffd59fbf49e3ee02756da8920218102214917,"This paper proposes a new knowledge distillation method, ProKT, which projects the supervision signals of a teacher model into the student’s parameter space by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that ProKT consistently achieves the state-of-the-art performance."
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a channel pruning method inspired by hypernet. The authors propose a hyper-structure network for model compression to capture inter-channel and inter-layer relationships. An architecture vector can be generated from HSN to select a subnetwork from the original model. The HSN can be updated by the gradients from them. Moreover, the authors also identify the problem of FLOPs constraint (bias towards latter layers), which limits the final search space of HSN. To solve it, they further propose layer-wise scaling to balance the gradient, and they can be optimized by hyper-gradient descent."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper studies the problem of training a theorem prover to prove higher-order logic theorems in the presence of a large knowledge base of potential premises without learning from human proofs. The authors propose to augment the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. The experiments show that the proposed method, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs, and approaches the performance of a system that combines reinforcement learning with imitation learning."
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes an automated data augmentation approach called MODALS (Modalityagnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augment to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Experiments on multiple datasets for text, tabular, time-series and image modalities demonstrate the effectiveness of MODALS."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper provides a global convergence result for unregularized feedforward three-layer networks in the mean field regime. The authors propose a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove the global convergence guarantee under suitable regularity and convergence mode assumptions, which does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time via an algebraic topology argument."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes a method for learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “what if” outcomes: given the current history of observations, what would happen if we took a particular action? The authors integrate counterfactual reasoning into batch inverse reinforcement learning (IRL) to learn these costbenefit tradeoffs associated with the expert’s actions. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making—where active experimentation is often impossible (e.g. in healthcare)."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper investigates the role of morphological information in graph neural networks (GNNs) in the context of multi-task reinforcement learning (MTRL). In particular, the authors show that existing GNN-based methods do not improve the performance of MTRL in incompatible environments. Motivated by this, they propose a transformer-based approach, called AMORPHEUS, which is able to leverage the information in the node features alone to learn a successful policy. The proposed method obviates the need to propagate messages far away in the graph and can attend to different regions of the observations depending on the input and the particular point in training."
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper proposes a simple and effective model named MoVie for visual counting by revisiting modulated convolutions that fuse queries to images locally. Different from previous works that perform explicit, symbolic reasoning, the counting is done implicitly and holistically in the proposed model and only needs a single forward-pass during inference. The proposed model achieves state-of-the-art performance on three major benchmarks in visual counting, namely HowManyQA, Tally-QA and COCO. The authors also show that the proposed method can be easily incorporated as a module for general VQA models like MCAN to improve accuracy on ‘number’ related questions. The strong performance helped us secure the first place in the 2020VQA challenge."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,This paper proposes a model-targeted poisoning attack that can target a desired model based on online convex optimization. Theoretical results show that the proposed poisoning attack has provable convergence to any achievable target classifier. The authors also provide a lower bound on the minimum number of poisoning points needed to achieve a given target classifiers. The proposed attack is evaluated on CIFAR-10 and ImageNet datasets.
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper proposes a method for efficient deep learning on point clouds based on binarization. The authors claim that the performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. The proposed method introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that the proposed method outperforms the existing methods by convincing margins."
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper proposes and studies the use of memory-augmented neural networks (MANNs) to improve the performance of Transformer-based models. Specifically, the authors propose and study several extensions of the Transformer baseline by adding memory tokens to store non-local representations, creating memory bottleneck for the global information, and controlling memory update with a dedicated layer. They evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modeling tasks."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes a method for unsupervised representation learning based on prototypical clustering and contrastive learning. Specifically, the authors introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization (EM) framework. They iteratively perform E-step to find the distribution of prototypes via clustering, and M-step as optimizing the network via contrastive loss. They propose ProtoNCE loss, a generalized version of the InfoNCE (InfoNCE) loss, which encourages representations to be closer to their assigned prototypes. The experimental results show that the proposed method outperforms state-of-the-art instance-wise contrastive methods on multiple benchmarks."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"This paper proposes a method to defend adversarial attacks by adding a block containing multiple paths to learn robust features. The parameters of these paths are required to be orthogonal with each other. Via forward learning and backward correction, one OMP block makes the neural networks learn features that are appropriate for all the paths and hence are expected to be robust. Experiments on both white-box and black-box attacks show the effectiveness of the proposed method."
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a self-supervised graph attention network (SuperGAT) for noisy graphs. Specifically, the authors exploit two attention forms compatible with a self supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. Experiments on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a new method for DSMAD (dialysis system for medical automatic diagnosis), which aims to learn an agent that mimics the behavior of a human doctor, i.e. inquiring symptoms and informing diseases. The authors propose a novel DSMAD agent, INS-DS (Introspective Diagnosis System), which consists of two separate yet cooperative modules: an inquiry module for proposing symptom-inquiries and an introspective module for deciding when to inform a disease. The idea is inspired by introspective decision-making process of human, where the inquiry module first proposes the most valuable symptom inquiry, then the introspective modules intervenes the potential responses of this inquiry and decides to inquire only if the diagnoses of these interventions vary. "
SP:10ae09d90d465125433a9b4f15b1405ab017920d,This paper proposes a method to tackle the long-tailed and fine-grained visual classification (FGVC) problems simultaneously. The authors propose a batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to address the natural world distribution of visual classification. BCN is a combination of the standard cross-entropy loss and the standard pairwise confusion energy. The proposed method is evaluated on three popular FGVC datasets and achieves state-of-the-art results.
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,This paper proposes a Bayesian approach to inverse reinforcement learning (IRL) by learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces. The proposed approach is based on a variational approach to the latent reward. The approach is evaluated on both simulated and real-world data sets.
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper proposes a method for policy improvement in partially observable Markov Decision Processes (POMDPs). The main idea is to use an approximate auto-regressive counterfactual belief model that is learned as a supervised task. The proposed method is applied to Hanabi, and is shown to outperform the state-of-the-art methods in Hanabi."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper proposes a new algorithm, called shoot tree search (STS), which is a combination of Monte Carlo Tree Search (MCTS) and random shooting. The main idea is to use multi-step expansion to control the depth of search and inject into planning more randomness via random multi-steps expansions. The authors show that STS can achieve the best of both worlds consistently, though often achieving higher scores."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper proposes a pretraining approach to learn inductive bias in the form of synthetic tasks. The authors claim that deduction, induction, and abduction form an irreducible set of reasoning primitives and design three synthetic tasks that are intended to require the model to have these three abilities. They specifically design these synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called LIME (Learning Inductive bias for Mathematical rEasoning)."
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper analyzes the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. The analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate learning rate. Experimental results on simple data sets and architectures support the claim on sparse EWN solutions."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,"This paper proposes a method to tackle the mode collapse problem in GANs. The authors claim that catastrophic forgetting in the discriminator is the cause of mode collapse, and propose a training procedure that dynamically spawns additional discriminators to remember previous modes of generation. They also propose a computationally efficient synthetic data generation procedure for studying high-mode collapse. They show that their method can be added to existing GAN frameworks to prevent mode collapse and generate more diverse samples."
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes an approach to regularize BERT by pruning its attention heads based on a proxy score for head importance. The proposed approach leverages reinforcement learning to automatically prune attention heads from BERT. Instead of relying on heuristics or rule-based policies, the proposed approach learns a pruning policy that determines which attention heads should or should not be pruned for regularization. Experimental results show that AUBER outperforms existing pruning methods by achieving up to 9.39% better accuracy."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,This paper proposes a framework to find observations and actions correspondence across two domains using dynamics cycle-consistency. The proposed method is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. The authors show the efficacy of their method on multiple downstream applications in both simulation and real robot.
SP:006434d56992836ab9420d7d4215bc70664de304,"This paper studies the problem of on- and off-manifold Shapley explainability, i.e. how to compute the Shapley value function on the data manifold. The paper proposes two methods: one based on generative modeling and the other based on directly learning the value function. Experiments on synthetic and real-world data show that the proposed methods outperform the existing methods."
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"This paper proposes a VAE-based opponent modeling approach for multi-agent reinforcement learning. The proposed approach is based on VAE encoders, which are trained to reconstruct the local actions and observations of the opponent based on embeddings that depend only on the local observations (i.e., observed world state, chosen actions, and received rewards). The learned embedding are used to augment the modelling agent’s decision policy which is trained via deep reinforcement learning; thus the policy does not require access to opponent observations. The authors provide a comprehensive evaluation and ablation study in diverse multi-Agent tasks, showing that the proposed method achieves comparable performance to an ideal baseline which has full access to the opponent’‘s information, and significantly higher returns than a baseline method which does not use the learned learned policy and the VAE model are optimised concurrently."
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"This paper proposes a consistency regularization term for contrastive learning, which is inspired by semi-supervised learning on unlabeled data. Specifically, the consistency term takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% in the top-5 accuracy on 1% and 10% labeled semisupervised settings. It also transfers to image classification, object detection, and semantic segmentation."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies the last-iterate convergence of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) in bilinear games over the probability simplex. The authors show that when the equilibrium is unique, OGDA converges with a learning rate whose value is set to a universal constant, improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. Then, the authors extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last iterate convergence rates with a constant learning rates whose value only depends on the smoothness of the objective function. The condition also holds for strongly-convex-strongly-concave functions. "
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper studies the problem of training User Verification (UV) models in federated learning, where the conventional loss functions are not applicable due to the constraints that each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. To address this problem, the authors propose a framework for private and secure training of UV models. In FedUV, users jointly learn a set of vectors and maximize the correlation of their instance embedding with a secret user-defined linear combination of those vectors. The authors show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors. They present the experimental results for user verification with voice, face, and handwriting data and show that FedUV is on par with existing approaches."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper studies the geometry of the class manifolds (CMs) of deep neural networks. The authors propose a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. They provide a theory for the technique and verify that their theoretical predictions agree with measurements on real neural network. They show that well-performing, robust models have higher dimensional CMs than worse performing models. Moreover, they offer a unique perspective on ensembling via intersections."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,This paper proposes a method to improve the performance of SAC by adding a curiosity term to the entropy of the soft actor-critic (SAC). The novelty of the proposed method is to use the state prediction error (X-RND) to model the curiosity of the agent to increase the entropy for unfamiliar states and decrease the target entropy for familiar states. The method is evaluated on the MuJoCo continuous control tasks. 
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. The proposed method is based on a simple insight that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. The authors propose to adapt the model first, relabeling all data from the meta-training tasks with this model, and then fine-tuning on that data using a standard off-Policy RL method."
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper proposes a gradient-based meta-learning algorithm Eigen-Reptile to address the meta-overfitting problem (overfitting on sampling and label noise) in few-shot learning (FSL). The main idea is to use the main direction of historical taskspecific parameters to alleviate gradient noise. The main direction is computed by a special mechanism for the parameter’s large size. Furthermore, the authors propose Introspective Self-paced Learning (ISPL) that constructs a plurality of prior models to determine which sample should be abandoned. Theoretical and empirical results show the effectiveness of the proposed method."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper proposes Adversarial batch normalization (AdvBN) to make neural networks robust to changes in image style and appearance, rather than small perturbations at the pixel level. The authors formulate a min-max game in which an adversary chooses adversarial feature statistics, and network parameters are then updated to resist these changes in feature space that correspond to appearance differences of input images. They find that adversarial fine-tuning on features perturbed in this way improves robustness to data stylization and corruption without ever training on auxiliary data. Training with AdvBN is computationally cheap and can quickly make pre-trained models less brittle."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,This paper proposes a new metric called Variance of Gradients (VoG) as a proxy metric for detecting outliers in the data distribution. The authors claim that VoG is a meaningful way to rank data by difficulty and can be used to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. The paper is well-written and easy to follow. The empirical results show that the proposed method is effective in identifying atypical examples.
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"This paper proposes a technique to improve samples from deep generative models by refining them using gradient flow of entropy-regularized f-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, the proposed method avoids wasteful sample rejection used by previous methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN variants, the refinement approach can be applied to GANs with vector-valued critics and even other deep models such as VAEs and Normalizing Flows."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents a variable encoder-decoder (VECO) pretraining approach to unify the two mainstreams in both model architectures and pre-training tasks. VECO splits the standard Transformer block into several sub-modules trained with both inner-sequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub- modules for understanding and generation tasks during inference. The proposed approach achieves new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval. "
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper proposes an intrinsic reward for deep reinforcement learning (RL) based on the prediction of auditory events. Specifically, the authors propose to use the error of the neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. The proposed approach is evaluated on Atari games, Habitat and TDW. The results show that the proposed approach outperforms the baselines."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper studies the problem of novel category discovery on single-and-multi-modal data with labels from different but relevant categories. The authors propose a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, the authors take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabeled data. In particular, they propose using category discrimination on labelled data and cross-modality discrimination on multi-modual data to augment instance discrimination used in conventional contrastive learning approaches. They further employ Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabeling data to better predict cluster assignments."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"This paper proposes a method for weakly-supervised semantic segmentation of images. The authors formulate the task as a metric learning problem, where pixels of the same semantics need to be mapped to the same (distinctive) features. They propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity. The pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. They deliver a universal weakly supervised semantic segmenter with significant gains on Pascal VOC."
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper proposes a new self-supervised distillation method, named BINGO, which bags related instances by matching embeddings of the teacher. The bag of instances indicates a set of similar samples constructed by the teacher and are grouped within a bag, and the goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. The method achieves new state-of-the-art performance on small scale models, i.e., 65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet."
SP:328866aad6544c81ded8980934df31dc4472435f,"This paper proposes an adversarial approach to simulation-based inference (SBI) for high-dimensional data. The authors propose to reformulate the variational objective in the adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, can work in high dimensional posterior spaces and supports implicit priors. The proposed approach is evaluated on two SBI benchmark problems and on two high dimensional simulators. On a model for wave propagation on the surface of a shallow water body, the authors show that the proposed approach can return well-calibrated posterior estimates even in high dimensions."
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"This paper proposes a method for the identification and estimation of treatment effects (TEs) under limited overlap, i.e., when subjects with certain features belong to a single treatment group. The authors use a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs, and build a generative prognostic model. The model is learned as a beta-Intact-VAE---a new type of variational autoencoder (VAE). The authors derive the TE error bounds that enable representations balanced for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using synthetic datasets."
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a framework for autonomous reinforcement learning (ARL) where the agent is not only learning through its own experience, but also contends with lack of human supervision to reset between trials. They introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. They show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"This paper investigates the state-of-the-art GNN-based QA systems, and discovers that they are over-parameterized and over-complex. The authors claim that the initial node embeddings and some GNN layers are completely dispensable. Furthermore, their work reveals that GNN essentially works as a counter in the QA reasoning process. To verify this point, they design soft/hard counter models, which achieve comparable or even better experimental results than existing GNN based methods."
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a method to improve the inference performance while keeping the model compressed near-optimally. The key insight of the paper is the concept of Succinct Data Structures, which supports fast queries directly on compressed representation without decompression. The method first transforms DNN models as their proposed formulations in either Element-wise or Block-wise manner, so that it can take advantage of. Then, the method compresses the transformed DNN model using the compressed data structures. Finally, it exploits the specialized execution pipelines for different model formulations, to retrieve relevant data for DNN inference."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes a new training method for improving the performance of tiny neural networks. The authors argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a generative adversarial network (GAN) for super-resolution of dynamic point cloud sequences without requiring point correspondence annotations. The proposed model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, the authors propose a learnable masking module to adaptively upsample points that have a irregular density distribution. This allows the model to produce non-uniform point distribution that matches the target distribution."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,This paper proposes a new method for fully pre-training an encoder-only transformer and fine-tuning it for object detection via a task adapter. The proposed method is inspired by the success of textual prompts in NLP and treats query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. Experiments on the challenging COCO dataset demonstrate that the proposed method achieves competitive performance.
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper proposes a new federated learning algorithm, FedPAGE, which uses the optimal PAGE method to further reduce the communication complexity for convex and non-convex optimization. Theoretical results show that the number of communication rounds is O(3/4 S^2/3) in the convex setting, improving the best-known result of SCAFFOLD (Karimireddy et al., 2020) by a factor of N, where N is the total number of clients, S is the sampled subset of clients in each communication round, and is the target error. In the non-consistency setting, the communication rounds are O(\sqrt{N+S^2}/3 S^3/3^2) and O(N^2 S^4/3 N^3 S^{2/4}) respectively. The authors also conduct several numerical experiments to verify the effectiveness of the proposed algorithm."
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper studies the decision boundary geometry of ANN classifiers by utilizing adversarial perturbations. The authors define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundaries from any given input sample. The distance to the boundary grows smoothly and sub-linearly as one increases the dimensionality of the subspace. The boundary curvature is more curved in the adversarial space than within a random subspace of equal dimensionality."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,This paper proposes a clustering-based contrastive learning method for weakly-supervised representation learning. The idea is to cluster data according to its auxiliary information and learn similar representations within the same cluster and dissimilar representations for data from different clusters. The proposed method is evaluated on the UT-50K dataset and compared with other baseline representation learning methods that also leverage auxiliary data information.
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper proposes a learning-based iterative sparsity recovery algorithm (PLISA) for sparse estimation problems. The algorithm is based on unrolling a path-following algorithm, with some components being more flexible and learnable. The authors theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, they analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a method to learn a compact and decodable latent representation space for discrete-continuous hybrid action space. The proposed method, called HyAR, constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variational Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space, which can be easily extended with modern DRL methods."
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper proposes SGEM, Stochastic Gradient with Energy and Momentum, to solve a large class of general non-convex stochastic optimization problems, based on the AEGD method that originated in the work [AEGD: Adaptive Gradient Descent with Energy. arXiv:2010.05109]. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. The authors show that SGEM features an unconditional energy stability property, and derive energy-dependent convergence rates in the general non convex setting, as well as a regret bound in the online convex settings. A lower threshold for the energy variable is also provided. "
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a Conditional Masked Language Model with Correction (CMLMC) to improve the performance of non-autoregressive (NAR) Transformer-based machine translation models. The authors identify two shortcomings of CMLM, i.e., the indistinguishability of tokens and the mismatch between training and inference, and propose a method to address them. CMLMC is evaluated on both raw and distilled data, and achieves state-of-the-art NAR performance on multiple datasets."
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper proposes a neural network architecture inspired by the WaveNet. The main difference between the proposed architecture and WaveNet is that WaveNet stores the activations of each neuron depending on their kernel size and dilation value. In contrast, the proposed WaveSense does not buffer any spikes(activations) in contrast to WaveNet, which stores the information of the neuron and synaptic states. This makes the proposed network extremely efficient in terms of memory utilization. The proposed network achieves near state-of-the-art performance of artificial neural networks and LSTMs."
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper proposes Shifty, a class of algorithms that provide high-confidence behavioral guarantees that hold under demographic shift. The proposed algorithms allow the proportions of demographics to change after training, provided the user has some information describing this change. Shifty algorithms can be used when the new demographic proportions are known, or when these proportions are bounded in known intervals. The authors evaluate Shifty on a real-world dataset of university entrance exams and show that the learned models avoid bias under demographic shifts."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes a meta-learning approach for multi-stage stochastic optimization (MSSO), which extends the state-of-the-art method Stochastic Dual Dynamic Programming (SDDP) by introducing a trainable neural model that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that it can accelerate optimization performance on new instances. The proposed neural SDDP continually self-improves by solving successive problems. Experiments show that the proposed method can significantly reduce problem solving cost without sacrificing solution quality over competitors such as SDDP and reinforcement learning algorithms, across a range of synthetic and real-world process optimization problems."
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a protocol for private next-token prediction, called SUBMIX, to prevent privacy violations by language models that were fine-tuned on a private corpus after pre-training on a public corpus. The authors show that the proposed protocol limits the leakage of information that is unique to any individual user in the private corpus via a relaxation of group differentially private prediction. Importantly, the proposed method admits a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing data-extraction attacks while maintaining the utility of the language model."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate with respect to a classification model’s intermediate activations on indistribution samples. The authors leverage a recent insight about label smoothing, which they call the Label Smoothed Embedding Hypothesis, and show that one of the implications is that the proposed method performs better as an OOD detection method both theoretically and empirically when the model is trained with Label smoothing. They show that their proposal outperforms many OOD baselines and provide new finite-sample high-probability statistical results."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"This paper proposes a new method for learning representations for semi-supervised image classification. The proposed method is based on diffusion-based methods represented as stochastic differential equations on a continuous time domain, which has recently proven successful as a non-adversarial generative model. Training models relies on denoising score matching, which can be seen as multi-scale autoencoders. Here, the authors propose to learn an infinite-dimensional latent code which achieves improvements of state-of-the-art models on semi-Supervised Image Classification. The paper is well-written and easy to follow."
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"This paper proposes an algorithm for goal-conditioned reinforcement learning that learns to reach distant goals by using planning at training time to automatically generate a curriculum of intermediate states. The proposed algorithm, called C-Planning, is a combination of two existing methods: E-Step and M-Step. The E-step learns to plan a sequence of waypoints using graph planning, while the M-step aims to learn a goal-conditional policy to reach those waypoints. Unlike prior methods, the proposed method performs planning only during training and not testing, significantly decreasing the compute costs of deploying the learned policy. Empirically, it is able to solve very long horizons manipulation and navigation tasks, tasks that prior methods and methods based on graph search fail to solve."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper extends mixup to k-mixup, i.e., perturbing k-batches of training data in the direction of other k-bits using displacement interpolation. The authors demonstrate theoretically and in simulations that k-Mixup preserves cluster and manifold structures, and extend theory studying the efficacy of standard mixup from the k=1 to k=k case. The empirical results show that training with k- mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,This paper proposes a non-linear kernelized classification layer for deep neural networks aiming to extract the best possible classifier with embeddings produced by a given representation learner. The proposed method maps the embedding vectors into a high-dimensional Reproducing Kernel Hilbert Space (RKHS) that optimally separates them into different classes. The authors theoretically show that their classification layer optimizes over all possible radial kernel functions to learn an optimal nonlinear classifier. They demonstrate the usefulness of this layer in learning more model-efficient classifiers in a number of computer vision and natural language processing tasks.
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"This paper investigates the sources of bias in node representations obtained via Graph Neural Networks (GNNs). The analysis reveals that both nodal features and graph structure lead to bias in the obtained representations. Based on the analysis, fairness-aware data augmentation frameworks are developed to reduce the intrinsic bias. Extensive experiments on node classification and link prediction are carried out over real networks in the context of graph contrastive learning."
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,This paper considers the problem of estimating treatment effects from observational data in the presence of unmeasured confounders. The authors propose a Confounder Balanced IV Regression (CB-IV) algorithm to jointly remove the bias from the unmeasureable confounder with IV regression and achieve better bias-variance trade-off in imbalanced treatment distributions due to the observed confoundsers by balancing for treatment effect estimation. The proposed method consists of three main modules: (1) treatment regression: regressing the treatment with IVs and confounds like previous nonlinear IV methods for removing the confounding from unme measureable confounds; (2) confoundER balancing: learning a balanced representation of confounds to eliminate the bias induced by the observed ones; and (3) outcome regression. Extensive experiments show that the proposed method achieves state-of-the-art performance in the treatment effects estimation.
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper analyzes the effect of MAML (Model-Agnostic Meta-Learning) on the performance of hard tasks in the linear regression setting. In particular, the authors show that the hardness of the tasks is related to the rate at which gradient descent converges on the task. They also show that in order to achieve substantial gain over NAL, there must be some discrepancy in hardness among the tasks, and the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions. The authors also provide numerical and analytical results suggesting that these insights apply to two-layer neural networks. "
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes a method for unrolling sparse sparse source separation (BSS) methods. The method is based on the Proximal Alternating Linearized Linearized Minimization (PALM) algorithm, which is a classical sparse BSS method. The authors propose to learn the hyperparameters and variables of the proposed LPALM method, which can leverage the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameter and variables. In particular, the proposed method is able to deal with variable mixing matrices (a.k.a. dictionaries) and thus enables to perform semi-blind source separation. The proposed method outperforms other unrolled source separation methods in the semi- blind setting."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper proposes a Legendre Memory Unit-based model for language modeling. The proposed model is based on the non-parametric Linear Time-Invariant (LTI) component of the Legendre memory unit (Voelker et al., 2019). The authors introduce a novel attention module called implicit self-attention, which is a modified attention mechanism that operates only on the output of the LMU at each time step, and not across time steps. The authors show that for the same amount of training the proposed model improves the loss over transformers about as much as transformers improve over LSTMs."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"This paper proposes LatentKeypointGAN, an end-to-end generative model that generates images with a set of space keypoints. These keypoints have associated appearance embeddings that respectively control the position and style of the generated objects and their generated images. The keypoints can be used to re-arrange the generated images by re-positioning and exchanging keypoint embedding. In addition, the paper proposes a new, GAN-based method for unsupervised keypoint detection."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper studies deep fully-connected neural networks with layer normalization using the mean field formalism, and carry out a non-perturbative analysis of signal propagation. The authors demonstrate that increasing the depth leads to gradient explosion or to another undesirable phenomenon we call representation shrinkage. The appearance of at least one of these problems is not restricted to a specific initialization scheme or a choice of activation function, but rather is an inherent property of the fully connected architecture itself. Additionally, the authors show that many popular normalization techniques fail to mitigate these problems."
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper proposes a line search method for learning rate scheduling for stochastic gradient descent (SGD) in deep learning. The paper builds on recent empirical findings that the full-batch loss tends to have a simple parabolic shape in SGD update step direction and that the trend of the optimal update step size changes slowly (Mutschler & Zell, 2021). Based on these and more found observations, this paper introduces a line-search method that approximates the full batch loss with a parabola estimated over several mini-batches. The learning rates are derived from parabolas during training. The experiments show that the proposed method outperforms SGD tuned with a piece-wise constant learning rate schedule and other line search approaches for Deep Learning across models, datasets, and batch sizes on validation and test accuracy."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper provides a theoretical analysis of noise-contrastive estimation (NCE) in the presence of ill-behaved loss landscape. In particular, the authors show that when the noise distribution is uninformative, the loss landscape can become extremely flat, forcing standard optimization methods to take an exponential number of steps to converge to a good parameter estimate. To address this issue, this paper proposes a new NCE method called eNCE, which uses an exponential loss and for which normalized gradient descent addresses the landscape issues provably when the target and noise distributions are in a given exponential family. The proposed method is shown to outperform existing methods."
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper studies the convergence of distributed SGD under Byzantine fault, where a fraction of the workers are malicious (Byzantine) and the other fraction are honest. The authors first show that the integration of standard practices in differential privacy (DP) and Byzantine resilience (BR) is not straightforward, and show that many existing results are invalid when honest workers enforce DP. To circumvent this shortcoming, the authors revisit the theory of (alpha, f)-BR to obtain an approximate convergence guarantee. The analysis provides key insights on how to improve this guarantee through hyperparameter optimization."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper considers the problem of code editing with few exemplars, i.e., code snippets that contain the original and modified support code snippets. The authors propose a novel deep learning approach to solve this code editing problem automatically. Their approach combines edit representations extracted from support exemplars and compositionally generalizes them to the query code snippet editing via multi-extent similarities ensemble. Specifically, they parse the support and query code snippets using language-specific grammar into abstract syntax trees. They apply the similarities measurement in multiple extents from individual nodes to collective tree representations for query and support sample matching, and ensemble the matching results through a similarity-ranking error estimator. They evaluate the proposed method on C# and Python datasets."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a neurosymbolic generative model for generating sequence data with high-level structure. The authors propose a program synthesis algorithm that infers the relational constraints present in the training data, and then learns a generative models based on the resulting constraint data. They show that the proposed approach significantly improves over state-of-the-art in terms of capturing high- level structure in the data, while performing comparably or better on the low-level structures."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper studies the problem of set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. In this paper, the authors address two common scaling problems encountered in such tasks: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. They propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. They also introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows them to skip iterations in the backward pass for improved efficiency and constant memory usage. Finally, they combine both contributions in a single set to hypergraph model that enables them to address problems with larger input set sizes."
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a post-processing method to mitigate bias in face recognition models. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre-trained model to give more representation power to the discriminated subgroups. Its training is supervised by the von Mises-Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper proposes a novel method PlaceboCIL for tackling the forgetting problem in CIL tasks. The proposed method selects high-quality placebo data from free data streams, and use them to improve the effect of KD between the models learned in adjacent phases, while not harming the learning of new classes in the new model. An RL algorithm is designed to make the selection of placebos more adaptive in different phases. Extensive experiments on multiple baselines show that the proposed method is general and efficient."
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes a method for sampling from discrete energy-based models (EBMs) using MCMC with a composition of local moves to efficiently explore large neighborhoods. The authors also propose a fast version of their algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, they show that their path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper proposes Variational Predictive Routing (VPR), an event-based hierarchical generative model that is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model’s latent hierarchy. The proposed method is based on a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. The authors show that VPR can detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate timeagnostic rollouts of the future."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,This paper proposes a new method for image retrieval that combines global and local features. The proposed method is an end-to-end and single-stage pipeline that learns local feature matching with convolutional neural networks instead of RANSAC algorithm. It also learns more accurate and semantic local information through combining spatial and channel attention with the aid of intermediate supervision. Experiments on Oxford and Paris datasets validate the effectiveness of the proposed method.
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper proposes RotoGrad, an algorithm that tackles negative transfer as a whole by homogenizing both gradient magnitudes and directions across tasks. The main idea is to re-weight task gradients at each step of the training while encouraging learning those tasks that have converged the least thus ensuring training convergence. Additionally, instead of directly modifying gradient directions, the proposed algorithm smoothly rotates the shared feature space differently for each task in order to avoid conflicting gradients. Experiments on CIFAR-10 and NYUv2 datasets show that the proposed method outperforms competing methods."
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"This paper proposes a method to fuse heterogeneous neural networks via cross-layer alignment. The proposed method extends layer-wise model fusion via optimal transport, named OTFusion, which applies soft neuron association for unifying different pre-trained networks to save computational resources. The authors propose to fuse neural networks with a different number of layers, which they refer to as heterogeneous Neural Networks (HNN). The proposed approach is based on solving a cross layer alignment problem, followed by a layer balancing step. Experiments on synthetic data and CIFAR10 dataset show that the proposed method achieves better performance compared to the individual networks trained on heterogeneous data without the need for any retraining."
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper studies the effect of implicit regularization in deep reinforcement learning (RL). In particular, the authors show that the regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. They derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicitly regularizer. Experiments are conducted on Atari 2600 games, D4RL domains and robotic manipulation from images."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes an extension of randomized least-square value iteration (RLSVI) to deep reinforcement learning. RLSVI is a well-known Bayesian algorithm that aims to address the exploration and exploitation trade-off problem in deep RL. The authors propose a new algorithm named HyperDQN that extends RLSVIs to deep RL by using a probabilistic hypermodel. The hypermodel is trained to generate approximate posterior samples regarding the parameter of the Q-value function. As a result, diverse Q-values are sampled to select exploratory action sequences, which retains the punchline of RLSVi for efficient exploration. "
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper proposes a method to learn causal representations from observational data by regularizing the learning procedure with mutual information measures according to a hypothetical causal graph. The proposed method involves a counterfactual loss, based on which the authors deduce a theoretical guarantee that the causality-inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by the approach is robust under adversarial attacks and distribution shift."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,This paper proposes a progressive training framework for efficient and effective federated learning. The proposed ProgFed can reduce computation and two-way communication costs while maintaining the strong performance of the final models. Theoretical analysis shows that the proposed method converges at the same asymptotic rate as standard training on full models. Extensive results on different architectures from small CNNs to U-Net and different tasks from simple classification to medical image segmentation show that the method is communication-efficient and effective.
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the generalization of adversarial training through the lens of the adversarial Rademacher complexity of deep neural networks. The authors provide upper bounds of the robustness to adversarial attacks for two-layer and depth-d neural networks, and show that the adversarially-trained weight norms are larger than the standard trained weight norms, thus providing an explanation for the bad generalization performance. The paper is well-written and easy to follow. "
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a kernel-based estimator for the differential entropy (DE) and mutual information (MI) for deep learning applications. The proposed estimator, called KNIFE, is parameterized, differentiable and can be used as part of any training objective, where differential entropy or mutual information estimation is desired. Experiments on a variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of the estimator. "
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"This paper proposes a new soft-greedy operator, called resmax, that takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space, but focuses exploration more on potentially promising actions like softmax. Further, it does not concentrate probability as quickly as softmax, and so better avoids overemphasizing sub-optimal actions that appear high-valued during learning. The authors prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a nonexpansion (called mellowmax)."
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper proposes a method to control the model's learnability on a specific dataset with a special key. In particular, they propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become “unlearnable” by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that it can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered."
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper proposes a method for handling missing node features in graph neural networks. The proposed method is based on minimizing the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which is called Feature Propagation. Experimental results show that the proposed approach outperforms previous methods on common node-classification benchmarks and can withstand surprisingly high rates of missing features."
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"This paper studies the problem of selecting a subset of the unlabeled data to label for active learning. The authors propose to minimize the discrete Wasserstein distance between the set to be labeled and the set of labeled data. They prove that this distance bounds the difference between training with a finite versus an unlimited labeling budget. Then, they propose a Generalized Benders Decomposition algorithm to solve the integer optimization problem. The proposed method is evaluated on four image classification tasks and shows good performance."
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a method for de novo genome assembly based on graph neural networks and finding a path through the assembly graph. A graph convolutional network is trained on a dataset generated from human genomic data to reconstruct the genome by finding a route through the graph. The authors show that their model can compute scores from the lengths of the overlaps between the sequences and the graph topology which, when traversed with a greedy search algorithm, outperforms the greedy search over the overlap lengths only. Moreover, the method reconstructs the correct path in the fraction of time required for the state-of-the-art genome assembly methods."
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a method for meta-learning to improve the performance of online-aware meta-continual learning (OML) by incorporating experience replay (ER) into meta-training. The authors propose to store the samples’ representations, instead of the samples themselves, into the replay buffer. They also propose a meta-learned Predictive Sample Selection to replace the widely used reservoir sampling to populate the replay buffers. Experimental results show that the proposed method outperforms the state-of-the-art on a number of real-world benchmark data sets."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper proposes a method for multi-agent joint Q-learning based on centralized training with decentralized execution (CTDE). The authors formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q-values to explicitly maximize the joint Q value. Theoretically, they give a gradient ascent solution for this problem. Empirically, the authors instantiate the core idea with deep neural networks and propose Explicit Credit Assignment Joint Q-Learning (ECAQ). Extensive experiments justify that ECAQ achieves interpretable credit assignment and superior performance compared to several advanced baselines."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper studies adversarial attacks against transductive learning-based adversarial defenses. The authors formulate and analyze threat models for these defenses and propose an attack framework called Greedy Model Space Attack (GMSA) that can be used as a new baseline for evaluating the adversarial robustness of these defenses. They show that GMSA, even with weak instantiations, can break previous transductian-learning based defenses, which were resilient to previous attacks, such as AutoAttack. On the positive side, they report a somewhat surprising empirical result of “transductive adversarial training”: Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks."
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies the problem of batch normalization (BN) in the context of neural network training. The authors cast BN as an approximation of the limiting case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. They demonstrate an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation, and a fully per-example training procedure. They further use their insights to improve batch renormalization for very small minibatches."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper proposes a low-rank adaptation method to reduce the number of trainable parameters for downstream tasks. The proposed method freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. Compared to GPT-3 175B fine-tuned with Adam, the proposed method can reduce the total number of parameters by 10,000 times and the GPU memory requirement by 3 times. The authors also provide an empirical investigation into rank-deficiency in language model adaptation."
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a regular-constrained linear-chain conditional random field (CRF) model, which is a generalization of standard CRF that can enforce a broad class of constraints, including non-local ones, by specifying the space of possible output structures as a regular language L. The authors prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice. They demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF into a deep neural model for semantic role labeling, exceeding state-of-the-art results on a standard dataset."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"This paper proposes two neural networks for camera-based physiological measurement, EfficientPhys, that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. They achieve state-of-the-art accuracy on three public datasets. They also evaluate the latency of the proposed networks and show that their most light weight network also achieves a 33% improvement in efficiency."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,This paper proposes hardware-aware latency pruning (HALP) that focuses on structured pruning for underlying hardware towards latency budgets. The authors formulate pruning as a resource allocation optimization problem to achieve maximum accuracy within a given latency budget. They further propose a latency-aware neuron grouping scheme to further improve latency reduction. The proposed method is evaluated on both classification and detection tasks on ImageNet and VOC datasets.
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes a molecular graph generation method via energy-based models (EBMs) to perform permutation-invariant and multi-objective molecule generation. The authors propose to learn the energy function by contrastive divergence and generate samples by Langevin dynamics. To generate molecules with a specific desirable property, they propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. Further, they explore to use their GraphEBM for generating molecules towards multiple objectives via compositional generation, which is practically desired in drug discovery."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"This paper proposes a neural network-based approach for learning to search for programs. The approach, called CROSSBEAM, uses the neural model to choose how to combine previously explored programs into new programs, taking into account the search history and partial program executions. The model is trained on-policy using data extracted from its own bottom-up searches on training tasks. The proposed approach is evaluated in two very different domains, string manipulation and logic programming. "
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper proposes an alternative to the standard squared Bellman error in deep reinforcement learning (DQN). The proposed method, called FR Squared Bellman Error (FR DQN), augments the squared bellman error with a functional regularizer, which allows to use up-to-date parameters as well as control the regularization. The method is evaluated on a range of Atari environments and compared with target-network based methods."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"This paper proposes a method to improve the expressive power of message-passing graph neural networks (GNNs) for node classification and graph classification. The proposed method is based on the idea of local isomorphism of subgraphs, and the authors prove that it is strictly more expressive than the Weisfeiler Lehman test (WL) in distinguishing graph structures. The authors empirically verify the strength of the proposed method on different graph learning tasks."
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a method for uncertainty quantification of neural networks (NNs) based on linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. The authors theoretically prove that PI3NN can calculate PIs without retraining NNs and it completely avoids the crossing issue. Moreover, it does not introduce any unusual hyperparameters resulting in a stable performance."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes a meta-learning method for online continual adaptation. The proposed method is based on MAML, where the parameters of the meta-network are updated online on each new batch of data, and a vector of meta-parameters are updated correspondingly with meta-updates to accelerate the online adaptation process, and influence the online updates via a regularizer. The authors propose a task sampling scheme that selects datapoints at random from a buffer of all seen data, which enables effective meta-training that accelerates the speed with which FOML can adapt to each new task. Experiments on Rainbow-MNIST and CIFAR100 datasets show that the proposed method achieves comparable performance to or better than baselines and prior methods."
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"This paper proposes a differentiable scaffolding tree (DST) to make a molecular graph locally differentiable, allowing a continuous gradient-based optimization. It is the first attempt to make the molecular optimization problem differentiable at the substructure level, rather than resorting to latent spaces or using RL/evolutionary algorithms. The authors constructed a general molecular optimization strategy based on DST, corroborated by thorough empirical studies."
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,This paper proposes a method to predict the target lab test result of a patient based on the drug-lab interaction and diagnosis-lab interactions. The authors propose a knowledge-augmented approach to predict patients’ response for a target lab result. They also take into consideration patients' past lab responses to personalize the prediction. Experiments on two real-world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors.
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper proposes a new open-set single domain generalization (OS-SDG) task, which aims to generalize models from a single source domain to unseen target domains and simultaneously deals with unknown classes. The authors propose a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. It also adopts a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by the model. Experimental results on benchmark datasets prove the effectiveness of CrossMatch."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper proposes Wasserstein policy optimization (WPO) and Sinkhorn trust region optimization (SPO) methods for policy optimization in reinforcement learning. WPO and SPO are natural extensions of TRPO and PPO, respectively. The main idea is to optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, the authors show that WPO guarantees a monotonic performance improvement, while SPO provably converges to WPO as the entropic regularizer diminishes. Experiments on tabular domains and robotic locomotion tasks demonstrate the performance improvement of both approaches."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper proposes a general framework for understanding the effect of weight perturbations on the forgetting and re-training of neural networks. The authors claim that the weight perturbing step is responsible for the disproportionate forgetting of undesirable information, and propose a forgetting-and-re-learning framework that unifies many existing iterative training algorithms in the literature. They also show that there exists a Goldilocks zone of forgetting, i.e., forget too little and the network could easily retrain to the same basin; forget too much and the networks would fail to accumulate progress over multiple relearning rounds."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper investigates the offline-online setting, where the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. In the experiments, the authors show that standard RL agents trained in an offline-Online manner can outperform agents trained only offline or online, sometimes by a large margin."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper studies the problem of meta-learning for domain generalization (DG), where the target domain is invisible and thus explicit training on the target is impossible. The authors propose a discrepancy-optimal meta-learner that learns to optimize Y-discrepancy between the unseen target domain and source domains only using source-domain samples. Theoretically, the authors give a PAC-style generalization bound for the proposed method and compare it with other DG bounds including ERM and domain-invariant learning. The theoretical analyses show that there is a tradeoff between classification performance and computational complexity for discrepancy-optimality meta-learners. "
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the role of policy and value networks in best-first search and Monte Carlo tree search (MCTS) for solving hard Sokoban planning problems. The authors show that the policy network plays an important role in guiding the search, which can lead to left heavy tails with polynomial scaling by avoiding exploring exponentially sized sub-trees. They also show that random restarts can improve the overall search effectiveness with larger search budgets."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,This paper proposes a method for meta-imitation learning from videos of human demonstrations. The main idea is to use a generative model (A-CycleGAN) to translate the human videos to the robot domain and train the meta-policy in the imagined compact latent space. The proposed method is evaluated on a set of vision-based tasks and shows comparable performance to the baseline. 
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper proposes LAWN, a simple and powerful method of modifying deep net training with a base optimizer to improve weight adaptivity and lead to improved generalization. Switching from free to weight norm constrained training at an appropriate point is a key element of the method. The authors study the performance of the LAWN technique on a variety of tasks, optimizers and batch sizes, demonstrating its efficacy."
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper proposes a method for learning group equivariant convolutional neural networks (G-CNNs) that is able to learn both partial and full equivariance at every layer. In particular, the authors propose to learn the equivariances at each layer based on the probability distribution defined over group elements. The proposed method is applicable to discrete groups, continuous groups, and combinations thereof. The authors show that the proposed method performs on par with G-CNN and outperforms them in settings in which partial group-equivariance better represents the data."
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes amortized Langevin dynamics (ALD) for deep latent variable models (DLVMs), which is a variant of variational inference (VI) that replaces optimization of datapoint-wise variational parameters with an inference model that predicts latent variables from observations. The authors prove that ALD has the target posterior as a stationary distribution under some assumptions. They also extend ALD to sampling from an unconditional distribution, enabling more flexible generative modeling by applying it to the prior distribution of the latent variable. Based on ALD, they construct a new deep latent variables model named the Langevin autoencoder (LAE), which uses ALD for posterior inference and sampling from the latent space EBM."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper proposes a method for hypergraph reasoning, i.e., predicting the relationship between several entities based on the input facts. The authors observe that in logical reasoning, logical rules (e.g., my parent’s parent is my grandparent) usually apply locally and sparsely. Inspired by these observations, the authors propose Sparse and local Neural Logic Machines (SpaLoc) to leverage the sparsity in hypergraph neural networks, which represents the grounding of relationships as sparse tensors and uses neural networks and finite-domain quantification operations to infer new facts. To enable training on large-scale graphs as real-world knowledge graphs, SpaLoc makes training and inference-time sub-sampling of the input graphs. To remedy the information loss in sampled sub-graphs, a novel sampling and label calibration paradigm based on an information-theoretic measure information sufficiency."
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper proposes a new family of differentiable top-k cross-entropy classification losses for classification, where k is conventionally a positive integer. The authors relax this assumption and propose to draw k from a probability distribution for training. They show that relaxing k leads to better top-5 accuracy and also makes models more robust, which leads to top-1 accuracy improvements."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a new method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. The proposed method builds on the celebrated Douglas-Rachford splitting technique and tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows the authors to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel."
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper proposes a framework for disentangling performance gaps in federated learning (FL) by separating performance gaps from unseen client data (out-of-sample gap) and performance gaps between unseen client distributions (participation gap). The authors propose to model clients’ data distributions as drawn from a meta population distribution (Wang et al., 2021), an assumption that they argue is reasonable in real-world FL settings. They use this framing to define two generalization gaps to study in FL: (1) Out-Of-Sample Gap, which is the difference between empirical and expected risk for participating clients, and (2) Participation Gap, or the difference in expected risk between participating and non-participating clients. They observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in FL."
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper studies the potential and limitations of prompt-based language models (PLMs) for the zero-shot setting. It proposes a simple Multi-Null Prompting (without manually/automatically created prompts) strategy that yields very promising results on a few widely-used datasets, e.g., 86.59%(±0.59) accuracy on the IMDB dataset, and 86.22%(**2.71) accuracy  on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 74.06%(~13.04) for comparison. However, the paper also observes some limitations of PLMs under the zero shot setting, particularly for the language understanding tasks (e.g. GLUE)."
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"This paper proposes a method to improve the alignment in attention mechanism through a sharpener module. Specifically, the sharpener aims to align the relevant parts of the encoded image with the target output. The paper claims that the alignment and interpretability of attention can be significantly improved. Experiments on synthetic handwritten digits and real-world scene text recognition datasets show that the proposed method outperforms the mainstream ones such as soft and hard attention."
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper proposes a method for learning to construct a complete tour plan for the vehicle routing problem (VRP). The proposed method is based on the Permutation Invariant Pooling Model (PIM), which is a reinforcement learning framework for learning a tour-plan for VRP. The proposed approach is evaluated on the VRP20, VRP50, and VRP100 problem sets, and compared with several state-of-the-art approaches. "
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a method for link prediction based on counterfactual inference. In particular, the authors propose a method that uses a causal model to learn the causal relationship between two sets of variables: (1) the observed graph structure (e.g., clustering effect) and (2) the existence of link between a pair of nodes. The proposed method is evaluated on several benchmark datasets and achieves state-of-the-art performance."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper proposes a two-stage Second-order unsupervised feature selection via knowledge contrastive disTillation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for feature selection. In the first stage, the authors learn a sparse attention matrix that can represent second- order relations between features. The second stage, they build a relational graph based on the learned attention matrix and perform graph segmentation. Experimental results show that SOFT outperforms classical and recent state-of-the-art methods."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"This paper proposes a novel approach for learning multimodal variational autoencoders (VAEs) that combines information between modalities implicitly through mutual supervision. The proposed approach is based on semi-supervised VAEs. The authors show that the proposed approach can be applied to both partial and complete data sets, and outperforms the existing approaches on MNIST-SVHN (image-image) and CUB (image–text) datasets. They also compare the quality of the representations learned by mutual supervision against standard approaches."
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes Deep Explore Options, a method for combining intrinsic and extrinsic rewards in deep reinforcement learning (RL). Intrinsic Motivation allows a RL agent to generate directed behaviors in an environment, even with sparse or noisy rewards. Explore Options let the agent call an intrinsically motivated agent in order to observe and learn from interesting behaviors in the environment. This paper proposes to consider intrinsic reward learning as an auxiliary task, with a resulting architecture achieving 50% faster wall-clock speed and building a stronger, shared representation."
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,This paper proposes a method for learning Hamiltonian dynamical systems from data. The authors propose a new metric SAI (stiffness-aware index) to classify the training data into stiff and non-stiff portions. This classification along with a resampling technique allows them to apply step size adaptation strategies to better capture the dynamical characteristics of the Hamiltonian vector fields. The proposed method outperforms the state-of-the-art approaches with a significant margin.
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper proposes to train Transformer-based language models to perform multi-step computations by asking them to emit intermediate computation steps into a “scratchpad” before producing the final answer. The scratchpad consists of a sequence of intermediate tokens, which are the results of a standard addition algorithm. The paper shows that the scratchpad can be used to improve the performance of language models on a series of tasks ranging from long addition to the execution of arbitrary programs."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"This paper proposes a feature-level adversarial perturbations for deep neural networks that are interpretable, universal to any source image, and physically-realizable. The proposed method is based on deep image generators and a novel optimization objective. The authors show that they are versatile and use them to generate targeted feature-levels attacks at the ImageNet scale that are simultaneously interpretable and universal. These attacks can also reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of natural objects."
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper proposes a reinforcement learning approach for simulated annealing (SA), which is a stochastic global optimisation technique applicable to a wide range of discrete and continuous variable problems. The authors frame the proposal distribution as a policy, which can be optimised for higher solution quality given a fixed computational budget. They demonstrate that this Neural SA with a learnt proposal distribution outperforms SA baselines with hand-selected parameters on a number of problems: Rosenbrock's function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. They also show that Neural SA scales well to large problems while again outperforming popular off-the-shelf solvers in terms of solution quality and wall clock time."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper proposes a new metric to measure the non-stationarity of a policy sequence in MARL. The metric is based on the KL-divergence of consecutive joint policies. The authors also propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust region of the local policies adaptively in an end-to-end manner. Experiments show that the proposed method can bring noticeable and stable performance improvement compared with baselines."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper proposes a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. The proposed method, Audio-Visual Hidden Unit BERT (AV-HuBERT), achieves state-of-the-art performance on the largest public lip-reading benchmark LRS3 (433 hours) trained with only 30 hours of labeled data, outperforming the former SOTA approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-read WER is further reduced to 26.9% when using all 433 hours of labelled data from LRS2 and combined with self-training."
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"This paper proposes a method for solving combinatorial optimization problems on graphs using reinforcement learning. The main idea is to use a single GNN to pre-process the problem and then use a recurrent unit to explore the solution space. The proposed method is evaluated on the Max-Cut problem, which is an NP-complete max-cut problem. The results show that the proposed method outperforms the state-of-the-art methods on the problem."
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper proposes to train VAEs with discrete latent variables by using evolutionary algorithms to optimize the parameters of the variational autoencoder (VAE). The authors show that this approach is more efficient than amortized VAE training, and is competitive in zero-shot denoising. The authors also show how the proposed approach is related to gradient ascent for the weights and evolutionary algorithms for the decoder. "
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"This paper proposes an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. The proposed Controlled Effect Network (CEN) is evaluated in a wide range of environments showing that it can accurately identify controlled effects. CEN is also evaluated as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a structure-regularized pruning (SRP) method to improve the performance of lightweight image super-resolution (SR) networks. The proposed method first prunes the weights of the residual blocks and then applies L2 regularization to drive the weights towards zero so that eventually their absence will cause minimal performance degradation. The authors apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-L and a very deep one SRPN. The experimental results show that the proposed method can achieve better performance than the existing methods."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a framework for cross-domain few-shot learning (CDFSL) that tackles large domain shift between base and novel categories. The proposed framework consists of three steps: pre-training a backbone network on a single-source dataset, a feature selection module on the target dataset, and fine-tuning the backbone network. The backbone network is trained in an unsupervised fashion where a self-supervised loss function is used. The masking module is trained to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuneed along with the backbone so that the backbone produces features similar to the relevant ones."
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,This paper studies the generalization properties of neural networks trained by gradient descent (GD) and Bayesian inference (BAI). The authors propose a generalization bound for the neural network-gaussian process (NNGP) posterior and show that GD can improve generalization by selecting networks with a large margin. The authors also show that the test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior.
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a method to improve the cross-lingual transfer performance of multilingual pre-trained models. The authors claim that the performance gap between the source and target languages is strongly associated with the representation discrepancy, and the proposed method, X-Mixup, aims to alleviate the discrepancy by adapting the manifold mixup between the target and source languages. Experiments on the XTREME benchmark show the effectiveness of X-mixup."
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of Byzantine robust distributed or federated learning, where a fraction of workers may deviate from the prescribed algorithm and send arbitrary messages to the central server. The authors propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. They also theoretically and empirically validate their approach, showing that combining bucketing with existing robust methods is effective against challenging attacks. "
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper studies the relationship between disentanglement and multi-task learning based on hard parameter sharing. The authors construct synthetic datasets that contain all necessary properties to be seen as a benchmark in this field. Next, the authors study the effects of multi- task learning with hard-parameter sharing on representation learning. They find that nontrivial disentangledness appears in the representations learned in the representation learning in the multi-Task setting."
SP:9851adb72e2918780f661f83f7da06eb866787be,"This paper proposes a framework of certifying robust policies (CROP) for reinforcement learning against adversarial state perturbations, which provides state level robustness certification and the first certification for cumulative rewards. Specifically, the authors develop a local smoothing algorithm that uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness of actions taken along this trajectory. Next, they develop a global smoothing algorithms for certifying the robusteness of a finite-horizon cumulative reward under adversarial attacks. Finally, they propose to use adaptive search to obtain tight certification bounds for the reward. "
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a new approach to conformal prediction, which aims to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. Conformal prediction provides the ability to adapt to model uncertainty by constructing a calibrated candidate set in place of a single prediction, with guarantees that the set contains the correct answer with high probability. In this paper, the authors propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i.e., the total number of false positives) is bounded according to a user-specified tolerance. Subject to this constraint, the algorithm optimizes for a generalized notion of set coverage that allows for any number of true answers for a given query (including zero). The authors demonstrate the effectiveness of this approach across a number of classification tasks in natural language processing, computer vision, and computational chemistry."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper studies the complexity of ReLU networks with random initialization. The authors prove that the expected length distortion does not grow with depth, and indeed shrinks slightly, for ReLU network with standard random initialization, and also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by the experiments. "
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes SAFEty skill pRiors (SAFER), a behavioral prior learning algorithm that accelerates policy learning on complex control tasks, under safety constraints. SAFER learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. In the inference stage, SAFER composes a safe and successful policy from the safety skills according to the inferred safety variable and abstract action. The authors demonstrate its effectiveness on several complex safety-critical robotic grasping tasks inspired by the game Operation."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a multi-branch network architecture for image restoration, inspired from the human retinal ganglion cells (HVS), which can achieve multiple restoration tasks in a general framework. The proposed method is evaluated on four datasets, including image dehazing, deraindrop, deblurring and deraining, which are very common applications for autonomous cars. The authors also propose a novel loss function for general restoration tasks and an MSC to replace the traditional skip connection. Both of them are proved to be effective in improving the restoration performance."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes a new federated learning setup, Inference-Time PFL (IT-PFL), where a model trained on a set of clients, needs to be later evaluated on novel unlabeled clients at inference time. The proposed approach is based on a hypernetwork module and an encoder module. Specifically, the encoder network learns a representation for a client given its unlabelled data. The client representation is fed to the hypernetwork that generates a personalized model for that client. Evaluated on four benchmark datasets, the proposed approach generalizes better than current FL and PFL methods. "
SP:960d0a63a82593f6e72275b65f0501f0469d1924,This paper proposes to use a conditional diffusion based generative model (RCDM) to visualize representations learned with self-supervised learning (SSL) models. The authors demonstrate how this model’s generation quality is on par with state-of-the-art generative models while being faithful to the representation used as conditioning. By using this new tool to analyze SSL (backbone) representation are not really invariant to many data augmentation they were trained on. SSL representations are more robust to small adversarial perturbation of their inputs.
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"This paper studies the problem of differentially private fractional frequency moments estimation. The authors prove that Fp sketch, a well-known streaming algorithm for fractional moments estimation, preserves differential privacy as is when p \in (0,1]. The main contribution of this paper is to show that the space complexity of the proposed algorithm is exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. "
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes a method for learning policies that are both locally optimal and sufficiently different from existing ones. The proposed method switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process to encourage the learning policy to consistently converge towards a previously undiscovered local optimum. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,This paper proposes a differentiable sampling method for diffusion models. The main idea is to optimize a perceptual loss over a space of diffusion processes that makes use of a pre-trained DDPM’s samples by leveraging the reparametrization trick and gradient rematerialization. The authors show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. The method is compatible with any diffusion model without fine-tuning.
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"This paper proposes P-Adapters, lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. The authors also investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (experts) and select one to query LLM and show that they require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. They perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa."
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes a method for continuous classification of time-series data. The proposed method is based on the idea of multi-distribution learning, which is to learn multiple distributions at the same time. The authors claim that the proposed method can overcome two main problems: catastrophic forgetting and overfitting. To achieve this, the authors propose a novel Adaptive model training policy ACCTS. Experiments on four real-world datasets show that the method can classify more accurately than all baselines."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes a kNN-augmented attention, which is an extension to the transformer architecture, which increases the length of the context that a language model can attend to by using k-nearest-neighbor lookup into a large external memory. The authors demonstrate the effectiveness of external memory in a series of language modeling experiments over a variety of long-document datasets, including LaTeX documents, source code, formal proofs, and books. They show that the model is capable of making use of newly defined functions and theorems during test time."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes to interpret masked language modeling (MLM) as energy-based sequence models and propose two energy parametrizations derived from the trained MLM. The authors develop a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm to draw samples correctly from these models, and propose to use the same masked conditionals used for training the masked language models as proposal distributions for transitioning to a new state in the Markov chain of MCMC sampler. The proposed method is evaluated on two tasks: open-ended unconditional generation and conditional machine translation."
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper proposes a method for learning data augmentation policies for NLP tasks. The main idea is to jointly optimize a data-augmentation policy while training the model, to construct the augmented samples with low confidence but a high semantic similarity with original ones. The authors propose a novel reward function for updating the augmentation policy to construct difficult but not too different samples (DND). Experiments on various text classification tasks and GLUE benchmark show that the proposed method is more effective on the challenging low-data and class-imbalanced regimes."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"This paper proposes a method for context-based meta-learning for offline reinforcement learning (OMRL). The proposed method improves upon FOCAL by incorporating intra-task attention mechanism and inter-task contrastive learning objectives, to robustify task representation learning against sparse reward and distribution shift. Theoretical analysis and experiments are presented to demonstrate the superior performance and robustness of the proposed method."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"This paper proposes a method to improve the performance of parametric sequential generative modeling methods for partially observable Markov decision processes (POMDPs). The authors propose a framework called belief fine-tuning (BFT) that leverages approximate dynamic programming to determine the model parameters at each time step. BFT can improve the accuracy of the belief model at test time because it specializes the capacity of the model to the space of local observations. The authors also show that BFT enables, for the first time, approximate public belief state search in imperfect-information games."
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper proposes Pixelated Butterfly, a method for training sparse neural networks. The authors propose to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. The proposed method uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., MLP, MLP-Mixer, Vision Transformer, and GPT-2 medium) with no drop in accuracy. The experiments on ImageNet classification and WikiText-103 language modeling tasks show that the proposed method can train up to 2.5x faster than the dense models."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper proposes a conditional diffusion probabilistic model that explicitly models the class center in the forward and reverse process, which enables controllable generation and gets interpretability. The authors also provide another direction for faster sampling and more analysis of their method. Extensive experiments verify the effectiveness and scalability of the proposed method."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper proposes a method for domain generalization (DG) that explores diverse latent sub-spaces and learns individual hypotheses for each sub-dimensional sub-space. In particular, the authors propose to learn label-informative latent representations from source latent representations and label-invariant representations from the target representations. The proposed method is evaluated on several well-known DG benchmarks and achieves competitive performance."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper improves upon the kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth squareroot kernel. Specifically, the authors show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the Hilbert space. They also show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square root kernel. Finally, they prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn, that do not have square-roots."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents an open-source benchmark suite for the max-min independence set (MIS) problem. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. The authors also conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. (2018), testing various configurations on small and large synthetic and real-world graphs. They show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Thus, the results from the original publication are not reproducible."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper proposes Wavelet Compressed Convolution (WCC) for activation maps compression for 1x1 convolution. WCC uses a hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and defines the convolution on the compressed activation map. By combining WCC with light quantization, WCC achieves compression rates equal to 2-bit and 1-bit with minimal degradation in image-to-image tasks."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper studies learning dynamics for extensive-form correlated equilibrium (EFCE) in multiplayer general-sum imperfect-information extensive form games. Theoretical results show that when all agents play T repetitions of the game according to the accelerated dynamics, the correlated distribution of play is an O(T 3/4)-approximate EFCE. This significantly improves over the best prior rate of $O(T 1/2)$. The authors connect predictive regret minimization with the framework of -regret. "
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes a method to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. By discretizing the action space, they can apply any discrete action deep RL algorithm to the continuous control problem. They evaluate the proposed method on three different setups: RL with demonstrations, RL with play data, and Imitation Learning. They show that AQuaDem consistently outperforms state-of-the-art continuous control methods."
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper studies the problem of domain generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic (source) data. The authors propose a novel adversarial style augmentation (AdvStyle) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training. Experiments show that AdvStyle can significantly improve the model performance on unseen real domains and show that we can achieve the state of the art."
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper proposes an event-based guided variational autoencoder (Guided-VAE) for mid-air gesture recognition. The proposed approach is based on the Hybrid Guided-Guided VAE, which is able to learn to represent sparse, high-dimensional visual data captured at the sensor in a small number of latent dimensions. The encoder is jointly trained by two classifiers that the latent space disentangles and accurately represents target features. The authors also implement the encoder component of the model on neuromorphic hardware and discuss the potential for their algorithm to enable real-time, self-supervised learning of natural midair gestures."
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper proposes a method for sparse deep learning for tabular data. The authors propose to use ferns (oblivious decision trees) instead of neural networks (NNs) as the basic computation unit, with a single active word for each fern. This allows the network to be trained end-to-end with back-propagation using SGD. The proposed method is evaluated on CIFAR-10 and ImageNet classification and regression datasets."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper tackles the problem of learning value functions from undirected state-only experience (i.e. state transitions without action labels). The authors first theoretically characterize the applicability of Q-learning in this setting. They show that tabular Q learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-Learning (LAQ), an offline RL method that can learn effective value functions. LAQ learns value functions using Q- learning on discrete latent actions obtained through a latent-variable future prediction model. The authors show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. The learned value function is then used to derive value functions to derive the reward. The proposed method is evaluated in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper proposes SWARM, a model-parallel training algorithm designed for swarms of poorly connected, heterogeneous unreliable devices. SWARM creates temporary randomized pipelines between available nodes that are rebalanced in case of failure. To further reduce the network usage of our approach, the authors develop several compression-aware architecture modifications and evaluate their tradeoffs."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a method to correct the transition dynamics of offline MARL data in order to improve the performance of online MARL training. The transition dynamics in offline data are different from the transitions in online data, which causes the extrapolation error, i.e., the error in value estimate. The authors propose two types of distances to measure the similarity between transitions and propose an adaptive rank-based prioritization to sample transitions according to the transition similarity. Experimental results show that the proposed method outperforms baselines in a variety of tasks."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a logarithmic unbiased quantization (LUQ) method to quantize both the forward and backward phase to 4-bits, achieving state-of-the-art results in 4-bit training. The authors analyze the difference between two rounding schemes: round-to-nearest and stochastic rounding. The former has lower MSE and works better for the quantization of the forward phase (weights and activations), while the latter is an unbiased approximation of the original data and works well for the backward phase (specifically, the neural gradients)."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper proposes a self-attention feature-selection mechanism that adaptively dilutes non-discriminative features. The authors demonstrate the effectiveness of their approach in meta-learning Boolean functions, and synthetic and real-world few-shot learning tasks. They show that in the presence of task-irrelevant features, inherent to meta- learning problems, attentional models are susceptible to misclassification. To address this challenge, they propose a selfattention features selection mechanism."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes an environment and training methodology to study emergent communication between agents using a continuous communication channel trained through reinforcement learning. The authors use a simple messaging environment where a “speaker” agent needs to convey a concept to a listener. The speaker is equipped with a vocoder that maps symbols to a continuous waveform, this is passed over a lossy continuous channel, and the listener needs to map the continuous signal to the concept. They show that basic compositionality emerges in the learned language representations. They also show that noise is essential in the communication channel when conveying unseen concept combinations."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"This paper studies the problem of backdoor attacks against pre-trained NLP models. The authors propose BadPre, a task-agnostic backdoor attack that can be applied to a variety of downstream NLP tasks, including text classification, question answering, and text generation. They design a two-stage attack strategy: first, they propose a trigger word insertion strategy, and second, they design a trigger insertion strategy to evade backdoor detection. Experimental results show that BadPre can successfully attack a wide range of downstream language tasks."
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper proposes a method for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn’t forget a learned skill. Experiments on HalfCheetah and Mujoco environments show that incremental skills significantly outperform current state-of-the-art skill discovery methods."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a new convolutional layer, called log-polar space convolution (LPSC), where the convolution kernel is elliptical and adaptively divides its local receptive field into different regions according to the relative directions and logarithmic distances. The proposed LPSC not only naturally encodes local spatial structures, but also greatly increases the single-layer receptive field while maintaining the number of parameters. Experiments on different tasks and datasets demonstrate the effectiveness of the proposed method."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper proposes a new information measure, i.e., information stored in weights (IIW), to study the generalization properties of neural networks (NNs). The IIW measures the mutual information between the weights of the network and the labels of the input. The authors propose an algorithm for the efficient approximation of IIW, and then propose an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, they can empirically identify the fitting to compressing phase transition during NNs’ training and the concrete connection between the IIW compression and the generalisation. Besides, they verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization, and noisy labels."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,This paper provides an explanation of the working principle of indiscriminate data poisoning attacks. The authors show that the perturbations of advanced attacks are almost linear separable when assigned with the target labels of the corresponding samples. They further confirm that linear separability is indeed the workhorse for recent attacks. This finding suggests that the shortcut learning problem is more serious than previously believed. It also suggests that pre-trained feature extractors can be a powerful defense.
SP:7b50be406138ad01db3ee112899f622637896fe9,"This paper proposes a new algorithm for offline policy optimization based on importance sampling. Importance sampling and its variants are a widely used type of estimator in offline policy evaluation, which can remove assumptions on the chosen function approximations used to represent value functions and process models. In this paper, the authors identify an important overfitting phenomenon in optimizing the importance weighted return, and propose an algorithm to avoid this overfitting. They provide a theoretical justification of the proposed algorithm through a better per-state-neighborhood normalization condition and show the limitation of previous attempts to this approach through an illustrative example. They further test the proposed method with less overfitting and better test performance compared with state-of-the-art batch reinforcement learning algorithms."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents CoLLIE, a method for continual learning of how language is grounded in vision. Given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case CLIP by OpenAI), it learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. The authors verify the model’s performance on two different tasks of continual learning and show that it can efficiently learn and generalize from only a few examples, with little interference with the original zero-shot performance."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper proposes a visual-linguistic learning framework (VLAF2) for novel object captioning (NOC), which leverages the intrinsic knowledge of BERT and CLIP for describing visual information of images with novel objects. VLAF2 leverages language knowledge from popular models to reward captions with precise and rich visual content associated with novel images. The authors propose objectives and rewards reflecting the desirable linguistic fluency and visual semantics for NOC. Guided by BERT, our model learns to refine wordings of novel object captions. Via reinforce algorithms, we have CLIP-based rewards assess the correctness of visual content described in the generated caption. Empirically, we showed that our model achieved SOTA results on the nocaps benchmark."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies the problem of transfer learning in the few-shot setting. The authors propose a new perspective on this problem by connecting it to the newly discovered phenomenon of neural collapse. They show that the within-class variance collapse tends to emerge in the test data associated with the classes encountered at train time and, more importantly, in new unseen classes when the new classes are drawn from the same distribution as the training classes. They also show that when neural collapse emerges in new classes, then it requires very few samples to train a linear classifier on top of the learned feature representation that accurately predicts new classes."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper proposes a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into 3D points. This paper further improves the performance of transformer by a newly proposed module called amplified positional encoding. Extensive experiments demonstrate that the network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"This paper proposes a new method, PipeGCN, for efficient full-graph GCN training. The main idea is to use pipelining inter-partition communication to hide the substantial communication overhead. The authors also propose a smoothing method to further improve the convergence of the proposed method. Extensive experiments show that the proposed approach can largely boost training throughput (up to 2.2x) while achieving the same accuracy as its vanilla counterpart and outperforming existing full-Graph training methods."
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper studies the problem of test time robustification, i.e., using the test input to improve the model robustness. The authors propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentation, while also maintaining confidence in its predictions. The experiments show that this approach achieves accuracy gains of 1-8% over standard model evaluation and also outperforms prior augmentation and adaptation strategies."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper proposes a model-based reinforcement learning algorithm where the model and the policy are jointly optimized with respect to the same objective, i.e., a global lower bound on the expected return. The proposed algorithm is conceptually similar to a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policies are updated to avoid states where the predictions are unrealistic. However, unlike prior work, this paper proposes an objective that is a lower-bound on the standard expected return objective, which guarantees that updating the model (using our objective) will result in a better policy."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a combination of behavioral cloning (BC) and observation history (OH) methods to improve the performance of BC-SO and BC-OH in the partially observed settings. The authors propose a coarse-to-fine approach inspired by human decision making: they first compute a coarse action based on the instantaneous observation, and then refine it into a final action using historical information. The experiments show that this approach outperforms all baselines on CARLA autonomous driving and various MuJoCo continuous control tasks."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a model-based meta-learning method, DyAd, for learning to learn dynamical systems with deep learning. The main idea is to use an encoder to infer the parameters of the task and a prediction network to adapt and forecast given the inferred task. The encoder maps different time-invariant hidden features representing constants of motion, boundary conditions, and the system state, and predicts the future system state. The prediction network takes the hidden representations and the past system states to predict the future state. Experimental results on turbulent flow prediction and real-world ocean temperature and currents forecasting tasks demonstrate the superior performance of the proposed method."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method first generates 2D boxes on the image and then selects the corresponding RoI LiDAR points as the weak supervision. A network is trained to predict 3D boxes which can tightly align with associated RoI points. This network is learned by minimizing the newly-proposed 3D alignment loss between the 3D box estimates and the corresponding object-LiDAR point.
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper proposes a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The authors also introduce a deep Transformer model that integrates GBST and operates on the byte level. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed model outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper proposes a method for detecting backdoors in black-box deep neural networks (DNNs), where only the final output label of the model is available. The proposed method, AEVA, is based on an adversarial extreme value analysis on the adversarial map computed from the monte-carlo gradient estimation. Theoretical results show that the objective of the proposed method is bounded by an objective with skewed distribution, which the authors call adversarial singularity phenomenon. Experiments are conducted on CIFAR-10, ImageNet, and ImageNet++ datasets to demonstrate the effectiveness of AEVA."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new class-wise uncertainty measure, Kullback-Leibler Divergence Criterion (KLoS), which is based on the second-order uncertainty representation provided by evidential models. KLoS captures class confusion and lack of evidence in a single score, which is built from in-distribution samples and does not require OOD training data, in contrast to current second order uncertainty measures. An auxiliary neural network is also designed to learn a refined criterion aligned with the evidential training objective. Experiments are conducted on CIFAR-10 and ImageNet datasets to demonstrate the effectiveness of the proposed method."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper studies the problem of learning convolutional neural networks (CNNs) under some natural distributional assumptions. Specifically, the authors assume that the distribution of patches in the input images has low-dimensional structure (e.g., when the patches are sampled from a low dimensional manifold) and that the important patches of the input have a moderate covering number (i.e., they are well-clustered). Under this assumption, they prove that the two-step semi-supervised learning algorithm PAC learns the distribution. They also show that the dependence of the algorithm on the dimension of the patch distribution is essentially optimal."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,This paper proposes a new approach for face clustering based on graph convolutional networks (GCNs). The proposed approach first transforms the features of the face images to a structure space and then uses an adaptive neighbour discovery strategy to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods.
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper studies the problem of generalizable person re-identification (DG ReID) in the setting where the model is trained on multiple large-scale datasets and tested on unseen domains directly without any data collection, annotation, and model updating. The authors propose a new setting, DGWD-ReID, that needs to learn domain-invariant representation without demographics. They further propose Unit DRO, a new method reformulated from KL constraint DRO. The proposed method is able to find semantically meaningful samples and subgroups without demographics and outperforms previous DG ReID methods."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"This paper proposes a noise regularization technique for GNNs to improve the training of GNN. The idea is to corrupt the input graph with noise, and add a noise correcting node-level loss. The authors claim that the proposed method reduces oversmoothing for shallower networks and improves performance for deeper networks. The method is evaluated on challenging 3D molecular property prediction tasks, and some generic GNN benchmark datasets."
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper proposes a differentiable EM model for set representation learning. The model is built from the perspective of fitting a Gaussian mixture model to the set data that are viewed as i.i.d. samples from a mixture distribution, which offers more flexibility and prior-induced model regularization in a principled Bayesian manner. The proposed model is also shown to generalize the recent set embedding models based on optimal transport and attention, leading to a computationally efficient model with superb performance on tasks in bioinformatics and NLP."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes an unsupervised feature selection method for contrastive analysis (CA), which aims to select a small number of informative features for use in unknown downstream tasks. The proposed method, CFS (Contrastive Feature Selection), is a method for performing feature selection in the CA setting. The authors experiment with multiple variations of their method on a semi-synthetic dataset and four real-world biomedical datasets, and they find that it consistently outperforms previous state-of-the-art methods designed for standard feature selection scenarios."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the effect of early stopping on the generalization properties of deep neural networks. The authors propose a new model of overparametrization that captures the phenomenon that the model size usually exceeds the number of features in practice. Moreover, they give an explicit characterization of the early stopping time and the resulting model. Theoretical analysis of the result is also provided. "
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper proposes a quasi-Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient (NPG) algorithm. For other entropy functions, this method results in new policy gradient algorithms. The authors provide a simple proof that all these algorithms enjoy the Newton-type quadratic convergence near the optimal policy. The proposed method is shown to converge in single-digit iterations."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper proposes a general method inspired by case-based reasoning to train agents and generalize out of the training distribution in text-based games. The proposed method collects instances of positive experiences from the agent’s interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on-policy neural agent in the literature for TBGs. The experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state of the art results on widely used environments."
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. The authors demonstrate an effective approach to training the sense disambiguation mechanism in their model with a distribution over word senses extracted from the output layer embedding of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi- sense embedding models on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this method for downstream applications."
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper proposes to transfer a 2D convolutional neural network trained on an image dataset to a 3D point-cloud dataset. The main idea is to apply the same network architecture and weights as in the 2D dataset to the 3D dataset, and then transfer the weights from 2D to 3D by inflating the convolution filters and finetuning the weights. The authors evaluate the performance of the proposed method on few-shot classification, indoor and outdoor scene segmentation, and image-to-image transfer, and show that the method is able to achieve comparable performance to the state-of-the-art methods."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper proposes an energy-based learning objective for training autoregressive generative models (ARGMs). The proposed approach is based on the idea of importance sampling, which is an extension of the idea proposed by Bengio et al. (2015) and Mihaylova & Martins (2019). The authors propose to use a constraint to fit the joint distributions at each time step. The proposed method is evaluated on language modeling, neural machine translation, and image generation tasks."
SP:51e748c55bd4134047098559577fa3f37aa7433a,"This paper proposes a unified distributional robustness framework for adversarial training, which unifies and generalizes standard AT approaches with improved adversarial robustness. The authors introduce a new Wasserstein cost function and a new series of risk functions, with which they show that standard AT methods are special cases of their counterparts in the framework. Extensive experiments show that the proposed algorithms are able to boost the model robustness against strong attacks with better generalization capacity."
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a method for unsupervised representation learning for multivariate time series. The authors claim that the existing representation learning methods for time series mostly use segment-level augmentation, which may bring about sampling bias and incorrect optimization with false negatives due to the loss of global context. To address these problems, the authors propose a novel framework, namely Bilinear Temporal-Spectral Fusion (BTSF), which applies dropout on the entire time series for better preserving global context and capturing long-term dependencies. An iterative bilinear temporal-spectral fusion module is devised to explicitly encode the affinities of abundant time-frequency pairs and iteratively refine representations of time series through cross-domain interactions with Spectrum-to-Time (S2T) and Timeto-Spectrum (T2S) Aggregation modules. Experiments on classification, forecasting and anomaly detection downstream tasks demonstrate the superior performance of the proposed method."
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent. The learning rate is optimized via a simple extra gradient descent step. The authors formulate first and second-order gradients with respect to learning rate as functions of consecutive weight gradients, leading to a cost-effective implementation. Extensive experimental evaluation is conducted, validating the effectiveness of the proposed method for a plethora of different settings."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta-learning algorithm for the continual multi-task learning setting, where tasks are experienced one at a time, without the option to collect additional data on previous tasks. The authors propose a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learnt to prepare for subsequent task learning. The proposed method is evaluated on several sequences of challenging continuous control tasks."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a new threat model for poisoned classifiers, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, they propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. They construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images with human interaction. They demonstrate the effectiveness of their attack through extensive experiments on high-resolution datasets."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper proposes a new method for distilling unconditional GANs, especially StyleGAN2. The authors claim that the main challenge of unsupervised GAN distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. Standard knowledge distillation losses typically fail under this heterogeneous distillation scenario, and the authors identify that the style module plays a vital role in determining semantic information of generated images. Based on this finding, the authors propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent. Moreover, a latent-direction-based distillation loss is proposed to preserve the semantic relations in latent space. Extensive experiments demonstrate the effectiveness of the proposed method."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a method for approximating offline algorithms in an online setting by encoding the behavior of an offline algorithm in graphs and training a multi-task learning model to simultaneously detect behavioral structures which have already occurred and predict those that may come next. The method is evaluated on both synthetic data and historical stock market data, where the contrast between explanation and prediction is particularly stark. The paper is well-written and easy to follow."
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper proposes a method to reduce the number of inducing points of Gaussian Processes (GPs) by amortizing the computation of the inducing points locations, as well as the parameters of the variational posterior approximation. The inducing points are learned by considering them as parameters of an approximate posterior distribution q. The paper proposes to use a deep neural network (DNN) to output inducing points for each point at which the predictive distribution of the GP needs to be computed. The DNN also outputs the parameters for the corresponding variational approximation on the inducing values associated to inducing points. "
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,This paper proposes a protocol for secure (Byzantinetolerant) decentralized training that emphasizes communication efficiency. The authors provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. They also conduct large-scale experiments on image classification and language modeling in presence of Byzantine attackers.
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper proposes to learn the parameters of the Smoothed particle hydrodynamics (SPH) method, a mesh-free Lagrangian method for obtaining approximate numerical solutions of the equations of fluid dynamics, which has been widely applied to weakly-and strongly compressible turbulence in astrophysics and engineering applications. The authors present a learn-able hierarchy of parameterized and physics-explainable SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators. The learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. They show that their physics informed learning method is capable of: (a) solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; (b) learning Lagrangians statistics of turbulence; (c) combining Lagrange trajectory based, probabilistic, and Eulerian field based loss functions; and (d) extrapolating beyond training sets into more complex regimes of interest."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"This paper proposes an approach to regularize a single deterministic neural network to obtain improved accuracy and reliable uncertainty estimates. The proposed approach, Mix-MaxEnt, simply puts an entropy maximization regularizer corresponding to the predictive distribution in the regions of the embedding space between the class clusters. This is achieved by synthetically generating between-cluster samples via the convex combination of two images from different classes and maximizing the entropy on these samples. A data-dependent regularization guides the maximum likelihood estimation to prefer a solution that maps out-of-distribution samples to high entropy regions (creating an entropy barrier); and (2) is more robust to the superficial input perturbations."
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a method for self-supervised auto-encoder-based motion transfer from videos to still images. The proposed method, Latent Image Animator (LIA), learns a set of orthogonal motion directions and uses their linear combination to represent any displacement in the latent space. The method is evaluated on three datasets (VoxCeleb, Taichi and TED-talk) and shows superior performance compared to existing methods."
SP:86f9f89f84e117c86478b9afaf087f65524f5472,"Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, this paper proposes to augment the task set through task interpolation. The proposed approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Theoretical analysis shows that the proposed approach corresponds to a data-adaptive meta-regularization and improves the generalization."
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper proposes a new approach for fair representation learning. The proposed approach is based on normalizing flows, where the encoder is trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. Experiments are conducted to show the effectiveness of the proposed approach."
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a graph neural network (GNN) for subgraph isomorphism counting. The proposed method is based on two modules: edge-centric message passing and query-conditioned graph modulation. In the edge level, the proposed method propagates and aggregates messages on and for edges following the edge adjacency, and in the graph level, it modulates the input graph representation conditioned on the query, so that it can be adapted to each query individually to improve the matching with specific structures in each query. The experimental results show that the proposed COUNT-GNN achieves superior performance in comparison to the state-of-the-art."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper studies the problem of agnostic personalized federated learning (APFL), where each client can have their own personalized labels, which might not be compatible with others (even for the same class), and can be also possibly from a variety of multiple domains. The authors propose a novel method, namely Similarity Matching and Kernel Factorization (SimFed), which measures task-level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. Furthermore, the authors factorize the model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. The experimental results show that the proposed method outperforms the current state-of-the-art approaches."
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper proposes an object dynamics distillation network (ODDN) which distills explicit object dynamic representations (e.g., velocity) and relation module that calculates object-pair interactions and applies it to the corresponding dynamic representations of objects. The proposed method is evaluated on tasks of video events reasoning and video prediction. The results show that visual representations of ODDN perform better in answering reasoning questions around physical events in a video compared to representaions of the previous scene representation methods."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"This paper studies the problem of generalization and stability of GNNs with positional encoding (PE) in the context of link prediction. In particular, the authors propose a new GNN layer, called Permutation Equivariant Graph Neural Network (PEG), which is a combination of two existing GNN layers: random features and deterministic distance encoding (DE). Theoretical analysis is provided to show that the proposed PEG is equivariant w.r.t. the original node features and rotation equivariance. Experiments on real-world link prediction datasets demonstrate the effectiveness of the proposed method. "
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes LaMer, a self-supervised text style transfer method built on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer and a newly proposed challenging task (political stance transfer), the model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate the model not only makes training more efficient, but also generates more readable and diverse expressions than previous models."
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper proposes a query embedding method for hyper-relational knowledge graphs (KGs). In particular, it extends the multi-hop logical reasoning (ML) problem to KGs, where the query consists of a set of relations and the goal is to predict the answer to the query. The proposed method is based on Graph Neural Networks (GNN) and query embeddings. The authors also propose a method to answer queries and demonstrate in their experiments that the proposed method outperforms existing approaches on a diverse set of query patterns. "
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper proposes a Bayesian optimization method for hyperparameter optimization (HPO) in the gray-box setting. The authors propose a new surrogate for Gaussian Processes that embeds the learning curve dynamics and a new acquisition function that incorporates multi-budget information. The proposed method is evaluated on 50 datasets (Tabular, Image, NLP) and diverse neural networks (MLP, CNN/NAS, RNN)."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper proposes a post-training quantization and integer-arithmetic-only inference to improve the performance of learned image compression. The authors also improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. With the proposed methods, the current state-ofthe-art image compression models can infer in a cross-platform consistent manner, which makes the further development and practice of learned images compression more promising."
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes an unsupervised noise reconstruction and removal network for denoising scanning electron microscopy images. The proposed method is inspired by gated recurrent units, which reconstructs and removes the noise by synthesizing the sequential data. The network is trained using a triplet of images as input and is trained to map one noise map to the other noise map. Experiments on the FIB-SEM dataset show that the proposed method achieves comparable/better results than supervised approaches."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"This paper studies the label trick in graph neural networks (GNNs) and label propagation algorithms (LPA). In particular, the authors propose to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. The authors prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts graph size and connectivity."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"This paper proposes a framework to analyze machine theory of mind in a symmetric multi-agent symmetric setting, which is a more realistic setup than the tasks currently used in the community. The authors provide a simplified setup on which to test the problem, and they show that even with this minimal set of rules, SymmToM proves algorithmically difficult for current multiagent deep reinforcement learning models, even when tailoring them to our specific task. The main goal in this work was not to solve symmetric theory ofmind, but rather to give a starting point to explore more complex models in this area."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper proposes a method for zero-shot object detection, which is a subset of unsupervised learning that aims to detect novel objects in the image with the knowledge learned from and only from seen objects. The authors propose to use a modified YOLOv5 neural network to perform generalized zeroshot detection on seen and unseen objects. They also propose a novel splitting method for YCB Video dataset to train and test gZSD algorithms."
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper proposes an autoregressive latent video prediction method for high-fidelity video prediction. The proposed method is built on top of the prior work of (Rakhimov et al., 2021) and (Chen et al. 2020) that pre-train an image generator and then use it to predict future frames. This paper proposes to use a VQ-GAN to predict the discrete latent codes of the latent space of the image generator, and use top-k sampling and data augmentation to further improve the video prediction quality. The experimental results show that the proposed method achieves competitive performance on standard video prediction benchmarks."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"This paper proposes to use vision transformers (ViTs) to improve the performance of GANs. Specifically, the authors propose to use ViTs for the discriminators and generators, and propose several regularization techniques for training GAN with ViTs. Experiments on CIFAR-10, CelebA, and LSUN bedroom show that the proposed method can achieve comparable performance to CNNs. "
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper proposes a two-stage training procedure for variational autoencoders (VAEs) that prioritizes the modeling of perceptible information to achieve good sample quality, and then models the bulk of the likelihood signal to achieve the good likelihoods. The authors claim that the main cause of VAEs being overwhelmed by the vast volume of visually-imperceptible information is that the ELBO does not distinguish the bits that matter perceptually from the ones that do not, and thus the model does not dedicate enough modeling capacity to the perceptually relevant information, thus causing poor sample quality. To address this issue, the authors propose to train a secondary high-rate model that trains on top of the low-rate part of the primary model, and a secondary model that models the majority of the visual information. Experiments show that the proposed approach can achieve better ELBOs and sample quality than conventional VAEs."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"This paper studies the analytic forms of the optimal reverse variance and the optimal KL divergence of diffusion probabilistic models (DPMs). Theoretical results show that both of them have analytic forms w.r.t. its score function (i.e., the gradient of a log density). Based on this result, the authors propose a training-free inference framework that estimates the analytic form of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, they derive both lower and upper bounds on the optimal variance and clip the estimate for a better result. Empirically, their analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and enjoys a 20x to 80x speed up."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper investigates whether it is feasible to switch to transformer-based models for medical image classification as well, or if we should keep working with CNNs – can we trivially replace CNNs with transformers? The authors consider this question in a series of experiments on several standard medical image benchmark datasets and tasks. Their findings show that, while CNNs perform better if trained from scratch, off-the-shelf vision transformers can perform on par with CNNS when pretrained on ImageNet, both in a supervised and self-supervised setting."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper analyzes the expressivity of neural language models (NLMs) with respect to the number of times two sentences are seen together in the same training example. The authors show that NLMs can model stronger dependencies between sentences that were seen together at least once in-context, i.e., in the training example, than between sentences seen at least twice in different training examples. This result is motivated by the fact that pretraining NLMs over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. The paper also shows that the pretrained NLM can model much stronger dependency between text segments that appeared in same training examples than it can between text segment that appear in different examples. "
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"This paper proposes a symbolic representation and analysis framework for learning to optimize (L2O), which aims to improve the scalability and interpretability of L2O models. The proposed method first learns a neural network to parameterize the optimization rules, and then distill the learned optimization rules into a symbolic form that can be used for interpretability and scalability. The authors also propose a lightweight model that is able to be meta-trained on large-scale problems and outperforms human-designed and tuned optimizers."
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper proposes a method to make RL models provably robust under adversarial perturbations. The proposed method is based on randomized smoothing where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Theoretical analysis shows that by adding Gaussian smoothing noise to the input of the policy, one can certifiably defend it against norm-bounded adversarial attacks of its input. Experiments on Cartpole, Pong, Freeway and Mountain Car show that the method can yield meaningful robustness guarantees."
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper studies the problem of classification accuracy estimation in the presence of distribution shift. The authors propose an approach called Average Thresholded Confidence (ATC) that learns a threshold on the model’s confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). The authors also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift."
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper proposes a method for partial distribution matching (PDM) problems, where point sets are regarded as discrete distributions and the goal is to partially match them. The authors formulate the registration problem as a partial Wasserstein-1 (PW-1) discrepancy, which they show can be efficiently optimized, and derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these theoretical results, the authors propose a PWAN method, which approximates the discrepancy by a neural network, and learns the transformation adversarially with the network. It also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"This paper proposes a transfer learning approach for hyperparameter optimization (HPO) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. The authors propose a novel Deep Kernel Gaussian Process (GP) surrogate with Landmark Meta-features (DKLM) that is enriched with end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, the proposed approach can learn contextualized dataset-specific similarity representations for hyper-parameter configurations. The proposed approach is evaluated on a wide range of HPO meta-datasets from OpenML and shows the empirical superiority of the proposed method."
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,This paper proposes a method to enable responsible disclosure of generative models by a novel fingerprinting mechanism. It allows scalable ad-hoc generation of a large population of models with distinct fingerprints. The core idea is to incorporate a fingerprint-encoder into a GAN framework while preserving the original generation performance. Experiments show that the method fulfills several key properties.
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,This paper proposes two methods for providing model-agnostic local explanations for similarity learners. The first method provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. The second method proposes analogies as a new form of explanation in machine learning. The goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model’s prediction.
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,This paper studies the certified robustness of ensembles of deep neural networks (DNNs) against adversarial perturbations. The authors provide sufficient and necessary conditions of robustness for ensemble ML models and propose a diversity-regularized training (DRT) method to improve the certified L2-robustness of ensemble models. Theoretical results show that diversified gradient and large confidence margin are necessary and sufficient conditions for certifiably robust ensemble models under the model-smoothness assumption. Extensive experiments show that DRT-enhanced ensemble models can achieve the highest certified L_2 robustness compared with existing baselines.
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper studies the expressive power of recursively pooling-based graph neural networks (GNNs). In particular, it shows that the proposed model can count subgraphs of size $k$ and overcomes a known limitation of low-order GNNs. It also shows how the proposed method can exploit sparsity to reduce the computational complexity compared to existing higher-order models. Finally, it provides a (near) matching information-theoretic lower bound for counting subgraph with graph representations that pool over representations of derived (sub-)graphs."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper proposes a simple probe model called Graph Convolutional Simulator (GCS) for interpreting knowledge-enhanced LMs and exposing what kind of knowledge is integrated into these models. The authors conduct experiments to verify that our GCS model can indeed be used to correctly interpret the KI process, and they use it to analyze two typical knowledge-augmented LMs: K-Adapter and ERNIE. They find that only a small amount of factual knowledge is captured in these models during integration. They also find that while K- adapter struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the effect of the adaptation learning rate in meta-learning with mixed linear regression. The authors present a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. Second, they interpret the underlying dependence between the optimal learning rate and the input data. Finally, they prove that compared with empirical risk minimization (ERM), MAMl produces an initialization with a smaller average distance to the task optima."
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a method for source-free domain adaptation (SFDA) which aims to adapt a model trained on labelled data in a source domain to unlabelled data in the target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation. The authors address these issues for a particularly pervasive type of domain shift called measurement shift, characterized by a change in measurement system, which can be resolved by restoring the source features. In the source domain, they store a lightweight and flexible approximation of the feature distribution under the source data. Then, they adapt the feature-extractor that the approximate feature distributions under the target data realigns with that saved on the original data. They call this method Feature Restoration (FR) and propose a bottom-up training scheme for FR which boosts performance by preserving learnt structure in the later layers of a network. They demonstrate that the proposed method outperforms existing SFDA methods on real and synthetic data in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source models in target domain."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper studies federated learning (FL) with adversarial robustness, i.e., adversarial training (AT), and proposes a method to propagate the robustness from high resource users that can afford AT to low resource users. The authors propose a simple yet effective propagation approach that transfers robustness through carefully designed batch-normalization statistics. They demonstrate the rationality and effectiveness of their method through extensive experiments. "
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper studies the problem of inferring the structure of the interaction network from observed equilibrium actions of a network game. The authors propose a novel framework to infer the network structure behind the games from their equilibrium actions. Unlike existing methods, the authors achieve so by learning a mapping from the actions to the structure without knowing the utility function of the game. This is especially beneficial in real-world scenarios where the nature of strategic interactions between players remains hidden or may evolve over time."
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes a novel relation prediction framework, GraphANGEL, that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraph with identical graph patterns. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. The model consistently outperforms existing models in terms of heterogeneous graph based recommendation and knowledge graph completion. The authors also demonstrate the capability of our model in generalizing to new relation types while producing explainable heat maps of attention scores across the discovered logics."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper studies the few-shot learning problem for histology images. The authors propose to combine contrastive learning (CL) with latent augmentation (LA) to build a few shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, the authors find that models learned by CL generalize better than supervised learning and LA brings consistent gains over baselines."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"This paper proposes a method for learning long-term dependencies of irregularly-sampled time-series using recurrent neural networks (RNNs) with continuous-time hidden states. The authors prove that similar to standard RNNs, the underlying reason for this issue is the vanishing or exploding of the gradient during training. This phenomenon is expressed by the ordinary differential equation (ODE) representation of the hidden state, regardless of the ODE solver’s choice. They provide a solution by equipping arbitrary continuous time networks with a memory compartment separated from its time-continuous state. This way, they encode a continuous time dynamical flow within the RNN, allowing it to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path."
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper proposes a method for quantizing BERT with 1-bit weights, embedding, and activation. The main idea is to use a bi-attention structure and a direction-matching distillation (DMD) scheme to optimize the full binarized BERT accurately. The proposed method outperforms existing quantization methods with ultra-low bit activation in terms of FLOPs and model size. "
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper proposes a transformer-based approach for multi-person detection and instance segmentation. Specifically, the self-attention in transformer measures dependencies between any pair of locations, which can provide association information for keypoints grouping. However, the naive attention patterns are still not subjectively controlled, so there is no guarantee that the keypoints will always attend to the instances to which they belong. To address this issue, the authors propose a novel approach of supervising self attention for instance-aware keypoint detection and segmentation, by using instance masks to supervise self attention and assign the detected keypoints to their instances based on the pairwise attention scores, without using pre-defined offset vector fields."
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper proposes a new method for minimizing the mean-variance (MV) trade-off in reinforcement learning (RL). The proposed method is based on maximizing the expected expected quadratic utility (EQUM) function, which corresponds to the Pareto efficient policy. The method is computationally friendly, as it does not include gradient estimation of the variance term. The paper also includes various interpretations, such as targeting optimization and regularization, which expands the scope of applications of the method. "
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper studies the problem of test-time domain adaptation for an autoencoder system whose channel is generatively-modeled using a mixture density network (MDN). Different from the setting of conventional training-time (unsupervised or semi-supervised) domain adaptation, here we have a fully-trained channel model and auto-encoder from a source domain, that we would like to adapt to a target domain using only a small labeled dataset (and no unlabeled data). Moreover, the distribution of the channel is expected to change frequently (e.g., a wireless link), making it challenging to collect sufficient data for frequent retraining of the auto-coder. To address this, this paper proposes a fast and sample-efficient method for adapting the autoencoders without modifying the encoder and decoder neural networks, and adapting only the Gaussian MDN channel model. The method utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present to the decoders samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt the MDN model using very limited number of samples, and improve or maintain the error rate of the autencoder under changing channel conditions."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new approach for the abductive natural language inference (αNLI) task. In this paper, the authors propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” in this paper. Based on the observation that the hypotheses are generally semantically related, they have designed a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. They name this new model for αNLI: Interactive Model with Structural Loss (IMSL). The experimental results show that our IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper proposes a method that combines a certifiable OOD detector with a standard classifier from first principles into an OOD aware classifier. It achieves the best of two worlds: certifiably adversarially robust OOD detection, even for OOD samples close to the in-distribution, without loss in either prediction accuracy or detection performance for non-manipulated OOD data. Moreover, due to the particular construction, the proposed method provably avoids the asymptotic overconfidence problem of standard neural networks."
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper proposes a transferable adversarial attack method called Image Classification Eraser (ICE) to erase classification information for any encountered images from arbitrary dataset. The authors define a new Generalized Transferable Attack (GTA) problem where the attacker has a set of surrogate models trained on different datasets (with different label sets and image sizes), and none of them is equal to the dataset used by the victim model. Extensive experiments on Cifar-10/100/TiedredImageNet demonstrate the effectiveness of the proposed ICE on the GTA problem."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper proposes a method to improve the robustness and effectiveness of masked language modeling (MLM) pretraining for large-scale pre-trained language models (PrLMs). The authors claim that the current PrLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on wrong data and leads to less efficiency and less robustness in the resulting PrLM. To counteract the intrinsic and critical issue, the authors employ extra pre-training objectives to correct or prune the harmful gradient update after detecting the false-negative predictions. Experimental results on GLUE and SQuAD benchmarks show that the proposed methods indeed bring about better performance together with stronger robustness."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes a novel open-world semi-supervised learning setting in which novel classes may appear in the unlabeled test data. The authors propose ORCA, an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes in the proposed setting."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes SLIM-QN, a light-stochastic quasi-Newton optimizer for training large-scale deep neural networks (DNNs). It addresses two key barriers in existing second-order methods: 1) the high computational cost of obtaining the Hessian matrix and its inverse in every iteration; 2) convergence instability due to stochastic training (e.g. L-BFGS). To tackle the first challenge, it uses the BFGS update rule that directly approximates the inverse using past parameters and gradients, without explicitly constructing the Hessians matrix and then computing its inverse. To achieve stable convergence, it introduces momentum in Hessian updates together with an adaptive damping mechanism. "
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes a method called Locality-Sensitive hashing (LSP) for graph pruning based on locality-sensitive hashing. LSP aims to sparsify a graph so that similar local environments of the original graph result in similar environments in the resulting sparsified graph, which is an essential feature for graph-related tasks. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of LSP."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper proposes IDAA, a data augmentation method for self-supervised learning. The main idea is to modify the training data to be hard positives/negatives without distorting the key information about their original identities. The authors decompose a sample x to be its variational auto-encoder (VAE) reconstruction plus the residual residual R(x) and then adversarially perturb the VAE’s bottleneck space and adds it back to the original original label space as an augmentation, which is sufficiently challenging for contrastive learning and meanwhile preserves the sample identity intact. The proposed method is applied to several SSL methods and shows improved efficiency and generalization performance. "
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper studies the problem of detecting distribution shifts in the data distribution of machine learning models. In particular, the authors propose a method that can detect harmful shifts while ignoring benign ones, and allow continuous monitoring of the model performance without increasing the false alarm rate. The proposed framework is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. The authors demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,This paper proposes a method for learning a physical model of a physical phenomenon (e.g. pendulum motion) from a single short video clip. The authors propose to use neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) to obtain interpretable physical models directly from visual observations. The proposed method is able to identify physical parameters from only a single video and can be used to process high-resolution videos and the synthesis of photo-realistic imagery. The paper is well-written and easy to follow.
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper considers the context-dependent reinforcement learning (C-MDP) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. The authors propose a variational inference algorithm for model learning using a sticky Hierarchical Dirichlet Process (HDP) prior, and derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. They then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, they demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that their approach succeeds where state-of the art methods fail and elaborate on the reasons for failures."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"This paper proposes a framework to pretrain knowledge-based multilingual language models (KMLMs) under a multilingual setting, which is rarely explored in prior work. Specifically, the authors generate two types of training data – the knowledge data and the reasoning data. The former is obtained by generating code-switched synthetic sentences and reasoning data using the Wikidata knowledge graphs. The latter is constructed by converting cycles from the knowledge graph into different languages. The authors evaluate the proposed framework on a series of knowledge-intensive cross-lingual benchmarks and demonstrate its effectiveness."
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"This paper proposes an approach to learn agents that can benefit other agents by maximizing the number of states that the other agent’s goal can reach in its future, without knowing the other agents’ goals. The authors propose a proxy objective that encourages the agent to maximize the total number of possible states that other agents can reach. They evaluate their approach on three different multi-agent environments, and show that their approach outperforms agents that are explicitly trained to work cooperatively."
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the double descent (DD) phenomenon in finite-width neural networks. Double descent refers to the phenomenon of a second descent in the test loss when the model is over-parameterized beyond a certain threshold (dubbed as the interpolation threshold). The authors derive a lower bound to the population risk and leverage results from Random Matrix Theory to show that it diverges at the threshold, thereby capturing double descent. They further investigate how the loss function affects double descent — and thus uncover interesting properties of neural networks and their Hessian spectra near the input-output threshold."
SP:b485114712055f39a7afb951dbc3db482ff523fd,"This paper studies the trainability of deep GNNs from a theoretical perspective. The authors analyze the asymptotic behavior of the graph neural tangent kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs and deep GCNs. They show that trainability drops at an exponential rate due to the aggregation operation of the GNTK, and extend the analysis to analyze residual connection-based techniques, which are found to be only able to mildly mitigate the exponential decay of trainability. Finally, the authors propose a connectivity-aware and graph-adaptive sampling method, inspired by the theoretical insights on trainability, which can achieve better results compared to relevant counterparts."
SP:25a92b3583afdc6892e59f1e769125d52c8011af,"This paper studies the problem of video-based cardiac measurement. The authors propose a method to estimate the left ventricle ejection time (LVET) using the second derivative of the PPG signal, which is more related to the rate-of-change of the blood volume change than the first derivative. They show that the model trained on the first-order dynamics (e.g., optical flow) is able to better estimate the second-order information. They also show that adding second-derivative inputs improves performance when estimating the second order dynamics."
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper proposes a method to help agents learn a compositional and symmetric language in complex settings like dialog games. The authors hypothesize that language may evolve from simple tasks to difficult tasks. They propose a novel architecture called symbolic mapping as a basic component of the communication system of agent. They find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks and explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex."
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper proposes a hierarchical modular approach to learn agents that navigate and manipulate objects in a divide-and-conquer manner. Specifically, the policy operates at three levels of hierarchy. It first infer a sequence of subgoals to be executed based on language instructions by high-level policy composition controller (PCC). Then discriminatively control the agent’s navigation by alternating between navigation policy and various independent interaction policies. Finally, it infer manipulation actions with the corresponding object masks using the appropriate interaction policy. The proposed method achieves the state-of-the-art performance on the challenging ALFRED benchmark."
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes Nuisance-randomized distillation (NURD), a method for training a classifier that is robust to spurious correlations in the nuisance-label relationship, i.e., the nuisance variable is correlated with the label variable. The nuisance variable, which is the nuisance, is the variable that predicts the type of the nuisance. The authors define a family of distributions where the nuisance and the label variables are independent. They then define a set of representations that are most informative of the label given an uncorrelating representation. They prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative and achieves the highest performance on every distribution in the family. "
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper proposes OTTER (Optimal TransporT distillation for Efficient zero-shot recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images."
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper proposes a simple and generic framework for object detection. It casts object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. The approach is based on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"This paper proposes a method to distill the knowledge of a policy network into a symbolic policy, which is composed from geometric and numerical symbols and operators. The symbolic policy can be treated as discrete and abstracted representations of the policy network, but are found to be more interpretable, robust and transferable. The proposed symbolic distillation approach is experimentally demonstrated to maintain the performance and “denoise” the CNN policy."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a StyleGAN2-style model for image-to-image translation. The authors propose to disentangle the coarse-level object arrangements (posture) and the fine-grained level styling (identity) of the generated image from two exemplar sources. To achieve this goal, the authors propose a Vector-Quantized Spatial Normalization (VQSN) module for the generator for better pose-identity disentanglement. The VQSN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training-set images. The paper also proposes a joint-training scheme with self-supervision methods for the GANInversion encoder and the generator. Specifically, the paper proposes to reconstruct images from two differently augmented variants of the original ones, one defining the pose and the other for identity."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a multi-layer perceptron (MLP) architecture for extracting information from speech signals. The model splits feature channels into non-overlapped chunks and processes each chunk individually. These chunks are then merged together and further processed to consolidate the output. By setting different numbers of chunks and focusing on different contextual window sizes, the proposed model learns multiscale local temporal dependency. The proposed model is successfully evaluated on two tasks: keyword spotting and speech enhancement."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,This paper provides a lower bound on the generalization error of any transfer learning algorithm (regardless of its computational complexity) as a function of the amount of source and target samples. The lower bound depends on a natural notion of distance that can be easily computed on real world data sets. It applies to any arbitrary source/target data distributions and requires minimal assumptions that enables it application to a broad range of problems. The authors also consider a more general setting where there are more than one source domains for knowledge transfer to the target task and develop new bounds on generalization errors in this setting.
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic shape completion method for large-scale 3D scenes. The proposed method is based on Generative Cellular Automata (GCA) that learns the multi-modal distribution and transform the formulation to process large scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. The training objective maximizes the variational lower bound of the complete shape distribution and therefore the progressive generation constitutes a valid generative model. Experiments show that the model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper proposes a non-Markovian generalization of behavioral priors for guiding exploration in reinforcement learning. The authors propose state-independent temporal priors, which exploit the idea of temporal consistency and are generally applicable and capable of transferring across a wide range of tasks. They show how dynamically sampling actions from a probabilistic mixture of policy and temporal prior can accelerate off-policy reinforcement learning in unseen downstream tasks."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a learning rate scheduling method for training deep neural networks. The proposed method is based on a graph-network-based learning rate scheduler (GNS), which uses a message-passing network to encode the current state of the neural network and uses reinforcement learning to control the learning rate. The authors claim that GNS is able to capture the intermediate layer information while being able to generalize to problems of varying scales. The method is evaluated on Fashion-MNIST and CIFAR10 for image classification and GLUE for language understanding."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper proposes a method for unsupervised object-centric generative modeling of scenes from 3D point clouds. The proposed method, SPAIR3D, is a VAE-based generative model that generates 3D spatial distributions on static point clouds to discover 3D objects in static scenes. The authors also propose a new Chamfer Mixture Loss function tailored for learning mixture models over point clouds with a novel graph graph neural network that can be used to model and generate a variable number of 3D points. Experiments on UOR and UOT datasets show that the proposed method can generalize well to previously unseen scenes with a large number of objects without performance degeneration."
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper proposes a method for grounding high-level tasks expressed in natural language to a chosen set of actionable steps (i.e. ""open fridge""). The authors propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Experiments on the VirtualHome environment show that the proposed method substantially improves executability over the LLM baseline."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a new generative model for VAEs based on the observation that VAEs naturally learn a Riemannian structure of the latent space. The authors then propose a sampling scheme consisting in sampling from a uniform distribution defined on the learned latent space and show that it improves the generation process of vanilla VAEs and makes it able to perform as well as more advanced VAE models in terms of Inception Distance (Heusel et al. 2017) and Precision and Recall (Sajjadi et al., 2019) scores on four benchmark datasets. They also show that the proposed method appears more robust to dataset size changes and outperforms more advanced models when only smaller sample sizes are considered. Finally, they validate the method on a complex neuroimaging dataset combining high dimensional data and low sample sizes."
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper proposes Transformer with a Mixture of Gaussian Keys (Transformer-MGK) that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer- MGK accelerates training and inference, has fewer parameters, and requires less FLOPs to compute while achieving comparable or better accuracy across tasks."
SP:82731dcce233e748f63382e09b6224a513fe9689,"This paper proposes a method for path integration in a two-dimensional continuous environment. The authors propose to fuse image and action related signals, allowing reconstruction of the action relating the two successive images, as well as prediction of the new image from its current value and the action. They also propose a minimalistic recurrent architecture, called Resetting Path Integrator (RPI), that can easily and reliably be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the (possibly noisy) self-motion signal, and occasionally resets it when the image signal is present. The internal state of this minimal model exhibits strong correlation with position in the environment due to the direct-inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions in the environments."
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the role of effective features in the training of neural networks. The authors consider the setting where the labels are determined by a set of class-relevant patterns and the inputs are generated from these along with some background patterns. They prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of the effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper provides lower bounds on the robustness of fixed feature extractors to adversarial perturbations. The lower bounds are based on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers. For linear features extractors, the paper provides closed-form expressions for collision finding and for arbitrary features, it proposes a bespoke algorithm that provably finds collisions. The bounds are then used to identify the layers of robustly trained models that contribute the most to a lack of robustness."
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper proposes Expectile V-Learning (EVL), which learns the V-function instead of the Q-function to naturally keep the learning procedure within the offline dataset. EVL avoids actions outside the dataset and provides a smooth tradeoff between generalization and conversation for offline learning. Further, VEM enables effective implicit planning along offline trajectories to accelerate the convergence of EVL and achieve better advantage estimation. The experimental results demonstrate that VEM achieves superior performance in most D4RL tasks."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. The proposed method is inspired by the hypothesis that reweighting via bilevel optimization offers a remedy to the issue of adversarial overfitting. Extensive experiments demonstrate that the approach improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines."
SP:3ad36be6b6900aabe43da043461cf178ce977082,"This paper proposes a generalization of equivariant graph neural networks (GNNs) that allow node and edge information to be vector- or tensor-valued. Specifically, node attributes can include position, force, velocity, spin, etc. The proposed method is composed of steerable MLPs that incorporate geometric and physical information in both the message and update functions. Experiments on several tasks in computational physics and chemistry demonstrate the effectiveness of the proposed method."
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"This paper proposes a new differentiable physics model for composite materials such as cloths. The main idea is to model the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. To this end, the authors propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems. The authors demonstrate the model’s explicability in learning meaningful physical parameters, versatility in incorporating complex physical structures and heterogeneous materials, data-efficiency in learning, and high-fidelity in capturing subtle dynamics."
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper studies the problem of transfer learning in lifelong reinforcement learning, i.e. how to decide whether a new task with possibly many goals can be solved using existing skills or whether a task-specific skill should be learned. The authors propose a method, SOPGOL, that leverages the Boolean algebra framework of Nangue Tasse et al. (2020) to determine which skills should be reused in a new tasks. They also provide bounds on the performance of the transferred policy and the number of tasks that need to be learned throughout an agent’s lifetime to generalize over a distribution."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper proposes a distributed approach to multivariate time series classification (MTSC) based on wavelet scattering and distributed feature selection. The proposed method, called LightWaveS, is able to achieve comparable performance to the state-of-the-art method (MINI)ROCKET, which is based on random convolutional kernels, while using only 2,5% of the features of MINIRocket. The method scales well with more nodes and large number of channels and can significantly reduce the input size."
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a method for pretraining ELECTRA-style text encoders with an adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. The main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, this paper jointly trains multiple MLMs of different sizes to provide training signals at various levels of difficulty. The authors propose to learn mixture weights over the auxiliary MLM’s outputs to maximize the discriminator loss by backpropagating the gradient from the main Transformer via Gumbel-Softmax. The experiments on the GLUE and SQuAD benchmarks demonstrate the empirical advantages of AMOS."
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper proposes an adaptive fine-tuning method for cloze-style fact extraction. The idea is to fine-tune a pre-trained language model on the standard fill-mask task using a small training dataset of existing facts from a knowledge graph. The proposed method, called BERTriple, outperforms all baselines, even by using significantly fewer training facts. The authors also analyze the transfer learning capabilities of this adapted language model."
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper proposes to learn the knowledge base embeddings in different geometric spaces and apply manifold alignment to align the shared entities. The aligned representations are evaluated on the out-of-taxonomy entity typing task, where the goal is to predict the types of the entities from the knowledge graph. Experimental results on YAGO3 demonstrate that the approach has significantly good performances, especially in low dimensions and on small training rates."
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a one-shot learning framework for link prediction in temporal knowledge graphs. The proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. The experiments show that the proposed algorithm outperforms existing state-of-the-art baselines in predicting new events for infrequent relations."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes Progressive Module Networks (PMN), a framework for multi-task learning by progressively designing modules on top of existing modules. Each module is a neural network that can query modules for lower-level tasks. The modules communicate by learning to query other modules and process their outputs, while the internal module processes are a blackbox. Examining the queries, replies, and the reasoning behind the choices a parent module’s makes, we can understand the reasoning processes."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper proposes a method to improve the efficiency of convolutional neural networks (CNNs) by learning channel-selectivity, i.e., redistributing its computations to important channels. The proposed method consists of two steps: (1) pruning unimportant channels, and (2) rewiring the parameters to the important channels during training. Experiments on CIFAR-10 and ImageNet show that the proposed method can improve the model compression and accuracy."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper studies the problem of PU learning, i.e., learning a binary classifier only from positive data and unlabeled data (PU learning). The authors propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, the authors show that the proposed method outperforms previous methods for PU learning on various real-world datasets."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes to use cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss. The authors show that their approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and RL on Atari games."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric framework to analyze the high-dimensional geometry of adversarial examples. In particular, it highlights the importance of codimension: for low-dimensional data manifolds embedded in high dimensional space, there are many directions off the manifold in which to construct adversarial attacks. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low dimensional data manifold well, but classifies points near the manifold incorrectly. The paper also provides sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes a method for learning interpretable representations of time-series data. The proposed method builds on ideas from interpretable discrete dimensionality reduction and deep generative modeling. The authors introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Moreover, to allow for a probabilistic interpretation of the method, the authors integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance and provides additional explanatory insights."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper investigates the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. The authors show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. They prove that there is a trade-off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. They also provide a general method that is easily applicable to a large family of commonly used latent distributions."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes to use hyperbolic geometry as an inductive bias for the activations of deep neural networks. In particular, the authors propose to use the number of objects in the space of nodes as a function of the distance from the query to the center of the network. The authors show that this induces more compact representations. The proposed method is evaluated on a variety of tasks, including neural machine translation, link prediction, shortest path length prediction, and visual question answering."
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper studies the problem of DNN fingerprinting attacks that exploit cache side-channels. The authors define the threat model for these attacks: the adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine where the victim’s deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, the authors introduce DeepRecon, an attack that reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload, a cache-side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes, the attacker can reconstruct the victim's entire network architecture."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a hierarchical network model, called Hierarchical Prediction Network (HPNet), to learn spatiotemporal memories in a representational hierarchy for predicting future video frames. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. The network learns by comparing the incoming signals with its prediction, updating its internal model of the world by minimizing the prediction errors at each level of the hierarchy in the style of predictive self-supervised learning.  The network processes data in blocks of video frames rather than a frame-to-frame basis. This allows it to learn relationships among movement patterns, yielding state-of-the-art performance in long range video sequence predictions in benchmark datasets. "
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper proposes a method to compute continuous embeddings for kmers from raw RNA-seq data, in a reference-free fashion. The authors claim that their model captures information of both DNA sequence similarity as well as DNA sequence abundance in the embedding latent space. They confirm the quality of these vectors by comparing them to known gene sub-structures and report that the latent space recovers exon information. They show that this latent space allows the detection of genomic abnormalities such as translocations and patient-specific mutations, making this representation space both useful for visualization and analysis."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes a method for model compression based on the architecture space. In particular, the proposed method trains a one-dimensional convolutional encoder/decoder neural network to learn a mapping from discrete architecture space to a continuous embedding and back. This embedding is jointly trained to regress accuracy and parameter count in order to incorporate information about the architecture’s effectiveness on the dataset. During the compression phase, the authors first encode the network and then perform gradient descent in continuous space to optimize a compression objective function that maximizes accuracy and minimizes parameter count. The final continuous feature is then mapped to a discrete architecture using the decoder. The proposed method is evaluated on CIFAR-10/100, FMNIST and SVHN."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper studies the problem of learning to control a robot with an internal dynamics model and a nominal dynamics model. The authors propose a framework called POLO, which combines trajectory optimization, value function learning, and exploration. They show that trajectory optimization is able to cope with approximation errors in the value function, and that approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. They also show how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper proposes an approach to zero-shot machine translation between language pairs for which there is no aligned data. It builds upon the multilingual NMT architecture of (Johnson et al. 2016) by applying reinforcement learning, using only monolingual data on the zero shot translation pairs, inspired by dual learning (He et al., 2016). Experiments show that this approach outperforms the mult bilingual NMT baseline model for unsupervised language pairs, on in-domain evaluations in the UN corpus."
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper presents an analysis of IRGAN, a generative adversarial network (GAN) framework for information-retrieval (IR) problems. IRGAN is a GAN-based approach that attempts to learn the correct conditional probability distribution p(d|q) over the documents (d) given the query (q). The authors claim that optimizing their minimax loss function will result in a generator which can learn the distribution, but their setup and baseline term steer the model away from an exact adversarial formulation, and this work attempts to point out certain inaccuracies in their formulation. The authors also propose a co-training setup where two models are trained in a cooperative fashion rather than an adversarial fashion."
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,This paper proposes a method to induce sparsity in the latent space of variational auto-encoders (VAEs) by using a Spike and Slab prior distribution. The authors claim that this approach is able to infer truly sparse representations with generally intractable non-linear probabilistic models. They show that these sparse representations are advantageous over standard VAE representations on two benchmark classification tasks (MNIST and Fashion-MNIST) by demonstrating improved classification accuracy and significantly increased robustness to the number of latent dimensions. The sparse elements capture subjectively understandable sources of variation.
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper proposes a non-adversarial feature matching-based approach to train generative models. The proposed approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. The experimental results demonstrate that the proposed approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"This paper studies the expressive power of graph neural networks (GNNs) in terms of the Weisfeiler-Lehman (WL) graph isomorphism test. The authors show that GNNs are not as expressive as the WL test if the GNN’s aggregation scheme is highly injective and can injective functions. They also show that the discriminative power of popular GNN variants, such as Graph Convolutional Networks (GCN) and GraphSAGE (GS), cannot learn to distinguish simple graph structures. Finally, they propose a simple architecture that is provably the most expressive among the class of GNN and is as powerful as WL. They empirically validate their theoretical findings on a number of graph classification benchmarks."
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"This paper proposes a variational variational continual learning (VCL) framework that uses saliency maps to provide explanations of performed tasks and proposes a new metric to assess the quality of the explanations. The authors claim that the proposed method is flexible and can enhance both the interpretability and performance of continual learning methods, especially in terms of mitigating catastrophic forgetting. Experiments show that interpretability is not only useful for increasing the understanding of the obtained results but can also improve the performance of a sequential learning procedure."
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a new evaluation framework for adversarial attacks on neural sequence-to-sequence (seq2seq) models taking the meaning preservation into account. The authors also propose new constraints for attacks on word-based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. Furthermore, they show that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper proposes a new approach to combine the policies using original rewards and inverse (negative) rewards to improve the performance of reinforcement learning algorithms. Specifically, the authors propose a hybrid policy that combines the policies based on deep Q-learning, double-Q-learning and on-policy actor-critic. The authors prove the convergence of the proposed inverse policies and show that the resulting policies are competitive with the original policies and help the original policy correct its mis-actions. Experiments on some games in OpenAI gym show the proposed approach can obtain the rewards up to 63.8%, 97.8% and 54.7% more than the original algorithms."
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper proposes a method for learning hierarchical object representations and dynamics models from videos. The proposed method, called PSD (Part, Structure, and Dynamics), is an end-to-end model that learns to predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that the proposed method works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e," of the generative classifier on top of the discriminative deep model can improve the classification accuracy, with neither re-training of the deep model nor changing its architectures. The authors propose a novel inference method, Deep Determinantal Generative Classifier (DDGC), which can obtain a more robust decision boundary under any softmax neural classifier pre-trained on noisy datasets. The experimental results demonstrate the superiority of DDGC given different learning models optimized by various training techniques to handle noisy labels or adversarial samples."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes a method for hierarchical reinforcement learning (HRL) that uses incremental unsupervised learning over a memory of the most recent experiences of the agent to learn subgoals and skills. The main idea is to identify a relatively small set of states that are likely to be useful to attain and then learn the skill policies to achieve those states. The proposed method can be combined with an intrinsic motivation learning mechanism, which is based on experiences in the environment. The method is evaluated on two RL problems with sparse delayed feedback."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a neural framework for learning to solve the Circuit Satisfiability problem (aka Circuit-SAT), which is an NP-complete combinatorial optimization problem. The authors propose an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. Experimental results show the superior out-of-sample generalization performance of the framework compared to the recently developed NeuroSAT method."
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes to combine the cross-entropy method (CEM) and Twin Delayed Deep Deterministic Policy Gradient (TD3), another off-policy deep RL algorithm, to improve the sample efficiency of DDPG. CEM-RL is mainly an evolutionary method, and the authors claim that it is competitive to the state-of-the-art even when considering sample efficiency, which is not the case for other deep neuroevolution methods. The authors also claim that importance mixing is not a clear benefit of using it, neither does the effect of adding noise on actions."
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes an interpretable multi-variable LSTM (IMV-LSTM) for time series with exogenous variables. In particular, the proposed model is equipped with hidden state matrix and update process, so as to learn variableswise hidden states. The authors also develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Extensive experiments on real datasets demonstrate the prediction performance and interpretability of the proposed method."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes a data augmentation method called feature smoothing to improve the adversarial robustness of neural networks. The proposed method is based on the intuition of generating virtual data points as close as adversarial examples to avoid the computational burden of generating data during training. The authors also propose a unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. They show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothed all produce an unbiased estimation of the decision boundary with smaller estimated variance. "
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for deep and locally connected neural networks. The framework is built upon teacher-student setting, by projecting the student’s forward/backward pass onto the teacher's computational graph. This framework could help facilitate theoretical analysis of many practical issues, e.g. disentangled representations in deep networks."
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper proposes a modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy. The authors claim that this approach allows efficient learning of behaviors and preferences representation. In the experiment with video games playing, the proposed method allows separation of main task’s objectives and behaviors between different BMs. The experiments also show network extendability through independent learning of new behavior patterns. Moreover, the authors demonstrate a strategy for an efficient transfer of newly learned BMs to unseen tasks."
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper introduces a biologically-inspired method for training networks to self-modify their weights. Building upon the differentiable plasticity framework, which already improved performance (sometimes dramatically) over non-plastic architectures on various supervised and RL tasks (Miconi, 2016; Miconi et al., 2018), here they introduce neuromodulated plasticity to let the network control its own weight changes. As a result, for the first time, neurommodulated plastic networks can be trained with gradient descent, opening up a new research direction into optimizing large-scale self modifying neural networks. "
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper proposes an iterative quantization technique that performs quantization once every few steps, combined with binary model training. The authors also propose a new loss function to regularize the weights, resulting in reduced quantization error. Experiments on CIFAR and WikiText-2 show the effectiveness of the proposed method."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a method for style transfer onto open-ended content, i.e., the method generalizes to style and content not present in the training data. The method starts by constructing a content embedding using an existing deep metric-learning technique. This trained content encoder is incorporated into a variational autoencoder (VAE), paired with a to-be-trained style encoder. An auxiliary loss, leakage filtering, ensures that no style information remaining in the content representation is used for reconstruction and vice versa. The proposed method is evaluated on few-shot learning tasks."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a new method to speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP), i.e., after an action at is performed in a state and the agent reaches the new state st+1, the agent can decide whether the action is permissible or not permissible in state st. An action is not permissible if the action can never lead to an optimal solution and thus should not be tried. The proposed method is based on the idea that it is quite clear that an action is non-permissible if it would never lead the agent to the optimal solution in the long run and thus accumulated reward accumulated in the run in the state st should not have been taken, thus reducing the possibility of making repetitive mistakes in the future. The paper shows that the proposed method can be leveraged to reduce the exploration of the action space of RL."
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper studies the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, the analysis reveals interesting phase transition phenomena when the depth becomes large. It also provides insights on pitfalls in training initialization practice, and demonstrates experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes SimBA, a simple black-box adversarial attack that takes small steps iteratively towards the decision boundary, and demonstrates through extensive experiments its unprecedented query efficiency in both the untargeted and targeted settings. SimBA randomly picks a low frequency component of the discrete cosine transform (DCT) and either adds or subtracts it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. The authors argue that the proposed algorithm should serve as a strong baseline for future adversarial attacks, because it is extremely fast and can be implemented in less than 20 lines of PyTorch code."
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"This paper proposes a method for hierarchical option discovery based on clustering the successor representations of options. The idea is to cluster the successor options in a well-connected region of the state space, such that the options that are likely to be better for exploration are clustered in the same region. The authors also propose a new pseudo-reward for learning the intra-option policies. Finally, the authors propose an iterative approach that iteratively builds options and explores in environments where exploration through primitive actions is inadequate to form the Successor representations. "
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper proposes a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, are introduced to uncover and fine-tune the decision boundary of each domain, and the uncertain domain is newly introduced in the framework to adopt those instances whose domain labels cannot be predicted confidently. Extensive experiments demonstrate that the proposed approach achieves the state-of-the-art performance on OSL and G-ZSL benchmarks."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes polar prototype networks, a class of networks that explicitly states the structure, i.e., the layout, of the output. The structure is defined by polar prototypes, points on the hypersphere of the input space. For classification, each class is described by a single polar prototype and they are a priori distributed with maximal separation and equal shares. For regression, training can be performed as a polar interpolation between two prototypes, arriving at a regression with higher-dimensional outputs."
SP:d1034342785d133cf8372b8624897963cc2ee83a,"This paper proposes a method for learning a reward function for a reinforcement learning agent (RL agent) that optimizes only the features specified in the reward function and is indifferent to anything left out inadvertently. The key insight is that when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want, and the RL agent can use this implicit preference information from the state to fill in the blanks. The authors develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. They find that the learned reward function can infer both side effects that should be avoided and preferences for how the environment should be organized."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed method combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, the authors express the latent variable space of a variational autoencoder in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters and the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variables values. The authors validate the framework in extensive experiments on MNIST, Omniglot, and CIFAR-10."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,This paper proposes a dynamical neural network (DNN) to learn dictionaries for sparse representations. The network consists of a set of spiking neurons that interact over time continuously. It is shown that the network’s evolution and/or limit points in the associated state space correspond to numerical solutions to certain mathematical optimization or learning problems. The authors show that the true gradients for learning are provably computable by individual neurons using only local information.
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"This paper proposes an end-to-end network for the task of semantic image segmentation and lane detection. The authors propose multiple encoder-decoders network to extract the spatial information and pinpoint the localization of the lane detection, and propose a small quantity of channel to reduce overfitting by considering interdependencies among channels. Extensive experiments on CUlane dataset demonstrate the effectiveness of the proposed method."
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper proposes a new method for batch contextual bandit learning from historical data. The proposed method, Maximum Likelihood Inverse Propensity Scoring (MLIPS), estimates a maximum likelihood surrogate policy based on the logged action-context pairs, and then uses this surrogate policy as the proposal. Theoretical analysis shows that MLIPS is asymptotically unbiased, and moreover, has a smaller mean squared error than the classical IPS estimator. Experimental results on multi-label classification problems and a large-scale ad placement dataset demonstrate the empirical effectiveness of MLIPS."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"Meta-learning is a promising learning framework to address few-shot classification tasks. In existing meta-learning methods, the meta-learner is designed to learn about model optimization, parameter initialization, or similarity metric. Differently, in this paper, the authors propose to learn how to create an individualized feature embedding specific to a given query image for better classifying, i.e., given a query image, a specific feature embeddings tailored for its characteristics is created accordingly, leading to an individualised feature space in which the query image can be more accurately classified. "
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper proposes a population-based genetic algorithm (GA) to train deep neural networks for deep reinforcement learning (RL) problems. The authors show that the proposed method, called Deep GA, outperforms ES, A3C, and DQN on Atari and humanoid locomotion. They also show that Deep GA is faster than ES, DQNs, and A3Cs on Atari. Finally, they show that deep GA can be used to improve the performance of other neuroevolution techniques."
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a new curiosity method that uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observations from those in memory — which incorporates rich information about environment dynamics. This allows the proposed method to overcome the known “couch-potato” issues of prior work — when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. The proposed method is evaluated on VizDoom, DMLab and MuJoCo."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper proposes a method for learning a transition model for complex uncertain domains using relational rules. For any action, a rule selects a set of objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct the set of deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a method for instance-wise feature selection. The proposed method is based on the actor-critic methodology, which is used to train a selector network, a predictor network, and a baseline network. During training, the selector network is trained to minimize the KL divergence between the conditional distribution of the full conditional distribution and the selected-only conditional distribution. Experiments on both synthetic and real-world data show that the proposed method outperforms state-of-the-art methods."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a domain adaptation method for semantic segmentation. The authors propose to learn discriminative feature representations of patches based on label histograms in the source domain, through the construction of a disentangled space, and then use an adversarial learning scheme to push the feature representations in target patches to the closer distributions in source ones. In addition, the authors show that their framework can integrate a global alignment process with the proposed patch-level alignment and achieve state-of-the-art performance on several benchmark datasets."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper proposes two new optimistic algorithms for AMSGrad and Adam, respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in OPTIMISTIC ONLINE LEARNING, which leads to speed up in training deep neural nets in practice. The proposed algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper introduces two new benchmarks for robustness to image corruption and perturbation. The first one, called ImageNet-C, is designed to evaluate the robustness of image classifiers to common perturbations. The second one is called Image-P, which is a new dataset for evaluating the performance of classifiers on perturbed images. The authors also propose several methods and architectures to improve the performance on these two datasets."
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper provides a theoretical analysis of dropout training as a family of conditional models whose objectives are lower bounded by the usual dropout objective. The authors show that the family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully-stochastic dropout. The deterministic subvariant’s bound is equal to its objective, and the highest among these models. It also exhibits the best model fit in the experiments."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a global soft filter pruning (GSFP) scheme to prune redundant filters of convolutional neural networks. Specifically, the authors adopt a robust pruning method, which measures the global redundancy of the filter in the whole model by using the soft pruning strategy. In addition, in the model recovery process after pruning, they use the cumulative saliency strategy to improve the accuracy of pruning. Experiments on MNIST and CIFAR-10 show that the proposed method achieves a much higher compression ratio compared with prior work while maintaining the same test accuracy."
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a cross-lingual document classification framework (CACO) between related language pairs. CACO jointly trains a character-based embedder and a word-based classifier. The embedder derives vector representations for input words from their written forms, and the classifier makes predictions based on the word vectors. The authors use a joint character representation for both the source language and the target language, which allows the embedder to generalize knowledge about source language words to target language words with similar forms."
SP:544e421f9c747640d949f433e3091763508b7237,This paper proposes a method for weakly-supervised temporal action localization. The authors propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. Extensive experiments on THUMOS14 and ActivityNet1.3 datasets show that the proposed MAAN achieves a superior performance on the task.
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper proposes to use holographic reduced representation (HRR) to learn disentangled representations for the task of language modeling. HRR is a generalization of the vector symbolic architecture (VSA) family, which is used to represent and manipulate structures. The authors propose a language model with HRR that explicitly encodes the underlying structure as role-filler pairs on both word-level and chunk-level representations. They show that by using HRR as a structured compositional representation, their models are able to discover crude linguistic roles, which roughly resembles a classic division between syntax and semantics."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,This paper studies the problem of joint active perception and planning in partially observable Markov decision processes (POMDPs). The authors propose a greedy strategy for observation selection that aims to minimize the uncertainty in state. They also develop a novel point-based value iteration algorithm that incorporates the greedy strategy to achieve near-optimal uncertainty reduction for sampled belief points. This enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning.
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"This paper proposes a new curriculum loss for training deep neural networks. The authors claim that training hard examples aggravates the distribution shifting and damages the training. To address this problem, the authors propose a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low-weighted samples. The intuition of the loss is very simple. It trains top layers on ”good” samples to reduce large shifting, and encourage ”bad’ samples to learn from ” good” sample. The proposed representation loss aims to encourage their training."
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper proposes a method to learn heuristics for combinatorial optimization problems. The authors propose a model based on attention layers with benefits over the Pointer Network and show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which they find is more efficient than using a value function. They significantly improve the performance of the heuristic for the Travelling Salesman Problem (TSP) and the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper proposes a differentiable neural architecture search (DNAS) framework to efficiently explore the exponential search space with gradient-based optimization (SGD) to find the optimal neural network architecture in a given search space. In particular, the authors focus on the problem of mixed precision quantization of a ConvNet to determine its layer-wise bit-widths. The proposed DNAS is very efficient, taking less than 5 hours to finish a search on ResNet18 for ImageNet. The quantized models with 21.1x smaller model size or 103.9x smaller computational cost can still outperform baseline quantized or even full precision models."
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"This paper proposes a new attention model for sequence to sequence learning. The proposed approach is based on the idea of conditioning the attention on the output of the previous step. Specifically, the output distribution is conditioned on the posterior distribution of the attention, which is a mixture of the output and the attention. The authors also propose a way to incorporate task-specific structural biases and prior knowledge into attention. Experiments are conducted on translation and morphological inflection tasks."
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper proposes a method for the unpaired image-to-image translation problem. The proposed method, called HarmonicGAN, is based on CycleGAN. The main idea is to introduce a smoothness term over the sample graph to enforce smoothness-consistency between the source and target domains. The method is evaluated on medical imaging, object transfiguration, and semantic labeling."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"This paper proposes a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing the exploding and vanishing gradient problem (EVGP) in recurrent neural networks. Specifically, the authors show that when the LSTMs weights are large, the gradient components through the linear path (cell state) in the computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which they show empirically), their suppression can prevent LSTm from capturing them. Their algorithm prevents gradients flowing through this path from getting suppressed, thus allowing the LstM to capture dependencies better."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a method for training binary weight networks (BWNs) from scratch under the Bayesian deep learning perspective, where the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, it generates binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. The policy network has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, thus it is applicable to any neural network architecture. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a probabilistic federated learning framework for neural networks. The authors propose a Bayesian nonparametric framework for learning a compressed global model of neural networks on non-centralized data without access to the original data. Specifically, the proposed method trains local models for each data source, in parallel, and then matches the estimated local model parameters (groups of weight vectors) across data sources to construct a global network. The matching is governed by the posterior of a Beta-Bernoulli process (BBP) that allows the local parameters to either match existing global ones or create a new global parameter if existing ones are poor matches. Crucially, the authors make no assumptions about how the data is distributed between the different sources or even about the local learning algorithms."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper studies the problem of learning in non-convex two-player zero-sum games. The authors propose a new algorithm called Stable Opponent Shaping (SOS) that interpolates between Learning with Opponent-Learning Awareness (LOLA) and a stable variant called LookAhead. Theoretical convergence guarantees are established for the proposed algorithm, which is shown to converge locally to equilibria and avoid strict saddles in all differentiable games. Experiments are conducted to show the effectiveness of the proposed method."
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes an alarm system to set off alarms when the segmentation result is possibly unsatisfactory. The proposed method is based on shape feature, which is a strong prior information shared among different data, so it is capable to predict the qualities of segmentation results given different segmentation algorithms on different datasets. The shape feature is captured using the value of loss function when the results are tested using a Variational Auto-Encoder (VAE). The VAE is trained using only the ground truth masks, and therefore the bad segmentations results with bad shapes become the rare events for VAE and will result in large loss value. Finally, the representation in the one-dimensional feature space is learned by projecting the results into a low dimensional feature space, and then learn classifiers/regressors in the feature space."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper proposes an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The proposed network is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper proposes an end-to-end approach to program synthesis from natural language (NL) specifications. The authors propose a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LstM, for which the authors propose novel signal propagation schemes and soft attention mechanism. The proposed method is evaluated on a large dataset of problems proposed in a previous work and shows comparable performance to the method proposed in the previous work."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper studies the adversarial robustness of deep neural networks (DNNs) against adversarial perturbations on the MNIST classification task. The authors propose a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. They derive bounds on the robustness and go to great length to empirically evaluate the model using maximally effective adversarial attacks by applying decision-based, score based, gradient-based and transfer-based attacks for several different Lp norms. The results suggest that the proposed approach yields state-of-the-art robustness on MNIST against L0, L2 and L_\infty adversarials."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, the authors propose a new reparameterization approach for the weights matrices in GAN training, allowing us to directly manipulate the spectra of the weight matrix through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, they further show that the spectrum control improves the generalization ability of GAN. The experiments on CIFAR-10, STL-10 and ImgaeNet datasets confirm that compared to other methods, the proposed method is capable of generating images with competitive quality."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,This paper proposes Anderson Accelerated Value Iteration (A2VI) for value iteration in reinforcement learning. The main idea is to use the Anderson acceleration technique to accelerate the value iteration. The authors also propose a deep Anderson accelerated Q-learning (DA2Q) algorithm based on A2VI. Theoretical analysis is provided to show the convergence of the proposed algorithm. Empirical results show that the proposed method is more efficient than the modified policy iteration (MPI) method.
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper proposes a novel method, SupportNet, to solve the catastrophic forgetting problem in the class incremental learning scenario. SupportNet combines the strength of deep learning and support vector machine (SVM), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The experimental results show that SupportNet drastically outperforms the state-of-the-art incremental learning methods and even reaches similar performance as the neural network trained from scratch on both old and new data."
SP:d228d213f79716774043cea253305fecece659ec,"This paper compares four measures of unit selectivity in AlexNet, namely, localist selectivity, precision, CCMAS, top-class selectivity and activation maximization (AM), and a new measure called “Top-Class Selectivity”. The authors claim that AlexNet fails to find any 100% selective ‘localist units’ and demonstrate that the precision and CCMAS measures provide a much higher level of selectivity than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. The interpretable images in the hidden layers are not associated with highly selective units."
SP:b9deae0392e0160b400d76c549d382e235196f8c,This paper proposes a graph neural network (GNN) based community detection method for node-wise and edge-wise classification problems. The proposed method is based on the line graph of edge adjacency matrices and the non-backtracking operator. The authors also provide a theoretical analysis of the optimization landscape of using (linear) GNNs to solve community detection problems.
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper studies the problem of online dictionary learning, where the goal is to learn a linear combination of a few columns of a matrix. The authors propose a new algorithm, called Neurally plausible alternating Optimization-based Online Dictionary Learning (NOODL), which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. The algorithm is scalable and amenable for large scale distributed implementations in neural architectures, by which they mean that it only involves linear and non-linear operations."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"This paper proposes a new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. The loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. The training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hash codes, the authors use multi-indexing. The authors demonstrate that these techniques provide large improvements to a similarity search tasks."
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"This paper proposes a method for neural architecture search (NAS) that learns to predict the parameters of unseen neural networks by directly operating on their computational representations. The proposed method, called Graph HyperNetwork (GHN), is a composition of graph neural networks and hypernetworks that generates the weights of any architecture by operating directly on their computation graph representation. The method is evaluated on CIFAR-10 and ImageNet and achieves competitive results compared to other random search methods. "
SP:65ccf43cd4e033d22239069057f5200d49f33724,"This paper proposes a method to improve generative adversarial imitation learning (GAIL) by using additional information from non-expert demonstrations which are easier to obtain. The key idea of the proposed method is to perform multiclass classification to learn discriminator functions where non-experts demonstrations are regarded as being drawn from an extra class. Compared to related methods that use an additional dataset for IL, M-GAIL relies on a less restrictive assumption on the dataset and can efficiently train deep neural networks and can avoid learning a mixture policy. Experiments in continuous control tasks demonstrate that the method learns better policies."
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper proposes a new class of neural networks, called Invertible Neural Networks (INNs), for the task of inferring the posterior distributions of the parameters of the forward process conditioned on a set of measurements. The authors argue that INNs are well-suited for this task, as they focus on learning the forward processes, using additional latent output variables to capture the information otherwise lost. They prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INN are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify parameter correlations."
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a method for quantifying the uncertainty of predictions in deep neural networks (NNs). The main idea is to replace the fixed mixing weights of a mixture density network (MDN) with an adaptive, input-dependent distribution (specifying the probability of each component) represented by an NN, and by considering uncountably many mixture components. The resulting model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks (CDNs). The authors empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper proposes a method to compress the weights of a neural network by using a variational distribution over weights. The proposed method is based on the bits-back argument, which states that the effective cost of the network weights is the coding cost of a weight-prior p, which is the number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distributions and the encoding distribution. By imposing a constraint on the KL-divergence, the authors are able to control the compression rate, while optimizing the expected loss on the training set."
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper proposes ProxylessNAS, a proxy-less approach to neural architecture search (NAS) that can directly learn the architectures for large-scale target tasks and target hardware platforms. The main idea is to remove the restriction of repeating blocks in previous proxy-based NAS works and allow all of the blocks to be learned and specified parameters to be specified. To achieve this, they reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper proposes to use second-order penalties to enforce fairness in non-convex, large-data settings. In particular, the authors argue that this results in a more practical training procedure that avoids the instability and potential lack of convergence associated with two-player min-max games. The authors also derive a method for efficiently computing the gradients associated with the second order penalties in stochastic mini-batch settings. The resulting algorithm performs well empirically."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"This paper proposes a reweighted wake-sleep (RWS) algorithm for learning generative models with discrete latent variables. The authors show that RWS outperforms current state-of-the-art methods in learning discrete latent-variable models. They also show that, unlike the importance weighted autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well."
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper proposes a method for training structured prediction energy networks (SPENs), a type of energy-based models for structured output prediction. SPENs learn an energy landscape on pairs of input x and structured outputs y, and the goal is to infer the target output y by finding the minimum of energy function E conditioned on input x: y = argminy E(x,y). The authors propose to use truncated randomized search in the reward function of the energy function to guide the SPEN to find the best output with respect to this reward function. In particular, they propose to sample from reward function through randomized search, which is used to generate informative optimization constraints. These constraints gradually guide gradient-descent inference toward finding better prediction according to reward function, leading to better performance on the test set."
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper proposes an active learning approach for robust policy search (RPS). RPS is the problem of learning policies that do not degrade in performance when subject to unseen environment model parameters. It is particularly relevant for transferring policies learned in a simulation environment to the real world. Several existing approaches involve sampling large batches of trajectories which reflect the differences in various possible environments, and then selecting some subset of these to learn robust policies. This paper proposes to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select a subset. The authors apply this framework to an existing method, namely EPOpt, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control tasks."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper proposes a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. The authors prove convergence for TTN, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation."
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the policy to execute, and updates the model based on new observations. The proposed approach outperforms strong baselines that do not explicitly plan using the semantic content."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a method to improve the generalization and explanation ability of end-to-end deep learning driving models. The proposed method consists of two modules: a perception module for see and think and a driving module for behave. The perception module is used for easier driving-related learning, which we refer to as ability of pixel level understanding of input, including what & where & how far and whether segmentation map and depth map. The driving module is trained with multi-task perception-related basic knowledge and driving knowledge stepwisely. "
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the trade-off between adversarial robustness and standard generalization. The authors show that the goal of adversarially robust generalization might fundamentally be at odds with that of standard generalisation. They also argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"This paper proposes Initialized Equilibrium Propagation, a method for gradient-based training of neural networks which uses only local learning rules and does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, this paper trains a feedforward network to initialize the iterative inference procedure for equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-points using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward networks. The experiments show that this network appears to work as well or better than the original version of equilibrium propagation while requiring fewer steps to converge."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper proposes a zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which requires only the sign information of gradient estimates but is able to achieve a comparable or better convergence speed than SGD-type algorithms. Theoretically, the authors show that ZO signSGD requires $\sqrt{\delta}$ times more iterations than sign SGD, leading to a convergence rate of $\tilde{O}(\sqrt{d}/\sqrt {T})$ under some mild conditions, where d is the number of optimization variables. The authors also analyze the effects of different types of gradient estimators on the convergence rate, and propose several variants of ZO SignSGD with $\delta$-optimal convergence rate. Empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance on the generation of adversarial examples from black-box neural networks."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper proposes a new optimization method to reduce the computation efforts of convolutional neural networks. The proposed method takes advantage of the fact that some convolution operations are actually wasteful since their outputs are pruned by the following activation or pooling layers. The authors propose to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. Furthermore, a fine-tuning process is conducted to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example model and Network in Network on the Cifar-10 and CifAR-100 datasets."
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper proposes to exploit the temporal dependency property in audio data to characterize audio adversarial examples. The experimental results show that while four primitive input transformations on audio fail to withstand adaptive adversarial attacks, temporal dependency is shown to be resistant to these attacks. The proposed method is easy to operate and does not require model retraining."
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes to incorporate object compositionality into the generator of a generative adversarial network (GAN). Specifically, the authors propose to explicitly consider the relation between objects and their relations explicitly, and generate images by means of composition. They evaluate their approach on several multi-object image datasets, and find that the generator learns to identify and disentangle information corresponding to different objects at a representational level. This provides a way to efficiently learn a more accurate generative model of real-world images, and serves as an initial step towards learning corresponding object representations."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper proposes a method for learning disentangled representations from visual data, where different high-level generative factors are independently encoded. The authors introduce a learning setting which they refer to as “reference-based disentangling”: given a pool of unlabelled images, the goal is to learn a representation where a set of target factors are disentanglement from others. The only supervision comes from an auxiliary ""reference set"" that contains images where the factors of interest are constant. In order to address this problem, they propose reference-based variational autoencoders, a novel deep generative model designed to exploit the weak supervisory signal provided by the reference set. During training, they use the variational inference framework where adversarial learning is used to minimize the objective function. By addressing tasks such as feature learning, conditional image generation or attribute transfer, they validate the ability of the proposed model to learn disentangle representations from minimal supervision."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper proposes a meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control. The underlying distribution that the authors aim to model is the dynamics distribution p(st+1|st,at, Tt), where the unknown Tt represents the underlying settings (e.g., state of the system, external details, environmental perturbations, etc.). The goal for MOLe is to estimate this distribution with a predictive model p\theta, which is parameterized as a neural network model with three hidden layers and ReLU nonlinearities. The authors formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, the authors observe that meta-training can be used to meta-train a model that this direct online adaptation with SGD is effective."
SP:5665e5f006f84927beb0440e145f476e02538077,This paper proposes a method for training RNN-based reinforcement learning agents from experience replay. The authors first show that zero state initialization can lead to representational drift and recurrent state staleness in the early states of replayed sequences. They then propose two approaches to mitigate these issues: (1) storing the context-dependent recurrent state and (2) using burn-in. The proposed method achieves state-of-the-art performance on Atari-57 and DMLab-30.
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper proposes a hierarchical framework for learning sequential generative models for capturing coordinated multi-agent trajectory behavior. The proposed approach is inspired by recent work on leveraging programmatically produced weak labels, which is extended to the spatiotemporal regime. In addition to synthetic settings, the authors show how to instantiate the framework to effectively model complex interactions between basketball players and generate realistic multiagent trajectories of basketball gameplay over long time periods. The authors validate the approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts."
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper presents a graph-structured variational recurrent neural network (Graph-VRNN) that is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. The proposed method combines ideas from graph networks, variational autoencoders, and RNNs in a novel way, to create what the authors call a Graph-Structured Variational RNN (GVRNN). The authors show that their method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"This paper proposes a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. At inference time, it replaces the differentiable estimator with its external blackbox non-differentiable counterpart so that the base network output matches the input arguments of the black box function. The authors show that the proposed method generalizes better than a fully differentiable model and learns more efficiently compared to RL-based methods."
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,"This paper proposes a method for meta-learning that learns a mixture of hierarchical Bayesian models that allows the meta-learner to adaptively select over a set of learned parameter initializations for gradient-based fast adaptation (Finn et al., 2017) to a new task. The method is a generalization of model-agnostic meta learning (MAML) algorithm introduced in Finn et al. (2017) by treating the assignment of task-specific parameters in the hierarchical model induced by recasting gradient recasting. The proposed method is evaluated on the miniImageNet benchmark for 1-shot classification and few-shot regression."
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes a meta auxiliary learning method for image classification, where the auxiliary task is hierarchical sub-class image classification. The proposed method is self-supervised and general, and therefore offers a promising new direction towards automated generalisation towards generalisation to new levels. Experiments on three different CIFAR datasets show that MAXL outperforms baseline auxiliary learning methods, and is competitive even with a method which uses human-defined sub-classes hierarchies."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,"This paper proposes a neural network-based approach for open set recognition, where the goal is to learn a representation that projects instances of the same class closer together while projecting instances of different classes further apart. The proposed approach is evaluated on two malware classification datasets and one image classification dataset. The results show that the proposed approach achieves statistically significant improvement compared to the baselines. "
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of finding low-precision neural networks that are energy-efficient and have good performance on ImageNet. The authors propose two approaches: (1) starting with pretrained fp32 precision baseline networks and fine-tuning, and (2) combatting noise introduced by quantizing weights and activations during training, by using larger batches along with matched learning rate annealing. They demonstrate the effectiveness of their approach on the ImageNet classification benchmark."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,This paper proposes a method to predict the post-bounce trajectory of an object after it is thrown into a scene. The method is built on top of a physics-based approach (PIM) and a visual-inference module (VIM) that is trained to infer the physical properties of objects in a scene given a single still image. The model is trained on a newly collected dataset of 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. The proposed model learns from the collected dataset and is bootstrapped with additional information from simple physics simulations.
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper shows that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, the authors prove that the `1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. The proofs rely on the network's weight distribution at initialization, but extensive experiments confirm that their conclusions still hold after usual training."
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper proposes an interactive agent modeling scheme that encourages an agent to learn to probe. In particular, the probing agent learns to interact with the environment and with a target agent (i.e., a demonstrator) to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used for enhancing the agent modeling to learn a more accurate mind model of the target agent. The framework consists of two learning processes: i) imitation learning for an approximated agent model and ii) pure curiosity-driven RL for an efficient probing policy to discover new behaviors that otherwise can not be observed. The experimental results suggest that the agent model learned by the approach i) generalizes better in novel scenarios than the ones learned by passive observation, random probing, and other curiosity driven approaches; ii) can be used to enhance performance in multiple applications including distilling optimal planning to a policy net, collaboration, and competition."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to traditional artificial neural networks (ANNs) that mimics the function of biological neuromodulators and are termed modulators, which enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns. In this manner, it enables the slope of the activation function to be context dependent. Experiments on CNN and LSTM networks show that the modulated models consistently outperform their original versions."
SP:287a577834fd2820a939a1113b39146a22727491,"This paper proposes a neural analysis and synthesis framework (NANSY) that can perform zero-shot voice conversion, formant preserving pitch shift, and time-scale modification with a single model. The proposed model can be trained in a fully self-supervised manner, that is, it can be learned without any labeled data as text or speaker information. The experiments show that NANSY can achieve significant improvement in performance in several applications such as zero shot voice conversion."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"This paper studies the generalization properties of gradient-based bilevel programming (GP) algorithms for hyperparameter optimization. In particular, it provides an expectation bound for the unrolled differentiation algorithm based on a notion of uniform stability on validation. The paper also provides a bound on the expectation bound of the classical cross-validation algorithm. Theoretical results suggest that unrolled gradient based algorithms can be better than cross validation algorithms under certain conditions. "
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper proposes a new approach for knowledge distillation, where the goal is to improve the transferability of knowledge from a teacher to a student. The authors propose a two-stage learning procedure, i.e., student-aware training of teacher network followed by distillation from teacher to student. They train teacher networks along with their student branches, and then perform distillation between teachers and students. The proposed approach is applicable to diverse teacher architectures and can be incorporated into various existing knowledge-distillation algorithms."
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies the problem of out-of-distribution (OOD) generalization, i.e., generalization from a training set to a test set, where the test set has a different distribution than the training set. The authors propose a theoretical framework to characterize the learnability of OOD generalization problems. They also introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. Based on their framework, they prove generalization bounds and give guarantees for OOD error. Finally, they design a model selection criterion to check the model’s variation and validation accuracy simultaneously."
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper proposes a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm that tackles the issues of negative transfer and catastrophic forgetting in an online setting with non-stationary task distributions. Specifically, the meta-parameters follow a mixture of dynamic Gaussians as opposed to the commonly-used single static Gaussian. To approximate the intractable posterior of interest, the authors develop a structural variational inference method. Extensive experiments show that the algorithm outperforms state-of-the-art baselines when adapting to diverse tasks and alleviating catastrophic forgetting."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a probabilistic algorithm for solving boundary value problems (BVPs), which are ordinary differential equations (ODEs) subject to boundary conditions. The authors introduce a Gauss–Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of well established, non-probabilistic methods. The model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. The proposed method is compatible with other statistical modelling tool-chain."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper studies the problem of learning a near optimal policy for two reward-mixing Markov decision processes (MDPs), where a reward function is drawn from one of multiple reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. The authors provide the first polynomial-time algorithm that finds an optimal policy after exploring poly(poly(H, S^2 A^2) episodes, where H is time-horizon and S and A are the number of states and actions respectively. Unlike existing approaches that rely on strong assumptions on the dynamics, the authors make no assumptions and study the problem in full generality."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes a two-step procedure to estimate the multi-cause conditional average treatment effect (CATE) in the multicause setting. The proposed method, called Single-cause perturbation (SCP), augments the observational dataset with the estimated potential outcomes under single-cause interventions and then performs covariate adjustment on the augmented dataset to obtain the estimator. It is agnostic to the exact choice of algorithm in either step. The authors show the performance gain of SCP on extensive synthetic and semi-synthetic experiments."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"This paper proposes a multi-wavelet-based neural operator learning scheme that compresses the associated operator’s kernel using fine-grained wavelets. The projected kernel is trained at multiple scales derived from using repeated computation of multiwavelet transform. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. Compared with the existing neural operator approaches, this model shows significantly higher accuracy and achieves state-of-the-art in a range of datasets."
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper proposes a frequency domain approximation (FDA) method for training binary neural networks (BNNs). FDA estimates the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely FDA-BNN. The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. The experiments on several benchmark datasets and neural architectures show that the proposed method achieves the state-of-the-art accuracy."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,The authors propose to use multi-area RNNs with neuroscience-inspired architecture constraints to derive key features of multi- area computation. They show that incorporating multiple areas and Dale’s Law is critical for biasing the networks to learn biologically plausible solutions. They also show that output-relevant information is preferentially propagated between areas. These results suggest that cortex uses modular computation to generate minimal sufficient representations of task information.
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,This paper proposes structured attention graphs (SAGs) for visualizing the saliency maps of convolutional neural networks (CNNs) for image classification. The authors argue that a single saliency map provides an incomplete understanding since there are often many other maps that can explain a classification equally well. They propose to utilize a beam search algorithm to systematically search for multiple explanations for each image. They conduct a user study comparing the use of SAGs to traditional saliency mapping for answering comparative counterfactual questions about image classifications.
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies the transferability of the hidden representations of convolutional neural networks trained on ImageNet. They show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Their results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper proposes a method to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. The proposed method, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The authors test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper proposes a hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node from the source tree. Both the source and target trees are treated as latent and induced during training. The authors develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. They apply this latent neural grammar to various domains—a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation—and find that it performs respectably compared to standard baselines."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a new efficient algorithm for the function-on-scalar feature selection problem, where the number of features is much larger than the observations. The proposed algorithm is based on the SsNAL algorithm, which is then extended to the function on scalar regression framework using a Functional Principal Components representation. Theoretical results are provided to show the effectiveness of the proposed method. Empirical results on synthetic and real-world datasets are presented to demonstrate the performance of the method. "
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,This paper proposes a method for clustering multi-level marked log-Gaussian Cox processes (LGCPs) for the clustering of point process data. The proposed method is based on the semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and real data analyses. 
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper proposes a meta-learning approach for adaptive nonlinear control, which is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theory guarantees. The authors provide instantiations of their approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi-task non-linear control. They also validate OMAC empirically on inverted pendulum and 6-DoF drone control tasks under varying wind conditions."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper proposes to improve the certified robust training of interval bound propagation (IBP) and CROWN-IBP by addressing two issues, namely exploded bounds at initialization and imbalanced ReLU activation states. The authors derive a new weight initialization method for IBP training and propose to fully add Batch Normalization (BN) to each layer in the model. They also design regularization to explicitly tighten the certified bounds and balance ReLU activations during warmup. The proposed methods are able to obtain better verified error on CIFAR-10 and TinyImageNet with shorter warmup and training schedules."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection in the presence of adversarial contamination of the data. The authors propose an extension of the Huber contamination framework, which allows the contamination distributions to be different at each time point. They derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of the contamination proportion. They also propose a computationally-feasible method, matching the lower bound under certain conditions, saving for logarithmic factors."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the power of learning via mini-batch stochastic gradient descent (SGD) and batch Gradient Descent (GD) on the empirical loss of a differentiable model or neural network, and asks what learning problems can be learned using these paradigms. It shows that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision of the gradient calculations relative to the minibatch size b and sample size m. The paper shows that with fine enough precision, namely when bρ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for b = 1. Similarly, it shows that GD can also simulate any data-driven learning algorithm based on m samples."
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"This paper considers the problem of estimating the Wasserstein distance between two probability distributions over a metric space. The authors propose a modified version of the standard Lloyd's algorithm, where Voronoi cells are replaced by Power cells, and show that it converges to a solution with a small Wassersteins error. They also provide upper bounds for the convergence of the algorithm, which are tight."
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,This paper proposes a new dynamic feature transform for video understanding. The proposed RSA feature transform leverages the rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. The experiments and ablation studies show that the proposed RSA network substantially outperforms convolution and self-attention counterparts on the standard motion-centric benchmarks for video action recognition.
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the fluctuation of the learning dynamics of multilayer neural networks in terms of the second-order mean field limit. The main contribution of this paper is the derivation of a system of dynamical equations that captures the limiting fluctuation distribution. The authors show that this system is characterized by the neuronal embedding framework recently introduced by Nguyen and Pham (Nguyen and Nguyen, 2020) and show that it captures not only the time dependence but also the cross-layer dependency across layers. The limit theorem is proven to relate quantitatively this limit to the limit of the network."
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"This paper proposes to learn generalized Casimirs for energy and entropy in a metriplectic dynamical system, which is a generalization of the Poisson brackets of Hamiltonian/Lagrangian mechanics which model not just a conserved energy, but also entropy. The authors propose a parametrization of dissipative brackets that is appropriate for learning irreversible dynamics with unknown a priori model form. Theoretical results show that the learned dissipative dynamics are guaranteed to be conserved and nondecreasing, respectively. In addition, for the case of added thermal noise, the authors guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. "
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection-based algorithm for fair and robust training. The authors formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption, and propose a greedy algorithm that is efficient and effective in practice. Experiments show that the algorithm obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper proposes periodic activation functions in Bayesian neural networks (BNNs) that establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. The authors also show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, the authors show that the proposed method achieves comparable performance for in-domain data and captures sensitivity to perturbed inputs in deep neural networks for out-of-domain detection."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"This paper proposes a reinforcement learning approach to automatically assign a score to interactive student code assignments. The approach is based on the idea of learning to classify Markov Decision Processes (MDPs), where the goal is to determine whether the input MDP is correct or not. The authors propose a cooperative approach to this problem, where they use an autoregressive model and an agent that learns to interact with a student assignment, and a reward function that encourages the agent to learn to understand what type of behavior is undesirable, as well as play and not play. The paper also introduces a dataset of 711,274 student submissions to a single assignment with hand-coded bug labels to support future research."
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,This paper proposes a method for interpretable deep reinforcement learning (DRL) models. The authors propose a Represent And Mimic (RAMi) framework for training an identifiable latent representation and mimic trees for DRL interpretation. The IMONet is used to learn an object representation for RL states and a MCRTS algorithm is proposed to find a mimic tree with promising fidelity and simplicity. Experiments show that the mimic tree achieves strong approximation performance with significantly fewer nodes than baseline models.
SP:84560de78af979354fff83d1370d8675c1e9191f,"This paper proposes a Bayesian framework for modeling the structure of dynamic predictions over time. The authors model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. In contrast to general methods for time series analysis, this approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths."
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration with fixed confidence in stochastic bandit environments, where the goal is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. The authors devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as, to learn and track the optimal proportion of arms draws, it relies on a single iteration of Frank-Wright algorithm applied to the lower-bound optimization problem. The proposed algorithm is applied to various pure exploration tasks, including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper proposes a Bayesian optimization (BO) approach for combinatorial spaces using expensive black-box function evaluations. The proposed approach is based on learning a latent space representation of structures using deep generative models (DGMs). The key idea is to define a structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representations for better surrogate modeling. The experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper studies the role of representation of state-action value functions in regret minimization in finite-horizon Markov Decision Processes (MDPs) with linear structure. The authors first derive a necessary condition on the representation, called universally spanning optimal features (UNISOFT), to achieve constant regret in any MDP with linear reward function. This condition encompasses the well-known settings of low-rank MDPs and, more generally, zero inherent Bellman error (also known as the Bellman closure assumption). They then demonstrate that this condition is also sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms (LSVI-UCB and ELEANOR). Finally, they propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper proposes a differentiable contact model, which can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. The authors demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper investigates the Benevolent training hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. This paper provides evidence for BTH by relating the NN’s Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. The authors first show that the Lipshitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. They then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity. Finally, they show that steady training with Dropout implies a training-and-datadependency generalization bound that grows poly-logarithmically with the number of parameters."
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper studies the problem of distribution independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. In particular, the authors show that any distribution can be decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. The main application of this result is to obtain the first polynomially-time algorithm for distribution-independent PAC learning with strongly-polynomial sample complexity. Specifically, they show how to adapt the algorithm of [DGT19], by appropriately transforming the set of points it is run on, to obtain a new algorithm with strong sample complexity guarantees."
SP:e5229305af00067ae2dbabd903e585964aec8928,"This paper proposes a Bayesian optimisation-based adversarial attack method for graph classification. The proposed method is black-box, query-efficient and parsimonious with respect to the perturbation applied. The authors empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. The paper also analyzes common interpretable patterns behind the adversarial samples produced, which may shed further light on adversarial robustness of graph learning models."
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of label shift in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to label shift scenarios."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper proposes a method for detecting and localization of gradual changes in the distribution of a sequence of time-ordered observations. The authors propose a general method that does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. The proposed method possesses proven theoretical guarantees for both detection and localization. "
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper proposes a biologically plausible algorithm for blind source separation (blind source separation) based on independent component analysis (ICA) neural networks. The main idea is to reformulate ICA as a min-max optimization problem with an intuitive geometric interpretation and solve it by online stochastic gradient optimization. The authors propose a novel objective function for ICA from which they derive biologically plausible neural networks, including both the neural architecture and the synaptic learning rules. The proposed algorithm relies on modulating synaptic plasticity by the total activity of the output neurons. In the brain, this could be accomplished by neuromodulators, extracellular calcium, local field potential, etc."
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper studies the space of solutions found by recurrent neural networks (RNNs) on three tasks: delayed discrimination, interval discrimination, and time-reconstruction. The authors show that RNNs trained on these tasks produce a rich set of solutions that are qualitatively different from the ones trained on the training set. They also show that the diversity revealed corresponds to different computations performed by the network. They further show that these solutions can be partially predicted by neural features."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper proposes a method for estimating the conditional density of any possible conditional distribution p(xu|xo) over a set of covariates over unobserved features xu and observed features xo. The proposed method, called Arbitrary Conditioning with Energy (ACE), is based on learning one-dimensional conditional densities over the set of unobserved variables xo and observed variables xu. The main idea is to use an energy function to specify the conditionals, which can be instantiated as any fully-normalized unnormalized distributions. The authors show that ACE achieves state-of-the-art performance for arbitrary conditional likelihood estimation and data imputation on standard benchmarks."
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new adaptive weighted loss for SISR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, it introduces variance estimation characterizing the uncertainty on a pixel-by-pixel basis into SIsR solutions so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, uncertainty estimation allows the authors to leverage conventional wisdom as sparsity prior for regularizing SisR solutions. For the first time, the authors demonstrate that uncertainty-driven loss can achieve better results than traditional loss functions without any increased computation during testing."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"This paper studies the problem of adversarial robustness, i.e., how much a model will be invariant to imperceptible perturbations in the input. The authors propose a general PAC-Bayesian framework to bound the averaged risk on the perturbation for majority votes (over the whole class of hypotheses). This theoretically founded analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e. the adversarial attacks), that are tight thanks to the PAC- Bayesian framework, and that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time."
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper proposes a probabilistic entity representation model (PERM) for logical reasoning over Knowledge Graphs (KGs). The proposed model encodes entities as a multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. It also defines the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. The authors also evaluate PERM’s competence on a COVID-19 drugrepurposing case study and show that it is able to recommend drugs with substantially better F1 than current methods."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"This paper proposes a gradient-based hyperparameter optimization method for long-horizon meta-learning. The proposed method is based on forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues and gradient degradation issues by sharing hyperparameters that are contiguous in time. The authors provide theoretical guarantees about the noise reduction properties of the algorithm, and demonstrate its efficiency empirically by differentiating through 10 gradient steps of unrolled optimization on CIFAR-10. "
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,This paper proposes a method to improve the robustness of neural sequence models by adding a symbolic reasoning module that can either accept or reject the generated generations. The proposed approach uses neural inference to mediate between the neural System 1 and the logical System 2. Experiments on the CRT and grounded instruction following show that this approach can increase the coherence and accuracy of neurally-based generations.
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper considers the problem of off-policy evaluation (OPE) in continuous treatment settings, where one aims to estimate the mean outcome under a new treatment decision rule using historical data generated by a different decision rule. The key ingredient of the proposed method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multiscale change point detection. This allows us to apply existing OPE methods in discrete treatments to handle continuous treatments. The method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing."
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a variational inference framework for continuous-time hybrid dynamical systems. The model is based on a Markov jump process modulating a subordinated diffusion process. The authors provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, the authors develop a new method to approximate the exact posteriors by minimizing the path-wise Kullback-Leibler divergence, which yields Bayesian latent state estimates for arbitrary points on the real axis and point estimates of unknown system parameters, utilizing variational expectation maximization. "
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the effect of the spectrum of the sensing matrices on the difficulty of recovering x from y in the nonlinear inverse problem. The authors define a notion for the spikiness of A and show the importance of this measure in the performance of the expectation propagation algorithm (EP). They define certain quantities based on the function f that enables them to describe the impact on EP recovery. They are able to show that for instance, in phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries."
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper proposes a method for generalized zero-shot learning (GZSL), which aims to recognize new categories with auxiliary semantic information, e.g., category attributes, by progressively improving cross-domain transferability and category discriminability of visual representations. The proposed method constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. With category prototypes, the proposed method further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts category discrimination. Experiments on four benchmarks prove the effectiveness of the proposed approach."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus blurring kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network (DNN) is developed by unrolling a fixed-point iteration of the GKM-based deblurring. Extensive experiments show that the proposed method achieves state-of-the-art performance."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper proposes a method for self-supervised video representation learning (SSVRL) that is storage- and computation-efficient and captures mutual information between two input streams. Specifically, the authors propose to directly decode RGB frames and motion vectors from compressed videos on-the-fly for storage and computation efficiency, and use a cross guidance contrastive learning algorithm based on multi-instance InfoNCE loss to enhance the representation ability of the motion vectors. Extensive experiments are carried out to validate the efficiency and effectiveness of MVCGC."
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"This paper studies the problem of Bayesian Bayesian neural networks (BNNs) with finite ReLU features. The authors show that the output variance of a BNN with finitely many features is quadratic in the distance from the data region. Meanwhile, Bayesian linear models with ReLU feature converge to a particular Gaussian process (GP) with a variance that grows cubically so that no asymptotic overconfidence can occur. This paper extends finite BNNs with infinite ReLU with infinite features via the GP and shows that the resulting model is maximally uncertain far away from the training data while the BNN’s predictive power is unaffected near the data. "
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of identifying the best causal effect estimator among a set of possible estimators. The authors propose to use the best-arm-identification bandit framework where the standard goal of learning the arm with the lowest loss is replaced with the goal to learn the arm that will produce the best estimate. They introduce new tools for constructing finite-sample confidence bounds on estimates of the asymptotic variance that account for the estimation of potentially complex nuisance functions, and adapt the best arm identification algorithms of LUCB and Successive Elimination to use these bounds. They validate their method by providing upper bounds on the sample complexity and an empirical study on artificially generated data."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper proposes ErrorCompensatedX, a compression method for variance reduced stochastic gradient descent (SGD) and momentum SGD (momentum SGD). Theoretical analysis shows that the proposed method achieves the same asymptotic convergence rate as without compression, and the convergence rate can be decomposed into the sum of two terms: (1) Runcompressed and (2) Run-compressed, which depends only on the compression rate of the original algorithm without compression and the magnitude of the compression error, and can easily attain the same convergence rate for any compressed algorithm in the form of (2). To the best of my knowledge, this is the first general result for error compensation."
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper proposes a generative probabilistic model, ReFine, to generate multi-grained explanations for graph neural networks (GNNs). Specifically, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of the explainer."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,This paper proposes a method to generate robust counterfactual explanations on Graph Neural Networks (GNNs) by explicitly modelling the common decision logic of GNNs on similar input graphs. The proposed method extracts decision boundaries from the given GNN model to formulate an intuitive and effective counter-factual loss function. The authors optimize this loss to train a neural network to produce explanations with strong counterfactually characteristics. Experiments on synthetic and real-life benchmark datasets demonstrate the superior performance of the proposed method.
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper proposes a method for voice style transfer (VST) based on self-supervised representation learning and adversarial feedback. In particular, the proposed method decomposes the content and style of the source speech into the content vector and the style vector, which is then used to train a discriminator. The discriminator is decomposed into content discriminator and style discriminator, which enables the model to achieve better generalization to the voice style of converted speech. The experimental results show the superiority of the proposed model in disentanglement and transfer performance, and improve audio quality. "
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,This paper proposes a Siamese voxel-to-BEV tracker for 3D single object tracking on sparse 3D point clouds. The proposed method consists of a shape-aware feature learning network and a target localization network. Shape-aware features learn the discriminative features of the object so that the potential target from the background in sparse point clouds can be identified. Target localization network regresses the target's 2D center and the z-axis center from the dense bird's eye view (BEV) feature map in an anchor-free manner.
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a positional encoding method based on learnable Fourier features for multi-dimensional position encoding. The proposed method learns a function to map a set of positions into a vector space, which is modulated with a multi-layer perceptron. The method is evaluated on image generation, object detection, image classification, and sparse spatial structure modeling. The results show that the proposed method outperforms existing methods."
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of learning the causal structure of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. This paper proposes a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of the approach is that at each iteration a specific type of variable is identified and removed. This allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests."
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,This paper introduces a batch Thompson Sampling framework for stochastic multi-arm bandit and linear contextual bandit problems. The main idea is to dynamically decide the duration of each batch in order to balance the exploration-exploitation trade-off. The authors prove that the proposed batch Thompson sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only O(log T) batch queries. They also demonstrate experimentally that dynamic batch allocation outperforms natural baselines.
SP:653a519e3c799c25e0d0b4240322642040b121a3,"This paper studies the problem of multiple source domain adaptation (MSDA) and domain generalization (DG) and characterizes two kinds of domain-invariant (DI) representations: general DI representation for learning invariant classifier and compressed DI representation motivated from reducing inter-domain representation discrepancy. The authors derive upper-bounds for the target general loss and characterize the properties of these representations, and develop a lower bound on the target loss which governs the trade-off between learning them. They conduct experiments on Colored MNIST dataset and real dataset to illustrate their theoretical claims."
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes an aligned structured sparsity learning (ASSL) strategy to train lightweight image super-resolution (SR) networks with smaller model size and lower computation than state-of-the-art methods. ASSL introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. To align the pruned filter locations across different layers, the authors propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. The authors conduct extensive comparisons with lightweight SR networks and show that ASSL achieves superior performance gains over recent methods quantitatively and visually."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper proposes a novel exploration method for cooperative multi-agent reinforcement learning (MARL). The proposed method is based on the observation that individual Q-values are the embeddings of local action-observation histories and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, the authors use prediction errors of individual Q values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. The method is evaluated on the StarCraft II micromanagement benchmark and achieves state-of-the-art performance."
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"This paper studies the problem of list-decodable linear regression in the presence of adversarial outliers, where a majority of the points are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining (1-alpha)-fraction of points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors that at least one of them is close to the target regression vector. The main result is a Statistical Query (SQ) lower bound of d for this problem, which qualitatively matches the performance of previous developed algorithms."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a method to incorporate expert domain knowledge into ML models for predicting the patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. The authors propose a latent hybridisation model (LHM) that integrates a system of expert-designed ODEs with machine-learned Neural ODE to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. They evaluated LHM on synthetic data and real-world intensive care data of COVID-19 patients."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper studies the problem of meta-learning in the setting where the available tasks require approximately the same representation. In this setting, the authors propose a theoretical framework for analyzing the sample complexity of fine-tuning using a MAML-like algorithm. They provide risk bounds on predictors found by finetuning via gradient descent, demonstrating that the method provably leverages the shared structure. They also establish settings where learning one representation for all tasks (i.e. using a “frozen representation” objective) fails."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of the approach is a collection of lexicon entries, which map each word to a syntactic type and a neuro-symbolic semantic program. A joint parsing and expected execution algorithm is introduced to facilitate learning in an exponentially-growing compositional space, which does local marginalization over derivations to reduce the training time. Experiments on two domains: visual reasoning and language-driven navigation show that the proposed approach can generalize from small amounts of data to novel compositions of words."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper proposes a distributed stochastic Newton algorithm for solving convex quasi-self-concordant convex optimization problems, where each machine has access to the same population objective and can calculate independent and Hessian-vector products. The authors show that the proposed algorithm can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving convergence guarantees and empirical evidence. The proposed algorithm is motivated by the observation of Woodworth et al. (2020a) that for quadratic objectives, first-order methods such as one-shot averaging can optimize the objective to a high degree of accuracy."
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes a new Chamfer distance (CD) based on Chamfer's distance (L2 distance) to measure the similarity between two point sets. The authors claim that the proposed DCD is stricter with detailed structures and significantly more computationally efficient than EMD and the bounded value range encourages a more stable and reasonable evaluation over the whole test set. They also propose a novel point discriminator module that estimates the priority for another guided downsampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper studies the problem of knowledge distillation, a popular technique for training a small student network to emulate a larger teacher model, as an ensemble of networks. The authors identify difficulties in optimization as a key reason for why the student is unable to match the teacher. They also show how details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalization."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper studies the problem of computing a (k, \epsilon)-coreset for a k-decision tree, which is a recursive partition of a 2D-signal into k parallel axis-parallel rectangles (axis parallel rectangles, leaves) where each axis parallel rectangle is assigned a real label. The size of the coreset is polynomial in k log(N)/ε, and its construction takes O(Nk) time. Experimental results on sklearn and lightGBM show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy."
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the problem of finding a subset of $m$ arms with the largest expected rewards under a fixed error rate $\delta$ (fixed-confidence Top-m identification), for misspecified linear bandit models. This problem is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms, but in which data inevitably deviates from linearity. In this work, the authors first derive a tractable lower bound on the sample complexity of any $d\epsilon$-correct algorithm for the general Top-$m$ identification problem. They then describe the first algorithm for this setting, which is both practical and adapts to the amount of misspecification. Finally, they evaluate their algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a method for learning disentangled graph representations with self-supervision. The authors first identify the latent factors of the input graph and derive its factorized representations. Then they propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method against several state-of-the-art baselines."
SP:0a7edbbdabab11273689c40c517001eb46491113,"This paper proposes a statistical simulation to make assessment on corruption robustness. It looks at this problem from a hypothesis testing (false positive/false negative) and from a certification (completeness / soundness) points of view. The procedure is scalable, efficient, complete and comes with guarantees on the lack of soundness."
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper proposes a framework for conditional data generation. The proposed framework, called CoPE, is a polynomial expansion of two input variables, i.e., a noise vector and a conditional variable. The authors show how SPADE and sBN can be considered as special cases of CoPE. CoPE can be trivially augmented to accept an arbitrary number of input variables. Inverse problems, such as super-resolution, can benefit from the proposed framework; the authors show that sampling different noise vectors results in plausible differences in the synthesized image."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper proposes a new neural network Maximum Mean Discrepancy (MMD) statistic by identifying a new connection between neural tangent kernel (NTK) and MMD. This connection enables the authors to develop a computationally efficient and memory-efficient approach to compute the MMD statistic and perform NTK based two-sample tests towards addressing the long-standing challenge of memory and computational complexity of MMD, which is essential for online implementation to assimilating new samples. Theoretically, a connection allows us to understand the NTK test statistic properties, such as the Type-I error and testing power for performing the two- sample test, by adapting existing theories for kernel MMD with NTK. Numerical experiments on synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed NTK-MMD statistic."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"This paper proposes a VAE+classifier structure “class-disentangled variational autoencoder (CD-VAE)” to decompose an input image x into x = G(x)-R(x), where R(x) captures the essential information for classification while G covers all the class-redundant information. The authors further propose an objective to jointly train the VAE and classifier to gain class disentanglement capability. The proposed approach is applied to both clean images and adversarial images and discovers that the perturbations generated by adversarial attacks mainly lie in the class dependent part x-G(x). The decomposition results also provide novel interpretations to classification and attack models. Inspired by these observations, the authors propose to conduct adversarial detection and defense respectively on x − G($x)$ and $G($x$)$, which outperform the results on the original x."
SP:2789874561620ba7894c4672f935056bb911e919,This paper proposes a differentially private federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) with distributed exploration (DE) to improve the utility of the FTS algorithm. The proposed algorithm is based on the general framework of differential privacy (DP) and integrates DP into FTS to preserve user-level privacy. The paper provides theoretical guarantees for both the privacy and utility trade-off. The empirical results show that the proposed algorithm achieves high utility (competitive performance) with a strong privacy guarantee (small privacy loss). 
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,This paper proposes a Gaussian process-Bayesian Bernoulli Mixture model (GP-BM) for multi-label active learning (ML-AL). The BM is integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. A novel auxiliary variable based variational inference algorithm is developed to tackle the non-conjugacy introduced along with the mapping process for efficient end-to-end posterior inference. A principled sampling function is designed accordingly to capture both the feature uncertainty (through GP) and label covariance (through BM) for effective data sampling. Experiments on both synthetic and real-world datasets demonstrate the state-of-the-art AL performance.
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"This paper proposes a streaming method for 3D object detection, lidar segmentation and panoptic segmentation. The proposed method uses a polar coordinate system to represent the point cloud sectors as wedge-shaped regions, which is more compact than previous methods which represent the sectors as rectangular regions. The authors also propose to increase the spatial context by using multi-scale padding from neighboring sectors and improve the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods."
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper studies the problem of learning with structured latent variables. The authors extend the Gumbel-Max trick to define distributions over structured domains by leveraging the score function estimators for optimization. In particular, they highlight a family of recursive algorithms with a common feature we call stochastic invariant. The feature allows us to construct reliable gradient estimates and control variates without additional constraints on the model. In the experiments, they consider various structured latent variable models and achieve results competitive with relaxation-based counterparts."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper proposes a method for adaptive denoising of convolutional neural networks (CNNs) based on a single multiplicative scaling parameter (the Gain) of each channel in the convolution layers of the CNN. The proposed method, GainTuning, allows CNNs pre-trained on large datasets to be adaptively and selectively adjusted for individual test images. The method is evaluated on standard image-denoising benchmarks, boosting their performance on nearly every image in a held-out test set. The adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type. "
SP:90afa1102683b456bc72a54abef466326827546a,This paper proposes a fully differentiable architecture for panoptic segmentation (a.k.a. semantic and instance segmentation) consisting of a convolutional neural network and an asymmetric multi-way cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptical labeling. The formulation allows to directly maximize a smooth surrogate of the panoptiﬁc quality metric by backpropagating the gradient through the optimization problem. Experiments on Cityscapes and COCO datasets show the effectiveness of the proposed method.
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes Recursive Bayesian Networks (RBNs), which generalizes and unifies PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. Two solutions: 1) generalize inside and outside probabilities from PCFG to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variable via gradient descent, while marginalising over network structures. 2) For Gaussian RBN, the authors additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. "
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a constrained backpropagation (CBP) algorithm that combines the pseudo-Lagrange multiplier (LMM) method and constraint optimization to obtain the optimal set of weights that satisfy a given set of constraints. The proposed algorithm is the utilization of a Lagrangian function (loss function plus constraint function) as its objective function. The authors considered various types of constraints — binary, ternary, one-bit shift, and two- bit shift weight constraints. As a post-training method, CBP applied to AlexNet, ResNet-18, Resnet-50, and GoogLeNet on ImageNet. "
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper studies the problem of active learning for Gaussian process classification (GPC). The authors propose efficient algorithms for EER-based active learning with GPC, which estimate the error reduction by querying instances based on the joint distribution of label pairs. They derive the joint predictive distribution as a one-dimensional integral with constant computation cost and calculate the predictive posterior based on it. They also derive the gradient chain rule to efficiently calculate the gradient of the acquisition function, which leads to the first query synthesis active learning algorithm. The experiments clearly demonstrate the computational efficiency of the proposed algorithms."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper studies the effect of unbounded gradients on the regularization of continuous variational autoencoders (VAE). In particular, the authors consider the case where the data lies on or near a low-dimensional manifold (e.g., natural images). They show that if the ultimate goal is to simultaneously avoid over-regularization (high reconstruction errors, sometimes referred to as posterior collapse) and under-regularisation (excessive latent dimensions are not pruned from the model), then an autoencoder-based energy function with infinite gradients around optimal representations is provably required per a certain technical sense which they carefully detail. This result suggests that heuristic modifications to or constraints on the VAE energy function may at times be ill-advised, and large gradients should be accommodated."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the min-max regret of the multi-armed bandit problem with graph feedback. The authors propose two notions, the fractional weak domination number and the k-packing independence number, which capture upper bound and lower bound for the regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual, and utilize the strong duality theorem to prove a general regret upper bound O(\sqrt{log}(V|V|) 1 3 T^2^3) and a lower bound $\Omega(\alpha)1 3 T^{3/\epsilon}/\alpha)$ where alpha is the integrality gap of the dual linear program. They also show that for several special families of graphs, they can get rid of the (log |V |) 1-3 factor and establish optimal regret."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper proposes a neighborhood SHAP method to improve the interpretability and robustness of Shapley values for black-box feature attributions. The proposed method is based on the formulation of neighbourhood reference distributions that improve the local interpretability of SHAP. The authors show that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, the authors observe that Neighbourhood SHAP identifies meaningful sparse feature relevance attributions that provide insight into local model behaviour."
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes a method to improve the data efficiency for deep reinforcement learning (RL) by augmenting cycle-consistent virtual trajectories (i.e., state-action sequences) for feature representation learning. The proposed method predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, the authors augment the actions to generate a large amount of virtual state-actions trajectories and enforce a trajectory to meet the cycle consistency constraint. Experimental results on the Atari and DMControl benchmarks demonstrate the effectiveness of the proposed method."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,This paper investigates how the network’s architecture impacts its robustness to noisy labels. The authors provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. They hypothesize that a network is more robust if its architecture is more aligned with the target function than the noise. They provide both theoretical and empirical evidence across various neural network architectures and different domains.
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes a data-driven approach to control, where examples of success states are used in place of a reward function. The method estimates the probability of reaching a success example in the future and optimizes a policy to maximize this probability of success. Unlike prior imitation learning methods, the approach is end-to-end and does not require learning a reward functions. Experiments show that the proposed approach outperforms prior methods that learn explicit rewards functions."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization in convex and non-convex settings. In the convex case, the authors focus on the family of non-smooth generalized linear losses (GLLs). The algorithm for the l2 setting achieves optimal excess population risk in near-linear time, while the best known algorithms for general convex losses run in super-linear running time. For the l1-case with smooth losses and polyhedral constraint, they provide the first nearly dimension independent rate, $\tilde{O}(\log d (n\epsilon)1/3)$. For the constrained l2-case, they obtain a linear-time algorithm with rate $\frac{1}{1/2} + d 1/5 (nε)2/5 )$. Finally, they extend all their results above for the non- convex l2 settings to the lp setting."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies the cooperative multi-agent bandit problem under three different imperfect communication settings: stochastic time-varying networks, instantaneous reward sharing over a network with random delays, adversarial networks with adversarial rewards, and byzantine communication. The authors propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well. In the setting with perfect communication, the authors also present an improved delayed-update algorithm that outperforms the existing state-of-the-art on various network topologies. Finally, they present tight network-dependent minimax lower bounds on the group regret."
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. The authors propose to search the optimal quantization interval for remaining the similarity between the quantized and original feature maps and introduce a ranking loss to keep the relative order of the attention values. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art quantization algorithms."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the convergence rate of double-Q-learning, a variant of Q-learning that uses two Q-estimators (one for each Q-function) to estimate the Q-values at each iteration. The main contribution of this paper is the analysis of the rate of convergence of the double-q-learning algorithm under the assumption of a constant learning rate, which improves upon the existing results of Xiong et al. (2020) by an order of magnitude in terms of the dependence on all the important factors. The analysis is based on two nested stochastic approximation recursions."
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper proposes a novel semi-supervised OOD detection setting, where the goal is to distinguish ID and OOD samples by using limited ID labeled data and large amounts of mixed unlabeled data. The authors propose a new technique, Structure-Keep Unzipping (Steps) approach, which learns a new representation space in which OOD data could be separated well, and an efficient optimization algorithm is derived to solve the objective. Comprehensive experiments on several benchmarks clearly show that the proposed approach outperforms other methods by a large margin and achieves remarkable detection performance."
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper proposes a transformer-based model for referring expression comprehension (REC) and segmentation (RES) tasks. The main idea is to fuse the visual-lingual encoder and the decoder in the transformer architecture, where the former is used to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. Experiments show that the proposed model outperforms state-of-the-art methods on both REC and RES tasks."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of multiclass boosting, where the number of classes $k$ is large. In this setting, the weak learner is assumed to belong to an easy-to-learn base class and is an agnostic PAC learner for that class with respect to the standard classification loss. The goal of the overall boosting algorithm is to learn a combination of weak hypotheses by repeatedly calling the weak learners. The authors study the resources required for boosting, especially how they depend on the number $k$, for both the booster and weak learners, and show that the boosting algorithm itself only requires $O(\log(k)$ samples, as they show by analyzing a variant of AdaBoost for our setting. They also prove a trade-off between number of weak-learner calls and the resources needed by a weak learners is at least polynomial in $k$."
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper proposes a new method for unsupervised object segmentation and object-centric scene generation. The proposed method is based on an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative refining. Experiments on the Sketchy and APC datasets show that the proposed method outperforms MONET-G."
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper studies the problem of online prediction sets in the online learning setting, where the data generating distribution is allowed to vary over time in an unknown fashion. The authors propose an adaptive approach that achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. The proposed approach is based on the idea of conformal inference, which is a general framework that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. The main contribution of this paper is the adaptive approach, which models the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. "
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"This paper proposes a Pose-level Inference Network (PINet) for multi-person pose estimation in crowded scenes. PINet first applies the Part-based Pose Generation (PPG) to infer multiple coarse poses for each person from his/her body parts. Those coarse poses are refined by the Pose Refinement module through incorporating pose priors, and finally are fused in the Pose Fusion module. Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet."
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,This paper studies the problem of solving robust Markov decision processes (RMDPs) with L_infty-constrained rectangular ambiguity sets. The authors propose a homotopy continuation method and a bisection method to solve RMDPs with S-rectangular ambiguity sets in quasi-linear time in the number of states and actions. The algorithm improves on the cubic time required by leading general linear programming methods. The algorithms also perform well in practice.
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,This paper studies the online knapsack problem with very weak predictions in the form of upper and lower bound for the number of items of each value. They systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction; they also extend the results to more general settings such as generalized one-way trading and two-stage online knapack. Their work shows that even seemingly weak predictions can be utilized effectively to improve the performance of known online algorithms for some known algorithms.
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes a new model-based episodic memory of trajectories to improve the sample efficiency of episodic control. The memory estimates trajectory values, guides the agent towards good policies, and is built upon the memory to construct a complementary learning model via a dynamic hybrid control model. Experiments demonstrate that the proposed model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non-Markovian settings."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"This paper proposes a data programming approach for semi-supervised learning (SSL) for unlabeled data. In particular, the authors propose a label model to generate probabilistic labels for unlabelled data. The label model is based on a multi-choice learning (MCL) approach to generate the initial labeling functions (LFs) in SSL style. The proposed method is evaluated on four standard SSL benchmarks and achieves better classification performance than existing SSL methods."
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,This paper proposes a multi-view multi-person 3D pose estimation method. The proposed method is based on a transformer-alike model architecture with a novel hierarchical joint query embedding scheme and projective attention mechanism. The paper also introduces a RayConv operation to integrate the view-dependent camera geometry into the feature representations. The experimental results show that the proposed method outperforms the state-of-the-art methods on several benchmarks.
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of recovering the support of unknown sparse vectors from a mixture of sign responses. In particular, the authors consider a generalization of the 1-bit compressed sensing problem, where the response to a query vector shows the sign of the inner product between a randomly chosen vector from the family and the query vector. The goal is to learn the supports of all the vectors in the family using a sequence of noisy responses. The main contribution of the paper is the existence of learning algorithms for the first problem which work without any assumptions. Under a mild structural assumption on the unknown vectors, under the assumption that the support matrix is resolvable, the paper also shows that there exists a learning algorithm for the second problem and analyze their query complexity."
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper studies the problem of bandit quickest changepoint detection (BQCD), which is a generalization of the classical quickest change detection (TNB14) problem. In this problem, the goal is to detect abrupt statistical changes in the online data stream with minimum detection delay. The authors derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. They then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. They derive expected delay bounds for the proposed scheme and show that these bounds match the lower bounds at low false alarm rates, establishing optimality of the proposed method."
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper considers the problem of stochastic nested optimization, which is a generalization of the non-nested problems. The authors propose an algorithm that unifies several SGD-type updates into a single SGD approach that they term ALternating Stochastic Gradient dEscenT (ALSET) method. They also provide a tighter analysis of ALSET for the nested problems. Under the new analysis, to achieve an-stationary point of the nested problem, it requires O(-2) samples in total."
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper proposes a Siamese sampling and reasoning approach for video question answering. The sampling mechanism is based on siamese samplers to generate sparse and similar clips from the same video, and a novel reasoning strategy is proposed to integrate the interdependent knowledge between contextual clips into the network. The reasoning strategy consists of two modules: (1) siamesese knowledge generation to learn the inter-relationship among clips; (2) siame knowledge reasoning to produce the refined soft label by propagating the weights of inter-relation to the predicted candidates of all clips."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a method to reduce the computational and memory complexity of a large class of structured models, including Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs), by viewing the central inference step as a matrix-vector product and using a low-rank constraint. Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that the proposed approach matches the accuracy of standard models at large state spaces while providing practical speedups."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper introduces Sample Average Uncertainty (SAU), a simple and efficient uncertainty measure for contextual bandits. SAU is a frequentist approach that directly estimates the uncertainty of the outcomes based on the value predictions. The authors show theoretically that the uncertainty measure estimated by SAU asymptotically matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. Because of its simplicity SAU can be seamlessly applied to deep contextual bandits as a very scalable drop-in replacement for epsilon-greedy exploration."
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper proposes a method to disentangle the dynamic behavioral factors (pose) from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder, and exploit the temporal structures of pose dynamics. The proposed method, called Disentangled Behavior Embedding (DBE), learns robust behavior embeddings from unlabeled, multi-view, high-resolution behavioral videos across different animals and multiple sessions. The authors further combine DBE with a stochastic temporal model to propose Variational Disentanglement Behavior Embeding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks such as fine-grained behavioral motif generation and behavior decoding."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,This paper proposes a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. The core of DMTET includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh.
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper proposes sliced mutual information (SMI) as a surrogate measure of dependence. SMI is defined as an average of MI terms between one-dimensional random projections. The authors show that it preserves many of the structural properties of classic MI, while gaining scalable computation and efficient estimation from samples. They also show that SMI can grow as a result of deterministic transformations. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) for both sequential and batch settings. The proposed method is based on a likelihood-ratio-based unbiased estimator of the gradient of the optimal acquisition function that does not use the reparameterization trick. In numerical experiments, the proposed method improves query efficiency by 2x or more over previous methods, and in some cases by 10x or even more."
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper proposes Multi-Dimensional Distributional DQN (MD3QN), a distributional RL method that learns a multi-dimensional joint return distribution for multiple sources of rewards. Specifically, the authors consider the source-specific returns from all sources of reward and capture its joint distribution to model the randomness of returns from different sources. The authors prove the convergence for the joint distributional Bellman operator and build their empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distributions and its Bellman target. The effectiveness of the proposed method is verified on pixel-input environments in terms of both the quality of modeled joint distribution and the final performance of learnt policies."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper proposes a method to generate high-resolution, accurate, and regular triangular meshes from volumetric images. The method is based on a flow-based neural network that predicts a dense 3D flow field from a 3D convolutional neural network with a modest GPU memory footprint. The paper also proposes a new Diffeomorphic Mesh Deformation (DMD) module, which is parameterized by a set of diffeomorphic mappings. The proposed method achieves state-of-the-art performance in the challenging brain cortical surface reconstruction task."
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper studies the problem of machine unlearning, i.e., when a user requests to delete data points from a trained model, the goal is to remove the influence of deleted data from the model at a lower computational cost than retraining the model. In this paper, the authors propose a general reduction from deletion guarantees against adaptive sequences to deletion guarantees for non-adaptive sequences, using differential privacy and its connection to max information. This leads to extremely flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletions guarantees for adaptive deletion sequences. The authors also show in theory how prior work in the non-convex setting fails against adaptive deletions."
SP:7150006590e268ab732c9be6c9048f67a377f956,"This paper studies risk-averse Bayes-adaptive reinforcement learning in Bayesian Markov decision processes (MDPs). In particular, the authors propose an algorithm that optimizes the conditional value at risk (CVaR) of the total return of the MDP given the prior distribution over MDPs. They reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Their experiments demonstrate that their approach significantly outperforms baseline approaches for this problem."
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model."
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,This paper proposes a method to detect coordinated groups on social media based on neural temporal point processes (NTPs) and prior knowledge. The authors propose a variational inference approach to learn a mean-field approximation for the distribution of the Gibbs distribution of group assignment. The proposed method is evaluated on the IRA and COVID-19 Vaccine Tweets datasets. The results show that the proposed method outperforms the existing methods.
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper studies the problem of binary classification on disjoint one-dimensional smooth manifolds with low-dimensional structure, where the goal is to classify data drawn from two smooth curves on the unit sphere. The authors prove that when the network depth is large relative to certain geometric properties that set the difficulty of the problem, and the network width and number of samples are polynomial in the depth, randomly-initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. This is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. "
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes two methods to improve the performance of classifier-based conditional adversarial networks (cGANs). First, the authors identify that gradient exploding in the classifier can cause an undesirable collapse in early training, and projecting input vectors onto a unit hypersphere can resolve the problem. Second, they propose the Data-to-Data Cross-Entropy loss (D2D-CE) to exploit relational information in the dataset. The experimental results show that ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets."
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper proposes an extension of the policy space response oracles (PSRO) algorithm for two-player zero-sum games. The proposed method, Extensive-Form Double Oracle (XDO), is an extensive-form double oracle algorithm that is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. Unlike PSRO, XDO mixes best responses at every infostate. The authors also introduce Neural XDO (NXDO) where the best response is learned through deep RL. In tabular experiments on Leduc poker, the authors show that XDO achieves an approximately Nash equilibrium in a number of iterations an order of magnitude smaller than PSRO."
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper proposes a permutation-invariant variational autoencoder for graph structured data. The proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. The authors demonstrate the effectiveness of the proposed model for graph reconstruction, generation and interpolation."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"This paper proposes to decouple the depth and scope of GNNs by first extracting a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. Theoretically, it shows that the proposed approach improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, the design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"This paper studies the universal approximation properties of affine-coupling normalizing flows. The authors show that any log-concave distribution can be approximated using well-conditioned affine coupling flows. They also show the connection between affine couplings, underdamped Langevin dynamics, and Hénon maps. "
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper studies the problem of coupon allocation in an online e-commerce market, where the goal is to allocate users within a fixed budget while maximizing the users’ retention on the platform. The authors propose a budget constrained offline reinforcement learning and evaluation with λ-generalization (BCORLE(lambda)) framework. The proposed method can help enterprises develop a coupons allocation policy which greatly improves users' retention rate while ensuring the cost does not exceed the budget. Experimental results show the effectiveness of the proposed method."
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a method for source-free domain adaptation (SFDA) where the source pretrained model is adapted to the target domain in the absence of source data. The method is based on the observation that target data might no longer align with the source domain classifier, but it still forms clear clusters. The authors propose to capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. The proposed method achieves state-of-the-art performance on several 2D image and 3D point cloud recognition datasets."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a method for learning embeddings for set-structured data that are permutation-invariant and geometrically-interpretable. The key idea is to treat the elements of a set as samples from a probability distribution and use an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set data effectively. The proposed method is evaluated on a variety of set-based data, including point cloud, graph, and image classification tasks, and shows superior performance over existing set representation learning approaches."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper proposes a new family of RNNs, called SBO-RNN, that can be formulated using stochastic bilevel optimization (SBO), where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively. The authors prove that under mild conditions there is no vanishing/exploding gradient in training the proposed RNN, and thus the training is easy and stabilized. Empirically, the proposed method achieves superior performance on several benchmark datasets, with fewer parameters, less training data and faster convergence."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"This paper studies the online problem of minimizing power consumption in systems with multiple power-saving states. During idle periods of unknown lengths, an algorithm has to choose between power- saving states of different energy consumption and wake-up costs. The authors develop a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods. The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem."
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper proposes a transferability measure for multi-source transfer learning problems, which is based on sample sizes, model complexity, and similarity between source and target tasks. Theoretical results are provided to show that the transferability of a particular source task is proportional to the number of samples and the measure of similarity to the target task, and is inversely proportional to model complexity. In addition, the authors also provide a quantifiable measure by exploiting a parameterized model. Finally, they develop an alternating iterative algorithm to implement their theoretical results for training deep neural networks in multi- source transfer learning tasks."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"This paper proposes an image-computable model (eccNET) to shed light on the fundamental inductive biases inherent to neural computations during visual search. The proposed model combines eccentricity-dependent sampling, and top-down modulation through target-dependent attention. The model spontaneously revealed visual search asymmetry and qualitatively matched the polarity of human behavior. This study highlights how classical perceptual properties can emerge in neural network models, without the need for task-specific training."
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"Certifiable training is the problem of training a neural network to be robust to adversarial perturbations. The goal is to minimize the worst-case loss over the allowed perturbation, i.e. the upper bound on the worst case loss. In this paper, the authors study the performance of different linear relaxation-based methods for certifiable training. They show that the smoothness of the loss landscape is an important factor that influences in building certifiably robust models. Based on this observation, they propose a new method with tighter bounds and a new loss landscape that has a favorable loss landscape. The proposed method achieves a decent performance under a wide range of adversarial examples."
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper considers the problem of online linear regression in the stochastic setting. The authors derive high probability regret bounds for online ridge regression and the forward algorithm. They advocate for the use of forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, they explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds."
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper proposes a two-time-scale and anchored extragradient method for smooth structured nonconvex-nonconcave minimax problems. The proposed FEG has an accelerated O(1/k^2) rate, with respect to the squared gradient norm, for the Lipschitz continuous and negative comonotone operators for the first time. The FEG also has value for smooth convex-concavity problems, compared to existing works. The paper further develops a backtracking line-search version, named FEG-A, for which the problem parameters are not available."
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies the problem of uniformity testing for statistical data that consists of rankings over m items, where the alternative class is restricted to Mallows models. The authors show that uniform distribution can be distinguished from Mallows model with O(m 1/2) samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if m is large enough. They also consider uniformity tests with central and local differential privacy (DP) constraints. "
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"This paper proposes a general greedy score-based algorithm for learning directed acyclic graphs (DAGs). Unlike edge-greedy algorithms such as the popular GES and hill-climbing algorithms, the proposed algorithm greedily adds vertices and requires at most a polynomial number of score evaluations. Theoretical analysis shows that the proposed greedy algorithm is a special case of recent polynomials-time algorithms for learning DAGs. This observation suggests new score functions and optimality conditions based on the duality between Bregman divergences and exponential families, which are explored in detail."
SP:b60989706296b963b6671c01f22384978a334be1,This paper studies the trade-off between standard accuracy and adversarial robustness of convolutional neural networks (CNNs). The authors propose to dilate the architecture of CNNs to increase the robustness while maintaining a competitive standard accuracy with a straightforward constraint. Experimental results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance accuracy and robustness.
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). The authors propose a new provably efficient algorithm, called UCRL-RFE, under the linear mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. They show that to obtain an $\epsilon$-optimal policy for arbitrary reward function, UCRLRFE needs to sample at most $\tilde{O}(\frac{H}{5d^2}{2\eps^{-2}})$ episodes during the exploration phase. They also propose a variant of UCRL_RFE using Bernstein-type bonus and show that it needs to samples at most $O(Hd^{d(H + d)^2}$ to achieve an $\eps^2$ optimal policy. The upper bound matches the lower bound."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a method for forecasting future events with seasonal patterns that evolve over time. The proposed method, Shifting Seasonal Matrix Factorization (SSMF), can adaptively learn multiple seasonal patterns (called regimes), as well as switching between them. The method has the following properties: (a) it accurately forecasts future events by detecting regime shifts in seasonal patterns as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. The authors demonstrate that the proposed method outperforms state-of-the-art baseline methods on three real-world data streams."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper proposes a novel neural network architecture for solving assignment problems. The core module, feature weaving layer, is stacked to model frequent communication between elements in a parameter-efficient way for solving the combinatorial problem of assignment. The experimental results showed its impressive performance among the learning-based baselines."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper systematically studies the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, they study MLP-based (PointNet), convolution-based(DGCNN), and transformer-based 3D architectures. Through extensive experimentation, they demonstrate that appropriate applications of self supervision can significantly enhance the robustness. The analysis reveals that local feature learning is desirable for adversarial robustness in point clouds since it limits the adversarial propagation between the point-level input perturbations and the model's final output. This insight also explains the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial attacks."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,This paper considers the problem of computing iterative projections of close-by points over submodular base polytopes. The authors propose a toolkit to speed up the computation of projections using both discrete and continuous perspectives. They also adapt the away-step Frank-Wolfe algorithm to use this information and enable early termination. Theoretical results show orders of magnitude reduction in runtime in preliminary computational experiments.
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,This paper considers the problem of learning the natural parameters of a $k$-parameter exponential family from i.i.d. samples in a computationally and statistically efficient manner. The authors propose an estimator that is consistent as well as asymptotically normal under mild conditions. They provide finite sample guarantees to achieve an $\alpha$-approximation to the true natural parameters with $O(k/\alpha)$ samples and $O(\sqrt(k))$ computations. They also provide an interpretation of their estimator in terms of a maximum likelihood estimation.
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a hybrid differentiable renderer that combines rasterization and ray-tracing, taking advantage of their respective strengths—speed and realism. The proposed renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. Compared to more advanced physics-based differentiable. renderers leveraging path tracing, the proposed DIB-R++ is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a differentiable training method that imposes implicit constraints to the shape of the probability map by minimizing the expectation of the localization error. To approximate the expectation, the authors introduce a continuous formulation of the output distribution and develop a sampling process. The expectation can be approximated by calculating the average error of all samples drawn from the target distribution. Sampling-argmax is shown to be effective and flexible by conducting comprehensive experiments on various localization tasks."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"This paper proposes a directed graph data augmentation method called Laplacian perturbation to generate contrastive views for graph contrastive learning (GCL). The proposed method, called DiGCL, dynamically learns from all possible contrastive view generated by Laplace perturbations. The authors train it using multi-task curriculum learning to progressively learn from multiple easy-to-difficult contrastives views. Experiments on various benchmarks reveal the dominance over the state-of-the-art approaches."
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes a new benchmark for evaluating language grounded agents across unique challenges posed by five symbolic interactive environments. The authors propose the first shared architecture and analyzed recent methodological advancements in grounded language learning across on these environments. They showed that a shared architecture achieves comparable result to environment-specific methods, and that most advances do not result in significant gains on environments other than the one they were designed for."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper proposes a sparse version of the Vision Transformer (ViT) that is scalable and competitive with the largest dense networks. The paper also proposes a batch-prioriated routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. Finally, the paper demonstrates the potential of V-MoE to scale vision models and train a 15B parameter model that attains 90.35% on ImageNet."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper studies the expressivity and generalization properties of training narrow neural networks. In particular, it shows that as long as the width $m$ > 2^n/d, there exists at least one global minimizer with zero training loss, and identifies a nice local region with no local-min or saddle points. It also shows that every KKT point is a nearly-global minimizer. Finally, the paper shows that projected gradient methods on this constrained formulation outperform SGD for training narrow networks."
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper proposes a continuous mean-covariance bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent’s objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, the authors consider three feedback settings, i.e., full-information, semi-bandit and full bandit feedback. They propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of the algorithms."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper considers the problem of computing Positive Semidefinite (PSD) matrix factorization of a matrix X, which is a collection of r-dimensional PSD matrices {Ai} and {Bj} satisfying the condition Xij = tr(AiBj) for all i and j in [m,n]. The PSD factorization task generalizes the Nonnegative Matrix Factorization (NMF) problem in which the goal is to find an approximate PSD matrix that minimizes the square loss over all entries of X. The most widely used algorithm for computing NMFs is the Multiplicative Update algorithm developed by Lee and Seung, in which non-negativity of the updates is preserved by scaling with positive diagonal matrices. In this paper, the authors describe a non-commutative extension of Lee-Seung’s algorithm, which they call the Matrix Multiplyicative Update (MMU) algorithm, for computing PSD factors. The proposed algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSd matrices and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys. The authors demonstrate the utility of the proposed method with experiments on real and synthetic data."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes a meta-learning framework for domain generalization (DG) that disentangles features in the latent space while jointly learning both domain-invariant and domainspecific features in a unified framework. The domain-specific representation is optimized through the meta learning framework to adapt from source domains, targeting a robust generalization on unseen domains. The paper provides the first theoretical analysis to understand and realize the efficiency of domain specific information in domain generalisation. The experimental results demonstrate mDSDI brings out competitive results with state-of-the-art techniques in DG."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"This paper proposes a method to improve the sample quality of diffusion-based likelihood-based generative models (Diffusion-Diffusion Generative Models) by using a series of ablations to find a better architecture and by using classifier guidance to trade off diversity for fidelity using gradients from a classifier. The proposed method achieves FID of 2.97 on ImageNet and 4.59 on the ImageNet 256-256 and 512-512, and matches BigGAN-deep in terms of wall-clock time. The authors also show that the proposed method works well with upsampling."
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes to leverage out-of-distribution samples for improving few-shot learning. Specifically, the proposed method maximizes the distance from prototypes to the out of distribution samples (i.e., unlabeled samples coming from outside target classes) while minimizing the distance to the in- distribution samples. The proposed approach is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed approach consistently improves the performance of pretrained networks with different architectures."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper studies the problem of prioritized sampling in off-policy reinforcement learning. The authors propose two algorithms, ReMERN and ReMERT, to compute the prioritization weights for the Bellman update. Theoretical analysis is provided to support the proposed algorithms. Empirical results on MuJoCo and Mujoco benchmarks show that the proposed methods outperform the baselines. "
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,This paper considers the problem of sequential prediction with expert advice in a nonstationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. The authors give a linear-time algorithm that improves on the best known regret bounds [27]. This algorithm incorporates a relative entropy projection step. This projection is advantageous over previous weight-sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization.
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper considers a variant of contextual linear bandits motivated by routing applications in navigational engines and recommendation systems. In this problem, the learner wishes to learn a hidden d-dimensional value w* in R^d, and every round, we are presented with a subset Xt \in R of possible actions. If we choose (i.e., recommend to the user) action xt, we obtain utility w* but only learn the identity of the best action arg maxx \in Xt. The authors design algorithms for this problem which achieve regret O(d log T) and exp(O(dlog d)). The authors also consider the variant where we are allowed to provide a list of several recommendations. In that case, they give an algorithm with O(\d log d) regret and list size poly(d)."
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper introduces a small set of orthogonal combinators for composing machine learning operators into pipelines. It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers. It presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper proposes a meta-learning method that learns to select the weights to change in the inner-loop learning process, i.e., where to change the weights and how to change them. The proposed method is based on the idea of sparse gradient descent (SGD) and the authors show that it induces a patterned sparsity of the gradient of the weights. The authors also show that the proposed method can be used for online learning.  "
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"This paper proposes a unified approach to group independent component analysis (ICA) for multi-view learning. The proposed approach models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. The authors show that this model is identifiable if the components are either non-Gaussian or have enough diversity in noise variances. They also show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset CCA fail. To solve this problem, the authors propose to use joint diagonalization after Multisets CCA, leading to a new approach called ShICA-J, which is based on second-order statistics. They further propose a maximum-likelihood method, ShICAML, that is both more accurate and more costly. Finally, they provide empirical evidence on fMRI and MEG datasets."
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper proposes a model-free reinforcement learning approach to train agents that can collaborate well with human partners without using human data. The authors argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, the authors propose to train the agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method they call Fictitious Co-Play (FCP). The experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. They find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes a new multi-agent actor-critic method that learns decentralised policies with a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX. The authors also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, the centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper proposes a biologically plausible key-value memory network that stores inputs using a combination of biologically plausible three-factor plasticity rules. The proposed network performs on par with classical Hopfield networks on auto-associative memory tasks and can be naturally extended to continual recall, hetero-associationative memory, and sequence learning. The results suggest a compelling alternative to the classical hopfield network as a model of biological long-term memory."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper studies the problem of differentially private online gradient descent (OGD) algorithms for pairwise learning. In particular, the authors propose simple stochastic and online algorithms for the problem. The main contributions of this paper are:  1. The authors propose a new algorithm that is based on the iterative localization technique for pointwise learning (Algorithm 1).  2. Theoretical stability results, optimization, and generalization error bounds for both convex and nonconvex problems are provided.  3.  The authors also extend their algorithms and stability analysis to develop differential private SGD algorithms for couplewise learning, which significantly improves the existing results. "
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper presents REDO, a framework to reconstruct the shape and dynamics of dynamic objects from videos. The authors propose a canonical 4D implicit function that is pixel-aligned with aggregated temporal visual cues and a 4D transformation module that captures object dynamics to support temporal propagation and aggregation. The proposed method is evaluated on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++ and real-world video data 3DPW."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. The authors derive high-probability bounds on the performance of LSA under weaker conditions on the sequence {(An,bn) : n 2 N⇤}. However, in contrast, they establish polynomial concentration bounds with order depending on the stepsizes. "
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper extends the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. The contributions include general convergent off-policy learning algorithms, intra-option algorithms for learning values and models, as well as sample-based planning variants of our learning algorithms. The experiments on a continuing version of the classic Four-Room domain demonstrate the efficacy of the proposed algorithms."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper proposes a self-supervised auxiliary task to improve the performance of visual transformers (VTs). Specifically, the authors propose a localization task, inspired by [12], which is densely defined for a random subset of final token embedding pairs, and it encourages the VT to learn spatial information. Experiments show that the proposed method can improve the final accuracy of the VTs."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"This paper proposes hyperbolic Procrustes analysis (HPA), a new method for label-free alignment of hierarchical data in the hyperboloid model of the Lorentz model. HPA consists of three components: translation, scaling, and rotation, which align the first and second Riemannian moments, and a wrapped rotation that aligns the orientation in the model. The authors analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. "
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper studies the problem of differentially private query answering systems that are not required to produce micro-data. The authors show that there is a trade-off between accuracy for a population of interest (sum query) vs. accuracy for its component sub-populations (point queries). They show lower bounds for pure, approximate, and concentrated differential privacy and propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"This paper proposes a framework for learning to plan and goal-conditioned reinforcement learning (RL) in long-horizon tasks. In particular, the authors propose a curriculum of tree-structured sub-tasks in which the planner learns to decompose a task into a few subtasks at first and then gradually increases the number of subtasks. The RL agent is then trained to learn from the sub-task sequences. The authors show that the proposed framework can improve the sample efficiency and success rate on navigation and continuous control tasks. "
SP:9911693a04a300b5a93634fb0267ef83e5489d77,"This paper proposes a Bayesian framework for generating local explanations for LIME and SHAP. The main idea is to use Bayesian methods to estimate the quality of the local explanations and the uncertainty associated with them. The authors also propose a new sampling technique to estimate how many perturbations to sample, and how to sample them for faster convergence. Experiments on several real world datasets and user studies demonstrate the efficacy of the proposed framework. "
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper proposes a method to improve the performance of Adder neural networks (ANNs) in classification tasks. The authors claim that the performance gap between ANNs and CNNs lies in the similarity measurement between filters and features, however how to alleviate this difference remains unexplored. They propose to pre-define ANN features to follow a mixture of Multivariate Skew Laplace distributions, with which the heavy tails in ANNs can be better controlled with high order moment skewness. They introduce an angle-based constraint on distribution parameters to incorporate high diversity of distribution tails in angle space so that the overlapping can be eliminated. They conduct experiments on several models and datasets where the proposed SLAC-ANN consistently achieves superior performance."
SP:cbccb65457564992d534504c0d060da44cafce8c,"This paper studies the phenomenon of Gradient Starvation (GS), a phenomenon that arises when training with cross-entropy loss in neural networks. The authors identify and formalize a fundamental gradient descent phenomenon leading to a learning proclivity in over-parameterized neural networks, i.e., gradient starvation. They show that GS can slow down the learning of certain features, even if they are present in the training set. They also derive spectral decoupling (SD) regularization as a possible remedy to GS."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper performs a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, the authors also quantify subjective measures of the human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. They find that humans have a clear preference toward a rule based AI teammate (SmartBot) over a learning based teammate (Other-Play) across nearly all subjective metrics, and generally view the learning based agent negatively. This result has implications for future AI design and reinforcement learning benchmarking."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper proposes a method for visual question generation (VQG) based on double visual and answer hints. The key idea is that the salient visual regions of interest can be viewed as a constraint to improve the generation procedure for producing high-quality questions. Given the predicted salient visual region of interest, the model can focus on estimating the probability of being ground-truth questions, which in turn implicitly measures the quality of predicted visual hints. Experimental results on two benchmark datasets show that the proposed method outperforms the state-of-the-art approaches."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes Generalized Data Weighting (GDW) to mitigate label noise and class imbalance by manipulating gradients at the class level. To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. Extensive experiments in various settings verify the effectiveness of GDW."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper proposes a method to learn spatio-temporal language grounding for embodied agents. Specifically, the goal is to learn a truth function that predicts if a given sentence is true of temporally-extended observations of an agent interacting with a collection of objects. The model learns to predict if a description matches a given history of observations. Experiments show that maintaining object identity in the attention computation of the transformer-based models is instrumental to achieving good performance on generalization overall."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,This paper proposes a method for multiple object tracking and segmentation (MOTS) in videos. The proposed method first distills the space-time memory into a set of prototypes and then employs prototypical cross-attention to retrieve rich information from the past frames. Experiments on Youtube-VIS and BDD100K datasets demonstrate the effectiveness of the proposed method.
SP:1175ad16382b349ab1a39895150172d266abe571,"This paper studies the question of whether gradient descent (and gradient flow) can be seen as a solution to the initial value problem of gradient flow. The authors show that the degree of approximation depends on the curvature around the gradient flow trajectory. They then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. Finally, they show that gradient descent with conventional step size is indeed close to gradient flow over simple neural networks."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers a stochastic multi-armed bandit problem with delayed impact of actions. In this setting, actions taken in the past impact the arm rewards in the subsequent future. The authors generalize the bandit setting to encode the dependency of this “bias” due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. They propose an algorithm that achieves a regret of $\tilde{O}(\KT^2/3)$ and show a matching regret lower bound of $\Omega(\KT^{-1/3})$ where K is the number of arms and T is the learning horizon."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,This paper proposes an end-to-end solution for video instance segmentation (VIS) based on transformers. The authors propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. The proposed method achieves state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set) while having a considerably fast runtime (89.4 FPS).
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a general graph embedding method that can debias various structural biases in graphs by using random graphs. In particular, the authors show that random walks are biased by the degree of each node, where a node is sampled proportionally to its degree. To mitigate this bias, they propose to use skip-gram negative sampling (SGNS), which happens to negate the bias due to the friendship paradox. They also propose a more general framework, residual2vec, that can also compensate for other systematic biases in random walks. They show that the proposed method performs better than conventional embedding methods in link prediction and community detection tasks."
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies the problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy, where the term ‘local’ means that each zi is produced using one individual attribute xi. The authors study two settings: sequentially interactive (i.e. they are allowed to use already published confidential data) or non-interactive. They describe the behavior of the quadratic risk for estimating the power sum functional F(\gamma) = \sum_{k=1} p\gamma^k, \gamma > 0 as a function of K, n and alpha. They also provide lower bounds over all α-LDP mechanisms and all estimators using the private samples."
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper studies the problem of online multiclass classification in the setting where the learner’s feedback is determined by an arbitrary directed graph. In this setting, feedback graphs allow a much richer set of applications, including filtering and label efficient classification. The authors introduce GAPPLETRON, a new algorithm that works with arbitrary feedback graphs and prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. Experiments on synthetic data show that the proposed algorithm is competitive with known baselines."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of explainable clustering in the setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). A k-clustering is said to be explainable if it is given by a decision tree where each internal node splits data points with a threshold cut in a single dimension (feature), and each of the k leaves corresponds to a cluster. The authors give an algorithm that outputs an explainable cluster that loses at most a factor of $O(\log k)$ compared to an optimal (not necessarily explainable) clustering for the k-medians objective, and a factor $O(k log k)$. This improves over the previous best upper bounds of O(k) and O(\k^2) respectively, and nearly matches the previous $\Omega(\k)$ lower bound for k-means. The algorithm is remarkably simple. It is oblivious to the data points and runs in time O(dk log k)."
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,This paper proposes a multilingual language model (PrLM) that supports both explicit universal dependency parsing and implicit language modeling. The universal dependency parses (UD) are used as the universal linguistic characteristics in the multilingual PrLM. The proposed multilingual-BERT and XLM-R are evaluated on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets. The results show that the proposed model outperforms the two existing multilingual BERT models in all tasks and achieves state-of-the-art results on syntactic parsing.
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper proposes a transformer-based model for solving vehicle routing problems (VRPs). The proposed model learns embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. Moreover, the positional features are embedded through a novel cyclic positional encoding (CPE) method to allow Transformer to capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). The model is trained using Proximal Policy Optimization and curriculum learning strategy for better sample efficiency. Experiments on the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) show that the proposed model outperforms existing Transformer-based improvement models and exhibits much better generalization performance across different problem sizes."
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"This paper proposes a method to compute the exact Bayes error of generative models learned using normalizing flows. The technique relies on a fundamental result, which states that the Bayesian error is invariant under invertible transformation. The authors also show that by varying the temperature of the learned flow models, they can generate synthetic datasets that closely resemble standard benchmark datasets, but with exact controlled Bayes errors. This allows them to evaluate the performance of trained models on an absolute, rather than relative, scale."
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper proposes a gradient-based initialization scheme for neural network parameters. The authors propose to re-scale each random weight tensor (e.g. convolution kernels) directly by a learned scalar coefficient. This small set of coefficients is optimized to make the first step of a stochastic optimizer (eg. SGD or Adam) as effective as possible at minimizing the training loss, while preventing the initial gradient norm exploding. The initialization learned by GradInit often decreases the gradient variance for most of the parameter blocks."
SP:f69731403592fa5bdd4ca327708582d615aa131c,"This paper proposes a new approach for disease progression modeling based on Riemannian metric learning for longitudinal data. The main idea is to learn the push-forward of the Euclidean metric by a diffeomorphism, which is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The proposed approach allows us to disentangle time and space variability while learning the inter-patient variability and the average trajectory from the data. Experiments on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset show that the proposed approach can improve the forecasting of imaging and clinical biomarkers."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, parallel Procedural Units (PUs) are introduced, which consists of a memory head and a procedure. For an intermediate feature, we search its closest memory and forward it to the corresponding procedure in both training and testing. Networks with the proposed mechanism can be trained efficiently using a four-step training strategy."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper considers equivariant polynomial functions, i.e. functions that are universal approximations of functions that can be expressed in terms of a collection of scalar products and scalar contractions of the scalar, vector, and tensor inputs. In particular, the authors show that the space of universal approximating functions can be constructed from a subset of a scalar collection of the input vectors and input scalar contracts. The scalar-based method is simple, efficient, and scalable. Numerical experiments are provided to support the theory."
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper proposes a generalization of the Intersection over Union (IoU) loss for bbox regression to a new family of power IoU-based losses. The proposed loss function, called alpha-IoUs, combines the IoU loss with a box transformation term and a regularization term. The authors analyze the order preservingness and loss/gradient reweighting properties of the proposed loss and show that it can improve the bbox accuracy through up-weighting the loss and gradient of high IoU objects. "
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper studies Distributionally Robust Imitation Learning (DROIL) and establishes a close connection between DROIL and Maximum Entropy Inverse Reinforcement Learning (MaxEnt). The authors show that DROIL can be seen as a framework that maximizes a generalized concept of entropy. They develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. Their approach lets them optimize both stationary and non-stationary policies and, unlike prevalent previous methods, it does not require solving an inner reinforcement learning problem. They experimentally show the significant benefits of DROIL’s new optimization method on synthetic data and a highway driving environment."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper proposes general post-processing algorithms for individual fairness (IF) in the setting where the learner only has access to the predictions of the original model and a similarity graph between individuals guiding the desired fairness constraints. The authors cast the individual fairness problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired “treat similar individuals similarly” interpretation. Theoretical results demonstrate the connection of the new objective function to a local relaxation of original individual fairness. Empirically, the proposed algorithms correct individual biases in large-scale NLP models such as BERT while preserving accuracy."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a question-schema linking method for the cross-domain Text-to-SQL task. The authors propose a structure-aware Dual Graph Aggregation Network (SADGA) to learn the mapping between the question-graph and the database schema-graph. SADGA adopts the graph structure to provide a unified encoding model for both the natural language question and database schema. The proposed method is featured with Global Graph Linking, Local Graph linking, DualGraph Aggregation Mechanism."
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of learning end-to-end learnable discrete-continuous models, i.e. models that combine discrete and continuous components in the computation graph. The authors analyze the behavior of learning stochastic computations graphs with multiple sequential discrete components. They show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. They propose two strategies to overcome these challenges. First, they show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, they propose dropout residual connections specifically tailored to stochastically-discrete continuous computation graphs."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"This paper studies the performance of Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo (HMC) for generalization under covariate shift. The authors show that BNNs with Bayesian model averaging (BMA) and Bayesian maximum a-posteriori (BPA) can be problematic for OOD generalization, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. They also show that the same issue does not affect approximate inference procedures or classical maximum a posteriori (MAP) training. Finally, the authors propose novel priors that improve the robustness of BNN models to covariate shifts."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper categorizes meta few-shot learning evaluation into two settings: in-distribution (ID) and out of distribution (OOD). It identifies why common FSL benchmarks reflect OOD evaluation, identifies realistic needs for ID FSL evaluation and provides new benchmarks as well as suggestions on how to modify existing OOD FSL benchmark to allow for ID evaluation. It also highlights concerns in 1) reliably performing model selection for a given meta-learning method, and 2) consistently comparing the performance of different methods."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper studies the problem of open rule induction, i.e., how to generate rules that generalize to more open and complex real-world rules. The authors argue that the current LM-based rule generation methods are “learning rules from rules”, which limits these methods to only produce “canned” rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. Therefore, the authors propose to induce open rules utilizing the knowledge in LMs. Besides, they propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision of annotated rule. "
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,This paper proposes Implicit Constraint Q-learning (ICQ) for multi-agent offline reinforcement learning. The main idea of ICQ is to only trust the state-action pairs given in the dataset for value estimation. The authors also extend ICQ to multi-Agent tasks by decomposing the joint-policy under the implicit constraint. Experimental results demonstrate that the extrapolation error is successfully controlled within a reasonable range and insensitive to the number of agents.
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper studies adversarial robustness against evasion attacks, with a focus on applications where input features have to comply with certain domain constraints. The authors propose a methodology to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. They propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. The results on three different applications demonstrate that non-uncertain perturbation sets in AT improve adversarial perturbance sets, and non-unsual bounds provide better robustness certification. "
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper extends the theory of Tikhonov regularization to generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. The authors show that fast and optimal rates of convergence can be achieved for GSC by using the iterated regularization scheme, which is intrinsically related to the proximal point method in optimization, and overcomes the limitation of the classical TikhONOV regularization. This paper studies a well-known regularisation scheme for least square, and extend it for the first time to other loss functions, which notably contain the Logistic loss used for classification. They prove that Iterated Tikhonsov has optimal learning rates and higher qualification than the classical one."
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper introduces a new kind of linear transform named Deformable butterfly (Deformable Butterfly) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. The authors apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering its favorable properties such as light weight and low inference complexity, without compromising accuracy."
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes MetA Reusable Knowledge (MARK), a method to mitigate catastrophic forgetting (CF) in continual learning. The proposed method builds a common knowledge base (KB) that is used to incrementally enrich the KB with new knowledge and to foster weight reusability among tasks. In addition, a set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. The method achieves state-of-the-art results on CIFAR-100 and Mini-ImageNet datasets."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data-driven framework for scheduling heuristics in an exact MIP solver. The main idea is to learn from data describing the performance of primal heuristic solvers, and obtain a problem-specific schedule of heuristical solvers that collectively find many solutions at minimal cost. The authors formalize the learning task and propose an efficient algorithm for computing a schedule. Compared to the default settings, the proposed approach can reduce the average primal integral by up to 49% on two classes of challenging instances."
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper studies reinforcement learning in the setting where the learner receives binary feedback only once at the end of an episode. This setting is more representative of real-world applications than the traditional requirement in RL practice. The authors show that learning is possible in this more challenging setting, and provide a statistically and computationally efficient algorithm that achieves sublinear regret. "
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. The proposed method is validated on graph reconstruction, generation, and graph classification tasks."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations learned by maximizing mutual information (MI) between random variables in the context of RL from a theoretical perspective. In particular, the authors study two popular MI-based representation learning objectives, i.e., mutual information maximization and mutual information minimization, from the perspective of sufficiency. They show that both objectives are insufficient for the general class of MDPs, in the most general case, and prove that another typical objective is sufficient. The experimental results corroborate their theoretical findings, and demonstrate that sufficiency has a substantial impact on the performance of an RL agent."
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper proposes a novel method for 3D steerable convolution (SS-Conv) for efficient processing of 3D data that are inherently sparse. The main idea is to use a feature-steering module that takes advantage of SE(3)-equivariance and is able to conduct an efficient pose refinement. Experiments on 3D object semantic analysis tasks, including instance-level 6D pose estimation, category-level pose and size estimation, and category level 6D tracking, show the effectiveness of the proposed method."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, a lightweight prediction module is added to different layers to estimate the importance score of each token given the current features. An attention masking strategy is also proposed to differentiably prune a token by blocking its interactions with other tokens. By hierarchically pruning 66% of the input tokens, the method greatly reduces the FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5%."
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper studies the problem of distribution-free inference, i.e., the problem where the goal is to provide confidence intervals for the true conditional mean of some unknown distribution $P$ at a newly observed feature vector $X$. The authors consider the setting where the features $X_1, X_n$ are drawn i.i.d. from an unknown distribution $\mathcal{P}$ on $R^d$ and they aim to estimate the conditional distribution of $Y_\in\mathbb{R}$ based on the information carried by features $x_i$ in $X^i$ and $y_i$. They show that there are several regimes in between the finite setting and the continuous setting, where vanishing-width confidence intervals are achievable if and only if the effective support size of the distribution of X is smaller than the square of the sample size. "
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes Representation Neutralization for Fairness (RNF), a new mitigation technique that achieves fairness by debiasing only the task-specific classification head of DNN models. The key idea of RNF is to discourage the classification head from capturing undesirable correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, the authors leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate the effectiveness of the proposed RNF framework."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper proposes a new convolutional layer that takes advantage of Bessel functions, well known in physics, to build Bessel-CNNs (B-CNN) that are invariant to all the continuous set of possible rotation angles by design. The authors integrate a new kind of convolutionsal layer in CNNs that build B-Convolutional Neural Networks to build rotational equivariant CNNs. They propose a new way to use this representation in order to compute feature maps in a rotation-equivariant way (see Figure 1 for an example). The authors then lead to rotational invariance equivariance with respect to translations and rotations."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,This paper proposes a new algorithm for large-scale kernel ridge regression. The proposed algorithm combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. Theoretical analysis characterizes the statistical-computational trade-off of a partitioned kernel estimator by the interplay of intuitive quantities. Numerical experiments show that the algorithm performs favourably against a state-of-the-art global method.
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper proposes a method for learning to communicate via discrete tokens derived from a learned, continuous space. The authors claim that discretizing messages by constraining them to conform to one-hot vectors fundamentally precludes agents from learning some desirable properties of language. They present a novel architecture and implementation for learning such communication and provide decision-theoretic analysis of the value of such an approach - the congruence of meaning and form of communications. "
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper proposes a hybrid convolutional-transformer model that combines the advantages of convolution and self-attention in terms of generalization and model capacity. The authors claim that depthwise convolution layers tend to have better generalization with faster converging speed, while attention layers have higher model capacity that can benefit from larger datasets. To achieve this, the authors propose to combine the attention and the convolution layer in a principled way by stacking them vertically. Experiments show that the proposed model achieves state-of-the-art performance under different resource constraints across various datasets."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper proposes a new second-order oracle bound for the expected risk of a weighted majority vote. The upper bound is based on a novel parametric form of the Chebyshev-Cantelli inequality, which is amenable to efficient minimization. The authors also derive a new concentration of measure inequality which they name PAC-Bayes-Bennett, which combines PAC-bayesian bounding with Bennett’s inequality. The paper provides an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a weakly-supervised method for audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, the method explores event co-occurrence across audio, visual, and audio- visual streams. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes a personalized federated learning (FL) algorithm QuPeD that allows clients to learn compressed personalized models with different quantization parameters and model dimensions/structures. The authors propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. They formulate a compressed personalization framework by introducing knowledge distillation loss for local client objectives collaborating through a global model. They develop an alternating proximal gradient update for solving this compressed personalisation problem, and analyze its convergence properties. Numerically, they validate that QuPeRD outperforms FedAvg and local training of clients in various heterogeneous settings."
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"This paper proposes a deep generative model-based approach for constrained clustering, where the clustering process is conditioned on prior clustering preferences, expressed as pairwise constraints. The proposed approach, called Deep Conditional Gaussian Mixture Model (DC-GMM), uncovers the underlying distribution of the data conditioned on the prior clusters preferences, which are expressed as probabilistic relations, and can be trained efficiently in the framework of stochastic gradient variational inference. The authors provide extensive experiments to demonstrate the superior clustering performances and robustness of the proposed approach."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper proposes a near-input-sparsity time approximation algorithm for convolutional Neural Tangent Kernel (NTK) and neural tangent kernel (CNTK). The proposed algorithm is based on sketching the polynomial expansions of arc-cosine kernels, which can transform any image using a linear runtime in the number of pixels. The authors also prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) with a sketching algorithm. The proposed algorithms are evaluated on various large-scale regression and classification tasks. "
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"This paper proposes a multi-range transformer (MRT) model for predicting multi-person 3D motion trajectory prediction. The MRT is composed of a local-range encoder for predicting individual motion and a global-range decoder for social interactions. The local range encoder predicts each person’s local motion and global range features, and the global range decoder takes a corresponding pose as input and outputs a natural long-term motion with a correspond pose as the query in the decoder. The proposed model outperforms the existing state-of-the-art methods on both individual motion prediction and social interaction prediction. "
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncertainty. The proposed approach outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper studies the problem of causal imitation learning, i.e. imitation learning in the presence of a mismatch between demonstrator and imitator. The authors propose a graphical criterion that is necessary and sufficient for determining the feasibility of causal imitation learning, providing conditions when an imitators can match a demonstrator’s performance despite differing capabilities. Finally, they provide an efficient algorithm for determining imitability and corroborate their theory with simulations."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper proposes a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to an object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. The authors demonstrate the application of the proposed model on a real world dataset of robot trajectories."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"This paper proposes a weighted weighted ERM algorithm for risk minimization (ERM) with adaptively collected data to minimize the average of a loss function over a hypothesis class. It provides generalization guarantees and fast convergence rates. The main contributions are: (1) Theorem 1, which shows that the generalization error of ERM with bandit-collected data does not depend on the exploration rate in the data. (2) Theorems 2 and 3, which show that the excess risk of ISWERM converges at the rate of $O(\frac{1}{\epsilon^2}{\log(1/\delta)^2)$, where $\delta$ is the number of samples. (3) Fast rates, which are faster rates, are given for regression and policy learning. "
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper proposes a novel scheme for kernel-reweighted regression by reparametrizing the sample weights using a doubly non-negative matrix. When the weighting matrix is confined in an uncertainty set using either the log-determinant divergence or the Bures-Wasserstein distance, they show that the adversarially reweighted estimate can be solved efficiently using first-order methods. Numerical experiments show that their reweighting strategy delivers promising results on numerous datasets."
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper proposes two gradient estimators for training models with discrete latent variables. The proposed estimators are based on reparameterizing categorical variables as sequences of binary variables and Rao-Blackwellization. The authors also propose a novel derivation of their estimator based on importance sampling and statistical couplings, which they extend to the categorical setting. They show that their proposed categorical gradient estimator provides state-of-the-art performance."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a new predictor-based NAS method that progressively shrinks the sampling space, by learning a series of weak predictors that can connect the best architectures to the subspace of best architectures. The proposed method is called WeakNAS, which learns a set of predictors to progressively refine the ranking of sampling space. Extensive experiments demonstrate that WeakNAS costs fewer samples to find top-performance architectures on NAS-bench-101 and NAS-Bench-201."
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes a new intrinsic control method for unsupervised reinforcement learning. The proposed method, Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. EDDICT’s globally consistent codes allow it to be far more exploratory, which allows it to reach more states in the long term while still optimizing a local objective. The authors demonstrate the effectiveness of the proposed method on Montezuma's Revenge."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"This paper proposes a fragment-based generative RL with Explorative Experience Replay for Drug design (FREED) to generate pharmacochemically acceptable molecules with large docking scores. The proposed method is based on the idea that the generated molecules should satisfy strong structural constraints as well as pharmacokinetic and pharmacochemical properties such as chemical suitability and pharmacochemical suitability. To achieve this, the authors propose to use an error-prioritized experience replay (PER) to guide the generation of molecules that satisfy these constraints. The experimental results show that the proposed method achieves state-of-the-art performance on two of the three targets in terms of the docking scores and ablation studies."
SP:b938bca513e7de1231212064caf8877a78d8b612,This paper studies the problem of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. The approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. The sample complexity is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature.
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper considers the setting where each user has $m$ samples and the privacy protection is enforced at the level of each user’s data. In this setting, the authors show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an ("", )DP algorithm using only $O(\log(1/\epsilon^2)/\eps)$ users. In both cases, they show a nearly-matching lower bound on the number of users required. A crucial component of their results is a generalization of global stability [BLM20] that allows the use of public randomness. Under this relaxed notion, they employ a correlated sampling strategy to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the numbers of samples."
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper studies the properties of implicit parameterization of linear function approximations in the context of value iteration networks (VINs). In particular, the authors show that, for a linear parametrization, gradient descent converges to global optima despite nonlinearity and non-convexity introduced by the implicit representation. They also provide conditions under which stochastic gradient descent (SGD) with this implicit representation converges substantially faster than its explicit counterpart. Finally, they provide empirical results in some simple domains that illustrate the theoretical findings."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes a KG refinement framework called IterefinE which iteratively combines the two techniques – one which uses ontological information and inferences rules, viz.,PSL-KGI, and the KG embeddings such as ComplEx and ConvE which do not. The proposed method is able to exploit not only the knowledge graph (KG) information to improve the quality of predictions, but also the power of KG-embeddings which (implicitly) perform longer chains of reasoning. The experiments over a range of KGs benchmarks show that the proposed method can reject noisy facts from KG and at the same time infer higher quality new facts resulting in upto 9% improvement of overall weighted F1 score."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a novel evaluation paradigm for the task of knowledge base completion (KBC). The authors argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose a new evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. They construct the data set FB14k-QAQ with an alternative evaluation data structure: instead of single facts, they use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. This way, they can explicitly measure a model's ability to handle queries that have more correct answers in the real world than in the KB."
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes an approach to improve the performance of pretrained language models for dialog generation. The main idea is to use two pre-trained language models, one for each speaker, to encode and decode the speaker’s utterances in alternating order, and then use the output of the encoder and decoder to generate the response. The approach is evaluated on two task-oriented dialog datasets, CamRes676 and MultiWOZ, and one non-task-oriented persuasion dataset. The results show that the proposed approach outperforms or is on par with state-of-the-art methods on both tasks. "
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"This paper proposes a new uncertainty measure for deep neural networks, which is based on the notion of implied loss. The authors show that if an uncertainty measure is an implied loss, then low uncertainty means high probability of correct (or top-k) classification on the test set. The proposed method is simple to use on existing networks: they proposed confidence measures for Top-k which can be evaluated by binning values on the data set. They demonstrate empirically that these values can be used to measure the confidence that the classification is correct."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the trainability and generalization of neural networks as a function of their architecture and hyperparameters in the infinite-width limit. It builds on the recent results that show that random networks before training are Gaussian processes governed by a kernel known as the Neural Network Gaussian Process (NNGP) kernel, and that at large depths the spectrum of the NNGP kernel simplifies considerably and becomes “weakly data-dependent”. The authors also show that gradient descent training of wide neural networks is described by the Neural Tangent Kernel (NTK) that is related to the NNGP kernel. The spectrum of NTK simplifies in much the same way as that of the NTGP kernel at the large depth limit. By analyzing this spectrum, the authors arrive at a precise characterization of trainability that is necessary for generalization across a range of architectures including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs)."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper introduces Graph Convolutional Neural Networks (GCNs) as a graph convolutional network (GCN) for the problem of protein quality assessment (QA) of computational models of protein structures. The authors propose a graph-based method to estimate the quality of protein models that possesses favorable properties as representation learning, geometric invariance, explicit modeling of sequential and 3D structure, simultaneous local and global scoring, and computational efficiency. Through extensive experiments, the authors show significant improvements over the state-of-the-art for both hand-engineered and representation-learning approaches."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper studies the landscape of the loss function of linear neural networks with different loss functions and different parameterizations. In particular, the functional space is either the set of all linear maps from input to output or a determinantal variety, i.e., a set of linear maps with bounded rank. The authors introduce a natural distinction between pure critical points and spurious critical points, which only depend on the function space, and arise from the network’s parameterization. The analysis clearly illustrates that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear mapping (filling architectures), but it holds only for the quadratic loss when the functional spaces is a determinantsal variety (“non-filling”). Without any assumption on the architecture, smooth convexiﬁed losses may lead to landscapes with many bad minima."
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper proposes a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Given an input graph, the proposed framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub-graph vectors, and uses the embedding of the sub graph vector distribution as the output vector representation for the input graph. Theoretical analysis shows the connection between SEED and graph isomorphism. The empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods."
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper proposes a new variant of counterfactual regret minimization (CFR) algorithm for solving two-player zero-sum extensive games with imperfect information (TEGI). The proposed algorithm Lazy-CFR adopts a lazy update strategy to avoid traversing the whole game tree in each round, which is time-consuming in large-scale games. Theoretical analysis shows that the regret is almost the same as the regret of the vanilla CFR and only needs to visit a small portion of the game tree. Empirical results consistently show that the proposed algorithm is fast in practice."
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper proposes a method for Unsupervised Domain Adaptation (UDA) that explicitly models and matches the deep feature distribution of the source and target data as Gaussian mixture distributions. Specifically, the authors propose two new domain discrepancy losses based on the Gaussian components distributions of the deep features called Gaussian Component Mean Matching (GCMM) and Pseudo distribution matching (PDM). Extensive experiments verify the effectiveness of the proposed method."
SP:40be996e8bb86e887077b762b87c7c34a786ac98,This paper proposes a conditional continuous normalizing flow (CNF) model that partitions the latent space into a class-specific supervised code and an unsupervised code that is shared among all classes for efficient use of labeled information. It also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. The authors show empirically that InfoCNF improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs on CIFAR10.
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the temporal-difference (TD) learning algorithm under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling arises naturally, implicit in the initialization of their parameters. Both in the under-and over-parametrized frameworks, the authors prove exponential convergence to local, respectively global minimizers of the above algorithm in the lazy training regime. They then give examples of convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the cases of neural network."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper proposes a method for training an agent that can generate observations that can help predict whether a hypothesis is true or false. The agent is given a hypothesis about the dynamics of the world and can take actions to generate observations to verify the hypothesis. The main contribution of this paper is to formulate the problem of hypothesis verification as a reinforcement learning problem. The authors show that agents trained end-to-end with the reward fail to learn to solve this problem. Instead, they exploit the underlying structure in the majority of hypotheses – they can be formulated as a triplet (pre-condition, action sequence, post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can then be fine-tuned to verify more general hypotheses."
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper proposes a method to predict the embeddings of mathematical formulas in the latent space of the interactive theorem prover HOList, a benchmark for automated theorem provers based on deep learning (Bansal et al., 2019b). The goal of the paper is to show that neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space, i.e., predicting the set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. The paper proposes to compress this information by embedding the formula in a vector space, so that it can be used to predict whether a statement can be rewritten by other theorems. The experiments show that graph neural networks (GNNs) can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps."
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,This paper proposes a method for unsupervised monocular dense depth estimation from images and very sparse depth measurements. The method is based on a global-local network architecture. The global parameters extracted by the network are predictive of the metric agent motion. Experiments on several datasets show that the proposed model can learn monocular depth estimation when trained with very sparse ground truth.
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper extends the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. The authors show that by applying a multi-layer Transformer to these Bloom filter digests, they are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models with a much larger size trained using sampled softmax."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper proposes a method for zero-shot shape part discovery on 3D point clouds in unseen object categories. The authors propose a learning-based agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of the approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the PartNet dataset, the authors demonstrate that their method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotations."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a technique called “neuron editing” that learns how neurons encode an edit for a particular transformation in a latent space. The authors use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in the latent space, they encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron’s activations. Their technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications."
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper studies the problem of meta-learning of initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of training data for a new task. The authors propose an extension of FOMAML and Reptile to image segmentation, a novel neural network architecture built for parameter efficiency and fast learning which they call EfficientLab, a formalization of the generalization error of meta learning algorithms, which they leverage to decrease error on unseen tasks, and a small benchmark dataset, FP-k, that contains 400 training examples for 5 tasks each. They show that meta-learned initializations provide value for both canonical few-shot learning problems and larger datasets, outperforming random and ImageNet-trained initializations."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a method for semi-supervised few-shot learning (SS-FSL) based on the Prototypical Networks (PN) framework. The main idea of the paper is to use the prototypical random walk (PRW) as a meta-learning objective to improve the performance of the PN. The proposed method is evaluated on the mini-Imagenet, Omniglot and CIFAR-10 datasets. "
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a new self-supervised training objective, Contrastive Sensor Fusion (CSF), which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. The authors train an encoder to produce semantically meaningful representations from any possible combinations of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,This paper proposes weight rewind and learning rate rewinding as techniques for retraining after neural network pruning. Weight rewind trains the unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate reweighing trains the weights from their final values using the same learning rate schedule as weight reweinding. Both reweighed techniques outperform fine-tuning in terms of accuracy and compression ratio.
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper studies the relationship between the output margin and the generalization of deep neural networks. The authors propose a new notion of margin, which they call the “all-layer margin”, and analyze its relationship with generalization. They show that if the margin is large, then generalization is better. They also propose a training algorithm that encourages the network to increase the margin."
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper studies the problem of knowledge-grounded dialogue generation in a low-resource setting. The authors propose a disentangled response decoder in order to isolate parameters that depend on knowledge from the entire generation model. By this, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only 1/8 training data, the proposed model can achieve the state-of-the-art performance and generalize well on out- of-domain knowledge."
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a mirror-generative neural machine translation model (MGNMT) that jointly learns bidirectional translation models as well as source and target language models in a latent space of the shared bilingual semantics. The main idea is that both translation directions of MGNMT can benefit from non-parallel data. Besides, the proposed model can naturally take advantage of its learned target-side language model for decoding, which leads to better generation quality. Experiments show that the proposed method consistently outperforms other approaches in all investigated scenarios."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper studies the role of the entropy term in the Soft Actor Critic (SAC) algorithm for Mujoco. The authors propose a new algorithm, Streamlined Off Policy (SOP), which does not employ entropy maximization but nevertheless matches the sampling efficiency and robustness performance of SAC. They also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. The experimental results show that SOP and IG outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. The authors describe a well-known music identification method and implement this system in the form of a neural net. They then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube’s Content ID system. Their goal is to raise awareness of the threats posed by adversarial examples in this space and highlight the importance of hardening copyright detection systems to attacks."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper proposes a method to generate point-to-point activation intensity between two images so that the relationship between different regions of the two images can be uncovered. The proposed method can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. The experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper studies the problem of learning control in an online lifelong learning setting, where the agent must simultaneously act and learn in the world continuously with limited computational resources. The authors propose a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times. Experiments show that AOP gracefully deals with novel situations, adapting behaviors and policies effectively."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes to replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and total-variation Sparse Attention (TVMAX). With sparsemax, we obtain sparse attention weights, selecting relevant features. By selecting relevant groups of features, the TVMAX transformation improves interpretability. The authors present results in the Microsoft COCO and Flickr30k datasets. "
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes a method for predicting the evolution of dynamic graphs, i.e., predicting the topology of the graph at the next time step. The authors use a graph neural network (GNN) and a recurrent architecture (RNN) to capture the temporal evolution patterns of the dynamic graphs. Then, they employ a generative model to generate the graph instance that corresponds to that topology. They evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as real-world datasets."
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The method consists of a generator network that generates imputations that a discriminator network is tasked to distinguish, and a predictor network that is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 and three real-world tabular classification datasets, under different missingness rates and structures. The experimental results show the effectiveness of the proposed method in generating imputations and providing estimates for the class uncertainties in a classification task when faced with missing values."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper studies the problem of long-horizon off-policy estimation in reinforcement learning, where the goal is to estimate the average long-term reward of a target policy given historical data collected by (possibly unknown) behavior policies. The authors formulate the problem as solving for the fixed point of a certain operator. They develop a new estimator that computes importance ratios of stationary distributions, and analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our approach."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper proposes a generative adversarial network (GAN) framework for Gaussian mixture model (GMM) which is a probabilistic framework which allows us to define a dataset containing K different modes when each of the modes is associated with a Gaussian distribution. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional likelihood p(x|k,\theta) and the responsibility probability p(k|x, \theta). However, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it’s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, the authors utilize the Generative Adversarial Network framework to achieve an alternative plausible method to compute these probabilities at the data's latent space z instead of x. They devise a modified GAN to allow to define the distribution using p(z|k^2,\tilde{x}^2), where z is the corresponding latent representation of x, as well as p(\k|z|z) through an additional classification network which is trained with the GAN in an “end-to-end” fashion. They demonstrate that the proposed method surpasses previous baselines in terms of image generation performance with only minor growth on the size of the network."
SP:2da1608209058d214f8671062cc9eb0833ba4831,"This paper presents a method for training large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. The authors achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, the authors introduce a new residual block architecture that gates convolution channels in a fine grained manner. They also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. They use this technique to force gates to be more conditional on the data."
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"This paper proposes a probabilistic importance inference approach for pruning DNNs. Specifically, they test the significance of the relevance of a connection in a DNN to the DNN's outputs using a nonparemetric scoring test and keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a method for learning hierarchical representations for hierarchical reinforcement learning. The proposed method is based on identifying behavioral ‘motifs’—repeated action sequences that can be compressed to yield a compact code of action trajectories. The learned representations are then used to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary length. The authors demonstrate the relevance of this approach for tasks with non-trivial hierarchical structure and show that the approach can be used to accelerate learning in recursively more complex tasks through transfer."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"This paper proposes Hierarchical Bayes Autoencoder (HBAE), a probabilistic generative model with a multimodal decoder. The decoder is modeled as an energy-based model (EBM) instead of the commonly adopted unimodal Gaussian distribution. The encoder is trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, the authors use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The proposed method is able to model sets, by inferring latent codes for a set of examples, and sampling set members through the multi-modal decoders."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper proposes a feature normalization technique for off-policy reinforcement learning (OTL) methods. The authors propose a cross-normalization technique, which is an extension of batch normalization that re-centers data for two different distributions, as present in OTL. They show that the proposed method improves the performance of DDPG and TD3 on a range of MuJoCo tasks. "
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper proposes a method to learn discriminative features that are unbiased and invariant to the confounder(s). The proposed method is based on adversarial training strategies by encouraging vanished correlation to learn features for the prediction task while being unbiased to the confounding variables in the study. Experiments on synthetic data, medical images, and a gender classification (Gender Shades Pilot Parliaments Benchmark) dataset show that the learned features by the method not only result in superior prediction performance but also are uncorrelated with the bias or confounders variables."
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper proposes a lightweight Transformer-based model for character-level language modeling. The authors propose a lightweight model, called Group-Transformer, that factorizes the calculation paths by grouped embedding operators and employs inter-group linear operators to prevent performance degradation from the group strategy. Experiments on enwik8 and text8 show that the proposed method achieves better performance than LSTM-based models."
SP:946c26d371297c88d0ac246257104099b4585edc,"This paper proposes an Optimal transport-based framework for training generative models with deep-latent hierarchies of latent variables using Optimal Transport. The proposed approach recursively applies the Wasserstein distance as the regularisation divergence, allowing the stacking of WAEs for arbitrarily deep latent hierarchies. The authors show that this approach enables the learning of smooth latent distributions even in deep latent hierarchical hierarchies, which otherwise requires extensive model design and tweaking of the optimisation procedure to train. They also show that their approach is significantly more effective at learning smooth hierarchical latents than the standard WAE."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"This paper proposes an autoregressive video generation model based on a three-dimensional self-attention mechanism. The proposed model is a generalization of the Transformer architecture of Vaswani et al. (2017) using a block-local attention mechanism, which can be implemented efficiently on Tensor Processing Units (TPUs) and can be scaled up substantially while retaining the ability to capture longer range spatiotemporal dependencies. Empirically, the authors obtain state-of-the-art results across a range of video generation benchmarks, while the scalability of the approach enables them to make an initial attempt at modeling videos of unusually high complexity and diversity as found in the Kinetics dataset."
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,This paper proposes an adversarial generative model for generalized zero-shot learning on multi-label text classification for automatic ICD coding. The proposed method generates semantically meaningful features for zero shot codes by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. Extensive experiments demonstrate the effectiveness of the proposed method on the public MIMIC-III dataset.
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,This paper proposes a self-supervised representation learning approach to improve sample efficiency in reinforcement learning (RL). The authors propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. The authors demonstrate that the action embedding alone improves the sample efficiency and peak performance of model-free RL on control from low-dimensional states. The proposed approach is evaluated on goal-conditioned continuous control from pixel observations.
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the new task. Experiments on toy regression and few-shot image classification demonstrate the superiority of ARML over state-of-the-art baselines."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a method for controlling attributes of the generated language (e.g. switching topic or sentiment) without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. The authors propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM’s hidden activations and thus guide the generation. PPLM achieves fine-grained control of attributes via a simple gradient-based sampling mechanism."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes to learn data representations with a novel type of denoising autoencoder, where the noisy input data is generated by corrupting clean data in the gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption. The learned representations perform well when transferring to other vision tasks."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper studies the problem of under-sensitivity in natural language inference, i.e., the issue that models become more confident as arbitrary subsets of input text are deleted. The authors propose a formal verification method based on the interval bound propagation (IBP) approach, which can be used to verify whether a particular input sample is free from the under-sensitive problem. They compare different training methods to address this problem, and compare metrics to measure it. The experiments show that IBP training leads to a significantly improved verified accuracy."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper proposes a new Markov decision process (MDP) for off-policy deep reinforcement learning (RL) based on Markov data graphs. The authors show that the Q-value for each transition in the simplified MDP is a lower bound of the Q value for the same transitions in the original continuous Q-learning problem. By using these lower bounds in TD learning, the proposed method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. "
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper studies the problem of unsupervised domain adaptation, i.e., generalizing the hypothesis trained in a source domain to an unlabeled target domain. The authors analyze the effect of the embedding complexity on generalization to the target domain, and show that this complexity affects an upper bound on the target risk; this is reflected in experiments, too. Then, they develop a strategy that mitigates sensitivity to the embeddings complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper studies generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. The authors develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalisation error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. They obtain new data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Their result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al (2018)."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper investigates the role of the hippocampus in continual learning in the context of continual reinforcement learning. The authors analyze population-level activity of hippocampal CA1 neurons of rodents learning to perform allocentric and egocentric spatial navigation tasks. They find that the hippocampus encodes task variables such as decisions, rewards and strategies, which suggests that the hippocampal plays a key role in the ability for animals to perform continual spatial learning. They also demonstrate that some of the components are consistent with theoretical models in reinforcement learning and suggest the existence of a non-standard form of TD learning implemented or used by the hippocampus."
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,This paper proposes a method for policy optimization in continuous action spaces. The main idea is to use Monte Carlo Tree Search (MCTS) as an expert to generate high-quality trajectories and then use them to update the policy. The proposed method is evaluated on Mujoco environments and shows improved performance compared to the baseline PPO.
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper studies the lottery ticket hypothesis, which claims that sparse subnetworks of over-parameterized neural networks could achieve good predictions when trained in isolation, as long as they are appropriately initialized; these “lucky starting points” have been termed winning tickets. This paper aims to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain “good” tickets without supervision? They find that winning tickets found in these scenarios are, perhaps surprisingly, competitive with winning tickets generated on the full ImageNet dataset when evaluated on ImageNet classification task. They also show that using deep networks trained on CIFAR-10 are sparse, making conclusions drawn about lottery tickets potentially misleading if this effect is not accounted for."
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"This paper focuses on the problem of excessive prediction undersensitivity, i.e., the problem that the model’s prediction does not change when it should. The authors formulate a noisy adversarial attack which searches among semantic variations of comprehension questions for which a model still erroneously produces the same answer as the original question – and with an even higher probability. They show that – despite comprising unanswerable questions – SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarially generated questions. This indicates that current models—even where they can correctly predict the answer—rely on spurious surface patterns and are not necessarily aware of all information provided in a given comprehension question. Developing this further, the authors experiment with both data augmentation and adversarial training as defense strategies: both are able to substantially decrease a model's vulnerability to undersensitivity attacks on held-out evaluation data."
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model-based approach to ensuring the safety of reinforcement learning agents. The proposed approach learns the transition dynamics of the environment and generates a directed graph that encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state, is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Experiments on two gridworld environments and a self-driving car simulator demonstrate that the proposed approach to safety visits unsafe states significantly less frequently than a baseline."
SP:c2796f28fb067138303df8d424d646f4ada31558,"This paper proposes a physics-aware difference graph networks (PA-DGN) which leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. In particular, it leverages neighboring information to learn finite differences inspired by physics equations. The proposed method is evaluated on synthetic data and real-world climate observations from weather stations."
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper considers the problem of training structured neural networks (NN) with nonsmooth regularization and constraints (e.g. interval constraints). The authors formulate training as a constrained nonsmoothed nonconvex optimization problem, and propose a proximal-type stochastic gradient descent (ProxSGD) algorithm. They show that under proper learning rates, with probability 1, every limit point of the sequence generated by the proposed PropsSGD algorithm is a stationary point. Theoretical analysis and numerical tests are provided to support the theoretical analysis."
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper proposes an end-to-end differentiable compression framework for lossy image compression, which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bits-back efficient. The authors apply it to training Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper proposes a new method for super-resolution (SR) based on compressed JPG (C-JPG) images. The authors propose a new SR structure with two specifically designed components, as well as a cycle loss. First, they propose a functional sub-model to recover information for C-jPG images, instead of the perspective of noise elimination in traditional SR approaches. Second, they further integrate cycle loss into SR solver to build a hybrid loss function for better SR generation. Experiments show that the proposed approach achieves outstanding performance among state-of-the-art methods."
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper proposes a deep learning approach to estimate the probability of passing events in soccer matches from low-level tracking-data input and weak labeling of each event’s success. The approach is based on a fully convolutional network architecture that is able to estimate a full surface of pass probabilities from single-location labels derived from high-frequency spatio-temporal data of professional soccer matches. The network is trained by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve both coarse and fine detail. The proposed approach presents an extreme case of weakly supervised learning where there is just a single pixel correspondence between ground-truth outcomes and the predicted probability map. By providing not just an accurate evaluation of observed events but also a visual interpretation of the results of other potential actions, the approach opens the door for spatiotemporal decision-making analysis, an as-yet little-explored area in sports."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion method for recommender systems. The proposed method learns a graph neural network (GNN) based on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps them to their corresponding ratings. It achieves competitive performance with state-of-the-art transductive baselines and generalizes to new matrices."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper considers the problem of unconstrained minimization of a smooth objective function in the setting where only function evaluations are possible. The authors propose and analyze stochastic zeroth-order method with heavy ball momentum. They show new complexity results for non-convex, convex and strongly convex functions. They test their method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. environments."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes a network architecture, Action Semantics Network (ASN), that explicitly represents the action semantics between agents in multi-agent reinforcement learning (MARL). In particular, the proposed ASN characterizes different actions’ influence on other agents using neural networks based on the actions' influence between them. The proposed method can be easily combined with existing deep reinforcement learning algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show that ASN significantly improves the performance of state-of-the-art DRL approaches."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper proposes to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. Specifically, the authors investigate the low-rank structure, which widely exists for big data matrices, and verify empirically the existence of such structure in the Q function in the context of control and RL tasks. The authors propose a general framework to exploit this structure in Q functions, which leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on “low-rank” tasks."
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes a new algorithm, Best-Action Imitation Learning (BAIL), which selects from the batch the actions it believes to be high-performing actions for their corresponding states; it then uses those state-action pairs to train a policy network using imitation learning. The proposed algorithm is simple and achieves state-of-the-art performance on the Mujoco benchmark. The paper is well-written and easy to follow."
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper proposes DeepXML, an algorithm for deep extreme multi-label learning (DEMA) on short text documents. The main idea is to learn word embeddings on head labels and transfer them through a novel residual connection to data impoverished tail labels. The authors also propose to increase the amount of negative training data available by extending state-of-the-art negative sub-sampling techniques and re-ranking the set of predicted labels to eliminate the hardest negatives for the original classifier. All of these contributions are implemented efficiently by extending the highly scalable Slice algorithm for pretrained embedding to learn the proposed DeepXml architecture."
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper proposes a method for collaborative filtering based on variational variational hashing-based collaborative filtering (VaHSM-CF) that learns binary vector representations (hash codes) of users and items so that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. In practice, some bits might encode more important properties than other bits, where the importance depends on the user. To address this problem, this paper proposes an end-to-end trainable VAE-based approach that uses the concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), that it learns to encode which bits are important to the user, rather than the user’s preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper studies the mode collapse issue of GANs. The authors propose a set of statistical tools that are applicable to quantitatively measure mode collapse. They also analyze possible causes of mode collapse and propose two simple yet effective “black-box” methods to calibrate the GAN’s learned distribution, without accessing either model parameters or the original training data."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper studies the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. The authors propose to randomize the neural networks, which allows them to escape their NTK and couple with quadratic models. They show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. They also prove concrete generalization and expressivity results on these randomized networks."
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"This paper proposes a new metric, Graph Filter Discriminant Score (GFD), to evaluate the effectiveness of graph convolutional filters for a given graph in terms of node classification. The authors claim that there is no single filter as a “silver bullet” that performs the best on all possible graphs, and graphs with different properties are in favor of different graph filters. Based on these findings, the authors develop Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn data-specific filters. Experiments on both synthetic and real-world benchmark datasets have demonstrated that the proposed model has the flexibility in learning an appropriate filter and consistently provides state-of-the-art performance."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies the problem of distributionally robust optimization (DRO) for over-parameterized neural networks, where the goal is to minimize the worst-case training loss over a set of pre-defined groups. The authors show that naively applying group DRO to overparametrized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst case training loss. Instead, the poor worst case performance arises from poor generalization on some groups. To address this issue, the authors propose to use a stronger-than-typical l2 penalty or early stopping to improve the performance of DRO models with increased regularization. They achieve 10-40% improvement on a natural language inference task and two image tasks, while maintaining high average accuracies."
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper presents a simple but effective mask predictor to provide local explanations for black-box classifiers. Specifically, the authors introduce the concept of distribution controllers and integrate it with a neural network to directly guide the distribution of relevance scores. Then they introduce the classification loss to optimize the proposed predictor. The benefit of this strategy is to enable discriminative scores over supporting features, and facilitate the setting of involved hyperparameters. The experimental results demonstrate that the proposed method also outperforms others in terms of faithfulness and explainability."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"This paper proposes a method for multi-instance image reconstruction and classification. The proposed method is based on a Transformer-based deep network that is trained to extract the most significant patches of the heatmap and feed these patches to a task-specific network to solve a domain specific problem. The method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and achieves state-of-the-art performance."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,"This paper proposes a reinforcement learning-based approach for neural program synthesis. The main idea is to use reinforcement learning to improve the performance of Monte Carlo Tree Search (MCTS), which is a well-known approach for learning to generate programs that can be executed to produce desired outputs from given inputs. The authors propose to use policy and value networks to reduce the breadth and depth of MCTS, and propose an effective multi-entropy sampling technique to alleviate online update correlations. The proposed approach is evaluated on a set of simple tasks, and the results show that the proposed approach outperforms existing baselines."
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper studies the impact of model architecture on the speed of training in the context of gradient descent optimization. The authors use the ideas from prior work that shows gradient descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H to characterize the convergence rate. They introduce a simple analysis technique that enumerates H in terms of all possible “paths” in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed-of- convergence."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper studies the connection between strongly over-parameterized neural networks (NNs) and kernel methods. In particular, the authors show that the generalization error of wide ReLU-NNs trained with squared loss are essentially a sum of two parts: the first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows the authors to transfer generalization bounds from minimum complexity interpolating kernels to NNs; (b) in the opposite regime, the test performance of wide NNs increases significantly with the initialization variance."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"This paper proposes a method to improve depth estimation and stereo-based 3D object detection in autonomous vehicles without expensive LiDAR. The authors propose a depthpropagation algorithm, guided by the initial depth estimates, to diffuse the few exact measurements across the entire depth map. They also propose a graph propagation algorithm to integrate the two data modalities and propagates the sparse yet accurate depth estimates using two sparse matrix solvers. The proposed method is evaluated on the KITTI object detection benchmark."
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a principled adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, they train K base detectors where the i-th detector is trained to discriminate natural data of class i from adversarial examples perturbed from other classes. At inference time, they first get the predicted label (say k) of the input, and then use the kth detector to identify whether the input is a natural sample (of class k) or an adversarial sample (perturbed from the other classes). They further devise a generative approach to detecting/classifying adversarial samples by interpreting each base detector as an unnormalized density model of the classconditional data."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper proposes an intrinsic reward for model-free reinforcement learning based on the difference in the learned representations of consecutive states. The authors propose to use the Euclidean distance between the predicted next state representations and the actual next state representation as intrinsic reward to encourage exploration. The proposed method is evaluated on procedurally-generated tasks in MiniGrid, and on tasks with high-dimensional observations. "
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper considers the large-scale query-document retrieval problem: given a query, return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps: the retrieval phase first reduces the solution space, returning a subset of candidate documents, and the scoring phase re-ranks the documents. The retrieval phase not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. In BERT, the scoring function f is a pre-trained deep bidirectional Transformer model. While BERT-style cross-attention models are very successful, it cannot be directly applied to large scale retrieval problems because computing f(q,d) for every possible document can be prohibitively expensive. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, the authors conduct a comprehensive study on the embedding-based retrieval models. They show that the key ingredient of learning a strong embeddings-based Transformer-based model is a set of pre-training tasks. The paragraph-level pretraining tasks are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three."
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper proposes a new graph convolution operation for graph neural networks (GNNs), which is a generalization of graph convolutions (GCNs) and graph pooling (GCN). The proposed method, called BiGraphNet, is able to subsume conventional GCNs and pooling as its special cases, and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures. The authors also show that the proposed method can be used to build efficient architectures such as graph skip connections, and graph autoencoders."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,This paper addresses the problem of few-shot classification under domain shifts for metric-based methods. The core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. The authors further apply a learning-to-learn approach to search for the hyper-parameters of the feature transformation layers. Experimental results demonstrate that the proposed method is applicable to various metrics-based models and provides consistent improvements on the performance under domain shift.
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper proposes to use convolutional networks with continuous convolutions on particles for learning fluid mechanics. The authors treat fluids as spatially continuous functions sampled at a finite set of (continuously evolving) positions and process them with a novel continuous convolution layer. This matches the continuous nature of the problem more closely and simplifies the definition of neural networks by abstracting the underlying particle representation. They show that their convolutions, despite their simplicity, perform better than more sophisticated representations."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes a new ensemble method, called BatchEnsemble, to improve the accuracy and uncertainty of any neural network. The proposed method is based on the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. The method is parallelizable across devices, where one device trains one member, and also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. The speedup at test time is 3x and memory reduction is 3X at an ensemble of size 4. The authors also apply the proposed method to lifelong learning."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network-based solver for solving the forward and inverse problems of PDEs. The solver is grid free, mesh free and shape free, and the solution is approximated by a deep neural network. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. The resulting solution is an explicit smooth differentiable function with a known analytical form. This framework enables the solution of high order non-linear PDES."
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper proposes a method to improve the performance of SAT solvers for binary neural networks (BNNs). BNNs are a class of neural networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT. The main bottleneck for all existing methods is their ability to reason about large BNN. In this paper, the authors analyze architectural design choices of BNN and discuss how they affect the performance. They propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solver without sacrificing accuracy on the primary task. "
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNnmp can lose a significant portion of their power when their depth and width is restricted. These results stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a localised generative flow (LGF) method for density estimation. The authors argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose LGFs to address this problem. The proposed LGFs are composed of stacked continuous mixtures of bijection, which enables each bijection to learn a local region of the target rather than its entirety. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but the authors propose a simple variational scheme that performs well in practice."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper focuses on the problem of vision-and-language navigation (VLN), where the goal is to perform well in unseen environments (i.e., environments that are not used during training). The authors claim that the current state-of-the-art VLN models are biased towards training environments. The authors propose two methods: (1) environment re-splitting and (2) feature replacement, which aim to improve the generalizability of the model to unseen testing environments. Experiments are conducted on R2R, R4R, and CVDN datasets. Results show that the low-level visual appearance conveyed by ResNet features directly affects the agent model and contributes to this environment bias in results."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper studies the problem of using implicit human feedback to accelerate and optimize the training of deep reinforcement learning (DRL) algorithms. The authors propose to use EEG data from a non-expert observer to capture the error-related electric potentials (ErrPs) of an RL agent and use them as an auxiliary reward function to accelerate the learning of the RL algorithm. They demonstrate the feasibility of capturing ErrPs of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as a reward function. They also show that the definition of ErrPs is generalizable across different environments."
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper proposes a method to compare the performance of different image classifiers in terms of the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, the goal is to compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The proposed method generalizes similar methods explored in previous works by combining and comparing the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance. The authors conduct experiments over the ILSVRC test-set to show that machine classifiers are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the IlsVRC-trained models."
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,This paper studies the robustness of perturbation-based defense methods for convolutional neural networks. The authors identify a family of defense techniques that are based on the instability assumption. The defenses include deterministic lossy compression algorithms and randomized perturbations to the input that all lead to similar gains in robustness. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed methods.
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper proposes a method for self-supervised learning of 3D feature maps for object detection. The proposed method is based on the idea of view-contrastive prediction, i.e. how would a given scene look from an alternative viewpoint. The main idea is to learn a 3D mapping network, which takes as input 2.5D video streams captured by a moving camera, and lifts them to stable 3d feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3D features to novel viewpoints, to predict and match against target views. The authors propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data."
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised domain translation (UDT), which is an extension of the classical domain translation problem of Isola et al. (2016) where the goal is to learn a mapping from one domain to another, without access to pairings between the two domains. The paper proposes a framework based on Optimal Transport (OT) and CycleGAN (Zhu et al., 2017) to solve UDT problems. Theoretical results show that CycleGAN is biased towards low energy transformations, leading the authors to cast UDT into an OT framework and prove that it is possible to solve any UDT problem within this framework, and propose a simple and robust approach to solve the UDT. "
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a novel regularization method, RotationOut, for neural networks, which adds noise to the input by randomly rotating the input layer and introduces regularization by random rotating the vector. The proposed method can also be used in convolutional layers and recurrent layers with small modifications. The authors also use a noise analysis method to interpret the difference between the proposed method and Dropout in co-adaptation reduction. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed methods."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a method to create universal adversarial perturbations (UAP) for a given CNN in a data-free manner. The authors show that the adversary generation with full training data can be approximated to a formulation without data. This is realized through a sequential optimization with the proposed dilate loss, which maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, the perturbation constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs."
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a transferable neural architecture search (NAS) method based on meta-learning. The proposed method learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. The proposed model is related to VIB and variational dropout, but provides a simpler and more direct realization via neuron regularization by a non-informative activation prior. The extensive experiments show that this simple framework has diverse benefits for network pruning, adversarial defense and label noise robust learning."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta-learning approach to generate curiosity mechanisms for reinforcement learning agents. The core idea is that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent’s life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. The authors formulate the problem of generating curious behavior as one of meta learning, where an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. They propose to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. They demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains such as grid navigation with image inputs, acrobot, lunar lander, ant and hopper."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,This paper proposes a new approach for the task of Any-Code-to-Code Generation (AnyC2C) - generating code given its surrounding code without any restriction on the vocabulary or structure. The proposed approach leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling (SLM). SLM estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. The authors present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node.
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper provides a theoretical analysis to explain why gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). The authors introduce a tool called canonical space and prove that the objective functions in learning NNs are convex in the canonical model space. They further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, they prove that gradient descent algorithms surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,This paper proposes two interactive graph-based segmentation algorithms for semantic and panoptic image segmentation. The main idea is to use the feature maps from a pre-trained deep convolutional neural network (DCNN) as input to the interactive graph based segmentation algorithm. The authors propose a heuristic heuristic of a discrete discrete Potts model and a class-aware integer linear programming (ILP) formulation that ensures global optimum. The proposed algorithms achieve competitive results on the PASCAL VOC 2012 and Cityscapes dataset. 
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper proposes to use learned saliency models to detect adversarial perturbations in real-time. The main idea is that the saliency maps of adversarial images differ from those of natural images. The proposed method is evaluated on MNIST, CIFAR-10, and ASSIRA, and can detect various adversarial attacks."
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper studies the problem of quantifying the global adversarial robustness of a neural network (NN) to adversarial perturbations. Specifically, given a trained model f, the authors consider the probability that its prediction at any point sampled from the (unknown) input distribution is susceptible to an adversarial attack. The authors propose to use concentration inequalities to compute global robustness with estimation error upper-bounded by, for any > 0 selected a priori. They then show how concentration inequalities can be used to compute robustness guarantees for a selection of local robustness properties used in the literature. Finally, they empirically evaluate the robustness/accuracy trade-off for a variety of neural networks architectures and training methods on MNIST, Fashion-MNIST and CIFAR."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the problem of robust reinforcement learning with Wasserstein distance. The authors show that the transition probability of the optimal policy can be bounded by the transition probabilities of the perturbation of the reference transition kernel. They also show that optimal robust policies can be found, and provide a sensitivity analysis to reveal the effects of uncertainty set, and design a corresponding two-stage algorithm. The effectiveness of the proposed algorithm is verified in the Cart-Pole environment."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method to approximate mixed strategy Nash equilibria in multi-player continuous games. The authors propose a pushforward measure technique to represent a mixed strategy in continuous spaces, which allows them to generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players’ joint strategy profile and a Nash equilibrium. Then they apply the gradient descent algorithm to the general GNI function, which converges to a mixed stationary Nash equilibrium under the convexity assumption on payoff functions, the same popular setting as in previous studies."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper proposes a neural execution tree (NExT) framework for augmenting training data for text classification with natural language (NL) explanations. The NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics. NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper extends the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness. Temporal properties are commonly desired from DNNs in settings where the outputs have a sequential nature. The authors extend verified training to tasks that require temporal properties to be satisfied, and to architectures such as auto-regressive RNNs. Experiments show that while models trained using standard training often violate desired specifications, the verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper studies the problem of generalization to visually diverse environments in deep reinforcement learning. The authors formalize the problem, illustrated the inefficiencies of standard domain randomization, and proposed a theoretically grounded method that leads to robust, low-variance policies that generalize well. They propose a regularization method for learning policies that are robust to irrelevant visual changes in the environment. "
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper proposes a method to sample pairs in a batch of data for deep metric learning (DML). The authors cast DML as a simple pairwise binary classification problem that classifies a pair of examples as similar or dissimilar. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization (DRO). The flexibility in constructing the uncertainty decision set of the dual variable allows the authors to recover state-of-the-art complicated losses and also induce novel variants. Experiments on several benchmark data sets demonstrate that the proposed method outperforms the existing methods."
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,This paper studies the problem of finding a local minimum in non-convex finite-sum minimization. The authors propose a sample-efficient stochastic trust region (STR) algorithm that finds an approximate local minimum within $\tilde{O}(\sqrt{n/\epsilon})$ stochastically Hessian oracle queries. They also develop Hessian-free STR algorithms that achieve the lowest runtime complexity.
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper proposes a method for training deep neural networks that ensures at least one neuron is active at a given layer. The proposed method is based on the interaction of the geometry of the weights with the data: by simply adding one linearly dependent row to the weight matrix, the proposed method ensures that no neurons are dead. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the method."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper studies the problem of certifying the robustness of deep neural networks to adversarial perturbations. In particular, the authors show that if the eigenvalues of the Hessian of the network are bounded, they can compute a robustness certificate in the l2 norm efficiently using convex optimization. Then, they derive a computationally-efficient upper bound on the curvature of a deep network and use it as a regularization term during the training of the classifier to boost its certified robustness against adversarial examples. "
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a method for compressed sensing recovery using untrained deep generative models. The method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. The authors show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. They also introduce a novel learned regularization technique, which incorporates prior information on the network weights."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes TAIC (Temporal Abstraction with Information Constraints), a method for learning temporal abstractions of action sequences in hierarchical reinforcement learning (HRL). The authors formulate the temporal abstraction problem as learning latent representations (called options) over action sequences and propose a novel approach of regularizing the latent space by adding information-theoretic constraints. Specifically, the authors maximize the mutual information between the latent variables and the state changes. The learned representations allow us to do RL at a higher level, and easily transfer knowledge between different tasks."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper proposes a layer-wise sampling strategy to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. It samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, it can potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-irectional influence (correlation). Further, it applies the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper presents a new unsupervised model for video modeling and planning. The proposed model is a combination of an image-based model and a dynamics model. The model explicitly reasons about objects and their positions, velocities, and interactions, and is capable of generating highly accurate video predictions in domains featuring complicated non-linear interactions between objects. The authors also demonstrate the strength of the model as a simulator for sample efficient model-based control in a task with heavily interacting objects."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,This paper proposes a new VAE-GAN model that combines the best of both worlds: sharp and coherent samples and can encode observations into low-dimensional representations. The authors propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. They provide a theoretical analysis of their objective and show that it is equivalent to the Jeffreys divergence. They demonstrate that their model achieves the state-of-the-art trade-off between generation and reconstruction quality.
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper shows that adversarial attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to attacks. The authors present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary, and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. The paper introduces new datasets of realistic images of faces and digits where the Bayesian-optimality classifier can be calculated efficiently. The experiments show that standard CNN training consistently finds a vulnerable classifier, while large-margin methods often find a robust classifier."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the impact of pruning on different classes and images on the accuracy of deep neural network models. The authors identify that certain classes and examples are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both sparse and non-sparse models, and the direction and magnitude of the impact is nuanced and surprising. The results show that some classes are relatively robust to the overall degradation experienced by the model whereas others degrade in performance far more than the model itself."
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper proposes a framework for interactive fiction (IF) games, where the goal is to generate natural language actions that are coherent, contextually relevant, and able to effect the desired change in the world. In order to achieve this goal, the authors propose an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. The key insight is the combination of a knowledge-graph-based state space and a template based action space, which constrains the vast space of possible actions into a compact space of sensible ones. Experiments on 28 IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size."
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper proposes a data-dependent Gaussian prior (D2GPo) that is poles apart from the data-independent Gaussian regularization (L2 regularization) commonly adopted in smoothing the training of maximum likelihood estimation (MLE) for language generation. The proposed method is evaluated on supervised and unsupervised machine translation, text summarization, and image captioning tasks. The results show that the proposed method makes effective use of a more detailed prior in the data and has improved performance in typical language generation tasks."
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper proposes to replace the widely used cross-entropy loss with focal loss to improve the calibration of multi-class classification networks. The authors provide a thorough analysis of the factors causing miscalibration, and use the insights to theoretically justify the empirically excellent performance of focal loss. They perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (SST, 20 Newsgroup) datasets, and with a wide variety of different network architectures, and show that the proposed approach achieves state-of-the-art accuracy and calibration in almost all cases."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper proposes a polynomial optimization framework LiPopt for computing tighter upper bounds on the Lipschitz constant of neural networks. The authors show how to use the sparse connectivity of a network to significantly reduce the complexity of computation. This is specially useful for convolutional and pruned neural networks, and the authors conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the $\ell_\infty$-Lipschits constant, the proposed approach yields superior estimates compared to baselines."
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. The method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). The authors also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition)."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,This paper proposes a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network. The selection masks are jointly optimized together with the corresponding network during the training phase. The experiments show that it is often possible to achieve a good accuracy with significantly less input data required to be transferred.
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a method for simultaneous classification and out-of-distribution detection. The proposed loss function includes two regularization terms where the first minimizes the l1 norm between the output distribution of the softmax layer of a DNN and the uniform distribution, while the second minimises the Euclidean distance between the training accuracy and its average confidence in its predictions on the training set. Experiments on both image and text classification tasks show that the proposed method outperforms the existing methods."
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction. The key idea is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, the authors demonstrate the superior performance of the proposed method."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper proposes a model-based reinforcement learning (RL) setting where the agents internalize their own predictive models of the environment and form a virtual simulation within which the agent plays trials of the episodes in entirety. The agents take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents’ biases vary, the collective policy developed while sequentially visiting the internal simulations complement one another’s shortcomings. In the experiment, the proposed algorithm consistently achieves significantly higher returns than the best individually trained policies."
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper proposes a dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that the proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light-and-shadows in nature. GLAS provides a useful coarseto-fine control benefiting from scalability of Gaussian mask. The authors also devised the ability to identify multiple instances through recursive GLAS. They prove the effectiveness of GLAS for fine-grained classification on the ImageNet dataset."
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"This paper proposes a method to remove both pixel-wise and channel-wise correlations in convolutional neural networks. The proposed method, called network deconvolution, aims to remove the correlation effects via: x = K-1b, assuming K is an invertible matrix. Experiments on CIFAR-10, Cifar-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet demonstrate the effectiveness of the proposed method."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,This paper studies the problem of quantizing generative adversarial neural networks (GANs). The authors propose a quantization method based on EM algorithms and propose a multi-precision approach to find the lowest number of bits for quantizing GAN models to satisfy the quality requirement for generated samples. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GANs to even 1-bit or 2-bit representations with results of quality comparable to original models.
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies last-iterate convergence of Hamiltonian gradient descent (HGD) for convex-concave min-max optimization problems. The main result is that HGD converges globally to the Nash equilibrium of the Dirac-GAN if the matrix A is square, well-conditioned, and sufficiently large compared to the convex loss and convex convex regularizer f. The authors also prove convergence rates for stochastic HGD and Consensus Optimization algorithm of Mescheder et al. (2017)."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of the forward/backward process of ResNet with respect to the number of residual blocks. It shows that for standard initialization used in practice, $\tau$ = 1/\Omega(\sqrt{L}$ is a sharp value in characterizing the stability. The paper also shows that if ResNet is properly over-parameterized, gradient descent is guaranteed to find the global minima 1, which significantly enlarges the range of $\tau\leq 1/ \Omega\left(L\right)$ that admits global convergence in previous work."
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. This approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"This paper proposes a method to control the latent space of a generative model by finding directions in its latent space along which we can move to control specific properties of the generated image such as the position or scale of the object in the image. The method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations such as translation, zoom or color variations. Experiments on GANs and variational auto-encoders demonstrate the effectiveness of the method qualitatively and quantitatively."
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a physics-as-inverse-graphics approach to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. This paper addresses this problem by bringing together vision as inverse graphics and physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"This paper considers the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the clean probability is exploited as a relevance measure. Experimental results show that the proposed method outperforms the transductive approach (Douze et al. 2018) that is using the same additional data without labels."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"This paper proposes Edge Information Maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The authors theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper proposes a method for verifying properties of generative models. The proposed method is based on EXACTLINE, which is a deterministic abstract interpretation method for neural networks. ExactLINE is able to capture sufficient non-convexity so to produce precise bounds on the output. Existing verification methods either fail to scale to generative networks or do not capture enough non convexity. The method is faster and more precise than previous methods for the same networks, including sampling."
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the performance degradation of graph neural networks (GNNs) when the model depth reaches the ""suspended animation limit"", i.e., the limit at which the model will not respond to the training data any more and become not learnable. The authors introduce GRESNET (Graph Residual Network) framework, which creates extensively connected highways to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. They prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper proposes a semi-supervised method for 3D face reconstruction from 2D images. The proposed method is based on linear 3D morphable models (3DMM), which is a non-linear parametric model. The main contribution of this paper is to use adversarial training to train the model with adversarial loss on hybrid batches of unlabeled and labeled face images. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, the proposed model disentangles identity, expression, pose, lighting, and expression representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper proposes a method for imitation learning that replaces the unknown transition kernel with a synthetic kernel that simulates the transition of state components for which the transition kernel is known (s) and extracts from demonstrations the state components that the kernel is unknown (su). The next state is then stitched from the two components: s = {sr, su}. The authors describe in detail the recipe for building an eMDP and analyze the errors caused by its synthetic kernel. The experiments show that combining a policy gradient algorithm with our model achieves superior performance compared to the simulation-free alternative."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self-supervised reinforcement learning method, called Mutual Information-based State-Control (MISC), which is an approach that learns to control states of interest by maximizing the mutual information between the context states and the states of the state of interest. The proposed method is evaluated on two robotic manipulation tasks from OpenAI Gym and a navigation task in the Gazebo simulator. The results show that the proposed method outperforms the baselines in both tasks."
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a more powerful trojaning attack method for large models, which outperforms existing studies in capability, generality, and stealthiness. The attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim’s deployment. One trojaned model on a large-scale dataset can affect applications of different domains that reuses its general features. The trojan shows no biased behavior for different target classes."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"This paper proposes a kernel-based few-shot regression (FSR) algorithm for drug discovery. The proposed algorithm is based on the idea of learning a deep network in combination with a kernel function and a differentiable kernel algorithm. The algorithm learns to find the appropriate kernel for each task during inference. It outperforms current state-of-the-art algorithms on both toy and novel, real-world drug discovery benchmarks."
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper proposes a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. They formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new evaluation metric for conditional generative adversarial networks (cGANs). The proposed metric, called Fréchet Joint Distance (FJD), is defined as the distance between joint distributions of images and conditioning, which allows it to implicitly capture the aforementioned properties in a single metric, i.e., image quality, conditional consistency, and intra-conditioning diversity. The authors conduct experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to currently established metrics. Moreover, they use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper proposes a method to identify decision states, i.e., the parsimonious set of states where decisions affect the future states an agent can reach in an environment. The decision states are discovered without extrinsic rewards – simply by interacting with the world. The authors utilize the VIC framework (Gregor et al., 2016), which maximizes an agent’s ‘empowerment’. They formulate a sandwich bound on the empowerment objective that allows identification of decision states. Their approach yields decision states that align with human intuition across environments, and aid directed exploration on external-reward tasks and subsequently lead to better success rate and sample complexity in novel environments (competitive to Goyal et al. (2019))."
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. The proposed method SEFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. The method is evaluated on multiple healthcare time series datasets and shows that it performs competitively while significantly reducing runtime."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper proposes a new convolution operation, called Harmonic convolution, to improve the performance of audio generative models. Specifically, the convolution kernels are modelled as sets of harmonic series, instead of convolutional kernels. The authors show that the proposed method improves the performance on unsupervised audio restoration tasks and improves the generalization performance on supervised music source separation tasks."
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper proposes a data echoing strategy to reduce the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipelines in order to reclaim idle capacity. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. They find that in all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes a combination of successor features and intrinsic rewards to improve the generalization ability of unsupervised reinforcement learning (RL) methods. In particular, the authors propose Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. The authors empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed briefly after a long unsupervisory phase. Achieving human-level performance on 12 games and beating all baselines, they believe VISR represents a step towards agents that rapidly learn from limited feedback."
SP:83500230586a9134f910ad067b7233dc563dc1ba,"This paper studies the generalization properties of shallow and deep fully connected univariate ReLU networks from a functional point of view. Theoretical and empirical results are provided to support the claim that generalization is due to the smoothness of the functional approximation, combined with a flat initial approximation. The paper is well-written and easy to follow. "
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper proposes a method for image-to-image translation based on the attention mechanism of GANs. Specifically, the authors propose to augment the discriminator with an attention mechanism so that it estimates the probability that its input is real, and also does it create an attention map that highlights the critical features for prediction. This attention map then assists the generator to produce more plausible and realistic images. Extensive experiments are conducted to demonstrate the superiority of the proposed approach."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper explores the role of multiplicative interactions as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions. The authors hypothesize that multiplicative interaction offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. They argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. They back up their claims by applying them in large-scale complex RL and sequence modelling tasks, where their use allows them to deliver state-of-the-art results, and thereby provides new evidence in support of multiplication playing a more prominent role when designing new neural network architectures."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper proposes a new algorithm for batch active learning with deep neural network models. The algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters, making it a useful option for real world active learning problems."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,This paper proposes a feature-leveling network to improve the interpretability of deep neural networks (DNNs) by separating low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. The proposed method is evaluated on CIFAR-10/100 and ImageNet datasets. 
SP:b70ceead1bf6c7dc684c74501716e7012b891022,This paper proposes a method to improve the signal-to-noise ratio of the adversarial negative sampling for extreme classification. The proposed method is based on a scalable approximation to the softmax loss function via a generalized form of negative sampling. The authors prove that this adversarial sampling minimizes the gradient variance while any bias due to non-uniform sampling can be removed. The experimental results on large scale data shows a reduction of the training time by an order of magnitude relative to several competitive baselines.
SP:29b52fee83309268d9864f3b1fc3617948577d41,This paper proposes a novel approach for efficient exploration in reinforcement learning. The proposed approach leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. The approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty. It then leverages these intrinsic rewards for sample-efficient exploration with planning routines in representational spaces. One key element of the approach is that it performs more gradient steps in-between every environment step in order to ensure the model accuracy.
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper proposes two new methods for out-of-distribution detection in the few-shot setting and establish benchmark datasets, based on four popular few shot classification datasets. The authors also propose two new confidence scores for this task and investigate their performance. The proposed confidence scores, -MinDist and LCBO, substantially outperform the baselines on both tasks. The paper is well-written and easy to follow."
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper proposes a generalized model of sequence generation that unifies decoding in directed and undirected sequence models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, the authors derive various neural sequence models as special cases, such as autoregressive, semi-autorgressive, and non-autoregressive models. This unification enables them to adapt decoding algorithms originally developed for directed sequence models to unify decoding in both directed and unsupervised sequences. The authors evaluate various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019) on WMT'14 English-German translation."
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper proposes a two-stage approach for the recognition of mathematical expressions (MEs) from images. In the first stage, this paper uses object detection algorithm to identify the math symbols of the input image, and in the second stage, it translates math symbols with position information into LaTeX sequences by a seq2seq model equipped with attention mechanism. The authors claim that the detection of mathematical symbols and the structural analysis of mathematical formulas are carried out separately in two steps, which effectively improves the recognition accuracy and enhances the generalization ability. The experimental results show that the proposed method significantly outperforms the end-to-end method."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,This paper proposes a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. Experiments on ResNet-50 and Mask R-CNN show that the proposed method can compress the model to a memory size of 5 MB while preserving a top-1 accuracy of 76.1% on ImageNet object classification.
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper proposes a new Transformer-based model that incorporates Tensor-Product Representations (TPR) into the transformer architecture to better support the explicit representation of relation structure. The authors propose a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The proposed model is trained end-to-end and infers the correct answer for novel examples without any task-specific structural biases. On the Mathematics Dataset, the proposed model beats the previous state-of-the-art by 8.24%."
SP:d319df820c6630c409fab32097652a083e8f53ea,"This paper studies the problem of generalization in deep learning. The authors derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity, and formulate an optimization problem to learn a more general classification function by extending the input features by concatenating encodings of them, and then train the classifier on the extended features. Theoretical findings and experimental results show that a learned classification function must be sufficiently complex for a classification task in order to be closer to the true classification function. "
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper proposes a new graph pooling operation based on compressive Haar transforms, called HaarPooling, to improve the performance of graph convolutional neural networks (GNNs) in graph classification and graph-based regression tasks. The proposed method is based on following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressed Haar basis of the corresponding clustering. It operates in the frequency domain by the synthesis of nodes in the same cluster and filters out fine detail information by compressive haar transforms. The authors show that the proposed method achieves state-of-the-art performance on diverse graph classification problems."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper proposes a sample-based point-cloud decoder that maps shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. The authors develop three sample based decoder architectures and compare their performance to each other and show their improved effectiveness over feedforward architectures. The paper also provides an open-source software platform to reproduce these results and serve as a baseline for future work."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper presents a large-scale study on the generalization ability of deep neural networks (DNNs) on real-world label-flipping noise. The main contribution of this paper is the introduction of a benchmark of real-real-world noisy labels at 10 different noise levels (0-80% to 0-100%). The main findings are that: (1) DNNs generalize much better than on synthetic labels, and (2) when networks are fine-tuned, ImageNet architectures generalize well on noisy data. (3) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (4) Robust learning methods that work well on synthetic noise may not work as well on real world noise, and vice versa."
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled that it is both natural for humans and synergistic for learning. The authors propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,This paper proposes a memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. The authors also show that the learned representations could correspond to chemical features in the molecule data.
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper studies the effect of initialization in deep linear networks, and provides a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. They show that for deep networks, the width needed for efficient convergence to a global minimum is independent of the depth. Their results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,This paper studies the problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. The authors propose an efficient method to solve the optimization problem via Lagrangian Formulation. Their method obtains excellent results on deep neural networks. It can compress deep CNN ResNet-50 down to 2 bits with only 0.7% accuracy loss.
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"This paper proposes a generative inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder and generative network using an iterative primal dual optimization process. The authors establish the generalization error bound of iWAGN and provide a rigorous probabilistic interpretation of our model under the framework of maximum likelihood estimation. The empirical experiments show that the proposed method mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample."
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper proposes an extension of the mention pair model of anaphoric annotation (MPA) to alleviate the effects of sparsity inherent in some crowdsourcing environments. Specifically, the authors use a nonparametric partially pooled structure (based on a stick breaking process), fitting jointly with the ability of the annotators hierarchical community profiles. The individual estimates can thus be improved using information about the community when the data is scarce. The authors show, using a recently published large-scale crowdsourced anaphora dataset, that the proposed model performs better than its unpooled counterpart in conditions of data sparsity and on par when enough observations are available."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes a method to learn intrinsic and extrinsic rewards for exploration in sparse-reward reinforcement learning. The authors propose a hierarchical intrinsic reward (SID) and successor feature control (SFC) which is a general intrinsic reward that takes into account statistics over complete trajectories. The proposed method is evaluated on VizDoom, DeepMind Lab and DeepMind Control Suite. The results show that SID and SFC can improve the exploration efficiency."
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper proposes a method for video moment retrieval in the weakly-supervised setting, where only video-sentence pairs are provided. The proposed method, called wMAN, is composed of a frame-by-word interaction module and a Word-Conditioned Visual Graph (WCVG), which is used to learn richer multimodal representations at fine-grained word and frame-level. The method is evaluated on DiDeMo and Charades-STA datasets, and achieves state-of-the-art performance."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a method for image-guided re-rendering of reconstructed objects. The main idea is to train an object-specific deep neural network to synthesize the view-dependent appearance of an object using an RGB video of the object. The video is used to reconstruct a proxy geometry of the reconstructed object via multi-view stereo. Based on this proxy geometry, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, which can lead to artifacts. To deal with this issue, the paper proposes EffectsNet, a deep network that predicts view-independent effects. It is able to convert observed images to diffuse images, which are then projected into other views. In the target view, the pipeline reinserts the views-dependent effects. To composite multiple reprojected images to a final output, they learn a composition network that outputs photo-realistic results. They demonstrate the effectiveness of their method in a variety of experiments."
SP:257d124367b1da9a595dc11a9df750d6bade298e," the Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. The authors also devise a novel low-rank approximation of this eigendecomposition that exploits spectral sparsity of DNNs. Both theoretical analysis and empirical evaluations over various benchmarks show the superiority of the proposed approach over existing methods."
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes a load-balanced hashing method to improve the performance of One Permutation Hashing (OPH) method. In particular, the authors propose to balance the load of the bins (the number of elements in a bin) so as to reduce the empty bins in advance. The proposed method can generate as few empty bins as possible. The experiments on real datasets validate the claim."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,This paper proposes a feature extraction method for periodic signals. The authors propose a method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. The paper demonstrates experimentally the effectiveness of the proposed method.
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"This paper proposes a confidence-oriented decoder for data-to-text generation, which assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on the structured data to text dataset WikiBio show that the proposed approach is more faithful to the source than existing state-of-the-art approaches, according to both automatic metrics and human evaluation."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"This paper proposes a lookahead pruning (LAP) method for neural network pruning, which is a generalization of magnitude-based pruning and its variants. LAP is motivated by the observation that the Frobenius distortion of a single-layer operation can be approximated by the lookahead distortion of the entire network. Theoretically, the authors show that LAP can be viewed as a solution to the minimization of the Frobensius distortion for a single layer operation, and propose an efficient algorithm to solve the optimization problem. Empirically, LAP outperforms magnitude based pruning on various networks, including VGG and ResNet."
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper proposes Moniqua, a technique that allows decentralized SGD to use quantized communication. Theoretical results show that the proposed method is able to communicate a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Empirical results on CIFAR-10 show that it converges faster with respect to wall clock time than other quantized decentralized algorithms."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper introduces a general family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations. The authors also propose a simple, yet effective, modification to partial models so that they can still make correct predictions under changes in the behavior policy, which they validated theoretically and experimentally. The proposed modifications address the correctness of the model against policy changes, but don’t addresses the correctness/robustness against other types of intervention in the environment."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a new variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed model is based on two information theoretic problems—distributed simulation and channel synthesis, in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. The paper decomposes a pair of correlated variables into their common representation (e.g., a shared concept) and local representations that capture the remaining randomness in respective data variables by imposing the mutual information between the data variables and the common representations as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images."
