paper_id,summary
SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,The paper proposes sqSGD (selective quantized stochastic gradient descent) for federated learning under local differential privacy constraints. The proposed algorithm is based on a novel privacy-preserving quantization scheme that uses a constant number of bits per dimension per client. The paper also proposes a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Experiments on benchmark datasets demonstrate the effectiveness of the proposed algorithm.
SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a simple representation method for self-attention networks (SANs) to consider prior knowledge related to language representation from the beginning of training. The proposed method allows SANs to leverage prior knowledge in a universal way compatible with neural networks. The authors apply it to one prior word frequency knowledge for monolingual data and other prior translation lexicon knowledge for the bilingual data, respectively, thereby enhancing the language representation. Experimental results on WMT14 English-to-German and WMT17 Chinese-toEnglish translation tasks demonstrate the effectiveness and universality of the proposed method over a strong Transformer-based baseline."
SP:e3e728837f26acb9da283a42c219b6c3b3e131cb," game is a classic game of cat-and-mouse, where the attacker and the defender play a leader-follower game. In this paper, the authors propose Bayesian Stackelberg Markov Games (BSMGs) to model uncertainty over attacker types and the nuances of an MTD system. They also propose a Bayesian Strong Stackeberg Q-learning (BSS-Q) approach to learn the optimal movement policy for BSMGs within a reasonable time. They show that their learning approach converges to an SSE of a BSMG and then highlight that the learned movement policy improves the state-of-the-art in MTD for web-application security and converges."
SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper proposes a VAE-based framework for unsupervised disentangled representation learning. The authors propose a simple yet effective VAE ensemble framework consisting of multi-variational VAEs. It is based on the assumption that entangled representations are unique in their own ways, and the disentanglement representations are “alike” (similar up to a signed permutation transformation). In the proposed ensemble, each model not only maintains its original objective, but also encodes to and decodes from other models through pair-wise linear transformations between the latent reprere sentations. The paper shows both theoretically and experimentally that the proposed framework encourages the linear transformations connecting the VAEs to be trivial transformations, aligning the latent representations of different models to be “like”."
SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The proposed method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The graph neural network generalizes to new datasets and new sets of datasets. Performance is competitive with state-of-the-art AutoML systems while reducing running time and prediction time from minutes to milliseconds.
SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the effect of gradient descent on the compositionality of neural networks. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. This is caused by gradient descent, trying to use all available and redundant information from input, violating the conditional independence property of compositionality. Based on this finding, the authors suggest that compositional learning approaches considering only model architecture design are unlikely to achieve complete compositionality, and propose a new approach based on compositional generalization. The paper is well written and easy to follow."
SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper studies the representation learning for entity alignment in the context of knowledge graph (KG) representation learning. The authors propose a new approach that explicitly learns KG-invariant and principled entity representations, while preserving the original infrastructure of existing methods. The model not only pursues the closeness of aligned entities on geometric distance, but also aligns the neural ontologies of two KGs to eliminate the discrepancy in feature distribution and underlying ontology knowledge. The experiments demonstrate consistent and significant improvement in performance against the existing embedding-based entity alignment methods."
SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes SACoD, a sensor algorithm co-design framework to develop more efficient CNN-powered PhlatCam. In particular, the mask in the sensor and the backend CNN model are jointly optimized in terms of both model parameters and architectures via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed framework achieves aggressive model compression and energy savings while maintaining or even boosting the task accuracy."
SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a temporal matrix factorization model to learn the average developmental path and structured variations of individuals in the social network over their entire lives in two honey bee colonies. The method is based on a unique dataset containing lifetime trajectories of all individuals over multiple generations in the two honey bees. The proposed method yields interpretable embeddings that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived."
SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a pipeline for data augmentation for image reconstruction tasks arising in medical imaging. In particular, the authors propose a pipeline to reconstruct an image from a few under-sampled linear measurements. The proposed pipeline is inspired by the success of Data Augmentation (DA) for classification problems in this paper. The authors show that the proposed pipeline can achieve comparable performance to the state-of-the-art on the fastMRI dataset while using significantly fewer training data."
SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. DRC is based on order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. Then, it group object instances into clusters according to their identity features using a repulsive term. Moreover, it estimates the rank of a test instance by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm can cluster ordered data effectively and also yield excellent rank estimation performance."
SP:a5775441639529d61b7fee4b4298fd82a0c93bb5," is a simple yet effective exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behavior. Experiments are conducted on several procedurally generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks."
SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) that learns to align the representations in the semantic space and the visual space. The proposed method is based on the idea of isometric propagation network (IPN), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within the two space and regularizes the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. The method achieves state-of-the-art performance on three popular ZSL benchmarks. "
SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes HyperGrid Transformers, a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, they propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. They conduct an extensive set of experiments on GLUE/SuperGLUE. On the SuperGLUE test set, they match the performance of the state-of-the-art while being 16 times more parameter efficient."
SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper proposes an algorithm to improve the overall performance of the task of “learning to steer” by analyzing the sensitivity of the learning algorithm with respect to varying quality in the image input for autonomous driving. Using the results of sensitivity analysis, the authors propose an algorithm that is able to enhance the learning outcomes up to 48%. A comparative study drawn between the approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of the proposed algorithm."
SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), an algorithm to enforce the hard constraints of large optimization problems with hard constraints. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. The method is evaluated on two synthetic optimization tasks and a real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid."
SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"-based regularization has been used to learn sparsity in deep neural network pruning. In this paper, the authors propose an L2 regularization variant with rising penalty factors and show it can bring significant accuracy gains compared with its one-shot counterpart, even when the same weights are removed. The growing penalty scheme also brings an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning settings."
SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper studies the role of planning in model-based reinforcement learning (MBRL) algorithms. Specifically, the authors study MuZero, a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. They perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Their results suggest that planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. They also show that using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks."
SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"The paper proposes a novel value iteration network (VIN) model for deep reinforcement learning. The proposed model is based on a combination of contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning. The authors show that the proposed model outperforms VIN-like models when the underlying MDP is discrete, fixed and known. "
SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the inductive bias of neural networks in learning read-once DNFs. The authors first show that the learned neurons are aligned with the terms of the DNF. Then, the authors propose a method to reconstruct the learned DNF from the learned network. Finally, they provide theoretical insights on the learning process and the optimization to better understand the resulting inductive biases."
SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes Graph Structured Reinforcement Learning (GSRL), a method for learning to use graph structure in historical trajectories to slowly adjust exploration directions and rapidly update value function estimation with related experiences. GSRL constructs a dynamic graph on top of state transitions in the replay buffer based on historical trajectory and develops an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. Results demonstrate that GSRL outperforms the state-of-the-art algorithms in terms of sample efficiency on benchmarks with sparse reward functions."
SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes Prioritized Level Replay, a method for estimating the future learning potential of a level given the current state of the agent’s policy. The method is based on the idea that different levels provide different learning progress for an agent at specific times during training. The authors show that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning capacity when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it. They report significantly improved sample-efficiency and generalization on the majority of Procgen Benchmark environments."
SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic approach to perform fine-grained manipulation of auxiliary task gradients. The authors propose to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. This allows weighting the update directions differently depending on their impact on the problem of interest. The method leverages efficient automatic differentiation procedures and randomized singular value decomposition for scalability. The proposed approach consistently outperforms strong and widely used baselines on Text and Image classification tasks."
SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper studies the problem of one-shot word learning in a simulated 3D world, where the agent is placed in a 3D environment and given an object in the environment. The agent is given a set of objects to manipulate, and the goal is to manipulate the object as instructed by the agent. The paper shows that the agent can learn to manipulate a given object by combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. It also shows that, under certain training conditions and with a particular memory writing mechanism, the agent’s word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. "
SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of class-imbalance in few-shot learning. The authors study the effect of dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. They extensively compare over 10 state-of-the-art few shot learning methods using backbones of different depths on multiple datasets. They show that compared to the balanced task, the performances of their class imbalance counterparts always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric based methods generally suffer less."
SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper studies the problem of graph convolutional neural networks (GCNs). The authors propose a novel approach, called Polynomial Graph Convolution (PGC), which is based on the idea that each node in the graph has a different receptive field and can be represented by a decoupled representation. The authors show that the proposed PGC is more expressive than the most common convolution operators and their linear stacking. They also prove that the common way multiple non-linear graph convolutions are stacked limits the neural network expressiveness. "
SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a method for sim-to-real transfer for scene graph generation. The authors propose to decompose the domain gap into appearance, label and prediction discrepancies between the two domains by introducing pseudo statistic based self-learning and adversarial techniques. Experiments on toy simulators and real-world data demonstrate the effectiveness of the proposed method."
SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,This paper proposes a model-free DRL algorithm for continuous-action Q-learning using a low update-to-data (UTD) ratio. The main idea is to use an ensemble of Q functions and in-target minimization across a random subset of the Q functions from the ensemble. The proposed algorithm is evaluated on MuJoCo benchmark and shows that it outperforms the state-of-the-art model-based DRL algorithms. 
SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a new neural network architecture, named DIDA, that is invariant under permutation of the features. The proposed architecture inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. The paper also empirically demonstrates the merits of DIDA on two tasks defined at the dataset level. On both tasks, DIDA learns meta-features supporting the characterization of a (labelled) dataset. The first task consists of predicting whether two dataset patches are extracted from the same initial dataset. On the second task, it predicts whether the learning performance achieved by a hyper-parameter configuration under a fixed algorithm (ranging in k-NN, SVM, logistic regression and linear SGD) dominates that of another configuration."
SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a graph learning method based on graph Laplacian-like Lasso to learn ultra-sparse undirected graphs from potentially high-dimensional input data. The key idea is to learn the spectral embedding (or approximate effective-resistance) distances on the graph will encode the similarities between the original input data points. By interleaving the latest highperformance nearly-linear time spectral methods, ultrasparse yet spectrally-robust graphs can be learned by identifying and including the most spectratically-critical edges into the graph. Compared with prior state-of-the-art graph learning approaches, GRASPEL is more scalable and allows substantially improving computing efficiency and solution quality of a variety of data mining and machine learning applications."
SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a novel unsupervised learning approach named goal-conditioned policy with intrinsic motivation (GPIM), which jointly learns both an abstract-level policy and a goal-conditional policy. The abstract- level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal-constrained policy. This discriminator serves as an intrinsic reward function to imitate the trajectory induced by the abstract level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed GPIM method."
SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning. The authors propose an adaptive policy switching criterion based on the feature distance between the deployed Q-network and the underlying learning Q-networks. They show that the proposed criterion significantly decreases the switching cost while maintaining a similar sample efficiency to the case without the low-switching-cost constraint. They also provide a theoretical justification from a representation learning perspective. 
SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a first-order stochastic algorithm based on homotopy-stochastic gradient descent (H-SGD) for solving large-scale non-convex optimization problems. The main contribution of the paper is a theoretical analysis of the proposed algorithm under some mild assumptions on the problem structure. Theoretical analysis shows that with a specifically designed scheme for the homotopic parameter, the proposed method enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations confirm the theoretical results. "
SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning approach for estimating the 3D shape of real-world objects from sparse point clouds. The key idea is to learn the posterior distribution of a latent representation conditioned on the sparse point cloud, and then use this latent representation to estimate the shape of a newly-encountered object from sparse observations. The proposed approach is evaluated on the standard ShapeNet and ICL-NUIM benchmarks. "
SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper studies the activation of adversarial examples in deep neural networks (DNNs) from a channel-wise activation perspective. The authors find that the activation magnitudes of the adversarial example are higher than that of natural examples, and that the channels are activated more uniformly by adversarial images than natural examples. They propose a method called Channel-wise Activation Suppressing (CAS) to train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness. "
SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,This paper studies the generalization performance of neural networks trained via gradient descent with random initialization. The main contribution of the paper is to provide a non-asymptotic analysis of the convergence rate of the squared loss of the gradient flow on the network parameters. The authors show that the loss converges exponentially at a rate that depends on the level of imbalance of the initialization. They also provide a novel upper bound on the operator norm distance between the trained network and the min-norm solution of the network.
SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of ReLU activations in deep networks. The authors show that the eigenvalue decay of the ReLU operator is the same for deep networks as it is for shallow networks. They show that for ReLU activation, deep networks have essentially the same approximation properties as their shallow two-layer counterparts. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. "
SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper studies the problem of continuous control through deep reinforcement learning. The authors propose a novel algorithm that can minimize the overestimation, avoid the underestimation bias and retain the policy improvement during the whole training process. Specifically, they add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which they provide theoretical and experimental guarantees for future policy improvement. They evaluate their method on a set of classical control tasks and show that the proposed algorithms are more computationally efficient and stable."
SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper studies the inverse reinforcement learning (IRL) problem, where the goal is to recover the reward functions from expert demonstrations. In this paper, the authors generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse RL (SIRL). They adopt the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the probability distribution as the first solution to the SIRL problem. The solution is succinct, robust, and transferable for learning task and can generate alternative solutions to IRL problems. Through their formulation, it is possible to observe the intrinsic property for the I RL problem from a global viewpoint, and their approach achieves a considerable performance on the objectworld."
SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervision learning. The main contribution of the paper is to provide a unified analysis of the generalization bound for self-trained neural networks. The generalization bounds are based on a simple but realistic assumption that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. The authors prove that under these assumptions, the minimizers of population objectives based on self training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. "
SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method to predict the long-term future of video frames by first estimating a sequence of semantic structures and subsequently translating the structures to pixels by videoto-video translation. The proposed method is based on a stochastic recurrent estimator with a discrete semantic structure space. The authors show that the proposed method can generate complex scene structures and motions over a very long time horizon (i.e., thousands frames) with orders of magnitude longer prediction time than existing approaches. "
SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new method for graph neural networks (GNNs) based on the iterated function system (IFS). The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. The paper shows that if the IFS induced by IGNNs is contractive, then the fractal representation of the graph nodes converges to the set of IFS in Hausdorff distance and the ergodic representation of that converges in Frobenius norm. The experimental results show that the proposed method is obviously better than the related methods."
SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,This paper proposes a geometric graph generative adversarial network (GG-GAN) to solve the problem of graph generation from a geometric perspective by associating each node with a position in space and then connecting edges in-between based on a similarity function. The main contribution is a Wasserstein GAN that is permutation equivariant and easily scales to generate graphs of tens of thousands of nodes. The proposed method also strikes a good trade-off between novelty and modeling the distribution statistics.
SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a novel method for exploring how neurons within a neural network interact. In particular, they consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they formalize the problem in terms of the Minimum Description Length principle, by which they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world: they identify shared, resp. class-specific traits, compositionality within the network, as well as locality in convolutional layers."
SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. The main contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for na’ve approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world, and derive families of algorithms which follow this principle. The theoretical findings are validated by experiments on a tabular gridworld and deep learning experiments on four MinAtar environments."
SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient ALF solver for neural ODEs. The proposed method is based on the asynchronous leapfrog (ALF) solver, which has a constant memory cost w.r.t. number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). The authors validate MALI in various tasks, including image recognition, time series modeling, and continuous generative models. "
SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provides an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) generalizing to unseen conditions composed of unseen object combinations. The authors observe that recent methods are able to generate recognizable scenes given seen conditions, and exploit compositionality to generalise to unseen conditioned conditionings with seen conditions. However, all methods suffer from noticeable image quality degradation when asked to generate images from unseen conditions. Moreover, through their analysis, the authors identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditioning, using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process, and enhancing the quality of generated masks and the individual objects are crucial steps to improve robustness."
SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper studies the expressive power of Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), a variant of Graph Neural Networks (GNNs) that first augments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. The authors show both theoretically and numerically that GA-MLP with suitable operators can distinguish almost all non-isomorphic graphs, just like the WeisfeilerLehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, the authors prove a separation in expressive power that grows exponentially in depth. In particular, unlike GNN, GA- MLPs are unable to count the number of attributed walks. They also demonstrate via community detection experiments that GAMLPs can be limited by their choice of operator family."
SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes a continual distillation method to transfer learning progress from a large capacity learner model to a small capacity actor model in partially-observable environments. The main contribution of the paper is to propose an actor-learner distillation (ALD) procedure that leverages a continual form of distillation to transfer the learning progress of the learner to the actor model. The proposed method is evaluated in two environments, where transformer models have had large improvements over LSTMs recently, but at the cost of significantly higher computational complexity. "
SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for few-shot image classification. URT is a meta-learning approach to leverage universal features to leverage domain-specific features for the task. The proposed method is evaluated on Meta-Dataset and shows state-of-the-art performance on the Meta-dataset. 
SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes an approach for learning representations from a stream of unlabeled data in which the number of object classes increases with time. The authors propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM), to solve the Unsupervised Progressive Learning (UPL) problem. To solve the UPL problem, STAM learns based on a combination of clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to describe the STAM architecture and evaluate the latter in UPL context. "
SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized training of deep learning models. The authors study the effect of network size, communication topology, and data partitioning on the performance of decentralized training. They show that when the consensus distance between devices does not grow too large, centralized training can be reached and sometimes surpassed. They also highlight the intimate interplay between network topology and learning rate at the different training phases. "
SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,This paper proposes a novel siamese Gated Recurrent Unit architecture for multi-variate sequence representation learning. The main idea is to learn a similarity metric between two sequences of the same length using a Gated Reactive Unit (GRU) network. The similarity metric is learned by learning the distance between the two sequences. The authors show that this similarity metric can be used to improve the performance of the proposed model. 
SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation on the performance of distributed models in a distributed learning setting. The authors propose to use codistillations to encourage the models to share knowledge among concurrently trained models by encouraging them to represent the same function through an auxiliary loss. They find that even at moderate batch sizes, models trained with codistills can perform as well as models trained using synchronous data-parallel methods, despite using a much weaker synchronization mechanism. The findings hold across a range of batch sizes and learning rate schedules, and different kinds of models and datasets."
SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The main contribution of the paper is a theoretical analysis of the effect of the Hessian of the loss at the minimum and the choices of the algorithm parameters. The authors show that the SGD iterates will converge to a heavy-tailed stationary distribution with infinite variance. They further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. Finally, the authors conduct experiments on both synthetic data and fully connected neural networks."
SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226," for community detection. This paper studies the effect of bandpass filtering on the performance of community detection models under spectral manipulations. The authors show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection, and it is possible to obtain state-of-the-art accuracy with simple classifiers that rely only on a few low frequencies."
SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,This paper proposes a method for learning graph structure using self-supervised learning. The main idea is to learn the graph structure and GNN parameters simultaneously. The proposed method is based on the Simultaneous Learning of Adjacency with Self-supervision (SLAPS) method. The authors show that the proposed method outperforms several baselines on several benchmark datasets.
SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel detection method for continual learning. The novelty detection method is based on the idea that the network confusion caused by training incoming data as a new class will cause the network to misclassify the incoming data. To this end, the authors propose a novel class-imbalance detection method to detect the new class. The proposed method is evaluated on a variety of image classification benchmarks. "
SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a framework for learning to interpret natural language constraints for safe RL. To this end, the authors propose a new multi-task benchmark that requires an agent to optimize reward while not violating constraints specified in free-form text. They then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Their model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. They show that their method achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) compared to existing approaches."
SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a meta-learning approach for few-shot semantic edge detection, which aims to localize boundaries of novel categories using only a few labeled samples. The proposed approach is based on a class-agnostic approach where the semantic segmentation mask is used to generate an attention map to highlight the target object region, and the decoder module is made to focus on that region. The authors also propose a new regularization method based on multi-split matching. "
SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method for generating causal explanations for graph neural networks (GNNs) based on the perspective of cause-effect. In particular, the authors propose a method called Causal Screening, which incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. The authors show that the proposed method can be used to generate faithful and concise explanations for any GNN model. "
SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure of network simplicity based on the smallest fraction of the network's parameters that can be kept while pruning without adversely affecting its training loss. The authors show that this measure is highly predictive of a model’s generalization performance across a large set of convolutional networks trained on CIFAR-10. They also study the mutual information between the predictions of their new measure and strong existing measures based on models’ margin, flatness of minima and optimization speed."
SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a novel method for learning discrete representations for long-horizon tasks. The authors propose to learn a sequence of abstract states for a low-level model-predictive controller to follow. They show that the proposed method is able to learn discrete representations from exploratory video data in an unsupervised fashion via a mutual information maximization objective. 
SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit level sparsity. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer to induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed precision quantization scheme of the original model. The method enables the exploration of the full mixed precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. "
SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7, adversarial attacks on quantized networks have been shown to suffer from gradient vanishing issues and show a fake sense of robustness. This paper proposes a simple temperature scaling approach to mitigate this issue while preserving the decision boundary. Experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate the effectiveness of the proposed method.
SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel approach to interpretable recurrent neural networks (RNNs) based on the concept of prototype trajectories. The idea is to find the most similar prototype for each sentence in a text sequence and feed an RNN with the proximity of each of the sentences to the prototypes. The RNN backbone then captures the temporal pattern of the prototypes, to which we refer as prototype trajectory, to enable intuitive, fine-grained interpretation of how the model reached to the final prediction. Experiments conducted on multiple public data sets reveal that the proposed method not only is more interpretable but also is more accurate than the current state-of-the-art prototype-based method."
SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper studies the problem of learning a hidden Markov model (HMM) with additional covariate information. The authors propose a special case of recurrent neural networks (RNNs) called HMRNNs, and prove that each HMRN has the same likelihood function as a corresponding discrete-observation HMM. They show that the parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. They then demonstrate how the HMRnn can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
SP:6355337707f1dd373813290e26e9c0a264b993f9," RNA-Seq is an important tool for studying the genetic organization of neurons. The authors propose a novel method to analyze the gene expression data in the context of neuronal phenotypic characteristics and their interactions. The proposed method, called factorized linear discriminant analysis (FLDA), seeks a linear transformation of gene expressions that varies highly with only one phenotypical factor and minimally with the others. They further leverage their approach with a sparsity-based regularization algorithm to select a few genes important to a specific feature or feature combination. They applied this approach to a single-cell RNA-seq dataset of Drosophila T4/T5 neurons. "
SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper proposes a new approach for interpretability of saliency maps. The proposed approach is based on a variational approximation of the posterior distribution over the saliency map. The posterior distribution is estimated by using the likelihood function that measures the distance between the classifier’s predictive probability of an image and that of a perturbed image. For the prior distribution, the authors make attributions of adjacent pixels have a positive correlation, and show that the approximate posterior is effective in explaining the classifiers’ behavior."
SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks."
SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a novel method to certify models under l2 adversarial perturbations. The proposed method is based on sample-wise randomized smoothing, which assigns different noise levels to different samples. Specifically, the authors propose a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. The experimental results demonstrate that the proposed method can achieve better accuracy-robustness trade-off in the transductive setting."
SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, the proposed method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The authors derive a novel reduced entropy condition approximate inference method that results in rich posteriors. They test their model in the data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods."
SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a method to incorporate multi-hop context information into attention computation in graph neural networks (GNNs). The main idea is to compute attention between nodes that are not directly connected to each other. The proposed method uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. Experimental results on node classification and knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results."
SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. While most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy."
SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"This paper proposes a pre-training approach for table semantic parsing based on synchronous context-free grammar (SCFG) to construct synthetic question-SQL pairs over high-quality tables via a synchronous SCFG. The authors propose to pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table-semantic parsing into the pre-trained language model. To maintain the model’s ability to represent real-world data, the authors also include masked language modeling (MLM) on several existing table-and-language datasets to regularize the training process. When incorporated with strong base semantic parsers, the proposed method achieves new state-of-the-art results on four popular fully supervised and weakly supervised tasks."
SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper studies the performance of the least-square support vector machine multi-task learning (LS-SVM MTL) method in the limit of large (p) and numerous (n) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance is shown to converge, as n, p→∞, to a deterministic limit involving simple (small-dimensional) statistics of the data. The authors prove that the standard MTL LS-SVMs are in general strongly biased and may dramatically fail (to the point that individual single-task LS-VMs may outperform the MTL approach, even for quite resembling tasks). The analysis provides a simple method to correct these biases, and that the sufficient statistics at play in the method can be efficiently estimated. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that the proposed method is computationally-efficient."
SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper proposes a group equivariant conditional neural process (EquivCNP) that is permutation-invariant and group-equivariant in the sense of group symmetries. The authors provide a decomposition theorem for permutation invariance and group symmetry, which leads them to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetry. They show that the proposed method achieves comparable performance to conventional CNPs in a 1D regression task and zero-shot generalization."
SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes a novel approach for offline text generation based on off-policy learning from expert demonstrations. The key idea is to use an importance-based weighting to up-weight confident tokens and down-weight unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. The proposed method is evaluated on three tasks: summarization, question generation, and machine translation. "
SP:e77eca51db362909681965092186af2e502aaedc,"-based end-to-end (E2E) training of deep networks usually suffers from high GPU memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. The paper shows that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, the paper proposes an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to e2E training."
SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a novel approach to improve the expressive power of graph neural networks (GNNs). The proposed approach is based on the idea of diverse sampling, i.e., the representation of target node is obtained by aggregating the representations of diverse neighborhoods obtained using any GNN model. Experiments are conducted on three benchmark datasets and on a dataset collected in this paper. Extensive experiments demonstrate the proposed method consistently improve the performance of base GNN models."
SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the robustness of video models to bit-level network and file corruptions, which can arise from network transmission failures or hardware errors, and explore defenses against such corruptions. They find that corruption-agnostic defenses such as adversarial training have limited effectiveness, performing up to 11.3 accuracy points worse than a no-defense baseline. In response, they propose Bit-corruption Augmented Training (BAT), a corruption-aware baseline that exploits knowledge of bit level corruptions to enforce model invariance to such corruption. They show that BAT outperforms corruption-AGnostic defenses on highly-corrupted videos while maintaining competitive performance on clean/near clean data."
SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c," of time-varying graphs. This paper proposes to use skip-gram embedding to perform implicit tensor factorization on different tensor representations of time varying graph. The authors show that higher-order skipgram with negative sampling (HOSGNS) is able to disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. They empirically evaluate their approach using time-resolved face-to-face proximity data, showing that the learned representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction. They also show the potential of this new method to estimate contagion risk, providing early risk awareness based on contact tracing data."
SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a new skip-tree task to evaluate the ability of self-supervised language models to perform logical reasoning. The paper proposes to train language models for formal mathematics by training them on a skip-sequence task, where the goal is to infer the types of missing assumptions and complete equalities. The authors propose to train the language models on the skip-trees task, and show that models trained on the task show surprisingly strong mathematical reasoning abilities, and outperform models training on standard skipsequence tasks."
SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b," gradients are a type of gradient masking. This paper studies the effect of imbalanced gradients on the adversarial robustness of defense models. The authors propose to use a margin decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. The MD attacks are tested on 12 state-of-the-art defense models and find that models exploiting label smoothing easily cause imbalanced gradient, and on which our MD attacks can decrease their PGD robustness by over 23%."
SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a graph-to-sequence deep learning system to generate axiomatic proofs of equivalence between program pairs. The system is trained using incremental graph to sequence networks. The paper shows that the system achieves 93% average true positive coverage on 10,000 test cases while ensuring zero false positives by design. "
SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a method to reduce the parameters of multimodal Transformers in the context of audio-visual video representation learning. The authors propose to decompose the Transformer into modality specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. They show that their approach reduces parameters of the Transformers up to 97% allowing us to train our model end-to-end from scratch."
SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes a new online few-shot learning setting where there is no separate training and testing phase, and instead models are evaluated online while learning novel classes. In this setting, the authors propose a new dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. The authors also propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) in the context of distribution shift and changing training data, when temporal graphs evolve over time. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs. They show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph. "
SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,This paper proposes a method to regularize the representation feature space by comparing representation similarity across different pairs of states. The proposed method is based on the implicit feedback between state and action from the agent’s experience. This constraint helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. The method is tested on the OpenAI ProcGen benchmark and shows significant improvement on generalization performance.
SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies the problem of adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting. In this setting, a small set of nodes are perturbed by perturbing the features of a small number of nodes, with no access to model parameters and model predictions. The paper draws a connection between this type of attacks and an influence maximization problem on the graph, and proposes a set of attack strategies. The proposed strategies significantly degrade the performance of three popular GNN models and outperform baseline adversarial attack strategies in the experiments."
SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of learning the adjacency matrix of directed acyclic graphs (DAGs) in the presence of sparse graphs. The authors propose to exploit a low rank assumption regarding the (weighted) adjacence matrix of a DAG causal model to mitigate this problem. They demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low rank assumptions. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes an end-to-end AutoML algorithm to jointly optimize automated data augmentation (DA), neural architecture search (NAS) and hyper-parameter optimization (HPO). The authors propose a differentiable joint optimization solution (DiffAutoML) to solve the problem of joint optimization of these components in an end to end manner without the need of model retraining. The proposed method achieves state-of-the-art results on ImageNet compared with end- to-end autoML algorithms."
SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends prior networks to regression tasks by considering the Normal-Wishart distribution. In particular, the authors propose Regression Prior Networks (RPN) which is a variant of the prior network that is used for classification tasks and Ensemble Distribution Distillation (EnD) to distill an ensemble of models into a single model. The proposed method is evaluated on synthetic data, selected UCI datasets, and two monocular depth estimation tasks and shows competitive performance with ensemble approaches. "
SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The proposed framework incorporates MAML into a Bayes online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed framework can effectively prevent catastrophic forgetting.
SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a new graph neural network (GNN) architecture based on local neighborhood pooling. The authors show that the proposed network can count subgraphs of size k and thereby overcomes a known limitation of low-order GNNs. They also show that, in several cases, the proposed architecture can greatly reduce computational complexity compared to the existing higher-order k-GNN and Local Relational Pooling (LRP) networks."
SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the behavior of multi-player games where the number of agents is arbitrary and the reward function can be arbitrary. In particular, the authors show that in two-person zero-sum games, if agents employ one of the most well-known learning algorithms, Multiplicative Weights Update (MWU), then Lyapunov chaos occurs everywhere in the cumulative payoff space. The authors also study how persistent chaos can occur in the general normal-form game settings, where the agents might have the motivation to coordinate (which is not true for zero- sum games). The authors characterize bimatrix games where MWU, its optimistic variant (OMWU) or Follow-the-Regularized-Leader (FTRL) algorithms are Lyapsunov chaotic almost everywhere in cumulative payoff spaces."
SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"This paper studies the problem of adaptive algorithms in deep learning. The authors propose a new motivation for designing the proximal function of the adaptive algorithms, named marginal regret bound minimization (MRL), which is based on the idea of marginal regret minimization. Based on this motivation, the authors propose an adaptive algorithm that achieves marginal optimality and can potentially converge much faster than existing adaptive algorithms. They show the superiority of the new algorithm both theoretically and empirically using experiments in deep Learning."
SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes a new algorithm for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The algorithm is based on the expected quadratic utility maximization (EQUM), which is a common objective of risk management in finance and economics. The proposed algorithm has several interpretations, such as reward-constrained variance minimization and regularization, as well as agent utility maximisation. In experiments, the authors demonstrate the effectiveness of the proposed algorithm in benchmark setting of RL and financial data."
SP:bf9d66f713b6502d274143c6273b2d071a0c045e,This paper proposes a method for multi-task learning based on implicit differentiation. The main idea is to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. The proposed method is evaluated on image segmentation and learning with attributes in the low-data regime.
SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,This paper proposes a new measure of uncertainty for long sequences of discrete random variables for neural machine translation. The proposed measure is based on dropout approximate inference. The authors show that their measure is able to detect out-of-distribution sentences in the German-English translation task.
SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the effect of pruning neural networks at initialization on the accuracy of magnitude pruning after training. In particular, the authors study SNIP (Lee et al. 2019), GraSP (Wang et al., 2020), and SynFlow (Tanaka et al, 2020) and show that random shuffling the weights within each layer or sampling new initial values preserves or improves accuracy. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both. "
SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper presents Fed-Learning, a federated learning protocol that is able to defend against both semi-honest clients and Byzantine malicious clients. The main contribution of the paper is the use of a robust mean estimator called FilterL2, which is the first FL protocol that provides dimension-free estimation error against Byzantine malicious client. The paper also proposes to split the clients into shards, securely aggregate each shard’s updates and run Filter-L2 on the updates from different shards. The evaluation shows that Fed-learning consistently achieves optimal or close-to-optimal performance under three attacks among five robust FL protocols."
SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper presents a benchmark suite of offline MBO tasks for black-box model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function using only previously collected data. The benchmark suite includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics that present distinct challenges. The authors provide a unified evaluation protocol and reference implementations of recent methods. "
SP:073958946c266bf760d1ad66bd39bc28a24c8521,-based ELBO is a generalization of ELBO to multi-modal data. The main contribution of this paper is to propose a new ELBO formulation for multimodal data that overcomes the limitations of existing self-supervised generative models. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. The experimental results demonstrate the advantage of the proposed method compared to state-of-the-art models in selfsupervised learning tasks.
SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes Prior-guided Bayesian Optimization (PrBO), a method for optimizing black-box functions that leverages the experience of domain experts. The main idea is to use priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). PrBO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. The authors show that PrBO is around 12x faster than state-of-the-art methods without user priors and 10,000 times faster than random search on a common suite of benchmarks. "
SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes to use binary neural networks to reduce the computational cost of deep generative models. The authors propose a new class of binary weight normalization, which is based on the idea of normalizing the weights of a binary neural network. They show that the proposed method can be applied to two state-of-the-art models, ResNet VAE and Flow++, and show that it can reduce the size of the models and speed up the execution time. "
SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning to address the problem of out-of-distribution shifts in the data distribution in a general context. The main contribution of the paper is to propose three novel methods to improve the robustness of deep learning with respect to natural variation. The proposed methods are based on the idea of learning models of natural variation, which vary data over a range of natural conditions, and then by exploiting these models, the authors develop three novel model- based robust training algorithms. The experiments show that the proposed methods significantly outperform classifiers trained via ERM, adversarial training, and domain adaptation techniques. "
SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the optimization of two-layer and three-layer CNNs with ReLU activations. The main contribution of the paper is to develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two-and-three layer CNN architectures. The authors prove that two- layer CNNs can be globally optimized via an `2 norm regularized convex program. They also show that multi-layer circular CNNs are equivalent to an `1 norm regularization of the program. Finally, the authors extend their results to three layer CNN with two ReLU layers. "
SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a probabilistic generative model for interpretable learning from demonstration. The proposed method is based on the use of weak labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. The method is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot – that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. "
SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b," is a novel approach to improve transfer learning in low-resource scenarios. The authors propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on target tasks. They show that their VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets."
SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,This paper proposes a method to recover the 3D shape of an image from a single 2D image using a pre-trained GAN trained on RGB images. The key idea is to use an iterative strategy to explore and exploit diverse viewpoint and lighting variations in the GAN image manifold. The proposed method is evaluated on 3D face reconstruction and 3D object rotation tasks.
SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tailed classifier called RIDE, which is based on the idea of long-tail classification. The authors propose to use a distribution-aware diversity loss to reduce the model bias and the variance of the model. They also propose a dynamic expert routing module to improve the performance of RIDE. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks."
SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper studies the effect of different types of pruning criteria on the quality of CNNs. The authors show that there are two major blind spots of the existing pruning methods: (1) Similarity: There are some strong similarities among several primary pruning metrics that are widely cited and compared. According to these criteria, the ranks of filters’ Importance Score are almost identical, resulting in similar pruned structures. (2) Applicability: The filters' Importance Scores are too close to distinguish the network redundancy well. The paper also shows that the results of `1 and `2 pruning are not always similar."
SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. The proposed model uses data flow in the pre-training stage, which is a semantic-level structure that encodes the relation of “wherethe-value-comes-from” between variables. The paper also introduces two structure-aware tasks to predict code structure edges and align representations between source code and code structure. The model is evaluated on four tasks, including code search, clone detection, code translation, and code refinement. "
SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,". This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The method forces the regression outputs to follow the true distribution; the forcing algorithm regularizes the regression results while keeping the information of the training data. The proposed method is evaluated on four real-world datasets (pLogP, Diamond, House, Elevators). "
SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of compositional generalization, i.e., the ability to extract compositional representations from the training distribution. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, they propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines."
SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper proposes a new poisoning method for online reinforcement learning. The authors propose a new metric, stability radius in RL, to measure the vulnerability of RL algorithms. The proposed method is based on the idea that a policy-based RL agent can be poisoned without knowing the underlying Markov Decision Process (MDP). Experiments on multiple deep RL agents and multiple environments show that the proposed method successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy."
SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper proposes Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is parameterized by eviction policy and supports dynamic models. The authors prove that DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only O(N) tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments."
SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collects new manually annotated evaluation sets for this task. The authors also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach – the average F1 of around 0.6 across all the benchmark datasets."
SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a new method for conditional generative adversarial networks (cGAN) based on NAS to find a distinct architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data.  The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability."
SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,This paper proposes an orthogonality regularization method for estimating the average causal effect. The proposed method is based on orthogonal networks for unconfounded treatments (DONUT) that learn outcomes that are orthogonic to the treatment assignment. The authors provide theoretical guarantees that this yields an asymptotically normal estimator for the average causality effect. They demonstrate that DONUT outperforms the state of the art substantially.
SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the expressive power of BatchNorm, a popular feature normalization technique for deep learning. The authors study the effect of the affine parameters on the performance of the network. They show that the network is able to achieve better performance when training only these parameters and freezing all the rest of the weights at their random initializations.   "
SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a test-time adaptation method based on test entropy minimization (Tent1) to adapt to new and different data during testing. Tent reduces the generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of testing without altering training."
SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate uncertainty in recurrent neural networks (RNNs) via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. The proposed method can be used to learn deterministic and probabilistic automata from data, learn well-calibrated models on real-world classification tasks, improve the performance of out-of-distribution detection, and control the explorationexploitation trade-off in reinforcement learning."
SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper proposes a new method for privacy-preserving deep learning models by injecting Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that residual perturbation guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, they show that the proposed method outperforms DPSGD in both membership privacy protection and maintaining the DL models’ utility."
SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of PoWER-BERT to address the issue of inefficiency and redundancy in inference. The proposed extension enables us to train a large-scale transformer, called Length-Adaptive Transformer, once and use it for various inference scenarios without re-training it. To do so, we train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. We then use a multi-objective evolutionary search to find the length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. Additionally, we significantly extend the applicability of the proposed extension beyond sequence-level classification into token-level classifier such as span-based question-answering, by introducing the idea of Drop-and-Restore."
SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the problem of improving the expressiveness of graph neural networks (GNNs) in the Weisfeiler-Lehman (WL) graph isomorphism test. The authors propose to reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of aggregation coefficient matrices for building more powerful aggregators and even injective aggregators. They also show the necessity of applying nonlinear units ahead of aggregation, which is different from most aggregation-based GNNs. Based on their theoretical analysis, the authors develop two GNN layers, ExpandingConv and CombConv, and show that their models significantly boost performance on large and densely connected graphs."
SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,This paper proposes a method for measuring disentanglement in generative models by measuring the topological similarity of conditional submanifolds in the learned representation. The proposed method is based on the idea of using the generative model to measure the topology of the learned representations. The method is evaluated on a number of datasets and compared with existing methods.
SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training data unlearnable for deep learning models. The proposed method is based on the idea of error-minimizing noise, where the noise is intentionally generated to reduce the error of one or more of the training examples close to zero, which can trick the model into believing there is “nothing” to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. The paper empirically verifies the effectiveness of the proposed method in both sample-wise and class-wise forms. "
SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper proposes an extension of MuZero for two-player, nondeterministic, zero-sum games of perfect information. Specifically, the authors propose to formalize chance as a player in the game and incorporate the chance player into the MuZero network architecture and tree search. Experiments show that NDMZ is capable of learning effective strategies and an accurate model of the game."
SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes a new option learning algorithm, Hindsight off-policy option learning (HO2), which is based on the idea of using temporal and action abstraction in the option framework by comparing flat policies, mixture policies without temporal abstraction, and finally option policies; all with comparable policy optimization. In addition, HO2 uses a dynamic programming inference procedure to train all components’ parameters independently of the data-generating behavior policy. Experimental results show that HO2 outperforms existing option learning methods and that both action and temporal abstraction provide strong benefits in more demanding simulated robot manipulation tasks from raw pixel inputs."
SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper studies the problem of synthesizable molecule generation in a real-world drug discovery pipeline. The authors propose a novel functional form of the Bellman equation, introduce the corresponding Bellman operators, and provide a proof of convergence. Using this formulation, the authors achieve state-of-the-art results on the task of synthesisable molecule generation. "
SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model that generates parameters for extending the base model to a new feature by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. To assess the performance of CHN, the authors use a CHN to augment a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data. The system obtains improved few-shot learning performance for novel features over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks."
SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method for Bayesian deep learning that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The proposed method is based on the idea that a point-estimated network can be used to perform inference over only a small subset of the model parameters while keeping all the other parameters as point estimates. This enables expressive posterior approximations that would otherwise be intractable for the full model. The method is evaluated on MNIST and CIFAR-10 datasets."
SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a new approach for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the proposed approach uses a model of the environment to obtain embedding for state and actions and present a generic architecture that uses these to learn a policy. In this way, the embedded representations obtained via this approach enable better generalization over both states and action by capturing similarities in the embedding spaces. The proposed approach is evaluated on several gaming, robotic control, and recommender systems."
SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a novel approach for unsupervised representation learning based on viewmaker networks. Viewmaker networks are generative models that learn to produce useful views from a given input by generating and then adding an `p-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. The authors show that the learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations on CIFAR-10. They also show that their learned views can also be combined with handcrafted views to improve robustness to common image corruptions."
SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of out-of-distribution (OOD) inputs based on their source and nature of uncertainty. The authors demonstrate how different existing detection approaches fail to detect certain types of outliers. They utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outlier inputs. The experiments on CIFAR10, SVHN, and MNIST as OOD data across different DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5."
SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model for image generation. The main idea is to learn to first generate global features at low resolution, then fill in local details in parallel at higher resolutions. The authors show that the proposed VAE can achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and is more easily applied to high-resolution images. The proposed method is evaluated on CIFAR-10, ImageNet, and FFHQ."
SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper studies the problem of contrastive learning, where the objective is to learn a representation of an image that maximizes the mutual information between two transformations of the image. The authors propose to use semi-hard negative examples to improve the performance of the learned representation. The proposed method is based on a family of mutual information estimators that sample negatives conditionally – in a “ring” around each positive. They prove that these estimators have higher bias but lower variance than NCE. They show that the proposed method outperforms NCE on four standard image benchmarks."
SP:613a0e2d8cbe703f37c182553801be7537333f64, in federated learning (FL). The paper proposes a new method to recover private data from the shared aggregated gradients. The proposed method is called catastrophic data leakage in FL (CAFE). The authors show that CAFE can recover large-batch data leakage attack with high data recovery quality. CAFE is evaluated on vertical and horizontal FL settings.
SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for dynamic relational inference. The proposed model is based on the DYnamic multi-agent Relational Inference (DYNAMIC) model, which is a deep generative network that can reason about dynamic relations. The main contribution of the paper is to study the trade-off between dynamic and inference period, the impact of training scheme, and model architecture on dynamic inference accuracy. The paper also shows that DYAMIC can be used to infer coordination and competition patterns from real-world basketball trajectories."
SP:9f4b77d39f1deca28324fb637a0a77e89976baa8," for collaborative filtering. The paper proposes an inductive collaborative filtering framework that learns a hidden relational graph among users from the rating matrix. The key advantage of the model is the capability to inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that the model achieves state-of-the-art performance on several matrix completion benchmarks."
SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage approach to learn disentangled representations of latent variables. The main idea is to first learn the disentanglement of the latent variables using a pre-trained autoencoder. Then, a generative model is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. The proposed approach is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit models like generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of mutual information maximization (MI) based objectives for learning representations of data in the context of reinforcement learning. In particular, the authors study two popular MI-based objectives and show that two of them can yield insufficient representations given mild and common assumptions on the structure of the MDP. They corroborate their theoretical results with empirical experiments on a simulated game environment with visual observations."
SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. In particular, the authors show that the non-convex neural networks training problem is equivalent to a finite-dimensional convex copositive program. The authors also provide the first algorithms for provably finding the global minimum of the vector output neural network learning problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. "
SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The key idea is to learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. Experiments show that the integration of LORL consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language."
SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes EM-RBR (embedding and rule-based reasoning) to predict the missing links among the knowledge graph (KG), i.e., predicting the possibility that a certain triple belongs to the KG. The authors propose a general framework that combines the advantages of reasoning based on rules and the state-of-the-art models of embedding. The proposed method is evaluated on FB15k, WN18, and a new dataset FB15K-R."
SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a new recurrent network architecture for video games. The proposed architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources called schemata that prescribe state updates. The authors propose to use attention to determine which object files to update, the selection of scheMata, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
SP:42a3c0453ab136537b5944a577d63412f3c22560,This paper proposes a neural module network (NMN) approach for video-grounded language tasks. The proposed approach is motivated by recent work on image-grounding tasks. VilNMN first decomposes all language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Experiments on video QA and video dialogues show the effectiveness of the proposed approach.
SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variants of the Policy-Space Response Oracles (PSRO) algorithm to reduce the amount of simulation required during Deep RL training. The first, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy. The second is Mixed-Opponents, which constructs a pure-strategy opponent by mixing existing strategy”s action-value estimates, instead of their policies. The experiments show that these algorithms substantially reduce the number of simulation during training required by PSRO."
SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper proposes to replace the attention mechanism in the Tacotron 2 text-to-speech model with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, a fine-grained variational auto-encoder is proposed to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
SP:ab9532306d294f85db84b9419ce826f046a7d95e,This paper proposes a method for bird’s eye view layout estimation from a pair of stereo images. The proposed method is based on using inverse perspective mapping (IPM) to map the input images and their features to the bird's eye view. The authors show that the proposed method can achieve state-of-the-art performance on KITTI and CARLA datasets.
SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1," GNNs suffer from a drop in performance when training deeper networks, which may be caused by vanishing gradients, over-parameterization, and over-smoothing. This paper proposes a novel relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup."
SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method to identify minimal regions in an input that are most relevant for a neural network’s prediction. The proposed method uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer of the network. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, the proposed method scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains “where a model is looking” when making a prediction."
SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a novel approach for 3D pose estimation based on a generative model of a 3D mesh. The proposed approach is based on differentiable rendering and a contrastive learning approach to minimize the reconstruction error between NeMo and the feature representation of the target image. Experiments on PASCAL3D+, occluded-PASCAL-3D+ and ObjectNet3D show that NeMo is more robust to partial occlusion and unseen pose compared to standard deep networks, while retaining competitive performance on regular data."
SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"The paper proposes a novel non-inherent feature compatible learning (NIL) approach for feature compatible embedding learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning. The proposed approach requires only features extracted by old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for FCL and extend it to handle the case where the old model is a black-box. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the effectiveness and efficiency of using gradient norm as the model selection criterion in hyper-parameter optimization. The authors propose to use an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200-20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). "
SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method to retrieve entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. The authors show that the proposed method can reduce the memory footprint of the encoder-decoder architecture and the exact softmax loss can be efficiently computed without the need to subsample negative data. Experiments on entity disambiguation, end-to-end entity linking and document retrieval tasks demonstrate the effectiveness of the proposed approach."
SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The routing requests are supplied adversarially. For each edge e in the selected path, the algorithm incurs a cost ce = fe(x t e) + η t e, where x t e is the flow on edge e at time t, fe is the congestion function, and η e is a noise sample drawn from an unknown distribution. The algorithm observes ce and can use this observation in future routing decisions. The authors propose an algorithm with cumulative regret Õ(|E|t), where the regret on each time step is defined as the difference between the total cost incurred by our chosen path and the minimum cost among all valid paths. "
SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes PMI-Masking, a principled masking strategy based on Pointwise Mutual Information (PMI) that jointly masks a token n-gram if it exhibits high collocation over the corpus. The authors show that the proposed PMI masking method outperforms other heuristics such as whole-word masking, entity/phrase masking and random-span masking. They show that PMIMasking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking by latching onto shallow local signals. "
SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the effect of partially-conditioned amortised variational posteriors on the ELBO objective in sequential latent variable models (LVMs). In particular, the authors show that the amortized posteriors tend to approximate products of smoothing posteriors instead of the true posteriors. They show that this is due to the fact that the posteriors are only partially conditioned on the past observations. The authors propose to use a more fully conditioned posteriors and show that it improves the performance of the generative model."
SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF) and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-the-art bounds. Specifically, the authors show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(|D|) memory and O(\sqrt{D}(D) time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, they derive generalization bound in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance of DK RR-RF."
SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address the problem of finding an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS’s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper shows that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top performing architectures (such as top 20% according to the ground truth) in the GS, such a correlation drops dramatically. The proposed EPS can achieve near-optimal NAS performance and surpass all existing state-of-the-art."
SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes PERIL, a method to combine imitation learning with meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. The main contribution of the paper is the introduction of Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL), which is based on the idea of preconditioning exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. The authors show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families within a set of meta-RL benchmarks under sparse rewards."
SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of learning over-parameterized convolutional neural networks with gradient-based optimization in the setting of orthogonal patches. The authors show that the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. They call this phenomenon Pattern Statistics Inductive Bias (PSI) and empirically verify it in a large number of instances. They prove that if a learning algorithm satisfies PSI then its sample complexity is O(d log(d) where d is the filter dimension. In contrast, they show a VC dimension lower bound which is exponential in d. "
SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper studies the problem of document classification in semi-supervised settings. The authors prove that contrastive learning can recover a representation of documents that reveals their underlying topic posterior information to linear models. They show that this representation can be used to improve the performance of linear classifiers with very few training examples. 
SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a novel contrastive generative model learning framework for multi-modal generative models. The main idea is to train the model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multimodal data. The authors show that their method enables data-efficient multimodality learning on challenging datasets for VAE models. "
SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes an energy-based prior for variational autoencoders (VAEs) to improve the generative performance of VAEs. In particular, the authors propose a reweighting factor to bring the base prior closer to the aggregate posterior. The proposed method is tested on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. "
SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"The paper proposes a regularized version of inverse reinforcement learning (IRL) based on strongly convex regularizers to avoid the expert’s behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. The authors propose tractable solutions and practical methods to obtain them for regularized IRL. They provide theoretical backing for their proposed IRL method and empirically validate their performance on a variety of tasks. "
SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths in MNMT. By manipulating these gates, CLSSR can schedule LS capacity across sub-layers subject to the guidance of translation signals and budget constraints. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many- to-one translation."
SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The authors propose to split the data into uncertain and certain samples based on small loss criteria. To this end, they impose geometric constraints on the uncertain samples by normalizing them into the wasserstein ball centered on certain samples. Experimental results demonstrate that the proposed WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets."
SP:e0029422e28c250dfb8c62c29a15b375030069e8,This paper proposes a method for quantifying the uncertainty of a classifier’s predictive set. The proposed method is based on the Platt scaling algorithm. The authors show that the proposed method provides a finite-sample coverage guarantee for every model and dataset. The method is evaluated on Imagenet-V2 with ResNet-152 and other classifiers and shows that it outperforms existing approaches.
SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper studies the problem of computing Wasserstein-2 barycenters, which is a geometric notion of the weighted average of probability measures based on optimal transport. The main contribution of this paper is the use of input convex neural networks and cycle-consistency regularization to avoid introducing bias. The authors provide theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach in low-dimensional qualitative scenarios and high-dimensional quantitative experiments."
SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the multiple manifold problem, a binary classification problem in which a network is trained to separate two low-dimensional submanifolds of the unit sphere. The authors prove that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L, and the number of i.i.d. samples from the manifolds is polynomials in L. Then, randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. The analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds. The width also enables concentration of the randomly-initialized network and its gradients."
SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The proposed method consists of two standard supervised steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised training methods. The paper provides a theoretical motivation for AWR and analyze its properties when incorporating off-Policy data from experience replay. The experimental results show that AWR achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms."
SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes WaveQ, a method to learn the bitwidth of neural networks by using a sinusoidal regularizer to find the quantized weights and the bit width of the layers by making the period of the sinusoid regularizer a trainable parameter. The paper shows that WaveQ is a gradient-based mechanism that jointly learns the quantised weights as well as the heterogeneous bitwidths. Experiments on a variety of deep networks demonstrate the effectiveness of the proposed method."
SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper proposes a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, the authors randomly replace words or mixup with their aligned alternatives in another language when training neural machinetranslation models. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"This paper proposes a real-time contribution measurement method for federated learning. The proposed method defines the impact of each agent. Furthermore, the authors comprehensively consider the current round and the previous round to obtain the contribution rate. The authors conduct pseudo-distributed training and an experiment on the Penn Treebank dataset to verify the proposed method."
SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,This paper studies the problem of learning Bayesian networks where an -fraction of the samples are adversarially corrupted. The authors propose a nearly-linear time algorithm for this problem with a dimension-independent error guarantee. The algorithm and analysis are considerably simpler than those in previous work. 
SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-term planning in RL using a probabilistic latent variable model. The proposed method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistically latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Experiments show that the proposed method outperforms prior model-based RL methods on challenging visual control tasks with sparse rewards and long term goals."
SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,. This paper proposes a generative model for curation of CIFAR-10. The authors argue that BNNs for image classification use the wrong likelihoods. They show that the likelihood under this new model closely matches the tempered likelihoods used in past work. 
SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the trade-off between speed and accuracy of autoregressive neural machine translation models. The authors argue that the speed tradeoff has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. They show that given a sufficiently deep encoder, a single-layer Autoregressive decoder can substantially outperform strong non-autoregressive models with comparable inference speed. "
SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"-variance decomposition shows that the bell-shaped variance is the major cause of model-wise double descent (when the DNN is widened gradually). This paper investigates epoch-wise Double descent, i.e., the test error of a DNN also shows double descent as the number of training epoches increases. Inspired by this result, the authors propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error."
SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies the robustness of model-agnostic meta-learning (MAML) in few-shot learning. The authors propose a robust regularization for MAML to promote adversarial robustness. They show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. Furthermore, they propose a general but easily-optimized robustness-regularized meta learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine tuning. "
SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the problem of learning-to-learn for quadratic loss. In particular, the authors study a simple problem of tuning the step size of the meta-gradient. They show that computing the meta gradient directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. They also characterize when it is necessary to compute meta-objective on a separate validation set instead of the original training set. Finally, they show that a similar phenomenon appears even for more complicated learned optimizers parametrized by neural networks."
SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a graph view-consistent learning network (GVCLN) for semisupervised learning when the number of the labeled samples is very small. The proposed GVCLN uses dual views to obtain different representations between the two views. Two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while a view consistent loss is applied to the two view to obtain the consistent representation and a pseudo-label loss is designed by using the common high-confidence predictions. Experiments on three citation network datasets of Cora, Citeseer, and PubMed show the effectiveness of the proposed method."
SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper studies the problem of predicting the dynamics of systems when the underlying dynamics shall be inferred from the data directly. In classical systems described via Hamiltonian dynamics this is achieved by using appropriate coordinates, so-called cyclic coordinates, which reveal conserved quantities directly. Without changing the Hamiltonian, these coordinates can be obtained via canonical transformations. The authors show that such coordinates are automatically searched for automatically with appropriate loss functions which naturally arise from Hamiltonians. They test their method on standard classical physics systems using synthetic and experimental data where our network identifies the conserved quantity in an unsupervised way and find improved performance on predicting the dynamic of the system compared to networks biasing just to Hamiltonian."
SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes a novel GNN-based approach for graph representation learning in the heterogeneous setting. The authors propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. The model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. Experiments are conducted on a variety of graphs with tabular features to demonstrate the effectiveness of the proposed approach."
SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of the train-validation split in meta-learning in the asymptotic setting where the number of tasks goes to infinity. The authors show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. They also show that if the data are generated from linear models (the realizable regime), both the splitting and non-Splitting methods converge to the best prior. Finally, the authors validate their theories by experiments on both simulations and real-world tasks. "
SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard confident examples from the noisy training data. Hard examples are examples that are close to the decision boundary and are essential to shaping accurate classifiers. To extract hard examples, the authors borrow the idea of momentum from physics. Specifically, they alternately update the confident examples and refine the classifier. Note that the extracted confident examples in the previous round can be exploited to learn a better classifier and that the better classifiers will help identify better (and hard) confident examples."
SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the robustness of kNN and radius nearest neighbors (rNN) against data poisoning attacks. In particular, the authors show that the intrinsic majority vote mechanism in kNNs and rNNs provides certified robustness guarantees against general data poisoning attack. Moreover, the empirical evaluation results on MNIST and CIFAR10 show the intrinsic certified robusts guarantees of knn and rnn outperform those provided by state-of-the-art certified defenses."
SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. To reduce the training time while keeping the decent model performance, the authors propose a metric that combines both the variance of gradients and compute time for each mini-batch. The authors theoretically analyze how batch-size influence such a metric and propose a formula to evaluate some rough range of optimal batch size. The empirical results show that in contrast to conventional deep learning models, GNNs benefit from large batch sizes."
SP:30d97322709cd292a49f936c767099f11b0e2913,This paper proposes a method to calibrate the classifier’s inherent confidence indicators and estimates uncertainty of the calibrated confidence scores using Gaussian Processes. The proposed method is evaluated on 125 UCI datasets. The results show that the proposed method can improve the robustness of neural network classifiers more broadly.
SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. The approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. The proposed approach is evaluated on question answering, obtaining state-of-the-art results."
SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper studies the use of finite automata to specify the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. The authors empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo and Atari environments."
SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a new decision tree model, called Cascading Decision Trees, to improve the comprehensibility of classifications. The key insight is to separate the notion of a decision path and an explanation path, and to build several smaller decision subtrees and cascade them in sequence. The cascading decision subtree is designed to specifically target explanations for positive classifications, and it is evaluated on standard datasets, as well as new real-world applications and finds that it shortens the explanation depth by over 40.8% for positive classification compared to the classic decision tree models."
SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of the number of parameters on the performance of neural networks. The authors show that for models with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while number of weights is secondary, as long as the model achieves high training accuarcy. They also show that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance."
SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module produces embeddings for entities in text while the language module generates context-aware initial embedding for entities and relations in the graph. Experimental results on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
SP:1db95a377f3d5ed129aa0511f840f647375e3528,This paper proposes an unsupervised learning method for learning autoregressive tokens. The proposed method is based on a variational inference approach to learn the order by which a sequence of tokens was generated. The authors propose an end-to-end optimization algorithm to find the best order for a given sequence. They show that the proposed method outperforms the state-of-the-art methods in terms of accuracy.
SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction scheme to speed up sampling in graph convolutional neural networks. The main idea is to decompose the induced variance of sampling methods into node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance (first-order) during backward propagation. The paper shows that the proposed scheme has a convergence rate of O(1/T) under the memory budget. Experiments are conducted to show the effectiveness of the proposed method.
SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a method to perform image manipulation by training a conditional adversarial generator on the single target image. The generator is trained to map a primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. The proposed method is evaluated on a variety of image manipulation tasks."
SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes GLSEARCH, a Graph Neural Network based model for Maximum Common Subgraph (MCS) detection, which learns to search. The model uses a state-of-the-art branch and bound algorithm as the backbone search algorithm to extract subgraphs by selecting one node pair at a time. In order to make better node selection decision at each step, the authors replace the node selection heuristics with a novel task-specific Deep Q-Network (DQN), allowing the search process to find larger common subgraph faster. Experiments on synthetic and real-world large graph pairs demonstrate that the model outperforms state-ofthe-art MCS solvers and neural graph matching network models."
SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes an end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments. The architecture gradually builds up the model: It starts by encoding the points into feature vectors, it identifies a pool of candidate vertices, then prunes those candidates to a final set of corners vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence. "
SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies the convergence of stochastic gradient methods under the assumption that the second moment of the oracle is non-stationary. In particular, the authors consider the case where the noise level in the gradient can change with time. The authors show that when the noise is known, it is always beneficial to adapt the step size and exploit the noise variability. When the noise statistics are unknown, they obtain similar improvements by developing an online estimator of the noise levels. "
SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a novel approach for word alignment in neural machine translation (NMT). The proposed approach is based on the idea that prior word alignment information can be used to provide hints or guidelines for the target sentence at running time. To this end, the authors propose an enhancement learning model that can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Experiments are conducted on English-Korean, English-to-German and English-Romanian translation tasks."
SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper proposes a new benchmark for RL algorithms with different hardness levels of reward, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. The proposed benchmark is based on the OpenAI Gym dataset and is designed to be fast-to-run. The authors show that the proposed MDP Playground is a good testbed for new RL algorithms and those wanting to unit test their algorithms."
SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper studies the problem of regression calibration for deep learning models. The main contribution of the paper is to propose a new quantile regularizer based on the idea of quantile calibration (Kuleshov et al., 2018), which is based on entropy estimation. The proposed method is trained end-to-end without requiring an additional dataset. Empirical results show the effectiveness of the proposed method on a variety of datasets."
SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a novel approach to solve the problem of 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. The approach leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. The combination of variational inference, neural networks and a differentiable raycaster ensures that the model is amenable to end-to-end gradient-based optimisation. The proposed approach is evaluated on realistic unmanned aerial vehicle flight data, nearing the performance of state-of-the-art visual-inertial odometry systems. The experiments demonstrate the applicability of the model to generative prediction and planning."
SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper studies the problem of using textual descriptions to improve generalization of control policies to new scenarios. The authors propose EMMA, a model that uses a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. EMMA is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. Empirically, EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines."
SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes a new method for training policy gradients in the actor-critic framework. The key idea is to learn the value of the states (resp. state-action pairs) relative to their mean value rather than their absolute value. The method is tested on a variety of continuous control tasks and algorithms and shows dramatic empirical improvement. 
SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the effect of depth on the generalization performance of deep neural networks in the overparameterized regime. The authors introduce local and global labels as abstract but simple classification rules. They show that the locality of the relevant feature for a given classification rule plays a key role. They also show that deeper is better for local labels, whereas shallower is good for global labels. Finally, they compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. "
SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies few-shot learning via representation learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2(n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. First, they study the setting in which this common representation is low-dimensional and provide a risk bound of Õ(dk n1T + k n2 ) on the target task. Then, they extend this result to handle a general representation function class and obtain a similar result. Finally, they consider a setting where the common representation may be high-dimensional but is capacity-constrained (say in norm). They again demonstrate the advantage of representation learning in both high dimensional linear regression and neural networks. "
SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,This paper proposes a new approach to study the robustness of neural networks to perturbations to the semantic features of the input data. The proposed approach is based on a black-box approach to determine the features for which a network is robust or weak. The main contribution of the paper is to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. The results show that the proposed approach outperforms the state-of-the-art. 
SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes an approach to automatically cluster together similar tasks during training for multi-task learning. The proposed approach is inspired by the expectation-maximization algorithm, which aims to find clusters of related tasks and uses these to improve sample complexity. The method is intuitive, simple to implement and orthogonal to other multi- task learning algorithms. Experiments on simple discrete and continuous control tasks as well as complex bipedal walker tasks and Atari games show the effectiveness of the proposed approach."
SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning approach for non-stationary time series. The key idea is to use the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, the framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution from non-neighboring signals. The proposed approach is evaluated on clustering and classification tasks for multiple datasets."
SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a novel approach for learning modular networks for multi-task learning, transfer learning and domain adaptation. The proposed approach is based on the idea that the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in case of multi- task learning, multi-source domain adaptation and transfer learning. "
SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of variational autoencoders (VAEs) that suffer from posterior collapse. This is related to local optima of the objective function that are often introduced by a fixed hyperparameter resembling the data variance. The authors propose AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter and thus avoids oversmoothing the model. Experiments on MNIST and CelebA datasets show the effectiveness of the proposed method."
SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,This paper proposes an unsupervised generative model based on variational autoencoder (VAE) that captures global dependencies among observations. The proposed model is based on a mixture model in the local or data-dependent space and a global Gaussian latent variable in the global space. The authors show that the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). The model performs domain alignment to find correlations and interpolate between different databases.
SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,This paper proposes to use human interaction and attention cues to learn better representations compared to visualonly representations. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. They propose to use these interactions as a training signal for representation learning to learn a visual embedding. They use the learned representation on a variety of diverse tasks and show consistent improvements compared to state-of-the-art self-supervised vision-only techniques.
SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the effect of negative pretraining on the generalization performance of a neural network trained on a target task. The authors propose three interventions to remove and fix this effect: (1) changing the learning rate after pretraining can yield better results than training directly on the target task; (2) increasing the discretization of data distribution changes from start to target task instead of “jumping” to a new task; and (3) resetting network biases to larger values can remove the effect, albeit to a smaller degree. "
SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of adversarial defense against adversarial perturbations. The authors propose a bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets. They also propose a new out-of-distribution detection algorithm that achieves the state of the art results on near-distributed outliers. 
SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequence. Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolutions is used to model the dynamics of the spatial regions along the time dimension. Furthermore, the proposed PSTconvolution is incorporated into a deep network, namely PSTNet, to extract features of point Cloud sequences in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point Cloud sequence."
SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. To handle different acoustic conditions, they model the acoustic information in both utterance and phoneme level. To better trade off the adaptation parameters and voice quality, they introduce conditional layer normalization in the mel-spectrogram decoder and fine-tune this part in addition to speaker embedding."
SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors propose to study the effect of different choices of optimizers, activation functions, and regularizers in training sparse networks. They show that the default choices of the optimizer, activation function and regularizer used for dense networks can disadvantage sparse networks, and show that gradient flow in sparse networks can be improved by reconsidering the architecture design and the training regime. "
SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs, but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. This paper studies the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature of one node is spread over its neighbors; and (2) feature and label influence of how much the initial feature/labels of a node influences the final feature of another node. Based on the theoretical analysis, the authors propose an end-to-end model that unifies GCN and LPA for node classification. In their unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. The proposed model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models."
SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error bounds for the problem of generating ground truth labels for a classifier and a generator function. In particular, the authors consider the case where the ground truth label generating function (LGF) has no constraints at all. The authors show that the R-Complexity of the generator function space depends on the classifier function space. They also propose a joint entropy-like measure of complexity between function spaces (classifier and generator) called co-complexity, which leads to tighter bounds on the generalisation error in this setting. "
SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a novel post-training quantization (PTQ) framework, BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. The main contribution of the paper is a comprehensive theoretical study of the second-order error of the proposed method. The paper also provides a good balance between cross-layer dependency and generalization error. Experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks. "
SP:3035318ac36cad693a5e4ee7bed43db8df6fb492," studies the effect of dataset size and class imbalance on the calibration of deep neural networks. In particular, the authors study the relationship between class imbalance and the calibration error. They also study the effects of label quality, showing how label noise dramatically increases calibration error, and show that poor calibration can come from small dataset sizes, which they motive via results on network expressivity. "
SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62," agents learn to communicate with each other by moving their joints in a 3D environment. The goal is to find a communication protocol that generalizes well beyond the training partners. The authors show that under realistic assumptions, a non-uniform distribution of intents and a common knowledge energy cost, these agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice."
SP:5ba686e2eef369fa49b10ba3f41f102740836859,This paper proposes a new method for estimating the uncertainty of sequential regression. The proposed method is based on the idea of symmetric and asymmetric uncertainty quantification. The authors show that the proposed method can be used to estimate the uncertainty in the presence of drift and non-stationary signals. They show that their method outperforms the state-of-the-art baselines on both the drift and the non-divergence scenarios.
SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,This paper proposes two unbalanced Gromov-Wasserstein formulations of the GW distance between metric measure spaces. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. The second formulation uses a distance between mm-spaces up to isometries based on the conic lifting. The authors show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme.
SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a novel pruning method, called all-alive pruning (AAP), to produce the pruned networks with only trainable weights. The method is based on the observation that dead connections do not contribute to model capacity, regardless of the pruning methods. The proposed method is tested on three benchmark datasets and shows that it outperforms existing pruning algorithms."
SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a novel approach for controllable semantic image editing. The authors propose to learn multiple attribute transformations simultaneously, integrate attribute regression into the training of transformation functions, and apply a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. The proposed approach is evaluated on synthetic and natural images, and achieves state-of-the-art performance for targeted image manipulation."
SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,The paper proposes a Gumbel-Softmax-based GAN architecture for discrete sequence generation. The proposed architecture is based on the Feature Statistics Alignment (FSA) paradigm to deliver fine-grained signals in the latent high-dimensional representation space. FSA forces the mean statistics of the fake data distribution to approach that of real data as close as possible in a finite-dimensional feature space. Experiments on synthetic and real benchmark datasets show the superior performance in quantitative evaluation and demonstrate the effectiveness of the proposed architecture.
SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes Spectral DQN, a deep reinforcement learning method for reward-based reinforcement learning with progressive rewards. The main idea is to decompose the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. Experiments are conducted on two domains with extreme reward progressivity, where standard value-based methods struggle significantly, and on a set of Atari games that do not overtly favour the approach."
SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper proposes a notion of “useful information” in deep neural networks, which is defined as the information contained in the representation learned by a deep network. The authors show that the implicit regularization coming from training with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations for the task. In particular, they find that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded. They also evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations. "
SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper studies the problem of nonconvex-strongly-concave min-max optimization. The authors propose a new algorithm called SREDA-Boost, which is based on the variance reduction algorithm proposed by Luo et al. (2020) to solve such a problem. The main contribution of this paper is to develop a novel analytical framework that guarantees the optimal complexity performance for a much enhanced algorithm. The proposed algorithm has less restrictive initialization requirement and an accuracy-independent (and much bigger) stepsize. The paper also proposes a zeroth-order algorithm named ZO-SREDA for the scenario that has access only to the information about function values not gradients. "
SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of few-shot object detection. The authors show that increasing the number of categories used during training can improve the generalization of the model from seen to unseen classes from 45% to 89% and improve the state-of-the-art on COCO by 5.4% AP50 (from 22.0 to 27.5). The authors verify that the effect is caused by the number the categories and not the training samples, and that it holds for different models, backbones and datasets. This result suggests that the key to strong few shot detection models may not lie in sophisticated metric learning approaches, but instead simply in scaling the size of categories."
SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a new approach for single-view implicit surface reconstruction. The proposed approach is based on a closed-form Differentiable Gradient Sampling (DGS) method. The key idea is to use the spatial gradient of the implicit field, rather than the value itself, as a source of supervision for single view reconstruction in part due to the difficulties of differentiably sampling a spatial gradient from a feature map. The paper shows that the proposed DGS method can be used for training on large-scale scenes without dense 3D supervision. Experiments on ScannetV2, ShapeNet and Pix3D show the effectiveness of the proposed method."
SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a simple training strategy called Pseudo-to-Real for high-memory-footprint-required large models. The proposed method is compatible with large models with architecture of sequential layers. The paper also provides a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. "
SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper studies the duality of energy-based generative models (EBMs) with shallow overparametrized neural network energies, both in the active (aka feature learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. The authors also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set and show that performing these restarts at every iteration step corresponds to score matching training."
SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,This paper studies the lower bounds of DP-ERM for general convex functions. The main contribution of this paper is to propose a new lower bound of $\Omega(\sqrt{p log(1/\delta) n)$ for both constrained and unconstrained cases. The lower bound is based on a novel biased mean property for fingerprinting codes. The authors also propose a novel `2 loss function instead of linear functions considered by previous works. 
SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key of the proposed method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The proposed method covers all the classical W.S. gradient flows including the heat equation and the porous medium equation. The authors demonstrate the performance and scalability of their algorithm with several numerical examples."
SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper proposes a meta-learning approach to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed meta-feature with the space of distributions on the hyperparameter configurations. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta- features boosts the performance of state of the art AutoML systems, AutoSkLearn (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al 2018)."
SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes Split-Mix, a method for federated learning with heterogeneous participants that allows in-situ customization of model sizes and robustness. The proposed method learns a set of base sub-networks of different sizes and levels of robustness, which are later aggregated on-demand according to the inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate that the proposed method provides better in-Situ customization than the existing heterogeneous-architecture FL methods."
SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes a new algorithm for nonconvex-nonconcave minimax minimax problems. The main contribution of the paper is the introduction of an algorithm for the weak-Minty variational inequality (MVI) problem. The authors show that the proposed algorithm converges globally even in settings where the underlying operator exhibits limit cycles. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. Moreover, a variant with stochastic oracles is proposed."
SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies the problem of neural contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves $O(\sqrt{T})$ finitetime regret, where T is the learning time horizon. Compared with existing neural contextual bandit algorithms, their approach is computationally much more efficient."
SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking. To reduce exponential-time shapley computations, the methodology includes a Monte Carlo simulation to avoid unnecessary explorations. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study. The proposed data-driven methodology is extensible to different domains, use cases, and reinforcement learning algorithms."
SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"This paper proposes a novel approach to quantify the uncertainty of model predictions in the presence of covariate shifts in the data distribution. The proposed approach is based on the notion of probably approximately correct (PAC) prediction sets. The main contribution of the paper is to propose an algorithm for quantifying the uncertainty in the setting where there is a covariate shift from the source distribution to the target distribution. In particular, the authors propose to use importance weights that encode how the probabilities of the training examples change under the covariate change. The authors show that their algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average normalized size among approaches that always satisfy the PAC constraints."
SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount of unlabelled data to progressively refine the model parameters. The main contribution of the paper is the analysis of the generalisation error of SSL algorithms using information-theoretic principles. The authors show that when the class conditional variances are not too large, the upper bound on the generalizability error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR datasets."
SP:570149eb8fb97928f94312e40bdc48dfe9885848,This paper proposes a model-free reinforcement learning algorithm that can generate multi-step plans for future exploration. The main contribution of the paper is that the proposed method is able to generate a plan for the current step as well as a number of future steps. This is achieved by training the policy to maximize the value of the current time step in order to maximize expected future return. Experiments are conducted on several benchmark environments to demonstrate the effectiveness of the proposed approach. 
SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorization methods to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors propose to use the factorization method to solve matrix/tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix and Tensor factorisation methods. The experiments on synthetic data and real datasets show that the proposed methods have much higher recovery accuracy than many baselines.
SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes a method to explain the behavior of structured output models. The main idea is to find the most important features utilized by the structured model to decide on the target in each locality of the input space. The goal is to train a function as an interpreter for the target output variable. The paper proposes an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. The effectiveness of the proposed method is confirmed on simulated and real data sets."
SP:cf9b6963c32d8689f7203dd41b17461676d08739,"-based deep reinforcement learning (DRL) approaches aim to maximize expected reward, considering collected experiences equally in formulating a policy. This differs from human decision-making, where gains and losses are valued differently and outlying outcomes are given increased consideration. Several approaches to distributional DRL have been investigated, with one popular strategy being to evaluate the projected distribution of returns for possible actions. This paper proposes a more direct approach, whereby the distribution of full-episode outcomes is optimized to maximize a chosen function of its cumulative distribution function (CDF). This technique allows for outcomes to be weighed based on relative quality, does not require modification of the reward function to modulate agent behavior, and may be used for both continuous and discrete action spaces. The authors show how to achieve an asymptotically consistent estimate of the policy gradient for a broad class of CDF-based objectives via sampling, subsequently incorporating variance reduction measures to facilitate effective on-policy learning. "
SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes an active learning framework for simulating large-scale, spatiotemporal, age-structured epidemic models. The proposed method is based on a neural process model to mimic the simulator dynamics. The model automatically infers the latent process which describes the intrinsic uncertainty of the simulator. This also gives rise to a new acquisition function based on the latent information gain. The authors design Bayesian active learning algorithms to iteratively query the simulator, gather more data, and continuously improve the model."
SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper studies the problem of using DP-SGD for NLP tasks. The authors propose a new algorithm that allows clipping in DP to run without instantiating per-example gradients for any linear layer in the model. They also propose a memory saving technique that allows privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. They show that the proposed algorithm outperforms state-of-the-art private training approaches and strong nonprivate baselines.
SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method to learn a conditional policy that first applies a sequence of transform actions to modify an agent’s skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods significantly in terms of convergence speed and final performance."
SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a method to speed up the training and inference of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, called CoordX. The initial layers are split to learn each dimension of the input coordinates separately, and the intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inferences, while achieving similar accuracy as the baseline MLP. The proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads."
SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to learn object-centric representations of visual scenes without relying on annotations. The proposed method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The paper empirically shows that INFERNO discovers objects in a scene without supervision. "
SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes Deconfounded Subgraph Evaluation (DSE) to assess the causal effect of an explanatory subgraph on the model prediction. The authors argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. To address the distribution shift, the authors propose a front-door adjustment and introduce a surrogate variable of subgraphs. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity."
SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper investigates the effect of pretraining on the ability of a model to learn to disambiguate between the possible tasks a user may be trying to specify. The authors propose to use uncertainty sampling to train a model that can learn to distinguish between the two possible tasks in a few-shot setting. They show that pretrained models are better active learners than unpretrained models, which do not select such examples. "
SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. Unlike sequence-tosequence models, GRAPHIX leverages the abstract syntax structure of code and represents the code using a multi-head graph encoder. Along with an autoregressive tree decoder, the model learns to perform graph edit actions for automated program repair. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. Experimental results on the Patches in The Wild Java benchmark, using both abstract and concrete code, show that the proposed model significantly outperforms a wide range of baselines including CodeBERT and BART and is as competitive as state-of-the-art models despite using fewer parameters."
SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"-based federated adversarial training (FAT) is a popular approach to improve the robustness of the model to adversarial attacks. However, the inner-maximization optimization of Adversarial training can exacerbate the data heterogeneity among local clients, which triggers the pain points of Federated Learning. In this paper, the authors introduce an α-weighted Federated Adversary Training (α-WFAT) method to overcome this problem. The authors provide the theoretical analysis about this method and its effect on the convergence of FAT. Empirically, the extensive experiments are conducted to comprehensively understand the characteristics of the proposed method. "
SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes Mako, a framework for continual continual learning (LML) that leverages data programming to leverage unlabeled data for continual learning. Mako is a wrapper tool that mounts on top of supervised LML frameworks, leveraging data programming. It achieves similar performance in terms of per-task accuracy and resistance to catastrophic forgetting, as compared to fully labeled data. "
SP:447df6679b2880def833d4f444bf10e61cdf0e1c,This paper studies the problem of finding adversarial examples that must simultaneously (a) be misclassified by the model and (b) be detected as non-adversarial. The authors propose two methods to generate adversarial example that avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. They show that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. They use their technique to evade four state-of-the-art detection defenses.
SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes an energy-based variational approach to solve the game-theoretic valuation problem in cooperative games. The main contribution of the paper is to propose a variational index for the ELBO objective, which is based on the maximum entropy principle. The paper shows that the proposed index has a lower decoupling error than the Shapley value and the Banzhaf value. Experiments are conducted on both synthetic and real-world problems. "
SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method to directly estimate the epistemic uncertainty of a model by learning to predict the generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of the uncertainty includes the effect of model bias (or misspecification) and is useful in interactive learning environments arising in active learning or reinforcement learning. The proposed method is evaluated on two downstream tasks: sequential model optimization and reinforcement learning, where it is shown to outperform existing methods."
SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objective. The proposed algorithm is based on geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n). The proposed algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to the experimental studies. "
SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The framework uses (1) a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and (2) a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The approach is evaluated on the Raven's Progressive Matrix (RPM) dataset."
SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a novel self-supervised method for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the framework is forced to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the proposed method outperforms state-of-the-art deep learning methods."
SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. The proposed spherical filter banks enable making equivariant and, after online optimization, invariant class predictions for known point sets in unknown orientations."
SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"In this paper, the authors study the continual learning (CL) of pre-trained language models (PLMs) and continual learning methods (CL methods and PLMs) in the context of incremental learning. The authors compare the performance of 5 PLMs and 4 CL methods on 3 benchmarks in 2 typical incremental settings. They also analyze the representativeness of PLMs in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting and the effect of different CL approaches on each layer."
SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper studies the problem of model poisoning attacks in federated learning. In particular, the authors propose TESSERACT, a defense against directed deviation attack, a state-of-the-art model poisoning attack. The proposed method is based on the intuition that certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. Experiments are conducted to show the effectiveness of the proposed method."
SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a neural net based debiasing method for estimating the average marginal effect of a linear regression function. The proposed method is based on learning the Riesz representation of the linear functional using Neural Nets and Random Forests. The authors propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined RiesZ representer and regression loss, while sharing representation layers for the two functions. The paper also proposes a Random Forest method which learns a locally linear representation for the linear function. Experimental results show that the proposed method outperforms Shi et al. (2019) for the average treatment effect functional."
SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. In this problem, a set of N agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network. The authors consider a practical MARL setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. They propose a mini-batch Markovian sampled fully decentralized actor-Critic algorithm and analyze its finite-time convergence and sample complexity. "
SP:8475e89f143c727e33147b652c2d0b3cdb420382,"-based contrastive learning is a promising approach for large-scale self-supervised learning. However, the theoretical understanding of how it works is still unclear. In this paper, the authors propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice. The new theory hinges on the insight that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastivelearning cluster intra-class samples together. The paper also shows that the proposed theory aligns well with existing contrastive methods on both synthetic and real-world datasets."
SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The proposed method consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. Experiments are conducted on additive noise, room reverberation, low-resolution, and clipping distortions to demonstrate the effectiveness of the proposed method."
SP:c80a7392ec6147395a664734601fb389a1eb4470," in multivariate time series. This paper proposes to use a tensor network based on the idea of low-rank approximation to model the variable space. The tensor components are shared to ensure the translation invariance of the network. In order to improve the ability to model long-term sequences, the authors propose an N-order residual connection approach and couple it to the space-approximated tensor networks. Moreover, the series variable encoder is designed to better the quality of the data."
SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967," is a commonly used technique for training Graph Neural Networks (GNNs) on large graphs. Previous work has shown that sampling-based GNN training can be considered as Stochastic Compositional Optimization (SCO) problems and can be better solved by SCO algorithms. However, SCO methods are impractical for training GNNs because they need to store the moving averages of the aggregated features of all nodes in the graph. The moving averages can easily exceed the GPU memory limit and even the CPU memory limit. In this work, the authors propose a variant of SCO algorithm with sparse moving averages for GNN learning. By storing the moving average in the most recent iterations, the algorithm only requires a fixed size buffer, regardless of the graph size. The authors show that their algorithm preserves the convergence rate of the original SCO method when the buffer size satisfies certain conditions. The experiments validate the theoretical results and show that the algorithm outperforms the traditional Adam SGD for training with a small memory overhead."
SP:72e0cac289dce803582053614ec9ee93e783c838,This paper proposes Circulant MinHash (C-MinHash) to reduce the number of independent random permutations in the MinHash algorithm. The main contribution of the paper is to prove that using only two random permutation in a circulant manner leads to uniformly smaller Jaccard estimation variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed method.
SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a simple and efficient training scheme to achieve adversarial robustness against the union of lp-threat models. The proposed method is based on geometric considerations of the different lp balls and costs as much as normal adversarial training against a single lp threat model. The authors show that using their E-AT scheme one can fine-tune with just 3 epochs any lp robust model (for p \in \mathcal{1, 2, \epsilon}^2) and achieve multiple norm robustness. "
SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning model where no negative sharing occurs. The proposed model consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder, where the gate is to learn how to combine the public decoder and the gate for the downstream decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed to allow the gate to choose either the public or the private encoding. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the problem of partitioning energy-based sequence models backed by expressive parametric families. The main contribution of the paper is to show that there are no good deterministic or randomized estimates of partition functions. The authors propose a pathological example where under common assumptions, no useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, the authors consider sequence model families whose partition functions are computable (if they exist) but at the cost of reduced expressiveness."
SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes a new Wasserstein distance metric for machine learning problems. The proposed method is based on the observation that (random) linear projections of samples residing on higher-dimensional hypersurfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. Numerical results demonstrate that the proposed ASWD significantly outperforms other Wassersteins for both synthetic and real-world problems."
SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,This paper proposes a method to improve coordination and performance of multi-agent reinforcement learning (MARL) by learning intrinsic rewards that coordinate the agents’ joint exploration and joint behaviour. The proposed method is based on learning an adaptive learner that observes the agents and learns to construct intrinsic rewards online. The authors show that the learned intrinsic rewards can be combined with reinforcement learning and switching controls to improve the performance of MARL. They demonstrate the effectiveness of the proposed method in Foraging and StarCraft II.
SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a novel approach for multi-hop QA, where the goal is to read long, hierarchical documents to answer complex questions. The proposed approach is based on the idea of “retrieving” either short passages or long sections of the document, thus emulating a multi-step process of ‘navigating’ through a long document to answer a question. To enable this novel behavior, DOCHOPPER does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact representation of a hierarchical part of the documents, which can potentially be quite large. Experiments on four different QA tasks that require reading long and complex documents are conducted to demonstrate the effectiveness of the proposed approach."
SP:4e79b326bbda5d1509e88869dde9886764366d41,". The paper proposes a semi-supervised learning method to extract refined labels from known initial labels (e.g. character played in a recording). The proposed method first suggests using a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm to finally train a refined representation extractingor. The method is validated on recordings from the MassEffect 3 video game."
SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed multi-task learning framework for image processing tasks. The key idea is to use a task-agnostic Vision Transformer body to learn the common representation through global attention of the embedded input sequence. The authors propose an alternating training strategy in which task-specific learning for the heads and tails is run on the clients by fixing the Transformer on the client side, and alternates with task-adaptive learning on the server side. Experiments on several low-level tasks demonstrate the effectiveness of the proposed method."
SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking, where RL agents exploit gaps in misspecified reward functions. To this end, the authors construct four environments in which the reward function is missingpecified. They investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. To address this, they propose an anomaly detection task for aberrant policies and offer several baseline detectors."
SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence Thermodynamic Variational Objective (f-TVO) that generalizes the TVO by replacing Kullback–Leibler (KL) divergence with arbitary differeitiable f divergence. In particular, f -TVO approximates dual function of model evidence f∗(p(x)) rather than the log model evidence log p(x) in TVO. The proposed method is derived from a deformed χ-geometry perspective. Experiments on VAE and Bayesian neural network show that the proposed f-VIO performs better than cooresponding baseline f-Divergence variational inference."
SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper studies the interplay between individual elements of the RL toolbox in the continuous control setting. In particular, the authors propose a method called Ensemble Deep Deterministic Policy Gradients (ED2) based on ensemble-based actor-critic exploration. The proposed method is evaluated on continuous control tasks from OpenAI Gym MuJoCo. The authors show that the proposed method outperforms the state-of-the-art methods in terms of exploration performance."
SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for Equalized Loss (EL), a fairness notion that requires the prediction error/loss to be equalized across different demographic groups. The authors propose an algorithm that finds the optimal EL fair predictor by reducing the non-convex optimization problem to a sequence of convex constrained optimization. They also propose a simple algorithm that is computationally more efficient compared to ELminimizer. Experiments on real-world data show the effectiveness of the proposed algorithm."
SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies the systematic generalization ability of neural networks from the perspective of meaningful learning. In particular, the authors propose to re-examine the ability of networks to learn new concepts by connecting them with other previously known knowledge. To this end, they propose to augment a training dataset in either an inductive or deductive manner to expose such semantic links to models. Experiments on the SCAN dataset and two real-world datasets on semantic parsing show that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking. They further demonstrate that both prior knowledge and semantic linking play a key role in achieving systematic generalisation and that inductive learning generally works better than deductive learning."
SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a novel approach for 3D shape representation learning using multi-scale wavelet decomposition. The main idea is to decompose 3D shapes into sub-bands components at multiple scales and all scales form a decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. The authors propose Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-band components, using lifting scheme at multiple scale recursively and hierarchically. Then, AWT-net exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions. The wavelet coefficient can be learned without direct supervision on coefficients, and the wavelet network is fully differentiable and can be trained in an end-to-end fashion. Extensive experiments demonstrate the effectiveness of the proposed method."
SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes a method to combine the benefits of full and lightweight finetuning to achieve strong performance both in-distribution and OOD. The authors show that an ensemble of lightweight and full finetuned models achieves the best of both worlds: performance matching the better of both. They also show that the proposed method can achieve similar improvements using a single model instead of two. Finally, they provide some explanatory theory in a multiclass logistic regression setting with a large number of classes."
SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models (WARM) to improve the performance of weakly supervised models via active learning. The main idea of WARM is to generate probabilistic training labels from simple yet imperfect heuristics (or labelling functions) obtained a priori from domain experts. WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probababilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling function in the weak supervision model. Experiments on multiple real-world medical classification datasets show that WARM can substantially improve the accuracy of probablistic labels used to train downstream classifiers, with as few as 30 queries to experts."
SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper studies the problem of training a classification model with group annotated training data. The authors propose a new algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind their proposed algorithm is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by GroupDRO. Empirically, the proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM and Group DRO on standard benchmarks. "
SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for explaining black-box models. The proposed method is based on the notion of directionality, which allows to identify the most influential features in a directed graph. The method is tested on Shapley value explanations and shows superior performance over univariate methods."
SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a novel approach for learning a probabilistic tree-based decision tree for fully-offline and partially-observable clinical decision environments. The proposed approach is based on learning a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. Experiments show that the proposed approach outperforms the state-of-the-art on both real and synthetic medical datasets."
SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a multi-agent reinforcement learning approach for data augmentation. The proposed method is based on the idea that each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art automated DA methods.
SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs). The authors propose to use a causal graph to model the generation process of the adversarial examples and define an adversarial distribution to formalize the intuition of DNNs. Based on the causal analysis, the authors propose a method to align the natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method."
SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for continual learning of deep neural networks. The proposed method is based on the idea of atom swapping. The idea is to learn a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. The atom swapping framework further enables flexible and efficient model ensemble with members selected within task or across tasks to improve the performance in different continual learning settings. The effectiveness of this simple scheme is illustrated both empirically and theoretically.
SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD) in high-dimensional settings. The authors show that SVGD suffers from the curse of dimensionality in the proportional asymptotic limit, where the number of particles n and dimensions d diverge at the same rate. In particular, the authors derive the exact equilibrium variance for both SVGD and MMD-descent, under certain near-orthogonality condition on the converged particles."
SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the relationship between adversarial training (AT) and noisy labels (NL) in terms of the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Given that natural data are clean, this measure reveals an intrinsic geometric property—how far a point is from its nearest class boundary. Based on this breakthrough, this paper analyzes how AT would interact with NL. The authors find that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training."
SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a statistical method to measure the expected robustness of a neural network model. Specifically, the proposed method is based on the probability that a random input perturbation might cause misclassification. The method allows us to provide formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The approach can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods. "
SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) to learn representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM and demonstrate that HCM learns meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. The interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems by reparametrizing the optimization variables as the output of a neural network. The authors propose to use a graph convolutional network (GCN) as the aggregation function of the loss function and reduce to the Hessian in early stages of the optimization. They show the utility of their method on two optimization problems: network synchronization and persistent homology optimization, and find an impressive speedup, with our method being 4 ∼ 80x faster."
SP:80346eeafb0a6d1d556c304a3f8753aff037469b," neural networks (DCNNs) have demonstrated superior power in their ability to classify image data. However, one of the downsides of DCNNs for supervised learning of image data is that their training normally requires large sets of labeled “ground truth” images. In this paper, the authors propose SVMnet, a method based on a layered structure of Support Vector Machine (SVM) ensembles for non-parametric image classification. By utilizing the quick learning of SVMs compared to neural networks, the proposed method can reach higher accuracy than neural networks when the training set is small."
SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes an end-to-end approach for anytime pixel-level recognition. The main idea is to use a cascade of “exits” to make multiple predictions and direct further computation. The authors propose to design the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, the authors propose a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. "
SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a novel approach to bootstrapping neural processes (NP) based on neural networks to model functional uncertainty. The proposed approach is based on the recent work of Bootstrapping Neural Processes (B(A)NP) to capture the functional uncertainty which can replace the latent variable in (Attentive) neural processes (ANP). However, B(ANP) is non-parallelizable and memory-inefficient and fails to capture diverse patterns in the stochastic processes. To resolve these problems, the authors propose NeuBANP, which learns to generate the bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. Experiments are conducted on Bayesian optimization and contextual multi-armed bandit tasks to demonstrate the effectiveness of the proposed approach."
SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes GeneBERT, a multi-modal and self-supervised pre-training approach for pre-trained genome data for regulatory downstream tasks. The proposed approach is based on the idea of taking the 1d sequence of genome data and a 2d matrix of (transcription factors + regions) as the input, where three pre-train tasks are proposed to improve the robustness and generalizability of the model.  The proposed method is evaluated on the ATAC-seq dataset with 17 million genome sequences. "
SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes IGEOOD, a method for out-of-distribution (OOD) detection based on the Fisher-Rao distance between the underlying data distribution and the learned features of a deep neural network. The proposed method can be applied to any pre-trained neural network, works under different degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD sample. The discriminator is able to combine confidence scores from the logits outputs and the learnable features of deep neural networks. Empirically, the proposed method outperforms competing state-of the-art methods on a variety of network architectures and datasets."
SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"The paper proposes a generalization of Cover’s Function Counting Theorem for group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. The authors show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomyies, the authors show that the fraction decreases with the dimension of the space that is fixed by the group action. "
SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The authors base CCE on two prior ideas: counterfactually explanations and concept activation vectors, and validate their approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,The paper proposes a modification of kernel point convolution (KPConv) to improve the efficiency and quality of KPConv. The proposed modification is based on the Inverted Residual Bottleneck (IRB) to craft a design space and employ a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on MAKPconv. Experiments on 3D point cloud classification and segmentation benchmarks show the effectiveness of the proposed modification.
SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of robust overfitting, robustness overestimation, and robustness-accuracy trade-off in adversarial training. The authors propose a method to measure the data quality based on the learning behavior of the data during the training and find that low-quality data may not be useful and even detrimental to the adversarial robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in the training of adversarial networks. They find that when low quality data is removed from the dataset, robust over-fitting and robusts overestimation can be largely alleviated; and the robusts accuracy tradeoff becomes less significant."
SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives — Korobov functions. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU. Further, the authors show that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate these functions, showing that neural networks are near-optimal function approximationators."
SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper studies the effect of population size on the speaker-listener Lewis Game in the context of neural networks. In particular, the authors show that the agent community is not homogeneous under the assumption that the population size is homogeneous. The authors propose two diversity factors: training speed and network capacity. They find out that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. "
SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,This paper studies the problem of graph neural networks (GNNs) in heterophilic graphs. The authors propose to learn multiple adaptive polynomial filters acting on different subsets of the spectrum to improve the performance of GNNs. The proposed method is based on an eigendecomposition of the graph and proposes to learn the filters based on the number of eigen-components. The paper shows that the proposed method achieves performance gains of up to 10% over the state-of-the-art models and outperforms existing GNN models in general.
SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a novel graph shortest distance embedding method called Betweenness Centrality-based Distance Resampling (BCDR) for embedding-based distance prediction. BCDR is based on the idea of betweenness centrality (BC) based random walk. The authors show that BC-based random walk can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure. In addition, the authors propose to use distance sampling from original walk paths before maximum likelihood optimization instead of the PMI-based optimization. "
SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL) to address the issue of catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. The authors construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. Through extensive experiments, the authors show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, CKL is a challenging and important problem that helps us better understand and train ever-changing LMs."
SP:639fd88482330389019fb5be7446a909b99a8609,This paper proposes a stochastic approach for the criterion minimization of decision trees. The proposed algorithm is faster than the standard exhaustive search algorithm by several orders of magnitude. It is shown that the proposed algorithm minimizes an upper bound on the criterion. Experiments on the MNIST dataset demonstrate the effectiveness of the proposed method. 
SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the problem of learning rate schedulers for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The authors propose Eigencurve, a learning rate schedule that can achieve minimax optimal convergence rates (up to a constant rate) when eigenvalues of the Hessian are skewed. They show that the proposed schedule can achieve better convergence rates than step decay in image classification tasks on CIFAR-10, especially when the number of epochs is small."
SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning. The authors propose to use Bayesian Optimization (BOO) to improve the performance of existing offline RL methods. The main contribution of the paper is to study the effect of the number of models, the rollout horizon, and the total variation distance between the learned dynamics model and the true dynamics model. They show that BOO outperforms the state-of-the-art methods by a large margin. "
SP:3833662cf92249d83e65a1200f9e2890b5b23e95,-based MfRL is an off-policy model-free reinforcement learning algorithm. The authors propose a novel approach called model-augmented priority experience replay (MaPER) that employs new learnable features driven from components in model-based RL (MbRL) to calculate the scores on experiences. The proposed MaPER brings the effect of curriculum learning for predicting Q-values better by the critic network with negligible memory and computational overhead compared to the vanilla PER method. The experimental results on various tasks demonstrate that MaPER can significantly improve the performance of the state-of-the-art offpolicy Mf RL and MbRL algorithms.
SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a novel approach to learn a human-in-the-loop learning agent with limited human intervention budget. The proposed approach is based on the idea that the human expert can take over the control and demonstrate to the agent how to avoid probably dangerous situations or trivial behaviors. The experimental results show that HACO achieves a substantially high sample efficiency in the safe driving benchmark and achieves high safety and generalizability. 
SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. DMIL uses the likelihood of state-action pairs from each sub-skill as the supervision for the high level network adaptation, and use the adapted highlevel network to determine different data set for each sub skill adaptation. Empirically, DMIL achieves state-of-the-art few-shot imitation learning performance on the meta-world (Yu et al., 2019b) benchmark and comparable results on the Kitchen environment."
SP:fb0efa670729796471a7a562b231172103bb8749,This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that each node is compactly represented with a bit vector instead of a float-point vector. The parameters utilized in the compression method can be trained together with GNNs. The authors show that the proposed method achieves superior performance compared to the alternatives.
SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper studies the asymptotic convergence of high-order ODE solvers in domain-adversarial training. The authors propose to replace gradient descent with Runge-Kutta solvers (i.e., runge-kutta) and show that they are more stable than gradient descent. They also show that the proposed method can be used as a drop-in replacement for standard optimizers. They show that their method can achieve up to 3.5% improvement with less than half of training iterations."
SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularization method called individual Flooding (iFlood) to prevent overfitting of machine learning models. The authors show that the design of the loss function of Flooding can lead to a discrepancy between its objective and implementation, and cause the instability issue. To resolve these issues, the authors propose an instance-level regularizer, iFlood, which encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The experiments on image classification and language understanding tasks confirm that the proposed method can stably converge to solutions with better generalization ability, and behave consistently at instance level. "
SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,This paper proposes Value Function Spaces (VFS) to represent the value function of each lower-level skill in Hierarchical Reinforcement Learning (HRL) by using the value functions corresponding to each lower level skill. The proposed method is evaluated on a maze-solving and robotic manipulation task and shows that it improves long-horizon performance and enables better zero-shot generalization.
SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism that learns to select the most relevant points from a trainable reference set. The authors propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. Top-N outperforms i.i.d. generation by 15% at SetMNIST reconstruction, generates sets that are 64% closer to the true distribution on a synthetic molecule-like dataset, and can generate more realistic molecules when trained on the classical QM9 dataset."
SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, the authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. The authors establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. They also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, following recent work which has shown that the deep model accuracy will improve with growing training sets according to a power law, the paper shows similar-behavior of dimension dependent power law for deep PDE solvers."
SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper studies the relationship between robustness and generalization in machine learning. The authors provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data augmentations (DA). In particular, they propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of domain generalization. They show that under certain conditions, DA can be viewed as regularization and therefore improve generalization, and conduct extensive experiments to verify their theoretical findings."
SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of memorization in meta-learning, where the meta-knowledge simply memorizing all meta-training tasks discourages task-specific adaptation and poorly generalizes. In this paper, the authors propose a novel causal perspective on the effect of label space as a confounder to cause memorization and frame the two lines of prevailing methods as different deconfounder approaches. The authors propose two simple but effective deconfounder algorithms based on the causal inference principle of front-door adjustment, i.e., sampling multiple versions of the meta knowledge via Dropout and grouping meta-k into multiple bins. The proposed causal perspective not only brings in the two deconfounder methods that surpass previous works in four benchmark datasets towards combating memorization, but also opens a promising direction for meta learning."
SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a novel reinforcement learning framework for ad hoc teamwork, where the agent is able to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the paper introduces an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc team building tasks."
SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes EMFlow, an online version of Expectation-Maximization (EM) algorithm that uses a normalizing flow (NF) model to map the data space to a latent space. The proposed EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results for high-dimensional multivariate and image datasets demonstrate the superior performance of EMFlow compared to a couple of recently available methods in terms of both predictive accuracy and speed of algorithmic convergence."
SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper studies the problem of disentanglement in DNNs with ReLUs. The main contribution of the paper is to propose deep linearly gated networks (DLGN) to disentangle the computations into two linearities: the primal linearity between the input and the preactivations in the gating network, and the pruned linearity of the pre-activation of the gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the neural path kernel (NPK) of the ReLU network. "
SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes Dynamic Token Normalization (DTN) to replace layer normalization (LN) in Vision Transformer (ViT) and its variants (e.g., Swin, PVT, LeViT, T2T-ViT). The idea is that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. The proposed DTN is built on a unified formulation and thus can represent various existing normalization methods. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead."
SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,This paper proposes a method to measure the spectral bias of deep neural networks in image classification. The authors show that deep networks trained with SGD exhibit spectral bias and that interventions that improve generalization sometimes increase and sometimes decrease the frequencies of the learned function. They also explore the connections between function frequency and image frequency and find that spectral bias is sensitive to the low frequencies prevalent in natural images. This work enables measuring the spectral behavior of neural networks used for image classification and is a step towards understanding why deep models generalize well.
SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the problem of mode-switching, non-monolithic exploration in reinforcement learning (RL). The main contribution of this paper is to propose a novel algorithm for switching between different modes of exploration. The algorithm is based on the idea of two-mode exploration and switching at sub-episodic time-scales. The main contributions of the paper are:  1. The authors propose a new algorithm for mode switching in RL.  2. The proposed algorithm is tested on Atari games.  3. They show that the proposed algorithm outperforms the baselines. "
SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median problem in the general metric space (e.g., discrete space induced by graphs) based on the construction of metric embedding tree structure of the data. From the tree, the authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Theoretically, the initial centers from HST can achieve lower error than those from another popular initialization method, k-Median++, in the non-DP setting. Moreover, with privacy constraint, the error of applying DP local search followed by our private HST initialization improves previous results, and approaches the known lower bound within a small factor. Empirically, experiments are conducted to demonstrate the effectiveness of our methods."
SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper studies the problem of video prediction. The authors argue that the inefficient use of parameters in the current video models is the main reason for underfitting. They introduce a new architecture, named FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. They analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and how it is mitigated using existing image augmentation techniques. "
SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the effect of data structure on the test loss dynamics of SGD on the generalization performance of a neural network. The authors propose an exactly solveable model of stochastic gradient descent (SGD) which predicts test loss when training on features with arbitrary covariance structure. They show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, and demonstrate the computational benefits of using SGD with small batch sizes. They also extend their theory to the more usual setting of training on a fixed subsampled training set, showing that both training and test error can be accurately predicted in their framework on real data."
SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of stochastic gradient descent (SGD) in the nonlinear, nonconvex problem of training deep neural networks. The authors show that SGD may converge to local maxima, SGD might only escape saddle points arbitrarily slowly, and AMSGrad may prefer sharp minima over flat ones. The paper also shows that AM SGD can also converge to a minimal neural network-like construction."
SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a novel hyperparameter optimization method for meta-learning. The main idea is to use a Jacobian-vector product (JVP) to approximate the second-order term with knowledge distillation and minimize the distance from the true second order term. The proposed method allows online optimization and is scalable to high-dimensional hyperparameters and horizon length. Experiments show the effectiveness of the proposed method on two different meta learning methods and three benchmark datasets.
SP:a64b26faef315c3ece590322291bab198932c604,This paper proposes a new meta-learning framework for few-shot image classification and cold-start recommendation tasks. The proposed method is based on the idea of clustering task-aware representations learned from both the features and the learning path. The clustering is done by learning a set of geometric quantities that describe the path of the meta-learner. These geometric quantities are then fed into a meta-path learner that learns the path representation for the downstream clustering and modulation. The paper also proposes a shortcut tunnel to directly map between the path and feature cluster assignments. 
SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based semi-supervised novelty detection (SSND) method for OOD detection. The proposed method uses a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. It crucially relies on regularization to promote diversity on the Ood data while preserving agreement on ID data. Extensive comparisons of the proposed method with state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical image data set show significant gains with negligible increase in computational cost 1.
SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes Latent Variable Sequential Set Transformers (LSST) for multi-agent trajectory prediction. The key idea is to learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal and Social dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on Argoverse vehicle prediction challenge."
SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,This paper proposes a synthetic dataset to test the ability of users to identify the relevant set of attributes compared to the ground-truth. The dataset is generated by generating a synthetic image of the input image and generating an explanation of the image using an invertible neural network. The authors conduct a user study (N=240) to test how well a baseline explanation technique performs against concept-based and counterfactual explanations. The results show that the baseline outperformed concept based explanations. 
SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes a method to remove poisoned data from the training set of deep neural networks. The proposed method is based on an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. Empirically, the proposed method successfully defends against a state-of-the-art dirty label backdoor attack."
SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively. The proposed method is conceptually simple but quite effective. It improves the state-of-the-art results on two widely used benchmark datasets by a large margin. Further experiments demonstrate that its effectiveness lies in label-correlation utilization rather than document representation."
SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of RKHSs, a simple hierarchical kernel network with two or three convolution and pooling layers, inspired by convolutional kernel networks. The authors show that the network consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK) in the finite-width limit of neural networks. The authors propose two novel algorithms that change the exponent of the compute and memory requirements of the NTK, dramatically improving efficiency. The proposed algorithms are general-purpose JAX function transformations that apply to any differentiable computation (convolutions, attention, recurrence, etc.)."
SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper studies the problem of offline constrained reinforcement learning in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. The paper proposes an algorithm that directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that the proposed algorithm achieves better policies in terms of constraint satisfaction and return-maximization, outperforming baseline algorithms."
SP:5a10c13eb78d26a25dac74601419deb68c53cb75," of Gated Recurrent Unit (GRU) networks is a challenging task, as the training procedure of GRU is inherently sequential. In this paper, the authors propose a novel parallel training scheme (called parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the new parallel learning scheme achieves up to 6.5x speedup over a serial approach."
SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a neural network architecture to learn a common embedding of fMRI data that denoises and reveals its intrinsic structure. The proposed architecture is based on the idea that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. The authors show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, as well as improves cross-subject translation of the fMRI signals. "
SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution detection for multi-class, multi-label, and multi-labelled datasets. The main contribution of the paper is the introduction of a new dataset of anomalous species for multiclass anomaly detection. The authors also propose a novel benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. The results show that a simple detector based on the maximum logit outperforms prior methods in all the large-scale multi class, multi label, and segmentation tasks. "
SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of tournaments that arise out of d-dimensional representations. The authors show that the tournament classes have forbidden configurations which must necessarily be union of flip classes, a novel way to partition the set of all tournaments. They further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows them to show the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure. For a general rank d tournament class, the flip class associated with a coned-doubly regular tournament of size O(d) must be a forbidden configuration. To answer a dual question, they show a lower bound of O(n) on the minimum dimension needed to represent all tournaments on n nodes."
SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a novel attention mechanism for neural processes (NPs) to capture appropriate context information. In particular, the authors propose a stochastic attention mechanism that encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in the target dataset and context embeddings independently. The proposed method is evaluated on 1D regression, predator-prey model, and image completion tasks. "
SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"This paper proposes a new approach to explainability and interpretability of Transformer language models. The main idea is to use prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the network’s decisions. Moreover, while the architecture performs on par with several language models, it enables one to learn from user interactions and uses human capabilities to incorporate knowledge outside of the rigid range of purely data-driven approaches."
SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. The authors propose a notion of ‘trust region’ to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layerwise scaling matrix. Extensive experiments show that TRGP achieves significant improvement over related state-of-the-art methods."
SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a generalization bound based on the length of the optimization trajectory of the gradient flow algorithm after convergence. The authors show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based bound, showing that short optimization paths after convergence indicate good generalization. The framework can be applied to underdetermined linear regression, kernel regression, and ReLU neural networks."
SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples. The authors show that adversarial attacks are not in high-frequency nor low-frequency components, but are simply dataset dependent. They show that robust models trained on CIFAR-10 and ImageNet-derived datasets are more robust than models trained with frequency constraints. They also propose a frequency based explanation for the commonly observed accuracy vs robustness trade-off."
SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f," of heterophily in graph neural networks (GNNs). This paper proposes a new metric based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. Based on the metrics and the observations, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophilies."
SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,This paper proposes a deep reinforcement learning approach for solving traveling salesman problems (TSPs). The main idea is to use local search heuristics to improve the generalizability of the proposed approach. The proposed approach is evaluated on a set of TSPs and is compared with state-of-the-art deep RL methods.
SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a new approach to solve the non-IID (independent and identically distributed) problem in federated learning. Specifically, the authors propose to use synthetic data to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation of synthetic data. Extensive experiments show that the proposed framework outperforms the baseline methods in several benchmark datasets under both the supervised and semi-supervised settings."
SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,This paper studies the trade-off between accuracy and adversarial robustness of smoothed classifiers under Gaussian noise. The authors propose a simple training method to obtain more robust classifiers through a sample-wise control of robustness over the training samples. They propose to use the “accuracy under gaussian noise” as an easy-to-compute proxy of the robustness for each input. They differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. They show that the proposed method exhibits improved certified robustness upon existing state-of-the-art training methods.
SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper studies the problem of packing the Wikipedia dataset for training BERT (Bidirectional Encoder Representations from Transformers). The authors show that by removing all padding tokens, they achieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic of the dataset, they develop and contrast two packing algorithms. The shortest-pack-first histogram-packing (SPFHP) algorithm determines the packing order for the Wikipedia datasets of over 16M sequences in 0.03 seconds. The non-negative least-squares histogram packing (NNLSHP) algorithms converges in 28.4 seconds but produces solutions which are more depth efficient, managing to get near optimal packing by combining a maximum of 3 sequences in one sample. Finally, the authors demonstrate that these changes are straightforward to implement and have relatively little impact on the achievable performance gain on modern hardware."
SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, this algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autoregression models. Empirically, the authors show that their adaptive tree-search algorithm finds outputs with substantially better model scores compared to beam search in autoresgressive models. The authors also characterise the correlation of several translation model objectives with respect to BLEU."
SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method to model the distribution of ""abnormalities"" by using Energy Based Model (EBMs). EBMs learn to associate low energies to correct values and higher energies to incorrect values. EBMs employ Langevin Dynamics (LD) in generating these incorrect samples based on an iterative optimization procedure, alleviating the intractable problem of modeling the world of anomalies. Then, in order to avoid training an anomaly detector for every task, the authors propose an adaptive sparse coding layer that can be used to quickly update what is normal during inference time. To avoid tedious data collection, they employ a meta learning scheme that simulates such a few shot setting during training."
SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper presents a large-scale dataset of contradicting contexts for question answering (QA) systems. ContraQA contains over 10K human-written and model-generated contradicting pairs of contexts. The authors show that QA models are vulnerable under contradicting context brought by misinformation. To defend against such threat, they build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection."
SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation learning that uses the distance between the expert and imitation agents to align and compare states between the different spaces of the agents. The authors show that GWIL preserves optimality in a variety of continuous control domains. GWIL is shown to outperform the state-of-the-art methods in a number of domains. "
SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a new method for self-supervised learning based on cross-level contrastive learning. The proposed method uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. Experiments on classification, detection, segmentation, and few-shot learning tasks show the effectiveness of the proposed method."
SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a multi-agent deep reinforcement learning (RL) approach to find Nash equilibria for a meta-game over agent types in economic simulations with many agents. The proposed approach is based on the use of structured learning curricula and efficient GPU-only simulation and training. The authors show that their approach can find Nash-equilibria by explicitly learning a spectrum of meta game-Nash equilibrium. They demonstrate their approach in real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes. "
SP:f885c992df9c685f806a653398736432ba38bd80,This paper proposes a method to prevent model extraction attacks by requiring users to complete a proof-of-work before they can read the model’s predictions. The method is based on differential privacy to measure the information revealed by a query. The proposed method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.
SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a multi-resolution version of the Neural Ordinary Differential Equation (ODE) model. The proposed model is based on the continuous normalizing flow (CNF) model, where the conditional distribution over the additional information required to generate a fine image that is consistent with the coarse image. The authors propose a transformation between resolutions that allows for no change in the log likelihood. They show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods."
SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect noisy labels in deep neural networks (DNNs). The proposed method is based on the idea that good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. Based on the neighborhood information, the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. Given good (but possibly imperfect) representations that are commonly available in practice, the paper theoretically analyzes how they affect the local voting and provides guidelines for tuning neighborhood size. Experiments with both synthetic and real-world label noise demonstrate the effectiveness of the proposed method."
SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a novel method for finding the optimal adversarial perturbations for reinforcement learning agents under the strongest/optimal perturbation on state observations (within some constraints) under the worst-case performance of a reinforcement learning agent. The key idea is to train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method that finds the optimal attacks through collaboration between a designed function named “actor” and an RL based learner named ‘director’. The actor crafts state perturbs for a given policy direction, and the director learns to propose the best policy perturbing directions. The proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL based works in environments with large state spaces. "
SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy generation algorithm for solving multi-objective problems with multiple solutions. The proposed algorithm is based on the interior point method commonly used in the constrained optimization literature. In particular, the authors propose a new metric to evaluate the difference between policies. The paper shows that the proposed algorithm can achieve a substantial improvement over previous novelty-seeking methods in terms of both the novelty of generated policies and their performances in the primal task."
SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes a method to learn to deverberate speech from audio-visual observations. The key idea is to use the visual environment surrounding a speaker to influence the precise reverberation effects in the audio stream. To this end, the authors develop a large-scale dataset that uses realistic acoustic renderings of speech in real-world 3D scans of homes offering a variety of room acoustics. The proposed method achieves state-of-the-art performance on both simulated and real imagery for speech enhancement, speech recognition and speaker identification."
SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper studies the problem of extrapolation of position embeddings to sequences that are longer than the ones seen during training. The authors propose a simple and efficient position embedding method, Attention with Linear Biases (ALiBi), to solve this problem. ALiBi does not add positional embedding to the word embedding, but instead it biases query-key attention scores with a penalty that is proportional to their distance. The proposed method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequence of length 2048, achieving the same perplexity as a sinusoidal position-based embedding model trained on inputs of size 2048 but training 11% faster and using 11% less memory."
SP:45ba2709aca444f50a133d71f33be9d2c1f887e8," in the multi-objective online learning setting. In this setting, the authors propose a novel dynamic dynamic regret in the unconstrained max-min form. They show that it is equivalent to the regret commonly used in the zero-order multi-Objective bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. Then they propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min-Norm solver. Extensive experiments demonstrate the effectiveness of the proposed algorithm and verify the theoretical advantage of the newly designed algorithm."
SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a new method for continual learning based on using generative models to provide replay patterns on demand. The authors show that the generated data are usually not able to improve the classification accuracy for the old classes, but they can be effective as negative examples (or antagonists) to learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on complex class-incremental and data incremental continual learning scenarios (CORe50 and ImageNet-1000)."
SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,This paper proposes an inductive graph partitioning (IGP) framework to alleviate the NP-hardness of graph partition (GP) problem. IGP first conducts offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. The trained model is then generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines.
SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGVAE employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. Importantly, the proposed framework is end-to-end permutation-equivariant with respect to node ordering. The proposed framework achieves competitive results with several generative tasks including general graph generation, molecular generation, unsupervised molecular representation learning, link prediction on citation graphs, and graph-based image generation."
SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper proposes a general framework for nonlinear ICA, in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. The authors provide an insightful proof of the identifiability of the proposed framework, and verify their theory by experiments on artificial data and synthesized images. Moreover, results on real-world images indicate that the framework can disentangle interpretable features."
SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional network (GNN) based on the BankGCN, which decomposes multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture. Experiments on several graph datasets demonstrate the effectiveness of the proposed method."
SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c," pre-training methods tend to push images from the same class close to each other despite of the large diversity in their visual contents. To alleviate this problem, this paper proposes a new supervised pre- training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and preserving part of intra-class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that LOOK outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method to generate geometric facial details that are consistent with any desired target expression. The proposed method is based on the use of FDS (Chen et al., 2019) to generate a geometric facial geometry that is consistent with the target expression, which is then used by a neural network to render novel images of any single image in any desired expression and view. The method is evaluated on a single input image, a target expression (in this case ‘Happy’), and an initial detailed geometry extracted using FDS as input, and the method hallucinates facial geometric details consistent with target expression and adds them to a smooth proxy geometry."
SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,This paper proposes a probabilistic generative model for unsupervised controllable content generation. The model is an attention-driven variational autoencoder (ADVAE) based on Transformer-based machine translation models. The authors show that ADVAE is able to learn representations of sentences where different syntactic roles correspond to different latent variables. They also propose an evaluation protocol to measure disentanglement with regard to the realizations of syntactic role. The experiments on raw English text from the SNLI dataset show that the proposed model can be induced without supervision. 
SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a new reward mechanism for multi-agent reinforcement learning (MARL) where agents are rewarded for contributing to a more diversified team behavior by employing proper intrinsic motivation functions. To learn meaningful coordination protocols, the authors propose a novel framework, where at each timestep, an agent simulates counterfactual rollouts of its policy and, through a sequence of computations, assesses the gap between other agents’ current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded. The authors evaluate their approach on a set of challenging tasks with sparse rewards and partial observability that require learning complex cooperative strategies under a proper exploration scheme."
SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc model editing for large pre-trained models. The proposed method is based on a low-rank decomposition of the gradient obtained by standard fine-tuning. The authors show that the proposed method can be trained on a single GPU in less than a day even for 10 billion+ parameter models. Once trained, MEND enables rapid application of new edits to the model."
SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2," PDEs (Burger’s, diffusion-sorption, diffusion reaction, Allen-Cahn) are studied in this paper. The authors propose a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The proposed method is based on combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in a Compositional manner. The results on both one-and two-dimensional PDE (in terms of accuracy and generalization ability) demonstrate the effectiveness of the proposed method."
SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper studies the connection between representations of code generated by unsupervised machine learning (ML) models and representations of computer programs in the human brain. The authors analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They discover brain representations, in different and specific regions of the brain, that encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also map brain representations to representations of a suite of ML models that vary in their complexity. "
SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model that consists of a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The proposed model outperforms the original Translatron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. The authors also propose a new method for retaining the source speaker’s voice in the translated speech."
SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of few-shot learning of semantic classes. In particular, the authors focus on the task of learning new attributes that were not previously labeled. They show that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. They also show that predictability of test attributes provides an informative estimate of a model’s generalization ability."
SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the Wasserstein gradient flow of relative entropy. The authors propose a novel nonparametric approach to estimate the logarithmic density ratio using neural networks. Extensive simulation studies on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate that the REGS outperforms the state-of-the-art sampling methods.
SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum machine learning approach to classify larger, realistic images using quantum systems. The approach relies on a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. The framework is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop. The authors also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance."
SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90," in federated learning. The paper proposes a differentially private local clustering (DPLC) mechanism to distill sanitized clusters from local class centers. A consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo results in more discriminative features. Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy of the proposed method."
SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a method for transfer learning based on linear probing. The main idea is to select features from all layers of the source model to train a classification head for the target-domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and compared with fine-tuning on average, but critically, for out-of-distribution transfer."
SP:d6f11fb32851f97af287f962f83220d27a8bc76a,This paper proposes an object-oriented text dynamics model for text-based games (TBGs). The proposed model is based on a memory graph that predicts the history of object observations and filters object-irrelevant information. The paper also proposes a variational objective to model the stochasticity of predicted dynamics. Empirical results show that the proposed model outperforms model-free baselines in terms of sample efficiency.
SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"-based loss minimization method is proposed to solve the problem of cost-sensitive hierarchical classification where a label taxonomy has a loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. In this paper, the authors propose a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. The authors prove that there is a bijective mapping between the original hierarchical loss and the set of layer-wise abstaining losses under symmetry assumptions. The proposed method is tested on large-scale bird dataset as well as cell classification problems."
SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks to process sound waveforms that have limited time-frequency resolution capability. The authors propose a Transformer-like backbone with two parallel soft-stitched branches to learn the filter banks. By alternating the periodicity term, each bank differs in its temporal length. Convolution of the proposed filterbanks with the raw waveform helps to achieve multi-scale perception in the time domain."
SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of domain adaptation where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors propose a method called GIFT (Gradual Interpolation of Features toward Target) that creates virtual samples from intermediate distributions by interpolating representations of examples from source and target domains. They show that under the following assumptions: (i) access to intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self-training can be successfully applied on gradually shifted samples to adapt the model toward the target distributions. They hypothesize that having (1) is enough to enable iterative self- training to slowly adapt the models to target distribution by making use of an implicit curriculum. In the case where (2) does not hold, they observe that iterative Self-Training falls short and propose GIFT. "
SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for video-text retrieval that learns representations from videos, videos, titles and comments, which are abundant on the internet. The authors propose an attention-based mechanism that allows the model to disregard text with irrelevant content. The experiments show that the proposed method is able to learn better, more contextualised, representations while also achieving competitive results on standard video- text retrieval benchmarks."
SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes DISDAIN, a method for unsupervised skill learning in which an ensemble of discriminators is trained to learn a discriminator for each skill, and the discriminator is penalized for not having seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. The authors propose to use an information gain auxiliary objective to encourage each discriminator to disagree with the policy for their disagreement. The proposed method is evaluated on a tabular grid world and 57 games of the Atari Suite."
SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a novel approach for molecular graph generation using deep neural networks (DNNs). The authors propose a spanning tree-based graph generation (STGG) framework based on formulating molecule graph generation as a construction of spanning tree and the residual edges. Based on the intermediate graph structure of the construction process, their framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. Experiments on QM9, ZINC250k and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Fréchet ChemNet distance, and fragment similarity."
SP:3a19340d6af65e3f949dda839a6d233369891c46,"The paper proposes a spectral analysis of the neural tangent kernel (NTK) of polynomial neural networks (PNNs) based on a recently proposed parametrization of PNNs. The analysis shows that the NTK has a spectral bias towards low-frequency functions, which leads to faster learning of low frequency components during training. The paper also proposes a new family of networks, called the Π-Net family, which is based on the recently proposed PNN-based neural network. The results show that the proposed network can speed up the learning of the higher frequencies. "
SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,This paper studies the problem of finding hidden subnetworks in sparse neural networks (NNs) that are not only hidden in the random networks but also “disguised” – hence can only be “unmasked” with certain transformations on weights. The authors propose a novel two-stage algorithm that plays a Peek-a-Boo (PaB) game to identify the disguised subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork’s remaining weights. They argue that the unmasking process plays an important role in enlarging the capacity of the subnetwork.
SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a method to jointly clean the graph and denoise features of data. The main contribution of the paper is to propose a General Unified Graph Neural Network (GUGNN) framework. The proposed method is based on the idea of reconstructing the graph with its intrinsic properties, including similarity of two adjacent nodes’ features, sparsity of real-world graphs and many slight noises having small eigenvalues in perturbed graphs. The other contribution is the convolution operation for features to find the optimal solution adopting the Laplacian smoothness and the prior knowledge that nodes with many neighbors are difficult to attack. Experiments on four real-life datasets demonstrate the effectiveness of the proposed method."
SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method is based on learning a continuous bijective mapping between 3D position and 2D texture-space coordinates. The surface parameterization network can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. The paper shows that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios. 
SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,", the paper proposes Adaptive Region Pooling (ARP), a novel downsampling algorithm that makes the network only focus on a smaller but more critical region and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation and show superior performance against the state-of-the-arts in both image classification and image retrieval."
SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,This paper proposes Explore-to-Extrapolate Risk Minimization (ERM) for node-level prediction on graph-structured data. ERM is a domain-invariant learning approach that uses GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that they design multiple context explorers (specified as graph editers) that are adversarially trained to maximize the variance of risks from multiple virtual environments. The authors prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts.
SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning approach for time series data augmentation for contrastive learning. InfoTS is based on the idea of information-aware meta learning, where the meta-learner and the encoder are jointly optimized in an end-to-end manner to avoid sub-optimal solutions. Experiments on various datasets show that InfoTS achieves competitive performance with up to 11.4% reduction in MSE on forecasting task."
SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the phenomenon of winning ticket universality in the Lottery Ticket Hypothesis. The authors propose a renormalization group theory approach to understand why winning tickets can be transferred across different tasks. They show that iterative magnitude pruning, the method used for discovering winning tickets, is a Renormalization Group scheme. This opens the door to a wealth of existing numerical and theoretical tools, some of which they leverage here to examine winning ticket universalality in large scale lottery ticket experiments. "
SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"-based models such as BERT have proven successful in information retrieval problem, which seek to identify relevant documents for a given query. There are two broad flavours of such models: cross-attention (CA) models, which learn a joint embedding for the query and document, and dual-encoder (DE) models. Empirically, CA models are often more accurate, which has motivated a series of works seeking to bridge this gap. However, a fundamental question remains less explored: does this performance gap reflect a limitation in the capacity of DE models, or in the training of such model? In this paper, the authors study this question, with three contributions. First, they establish theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. Second, they show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. Third, they propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural re-ranking datasets."
SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the relationship between importance sampling (IS) and variance minimization (VAE) in the context of off-policy policy optimization. In particular, the authors show that VAE can be used to improve the performance of the proposed policy optimization algorithm POPE, which uses VAE as an inner loop in order to reduce the variance of the estimator. In addition, they show that the proposed algorithm is more robust to small batch sizes. "
SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes Graph Parallelism, a method to train graph neural networks (GNNs) for modeling atomic simulations. The main idea is to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate their method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude on the large-scale Open Catalyst 2020 (OC20) dataset."
SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes Time Control (TC), a language model that implicitly plans via a latent stochastic process. The authors propose to learn a representation that maps the dynamics of how text changes in a document to the dynamics in the latent process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a process, and then generating text that is consistent with this latent plan. TC improves performance on text infilling and discourse coherence on long text generation settings."
SP:56a74403d4471cd95641dc669f5eac89a2c93144,This paper proposes a method to learn to predict future scenes in the presence of moving objects by learning to predict the coherent motion of their parts in visual input. The proposed method learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. The network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The model requires no supervision or pre-training of any part of the network.
SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning the disentangled representation from real spatio-temporal data for mobility forecasting. The authors propose a deep generative model that learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. The proposed method is able to achieve state-of-the-art performance across multiple spatial and temporal datasets. 
SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series. The authors propose a novel input layer to encode information about input observation sparsity, a temporal VAE architecture to propagate uncertainty due to input sparsity and a heteroscedastic output layer to enable variable uncertainty in output interpolations. The experimental results show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models."
SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper considers the question of how to measure the modularity of a neural network. The authors propose two proxies for this: importance and coherence, which are measures of how important a set of neurons are to the network's performance and how consistently their neurons associate with features of the input. The paper proposes to measure these proxies by spectrally clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights, reveal groups of neurons that are important and coherent."
SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the lottery ticket hypothesis to discover lightweight speech recognition models that are robust to various noise existing in speech, transferable to fit the open-world personalization, and compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. They obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparsification with no performance loss, and transfer exceptionally from large source datasets to various target datasets."
SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and ones entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Surprisingly, the ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization."
SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for backdoor removal from a poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. The authors propose the Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Theoretical analysis is provided to show its convergence and generalizability of the robustness gained by solving minimax on clean data to unseen test data. The proposed algorithm is evaluated on 11 backdoor attacks over two datasets and various attack settings.
SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD for strongly convex functions with smooth second derivatives. The authors show that for 1-dimensional strongly-convex functions, there exist permutations that offer exponentially faster convergence compared to random. They also show that in the case of quadratic functions, the convergence gap between optimal and random permutations can vary from exponential to nonexistent. "
SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes an automated normalizing flow (NF) architecture search method to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. A mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient NF optimization of deep flow models."
SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes to use Intrinsic Dimension (ID) and Cluster Learnability (CL) to measure the expressive and learnable representations induced by self-supervised learning (SSL) methods. To measure expressiveness, ID and CL can be combined to predict downstream classification performance better than the existing techniques based on contrastive losses or pretext tasks, while having no requirements on data augmentation, model architecture or human labels. To further demonstrate the utility of the framework, the authors propose modifying DeepCluster (Caron et al., 2018) to improve the learnability of the representations."
SP:4f5c00469e4425751db5efbc355085a5e8709def,This paper proposes to use segmentation-based black-box adversarial attacks to improve the imperceptibility of adversarial perturbations. The proposed method is based on the idea that the perturbation should be limited to the salient region of the network. The authors show that the proposed method improves the performance of the adversarial attack. They also propose a new gradient-free attack that can further improve the performance by refining the perturbs in the salient area.
SP:779821ed85084f8bf1b29d8822b312989b186ee9," of SMILES representations are not efficient for capturing information about molecular structures. This paper proposes a novel Graph2SMILES model that combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. The encoder is an attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding. The proposed model improves the top-1 accuracy of the Transformer baselines."
SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes to use reinforcement learning to solve the problem of automatic disease diagnosis with dialogues in task-oriented dialogues setting. The authors propose to integrate a hierarchical policy of two levels into the dialogue policy learning. The high level policy consists of a master model that is responsible for triggering a low level model, which consists of several symptom checkers and a disease classifier. Experimental results on both self-constructed real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing these challenges. The authors analyze the compatibility of various centralized self supervised learning methods in the FL setting and demonstrate that SimSiam networks performs the best with the standard FedAvg algorithm. Moreover, to address the data heterogeneity at the edge devices in this framework, the authors have innovated algorithms that broaden existing supervised personalization algorithms into the setting of selfsupervised learning including perFedAvg, Ditto, and local fine-tuning, among others. To provide a comprehensive comparative analysis of all proposed algorithms, they also develop a distributed training system and related evaluation protocol for SSFL."
SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper proposes a general form of PDEs for the design of ResNet-like DNNs. To achieve this goal, the authors formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show that the adjustment operator is the solution operator of the PDE. To show the effectiveness of the general form, several effective networks can be interpreted by our general form. Theoretically, the robustness of DNN trained with our method is certifiable and our training method reduces the generalization gap for DNN."
SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the expressivity of emergent languages in the context of language games, where agents interact with each other and use a language to solve a task. The authors propose to measure the expressiveness of the emergent language based on its generalisation performance across different games. They show that expressivity is a trade-off between the complexity and unpredictability of the context those languages are used in. They also show that using the contrastive loss proposed by Chen et al. (2020a) can alleviate this problem, compared with the standard referential loss used by the existing works."
SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a variant of Sample Average Uncertainty (SAU) to tackle the exploration-exploitation dilemma in reinforcement learning. The main contribution of this paper is to extend SAU from the bandit setting to the general sequential Reinforcement Learning scenario. In particular, the authors propose a new exploration strategy called “δ-exploration”, which extends SAU to the tabular setting. The authors show that the proposed strategy outperforms SAU in both tabular and deep Q-learning settings."
SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes an implicit generative adversarial network (IGAN) for video generation. The proposed method is based on implicit neural representations (INRs) that encodes a continuous signal into a parameterized neural network. In particular, the authors propose a video generator that improves the motion dynamics by manipulating the space and time coordinates differently and a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. They demonstrate the superiority of DIGAN under various datasets, along with multiple intriguing properties, e.g., long video synthesis, video extrapolation, and non-autoregressive video generation on UCF-101."
SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of private multi-winner voting. The authors propose three new privacy-preserving multi-label mechanisms: Binary, τ, and Powerset voting. Binary voting operates independently per label through composition, and the authors show that it votes optimally in their `2 norm. They also provide a theoretical analysis of the trade-off between binary and powerset voting, and show that the latter requires strong correlations between labels to outperform Binary voting. They empirically show that their proposed methods outperform DPSGD on large real-world healthcare data. "
SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a method to transfer the implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. The proposed method is based on the idea of grafting, which allows for the transfer of the overall implicit step-size schedule from the original optimiser to the new optimiser. The authors show that this method leads to a reduction in the computational cost of optimizer hyperparameter search. They also propose a non-adaptive learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance."
SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes an algorithm for online reinforcement learning in sparse reward settings. The main idea is to use the offline demonstration data generated by a sub-optimal behavior policy for faster and efficient online RL in such sparse reward setting. The proposed algorithm, called Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demo data. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimality policy, while yet being able to learn beyond and approach optimality. Theoretical analysis of the proposed algorithm is provided, and a lower bound on the performance improvement in each learning episode is provided. The paper also extends the algorithm to the even more challenging incomplete observation setting where the demonstration data contains only a censored version of the true state observation."
SP:cf857736e3dc01325948488c791cbafc24b1c0fe,This paper proposes a two-stage Pareto framework for multi-task learning (MTL). The first stage uses a neural network to extract the weak Paredto front from a set of problems. The second stage uses an FJC guided diffusive manifold to reduce the error between the true and the weak front. Experiments show the effectiveness of the proposed method. 
SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce," distillation is an approach to learning a unified representation of a set of expert models for a given task. The idea is to distill the knowledge of a teacher into a unified set of teacher representations. The teacher representations are then used to train a student model. The student model is trained to emulate the input/output functionality of the teacher. The proposed method is based on a multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher."
SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to the constrained empirical risk minimization (ERM) problem. The main contribution of the paper is to propose a gradient-based algorithm for the constrained ERM problem using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications such as prediction intervals with improved length, minimum-volume prediction sets, and label prediction sets for image classification."
SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a method to learn a transferable reward signal for object localization. The proposed method is based on the idea of using a small set of exemplary objects to train an agent to find the object of interest. The reward signal is learned by using ordinal metric learning. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of the proposed method."
SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes QuadTree Attention, a transformer-based approach to reduce the computational complexity from quadratic to linear. The proposed approach builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patch. The experimental results show that the proposed approach achieves state-of-the-art performance in various vision tasks."
SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper studies the problem of learning reusable temporally extended actions, or options, in reinforcement learning. Motivated by the recent success of mutual information (MI) based skill learning, the authors propose a method for learning termination conditions of options by maximizing MI between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards, combined with intrinsic rewards, and the reusability of learned option by transferring options into various tasks."
SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper studies the problem of open-world object detection in the context of semantic topology. In this paper, the authors propose to use a pre-trained language model to generate semantic topologies for the object detection task. The proposed approach is based on the idea that the object instances from the same category are assigned to their corresponding pre-defined node in the topology, including the ‘unknown’ category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed approach outperforms the current state-of-the-art open world object detectors by a large margin."
SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a greedy algorithm to identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. The proposed algorithm outperforms competing baselines on standard classification datasets such as CIFAR10, CifAR-100, ImageNet, as well as long-tailed datasets."
SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper studies the problem of out-of-distribution generalization in semantic segmentation. The authors propose two methods to improve the generalization performance of the model. First, they propose Instance-adaptive Batch Normalization (IaBN) to modify the normalization layers by combining the feature statistics acquired at training time with those of the test sample. Second, the authors propose Seg-TTT to adapt the model parameters to the test samples using a self-supervised loss. Experimental results show that the proposed methods consistently and significantly outperform the baseline."
SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a method for finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. To this end, the authors propose to learn mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, the mappings can be used to score a test sample by measuring whether the learned mappings lead to a small loss using the masked parts of this sample. The experiments show that the proposed method leads by a sizable accuracy gap in comparison to the literature. "
SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a novel approach to learn a low-dimensional embedding space that preserves the diagnostic attributes of represented disorders. To this end, the authors propose a conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The proposed approach is evaluated on two neuropsychiatric neuroimaging datasets and discovers a reliable nosological relation among autism spectrum disorder, major depressive disorder and schizophrenia."
SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes an end-to-end learning framework named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit. The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding for QNQ embedding. The authors theoretically characterize QTN by analyzing its representation power of input features and show that QTN enables an end to end parametric model pipeline from the generation of quantum embeddings to the output measurement. The experiments on the MNIST dataset demonstrate the advantages of the proposed QTN for QTN."
SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method to construct low-dimensional manifolds where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The proposed method, called DYNAMO, takes as input a collection of pre-trained neural networks and outputs a meta-model that emulates the dynamics of the hidden states as well as the outputs of any model in the collection. The specific model to be emulated is determined by a model embedding vector that the meta-models takes as an input. The authors apply the method to both RNNs and CNNs and find that the resulting model embeddings spaces enable novel applications such as clustering of neural networks in a manner that is less sensitive to reparameterization. "
SP:29a42fdae15b9da955513f71e3100ebd0146a28a,This paper proposes a method for constraint-based learned simulation where a scalar constraint function is implemented as a trainable function approximator and future predictions are computed as the solutions to a constraint satisfaction problem. The proposed method uses a graph neural network as a constraint function and gradient descent as the constraint solver. The architecture can be trained by standard backpropagation. The paper shows that the proposed method achieves better or comparable performance to top learned simulators.
SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes EDO-CS, a new method to find a set of policies with both high quality and diversity in reinforcement learning. The proposed method is based on the idea of clustering-based selection, where the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. Experiments on several continuous control tasks show the effectiveness of the proposed method."
SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a multi-agent linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction. The interconnection structure among the agents is described by a time-varying directed graph. The paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. The equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication."
SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes Graph Mechanics Network (GMN) which is a graph neural network (GNN) that is combinatorially efficient, equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Theoretically, the proposed GMN formulation is proved to be universally expressive under certain conditions. Extensive experiments show that GMN outperforms the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on simulated systems consisting of particles, sticks and hinges."
SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper studies the problem of federated learning with partial model personalization. The authors propose two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. They give convergence analyses of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalisation with a small fraction of personalized parameters."
SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations by simulating viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in such unsegmented viewing sequences. The authors develop a new data set using a near-photorealistic training environment based on ThreeDWorld (TDW). They consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. "
SP:2fb4af247b5022710b681037faca2420207a507a,"This paper studies the problem of goal-directed planning under a deterministic transition model. In particular, the authors propose an extension of the AlphaZero family of algorithms to tackle the problem. The main contribution of the paper is the extension of AlphaZero with Hindsight Experience Replay to tackle goal-driven planning tasks. The proposed approach is evaluated on a number of simulated domains, including a quantum compiling domain. "
SP:e2d33c7331db7f52b84ad1018152564d91a9f126, learning multiple tasks sequentially without forgetting previous knowledge. The paper proposes Recursive Gradient Optimization (RGO) which is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer (FEL) that represents different network structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines.
SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper studies the properties of aligned generative models (i.e., models that share the same architecture and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. The paper shows that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. It also shows that zero-shot vision tasks may be performed in the child domain while relying exclusively on supervision in the parent domain."
SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes to relax the Gromov-Wasserstein (GW) distance based on Optimal Transport (OT) by introducing a new semi-relaxed GW divergence. The proposed divergence is based on the idea of conservation of mass, which imposes a coupling between all the nodes from the two considered graphs. The authors argue that this property can be detrimental for tasks such as graph dictionary or partition learning, and propose to relax it by proposing a new GW divergence based on a semi-Relaxed Wasserstein divergence. They show that the proposed GW divergence can lead to an efficient graph dictionary learning algorithm, and empirically demonstrate its relevance for complex tasks on graphs."
SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method to model systematic suboptimality of human behavior. The authors propose to use the Boltzmann policy distribution (BPD) as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human-AI collaboration equally as well as imitation learning-based human models while using far less data."
SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free quantization method for deep neural networks (DNNs) with sub-second quantization time. The main idea is to decompose the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, the authors propose a novel optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (CASE), which surprisingly does not need any dataset and is even not aware of network architecture. The authors also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver. Finally, without fine-tuning and synthetic datasets, SQuant accelerates the data free quantization process to a subsecond level with > 30% accuracy improvement over the existing data-based quantization works."
SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes SegTime, a method for time series segmentation. The proposed method is based on a bi-pass architecture with several structures that can process information in a multi-scale fashion. The main contribution of SegTime is to find precise breakpoints, obviates sliding windows, handles long-term dependencies, and it is insensitive to the label changing frequency. Experiments show that SegTime outperforms the baselines."
SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. The main idea is to decompose the information generation and aggregation mechanism of GNNs by tracking the contributions of specific components of the input graph to the final prediction. Based on this, a subgraph level interpretation algorithm is proposed to reveal complex interactions between graph nodes that are overlooked by previous methods. Experiments are conducted on synthetic and real-world datasets to demonstrate the effectiveness of the proposed method."
SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,", the authors propose a new way to learn the stride of a down-sampling layer. The idea is to use a cropping mask in the Fourier domain to learn a differentiable stride. The authors show that their approach can be used as a drop-in replacement to standard downsampling layers and outperform them. "
SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for soft local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on a novel localized randomized smoothing approach, where the random perturbation strength for different input region is proportional to their importance for the outputs. The proposed method yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks."
SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes an approach for embedding domain-specific inductive biases in normalizing flows. The proposed approach is based on embedding the embeddings of a probabilistic model into a bijective embedding. The embedding is constructed by converting the embedding into an embedding of a differentiable model. In addition, a gated structure is introduced to bypass the parts of the model that fail to capture the statistics of the data. Experiments are conducted to show the effectiveness of the proposed approach. "
SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of text-only language models (LMs) to learn to map a conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. The authors show that large pre-trained LMs have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. They investigate a range of generative language models of varying sizes, and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well."
SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the effect of shaped rewards on the learning process of emergent language experiments in a simple sender-receiver navigation game. The paper shows that shaped rewards can explicitly bias the semantics of the learned language, significantly change the entropy of the learning language, and mask the potential effects of other environmental variables of interest. "
SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. IWDA is based on the observation that invariance of the feature representations strongly correlates with transfer performance, and distributional shift in class priors between data in the source and target languages negatively affects performance. The proposed method is evaluated on a set of synthetic and real-world datasets. "
SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a new meta-learning algorithm based on bootstrapping. The algorithm first bootstraps a target from the metalearner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-metric) metric. The proposed algorithm achieves a new state-of-the-art for model-free agents on the Atari ALE benchmark and shows that it yields both performance and efficiency gains in multi-task meta learning. "
SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based reinforcement learning agents in comparison to their model-free counterparts. The authors evaluate MuZero on both procedural and task generalization. They identify three factors of procedural generalization --planning, self-supervised representation learning, and procedural data diversity -- and show that by combining these techniques, they achieve state-of-the-art generalization performance and data efficiency on Procgen. However, they find that these factors do not always provide the same benefits for task generalisation in Meta-World, indicating that transfer remains a challenge. Overall, the authors suggest that building generalizable agents requires moving beyond the single-task, model-Free paradigm and towards self-Supervised model-Based RL."
SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph inverse network (GDN) to solve the problem of graph reconstruction in the presence of unobserved, noisy, or dynamic graphs. In particular, the authors propose a graph convolutional network (GCN) to learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. The proposed method can generalize to graphs orders of magnitude larger than those seen in training. Experiments are conducted on the Human Connectome Project-Young Adult neuroimaging dataset."
SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes an algorithm for reward shaping in RL where the reward function is constructed in a Markov game between two agents. In the game, the reward-shaping agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (controller) learns the optimal policy for the task using these shaped rewards. The authors prove that ROSA learns to construct a shaping-reward function that is tailored to the task thus ensuring efficient convergence to high performance policies. They demonstrate the effectiveness of the proposed algorithm in three experiments."
SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a novel approach to improve the robustness of vertical federated learning (VFL) models against backdoor attacks and inference phase adversarial attacks. In particular, the authors propose to recover the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. The authors conduct extensive experiments on NUS-WIDE and CIFAR-10 datasets and show that RVFR outperforms different baselines in terms of robustness."
SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes to use contrastive learning as a way to train unsupervised dense retrievers for information retrieval. The authors show that the proposed method outperforms BM25 on 8 out of 14 datasets. They also show that fine-tuning on the MS MARCO dataset leads to state-of-the-art results on the BEIR benchmark. 
SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the trade-off between fine-tuning and linear probing in-distribution (ID) and OOD (OOD) transfer. The main contribution of the paper is the analysis of the tradeoff between the two-layer linear networks. The paper shows that fine-tuneing can achieve worse accuracy than linear probing when the pretrained features are good and distribution shift is large. In addition, the paper theoretically analyzes the tradeoffs arising in overparameterized overparametrized linear networks and characterizes how fine tuning can distort high-quality pretrained feature which leads to low OOD accuracy."
SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning to discover novel classes (L2DNC) from unlabeled data from seen and unseen classes. The authors propose a meta-learning approach to solve the problem. The main contribution of the paper is to show that under certain assumptions, the proposed approach is theoretically and empirically solvable. The paper also shows that the proposed method can be used to reduce the amount of unlabelled data needed for training."
SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper studies the problem of learning a causal transition model in POMDPs, where the agent has access to a large collection of offline experiences obtained by observing another agent interacting with the environment (observational data). The authors propose a general yet simple methodology for safely leveraging offline data during learning. In a nutshell, the method relies on learning a latent-based causal model that explains both the interventional and observational regimes, and then inferring the standard POMD transition model via deconfounding using the recovered latent variable. The authors prove their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case)."
SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper studies the problem of using a retriever to retrieve passages from a textual knowledge corpus (e.g., Wikipedia) and providing these passages as additional context to the generator for open-ended generation tasks. The authors propose using an additional guide retriever that is allowed to use the target output and “in hindsight” retrieve relevant passages during training. The approach is based on the posterior distribution Q of passages given the input and target output, and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. Experiments on Wizard of Wikipedia dataset show that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage, and the end-to-end system produces better overall output."
SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper studies the problem of influence estimation and influence maximization in combinatorial optimization. The authors propose a graph neural network (GNN) that learns to predict the influence of a given node in a graph. The network is trained on a small number of simulated graphs, and is then used to train a GNN for influence estimation. Experiments on real graphs are conducted to show the effectiveness of the proposed method. "
SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper studies the problem of active learning with Lipschitz functions in the context of domain adaptation. The main contribution of the paper is to derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds. The numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning techniques in the domain adaptation setting.
SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,".  The paper proposes a new variational approximation for Bayesian neural networks. The approximation relies on a central result from singular learning theory according to which the posterior distribution over the parameters of a singular model is asymptotically a mixture of standard forms. Affine coupling layers are employed to learn the unknown desingularization map, effectively rendering the proposed methodology a normalizing flow with the generalized gamma as the source distribution."
SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"-based methods have been proposed for domain generalization (DG) problems. The authors propose a novel generalization bound for DG based on the Rademacher complexity of the model. Based on this bound, the authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. Algorithmically, this analysis suggests that domain generalisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective."
SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the maximum n-times coverage problem, which is a generalization of the multi-set multi-cover problem, where each element must be covered at least n times. The authors show that this problem is NP-complete and is not submodular, and propose two new practical solutions based on integer linear programming and sequential greedy optimization. They also show that the proposed method produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual’s HLA molecules."
SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes a new initialization method for spiking neural networks (SNNs) based on the slant asymptotic formula for the response curve of the spiking neurons. The proposed method is based on a slant-asymptotically-invariant weight initialization. The authors show that the proposed method can improve the training speed and the model accuracy compared with existing SNN initialization methods and existing deep learning initialization methods. 
SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a federated learning algorithm to mitigate the impact of the distribution shift in the client data. The authors propose to use a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments on EMNIST and CIFAR datasets show that the proposed algorithm can effectively mitigate the effect of distribution shift and significantly improve the final model performance."
SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"-based deep network (DNN) pruning is a well-studied and well-understood problem in the literature. The authors propose a novel approach to pruning DNNs based on a recently developed spline interpretation of DNs. They show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis. They leverage this new insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate that their new spline-based DN pruning approach reduces training FLOPs by up to 3.5x while achieving similar or better accuracy than current state-of-the-art methods."
SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning, where the representation is learned in the outer-level, and the optimal group predictors are updated in the inner-level. The authors propose an implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The proposed bi-level objective is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in the literature. The paper further analyzed the error gap of the implicit approach."
SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the effectiveness of reinforcement learning via supervised learning (RvS) methods for offline reinforcement learning. RvS methods are based on the idea that data that is suboptimal for one task may nonetheless be useful for learning to solve another task. The main idea is to use a policy architecture and a conditioning variable to learn the optimal policy. The authors conduct extensive experiments on a range of offline RL benchmarks, including datasets with little or no optimal data. They find that the most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). "
SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to model the exploration behavior of humans using a Hierarchical Bayesian Bayesian framework. The proposed method is based on the idea that humans explore new environments by inferring the structure of unobserved spaces by re-using spatial information collected from previously explored spaces. The authors propose a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as their MapInduction framework. They show that their computational framework better predicts human exploration behavior than non-inductive models. "
SP:cccdcc95c4177b5531bad23b662060fdd0d88849, and pose tracking tasks. The paper proposes a differentiable approach to learn the probabilistic factors used for inference by a non-parametric belief propagation algorithm. The authors propose to replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The method learns to maintain a set of marginal posterior samples using end-to-end training. 
SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,". This paper proposes MoLeR, a graph-based generative model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The experiments show that the proposed method performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffoldbased tasks, while being an order of magnitude faster to train and sample from than existing approaches."
SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies the problem of imitation learning for deterministic experts. The authors show that imitation learning can be done by reduction to reinforcement learning with a stationary reward. The main contribution of the paper is the theoretical analysis that certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. They conduct experiments which confirm that their reduction works well in practice for continuous control tasks."
SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the effect of reweighting algorithms on the worst-group performance of machine learning models for fairness. In particular, the authors prove that under the overparameterized setting, an overparametrized model always converges to the same ERM interpolator that fits all training samples. Then, they analyze whether adding regularization helps fix the issue, and they prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper studies the use of the Rational Inattention (RI) model in multi-agent reinforcement learning (MARL) to study emergent behavior in complex agent-based simulations. In particular, the authors propose to use the RIRL model to model the cost of cognitive information processing using mutual information. The authors show that the proposed model generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs. They also show that using the proposed method yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. "
SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a novel approach to generate imperceptible audio adversarial examples by exploiting the phase-oriented energy dissipation of the audio spectrogram. The authors propose to use the spectrogram consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrograms. Moreover, the authors propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate that the proposed approach can generate full-sentence imperceptibly audio adversaries examples and achieve a 6.64x generation speed-up over the state-of-the-art. The proposed approach is also shown to be robust to simulated environmental distortions."
SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,", the authors propose a generalized version of DirectPred, called DirectSet(α) that learns a desirable projection matrix and also reduces the sample complexity on downstream tasks. Inspired by their theory, they simplify DirectPred by removing the expensive eigen-decomposition step. On CIFAR-10, CifAR-100, STL-10 and ImageNet, DirectCopy, our simpler and more computationally efficient algorithm, rivals or even outperforms DirectPred."
SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is a gradient-based recurrent sequential learning method. The main contribution of the paper is to derive a time-discretized version of the system of ODEs. The authors show that LEM can mitigate the exploding and vanishing gradients problem, which is a well-known challenge for gradient based sequential learning methods. They also prove that the proposed method can approximate a large class of dynamical systems to high accuracy. "
SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a geometric deep learning algorithm for learning to predict rotation and permutation equivariance on small point clouds. The proposed algorithm is based on the geometric algebra, which is used to construct a set of products of terms from geometric algebra and reduces over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation-equivariance. The authors demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology."
SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem in a real-time fashion. The proposed method is based on the edge-feature-embedded graph attention mechanism, which considers the high-dimensional edge features and accounts for the heterogeneous information. The model is size-invariant for problem instances of any scale and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality."
SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a novel framework for out-of-context (CODC) dialogue summarization task. The proposed framework consists of a CODC inference module leveraging external knowledge from WordNet and a knowledge attention module aggregating the inferred knowledge into a neural summarization model. To evaluate the inference capability of different methods, the authors also propose a new evaluation metric based on CODD. Experiments suggest that current automatic evaluation metrics of natural language generation may not be enough to understand the quality of out of-context inference in generation results, and the proposed model can provide statistically significant improvements on CIDEr."
SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of embedding entity embeddings in the context of learning semantic dependencies between entities and their associated attributes. In particular, the authors focus on the setting where the embedding of an entity is obtained by pooling its embedding from its known attributes. The authors study the theoretical limitations of different embedding strategies, rather than their ability to effectively learn attribute dependencies in practice. They first show a number of negative results, revealing that some of the most popular embedding models are not able to capture even basic Horn rules. However, they also find that some embedding methods are capable of modelling both monotonic and non-monotonic attribute dependencies."
SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based models on different natural language processing tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. The paper shows that the transformer models perform well on the mathematical reasoning task, but fail on the logical reasoning task. It also shows that transformers perform poorly on the reasoning tasks on the natural language task."
SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper introduces the compositional problem graph as a generalization framework to relate tasks of different complexity in terms of problems with shared subproblems. The compositional generalization problem is used to measure how readily old knowledge can be reused and hence built upon. The paper proposes a compositional recursive learner, a framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. Experiments are conducted on a symbolic and a high-dimensional domain to show the effectiveness of the proposed method."
SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP) which integrates the activation pruning with the weight pruning. Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency. The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets. "
SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a novel method for feature selection from large, high-dimensional, nonlinear observational datasets by generating a subset of features for experts to focus on. The proposed method is based on the Generative Adversarial Networks framework to allow us to generate knockoffs with no assumptions on the feature distribution. The method consists of 4 networks, a generator, a discriminator, a stability network and a power network. Experiments show that the proposed method performs as well as the original proposed knockoff generation model in the Gaussian setting and that it outperforms the original model in non-Gaussian settings."
SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes spatial-Winograd pruning to reduce the computational cost of deep convolutional neural networks (CNNs). The main contribution of the paper is to propose a new pruning method for the Winograd convolution. The main idea is to prune the spatial-domain weights in a structured way to transfer the sparsity from the spatial domain to the winograd domain. The authors also propose to use an importance factor matrix to adjust the weight importance and gradients of the pruned network. The proposed method is evaluated on CIFAR-10, Cifar-100, and ImageNet datasets."
SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed model is based on the Adversarially Learned Mixture Model (AMM). The AMM is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available.
SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes a new estimator for backpropagating gradients through stochastic binary layers. The proposed estimator is based on the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. The estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric “self-control” baseline function together with the REinFORCE estimator in that augmented space. Experimental results show that the estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation."
SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a new probabilistic programming language called MXFusion, which includes a new type of re-usable building blocks, called probababilistic modules, which consists of a set of random variables with associated probablistic distributions and dedicated inference methods. Under the framework of variational inference, the pre-specified inference methods of individual probabularies can be transparently used for inference of the whole probabulary model. Experiments on real data demonstrate the power and convenience of the proposed method. "
SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method to prune a neural network once at initialization prior to training. The proposed method is based on a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. The method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to various architectures."
SP:986b9781534ffec84619872cd269ad48d235f869,This paper studies the effect of beam search on the performance of the beam search algorithm on sequence synthesis tasks. The authors find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. They propose two methods to constrain the search and show that constrained beam search effectively eliminates beam search degradation and in some cases even leads to higher evaluation scores.
SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes Backplay, a method to improve the sample efficiency of model-free reinforcement learning (RL) by using a single demonstration to construct a curriculum for a given task. The idea is to start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. The method is evaluated on grid worlds and a four-player zero-sum game. "
SP:426c98718b2dbad640380ec4ccb2b656958389bc,This paper proposes a method to automatically decide the compression ratio for each layer of a neural network. The proposed method is based on a Kronecker-factored approximation of the Hessian as the pruning criterion. The paper shows that the proposed method outperforms previous state-of-the-art methods on several datasets and architectures.
SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper proposes a framework to visualize and understand GANs at the unit-, object-, and scene-level. The authors first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, they quantify the causal effect of interpretability units by measuring the ability of interventions to control objects in the output. They examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. They provide open source interpretation tools to help researchers and practitioners better understand their GAN models."
SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper studies the relationship between the distance between the parameters of a neural network and the function that maps the input to the output of the network. In particular, the authors show that the L/` ratio decreases throughout optimization, reaching a steady value around when test error plateaus. The authors also propose a new learning rule that constrains the distance a network can travel through L-space in any one update. This allows new examples to be learned in a way that minimally interferes with what has been learned."
SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a deep neural network framework called Dynamics Modeling Network (DyMoN) to model biological systems. DyMoN is trained as a deep generative Markov model whose next state is a probability distribution based on the current state. It is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. It can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements."
SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a novel unsupervised classification method based on graph Laplacian. The key idea is to introduce a approximate linear map and a spectral clustering theory on the dimension reduced spaces into generative adversarial networks. Inspired by the human visual recognition system, the proposed framework can classify and also generate images as the human brains do. The proposed method can also classify the images by using the estimated number of classes."
SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning permutation-invariant representations of sets. The proposed method is based on a permutation optimisation module that learns how to permute a set end-to-end. The permuted set is further processed to learn a representation of that set, avoiding a bottleneck in traditional set models. Experiments on four datasets demonstrate the effectiveness of the proposed method."
SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes Projective Subspace Networks (PSNets) to learn non-linear embeddings from limited supervision. In contrast to previous works, the embedding in PSN deems samples of a given class to form an affine subspace. The authors show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification."
SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), CAML can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,This paper proposes a framework where the user controls what characteristics of the data they want to share (utility) and what they wish to keep private (secret) without asking the utility provider to change its existing machine learning algorithms. The paper first analyzes the space of privacy-preserving representations and derive natural informationtheoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of the original data X. The paper then presents explicit learning architectures to learn privacy preserving representations that approach this bound in a data-driven fashion. The utility providers are willing to collaborate with the sanitization process. 
SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,". This paper proposes a progressive augmentation method to improve the stability of GANs. The main idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. Experiments on MNIST, Fashion-MNIST, CIFAR10 and CELEBA show the effectiveness of the proposed approach."
SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a method to identify common features among neurons in the primary visual cortex (V1) by using a rotation-equivariant convolutional neural network (CNN) to predict neural responses to natural stimuli. The proposed method is based on the observation that V1 neurons perform similar computations (e.g., orientation selectivity, phase invariance, etc). The authors show that the proposed method outperforms a regular CNN with the same number of feature maps and reveals a number of common features which are shared by many neurons and are pooled sparsely to predict the neural activity."
SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper studies the problem of distributed training of deep neural networks on large datasets. The authors propose a simple algorithm called SIGNSGD, where workers only send the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision distributed SGD. The paper proves that the algorithm converges in the large and mini-batch settings, establishing convergence for a parameter regime of ADAM as a byproduct. "
SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a GAN approach to generate realistic and high-fidelity stock market data based on generative adversarial networks. The order stream is modeled as a stochastic process with finite history dependence, and a conditional Wasserstein GAN is used to capture history dependence of orders in a stock market. Experiments on synthetic and real data show that the generated data is close to real data."
SP:ba66503753b3c57781b435c55c47fc9f69450e65,This paper proposes an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. The proposed framework draws upon approaches for supervised learning with noisy data. The core ideas of the proposed framework include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that policies based on the estimated surrogate reward can achieve higher expected rewards and converge faster than existing baselines.
SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network structure (depth and width) on halting time and shows that larger models (wider models in particular) take fewer training steps to converge. The authors show that halting time improves when growing model’s width for three different applications, and the improvement comes from each factor: the distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with power law, and gradient vectors become more aligned with each other during traversal."
SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper studies the problem of finding optimal algorithms for online optimization problems. To this end, the authors propose a novel learning framework based on primal-dual reinforcement learning. The authors introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst-case. They test their new ideas on the AdWords problem, the online knapsack problem, and the secretary problem. The results indicate that the models have learned behaviours that are consistent with the optimal algorithms. "
SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper studies the trade-off between generality and performance when we use inductive biases in deep RL algorithms. The authors propose to replace several domain-specific components in RL algorithms with adaptive components that modify the agent’s objective and environment interface. They show that the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature, but sometimes the adaptive components perform better. They also investigate the main benefit of having fewer domain specific components by comparing the learning performance of the two systems on a different set of continuous control problems without additional tuning of either system."
SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for the vision-and-language navigation (VLN) task. The proposed method consists of two components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images, and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. The method is tested on a standard benchmark and analyzed through a series of ablation studies that elucidate the contributions of the primary components."
SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes two methods to improve the performance of neural program synthesis from input-output examples. The main contribution of the paper is to propose execution-guided synthesis and synthesizer ensemble, which are general enough to be combined with any existing encoder-decoder-style neural program synthesizer. Experiments on the Karel dataset show the effectiveness of the proposed methods."
SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the stability, convergence and acceleration properties of batch normalization (BN) from a theoretical perspective. The main contribution of the paper is a theoretical analysis on BN applied to a simplified model: ordinary least squares (OLS). The authors show that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, acceleration effects, as well as insensitivity to the choice of learning rates. The authors also demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems."
SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,This paper studies the problem of data noising in recurrent neural network language models (RNNs). The main contribution of the paper is to provide a theoretical analysis of the variational distribution of RNNs. The authors show that each RNN is an instance of a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution. They use this insight to propose a more principled method to apply at prediction time and propose natural extensions to data noisings under the Variational framework. 
SP:f4a914d3df1a5a21a7365ba78279420f39210884,". This paper proposes a method for classifier-agnostic saliency map extraction. The idea is to find all parts of the image that any classifier could use, not just one given in advance. The proposed method outperforms existing weakly-supervised localization techniques on ImageNet dataset."
SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,This paper proposes a method to learn a student model from a large number of deep neural networks. The student model is trained to be independent of the teacher network. The teacher network is used as an initialization for fine-tuning a new student model. The proposed method is evaluated on a variety of supervised and reinforcement learning tasks.
SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes a soft-clustering approach to learn a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. The authors propose an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. They cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. They provide theoretical guarantees concerning the convergence of the proposed learning algorithm. 
SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a variational autoencoder-based neural probabilistic model that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in “one shot”. The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems shows the effectiveness of the proposed approach and diversity of the generated samples."
SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes an approach to reduce the memory footprint of deep neural networks during training. During the forward pass, the authors replace activations with lower-precision approximations immediately after they have been used by subsequent layers, thus freeing up memory. The approximate activations are then used during the backward pass. This approach limits the accumulation of errors across the forward and backward pass, because the forward computation across the network still happens at full precision, and the approximation has a limited effect when computing gradients to a layer’s input. Experiments on CIFAR and ImageNet show that using the approach with 8and even 4-bit fixed-point approximation of 32-bit floating-point activations has only a minor effect on training and validation performance, while affording significant savings in memory usage."
SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a novel approach for face completion using GANs. The proposed approach is based on an end-to-end framework that trains a GAN network progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. The network is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse to fine way. The system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x 1024 resolution. "
SP:a300122021e93d695af85e158f2b402d21525bc8,This paper studies the effect of reducing the precision of partial sum accumulators in deep learning networks on the quality of training. The main contribution of the paper is to derive a set of equations that relate the variance in the variance of the partial sum to the length of the accumulator and the minimum number of bits needed for accumulation. The authors show that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums. They also show that reducing accumulation precision further degrades the training quality of the trained network. 
SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic weight matrix alignment of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. The authors show that the risk converges to 0, the normalized ith weight matrix is aligned across layers, and the rank-1 approximation of the weight matrices are aligned across the layers. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network — the product of its weights matrices — converges in the same direction as the maximum margin solution. "
SP:868dd531fe7886b0260295d25b75cc6d6d28f12d," of phredGAN is an extension of hredGAN to the multi-turn dialogue scenario. PhredGAN captures utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system has a persona-based HRED generator (PHRED) and a conditional discriminator (PhredGANa) to capture the attribute representation. Experiments are conducted on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends."
SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a goal-driven collaborative image-drawing game between two agents, called CoDraw. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas. The two players communicate via two-way communication using natural language. The authors collect the CoDraw dataset of 10K dialogs consisting of 138K messages exchanged between human agents. They define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition which pairs agents trained independently on disjoint subsets of the training data for evaluation."
SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learning neural random fields (NRFs) for continuous data (e.g. images). The proposed approach is based on the idea of using an auxiliary generator (inclusive-divergence-minimized auxiliary generator) to learn the NRF. The authors show that the proposed approach can be used in both unsupervised/supervised image generation and semi-supervised classification tasks. The proposed method is evaluated on MNIST, SVHN and CIFAR-10. "
SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper studies training time attacks on graph neural networks for node classification that perturb the discrete graph structure. The main idea is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. The experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks and even transfer to unsupervised embeddings."
SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model called Cramer-wold auto-encoder (CWAE) based on the CramerWold kernel. The proposed method is inspired by Sliced-Wasserstein Autoencoders (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). CWAE cost function is based on a characteristic kernel, which has a simple closed-form in the case of normal prior. As a result, CWAE performance matches quantitatively and qualitatively that of SWAE."
SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper studies the many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios. In this paper, the authors propose a memory-augmented hierarchical-classification network (MahiNet) for MCFS learning. It addresses the “many-class” problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain. MahiNet uses a convolutional neural network (CNN) to extract features, and integrates an attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes."
SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper presents Structural-Jump-LSTM, a neural reading model that can skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one able to exploit punctuation structure (sub-sentence separators, sentence end symbols, or end of text markers) to jump ahead after reading a word. A comprehensive experimental evaluation of the model against all five state-of-the-art neural reading models shows that the proposed model achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or improving it compared to a vanilla LSTMs that reads the whole text."
SP:9be782b532e64c6aad140531a17fbba1dd3342cd," adversarial perturbations. This paper proposes a non-linear radial basis convolutional feature transformation by learning the Mahalanobis distance function that maps the input convolutionsal features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, which prevents small adversarial attacks from forcing a sample to cross the decision boundary. The proposed method is tested on MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray."
SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a novel policy exploration strategy for deep reinforcement learning agents. The proposed method is based on dropout, which is a global random variable for conditional distribution. The authors propose to use dropout to improve the policy’s temporal consistency in the presence of sparse reward signals. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee the stable improvement of the policy. Experiments show that the proposed method outperforms naive exploration and parameter noise. "
SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes a new metric, the nonlinearity coefficient (NLC), to predict the performance of a neural network after training. The NLC is computed in the network’s randomly-initialized state and is a powerful predictor of the test error. The authors show that the NLC can be used as a predictor of test error and that attaining a right-sized NLC for a given network is essential for attaining an optimal test error, at least in fully-connected feedforward networks. The proposed metric is also conceptually simple, cheap to compute, and is robust to a range of confounders and architectural design choices."
SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in classifiers, where a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. The authors demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification – a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, the authors show that while some bias cannot be mitigated without sacrificing accuracy, the bias amplification is mitigated through targeted feature selection. They present two new feature selection algorithms for mitigating bias amplification and show how they can be adapted to convolutional neural networks efficiently. "
SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the effect of over-parametrization on generalization in neural networks. The authors show that for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees. They also show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time."
SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes an approach to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. The experiments on the Multi-MNIST, CLEVR, and more complex MSCOCO data set show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations."
SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a method to learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. The method is evaluated on a suite of robotics tasks, including a manipulation task on a real Sawyer robotic arm directly from camera images."
SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"This paper proposes a method for learning policies in POMDPs from off-policy experience. The proposed method is based on structural causal models for counterfactual evaluation of arbitrary policies. It leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. Experiments are conducted on a grid-world task to demonstrate the effectiveness of the proposed method."
SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,This paper studies the relationship between loss surface in parameter space and adversarial robustness. It shows that the geometry property of the decision surface in input space correlates well with the robustness of the network against adversarial attacks. The authors propose a robust training method to evaluate the network’s intrinsic robustness property without testing its accuracy under adversarial attack.
SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. The authors prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, this framework trains DNNs that provide higher accuracies under same or lower energy budgets."
SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper studies the problem of learning a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. The authors formulate the problem as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. They propose a new policy network architecture that encodes the belief distribution independently from the observable state. They show that their method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable MDP solvers."
SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for building unsupervised representations of entities and their compositions by viewing each entity as a histogram (or distribution) over its contexts. This enables us to take advantage of optimal transport and construct representations that effectively harness the geometry of the underlying space containing the contexts. The method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. The key tools at the core of this framework are Wasserstein distances (Wasserstein barycenters), hence raising the question from the title. Empirical results show strong advantages gained through the proposed framework."
SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,This paper investigates the relationship between model-based and model-free reinforcement learning methods in MuJoCo environments. The authors propose to use a long-range dynamics model to predict the future state of the environment. They show that the long-term planning horizon of the dynamics model is more important than the short-term planner horizon. They also show that long planning horizons lead to better performance than the planner horizon in the long term. 
SP:da14205470819495a3aad69d64de4033749d4d3e,"This paper proposes a method to reduce the accumulated quantization error in neural network quantization, which is the key obstacle towards ultra-low precision, e.g., 2-bit or 3-bit precision. The proposed method, called precision highway, forms an end-to-end information flow while performing the ultralow-precision computation. In experiments, the proposed method outperforms the best existing quantization methods while offering 3 bit weight/activation quantization with no accuracy loss and 2 bit quantization."
SP:0355b54430b39b52df94014d78289dd6e1e81795," image restoration problem is formulated as a constrained optimization problem, where the objective is to maximize a posteriori probability of latent variables, and its constraint is that the image generated by these latent variables must be the same as the degraded image. The paper proposes a generative adversarial network (GAN) as a density estimation model to solve this problem. Experiments on MNIST dataset show the effectiveness of the proposed method."
SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a novel method for knowledge distillation from few samples. The main idea is to add a 1x1 conv-layer at the end of each block in the student-net and align the block-level outputs between ""teacher"" and ""student"" by estimating the parameters of the added layer with limited samples. Experiments verify the proposed method is very efficient and effective to distill knowledge from teacher-net to student network constructing in different ways on various datasets."
SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a new metric, H-score, to measure the transferability of representations from one task to another in classification problems. Inspired by the information theoretic approach of information theory, the authors propose to use the asymptotic error probability of the decision function based on the transferred feature as a measure of transferability. This metric can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments using both synthetic and real image data show that the proposed metric is meaningful in practice."
SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes a novel approach to control the global sentence structure in neural machine translation (NMT) by learning discrete structural representations to encode syntactic information of target sentences. During translation, the word generation is conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. By evaluating with a structural diversity metric, the sentences sampled using different codes have much higher diversity scores."
SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"This paper proposes a novel loss function for generative adversarial networks (GANs) that is based on the integral probability metric (IPM) loss function. The authors argue that the proposed loss function is similar to the identity function in GANs. The main difference is that the discriminator is trained to estimate the probability that the given real data is more realistic than a randomly sampled fake data. They show that this property can be induced by using a “relativistic discriminator” which estimates the probability of the real data being more realistic. They also propose a variant of the loss function, called “average GAN”, in which the discriminators estimate the likelihood that the real samples are more realistic on average than the fake samples. "
SP:8df1599919dcb3329553e75ffb19059f192542ea,This paper proposes Parameter Generation and Model Adaptation (PGMA) to deal with the catastrophic forgetting problem. The proposed approach learns to build a model with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach.
SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,This paper proposes Relational Forward Models (RFM) for multi-agent learning. RFM is based on the idea of learning to predict the future behavior of agents in an environment. The authors show that RFM can learn to make accurate predictions of agents’ future behavior in multi agent environments. They also show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines.
SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL) that learns a prior that is optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. The method is motivated by the observation that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a “prior” that is specifically optimized for learning expressive reward function from limited number of demonstrations, and the paper shows that the proposed method can efficiently recover rewards from images for novel tasks and provides intuition as to how our approach is analogous to learning a prior."
SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a new meta-learning framework for few-shot learning. The main contribution of the paper is the introduction of a new amortization network, called VERSA, which replaces optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. The method is evaluated on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary number of shots, and for classification, arbitrary numbers of classes."
SP:44e0f63ffee15796ba6135463134084bb370627b,This paper proposes a novel approach to learn a linear-chain CRF model for image classification. The proposed approach is based on a novel surrogate likelihood that allows for a local likelihood approximation of the original CRF with integrated batch-normalization. Experiments on a large dataset show the effectiveness of the proposed approach. 
SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a GAN-based approach to generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. The authors demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts. "
SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper studies the problem of verifying the robustness of piecewise-linear neural networks to adversarial perturbations. The main contribution of the paper is to formulate verification of the network as a mixed integer program. The authors propose a novel presolve algorithm that makes full use of all information available to verify the network's properties. The proposed algorithm is able to verify properties on convolutional and residual networks with over 100,000 ReLUs — several orders of magnitude more than networks previously verified by any complete verifier. "
SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper proposes to learn a default policy by restricting the amount of information the default policy receives, forcing it to learn reusable behaviours that help the policy learn faster. The authors show that this approach can significantly speed up and improve learning in both discrete and continuous action domains and demonstrate that for certain tasks, the learning of the policy is significantly faster than learning a fixed default policy."
SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes FLOWQA, an approach for conversational machine comprehension. The proposed approach is based on FLOW, which is an alternating parallel processing mechanism that can incorporate intermediate representations generated during the process of answering previous question/answer pairs, such as the document context and the current question. Compared to previous approaches that concatenate previous questions/answers as input, FLOW integrates the latent semantics of the conversation history more deeply. The model shows superior performance on two recently proposed conversational challenges, CoQA and QuAC."
SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper proposes a generative model for generating source code by modeling the edits that software developers make to source code files. The authors propose to use attentional and pointer networks to learn to predict future edits. They train their models on a large-scale dataset consisting of millions of fine-grained edits from thousands of Python developers. They show that a new composition of attention and pointer network provides the best overall performance and scalability.
SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-classification method for multi-class classification that leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method optimizes a binary classifier for pairwise prediction and through this process learns a sub-class classifier as a submodule. The authors formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. They demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings and shows superior or comparable accuracy."
SP:c5c84ea1945b79b70521e0b73f762ad643175020,. This paper studies the problem of visual question answering in the context of psycholinguistic question answering. The main contribution of the paper is to study the performance of FiLM on the question answering task. The authors show that FiLM performs poorly on the most difficult scenes. They also show that the performance declines with more difficult scenes as predicted by Weber's law. 
SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic approach for link prediction in relational knowledge graphs. The authors propose to use variational inference to estimate a lower bound on the marginal likelihood of the data. The main benefit of the proposed approach is that it allows for efficient, gradient based optimization over hyperparameters, which would lead to divergences in a non-Bayesian approach. The proposed approach outperforms the state-of-the-art on several benchmarks."
SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,This paper proposes a new online learning algorithm for supervised dimension reduction. The proposed algorithm is based on the sliced inverse regression (SIR) algorithm. The key idea is to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in. The authors also refine the algorithm by using an overlapping technique and develop an incremental overlapping sliced inverse regressions (IOSIR). The proposed method is evaluated on simulated data and real data applications.
SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"This paper proposes a generative-discriminative model for learning multimodal representations. The proposed model is based on a two-step approach: (1) multimodality-specific generative factors are shared across all modalities, and (2) multi-modal discriminative features are unique for each modality and contain the information required for generating data. Experiments show that the proposed model can learn meaningful representations that achieve state-of-the-art or competitive performance on six multimodale datasets."
SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, the authors learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to generate a network that requires few data at deployment time to rapidly adapt to new speakers. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the Wavenet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker encoder with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-Speaker neural network to new speaker, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers, and obtaining state of the art results in terms of audio naturalness."
SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper studies the problem of robust estimation under Huber’s contamination model. The main contribution of the paper is to establish a connection between f-GANs and various depth functions through the lens of f-Learning. Similar to the derivation of fGANs, the authors show that these depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning. This connection opens the door of computing robust estimator using tools developed for training GANs. The authors show in both theory and experiments that some appropriate structures of discriminator networks with hidden layers in GAN lead to statistical optimal robust location estimators for both Gaussian and general elliptical distributions where first moment may not exist."
SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes a method to represent a scene as a symbolic program. The program consists of a set of objects, attributes, and their relations, which are represented by a hierarchical representation of the scene. The proposed method is based on the idea that the program can be represented as a sequence of symbolic programs. The authors show that the proposed method works well on synthetic data and transfers to real images with such compositional structure."
SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a timing-gated LSTM RNN model, called the Gaussian-LSTM, for reducing state updates. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. Because the time gate limits the updates of the neuron state, the number of computes needed for the network update is also reduced. By adding a computational budget term to the training loss, the network further reduces the network's computational budget by at least 10x. Finally, a temporal curriculum learning schedule is proposed to speed up the convergence of the network."
SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae," in this paper, the authors propose a plug-and-play method for out-of-distribution detection. The proposed method is based on a simple ensembling of first and second order deep feature statistics. The method outperforms the state of the art by a large margin in all standard benchmarking tasks while being much simpler to implement and execute."
SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper studies the problem of learning a one-hidden-layer neural network with sigmoid activations, where the goal is to recover the weight vectors of the neural network. The authors prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground-truth without requiring a fresh set of samples at each iteration. To the best of the knowledge, this is the first global convergence guarantee established for empirical risk minimization using cross-entropy via gradient descent for learning one hidden layer neural networks."
SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutionsal layers. Experiments show that FBS can respectively provide 5x and 2x savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss."
SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the training objective of GANs from the mixed Nash Equilibria perspective. In particular, the authors propose a novel algorithm for training a GAN based on the infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE. They then propose a principled procedure to reduce their novel prox methods to simple sampling routines, leading to practically efficient algorithms. They provide experimental evidence that their approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality."
SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method for parameter-efficient transfer and multitask learning with deep neural networks. The basic idea is to learn a model patch a small set of parameters that will specialize to each task, instead of finetuning the last layer or the entire network. The authors show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, they show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly."
SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method for multi-modal data. The authors propose to extend the normalization to more than a single mean and variance to detect modes of data on-the-fly, jointly normalizing samples that share common features. They demonstrate that their method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets."
SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper studies the lottery ticket hypothesis, which claims that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that reach test accuracy comparable to the original network in a similar number of iterations. The authors show that a standard pruning technique naturally uncovers subnetwork whose initializations make them capable of training effectively. Based on these results, the authors propose a novel method to find the winning tickets."
SP:08c662296c7cf346f027e462d29184275fd6a102,"This paper studies the problem of learning to explore in Atari games. In particular, the authors propose an attentional dynamics model (ADM) that learns to find controllable elements of the observations. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. Experiments on Atari games show that the proposed ADM achieves state-of-the-art results on a set of Atari games with sparse rewards."
SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. The authors apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples."
SP:230b3e008e687e03a8b914084b93fc81609051c0,This paper proposes a differentiable estimator for the ELBO of the variational auto-encoder (VAE) based on importance sampling. The ELBO estimator is based on the importance sampling of the latent variables. The authors show that the proposed estimator can be used to train VAEs with binary or categorically valued latent representations. The proposed method is evaluated on two datasets. 
SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes a method to train a feed-forward neural network to improve the robustness of the network against adversarial attacks. The proposed method is based on the Boltzmann machine. The authors show that the adversarial performance of the model is correlated with the generative ability of the underlying Boltzman machine. 
SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,This paper studies the effect of changes in the size and location of the visible region of a minimal image on human and DNN recognition accuracy. The authors show that a slight modification of the location and size of the region of the minimal image produces a sharp drop in human recognition accuracy in DNNs. They also show that this phenomenon is independent of previous works that have reported lack of invariance to minor modifications in object location in Dnns. The results thus reveal a new failure mode that also affects humans to a much lesser degree. 
SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper studies the problem of multi-agent reinforcement learning where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. To achieve optimal coordination among these agents, a super agent is trained to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. To train the manager, the paper proposes Mind-aware Multi-agent Management Reinforcement Learning (MRL), which consists of agent modeling and policy learning. The experimental results have validated the effectiveness of the approach in modeling worker agents’ minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation."
SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a unified RNN model for time series. The proposed model is based on a recurrent neural network (RNN) architecture. The main idea is to separate sequential features into two groups dependent on their frequency, which are sparse and dense features, and also incorporate time features at the sequential level that relate to the time between specified events in the sequence and are used to modify the cell’s memory state. Two types of static (whole sequence level) features, one related to time and one not, are combined with the encoder output. "
SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a simple neural network model for predicting whether a given propositional property has the given property. The proposed model is based on a feedforward neural network which is recursively built for the given formula in a top-down manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of this model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper proposes a method to sample mini-batches with a gradually increasing level of difficulty in order to improve the speed and accuracy of training neural networks. The method is based on curriculum learning, where a teacher network is trained on the Imagenet database. The authors propose a bootstrap approach to evaluate the difficulty of points using the same network without relying on a “teacher” network, thus increasing the applicability of the proposed method. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets."
SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to provide generalization guarantees for deterministic and uncompressed neural networks. The main contribution of the paper is to show that if the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, then these conditions generalize to the interaction between the matrices on test data, thereby implying a wide test loss minimum. The paper also provides a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weights matrices."
SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,This paper proposes a novel training method for the vector quantized autoencoder (VQ-VAE) based on the Expectation Maximization (EM) algorithm. The main idea is to use EM to train a discrete latent variable model with a sequence-level knowledge distillation to improve the performance of the model. Experiments on CIFAR-10 show the effectiveness of the proposed method. 
SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,". This paper proposes a multi-view learning framework for learning sentence representations in an unsupervised fashion. One framework uses a generative objective and the other a discriminative one. In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the others encodes it with a simple linear model. The authors show that, after learning, the vectors produced by their multi-View frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement."
SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes an online distributed optimization method called Anytime minibatch to mitigate the impact of stragglers. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper presents a convergence analysis and analyze the wall time performance of the proposed method."
SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to learning a reward function that is independent of the reward function. The reward function is based on a measure of the peripheral pulse of the autonomic nervous system. The authors propose to use this measure to train a neural network to predict the intrinsic reward function based on the physiological changes in the body. The proposed method is tested on a simulated driving environment and shows that it can increase the speed of learning and reduce the number of collisions.
SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper studies the problem of underfitting Neural Processes (NPs) by learning to map a context set of observed input-output pairs to a distribution over regression functions, conditioned on the context. The authors show that underfitting leads to inaccurate predictions at the inputs of the observed data they condition on. They address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. They show that this greatly improves the accuracy of predictions and results in noticeably faster training."
SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,"This paper studies the problem of credit assignment in meta-reinforcement learning and proposes a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-Policy search, the proposed algorithm endows efficient and stable meta-Learning. The proposed algorithm leads to superior performance in terms of sample-efficiency, wall-clock time, and asymptotic performance."
SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. The main contribution of the paper is to propose a message-passing neural network for solving SAT problems. The network is trained to predict the satisfiability of a given SAT problem. It is shown to generalize well to new distributions and generalizes well to novel distributions. "
SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method for learning a policy for autonomous driving that is robust enough to drive a real vehicle. The authors propose to augment the imitation loss with additional losses that penalize undesirable events and encourage progress. They show that the model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of their proposed changes. Finally, they demonstrate the model driving a car in the real world."
SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method for selecting a subset of training data to achieve faster training with no loss in model predictive performance. The idea is to first train a small proxy model to estimate the utility of individual training data points and then select the most informative ones for training the large target model. Extensive experiments show that the proposed method leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data, while maintaining the predictive performance of the model trained on the entire dataset."
SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper proposes a new method for planning in continuous control problems. The main contribution of the paper is the introduction of a new algorithm, Sequential Monte Carlo Planning (SMCP), which is based on the idea that planning is a probabilistic inference problem over future optimal trajectories. The authors show that SMCP can capture multimodal policies and can quickly learn continuous control tasks. "
SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper studies the robustness of adversarial training. The authors show that adversarial robustness is sensitive to the input data distribution, unlike clean accuracy. Even a semantics-preserving transformations on the input distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. The paper further shows that standardly trained models achieve comparable clean accuracies on MNIST and CIFAR-10, but adversarially trained models can achieve significantly different clean accuracy and robustness accuracies."
SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique for batch normalization. The authors propose to use a transformation of layer weights instead of layer outputs. The proposed method keeps the contribution of positive and negative weights to the layer output in equilibrium. They validate their method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet."
SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper proposes a novel algorithm for data denoising in over-parameterized neural networks. The main idea is to use the implicit regularization effect of stochastic gradient descent with large learning rates to identify mislabeled examples by the loss statistics and discard them on the fly. This leads to ON-THE-FLY-Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeling examples while introducing almost zero computational overhead. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world examples."
SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper studies the problem of training a latent variable generative model of text for a downstream NLP task. The authors propose a theoretical framework that links the pretraining and downstream tasks with an underlying latent variable generation model. They analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. They show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task. They also show that prompt tuning obtains downstream guarantees with weaker non-degree of degeneracy conditions, and that the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory."
SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization, i.e., how to learn transferable features among source and target domains. The main contribution of this paper is to define transferability as the difference and connection between total variation and Wasserstein distance. The authors then prove that transferability can be estimated with enough samples and give a new upper bound for the target error based on our transferability. Empirically, they evaluate the transferability of the feature embeddings learned by existing algorithms for domain generalization. They find that many algorithms are not quite learning transferable feature, although few could still survive. In light of this, they propose a new algorithm for learning transferability features and test it over various benchmark datasets, including RotatedMNIST, PACS, Office-Home and WILDS."
SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the problem of learning a reward function that can capture tasks that we would want an agent to perform. The authors propose three new abstract notions of tasks that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a subset of trajectories. They show that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. They then provide a polynomial-time algorithm that allows the agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists."
SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning. The authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Based on this observation, the authors recast the generalization problem in RL as solving the induced partially observed Markov decision process, which they call the epistemic POMD. They demonstrate the failure modes of algorithms that do not appropriately handle this partial observable problem and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, the proposed algorithm achieves significant gains in generalization over current methods on Procgen benchmark suite."
SP:10de45510320b7ddb7ffb18b33e67f7cad609418,This paper proposes a new framework for estimating higher-order derivatives of value functions in meta-reinforcement learning. The authors propose to use off-policy evaluation to estimate the Hessian matrix of the value function. The proposed method is based on the idea that the Hessians of value function can be estimated by the gradient of the policy gradient. The paper shows that the proposed method outperforms a number of prior methods in terms of the bias and variance trade-off of Hessian estimates. 
SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper studies the problem of distributed learning with a central server. The main contribution of the paper is to propose a new algorithm that performs bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. To obtain this improvement, the authors design MCM, an algorithm such that the downlink compression only impacts local models, while the global model is preserved. This analysis opens new doors, e.g. incorporating worker dependent randomized-models and partial participation."
SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of counterfactual invariance, i.e., the requirement that changing irrelevant parts of the input shouldn’t change model predictions. The authors propose a simple way to test for spurious correlations by perturbing irrelevant part of input data and seeing if model predictions change. They show that both the means and implications depend fundamentally on the true underlying causal structure of the data. They also provide practical schemes for learning (approximately) counterfactually invariant predictors. "
SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a novel method for training GANs in the low-data regime. The main idea is to use the generator to augment the real data distribution with generated images, which deceives the discriminator adaptively. The paper provides a theoretical analysis to examine the convergence and rationality of the new training strategy. Extensive experiments demonstrate the effectiveness of APA in improving the synthesis quality."
SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a new framework for causal inference between pairs of event variables in multivariate recurrent event streams by extending Rubin’s framework for the average treatment effect (ATE) and propensity scores to multivariate point processes. The authors theoretically justify their point process causal framework and show how to obtain unbiased estimates of the proposed measure. They conduct an experimental investigation using synthetic and real-world event datasets, where their proposed causal inference framework is shown to exhibit superior performance against a set of baseline pairwise causal association scores."
SP:5db39fbba518e24a22b99c8256491295048ec417," GNNs have shown good performance in many tasks. However, they have shown that residual connections in the message passing of GNN can amplify the vulnerability against abnormal node features. The authors propose a novel GNN with Adaptive residual, AirGNN1, to address this issue. The experiments show that the proposed method outperforms the baselines. "
SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper considers the problem of online sequential decision problems where an agent must balance exploration and exploitation. The authors derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. They provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys Bayesian regret for a problem with A actions after T rounds. They extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case, they show that Thompson sampling can produce policies outside of the optimism set and suffer linear regret in some instances. "
SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the convergence of the Finito algorithm for composite finite-sum minimization under without-replacement sampling orders. The authors propose a damped version of Finito, Prox-DFinito, and show its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. They also provide a practical method to discover the optimal cyclic ordering numerically. "
SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,This paper studies the sub-optimal sub-optimality of a policy learned using REPS using first-order optimization methods applied to the REPS objective. The main contribution of the paper is to provide guarantees and convergence rates for the sub optimality of the policy learned by REPS. The paper shows that the near-optimality of REPS with exact gradients and stochastic gradient-based solvers is equivalent to the near optimal policy. The authors also propose a method that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy.
SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method to evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. The authors propose to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, they also propose metrics to evaluate spatial smoothness of encoding 3D structure, and representation complexity of the DNN. The experiments expose representation problems with classic DNNs and explain the utility of the adversarial training."
SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes PreferenceNet, an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. In addition, the authors introduce a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that their proposed method is competitive with current state-of-the-art neural network based auction designs. "
SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of user-level differential privacy in supervised learning. In this setting, each of the users has a training data set drawn from their own distribution, and each of them has a set of problems that are linear regression problems. The goal of the paper is to learn the shared structure among the problems (i.e., to solve their tasks better than they could individually) while preserving the privacy of their data. The authors formulate this question using joint differential privacy, which is to control what is leaked about each user’s entire data set. They provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and show that their algorithms satisfy nearly optimal estimation error guarantees. They also establish a general information-theoretic upper bound via an exponential mechanism-based algorithm."
SP:3925fc528de17b8b2e93808f5440ea0503895b75,"This paper proposes a new benchmark to test the performance of state-of-the-art VQA models against human-adversarial examples. The proposed benchmark is based on an adversarial version of the Visual Question Answering (VQA) dataset, where a human is asked to find an image in the dataset where the model’s predicted answer is incorrect. The authors conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions."
SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of different types of neurons in the MEC, i.e., grid, border, and head-direction cells. The authors show that the majority of MEC neurons do not exhibit stereotypical firing patterns, and propose a computational approach to address these questions. They show that heterogeneous MEC cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. Next, they evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous cells. They found that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles — including those of grid cells themselves — despite not being explicitly trained for this purpose."
SP:57f9812fa5e7d0c66d412beb035301684d760746,". This paper studies the problem of pathological training dynamics of KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. Finally, the pathology can be remedied by non-parametric behavior reference policies and this allows KL-based RL to significantly outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks."
SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the learning curve of a teacher-student kernel regression problem in the case where the filter size of the teacher is smaller than that of the student. The authors show that in the ridgeless case, the teacher's filter size does not depend on the input dimension, but on the number of samples in the training set. They show that if the teacher has a smaller filter size than the student, then the student has a larger filter size. They also show that under a natural universality assumption, performing kernel regression with a ridge that decreases with the size of training set leads to similar learning curve exponents to those obtained in the ridgless case."
SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes an end-to-end training method for variational autoencoders (VAEs) based on a deterministic autoencoding (DAE) framework. The main contribution of the paper is to propose a new training procedure for VAEs based on multi-modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. Experiments show that the proposed method outperforms the state-of-the-art VAEs in continuous and discrete domains.
SP:6232d8738592c9728feddec4462e61903a17d131,"This paper proposes a method for self-supervised adversarial detection based on disentanglement of class and semantic features in the autoencoder. By disentangling the features, the proposed method is able to reconstruct benign and adversarial examples. The method is compared with the state-of-the-art in terms of AUC, FPR, and TPR on CIFAR-10 under 30 different attacks."
SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper proposes a method to model the brain's representation of the syntactic structure of a sentence. The authors propose a novel multi-dimensional embedding space to encode information about the structure of the sentence, and use it to predict the brain activity of different parts of the language system. They show that the proposed approach can explain additional variance in brain activity even after controlling for complexity metrics that capture processing load. They find that regions well-predicted by syntactic features are distributed in the language systems and are not distinguishable from those processing semantics."
SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes an energy-based model (EBM) to handle compositional generation over a set of attributes. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). Given a pre-trained generator, all we need for controllable generation is to train an attribute classifier. Sampling with ODEs is done efficiently in the latent space and is robust to hyperparameters. Experimental results show that the proposed method outperforms the state-of-the-art in both conditional sampling and sequential editing."
SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits model where individual clients face different K-armed stochastic bandits coupled through common global parameters. By leveraging the geometric structure of the linear rewards, a collaborative algorithm called Fed-PE is proposed to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near-optimum regret for both disjoint and shared parameter cases with logarithmic communication costs. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes COMBO, a new model-based offline RL algorithm that trains a value function using both the offline dataset and data generated using rollouts under the model while also additionally additionally regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the authors show that COMBO satisfies a policy improvement guarantee in the offline setting. Experiments are conducted on a variety of tasks that require generalization to related but previously unseen tasks, and COMBO consistently matches or outperforms prior offline RL methods."
SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the effect of local elasticity in the training dynamics of deep neural networks on the intra-class separability of features during training. The authors propose to model the evolution of features using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. The main finding is that if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, then the features of the training data become linearly separable, meaning vanishing training loss. Otherwise, the features are not separable regardless of how long the training time is."
SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,This paper proposes a two-stage learning scheme to learn a program embedding space for learning a policy that maximizes the return for a given task. The first step is to learn an embedding of the reward function and the second stage is to search the learned embedding to find a program that maximises the return. The authors show that the proposed method outperforms DRL and program synthesis baselines in terms of generalization performance.
SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the problem of physics-informed neural network (PINN) models for learning differential equations with convection, reaction, and diffusion operators. The authors show that PINN models can fail to learn relevant physical phenomena for even slightly more complex problems. They provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN’s setup makes the loss landscape very hard to optimize. They then describe two promising solutions to address these failure modes. "
SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, CST generates target pseudo-label with a source-trained classifier and then updates the shared representations to make the target classifier perform well on the source data. CST introduces a Tsallis entropy as a confidence-friendly regularization to improve the quality of target pseudo labels. The authors analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self training fail."
SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a novel method for structured pruning in deep neural networks. The proposed method is based on the idea of discriminative masking. The main idea is to first select some neurons to be refined during the training process, while gradually masking out other neurons. The paper shows that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. "
SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a new architecture for self-attention networks, called Neural Interpreters, which is based on the idea that the network is a system of modules, which are learned in an end-to-end manner. The proposed architecture is evaluated on image classification and visual abstract reasoning on Raven Progressive Matrices. In the former setting, the proposed architecture performs on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. On the latter setting, it is competitive with the state-of-the-art in terms of systematic generalization."
SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT) to transfer knowledge learned during unsupervised pre-training to a supervised setting. The authors argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. To this end, the authors propose a technique that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. The experiments show that, when combined with large-scale pre- training in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the policies is misaligned with the downstream task."
SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a new differentiable surrogate loss function for ranking. The authors propose a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. They show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divide-and-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, they demonstrate the role of larger list sizes during training and show the performance of PiRank significantly improves over comparable approaches on publicly available internet-scale learning-to-rank benchmarks."
SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for the problem of estimating the ground-state energy of lithium hydride (LiH) using a variational quantum entanglement (VQE). The proposed algorithm is based on a curriculum learning approach, where the learning objective is to find a circuit that maximizes the expressivity of the circuit while maintaining the circuit depth. The algorithm is evaluated on LiH in various configurations and shows state-of-the-art performance. "
SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the effect of arbitrary class distributions within the query sets of few-shot tasks at inference, removing the class-balance artefact. Specifically, the authors propose to model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirically, they show that the proposed method outperforms state-of-the-art methods across several data sets, models and few shot settings."
SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a patch-by-patch inference scheduling method to reduce the memory consumption of neural networks. The proposed method is based on patch-based inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory usage. The paper also proposes receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. "
SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of Bayesian automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent and takes actions based on the strategic agent's report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal’s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. For environments with large time horizons, the authors show that the optimal utility is hard to approximate within a certain constant factor, complementing their algorithmic result."
SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29, GASSO is an architecture search method for graph neural networks (GNNs). The main contribution of the paper is a theoretical analysis of the performance of gradient-based NAS methods. The authors show that gradient based NAS methods tend to select proper architectures based on the usefulness of different types of information with respect to the target task. The experiments show that the proposed method is able to achieve the state-of-the-art performance compared with existing baselines.
SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of group fairness in the context of unsupervised clustering, where a dataset is partitioned into clusters that consist of nearby points in a metric space. The authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. They derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. They also derive impossibility results for other natural fairness objectives. "
SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge-independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that under a bounded overlap condition, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
SP:0d77c22df0830cb675b11ad883d014e3a1933c8e," of ReLU′(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. This paper investigates the importance of the value of the ReLU(0), for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU($0) = 0 seems to be the most efficient."
SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes PC, a method for learning simple policies in reinforcement learning. PC combines ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. PC jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. PC achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck. PC is shown to generalize well to new tasks."
SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"This paper proposes a new transformer architecture for graph representation learning. The proposed architecture uses a learned positional encoding (LPE) to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, the model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Experiments on 4 standard datasets show that the proposed model performs on par or better than state-of-the-art GNNs."
SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper studies the problem of two-alternative elections where the voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. The authors propose an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium where no coalition of voters can deviate and receive a better outcome."
SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,This paper studies the Hessian of deep linear networks. The authors provide a tight upper bound on the rank deficiency of the rank of a deep linear network. They show that this upper bound can be interpreted in terms of rank deficiency. They also show that the upper bound remains faithful as an estimate of the numerical Hessian rank for a larger class of models such as rectified and hyperbolic tangent networks. 
SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new metric for quantifying linear-symmetry-based disentanglement (LSBD). The proposed metric, called DLSBD, is based on the notion of linearly disentangled representations. The authors also propose LSBD-VAE, a semi-supervised method to learn LSBD representations. Experiments are conducted to demonstrate the effectiveness of the proposed metric. "
SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes an extension of the Kalman VAE framework to deep state-space models (DSSMs). The proposed method combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The proposed approach outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can also successfully learn state space representations where static and dynamic features are disentangled."
SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,This paper proposes a method to generate counterfactual explanations for a given query image by using deep inversion to generate images from the training distribution. The proposed method is based on the idea of using a manifold consistency objective and a progressive optimization strategy. The authors show that the proposed method outperforms existing methods in terms of interpretability and robustness to corruptions.
SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper studies the problem of finding a region where the decision-maker assignment has a large causal effect on the decision. The problem is formulated as a causal inference problem, where the goal is to find a region of high inter-decision-maker disagreement. The authors propose an algorithm to find such a region by maximizing an empirical objective, and they give a generalization bound for their algorithm. They show that their algorithm recovers the correct region of heterogeneity accurately compared to baselines. They also apply their algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,This paper proposes a token-based approach for image synthesis. The key idea is to learn a sequence of latent tokens to predict the visual tokens for synthesizing an image. The proposed approach is evaluated on FFHQ and LSUN Church datasets and shows state-of-the-art results. 
SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the effect of ridge regularization on the robust risk of linear regression and classification. The authors show that avoiding interpolation through ridge regularisation can significantly improve generalization in the absence of noise. They prove this phenomenon for both linear regressions and classification, and hence provide the first theoretical result on robust overfitting."
SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a method for learning dense correspondences between 3D shapes of the same category. The proposed method is based on a canonical point autoencoder (CPAE) that encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. The primitive plays a key role to map all the unordered point clouds on the canonical surface and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the proposed method performs favorably against state-of-the-art correspondence learning methods."
SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper proposes Stochastic weight averaging Densely (SWAD) to find flat minima for empirical risk minimization (ERM) in domain generalization (DG) problems. The authors show that SWAD finds flatter minima and suffers less from overfitting than the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6% averagely on out of domain accuracy."
SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper presents a large-scale study of performance predictors for NAS. The main contribution of the paper is the analysis of 31 different NAS predictors, which are based on learning curve extrapolation, weight-sharing, supervised learning, and zero-cost proxies. The authors show that each of these predictors can be combined to achieve even better predictive power. "
SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,This paper studies the privacy guarantee of Dirichlet posterior sampling in the context of truncated concentrated differential privacy (tCDP). The main contribution of this paper is to derive a simple privacy guarantee for the Dirichlets posterior sampling. The privacy guarantee is based on the notion of tCDP. The paper also studies the accuracy guarantees of the posterior sampling for Multinomial and private normalized histogram publishing. 
SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm to compute random walks efficiently and locally at the same time. The main contribution of the paper is to propose an algorithm that is both memory and round efficient, and yields an efficient parallel local clustering algorithm. The paper also provides theoretical analysis and experimental results to support the proposed algorithm."
SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper theoretically analyzes the typical learning performance of `1-regularized linear regression (`1-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity is obtained. Remarkably, despite the model misspecification, it is model selection consistent with the same order of sample complexity as that of the logistic regression. Moreover, the authors provide an efficient method to accurately predict the nonasymptotic behavior of the model for moderate M,N for moderate precision and recall."
SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means problem, where the learner is allowed to interact with an oracle (domain expert) and asks for the similarity between a set of chosen items. The authors prove that having a few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomials time-complexity, where n is the number of items."
SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a new approach to augmenting model-based reinforcement learning by encouraging a learned model and value function to be jointly self-consistent. The proposed approach is based on the idea that the value function should be consistent with the learned model. The authors propose multiple self-Consistency updates, evaluate these in both tabular and function approximation settings, and find that, with appropriate choices, self-conformity helps both policy evaluation and control."
SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,This paper proposes a method to sample episodes in episodic few-shot learning based on the difficulty of the episodes. The authors propose to sample the episodes uniformly over the episode difficulty. The proposed method is algorithm-agnostic and can be used to improve the performance of many episodic training algorithms. Experiments show that the proposed method outperforms other sampling schemes. 
SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of logistic bandits in which the learner aims to maximize the profit over a user that can select one of two possible outcomes (e.g., ‘click’ vs. ‘no click’, ‘show me later’ or ‘never show again’). The main contribution of this paper is the use of multinomial logit (MNL) to model the probability of each one of K+1 2 possible outcomes (+1 stands for the “not click” outcome). For this problem, the authors propose MNL-UCB, an upper confidence bound (UCB)-based algorithm that achieves regret $O(dK p T)$ with small dependency on problem dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. "
SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The proposed algorithm aims to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework based on disentanglement of exploration and exploitation. Numerical results show the proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of learning continuous-time Markov chains in the context of time evolution of discrete sets of items (e.g., genetic mutations). The authors show that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors propose an approximate likelihood maximization method that can scale to hundreds of items and is orders of magnitude faster than previous methods. They demonstrate the effectiveness of their approach on synthetic and real cancer data."
SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes UDoc, a unified pretraining framework for document understanding. UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. It learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks."
SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of clustering data with lp-norm objectives in the context of individual fairness. The authors propose a new algorithm based on linear programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. "
SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper studies the problem of graph partitioning. The authors propose a polynomial-time Gaussian sampling-based algorithm for MAX-K-CUT and the MAX-AGREE variant of correlation clustering. The main contribution of the paper is the proposed algorithm that uses O(n + |E|) memory and nearly achieves the best existing approximation guarantees. For dense graphs arriving in a stream, the authors eliminate the dependence on the storage complexity at the cost of a slightly worse approximation ratio."
SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a motion-aware unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. Based on the learned attention map, the historical temporal states are aggregated to an augmented motion information (AMI). In this way, the predictive unit can perceive more temporal dynamics from a wider receptive field. The fusion module is utilized to further aggregate the augmented motion Information (AMI) and current appearance information (current spatial state) to the final predicted frame. Experimental results show that the MAU outperforms the state-of-the-art methods on both video prediction and early action recognition tasks."
SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,This paper studies the problem of nonlinear function approximation with two-layer neural networks. The main contribution of the paper is to provide a computationally and statistically efficient algorithm in the generative model setting under completeness of the neural network. The authors show that the sample complexity scales linearly in the algebraic dimension. They also provide a second result in the deterministic setting.
SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization performance of deep metric learning (DML) under out-of-distribution test distribution shifts. The authors propose to construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under out of distribution shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of state-of the-art DML methods. They find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. "
SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier model selection in the context of outlier detection. The main idea is to use meta-training to automatically select a good model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity, the authors introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection (e.g. always using the same popular model or the ensemble of many) as well as other meta learning techniques that are tailored for UOMS."
SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. The proposed framework is based on the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. Experiments are conducted on synthetic linear programming, portfolio optimization, and resource provisioning."
SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNN, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these run to obtain the final result. Theoretical bounds are derived for the number of runs required to ensure a reliable distribution of dropouts. Experimental results validate the theoretical findings on expressiveness."
SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. The proposed DS-Net can simultaneously calculate fine-grained and integrated features and efficiently fuse them. The authors propose an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, they also design a Dual-Stream FPN to further enhance contextual information for downstream dense predictions."
SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a framework for learning visual concepts and physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual representation module parses each video frame into object-centric trajectories and represents them as latent scene representations based on the language. A differentiable rigid-body simulator is used to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. These learned concepts can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios."
SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions for active learning. SimILAR is shown to outperform existing active learning algorithms by as much as 5%-18% in the case of rare classes and 5%+10% in case of out-of-distribution data on several image classification tasks."
SP:c141dc29b487ebfaa20ee50786886b0383d938bc,This paper proposes two new identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test for the asymetrical setting. The second one is derived from an optimal learning algorithm for the Mallows models and is sample-optimal for a wide range of parameters. The proposed tests scale gracefully with the number of items to be ranked.
SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for generalizing neural radiance fields (NeRF) to capture a video of a human performance. The proposed method is based on a parametric human body model for robust performance capture. Specifically, a temporal transformer aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes to use neural architecture search to automate the manually designing more effective vision transformers by searching not only the architecture but also the search space. The main idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, the authors provide design guidelines of general vision transformer with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. The searched models from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet."
SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this setting, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. First, the authors provide an algorithm that efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. If all the bags are non-monochromatic (i.e., bags of size two with differently labeled feature-vesctors) the algorithm satisfies (1/2)-fractions of them. For the special case of OR over the d-dimensional boolean vectors, the algorithm achieves an additional $\�(1/d) in accuracy for the two cases."
SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a method to use singular value decomposition and noise modeling to create surrogate benchmarks that output the full training information for each architecture, rather than just the final validation accuracy. The authors also propose a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-Fidelity algorithms which claimed to be state-of-the-art."
SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a method for providing post-hoc explanations of latent representations in machine learning models. The method is based on the idea of using a corpus of examples to explain the latent representations of the model to the user. The authors propose to reconstruct the test latent representation as a mixture of corpus latent representations and propose a novel approach, Integrated Jacobian, to make explicit the contribution of each corpus feature in the mixture. Experiments on mortality prediction and image classification tasks demonstrate the effectiveness of the proposed method."
SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based approach for vision-language pre-training (VLP) to learn multi-modal representations from image-text pairs. The authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., inter-modality). They also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the intermodality learning in VLP. The proposed method is evaluated on a range of visual-language tasks. "
SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the problem of the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is private. The authors study this problem for noisy gradient descent algorithms, and model the dynamics of Rényi differential privacy loss throughout the training process. They prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy losses by upper-bounding its total value over all intermediate gradient computations). "
SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,This paper proposes a method to learn a policy to tune the hyperparameters of a quadratic optimization problem using reinforcement learning. The proposed method is based on the idea of learning a policy that learns a hyperparameter tuning strategy to speed up the convergence of the QP solver. The authors show that the proposed method outperforms the state-of-the-art solvers by up to 3x. They also show that their method generalizes well to new problems.
SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the asymptotic convergence of deep linear networks in terms of the Principal Components Bias (PC-bias). The authors show that the convergence rate of this model is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They show how the PC-Bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also show that both biases can be seen independently, and affect the learning in different ways. Finally, the authors discuss the connection between the PC bias and the effect of early stopping and its connection to PCA."
SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes an approach to prevent backdoor attacks from being injected into the training of deep neural networks (DNNs). The main idea is to train clean models given backdoor-poisoned data. The authors propose a two-stage gradient ascent mechanism for standard training to help isolate backdoor examples at an early training stage, and to break the correlation between backdoor examples and the target class at a later training stage. Experiments on multiple benchmark datasets against 10 state-of-the-art attacks are conducted to demonstrate the effectiveness of the approach."
SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a generative implicit model for 3D-aware image synthesis. The main contribution of this paper is to propose a novel shading-guided generative model that is able to learn a starkly improved shape representation. The key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions, and a discriminator is derived by feeding the synthesized images to the discriminator. To compensate for the additional computational burden of calculating surface normals, the authors devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48% respectively. The experiments on multiple datasets show that the proposed approach achieves photorealistic 3D image synthesis while capturing accurate underlying 3D shapes."
SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a quasi-Bayesian approach for instrumental variable (IV) regression. The proposed approach is based on the recently developed kernelized IV models. The authors propose a scalable approximate inference algorithm with time cost comparable to the corresponding point estimation methods. They analyze the theoretical properties of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of our method."
SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,This paper proposes a method for cross-lingual open QA. The authors propose a dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose a multilingual autoregressive generation model that answers directly in the target language without any translation or in-language retrieval modules as used in prior work. They propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. The results show that CORA substantially outperforms the previous state-of-the-art on 26 languages across 26 languages.
SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the generalization performance of Empirical Risk Minimization (ERM) models trained on three popular domain generalization datasets. The authors find that the domain adaptation theory of Ben-David et al. (2007) does not provide a tight explanation of the out-of-domain generalization observed across a large number of ERM models. This motivates them to investigate other possible measures that could explain generalization in this setting. They find that measures relating to the Fisher information, predictive entropy, and maximum mean discrepancy are good predictors of generalization."
SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper studies backdoor data poisoning attacks for classification problems. In this paper, the authors propose a theoretical framework within which one can discuss backdoor attacks and analyze important statistical and computational issues surrounding these attacks. They identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows them to argue about the robustness of several natural learning problems to backdoor attacks. The authors show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. "
SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of the width of deep neural networks on their generalization to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsumes neural nets. In particular, the authors show that Deep GP converges to Gaussian processes, effectively becoming shallower without any increase in representational power. The posterior, which corresponds to a mixture of data-adaptable basis functions, becomes less datadependent with width. The authors also show that there is a “sweet spot” that maximizes test performance before the limiting GP behavior prevents adaptability. "
SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the problem of federated learning (FL) where a group of clients periodically coordinate with a central server to train a statistical model. The main contribution of this paper is to provide a general algorithmic framework called FedLin to tackle some of the key challenges intrinsic to FL, namely objective heterogeneity, systems heterogeneity, and infrequent and imprecise communication. The authors show that FedLin guarantees linear convergence to the global minimum when the clients’ local loss functions are smooth and strongly convex. They also establish matching upper and lower bounds on the convergence rate of FedLin that highlight the effects of infrequent, periodic communication. Finally, FedLin preserves linear convergence rates under aggressive gradient sparsification, and quantify the effect of the compression level on convergence rate."
SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the problem of estimating the Sliced-Wasserstein distance (SW) using the concentration of measure phenomenon. The authors show that under mild assumptions, one-dimensional projections of a high-dimensional random vector are approximately Gaussian. Based on this observation, they develop a simple deterministic approximation for SW that is both accurate and easy to use compared to the usual Monte Carlo approximation. They derive nonasymptotical guarantees for their approach, and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. "
SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper investigates the relationship between the representations learned by neural language models, translation models, and language tagging tasks. The authors propose to use an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embedding. The paper also finds that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI."
SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes a method for few-shot conditional image generation using a diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, the proposed method achieves superior performance over state-of-the-art VAEs and diffusion models."
SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies the problem of self-supervised learning by pushing positive pairs or similar examples from the same class closer together while keeping negative pairs far apart. The authors propose a novel concept of the augmentation graph on data. They propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss."
SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. It follows up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. They proceed by showing that this result can be strengthened to a localized version of the feedback edge sets, and provide corresponding lower bounds that complement previous results to provide a complexity classification of nearly all well-studied graph parameters. Finally, the authors show how their results can be extended to the closely related problem of Polytree Learning."
SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification in the streaming setting. The algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a set of labeled and weak-labeled points. The theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate the algorithm outperforms standard baselines."
SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov growth (KG) to measure the generalization ability of a classifier. The authors propose a new generalization error bound that only depends on the final choice of the classification function. Based on this bound, the authors propose to regularize the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalisation ability of classifiers."
SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,. This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the idea that the variance of each embedding dimension should be kept above a threshold. The authors show that the proposed term improves the performance of existing methods on several downstream tasks.
SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes Information Directed Reward Learning (IDRL) to learn a model of the reward that allows standard RL algorithms to achieve high expected returns with as few expert queries as possible. To this end, the proposed method uses a Bayesian model of reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. The proposed method achieves similar or better performance with significantly fewer queries than existing active reward learning methods."
SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,This paper proposes a method to predict the parameters of a neural network in a fraction of a second. The method is based on graph neural networks (GNNs). The authors propose a hypernetwork that can predict performant parameters in a single forward pass. The proposed model achieves surprisingly good performance on unseen and diverse networks. 
SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the distortion-perception tradeoff between the mean squared error (MSE) distortion and the Wasserstein-2 perception index. The authors prove that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, they provide a closed form expression for such estimators. For general distributions, they show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer and a minimizer of the MSE under a perfect perceptual quality constraint."
SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a, of GraphFormers. This paper proposes a novel architecture for representation learning on textual graphs. The main idea is to use GNNs to learn the embeddings for the nodes based on the individual textual features and the neighbourhood information. The proposed architecture is evaluated on three large-scale benchmark datasets. 
SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy (DP) in the setting of mean estimation, stochastic convex optimization, empirical risk minimization, and learning hypothesis classes with finite metric entropy. It shows that for high-dimensional mean estimation with smooth losses, the privacy cost decreases as O(1/\sqrt{\sqrt{m}$) as users provide more samples. In contrast, when increasing the number of users n, it decreases at a faster rate. The paper also provides lower bounds showing the minimax optimality of the proposed algorithms for mean estimation and stochastically convex optimisation."
SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper studies the effect of feature learning in deep neural networks trained with noisy gradient descent on a large training set and derive a self-consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. The main contribution of the paper is the development of a new framework for studying the effects of deep learning in finite DNNs on feature learning. The authors show that the transition between a feature learning regime and a lazy learning regime in a two-layer linear convolutional neural network (CNN) is sharp. They also show that this transition is also sharp in a non-linear two layer fully connected network. 
SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the question of whether communication is compositional, i.e., if complex signals can be represented as a combination of simpler subparts. The authors show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, they prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence."
SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation 5 and optionally distillation, it attains surprisingly good accuracy/complexity trade-off offs on ImageNet. The authors also train the models in a self-supervised setup to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results."
SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper studies the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, this paper focuses on minimizing the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e., the label of a point is given by which of k centers it is closest to in Euclidean distance – the authors show that one can achieve a loss that is independent of the total number of queries. They also show that learning general convex sets requires an almost linear loss per query. "
SP:5c0114535065d5125349f00bafdbccc911461ede," QA methods are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. This paper proposes a method for knowledge transfer based on a regularization term in the loss function, supervising the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. They also demonstrate the effectiveness of this approach experimentally on the GQA dataset."
SP:40fd96105e77063de4a07d4b36fe19385434c533,This paper proposes a new memory module for recurrent neural networks (RNNs) that can be used to simulate a Universal Turing Machine. The memory module is composed of fixed-precision neurons of fixed precision and a dynamically growing memory module. The authors prove that a 54-neuron RNN with growing memory modules can simulate a universal Turing machine with time complexity linear in the simulated machine’s time and independent of the memory size. The result is extendable to various other stack-augmented RNNs. 
SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper studies the under-cover bias of quantile regression in the setting where there is more data than the number of parameters. The main contribution of the paper is a theoretical analysis of the coverage of uncertainty estimation algorithms in learning quantiles. In particular, the authors prove that under-coverage bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regressions. Experiments on simulated and real-world data verify the theory. "
SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,This paper proposes a reinforcement learning-based reinforcement learning approach for memory allocation in class-incremental learning (CIL). The main idea is to use reinforcement learning to learn a policy function to allocate memory for each incremental phase. The policy function is trained on pseudo-CIL tasks and then applied to target tasks. Experiments show that the proposed approach outperforms the baselines on three benchmarks. 
SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper studies the problem of speeding up SGD by parallelizing it across multiple workers. In this paper, the authors propose a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. They show that this can achieve an error that scales as 1/(NT) with a number of communications that is completely independent of T. In particular, they show that $\�(N) communications are sufficient. Empirical evidence suggests that this bound is close to tight."
SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,This paper studies Online Lazy Gradient Descent for optimisation on strongly convex domains. The algorithm is known to achieve O(\sqrt{N}log N) regret against adversarial opponents. This paper shows that it is universal in the sense that it also achieves O(logN) expected regret against i.i.d opponents. It improves upon the more complex metaalgorithm of Huang et al [20] that only gets O( \sqrt{\log N log N) and $O(log N^2)$ expected regret. 
SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the Bures-Wasserstein (BW) metric for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors show that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the Affine-Invariant (AI) metric. Further, the BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Extensive experiments on various applications support the findings."
SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. The main contribution of the paper is the introduction of the Dynascore, a utility-based aggregation of metrics for evaluating the quality of the model. The paper also proposes a new metric for evaluating model robustness, memory use, throughput, and robustness."
SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes Neural Dubber, a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. An image-based speaker embedding (ISE) module is developed for the multi-speaker setting to generate speech with a reasonable timbre according to the speaker’s face. Experiments on the chemistry lecture dataset and LRS2 dataset show that the proposed model can generate speech audios on par with state-of-the-art TTS models in terms of speech quality."
SP:24ea12428bd675459f0509aa7cee821fa236382e, for collaborative learning in medical imaging. The authors propose to use the Vision Transformer architecture for split learning. They show that the proposed framework achieves comparable performance to data-centralized training. They also show that heterogeneous multi-task clients also improves individual task performance. 
SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable point-to-mesh point cloud representation based on Poisson Surface Reconstruction (PSR) that allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end to end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points model is more interpretable, lightweight, and accelerates inference time by one order of magnitude."
SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper studies the problem of learning representations of corrupted images using a pre-trained representation learning network that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x) if we are only given a corrupted version A(x), for some known forward operator A. The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. The proposed method achieves a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking."
SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of credit assignment in neural networks. The authors propose to use a finite-horizon reinforcement learning approach to solve the problem. They show that the standard REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. They introduce an off-policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. They also show that these networks of agents can be more robust to correlated samples when learning online."
SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,. This paper proposes a self-supervised predictive learning approach to model both the dorsal and ventral pathways of the mouse visual cortex. The authors show that the proposed approach is able to capture the properties of both the ventral and dorsal pathways using a single loss function. They also show that their approach can outperform other models in fitting mouse visual cortices.
SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model that can inject prior structural knowledge as an inductive bias to influence the learning of the topic hierarchy. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,This paper proposes a novel pretraining method for object detection based on image-level contrastive representation learning. The proposed method is based on the idea of selective search bounding boxes as object proposals and the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN). The pretraining is equipped with object detection properties such as object-level translation invariance and scale invariance. The method achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework.
SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,This paper proposes a learning-augmented local search framework to solve large-scale vehicle routing problems (VRPs). The method iteratively improves the solution by identifying appropriate subproblems and delegating their improvement to a black box subsolver. The proposed method accelerates state-of-the-art VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000.
SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The authors propose to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. The method is based on the Bayesian continual learning framework and proposes to dynamically expand parameters to learn each new task and then selectively combines them, which is formally consistent with the underlying mechanism of biological active forgetting in biological neural networks. Experiments on CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks demonstrate the effectiveness of the method."
SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for the prior guided random gradient-free (PRGF) algorithms. Moreover, to further accelerate over greedy descent methods, the authors propose an accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the generalization of the lottery ticket hypothesis (LTH) by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. It shows that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, they theoretically show that the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer. Theoretical results are acquired from learning a pruning neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks."
SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper studies the problem of private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. The authors propose two new methods, GEM and PEP, to solve this problem. The first method is GEM, which is a generative network with exponential mechanism (GEM) that optimizes over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method is private entropy projection (PEP), which is an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy."
SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a simple mask classification method for semantic and instance-level segmentation tasks. The proposed method is based on the idea that mask classification is sufficiently general to solve both semantic and instances-level tasks in a unified manner using the exact same model, loss, and training procedure. The paper shows that the proposed method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models."
SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper extends the work of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also sub-exponential in the dimension). The authors prove that a single step of gradient descent suffices. They also show this result for any subexponential width random neural network with smooth activation function."
SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that uses a variational autoencoder (VAE) framework to train a latent score model. The authors propose a score matching objective for LSGM and propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative models on this dataset. The proposed LSGM is on par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude."
SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and neural tangent kernels in image classification. The authors prove that for a simple data distribution with sparse signal amidst high-variance noise, a simple convolutional neural network trained using stochastic gradient descent simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding neural network with a fixed set of predetermined features is unable to adapt to the signal in this manner. They supplement their theoretical results with experiments on CIFAR-10 and MNIST images with various backgrounds."
SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the problem of decentralized machine learning over a network where the training data is distributed across n agents, each of which can compute stochastic model updates on their local data. The authors provide a tighter analysis of the gradient tracking (GT) algorithm in the strongly convex, convex and non-convex settings. They improve the dependency on p from O(p-2/2) to $O(p+1/2c-1)$ in the noiseless case and $O(\sqrt{p-3/2})$ in general stochastically. This improvement was possible due to a new proof technique. "
SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm for the multi-armed bandit problem. The main contribution of the paper is the analysis of the arm-sampling behavior of UCB. The paper shows that UCB is asymptotically deterministic, regardless of the problem complexity. It also provides the first complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling. "
SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper studies the problem of cross-domain cold-start recommendation (CDCSR). The authors propose DisAlign, a framework that utilizes both rating and auxiliary representations from the source domain to improve the recommendation performance of the target domain. Specifically, they propose Stein path alignment for aligning the latent embedding distributions across domains and then further propose its improved version, i.e., proxy Stein path, which can reduce the operation consumption and improve efficiency. The empirical study on Douban and Amazon datasets demonstrates that the proposed framework significantly outperforms the state-of-the-art models under the CDCSR setting."
SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a novel architecture for learning long-term spatial dependencies in the frequency domain with log-linear complexity. The proposed architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a learnable 2D inverse Fourier transforms. Experiments on ImageNet and downstream tasks demonstrate the effectiveness of the proposed architecture."
SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of predicting trustworthiness on large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. To this end, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, namely Vision Transformer and ResNet, as trustworthiness predictions. "
SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8," of adversarial robustness. This paper studies the robustness of robust models from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of adversarian robustness and apply it on more tasks including domain adaption and robustness boosting."
SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then solved iteratively till convergence. The authors demonstrate that their approach outperforms state-of-the-art unsupervised methods and that it outperforms or is on par with supervised data-driven reconstruction approaches."
SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, ALBEF does not require bounding box annotations nor high-resolution images. To improve learning from noisy web data, the authors propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. The authors provide a theoretical analysis of the mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair."
SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. They also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the problem of non-smooth convex stochastic convex optimization with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for the convex convex problem with heavy-tailed noise. In particular, the authors show that the first (accelerated) method also has optimal iteration and oracle complexity in all regimes, and the second one is optimal in the nonsmooth setting. "
SP:a22a893e25ce739dc757861741014764e78aa820, for long-term forecasting of time series. This paper proposes Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. The proposed method outperforms self-attention in both efficiency and accuracy in long term forecasting. The paper is well-written and easy to follow. 
SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic crossword clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. The dataset is composed of two parts: a definition and a wordplay cipher requiring character-level manipulations. The authors show that three non-neural approaches and T5, a state-of-the-art neural language model, do not achieve good performance. They propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. "
SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper studies the internal representation structure of ViTs and CNNs on image classification tasks. The authors find that ViT has more uniform representations across all layers. They find that self-attention and ViT residual connections play a key role in the structure of the ViT network. They also study the effect of pretraining dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to MLP-Mixer. "
SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies the convergence of Thompson sampling (TS) in combinatorial multi-armed bandit (CMAB) problems with a common approximation oracle. The authors provide a regret lower bound of order ⌦(log T/2) to quantify the hardness of TS to solve CMAB problems with greedy oracle, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in which is a hedonic game in which the agents are given a set of data and the goal is to learn a global model. The main contribution of this paper is to provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players. The authors show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, they show that there exist examples of stable arrangements with higher cost than optimal. Finally, the authors provide the first constant-factor bound on the performance gap between stability and optimality."
SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The capsule decompositions of objects are computed through permutation-equivariant attention. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. To train our neural network we require neither classification labels nor manually-aligned training datasets."
SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. The proposed method uses black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Experiments on simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the effect of invariance in kernel ridge regression when the target is invariant to the action of a compact group. The authors derive a strictly non-zero generalisation benefit of incorporating invariance. They study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In building towards this result, they find an orthogonal decomposition of the reproducing kernel Hilbert space and its kernel."
SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. The authors propose a Decodable Neural Network, or DecNN, which enables a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. The experiments demonstrate applications of this uncertainty to out-of-distribution detection, adversarial example detection, and calibration — while matching standard neural networks in accuracy."
SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the rates of convergence of plug-in estimators of optimal transport maps using barycentric projections. The main contribution of the paper is a new stability estimate for barycenters that proceeds under minimal smoothness assumptions and can be used to analyze general plug in estimators. The authors show that under additional smoothness assumption Sobolev type, Besov type, kernel smoothed or wavelet based estimators speed up the rate of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete/semi-discrete/discrete estimators in the paper. They also provide faster rates of convergences for W2(μ, ν), the Wasserstein distance between probability distributions."
SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a method for dataset distillation using infinitely wide convolutional neural networks. The authors propose a distributed kernel-based meta-learning framework to achieve state-of-the-art results on CIFAR-10 and Fashion-MNIST datasets. They show that their method can achieve over 65% test accuracy on Cifar10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The authors also perform some preliminary analyses of their distilled datasets to shed light on how they differ from naturally occurring data."
SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a novel semi-supervised learning algorithm OpenMatch for detecting outliers in unlabeled data. The proposed algorithm is based on one-vs-all (OVA) classifiers to detect inliers in the unlabelled data. In addition, the authors propose an open-set soft-consistency regularization loss to improve the smoothness of the OVA-classifier with respect to input transformations and greatly improves outlier detection. Experiments on CIFAR-10 show that OpenMatch outperforms the state-of-the-art methods."
SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching in robotic environments. The proposed method is based on the idea of learning a world model from image inputs and using it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the exploration phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. "
SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper studies the problem of reducing the number of parameters in language models. The authors propose to use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. They prove that stacking such low rank layers increases their expressiveness, providing theoretical understanding for their effectiveness in deep networks. The approach operates out-of-the-box, replacing each parameter matrix with its compact equivalent while maintaining the architecture of the network. In Transformer models, the proposed approach leads to more than tenfold reduction in the total trainable parameters including embedding, attention, and feed-forward layers, with little degradation in on-task performance."
SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper studies the problem of learning a representation of source code in a distributed manner. The authors propose to encode the pairwise path between tokens in the source code and the path from the leaf node to the tree root for each token in the syntax tree. The proposed approach is based on the idea of positional encoding. The experimental results show that the proposed approach outperforms the baselines.
SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new approach for high-resolution image generation based on GANs. The proposed approach replaces global self-attention in low-resolution stages of the generative process with multi-axis blocked attention, which allows efficient mixing of local and global attention. To further improve the performance, the authors introduce an additional self-modulation component based on cross attention. The resulting model, called HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images."
SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The main contribution of this paper is to provide an upper bound on the query cost of learning Geodesic Convex HalfSpaces (GCs) which is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. The authors show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. They provide evidence that ground-truth communities in real-world graphs are often convex and empirically compare their proposed approach with other active learning algorithms.
SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity video encoder optimization method for temporal action localization (TAL) learning. The key idea is to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoders and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Experiments show that the proposed method can significantly enhance the performance of existing TAL methods."
SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The main contributions are three-fold. First, the authors provide general formulae for the derivatives of regularized M-stimators where differentiation is taken with respect to both y and X. This reveals a simple differentiability structure shared by all convex regularized estimators. Using these derivatives, they characterize the distribution of the residual ri = yi−xi β̂ in the intermediate high-dimensional regime where dimension and sample size are of the same order. Motivated by this distribution, they propose a novel adaptive criterion to select the tuning parameters of regularised M-ESTimators. The criterion approximates the out-of-sample error up to an additive constant independent of the estimator. "
SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a beam-based algorithm for neural abstractive summarization based on attention-aware inference. Specifically, a global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a new self-attention mechanism based on gauge equivariant transformer. The main idea is to use cyclic groups as feature fields in intermediate layers and parallel transport the feature vectors in these fields to disentangle the orientation of the coordinate system in ambient space (i.e., global coordinate system), achieving rotation invariance. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to combine the expectation maximization and Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components of the mixture, thus substantially reducing the computational cost. The proposed method is tested on synthetic and real-data mixtures."
SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training deep neural networks with completely sparse forward and backward passes. The authors formulate the training process as a continuous minimization problem under global sparsity constraint. They then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, they use the conventional chain rule based gradient estimator, which can be sparse via exploiting the sparse structure. The latter step, instead, they propose a variance reduced policy gradient estimators as in existing methods, which only requires two forward passes without backward propagation, thus achieving completely sparse training. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, their algorithm is much more effective in accelerating the training."
SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on non-equilibrium orbits (NEO) to sample from complex distributions. The proposed method is based on the invertible map T, where T is a discrete-time integrator of a conformal Hamiltonian system. The main contribution of the paper is the introduction of NEO-IS and NEO-MCMC, which combines multiple IS and MCMC estimators of the normalizing constant and an iterated sampling-importance resampling mechanism. "
SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,This paper proposes Mini-Batch Consistency (MBC) for large-scale mini-batch set encoding. The proposed method adheres to the required symmetries of permutation invariance and equivariance as well as maintaining MBC for any partition of the input set. It is shown that the proposed method is computationally efficient and results in rich set encoding representations for set-structured data.
SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper studies the problem of learning a policy for a combinatorial game of Diplomacy, where there are more than 10 possible actions per turn. The main contribution of this paper is to propose an algorithm for action exploration and equilibrium approximation in games with combinatorially action spaces. This algorithm simultaneously performs value iteration while learning the policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, the authors train an agent, DORA, completely from scratch for two-player Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy where they show that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multi-language and multi-domain sequence modeling. In particular, the authors show that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. To address this problem, they propose to learn shared and specialized attention heads for different languages and domain. The proposed approach is evaluated on speech recognition, text-to-text and speech-to text translation tasks. "
SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift. The main contribution of the paper is to provide a precise characterization of the limiting test error, bias, and variance in this setting. The authors show that overparameterized models exhibit enhanced robustness to covariate shifts. They also provide an exact linear relationship between the in-distribution and out-of distribution generalization performance."
SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the performance of Thompson Sampling (TS) with misspecified prior in the context of Bayesian POMDPs. The authors prove that the expected reward of TS with a misspecification of the prior differs by at most $O(\sqrt{H}(H)$$ from TS with well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. They show that the bound does not require the prior to have any parametric form and is independent of the cardinality or structure of the action space. They also show that it is tight up to universal constants in the worst case. "
SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"This paper studies the sample/query complexity of the standard PAC-learning model and a version of the Equivalence-Query model. In the PAC model all samples are provided at the beginning of the learning process, while in the EQ model the samples are acquired through an interaction between a teacher and a learner, where the teacher provides counterexamples to hypotheses given by the learner. The authors prove that in order to achieve an error $O(\epsilon)$ exponentially smaller samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness."
SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of finding the best weights to fine-tune to your use-case. The authors propose several benchmarks for evaluating on this task. They find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. They then introduce simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC which outperforms all other methods on diverse model selection."
SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1," of learning binary representations of instances and classes is a classical problem with several high potential applications. In this paper, the authors propose a novel method for Learning Low-dimensional Binary Codes (LLC) for instances as well as classes. The method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes (20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for ResNet50 on ImageNet1K. The authors demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. They further quantitatively measure the quality of our codes by applying it to the efficient image retrieval aswell as out-of-distribution (OOD) detection problems."
SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. GDWS significantly improves the throughput of a pre-trained network on real-life hardware while preserving its robustness to adversarial perturbations. The authors demonstrate the effectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and ImageNet datasets."
SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,". The paper proposes a graph-based approach for predicting the graph topology of precursor molecules during a chemical reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. The proposed method achieves a top-1 accuracy of 53.7% outperforming previous template-free and semi-template-based methods."
SP:772277d969c95924755113c86663fb0e009f24cc,This paper studies the problem of downscaling low-resolution spatial fields with high-resolution (HR) information. The authors propose a Bayesian formulation of deconditioning which naturally recovers the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1]. They extend this formulation to a down-scaling setup and devise an efficient conditional mean embedding estimator for multiresolution data. They show that this solution can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions.
SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a search space for deep sparse networks (DSNs) by automatically searching the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world benchmark datasets show promising results of PROFIT in both accuracy and efficiency."
SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. The authors propose a regularized self-labeling method based on the PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose layer-wise regularization to constrain the distance of each layer, self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points. They validate their approach on an extensive collection of image and text data sets using multiple model architectures. "
SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper studies the problem of identifying the arm from among finitely many that has the smallest conditional value-at-risk (CVaR) or weighted sum of CVaR and mean. The main contribution is an optimal $\delta$-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically. The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The authors develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a transformer-based approach for learning intrinsic inductive bias (IB) from convolutions. The authors propose to use multiple convolutions with different dilation rates to learn the intrinsic IB. The proposed approach is evaluated on ImageNet and CIFAR-10 datasets. "
SP:5e3572a386f890c5864437985cf63b13844f338f," of adversarial fine-tuning. This paper proposes Robust Informative Fine-Tuning (RIFT), a novel approach to fine-tune a pre-trained language model. RIFT encourages the model to retain the features learned from the pre-training model throughout the entire fine-training process, whereas a conventional one only uses the pre trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference. "
SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper proposes a Stochastic Anderson Mixing (SAM) scheme to solve nonconvex stochastic optimization problems. Under mild assumptions, the authors prove the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. To further accelerate the convergence, they incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability."
SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a novel stochastic algorithm for solving linear inverse problems where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed algorithm is based on Langevin dynamics and Newton’s method and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The authors show that the proposed algorithm can produce multiple high perceptual quality samples for the same noisy observation. They demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution and compressive sensing."
SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes MetaHG to automatically detect illicit drug traffickers on social media. The authors propose a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on Instagram. Then, they employ a relation-based graph convolutional neural network to learn node (i.e., user) representations over the built HG, in which they introduce graph structure refinement to compensate the sparse connection among entities in the HG for more robust node representation learning. Afterwards, they propose a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments based on the real-world data collected from Instagram demonstrate that the proposed metaHG outperforms state-of-the-art methods."
SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the question of whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size) in a neural network with ReLU activations and a given architecture. The authors use techniques from mixed-integer optimization, polyhedral theory, and tropical geometry to provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. They also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a generalization of the worst-case adversarial training (AT) approach to adversarial attacks. The main idea is to reformulate the adversarial attack as a min-max optimization problem, where the loss function is minimized over the probability simplex of the domain set. The proposed approach is applied to three types of attacks: attacking model ensembles, universal perturbation under multiple inputs, and attacks that are resilient to data transformations. Experiments show that the proposed approach leads to substantial attack improvement over the existing heuristic strategies and robustness improvement over state-of-the-art defense methods."
SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis, where the goal is to recover the k-sparse unit vector x \in R with i.i.d. Gaussian entries. The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For any 1-t-k, the algorithms recovers the sparse vector for signal-tonoise ratio $L_1$ in time $O(\sqrt{T}(k/t)$, capturing the state-of-the-art guarantees for the matrix settings (in both the polynomials-time and sub-exponential time regimes). In the restricted case of sparse PCA, the authors show that their algorithms only recover the sparse vectors for $L_{\epsilon}$ while the existing algorithms require $L^{O(k)$. "
SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a novel approach for learning multilayer-perceptron (MLP) networks to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. The proposed approach gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space."
SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of finding the quantal response equilibrium (QRE) of two-player matrix games with entropy regularization. The main contribution of the paper is to develop a novel algorithm for finding the QRE at a linear rate. The algorithm is based on the last-iterate convergence of extragradient methods in the unconstrained setting. The proposed algorithm can be implemented in a decentralized manner where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly. In addition, the proposed algorithms can locate an approximate Nash equilibrium of the unregularized matrix game at a sublinear rate without assuming the Nash equilibrium to be unique."
SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on an implicit position encoding scheme, where pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs."
SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes to use sumproduct networks (SPNs) to learn interventional distributions using over-parameterized gate functions, e.g., neural networks. The proposed method is motivated and illustrated by a structural causal model themed around personal health. The empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate."
SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model based on deep Markov factor analysis (DMFA) to capture temporal dynamics in functional magnetic resonance imaging (fMRI) data and tackle their high spatial dimensionality. DMFA is able to cluster fMRI data in its low dimensional temporal embedding with regard to subject and cognitive state variability, therefore, enables validation of a variety of fMRI-driven neuroscientific hypotheses. Experimental results on both synthetic and real fMRI datasets demonstrate the capacity of DMFA in revealing interpretable clusters and capturing nonlinear temporal dependencies in these high dimensional imaging data."
SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper introduces Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNFs). Unlike other CNFs, MF also produces a CNF via a solution to the change-of-variable formula, however, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of unsupervised representation learning in the setting of non-linear blind source separation. The authors propose an approach based on the principle of independent causal mechanisms, which is motivated by thinking of each source as independently influencing the mixing process. This gives rise to a framework which they term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of nonidentifiability issues arising in nonlinear blind sources."
SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes an approach to obtain approximate samples from an unnormalized target distribution and a tight lower bound on its (log) normalization constant logZ. The authors propose to use an AIS-like procedure with Uncorrected Hamiltonian MCMC, which leads to tight and differentiable lower bounds on logZ, and they show that their method yields better performances than other competing approaches. "
SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes an efficient and trainable local Lipschitz bound for the certified robustness of deep neural networks. The main contribution of the paper is to reduce the number of rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global LPschitz constant of the neural network. Furthermore, the authors propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local LPS bound. The proposed method outperforms state-of-the-art methods in both clean and certified accuracy on MNIST, CIFAR-10 and TinyImageNet datasets."
SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a new approach to obtain conformal Bayesian predictive intervals with finite sample calibration guarantees. The proposed approach is based on adding-one-in importance sampling (i.e., re-weighting the posterior samples of the model parameters) to obtain the conformal predictive intervals. The authors show that the proposed method is computationally efficient compared to existing conformal methods that require expensive refitting of models or data-splitting to achieve computational efficiency. "
SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoisers for inverse problems. The main contribution of the paper is to propose a new denoiser for image denoising based on the gradients of smooth scalar-valued deep neural networks. The authors show that the proposed denoiser has symmetric Jacobians, allowing for MAP and MMSE estimators interpretation. They also show that it can be integrated into RED and PnP schemes with backtracking step size, removing the need for enforcing their Lipschitz constant. "
SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music that is in-sync with the human body movements in a video. The method is based on the idea of music improvisation, which involves the prescription of streams of the beat, the rhythm and the melody. The authors propose a system that takes as an input a video with human movements and generates a soundtrack for it. The system first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Next, it implements a transformer-based model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and the offsets of the instruments."
SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a one-step policy improvement algorithm for offline reinforcement learning (RL) using an on-policy Q estimate of the behavior policy. The proposed algorithm is shown to outperform iterative algorithms on a large portion of the D4RL benchmark. The authors argue that the performance of iterative approaches is a result of the high variance inherent in doing off-policy evaluation and magnified by the repeated optimization of policies against those estimates. 
SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of nonsmooth low-rank matrix recovery using convex relaxations. The authors show that the extragradient method, when initialized with a warm-start point, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also provide a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method. They support their theoretical results with empirical experiments on several nonsmoothing low rank matrix recovery tasks."
SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper studies the problem of estimating conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, the authors propose the generalized Robinson decomposition of the causal estimand, which allows one to plug in arbitrary models for learning, and possesses a quasi-oracle convergence guarantee under mild assumptions. In experiments with small-world and molecular graphs, the proposed approach outperforms prior work in CATE estimation."
SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper studies the problem of causal effect identification, which is concerned with determining whether a causal effect is computable from a combination of qualitative assumptions about the underlying system (e.g., a causal graph) and distributions collected from this system. In this paper, the authors first characterize the relationships between certain graphically-driven formulae and matrix multiplications and further propose novel intermediary criteria based on the pseudo-inverse of a matrix. Finally, they devise a new graphical identification algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach."
SP:db15860d08418f6bc792c2ade2eade32840a12b8,"In this paper, the authors propose Dual Curriculum Design (DCD) as a special case of Prioritized Level Replay (PLR) for unsupervised environment design (UED). The authors show that PLR is similar to UED in the sense that it can generate novel and complex levels for effective training. They show that by curating completely random levels, PLR, too, can generate new and complex training environments. The authors also show that DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. "
SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit problem in the shuffle model. The authors propose an algorithm with a distribution-dependent regret of $O(\sqrt{kT} + kT + k \log T + k^2 log T + 1/\delta log T)$, where $k$ is the number of arms, $T$ is number of rounds, and $a$ is suboptimality gap of the arm $a$. The authors show that their algorithm achieves a regret that matches the regret of the best known algorithms for the centralized model, and significantly outperforms the best-known algorithm in the local model."
SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration to ensure that decision loss is predicted accurately for threshold decisions. The authors propose an efficient algorithm that takes an uncalibrated forecaster as input and provably outputs a threshold-calibration-based forecaster. The proposed algorithm allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution). The authors show that the proposed method can minimize a surrogate of Wasserstein distance between the ground-truth arg max distribution and the centroid approximations under proper conditions. The authors demonstrate the effectiveness of their method on few-shot image classification, personalized dialogue systems, and multi-target domain adaptation."
SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of multi-objective reinforcement learning where the objectives are balanced using preferences. The authors formalize this problem as an episodic learning problem on a Markov decision process where transitions are unknown and a reward function is the inner product of a preference vector with pre-specified reward functions. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. They provide a model-based algorithm that achieves a nearly minimax optimal regret bound Ú (√ min{d, S} ·H2SAK ), where d is the number of objectives, S is number of states, A is the length of the horizon, H is the time horizon, and K is the total number of episodes. Furthermore, they consider preference-free exploration, i.e., the agent first interacts with the environments without specifying any preference and then is able to accommodate arbitrary preference vector up to error. Their proposed algorithm is provably efficient with a nearly optimal trajectory complexity."
SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper studies the problem of generating explanations for dialogue response generation. The authors propose a new method, LERG, that considers the explanation as the mutual interaction of segments in input and output sentences. They show that the proposed method adheres to desired properties of explanation for text generation, including unbiased approximation, consistency, and cause identification. Empirically, the method consistently improves other widely used methods on proposed automatic and humanevaluation metrics for this new task by 4.4-12.8%."
SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence of error-compressed gradient compression methods for distributed training of large-scale machine learning models. In particular, the authors propose and study the error compensated loopless Katyusha method and establish an accelerated linear convergence rate under standard assumptions. They show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a new approach for learning a neural network using a neuron-astrocyte liquid state machine (NALSM) model. NALSM is inspired by the observation that astrocytes, a long-neglected non-neuronal brain cell, modulates synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. The authors propose to use the astroCyte model to self-organize the network's dynamics around a critical branching factor that is associated with the edge-of-chaos. The proposed method is evaluated on MNIST and Fashion MNIST datasets and shows comparable performance to the state of the art."
SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of graph topology imbalance in semi-supervised node classification. The authors propose a new metric, Totoro, to measure the degree of the imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. The experiments show the effectiveness and generalizability of the proposed method."
SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of estimating the partition of the lattice induced by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the Dyadic classification and regression tree (DCART) methodology proposed by [14] to solve this problem. They prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k2k\log(N)/k(\sqrt{k}^2k) where k is the minimal number of rectangular sub-graphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, and $\gamma$ is the smallest magnitude of the signal difference among contiguous elements. They also show that under stronger assumptions, their method attains a sharper estimation error of order $O(k^2)$ log(N)$, which is minimax rate optimal. "
SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. The authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely counterfactual maximum likelihood estimation (CMLE). They derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. They conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations."
SP:c5a59c8d6db0f5491721aaaef182609c360930d3,This paper studies the problem of multi-task learning in which the objective is to minimize the average loss across all tasks. The authors propose a new algorithm called Conflict-Averse Gradient Descent (CAGrad) to address this problem. The main idea is to use the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average losses. 
SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of language models. In particular, the authors study how language models can identify simple algorithmic concepts from small witness sets. They study how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. This first joint analysis of language model and machine teaching can address key questions for artificial intelligence and machine learning. "
SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method to identify the robust and non-robust features in the feature space using Information Bottleneck. The proposed method is based on injecting noise variation to each feature unit and evaluating the information flow in the representation to dichotomize feature units based on the noise variation magnitude. Through comprehensive experiments, the authors demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of non-Robust features that is directly related to the model prediction."
SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the phenomenon of support vector proliferation in support vector machine (SVM) and minimum Euclidean norm least squares regression (MNSR). The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vectors to proliferate in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and bound the width of this transition. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the active pure exploration problem in Markov Decision Processes (MDPs) where the agent sequentially selects actions and, from the resulting system trajectory, aims to identify the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. They further provide the first algorithm with an instance-specific sample complexity in this setting. This algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption."
SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a novel query embedding model, Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjoint operations. By further noticing that the closure of complement of cones remains cones, ConE design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper studies the value iteration (VI) algorithm for the infinite-horizon, discounted cost, optimal control of stochastic nonlinear systems with separable cost and constraints in the state and input variables. The authors propose a novel numerical scheme for implementation of the corresponding value iteration algorithm in the conjugate domain. The proposed approach reduces the time complexity of each iteration in the VI algorithm from O(XU) to $O(X + U)$ by replacing the minimization operation in the primal domain with a simple addition in the convolutional domain."
SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a method for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multi-modal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text to video retrieval. They show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task."
SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"-based transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. In this paper, the authors propose Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance while requiring fewer computation resources."
SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation approach for policy cloning in the policy cloning setting, which allows for offline queries of an expert or expert policy. The proposed method combines image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show the benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper studies the problem of designing robust objects for computer vision, i.e., objects that are explicitly optimized to be confidently classified. The authors propose a framework that leverages deep networks’ sensitivity to input perturbations to design “robust objects” to design robust objects. They show that the proposed framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments."
SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the effect of data augmentation in off-policy RL algorithms on sample efficiency and generalization. The authors propose a simple yet effective technique to stabilize these algorithms under augmentation. The proposed method is based on the observation that high-variance Q-targets are the main causes of instability. The method is evaluated on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks."
SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the trade-off between the number of local updates and the communication rounds in FedAvg, a stochastic algorithm for non-convex Federated Learning (FL). The paper shows that when both the WN and the server’s directions are chosen based on a momentum estimator, the algorithm requires $O(\sqrt{3/2})$ samples and $O(1/2)$ communication rounds to compute an $\epsilon$-stationary solution. This is the first FL algorithm that achieves such near-optimal sample and communication complexities simultaneously. The paper also shows that for the classical FedAvg algorithm, a similar tradeoff curve exists, albeit with worse sample complexity."
SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the generalization bound of SGLD with isotropic noise. The main contribution of the paper is to provide a new information-theoretical bound for the noise structure of the noise. In particular, the paper shows that the optimal noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The paper also provides a matrix analysis to derive the form of optimal noise covariances."
SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a new video compression method for video compression. The proposed method is based on the idea of multi-reference frame prediction. The authors propose to use multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. They show that the proposed method achieves comparable R-D performance with the latest Versatile Video Coding standard in terms of MS-SSIM. "
SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies a new type of multi-task generalization of Online Mirror Descent (OMD) that operates by sharing updates between tasks. The authors prove that the regret of MT-OMD is of order 1 + σ2(N-1) + \sqrt{\sqrt{T}^2}(N - 1)^2(T), where N is the number of tasks and T is the time horizon. This improves upon the NT bound obtained by running independent OMDs on each task. The paper also provides numerical experiments on several real-world datasets which support the theoretical findings."
SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with stronglyconvex potential consisting of finite summation of N smooth components, and propose an efficient discretization method, which requires O(N + d 1 3N 2 3 /ε 2 3 ) gradient evaluations to achieve $\�-error (in $\�E\sqrt{2}$ distance) for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $O(\log N + d)$ in dependence of N, \epsilon, and d. The authors apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms."
SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"This paper proposes a method for continual learning based on Bayesian weight regularization to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. The proposed method outperforms existing methods in both feedforward and recurrent networks in continual learning problems. The paper also shows that the trained networks evolve task-specific dynamics that are strongly preserved as new tasks are learned."
SP:26de056be14962312c759be5d284ef235d660f9c,"This paper studies the problem of learning invertible normalizing flows with tractable change-of-volume terms. The authors propose two methods to calculate the gradient of this term with respect to the parameters of the model, relying on careful use of automatic differentiation and techniques from numerical linear algebra. Both approaches perform end-to-end nonlinear manifold learning and density estimation for data projected onto this manifold. They show that their methods outperform approaches ignoring the volume-change term by more accurately learning manifolds and the corresponding distributions on them, and show promising results on out of distribution detection."
SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a new framework for inference and learning in networks of slow components by harnessing the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. The authors jointly derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. The resulting model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. They demonstrate successful learning of standard benchmark datasets, achieving competitive performance using both fully-connected and convolutional architectures."
SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a novel approach for graph neural networks (GNNs) to represent a graph with rooted subgraphs instead of rooted subtrees. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. The authors provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. "
SP:7b8284aa82022ce73802bfc57238b0d82031b226," of nested variational inference (NVI) is a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. The experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristic that approximate the likelihood of future observations in a hidden Markov model and (c) perform amortized inference in hierarchical deep generative models."
SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order black-box optimization of a Lipschitz function f defined on a compact subset X of R with the additional constraint that algorithms must certify the accuracy of their recommendations. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral $\in \mathcal{O}(\sqrt{X}(f(x) + \epsilon)^2)$. This result was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, the upper bound relies on a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that they link to the above integral. They also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a novel attack that attacks the network’s capacity for uncertainty estimation. The attack is based on the idea that the network is more confident of its incorrect predictions than about its correct ones after the attack. The authors show that the proposed attack can be used to attack three popular uncertainty estimation methods: vanilla softmax score, Deep Ensembles and MC-Dropout. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in unusable uncertainty estimations."
SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in dynamic networks. The authors propose a simple model for networks growing over time which they refer to as streaming stochastic block model (StSBM). Within this model, they prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. "
SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the representation cost of linear convolutional neural networks (LNNs) and residual networks (RNNs). The authors study the regularization cost in predictor space induced by l2 regularization on the parameters (weights) of LNNs and RNNs. In particular, they focus on linear neural networks as parameterizations of linear predictors. They identify the representation costs of certain sparse linear ConvNets and residual Networks. They also study the reverse problem, identifying which regularizers (e.g., group quasi-norms, the k-support-norm, elastic net) can be representation cost induced by simple l2-regularization and designing the parameters that do so."
SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a novel approach to solve the problem of finding a target entity given a source entity and a binary relation in a knowledge graph (KG). The key idea is to find a graph path pattern that connects similar source entities through the given relation. The proposed approach is based on the idea that the graph path patterns can be seen as a set of logical rules for each query. The method is evaluated on two datasets, NELL-995 and FB-122, and shows state-of-the-art performance. "
SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,"This paper presents an entity linking model that combines a Transformer architecture with large scale pretraining from Wikipedia links. The model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The paper presents detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, transformer architecture, and input perturbations. It also presents promising results on end-to-end entity linking and entity linking without in-domain training data."
SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes an ensemble active learning method for large-scale training of deep neural networks (DNNs). The main idea is to use an ensemble of hundreds of models to search the training data subset search for large labeled datasets. The authors show that their approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. The experiments on three image classification benchmarks (CIFAR10, CIFAR100 and ImageNet) demonstrate the effectiveness of the proposed method."
SP:4a1cce61f12c68846c507130bd055b3444ac8101,This paper proposes a new routing algorithm for capsule networks based on inverted dot-product attention. The proposed method is evaluated on CIFAR-10/100 and ImageNet-100 datasets. It is shown that the proposed method improves the performance of ResNet-18 with 4x fewer parameters. The authors also show that their method can be applied to real-world tasks.
SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a novel approach for hyperparameter optimization in deep neural networks. In particular, the authors propose to use the parallel tempering technique of statistical physics to sample the hyperparameters of each instance of the network during training. Each simulation corresponds to a unique path, or history, in the joint hyper-parameter/model-parametrization space. The proposed approach is evaluated on dropout and learning rate optimization problems, where the authors show that the proposed approach outperforms existing methods."
SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper studies the problem of machine translation (MT) in the context of Reinforcement Learning (RL). The authors show that the most common RL methods for MT do not optimize the expected reward, and show that other methods take an infeasibly long time to converge. They also show that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. The authors further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve."
SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,This paper studies the large-sample behavior of the Q-value estimates with closed-form characterizations of the asymptotic variances. The authors propose a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. Numerical experiments show superior performances of the exploration strategy than other benchmark approaches.
SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"This paper proposes a method to improve the efficiency of Top-k recommendation by denoting users and items as binary codes, which applies to various settings: cold start users, cold-start items and warm-start ones. The proposed method is based on the Minimum Description Length (MDL) principle to learn hash functions of users and item hash functions through the MDL principle. In addition, CGH initiates a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages of the proposed method over competing baselines."
SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"This paper proposes Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. The proposed method is applied to pharmacogenomics where the goal is to predict drug response in patients using their genomic information. Experimental results show that the proposed method outperforms state-of-the-art pharmacogenomic and transfer learning baselines."
SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward. The proposed approach is tested on a collection of benchmark tests including simulated robotic locomotion.
SP:2aaddb6dda434b49487857d99c9d143e2f54d350,This paper studies the problem of detecting adversarial examples using a class-conditional reconstruction of the input image. The authors propose a reconstruction attack that aims to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial example but with a much smaller success rate. They find that CapsNets always perform better than convolutional networks. They also find that the success of the reconstructive attacks is highly related to the visual similarity between the source and target class.
SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,This paper studies the effect of initialization and activation function on the NTK as the network depth becomes large. The authors show that training a neural network of any kind with gradient descent in parameter space is equivalent to kernel gradient descent with respect to the Neural Tangent Kernel (NTK). They provide experiments illustrating their theoretical results. 
SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. The key idea is to learn to represent sentences by contrasting these diverse views of the same sentence. By contrasting different linguistic views, the authors aim at building embeddings which better capture semantic and are less sensitive to the sentence outward form."
SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper proposes FinBERT, a language model based on BERT, for financial sentiment classification task in FinancialPhrasebank dataset. The proposed model outperforms the state-of-the-art performance by 14 percentage points. The main contribution of the paper is the use of NLP-based transfer learning to improve the performance of the proposed model. "
SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a method for singing voice generation without pre-assigned musical scores and lyrics. The authors propose three different methods to generate singing voice waveforms. The main contribution of the paper is the development of source separation and transcription models for data preparation, adversarial networks for audio generation, and customized metrics for evaluation. "
SP:99d41c8285fd0270ff16e915ef03187a0a7005b0, adversarial attacks are very sensitive to adversarial perturbations. This paper proposes a novel adversarial defence technique that leverages a latent high order factorization of the network. The proposed approach can be easily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. The experimental results demonstrate the effectiveness of the proposed approach on standard image classification benchmarks. 
SP:762729b64c1c1494de0f7410ea3662da61e93b6d," forecasting is a well-known problem in the field. However, most of the existing approaches fail to account for the unsmoothness issue of urban data in their architecture design, which significantly deteriorates their prediction performance. This paper proposes a novel clustered graph transformer framework that integrates both graph attention network and transformer under an encoder-decoder architecture to address such unsmoothedness issue. In spatial domain, the authors propose a gradient-based clustering method to distribute different feature extractors to regions in different contexts. In temporal domain, they propose to use multi-view position encoding to address the periodicity and closeness of urban time series data. Experiments on real datasets obtained from a ride-hailing business show that their method can achieve 10%-25% improvement than many state-of-the-art baselines."
SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper studies the algorithmic and statistical properties of deep Q-network (DQN) algorithms. The main contribution of the paper is the analysis of the FQI algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network used in the algorithm. Under mild assumptions on the action-value function of the iterative policy sequence obtained by FQN, the authors show that the statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithm error converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques of experience Replay and target Network, which are crucial to the empirical success of the algorithms. "
SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes a new attention architecture called PhraseTransformer to represent the compositional attentions in phrases. In particular, the authors propose to use hypernodes to represent candidate phrases in attention. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer, and the second phase represents the inductive bias within each phrase. The experimental results on the WMT16 English-German translation task show the effectiveness of the proposed architecture. "
SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper presents a modular neural network architecture MAIN that learns algorithms given a set of input-output examples. MAIN consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, MAIN uses a general domain-agnostic mechanism for selection of modules and their arguments. The authors evaluate MAIN on five algorithmic tasks and show that it can learn policies that generalize perfectly to inputs of much longer lengths than the ones used for training."
SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,This paper proposes a method to measure the sensitivity of deep neural networks to quantization in floating point arithmetic. This is done by applying Monte Carlo arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The method makes no assumptions regarding the underlying parameter distributions. The proposed method is evaluated on the CIFAR-10 and ImageNet datasets. 
SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). In particular, the authors study the mismatch between the likelihood of the forward dynamics model w.r.t. the one-step ahead prediction and the overall goal of improving performance on a downstream control task. They propose an initial method to mitigate the mismatch issue by reweighting the dynamics model training."
SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions. The proposed method achieves state-of-the-art targeted blackbox transfer-based attack results for undefended ImageNet models. Further, the authors place a priority on explainability and interpretability of the attacking process. "
SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper proposes a new approach for comparing policies using Wasserstein distances (WDs) in a newly defined latent behavioral space. The authors show that by utilizing the dual formulation of the WD, they can learn score functions over trajectories that can be used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, this dual formulation allows them to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. They incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-guided Evolution Strategies, which outperform existing methods in a variety of challenging environments."
SP:ef1c6403597c3a6083c1ad4256449325ac99416c,This paper proposes an adaptive learning rate for interpolation with gradients (ALI-G) algorithm. The main idea is to use the interpolation property to compute the learning-rate in closed form at each iteration. The authors provide convergence guarantees in the stochastic convex setting and show that the proposed algorithm converges up to some tolerance. The proposed algorithm is evaluated on a variety of tasks and shows comparable performance with SGD. 
SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper studies the effect of connections between neurons in a neural network on the ability of the network to form perceptual groups. In particular, the authors study the role of bottom-up, horizontal, and top-down connections in the learning of perceptual groups on two synthetic visual tasks: Gestalt vs. high-level object cues for perceptual grouping. They show that increasing the difficulty of either task strains learning for networks that rely solely on bottom up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top down connections rescue learning on task with high level object cues by modifying coarse predictions about the target object. "
SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems, the authors propose a new regularizer that is differentiable and scale invariant. The experiments show that the proposed regularizer can produce even sparser neural network models than previous works, under the same accuracy level."
SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam that achieves a data-dependent O(log T) regret bound for strongly convex functions. The main idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. The paper shows that under a special configuration of hyperparameters, the proposed SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly-convex functions, for which it provides the first data- dependent logarithmic regret bound. Empirical results on optimization problems and training deep networks demonstrate the effectiveness of the proposed method."
SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a new BERT-based model BlockBERT, which uses sparse block structure to reduce both memory consumption and training time. The proposed model also enables attention heads to capture either short or long-range contextual information. The authors conduct experiments on several benchmark question answering datasets with various paragraph lengths. They show that the proposed model uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%."
SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the effect of pruning on the generalization ability of neural networks. The authors show that pruning can lead to instability in the test accuracy of the network. They show that even the pruning of unimportant parameters leads to such instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements. "
SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,This paper proposes NAS in embedding space (NASES) to solve the problem of neural architecture search in non-continuous and high-dimensional search spaces. The main idea is to use an encoder and decoder network to search for the final architecture network. The proposed method is evaluated on CIFAR-10 and shows that NASES outperforms other NAS methods on the image classification task.
SP:e2e5bebccc76a51df3cb8b64572720da97174604,"This paper proposes a homotopy training algorithm to solve optimization problems arising from neural networks. The algorithm starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupling systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopic path guided by the HTA. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the proposed method can be combined with the dropout technique to provide an alternative way to train the neural network."
SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,This paper proposes an extension of the Transformer architecture that uses higher-dimensional attention to update entity representations with tensor products of value vectors. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning. The paper also shows that the proposed architecture can be used to train deep RL models.
SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,This paper proposes a method for pose estimation. The proposed method is based on the Conditional Variational Autoencoders (CVAEs) from Kingma et al. (2014) and uses circular latent representations to estimate the corresponding 2D rotations of an object. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing.
SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a curriculum learning approach for multi-agent reinforcement learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. The main idea is to maintain multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes agents with the best adaptability to the next stage. Experiments on MADDPG show that the proposed approach consistently outperforms baselines by a large margin as the number of agents grows exponentially."
SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,"This paper studies the problem of using multi-scale graph networks for graph reasoning. The main contribution of the paper is to propose a method for computing the log-likelihood of the output of a graph network. The paper is well written and easy to follow. However, there are a few issues that need to be addressed. "
SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors show that ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood on the Cifar10 and ImageNet benchmarks.
SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper studies the problem of identifying winning tickets in deep neural networks. The authors propose to identify winning tickets at a very early stage of training using low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. They also propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, they leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments on various deep networks and datasets validate the effectiveness of the proposed methods."
SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"-based adversarial robustness is a well-studied problem in deep learning. The authors propose an approach to improve the robustness of deep neural networks by regularizing the embedding of the input images. The proposed approach is based on the observation that the intrinsic dimension of image data is much smaller than its pixel space dimension, and the vulnerability of neural networks grows with the input dimension. "
SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and their relations. The model is based on a large pre-trained language model. The main contribution of the paper is to propose a model that does not rely on external NLP tools and which integrates a large, pre-training language model and is fast to train. "
SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of representation learning from a new perspective. The authors propose a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers to the above-mentioned triplet comparisons. This problem has been studied in a sub-community of machine learning by the name “Ordinal Embedding”. The proposed approach is significantly faster than existing methods, and can scale to real-world large datasets."
SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,This paper proposes a meta learning framework to learn data values jointly with the target task predictor model. The authors propose a data value estimator (modeled by a deep neural network) to learn how likely each datum is used in training of the predictor model to learn the value of each data point. The paper shows that the proposed method yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes.
SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the per-layer Jacobian squared norm of ReLU networks. The authors show that the variance of the norm is exponential in depth for ResNets and polynomial for DenseNets, and that there exists an initialization strategy for both networks such that the norm can be preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. They also show the statistics of the per layer Jacobian norm is a function of the architecture and the layer size, but surprisingly, not the layer's depth."
SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4," is a novel approach to speed up the execution of deep neural networks. The main contribution of the paper is the use of reinforcement learning to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. The authors propose an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experiments on real hardware shows that CHAMELEON provides 4.45x speed up in optimization time over AutoTVM while also improving inference time of the modern deep networks by 5.6%."
SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper proposes a method to derive certified robustness for top-k predictions. The proposed method is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. This method is scalable to large-scale neural networks and applicable to any classifiers. The authors derive a tight robustness in `2 norm for topk predictions when using randomized smoothhing with Gaussian noise. They also empirically evaluate their method on CIFAR10 and ImageNet."
SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the relationship between the gradient signal to noise ratio (GSNR) of parameters during training process of deep neural networks (DNNs) and the generalization gap. The GSNR of a parameter is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. Based on several approximations, the authors establish a quantitative relationship between model parameters’ GSNr and the Generalization Gap. This relationship indicates that larger GSNRs during the training process leads to better generalization performance. Moreover, they show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNrs during training, which is probably the key to the remarkable generalization ability."
SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a new embedding-based framework for logical queries with logical disjunctions in massive and incomplete KGs. The main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to the answer entities of the query. The authors show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunction would require embedding with dimension proportional to the number of KG entities. However, they show that by transforming queries into a Disjunctive Normal Form, QUERY2BOX is capable of handling arbitrary logical queries in a scalable manner. Experiments on three large KGs demonstrate the effectiveness of the proposed method."
SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper studies the convergence of consistent gradient estimators for SGD for strongly convex, convex and nonconvex objectives. The authors show that consistent estimators result in the same convergence behavior as do unbiased ones. They show that the convergence rate is the same as that of unbiased estimators. The paper also shows that the consistency of the estimator can be used to reduce the computational cost of SGD. "
SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. To efficiently train OFA networks, the authors propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top-1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetv3, 2.6X faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."
SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,This paper proposes an extension of Neural Module Networks (NMNs) to the open-domain QA setting. The authors propose to use an auxiliary loss to help extract arguments associated with the events in text. The auxiliary loss is a probabilistic and differentiable loss. The proposed method is evaluated on a subset of the DROP dataset. 
SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper studies the effect of connection sensitivity on the performance of network pruning. The authors propose to use the connection sensitivity as a form of gradients in the initialization of the network, and propose a data-free pruning method to improve the signal propagation properties of the pruned networks. The proposed method is evaluated on a variety of image classification tasks. "
SP:d5899cba36329d863513b91c2db57675086abc49,"This paper studies the choice of intra-layer topology in sparse neural networks. The authors derive a new sparse neural network initialization scheme that allows them to explore the space of very deep sparse networks. They evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. They develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. They then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them."
SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,This paper proposes a novel initialization scheme for recurrent neural networks (RNNs) that is based on the mean field theory of signal propagation in LSTMs and GRUs. The authors propose to use the spectral properties of the state-to-state Jacobians to optimize the initialization hyperparameters of RNNs. They demonstrate the efficacy of their initialization scheme on multiple sequence tasks on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower.
SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,This paper proposes a novel approach for post-classification in remote sensing. The proposed approach is based on a siamese network to improve the discriminative power of convolutional neural networks on a pair of neighboring scene images. It then exploits semantic coherence between this pair to enrich the feature vector of the image for which it wants to predict a label. Empirical results show that this approach provides a viable alternative to existing methods.
SP:99c10e038939aa88fc112db10fe801b42360c8dc,"This paper proposes a method for self-supervised monocular depth estimation using semantic segmentation networks to guide representation learning via pixel-adaptive convolutions. The authors propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. The proposed method improves upon the state-of-the-art on all pixels, fine-grained details, and per semantic categories."
SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper considers the problem of label-flipping attacks, where an adversary relabels a small number of examples in a training set in order to degrade the performance of the resulting classifier. In this paper, the authors propose a strategy to build linear classifiers based on deep features that are certifiably robust against a strong variant of label flipping attacks where the adversary can target each test example independently. The proposed approach leverages randomized smoothing, a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a classifier, to guarantee the certification that its prediction would be the same if some number of training labels were changed adversarially. The authors obtain these certified bounds with no additional runtime cost over standard classification."
SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper proposes to use differential privacy to improve the detection of outlier detection and novelty detection in backdoor poisoning attacks. Differential privacy is a privacy-based method for aggregated analysis of a given dataset. It is typically achieved by adding random noise, either directly to the input dataset or to intermediate results of the aggregation mechanism. In this paper, the authors show that applying differential privacy can improve the utility of the proposed method. The authors also conduct extensive experiments to validate the effectiveness of differential privacy in improving the detection. "
SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the calibration of uncertainty prediction for regression tasks. The authors propose a histogram-based approach inspired by reliability diagrams used in classification tasks. They show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions. They propose a new definition that escapes this caveat and an evaluation method using a simple histogram based approach. They also propose a simple scaling-based calibration that preforms well in their experimental tests."
SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes to combine the structured representation power of Tensor-product representations (TPRs) and BERT, a pre-trained bidirectional Transformer language model. The authors show that there is shared structure between different NLP datasets that HUBERT, but not BERT is able to learn and leverage. The experiments show that untangling data-specific semantics from general language structure is key for better transfer among NLP tasks."
SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper proposes a new approach for multi-agent reinforcement learning in the context of humanoid navigation. The proposed approach is based on a partial parameter sharing approach where the lower level of the hierarchy is shared enabling learning using decentralized methods. The lower-level controllers are task-agnostic and can be shared by higher-level policies. The goal conditioned policies are used to learn goal-conditioned policies to learn low-level physical controllers for balance and walking. The authors also train decentralized heterogeneous policies for goal-directed collision avoidance. The results show that with this combination of methods, RL techniques can be used for finding strong policies."
SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,This paper proposes a new graph pooling method for graph neural networks (GNNs). The key idea is to use a spatial representation of the graph to make the neural network aware of the differences between the nodes and also their locations in the graph. The spatial representation is obtained by a graph embedding method. The proposed method is shown to achieve competitive or better results in comparison with the state-of-the-art methods.
SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,This paper proposes a method for down-sampling convolutional neural networks (CNNs) that is shift-equivalent. The proposed method is based on inverse discretization of the discrete Fourier transform (DFT) and is called frequency pooling. Experiments on image classifications show that the proposed method improves accuracy and robustness w.r.t shifts of CNNs. 
SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a novel method to improve the generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. The proposed method is evaluated on CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks."
SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation approach for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. The authors propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match. They find that their explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. The approach is demonstrated on two datasets from diverse domains."
SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper presents WaveFlow, a small-footprint generative flow for raw audio, which is trained with maximum likelihood without density distillation and auxiliary losses as used in Parallel WaveNet. It provides a unified view of flow-based models for audio, including autoregressive flow (e.g., WaveNet) and bipartite flow as special cases. The authors systematically study these likelihood-based generative models for raw waveforms in terms of test likelihood and speech fidelity. They demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms."
SP:963e85369978dddcd9e3130bc11453696066bbf3,This paper proposes a novel GAN-based approach for graph generation. The main idea is to use graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A new conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate the effectiveness and scalability of the proposed method.
SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence model called METAGROSS (Meta Gated Recursive Controller), which is based on meta-gating and recursively parameterizing a recurrent model. The main idea is that the gating mechanism of the model is controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta gating. The experiments show that the proposed model achieves state-of-the-art (or close) performance on all tasks."
SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a self-supervised approach for speech recognition based on local prior matching (LPM). The LPM objective leverages a strong language model to provide learning signal given unlabeled speech. Since LPM uses a language model, it can take advantage of vast quantities of both unpaired text and speech. The loss is theoretically well-motivated and simple to implement. More importantly, LPM is effective. LPM reduces the WER by 26% and 31% relative on a clean and noisy test set, respectively. This bridges the gap by 54% and 73% WER on the two test sets relative to a fully supervised model."
SP:e6af249608633f1776b608852a00946a5c09a357,This paper studies the problem of fair and robust model training in the presence of data poisoning. The authors propose a generative adversarial network (GAN) framework to perform fair training using GANs. They propose two discriminators: (1) a fairness discriminator that predicts the sensitive attribute from classification results and (2) a robustness discriminator to distinguish examples and predictions from a clean validation set. They show that the proposed method shows almost no decrease in fairness and accuracy. 
SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a novel approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. In particular, the authors propose an Eulerian-Lagrangian representation of the data using a static background grid and a Lagrangian material space, using moving particles. By introducing this representation, they are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. They demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance."
SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper studies the robustness of gradient clipping in the context of label noise in classification. The authors show that standard gradient clipping does not in general provide robustness. Instead, they show that a simple gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. They show that this simple modification yields a simple, noise-robust modification of the standard cross-entropy loss which performs well empirically."
SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state alignment based imitation learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and is combined into a reinforcement learning framework by a regularized policy update objective. The authors show the superiority of their method on standard imitation learning settings and imitation learning where the expert and imitators have different dynamics models.
SP:91761d68086330ce378507c152e72218ed7b2196, is an extension of SGD to deep neural networks. The key idea of DGB is that back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. Thus at each layer of a neural network the weight update is calculated by solving the corresponding boosting problem using a linear base learner. The resulting weight update formula can also be seen as a normalization procedure of the data that arrives at each layers during the forward pass. The new architecture shows improved performance on image recognition tasks when compared to the same architecture without normalization layers.
SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"This paper proposes a novel approach for differentiable architecture search (DARTS) by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, it performs operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of the super-net caused by sampling different channels. To alleviate it, the paper proposes edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, the proposed method can be trained with a larger batch size and enjoys both faster speed and higher training stability."
SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a hybrid method for model-based control that combines the best aspects of gradient-based methods and deep reinforcement learning (DRL). The main contribution of the paper is a modification of the deterministic policy gradients (DDPG) algorithm that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. Empirical results show that the proposed method boosts the performance of DDPG without sacrificing its robustness to local minima.
SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a novel approach to learn a task-agnostic world graph. The world graph is an abstraction that enables agents to focus exploration on a subspace of the environment. The nodes of a world graph are important waypoint states and edges represent feasible traversals between them. The authors propose two learning phases: 1) identifying world graph nodes and edges by training a binary recurrent variational autoencoder (VAE) on trajectory data and 2) a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. Experiments on a suite of maze tasks show that using world graphs significantly accelerates RL, achieving higher reward and faster learning."
SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering a function constructing network. The proposed method is based on discretization of function blocks to construct a target function to reveal its design. The network uses discretized layers, thus rendering the model interpretable without disordering the function blocks. Additionally, the authors introduce an end-to-end PathNet structure through this discretisation."
SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper studies the problem of self-supervised imitation learning in the multi-task setting, where suboptimal trajectories generated by a sub-optimal policy can still serve as optimal examples for other tasks. In this setting, the authors propose a simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. The proposed algorithm simply maximizes the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. Theoretical results show that the proposed algorithm can learn goal-reaching policies entirely from scratch."
SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper proposes Zeno++, a new asynchronous Stochastic Gradient Descent (SGD) algorithm that is robust to Byzantine failures of the workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. Experimental results show that the proposed algorithm outperforms existing approaches."
SP:d16ed9bd4193d99774840783347137e938955b87,"This paper proposes a method to generate semantic adversarial examples by manipulating image-based visual descriptors (color and texture) in order to generate effective and photorealistic examples. The proposed method is tested on image classification and image captioning tasks on ImageNet and MSCOCO. The method is shown to be effective against JPEG compression, feature squeezing and adversarially trained model. "
SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes Learning to Control (LTC) for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset. The key idea is to train a controller to execute an optimal sequence of read and write operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. LTC is trained with two degrees of memory plasticity: 1) the controller is trained to control the encoder and 2) the encoders are trained to predict the output of the controller. Experiments show that LTC achieves good performance on the Stanford task-o-riented dialogue dataset.
SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,This paper proposes a method to learn motor primitives for large-scale and diverse manipulation demonstrations. The main idea is to jointly learn both the underlying motor primitive and recomposition these primitives to form the original demonstration. The proposed method is based on a hierarchical reinforcement learning setup to efficiently solve robotic manipulation tasks like reaching and pushing. Experiments show that the learned primitives capture semantically meaningful aspects of a demonstration.
SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for learning a universal control policy in a single episode. The key idea is to use a probe and an inference model to rapidly estimate latent variables of test dynamics, which are then immediately used as input to a universal policy. The proposed method is based on variational inference. The authors show that the proposed method outperforms existing adaptive approaches and shows favorable performance against baselines for robust transfer. "
SP:f2f1aff9a5b91d748b24fee0155367f650401aab," in AlphaZero is a search-based reinforcement learning algorithm for two-player games Go, chess and Shogi. The paper proposes a three-head network architecture that can learn a third action-value head on a fixed dataset the same as for the two-head net. The proposed network is tested on the game of Hex. The results show that the proposed network outperforms AlphaZero in zero-style iterative learning."
SP:89d6d55107b6180109affe7522265c751640ad96,"This paper studies the problem of policy transfer in reinforcement learning. In particular, the authors propose a method called Adapt-to-Learn (ATL) to adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. They show through theory and experiments that their method leads to a significantly reduced sample complexity of transferring the policies between the tasks. "
SP:626021101836a635ad2d896bd66951aff31aa846,This paper studies the problem of scale-equivariance in convolutional neural networks. The main contribution of the paper is a generalization of the previous work on scale-invariant convolution with steerable filters. The proposed method is based on the idea of steerable convolution and generalizes other common convolution blocks to be scale equivariant. The authors show the computational efficiency and numerical stability of the proposed method on MNIST and STL-10 datasets.
SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper proposes a method for 3D scan completion, i.e., filling in regions that were missed in the raw scans. The proposed method is based on point clouds and does not require paired training data, and hence can directly be applied to real scans for scan completion. The method is evaluated qualitatively on several real-world datasets (ScanNet, Matterport3D, KITTI), quantitatively on 3D-EPN shape completion dataset, and demonstrate realistic completions under varying levels of incompleteness."
SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data for authenticating systems that rely on sensor data for authentication and anomaly detection. The authors formulate the scenario of such an authentication system facing generative impersonation attacks, characterize it from a theoretical perspective and explore its practical implications. They cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. The analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights, they design practical learning approaches that are more robust to various attacks on real-world data."
SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper studies the trade-off between robustness and standard accuracy in machine learning. The authors propose a new adversarial learning algorithm that trains a robust model with sensible adversarial examples, without a significant drop in natural accuracy. They show that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under the proposed approach. They also propose a novel and efficient algorithm to train the robust model. The proposed method is evaluated on CIFAR-10."
SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish the feature maps distributions of different networks. Discriminators and networks are trained concurrently in a minimax twoplayer game. Also, a novel cyclic learning scheme is proposed for training more than two networks together. Experiments on various network architectures on the classification task demonstrate the effectiveness of the proposed method."
SP:e43fc8747f823be6497224696adb92d45150b02d," word embedding models mainly focus on the rich semantic meanings while are challenged by capturing the sentiment information. For this reason, this paper proposes a novel sentiment word embeddings model. The parameters in the proposed model are determined by using both the maximum likelihood estimation and the Bayesian estimation. The proposed model significantly outperforms the baseline methods in sentiment analysis for low-frequency words and sentences and is also effective in conventional semantic and sentiment analysis tasks."
SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-step approach for training deep neural networks in the presence of noisy labels. The first step is to early stop the network before the noisy labels are memorized, and the second phase is to resume training the network using a “maximal safe set” that maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Experiments are conducted on four image datasets to demonstrate the effectiveness of the proposed approach."
SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes an approach to generalize to a new set of actions in a zero-shot setting where the agent is given a set of previously unseen actions. The proposed approach is based on unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. The authors propose a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. Experiments are conducted to demonstrate the generalizability of the representation learning method and policy."
SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes word2ket, a method for storing word embedding matrix during training and inference in a highly efficient way. The method is inspired by quantum computing. The proposed method is based on the idea that word embeddings can be represented as a sequence of continuous vectors. The authors show that the proposed method can achieve a 100-fold or more reduction in the space required to store the embedding vectors with almost no relative drop in accuracy in practical NLP tasks."
SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning that learns a policy from a set of demonstrations. The method is based on the idea that the state-action pairs are augmented with behavioral descriptions, and the policy is conditioned on a behavior description that can be precisely modulated. The proposed method is applied to the StarCraft II build-order planning task, where it is shown that the learned policy can be effectively manipulated to express distinct behaviors."
SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis of Frankle & Carbin (2018), which claims that small, sparsified neural networks can be trained as long as the network is initialized properly. The authors conduct an in-depth investigation of the structure of winning lottery tickets. They discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. They provide insights into the structure and structure of these early winning tickets with supporting evidence. "
SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes a novel method for detecting unknowns in deep neural networks. The key idea is to use a product operation to model the product relationship among the features produced by convolutional layers. This way, missing a single key feature of a target class will greatly reduce the probability of assigning an object to this class. To further improve the performance of the method, the authors propose an information-theoretic regularization strategy that incorporates the objective of rejecting unknowns into the learning process of UDN."
SP:fa3e729469e74cac44745008fe65c01cc97c9820,This paper proposes a method for variational inference (VI) for deep neural networks. The main idea is to learn a variational distribution that can be used to approximate the posterior over the model parameters. The method is based on the idea of learning a coarse approximation of the distribution and then iteratively refining it. The paper shows that the proposed method improves the lower bound on the approximation (the Evidence Lower BOund) and shows that it outperforms existing VI methods in terms of log-likelihood and ELBO.
SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,This paper proposes to use correlated Monte Carlo rollouts to control the variance of the policy gradient estimator for categorical sequence generation. The proposed method is based on the idea that the number of rollouts is random and adaptive to the model uncertainty. The authors propose to use the correlated MC rollouts for binary-tree softmax models to reduce the high generation cost in large vocabulary scenarios by decomposing each categorical action into a sequence of binary actions. They evaluate their methods on both neural program synthesis and image captioning. 
SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) problem with two main stages: (1) deceptive opponent modeling based on maximum entropy regularized Markov decision processes (MDPs) and (2) goal recognition under proactively static interdiction. The main idea is to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, the task of S-G RC is to interdict a set of actions that improve or reduce the wcd. The paper shows that the proposed approach can control the goal recognition process based on opponent’s deceptive behavior."
SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batch sizes for GANs. The idea is to use Coreset-selection to generate a large batch of samples from the prior and then compress that batch using Coreset selection. To create effectively large batches of ‘real’ images, the authors create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Core set-selection on those projected activations at training time. The experiments show that the proposed method substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows GAN to reach a new state of the art in anomaly detection."
SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper studies the effect of noise in recurrent neural networks (RNNs) on the ability of RNNs to extract predictive information for both maximum likelihood and contrastive loss training. In particular, the authors propose to use noise to constrain past information by injecting noise into the hidden state of the RNN to improve the performance of the network. The experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, show that the proposed method is sub-optimal in the information plane. "
SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate delusional bias in approximate Q-learning by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class, and propose a search framework that allows multiple Q-Approximator to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-Learning in a variety of Atari games, sometimes dramatically."
SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,", the paper proposes a generative latent variable model that combines the best of spatial-attention and scene-mixture approaches. The main contribution of the paper is to propose a unified probabilistic modeling framework for unsupervised object-oriented scene representation learning. The proposed model can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. The paper also resolves the scalability problems of previous methods by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations. "
SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes FALCON, an efficient and lightweight method for compressing convolutional neural networks (CNNs). The main idea is to use EHP to approximate the standard convolution kernel. The authors propose a generalized version of rank-k FAL CON which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. They also propose FAL-branch, which is a variant of FALcon that is based on Shuffle UnitV2. Experiments show that the proposed method outperforms existing methods by up to 8x compression and 8x computation reduction while ensuring similar accuracy."
SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper proposes four improvements to Batch Normalization (BN) to improve its performance across all batch sizes while requiring no additional computation during training. The authors propose a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy, recognizing and validating the powerful regularization effect of Ghost Batch normalization for small and medium batch sizes, examining the effect of weight decay regularization on the scaling and shifting parameters, and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. They validate their results empirically on six datasets."
SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper proposes a method for data inspection in federated learning, where the data is stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. The authors propose to use generative models trained using federated methods and with formal differential privacy guarantees to identify and fix problems in the data. The proposed method is tested on text with differentially private federated RNNs and to images using a novel algorithm for federated GANs."
SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the provided motion material and modifies it to become natural. On skeleton datasets with diverse motion, the proposed method outperforms existing parametric and nonparametric baselines."
SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,This paper studies the problem of hierarchical explanation of neural network predictions. The authors propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling & Occlusion (SOC) algorithm to quantify the importance of each word and phrase. They show that SCD and SOC outperform prior hierarchical explanation algorithms on both LSTM models and BERT Transformer models on multiple datasets. 
SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,This paper proposes a new method for learning saliency maps for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order-equivalence between the saliency map and the scale information of the network. The authors show that their method is 97x faster than Guided Backprop and much more accurate than guided backprop. They also show that the proposed method has a significantly smaller memory footprint and is more accurate.
SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42, for text generation. This paper proposes to use position as a latent variable in the text generation process. The proposed method is evaluated on machine translation and paraphrase generation tasks. The experimental results show that the proposed method outperforms several strong baselines on both tasks. 
SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a new generative model, called Random Path Generative Adversarial Network (RPGAN), which is based on the idea that the latent space of a GAN consists of random paths in a generator network. The authors show that this approach can be used to understand factors of variation, captured by different generator layers, providing their natural interpretability. Experiments on standard benchmarks demonstrate that RPGAN reveals several interesting insights about the roles that different layers play in the image generation process."
SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolutional neural network based on a graph representation of the sampled sphere. The authors study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation."
SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the trade-off between invariance and adaptability in learning representations for domain adaptation. In particular, the authors show that the search for invariance favors the compression of representations, and propose a new bound on the target risk that reveals a tradeoff between compression and invariance of learned representations. They also show that learning weighted representations plays a key role in relaxing the constraint of invariance, and then preserving the risk of compression."
SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for learning to infer user interface attributes (e.g., colors, border radius, shadow or text properties) from input images. The key idea is to use a black box rendering engine and a set of attributes to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, the paper proposes to use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. The paper demonstrates the effectiveness of the proposed method on the task of inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications."
SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, in which a network is trained on one task and re-purposed on another. In particular, the authors propose to use the robustness of the source network to improve the performance of the target network. The authors show that by training on top of the robust feature extractors, they can produce new models that inherit the robusts of their parent networks. They also propose to fine-tune a network by re-training end-to-end in the target domain. "
SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm to improve the communication speed of neural agents in a language game. The authors propose a probabilistic model of NIL and provide an explanation of why the advantage of compositional language exists. The experiments confirm that the emerged languages largely improve the generalizing power of the neural agent communication.
SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two Variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent (SGD) and is proven convergent under certain conditions."
SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a reward function for goal-conditioned reinforcement learning. The reward function is based on the indicator reward function, which is defined as a positive reward when the robot’s observation exactly matches a target goal observation. The paper shows that by relabeling the original goal with the achieved goal to obtain positive rewards, the reward function can be learned even in continuous state spaces. The authors propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering."
SP:8ae78a6640be13e511242eab64101f74ebc4b30a,This paper studies the problem of certified robustness verification for Transformers. The main contribution of the paper is to propose a new method to verify the robustness of the network. The proposed method is based on the idea that the network's self-attention layer is cross-nonlinear and cross-position dependent. The authors show that their method is more robust than previous methods. 
SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a novel weakly supervised pretraining objective to force the model to incorporate knowledge about real-world entities. The proposed objective is based on a zero-shot fact completion task. The paper shows that the proposed objective improves the performance of BERT on four entity-related question answering tasks (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset with 5.7 accuracy gains."
SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper tackles the problem of discovering novel classes in an image collection given labelled examples of other classes. The authors propose to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. They propose to use self-supervised learning to train the representation from scratch on the union of labelled and unlabelling data, and use rank statistics to transfer the model’s knowledge of the labelled classes to the task of clustering the unlabeled images. They train the data representation by optimizing a joint objective function on the labelled and unlabeled subsets of the data, improving both the supervised classification of labelled data and the clustering of unlabels. They evaluate their approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin."
SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a novel method for visual planning based on hallucinative topological memory (HTM) to learn to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. The proposed method is based on the semiparametric topology memory (SPTM) method, where image samples are treated as nodes in a graph, and the connectivity in the graph is learned using deep image classification. However, training SPTM necessitates a suitable loss function for the connectivity classifier, which requires non-trivial manual tuning. More importantly, SPTM is constricted in its ability to generalize to changes in the domain, as its graph is constructed from direct observations and thus requires collecting new samples for planning. In this paper, the authors propose to use a conditional VAE model to generate samples given a context image of the domain and use these hallucinated samples for building the connectivity graph, allowing for zero-shot generalization to domain changes. In simulated domains, the proposed method outperforms conventional SPTM and visual foresight methods in terms of both plan quality and success in long-horizon planning."
SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a method to reduce the spurious biases in existing benchmark datasets. The authors propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. They apply AFLITE to popular benchmarks that are practically solved — ImageNet and Natural Language Inference (SNLI, MNLI, QNLI) — and present filtered counterparts as new challenge datasets where the model performance drops considerably."
SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a novel method for training prototypical few-shot models for a single class. The proposed method is based on the idea of using a null class centered around zero, and enforcing centering with batch normalization. The authors show that the proposed method outperforms the state-of-the-art methods on Omniglot and MiniImageNet."
SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. The proposed method first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. This allows any existing embedding methods to be applied to the coarsening graph, before it progressively refine the embeddings obtained at the Coarsest level to increasingly finer graphs. The experiments show that the proposed method can substantially increase the classification accuracy and significantly accelerate the entire graph-embedding process by up to 40.8x."
SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relatively, by predicting new pixels relative to previously generated pixels (or pixels from the conditioning context, when available). The authors show that this form of prediction fare favorably to its absolute counterpart when used independently, but their coordination under an unified probabilistic model yields optimal performance. Experiments on multiple benchmarks for unconditional image generation, image colorization, and super-resolution indicate that their presented mechanism leads to improvements in terms of likelihood compared to the absolute prediction counterparts."
SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper studies the problem of estimating quantities defined by the stationary distribution of a Markov chain. In this setting, the authors propose to use variational divergence minimization to estimate a ratio that corrects for the discrepancy between the stationary and empirical distributions. The authors prove the consistency of their approach under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems such as PageRank and off-policy policy evaluation."
SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper proposes a method for training models to be less sensitive to spurious patterns in natural language processing. The authors propose to train a classifier on a set of documents and their initial labels, where each document is given a counterfactual target label and humans are tasked with revising the document so that it accords with the target label, retains internal coherence, and avoids unnecessary changes. They show that the classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. "
SP:b720eb5b6e44473a9392cc572af89270019d4c42,This paper studies the relationship between spatial frequency and orientation of CNN channels in deep convolutional neural networks (CNNs) and human visual perception. The authors show that CNN channels are sensitive to spatial frequencies that have lower contrast masking thresholds in human perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features. 
SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to explicitly model link type correlations. The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrated that GENN is superior to many baselines without consideration of link type correlation and achieved 13.77% and 5.01% PR-AUC improvement on the two datasets."
SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6, pre-training is used to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments are conducted on TIMIT phoneme classification and WSJ speech recognition.
SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning approach to improve the performance of collaborative filtering models based on ranking-based objective functions. In particular, the authors propose to train a critic network to approximate ranking based metrics, and then update the actor network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, the critic-based method amortizes the scoring process with a neural network and can directly provide the (approximate) ranking scores for new list. The authors demonstrate the effectiveness of their approach on three large-scale datasets."
SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a novel approach for multi-step TD-based off-policy reinforcement learning. The main idea is to use truncated Q-functions to represent the return for the first n steps of a target-policy rollout w.r.t. the full action-value, and shifted Q-function as the farsighted return after this truncated rollout. The authors prove that the combination of these short and long-term predictions is a representation of the full return, leading to the Composite Q-learning algorithm. They show on three simulated robot tasks that Composite TD3 outperforms TD3 as well as state-of-the-art multi-stage approaches in terms of data-efficiency."
SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a reinforcement learning system for real-world robotic manipulation tasks. The proposed system is based on the idea of continuous reinforcement learning (RL) where the goal is to learn a reward function that can be used to guide the learning process. The authors propose a number of simple and scalable solutions to these challenges, and then demonstrate the efficacy of their proposed system on a set of dexterous robotic manipulations tasks."
SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c," adversarial robustness is an important topic in machine learning. This paper studies the problem of adversarial generalization in the presence of unlabeled data. The main contribution of the paper is the analysis of the risk decomposition of the expected robust risk. In particular, the authors show that the stability part of the loss function is independent of the label information. The authors further show that for a specific Gaussian mixture problem, adversarially robust generalization can be almost as easy as the standard generalization if a sufficiently large amount of unlabelled data is provided."
SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,", the paper proposes a method to estimate the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. The proposed method is evaluated on MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments. The results show that the proposed method can improve the performance of the proposed methods."
SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG) for dynamic dual-precision quantization. PG computes most features in low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach significantly reduces the computational cost of DNN execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs, including static compressed networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4x less compute on ImageNet."
SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper studies the problem of unsupervised domain adaptation (UDA) where the classifiers are trained with clean labeled data from the source domain (SD) and unlabeled data from target domain (TD). The authors propose a new problem setting, called wildly UDA (WUDA), where classifiers have to be trained with noisy labelled data from SD and unlabelled data from TD. They show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, they propose a Butterfly framework, which maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled to-unlabeled, and SD to TD-distributional) and the other two can focus on classification in TD. Experiments demonstrate that the proposed method outperforms existing baseline methods."
SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,This paper studies the problem of image-based RL. The authors propose an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. They show that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image based RL. 
SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265," dropout is a simple but effective regularization technique for achieving better generalization of deep neural networks (DNNs). During training, dropout randomly discards a portion of the neurons to avoid overfitting. This paper presents an enhanced dropout technique, which they call multi-sample dropout, for both accelerating training and improving generalization over the original dropout. This technique can be easily implemented without implementing a new operator by duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experimental results show that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks using the ILSVRC 2012 (ImageNet), CIFAR-10, CifAR-100, and SVHN datasets."
SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43," to reduce the filter ambiguity in CNNs. This paper proposes a novel Label Sensitive Gate (LSG) structure to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization. Extensive experiments demonstrate the effectiveness of LSG in generating sparse and highly class-related representation of the input."
SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"This paper proposes a novel framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, where agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. Experiments on the StarCraft unit micromanagement benchmark show that the proposed method significantly outperforms baseline methods and allows us to cut off more than 80% of communication without sacrificing the performance."
SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a GAN-like algorithm to generate programming puzzles for computer programmers. The proposed algorithm is based on the idea of programming puzzles, which is a short program for a Boolean function f(x) with the goal of finding an input that makes f return True. The paper proposes an algorithm called Troublemaker to generate puzzles adaptively targeted at any given puzzle-solver. The algorithm generates a diverse set of puzzles that are difficult for the solver to solve."
SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a hierarchical reinforcement learning approach that decomposes a policy into a set of primitives and a meta-policy, where each primitive can decide for themselves whether they wish to act in the current state. The paper proposes to use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the state to make a decision and the primitive that requests the most information about the current states acts in the environment. Experiments show that the proposed approach improves over both flat and hierarchical policies in terms of generalization."
SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-based planning framework that learns a latent reward prediction model and then plans in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which is shown to be the only necessary information for successful planning. Experiments are conducted on multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of them produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations. The authors show that their method can successfully learn an accurate latent rewards prediction model in the presence of the irrelevant information while existing model based methods fail."
SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69," in this paper is to reduce the number of parameters in the BERT model. The main contribution is to use a self-supervised loss to model the inter-sentence coherence. The proposed method is evaluated on GLUE, RACE, and SQuAD tasks. "
SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,-based neural networks are a popular architecture for language understanding. This paper proposes an extension of the Transformer architecture to support multi-modality tasks. The proposed architecture is based on a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The authors show that the proposed architecture enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning.
SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,This paper proposes a new method for quantifying the predictive uncertainty of deep learning models. The proposed method is based on the use of higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence intervals. Experiments demonstrate that the proposed method outperforms existing Bayesian and non-Bayesian baselines.
SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,This paper proposes a GAN-based method for video synthesis and video prediction. The proposed method is based on GANs trained on the Kinetics-600 dataset. The main contribution of the paper is to propose a new GAN based on the Dual Video Discriminator GAN (DVD-GAN) that scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. The experimental results show that the proposed method can produce video samples of substantially higher complexity and fidelity than previous work. 
SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,". This paper studies the problem of few-shot learning where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. In doing so, the authors propose a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base class examples are few, the network cannot learn where to focus implicitly. The authors also show that a pre- trained network may be easily adapted to novel classes, without meta-learning."
SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"The paper proposes a method for generating structured objects that satisfy constraints on the structure of the object. The proposed method, called constrained adversarial networks (CANs), embeds the constraints into the model during training by penalizing the generator whenever it outputs invalid structures. The authors propose to use knowledge compilation techniques to efficiently evaluate the expected disagreement between the model and the constraints. This setup is further extended to hybrid logical-neural constraints for capturing complex requirements like graph reachability. Experiments on constrained images, molecules, and video game levels show that CANs efficiently generate valid structures that are both high-quality and novel."
SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which gives equal expressive power to both the positive and negative halves on the input space and is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power. Finally, they show that the use of the proposed activation functions can significantly increase the robustness of deep neural networks to adversarial attacks."
SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,This paper proposes to use a Dirichlet layer to enrich the output of a classification black-box with a measure of uncertainty to improve the trustability of the resulting system. The authors propose a probabilistic neural network that works in parallel to the black box classifier and uses a Dirchlet layer as the fusion layer with the blackbox classifier. They demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications in two real-world scenarios.
SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes to use blockwise adaptive stepsize in Adagrad to improve the generalization performance. The authors show that blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance between adaptivity and generalization. They show theoretically that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective as its counterpart with coordinate-wise adaptive step size. They also study its uniform stability and show that it can lead to lower generalization error."
SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new video dataset called CATER, which is a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. The dataset is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatio-temporal video architectures by being completely observable and controlled. "
SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes a new GAN model called Boundary-calibration GAN (BCGAN) to improve model compatibility in GANs. The main idea is to use the boundary information from a set of pre-trained classifiers using the original data. In particular, the authors propose an auxiliary loss called BC-loss to match the statistics between the posterior distributions of original data and generated data with respect to the boundaries of the pre-training classifiers. The authors show that BCGANs can achieve better model compatibility than the original GAN. "
SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the authors learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and Cifar-100 datasets demonstrate that the L2L outperforms existing adversarial methods in both classification accuracy and computational efficiency."
SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper studies the problem of Inverse Reinforcement Learning (IRL) on Markov Decision Processes (MDPs). In this setting, the goal is to estimate the state, action, and feature constraints in the environment that motivate an agent’s behavior. The authors propose an algorithm that iteratively infers the Maximum Likelihood Constraint to best explain observed behavior. They evaluate the proposed algorithm on simulated and real-world data. "
SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a deep recurrent neural network (DNN) architecture to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. The authors show that this network learns to prefer low-level edges over high-level object boundary contours. They also show that correcting this optical illusion significantly reduces the accuracy of the network. Overall, this paper suggests that the optical illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detections."
SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"-based object detection methods rely on the conditional random field (CRF) model. This paper proposes a context-aware CNN (or conCNN for short) that enforces the semantics context constraints in the CNN based object detector by leveraging the popular CRF model in CNN. The proposed conCNN can be seamlessly plugged into any existing region based object detection paradigm. ConCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads."
SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb, of adversarial perturbations in the BatchNorm layer. The authors propose a new normalization method called RobustNorm (RobustNorm) to improve the robustness to adversarial attacks. The proposed method is tested on a variety of datasets and architectures. 
SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization ability of deep neural networks trained by simple first-order methods in the presence of mislabeled training examples. The authors propose two simple and intuitive regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, the authors prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. The generalization analysis relies on the connection between wide neural network and neural tangent kernel (NTK) and is independent of the network size."
SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper studies the problem of improving the performance of convolutional neural networks (CNNs) when the number of channels in the network goes to infinity. In particular, the authors show that the width of the network corresponds to regression with respect to the CNN Gaussian Process kernel (CNN-GP) if only the last layer is trained, and correspond to regression if all layers are trained (CNTK) if the network is trained. The authors propose to use a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. They show that LAP is equivalent to full translation data augmentations for CNN-GP and CNTK and achieves 89% accuracy."
SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for combining model-free Q-based Q-learning with model-based Monte-Carlo Tree Search (MCTS). The proposed method is based on a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTs, resulting in a cooperative relationship between model free learning and model based search. Experiments are conducted on physical reasoning tasks and Atari."
SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or “Deep 3D Pan”, with “t-shaped” adaptive kernels equipped with globally and locally adaptive dilations. The proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with locally and globally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB STEREO indoors dataset to prove the efficacy of our method."
SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,This paper studies the Variational AutoEncoder (VAE) from an information theoretic perspective. The authors propose a variational lower bound for the capacity-constrained InfoMax (CCIM) of the variational autoencoder. They show that the optimal generative model is the one that maximizes the capacity of the InfoMax while maintaining bounded the network capacity. The paper also shows that the CCIM can be used to learn the maximal informative generative models while maintaining the capacity. 
SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes a method for embedding information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. The proposed method is based on the idea of pooling information from different neighborhoods of different sizes. The authors prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that their algorithms are robust, computationally efficient and outperform comparable models on social network datasets."
SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the phase transitions in the Information Bottleneck (IB) objective. The authors propose a new definition of IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phase transition, and draw its connection with the Fisher information matrix for parameterized models. Based on the theory, they present an algorithm for discovering phase transition points in the IB objective. "
SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method for using the independence property between current action and future states in environments to reduce the variance of the advantage estimation. The independence property can be naturally utilized to construct a novel importance sampling advantage estimator with close-to-zero variance even when the Monte-Carlo return signal yields a large variance. To further remove the risk of the high variance introduced by the new estimator, the authors propose a reward decomposition model learned by minimizing the estimation variance. The proposed method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,This paper proposes a bias-reduced augmentation of Liu et al. (2018a) for infinite horizon off-policy policy evaluation. The proposed method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. Both theoretical and empirical results show that the proposed method yields significant advantages over previous methods.
SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a novel Metric-Softmax loss for few-shot classification. The Metric Softmax loss is trained on the whole label set and learns more discriminative feature than episodic training. In the second stage, the authors propose a task-adaptive transformation which adapts the classifier to each few shot setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms existing state-of-the-art methods."
SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper presents a data-driven approach that learns to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data, then uses a neural network that infers a correction to move a coarse thus quickly obtainable result closer to the reference data. The paper provides insights into the targeted learning problem with different learning approaches: fully supervised learning methods with a naive and an optimized data acquisition as well as an unsupervised learning method with a differentiable Navier-Stokes solver. While the approach is very general and applicable to arbitrary partial differential equation models, the authors specifically highlight gains in accuracy for fluid flow simulations."
SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper studies the problem of online continual learning, where one attempts to learn to compress and store a representative dataset from a non-i.i.d data stream while only observing each sample once. To address this problem, the authors propose a new architecture which Stacks Quantization Modules (SQM) consisting of a series of discrete autoencoders, each equipped with their own memory. Every added module is trained to reconstruct the latent space of the previous module using fewer bits, allowing the learned representation to become more compact as training progresses. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, SQM does not require pretraining, even on challenging datasets. The authors show that SQM can replace the episodic memory used in Experience Replay with SQM, leading to significant gains on standard continual learning benchmarks using a fixed memory budget. They also apply their method to online compression of larger images like those from Imagenet, and show that it is also effective with other modalities such as LiDAR data."
SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes an extension of Bidirectional Representation Learning (BDL) for multi-label metric learning. The proposed method is based on deep neural networks that encode both input data and output labels, then obtains a metric space for testing data. The model scales linearly in the number of instances and trains a deep neural network that encodes both input and output data. Experiments show that the proposed approach is better than related methods based on systematic metric and its extendability."
SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the effect of delay on the learning rate of stochastic gradient descent (SGD) in the context of asynchronous training. The authors show that for high delay values, SGD should be kept inversely proportional to the delay, and that momentum should be either turned off or modified to improve the training stability. They provide empirical experiments to validate their theoretical findings."
SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes a method for representation learning in reinforcement learning. The key idea is to learn what features of the future trajectory provide useful information to predict the associated return. This provides tractable prediction targets that are directly relevant for a task and can thus accelerate learning of the value function. The idea can be understood as reasoning, in hindsight, about which aspects of future observations could help past value prediction. The paper shows how this can help dramatically even in simple policy evaluation settings. "
SP:6388fb91f2eaac02d9406672760a237f78735452,. This paper proposes a novel rewiring operation to improve the performance of graph neural networks (GNNs) in the presence of adversarial attacks. The authors propose to use reinforcement learning to learn the attack strategy. Experiments on real-world graphs demonstrate the effectiveness of the proposed framework. 
SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper studies the problem of estimating the prediction error of encoder-decoder convolutional neural networks (CNNs) for denoising problems. The authors propose to use a close form representation of the unbiased estimator for the prediction errors of the encoder and decoder networks. This is inspired by the finding that an encoderdecoder CNN can be expressed as a piecewise linear representation, and the authors propose a novel bootstrap and aggregation scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems."
SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning approach for few-shot classification where the number of instances per task and class is not fixed. The proposed approach is based on the idea of balancing the effect of meta-knowledge and task-specific learning within each task. The authors formulate this objective into a Bayesian inference framework and tackle it using variational inference. They validate their Bayesian task-adaptive Meta-Learning (Bayesian TAML) on multiple realistic task- and class-imbalanced datasets.
SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for learning to imitate expert behavior from demonstrations. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. The proposed method, called soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretical analysis shows that SQIL can be interpreted as a regularized variant of behavioral cloning (BC) that uses a sparsity prior to encourage long-horizon imitation. Empirically, SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box."
SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions and encourages mingling, i.e., to prevent the aforementioned halos. They combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. They show that their method works for large, deforming point sets from different sources to demonstrate the flexibility of their approach."
SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer for aligning multiple datasets. The authors propose to learn the optimal transport cost function using a small amount of side information which is often available. The side information captures subset correspondence, i.e. certain subsets of points in the two data sets are known to be related. The proposed method outperforms state-of-the-art approaches on several datasets."
SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,". This paper proposes a novel method to detect anomalies in large datasets under a fully unsupervised setting. The key idea is to learn the representation underlying normal data. To this end, the authors leverage the latest clustering technique suitable for handling high dimensional data. This hypothesis provides a reliable starting point for normal data selection. The authors train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset based on clustering and representation learning. Experimental results on several public benchmark datasets show that the proposed method outperforms state-of-the-art unsupervision techniques."
SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper proposes Projection-Based Constrained Policy Optimization (PCPO), an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. The authors theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. "
SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper studies the behavior of word embeddings in terms of the low-rank transformation between word-context co-occurrence space and embedding space. Specifically, the authors show that the embedding can be viewed as a low rank transformation from the word context to the word-embedding space, which preserves the relative distances among words. The authors also provide a theoretical explanation for this behavior and derive a method to automatically find its optimal value. The experiments on real datasets verify the analysis."
SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a novel approach to the problem of similarity measurement. The proposed approach is based on a group of approximate Random Projection Trees (RP Trees) that are used to measure the similarity between two data points. The authors propose to use a combination of two existing methods to improve the accuracy and efficiency of the proposed approach. The main contribution of the paper is the introduction of the idea of randomness into the partitioning of the trees to reduce the reliance on prior knowledge. The experiments on three real-world datasets demonstrate the effectiveness of the approach.
SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method to reduce the simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. The authors show that their method produces promising results on popular benchmarks when compared to MCMC and VI."
SP:64f2744e938bd62cd47c1066dc404a42134953da, of missing data greatly complicate causal inference procedures as they require an adapted unconfoundedness hypothesis which can be difficult to justify in practice. This paper proposes to use latent confounders whose distribution is learned through variational autoencoders adapted to missing values. They can be used either as a pre-processing step prior to causal inference but also as a multiple imputation strategy to take into account the variability due to missing data. Numerical experiments demonstrate the effectiveness of the proposed methodology especially for non-linear models.
SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning (RL) policies by combining ENAS (Vinyals et al., 2015; Pham et al. 2018; Zoph & Le, 2017) and ES in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the paper proposes to represent compact architectures via efficient learned edge partitionings. For several RL tasks, the proposed method is able to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices (Choromanski et al, 2018)."
SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper studies the problem of representation learning for time-series by considering a Group Transform approach. This framework allows us to generalize classical time-frequency transformations such as the Wavelet Transform, and second, to enable the learnability of the representation. This is achieved by sampling a subset of invertible maps on R. The subset considered contains strictly increasing and continuous functions. The transformations induced by such maps enable us to span a larger class of signal representations, from wavelet to chirplet-like filters. The authors propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific loss and signal. "
SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to non-Euclidean spaces, e.g. hyperbolic or spherical spaces. The authors propose a unified formalism that can interpolate smoothly between all geometries of constant curvature, leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Empirically, the authors show that the proposed method outperforms the state-of-the-art in node classification and distortion minimization for symbolic data."
