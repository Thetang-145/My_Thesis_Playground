paper_id,summary
SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper proposes sqSGD (selective quantized stochastic gradient descent), a gradient-based learning algorithm for federated learning under local differential privacy constraints. The proposed algorithm is based on a novel privacy-preserving quantization scheme that uses a constant number of bits per dimension per client. It improves the base algorithm in two ways: first, it applies a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, it utilizes randomized rotation as a preprocessing step to reduce quantization error. Experiments on benchmark datasets show the practicality of the proposed framework."
SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a simple and general representation method to consider prior knowledge related to language representation from the beginning of training. The proposed method allows SANs to leverage prior knowledge in a universal way compatible with neural networks. The authors apply it to one prior word frequency knowledge for monolingual data and other prior translation lexicon knowledge for the bilingual data, thereby enhancing the language representation. Experimental results on WMT14 English-to-German and WMT17 Chinese-toEnglish translation tasks demonstrate the effectiveness and universality of the proposed method over a strong Transformer-based baseline."
SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"This paper proposes a Bayesian Stackelberg Markov game-theoretic model (BSMG) to model uncertainty over attacker types and the nuances of a moving target defense (MTD) system. BSMGs can be used to model various MTD scenarios, capturing the uncertainty over the attacker type and the sequential impacts of attacks and switching defenses, and characterize the notion of Strong Stacekelberg Equilibrium (SSE) in MTD systems. The authors also propose a BSM-Q-learning approach that can, via interaction, learn the optimal movement policy for BSMG within a reasonable time. Experiments show that the proposed approach improves the state-of-the-art for MTD."
SP:97911e02bf06b34d022e7548beb5169a1d825903,This paper proposes a VAE ensemble framework to improve the disentangled representation learning. The proposed ensemble consists of multiple VAEs. The latent variables in every pair of these models are connected through a linear transformation. Theoretical analysis shows that the linear transformations connect the individual models in the ensemble tend to converge to trivial transformations. Experiments show that the proposed method outperforms the state-of-the-art unsupervised disen18 tangled representation learning approaches.
SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The proposed method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The authors train a graph neural network (GNN) in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. The GNN generalizes to new datasets and new sets of datasets.
SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural networks. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. This is caused by gradient descent, trying to use all available and redundant information from input, violating the conditional independence property of compositionality. Based on this finding, the authors suggest that the existing compositionality-learning approaches considering only model architecture design are unlikely to achieve complete compositionality, suggesting that only compositionality approaches with only manual or search-based approaches are likely to fail."
SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper provides a theoretical analysis of the representation discrepancy between two potentially-aligned entities in the embedding-based entity alignment (EEA) methods. The authors show that the representation discrepancies of an underlying aligned entity pair is bounded in an indirect way by a margin $\lambda$ in the scoring function. Unfortunately, this margin-based bound cannot be set as tight as expected, causing that little constrain can be put on the entities with few neighbors. To mitigate this problem, the authors propose a new approach that explicitly learns KG-invariant and principled entity representations, while preserving the original infrastructure of existing methods."
SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design framework to develop more energy-efficient CNN-powered PhlatCam. In particular, the mask coded in the Phlatcam sensor and the backend CNN model are jointly optimized in terms of both model parameters and architectures via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed SACoD framework achieves aggressive model compression and energy savings while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art designs with six datasets on four different tasks."
SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a method for learning semantic embeddings of individuals in a population of bees. The authors use a dataset of trajectories of individuals over multiple generations in two honey bee colonies. They propose a new temporal matrix factorization model that jointly learns the average developmental path and structured variations in the social network over their entire lives. The method yields interpretable embedding that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived."
SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation pipeline for image reconstruction tasks arising in medical imaging and explores its effectiveness at reducing the required training data in a variety of settings. In particular, the authors focus on accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. The proposed pipeline is designed to utilize the invariances present in the medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. The authors demonstrate the effectiveness of the proposed pipeline by showing that for some problem regimes, DA can achieve comparable performance to the state-of-the-art on the fastMRI dataset while using significantly fewer training data."
SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a clustering algorithm for order learning. The proposed algorithm is based on order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. Then, it group object instances into clusters according to their identity features using a repulsive term. Moreover, it estimates the rank of a test instance by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical image classification show that the proposed algorithm can cluster ordered data effectively and yield excellent rank estimation performance."
SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behavior. The authors demonstrate the method on several procedurally generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that the proposed method significantly outperforms the state-of-the-art intrinsic reward strategies."
SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes Isometric Propagation Network (IPN) for zero-shot learning (ZSL), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within the semantic space and the visual space, and regularizes the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. To get more diverse graph motifs, the authors also apply episodic training which samples different subset of classes as the nodes for every training episode."
SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a Transformer-based multi-task learning method for natural language understanding tasks. The proposed method is based on task-conditioned hyper networks for controlling its feed-forward layers. Specifically, the authors propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. In order to construct the proposed hypernetwork, the method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state."
SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper proposes a method to improve the robustness and generalization of neural network training for self-driving cars. The proposed method builds a dataset of adversarially degraded images by apply evolutionary optimization within the space of possible degredations during training. The method begins by training on a combination of real and simulated/degraded data using arbitrary degradation parameters. On each training iteration, the parameters are updated to generate a new degradation combination so that system performance is minimized. Experiments show that the proposed method improves the task performance of “learning to steer” up to 48% over strong baselines."
SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes a method for solving constrained optimization problems with hard constraints. The authors propose a method called Deep Constraint Completion and Correction (DC3) that enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. They demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world AC optimal power flow, where hard constraints encode the physics of the electrical grid."
SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper proposes a growing L2 regularization scheme for neural network pruning. The authors show that the growing regularization can be applied to two problems: pruning schedule and weight importance scoring. The growing penalty scheme also brings an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks."
SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper systematically studies the role of planning and its algorithmic design choices in MuZero, a state-of-the-art model-based RL algorithm. The authors perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. The results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization."
SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes a method for implicit planning in reinforcement learning (RL) that combines ideas from contrastive self-supervised learning, graph representation learning, neural algorithmic reasoning, and neural message passing. The proposed method, called XLVIN, is a combination of VIN and Generalized Value Iteration Networks (GVIN) with a message passing module. The authors show that the proposed method outperforms VINs and GVINs when the underlying MDP is discrete, fixed and known, and provide significant improvements to model-free baselines across three general MDP setups. "
SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the inductive bias of learning read-once DNFs under the uniform distribution. The authors show that the population risk can be minimized by multiple networks: from ones that memorize data to ones that compactly represent the DNF. They then set out to understand why gradient descent “chooses” the compact representation. They use a computer assisted proof to prove the inductivity of the learning process, and use it to design a process for reconstructing the learned network."
SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for goal-oriented reinforcement learning (GRL) that leverages graph structure in historical trajectories to slowly adjust exploration directions and rapidly update value function estimation with related experiences. The proposed method constructs a dynamic graph on top of state transitions and develops an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. Experimental results show that the proposed method outperforms the state-of-the-art algorithms in terms of sample efficiency on sparse reward functions."
SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for sampling training levels from procedurally generated (PCG) environments for evaluating systematic generalization of reinforcement learning agents. The authors claim that different levels provide different learning progress for an agent at specific times during training. They introduce Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent’s policy. They find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning progress when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it."
SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a method for multi-task learning by decomposing auxiliary task gradients into directions that help, damage or leave the primary task loss unchanged. The authors propose to decompose the gradients of the auxiliary tasks according to three directions: positive, neutral, and negative directions. This allows weighting the update directions differently depending on their impact on the problem of interest. The method leverages efficient automatic differentiation procedures and randomized singular value decomposition for scalability. The proposed method is generic and encompasses some prior work as particular cases. "
SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper presents a method for one-shot word-object binding in a 3D environment, where the goal is to perform fast-mapping, i.e. the ability to bind a new word to an unfamiliar object after a single exposure. The proposed method is based on a combination of conventional RL and predictive (semi-supervised) learning, with a novel dual-coding external memory. The main contributions of the paper are: (1) the method is able to generalize to novel exemplars within the same ShapeNet category, (2) it generalizes to unfamiliar numbers of objects, and (3) it can be used as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later."
SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the effect of class-imbalance in few-shot learning (FSL) on the performance of several state-of-the-art FSL methods. The authors consider three scenarios: dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. They show that the performances of the balanced tasks always drop, by up to 18.0% for optimization-based methods. They also show that strategies used to mitigate imbalance in supervised learning can be adapted to the FSL case resulting in better performances."
SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a new convolution layer for graph convolutional neural networks (GCNs). The proposed method is based on the idea of Polynomial Graph Convolution (PGC) layer, which independently considers neighbouring nodes at different topological distances (i.e. arbitrarily large receptive fields). The PGC layer can represent a richer set of functions compared to the linear stacking of two or more GC layers, i.e., it is more expressive. Experiments are conducted to show the effectiveness of the proposed method."
SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a scalable technique for sim-to-real transfer for scene graph generation. It decomposes the domain gap by decomposing it into appearance, label and prediction discrepancies between the two domains, and introduces pseudo statistic based self-learning and adversarial techniques to handle these discrepancies. The experiments demonstrate significant improvements over baselines in reducing the domain gaps both qualitatively and quantitatively."
SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper introduces a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), for continuous-action reinforcement learning (RL). The authors show that REDQ can achieve the performance of Model-Based Policy Optimization (MBPO) on the OpenAI MuJoCo benchmark with fewer parameters than MBPO and with less wall-clock run time. The key ingredients of REDQ are 1) a high Update-To-Data (UTD) ratio of 1; 2) an ensemble of Q functions; and 3) in-target minimization across a random subset of Q function from the ensemble. The authors also provide a theoretical analysis providing insights into the algorithm."
SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a distribution-based invariant deep architecture (DIDA), which extends the work of (Maron et al., 2020) by extending the topology of point clouds as it handles (discrete or continuous) probability distributions instead of continuous distributions. The proposed architecture inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. The paper also empirically and comparatively demonstrate the merits of the approach on two tasks defined at the dataset level. On both tasks, DIDA learns meta-features supporting the characterization of a labeled dataset."
SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a spectral graph densification approach (GRASPEL) for learning ultra-sparse graphs from data by leveraging the latest spectral results in graph theory. By limiting the precision matrix to be a graphLaplacian-like matrix in graphical Lasso, the approach aims to learn ultra-Sparse undirected graphs from potentially high-dimensional input data. Compared with prior state-of-the-art graph learning approaches, the proposed method is more scalable and allows substantially improving computing efficiency and solution quality of a variety of data mining and machine learning applications, such as manifold learning, spectral clustering, and dimensionality reduction."
SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a method to learn a goal-conditioned policy in an unsupervised manner. The main idea is to learn an extra abstract-level policy conditioned on latent variables, which is trained to generate diverse abstract skills, and a discriminator which is used as an intrinsic reward function for the goal conditioned policy to imitate the trajectory induced by the abstract level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed GPIM method. "
SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"This paper studies the problem of policy switching in deep reinforcement learning (DQN) with low switching cost, i.e., a small number of policy switches during training. Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc., where the deployed policy that actually interacts with the environment cannot change frequently. The paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the deployed Q-network and the underlying learning Q-Network. Through extensive experiments on a medical treatment environment and a collection of the Atari games, the authors find that the feature-switching criterion substantially decreases the switching cost while maintaining a similar performance to the case without the switching-cost constraint. They also complement this empirical finding with a theoretical justification from a representation learning perspective."
SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a first-order stochastic algorithm based on a combination of homotopy methods and SGD, called Homotopy-Stochastic Gradient Descent (H-SGD), which finds interesting connections with some proposed heuristics in the literature, e.g. optimization by Gaussian continuation, training by diffusion, mollifying networks. Under some mild assumptions on the problem structure, the authors conduct a theoretical analysis of the proposed algorithm. Theoretical analysis shows that, with a specifically designed scheme for the homotopic parameter, H- SGD enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations confirm the theoretical results."
SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning method for the task of shape completion, i.e., estimating the 3D geometry of an object from sparse observations. The proposed method is based on the Bayesian approach of (Maeda et al., 2020) whose posterior estimate behaves asymptotically well with respect to the size of contextual dataset, and combine their method with Implicit Geometrical Regularization (IGR) (Gropp et al. 2020). The paper also proposes a simple MLP-based encoder to learn task-specific parameters, while other methods like PCN use hierarchical models to capture both global and local geometric properties of objects. The method is evaluated on ShapeNet and ICL-NUIM datasets."
SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper proposes a channel-wise activation suppression (CAS) strategy to improve the adversarial robustness of deep neural networks (DNNs) against adversarial perturbations. The proposed method is motivated by the observation that adversarial examples have higher activation magnitudes than natural examples, and that channels are activated more uniformly by adversarial images than natural images. The authors propose a simple but generic training strategy for robustifying the intermediate layer activation of DNNs. The effectiveness of the proposed approach is demonstrated on CIFAR-10 and ImageNet."
SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper provides a non-asymptotic analysis of the gradient flow dynamics of overparametrized single-hidden layer linear networks, which provides disentangled conditions on initialization that lead to acceleration and generalization. Specifically, they show that imbalanced initialization ensures acceleration, while orthogonal initialization ensures that trajectories remain close to the generalization manifold. They also derive a novel upper-bound on the operator norm distance between the trained network and the min-norm solution."
SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of deep ReLU networks. The authors show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their “shallow” two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures."
SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper focuses on the overestimation issues on continuous control through deep reinforcement learning (DRL) and proposes a novel algorithm that can minimize overestimation, avoid the underestimation bias and retain the policy improvement during the whole training process. Specifically, the authors add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which the authors provide a novel method to provide theoretical and experimental guarantee for future policy improvement. The authors evaluate their method on a set of classical control tasks, and show that the proposed algorithms are more computationally efficient and stable than several existing algorithms for continuous control."
SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes a generalization of the inverse reinforcement learning (IRL) problem to a stochastic expectation optimization problem, where the goal is to recover the probability distribution over reward functions from expert demonstrations. The authors propose to use the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the reward distribution as the first solution to the SIRL problem. The solution is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem."
SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervisory learning. The authors assume a simple but realistic “expansion” assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. They also assume that neighborhoods of examples in different classes have minimal overlap. They prove that under these assumptions, the minimizers of population objectives based on self training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels."
SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a hierarchical model for long-term video prediction. The proposed method predicts future frames by first estimating a sequence of semantic structures and then translating the structures to pixels by videoto-video translation. The authors evaluate the method on three challenging datasets involving car driving and human dancing and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (i.e., thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches."
SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new framework for graph neural networks (GNNs) that can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. The experimental results show that the performance of our method is obviously better than the related methods."
SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a geometric graph generative adversarial network (GG-GAN) which is a Wasserstein GAN that is permutation equivariant and easily scales to generate graphs of tens of thousands of nodes. The authors claim that the proposed method is capable of modeling complex relations, isomorphism consistency, isomorphic graphs consistently, and fully exploiting the latent distribution. The proposed method also strikes a good trade-off between novelty and modeling the distribution statistics, being competitive or surpassing the state-of-the-art methods."
SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a novel method for exploring how neurons within a neural network interact. The authors consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they formalize the problem in terms of the Minimum Description Length principle, by which they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world: they identify shared, resp. class-specific traits, compositionality within the network, and locality in convolutional layers."
SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of fixed-dataset policy optimization (FDPO), in which a dataset of transitions from an environment is used to find a policy with high return. The main contribution is a theoretical justification of the pessimism principle in FDPO, based on a bound that characterizes the suboptimality incurred by an FTPO algorithm. It further shows how this bound may be used to derive principled algorithms. The theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments."
SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a method to train neural ordinary differential equations (NODE) models with continuous depth. The proposed method is based on the asynchronous leapfrog (ALF) solver, which has a constant memory cost w.r.t number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory. Extensive experiments are conducted to validate the effectiveness of the proposed method."
SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provides an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) to generate images from conditionings with unseen object combinations. The authors observe that recent methods are able to generate recognizable scenes given seen conditions, and exploit compositionality to generalise to unseen conditions. However, all methods suffer from noticeable image quality degradation when asked to generate image from conditions composed of unseen objects. Moreover, through the analysis, the authors identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionsings, using semantically aware losses such as scene-graph perceptual similarity helps improve some dimensions of the generation process, and enhancing the quality of generated masks and individual objects."
SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper studies the expressive power of Graph Augmented Multi-Layer Perceptrons (GA-MLPs) and Graph Neural Networks (GNNs) from the perspective of graph isomorphism testing and community detection. The authors show that GA-MLP can distinguish almost all non-isomorphic graphs, just like the WeisfeilerLehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, they prove a separation in expressive power between GNN and GA- MLPs that grows exponentially in depth. They also demonstrate via community detection experiments that GAMLPs can be limited by their choice of operator family, whereas GNNS have higher flexibility in learning."
SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes a method to improve the sample efficiency of transformer-based RL agents in the context of distributed reinforcement learning. Specifically, the authors propose a method called Actor-Learner Distillation (ALD) that transfers the learning progress from a large capacity learner model to a small capacity actor model. The main idea is to use a continual form of policy distillation (PD) to transfer the progress of the large-capacity learner to the small-capacity actor. The authors show that the proposed method recovers the sample-efficiency gains of the transformer model while maintaining the fast inference and reduced total training time of the LSTM actor."
SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for multi-domain few-shot image classification. The URT layer meta-learns to leverage universal features by dynamically re-weighting and composing the most appropriate domain-specific representations. The authors show that URT sets a new state-of-the-art result on Meta-Dataset and achieves top-performance on the highest number of data sources compared to competing methods.
SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes an algorithm for the Unsupervised Progressive Learning (UPL) problem, where the number of classes is gradually increasing over time and the goal is to learn salient representations of the unlabeled input stream so that the agent can, at any point in time t, classify a given set of test data based on the set of classes it knows about so far. To solve the UPL problem, the authors propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples."
SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between centralized and decentralized deep learning training. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between decentralized and centralized training. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They also empirically investigate what is the desirable level of consensus distance during different phases of training, in order to ensure high generalization performance."
SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,"This paper proposes a method for sequence similarity learning based on dynamical system theory. The proposed method is based on the siamese recurrent neural network (RNN) architecture, which comprises two identical sub-networks, two identical dynamical systems, which can theoretically achieve complete synchronization if a coupling is introduced between them. The coupling is implemented through a new gate inside the Siamese GRU architecture which allows it to bring closer the embedding of some similar input pairs, especially positive samples. The experiments show that introducing such a coupling improves the performance of the proposed method on an activity recognition dataset."
SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of distributed codistillation (codistillation) on the performance of distributed training of neural network models. The authors propose a distillation-like loss that penalizes the predictions made by one model on a batch of training samples from other models on the same batch. They show that even at moderate batch sizes, models trained with codistilliation can perform as well as those trained with synchronous data-parallel methods, despite using a much weaker synchronization mechanism. The findings hold across a range of batch sizes and learning rate schedules."
SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The authors claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters, the SGD iterates will converge to a heavy-tailed stationary distribution. They rigorously prove this claim in the setting of quadratic optimization. They show that even in a simple linear regression problem with independent and identically distributed Gaussian data, the iterates can be heavy-tail with infinite variance. They further characterize the behavior of the tails with respect to algorithm parameters and the dimension. They finally support their theory with experiments conducted on both synthetic data and fully connected neural networks."
SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the impact of bandpass filtering on the performance of standard community detection models for spectral manipulations. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection. In particular, it is possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies."
SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper studies the problem of semi-supervised graph representation learning, where the graph structure is not available. The authors propose to learn the adjacency matrix and GNN parameters with self-supervision (SLAPS), which is a method that provides more supervision for inferring a task-specific latent structure and then applies a GNN to the inferred graph. The paper identifies a supervision starvation problem that emerges for graph structure learning, especially when training data is scarce. They propose a solution to this problem by adopting a multi-task learning framework in which they supplement the classification task with a self-Supervised task. The proposed method can be combined with several existing latent graph learning approaches and achieves state-of-the-art performance on various benchmarks."
SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a method for learning in an incremental learning setting, where the exposure identities are unknown. The proposed method is based on the idea of novelty detection, which measures the percentage of accuracy that was maintained by performing a model update using the incoming exposure. The novelty detection relies on the network confusion caused by training incoming data as a new class. The authors show that incorporating a class-imbalance during this detection method substantially enhances performance."
SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a framework for learning to interpret natural language constraints for safe reinforcement learning. The authors first introduce a new multi-task benchmark, called HazardWorld, that requires an agent to optimize reward while not violating constraints specified in free-form text. They then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Their model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. They show that their method achieves higher rewards (up to 11x) and fewer constraint violations compared to existing approaches."
SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper tackles the problem of few-shot semantic edge detection, which aims to localize boundaries of novel categories using only a few labeled samples. The proposed method is based on a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching."
SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method, Causal Screening, for generating causal explanations for graph neural networks (GNNs). The proposed method incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, it can be used to generate faithful and concise explanations for any GNN model. Experiments on three graph classification datasets show that the proposed method achieves significant improvements over state-of-the-art approaches w.r.t. predictive accuracy, contrastivity, and sanity checks."
SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure of model simplicity, i.e., the smallest fraction of the network’s parameters that can be kept while pruning without adversely affecting its training loss. The authors show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10. They also study the mutual information between the predictions of their new measure and strong existing measures based on models’ margin, flatness of minima and optimization speed."
SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,"This paper proposes a method for long-horizon planning from video observations. The method learns a sequence of abstract states for a low-level model-predictive controller to follow, which are then used as waypoints in an unsupervised fashion via a mutual information maximization objective. In the experiments, the authors show that the proposed method is able to solve unseen long-term tasks and discover independent representations per object and binary properties such as key-and-door."
SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit level sparsity. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer to induce all-zero bits across a group of weight elements and realize the dynamic precision reduction. The method enables the exploration of the full mixed precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on the CIFAR-10 and ImageNet datasets."
SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of binary neural networks (BNNs) against gradient-based adversarial attacks. The authors claim that BNNs suffer from gradient vanishing issues and show a fake sense of robustness. To address this issue, the authors propose a simple temperature scaling approach to mitigate this issue while preserving the decision boundary. Experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate the effectiveness of the proposed method."
SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel interpretable recurrent neural network (RNN) model, called ProtoryNet, in which it introduces a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, the proposed method makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each of the sentences to the prototypes. The proposed method enables intuitive, fine-grained interpretation of how the model reached to the final prediction, resembling the process of how humans analyze paragraphs. Experiments conducted on multiple public data sets reveal that the proposed model not only is more interpretable but also is more accurate than the current state-of-the-art prototype-based method."
SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper proposes a hidden Markov recurrent neural network (HMRNN) that is a special case of recurrent neural networks (RNNs). The authors prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. The authors also show that the parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. Finally, the authors demonstrate that the proposed method can be combined with other neural networks to improve parameter estimation, using an Alzheimer's disease dataset."
SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method for analyzing single-cell RNA-seq data in the context of neuronal phenotypes. The authors propose a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. They further leverage a sparsity-based regularization algorithm, which selects a few genes important to a specific phenotyping feature or feature combination. They applied this approach to a dataset of Drosophila T4/T5 neurons. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper proposes a new approach for interpretability of saliency maps, i.e. the saliency map is treated as a random variable and aims to calculate the posterior distribution over it. The likelihood function is designed to measure the distance between the classifier’s predictive probability of an image and that of a perturbed image. For the prior distribution, the authors make attributions of adjacent pixels have a positive correlation, and use a variational approximation, and show that the approximate posterior is effective in explaining the classifiers’ behavior. It also has benefits of providing uncertainty over the explanation, giving auxiliary information to experts."
SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a neural debiasing method for pretrained sentence encoders, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoder, while continuously showing desirable performance on downstream tasks."
SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a method to improve the robustness of neural networks to l2 perturbations by adding different levels of Gaussian noise to each test sample. Specifically, the authors propose a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. The method is evaluated on CIFAR-10 and MNIST datasets and the experimental results demonstrate that the proposed method can achieve better accuracy-robustness trade-off in the transductive setting."
SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a variational auto-encoder (VAE) method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, the proposed method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The authors derive a reduced entropy condition approximate inference method that results in rich posteriors. The proposed method is evaluated on a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising."
SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a multi-hop self-attention mechanism for graph neural networks (GNNs). The proposed method, called MAGNA, uses a diffusion prior on attention values, to account for all paths between the pair of disconnected nodes. This helps MAGNA capture large-scale structural information in every layer, and learn more informative attention. Experimental results on node classification and knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results."
SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new multi-task test to evaluate the performance of text models. The proposed test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. The authors find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, the best models still need substantial improvements before they can reach expert-level accuracy."
SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"This paper proposes a pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). They also include masked language modeling (MLM) on several table-and-language datasets to regularize the pretraining process. When incorporated with strong base semantic parsers, GRAPPA achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic Parsing tasks."
SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper studies the performance of the least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance is shown to converge, as n, p→\infty, to a deterministic limit involving simple (small-dimensional) statistics of the data. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that the proposed method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi task and transfer learning techniques."
SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper introduces a new family of conditional neural processes (CNPs) that are permutation-invariant and group-equivariant. The authors show that the encoder and decoder of the CNPs can be decomposed into a permutation invariant part and a group equivariant part. The encoder is a convolutional layer that maps the context points into a latent representation, and the decoder is an encoder-decoder network that predicts the labels of the target points given the latent representation. The paper provides a decomposition theorem for the permutation and transformation equivariance, which leads to the construction of an infinite-dimensional latent space to handle group symmetries. Experiments are conducted on a regression task and an image completion task."
SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes an approach for offline text generation based on off-policy learning from expert demonstrations. The main idea is to use importance weighting to upweight confident tokens and downweight unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. The proposed approach is evaluated on summarization, question generation, and machine translation tasks."
SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The authors show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, the authors propose an InfoPro loss which is difficult to compute in its original form, and derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to end-to-end training."
SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a method to improve the expressive power of graph neural networks (GNNs). The authors propose a novel framework, namely diverse sampling, to enhance the diversity of sub-graphs for each node. For a target node, diverse sampling offers it diverse neighborhoods, i.e., rooted subgraphs, and representation of target node is obtained via aggregating the representation of diverse neighborhoods obtained using any GNN model. Experiments are conducted at multi-class node classification tasks on three benchmark datasets and multi-label node classification task on a dataset collected in this paper."
SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the robustness of video machine learning models to bit-level network and file corruptions, which can arise from network transmission failures or hardware errors. The authors explore two types of defenses against such corruptions: corruption-agnostic and corruption-aware defenses. They find that corruption-neutral defenses such as adversarial training have limited effectiveness, and propose a corruption aware baseline that exploits knowledge of bit- level corruptions to enforce model invariance. The proposed method, called Bit-corruption Augmented Training (BAT), is evaluated on two action recognition and one multi-object tracking tasks."
SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a representation learning method for time-varying graphs. The proposed method is based on the skip-gram embedding approach, which can be used to perform implicit tensor factorization on different tensor representations. The paper shows that the proposed method can disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. The method is evaluated on time-resolved face-to-face data and using such representations as predictors for two different tasks: network reconstruction and predicting the outcomes of a process over time."
SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper studies the question of whether self-supervised language modeling applied to mathematical formulas enables logical reasoning. To measure the logical reasoning abilities of language models, the authors formulate several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions, and completing equalities, for training language models for formal mathematics. They also analyze the models’ ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs. They find that models trained on the skip-tree task show surprisingly strong mathematical reasoning abilities, and outperform models training on standard skip-sequence tasks."
SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper introduces a new type of gradient masking called imbalanced gradients, where the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. The authors formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. The MD attacks are evaluated on 12 state-of-the-art defense models and find that the MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23%. For 6 out of the 12 defenses, the attack can reduce the PGD accuracy by at least 9%."
SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes an incremental graph-to-sequence neural network system for proving semantic equivalence between two programs represented as trees of linear algebra expressions. The main idea is to learn a set of production rules (i.e., axioms of equivalence) for proving the equivalence of two programs, and then train a neural network to predict the output of these rules. The paper provides a complete implementation of the proposed system on a rich multi-type symbolic language for linear algebra, and shows that it achieves 93% average true positive coverage on 10,000 test cases."
SP:19e32803278a7ad2be5343187468cd2e26335bc8,This paper proposes an end-to-end trainable multimodal transformer architecture that learns contextualized audio-visual representations of long videos. The authors propose to decompose the Transformer into modality specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. They show that their approach reduces parameters of the Transformers up to 97% allowing us to train our model end to end from scratch.
SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes an online contextualized few-shot learning (OCL) setting, which extends the standard framework of FSL to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. The authors also propose a new dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. Furthermore, they convert popular FSL approaches into online versions and propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) on temporal graphs. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs to account for distribution shift. They experiment with three representative GNN architectures and two scalable GNN techniques, on three new datasets. Their results show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph."
SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,This paper proposes a cross-state self-constraint (CSSC) technique to regularize the representation feature space by comparing representation similarity across different pairs of state. The authors claim that this constraint helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. They test their proposed method on the OpenAI ProcGen benchmark and see significant improvement on generalization performance across most of Procgen games.
SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies the problem of adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting, where the attacker has no access to model parameters and model predictions. The authors formulate the adversarial attack as an optimization problem to maximize the mis-classification rate over the selected set of nodes, and they carry out formal analysis regarding this optimization problem. The proposed optimization problem is combinatorial and seems hard to solve in its original form, and the authors mitigate these difficulties by rewriting it and connecting it with influence maximization on a special linear threshold model related to the original graph structure. They show that, under certain distribu-tional assumptions about the GNN, the expected mis- classification rate is submodular with respect to the selected subset of nodes to perturb, and can be efficiently optimized by a greedy algorithm thanks to its submodularity. Therefore, by specifying concrete distributions, they are able to derive a group of near black-box attack strategies that maximize the expected rate. The experiments verify that the proposed strategies significantly degrade the performance of three popular GNN models."
SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of learning causal structures from directed acyclic graphs (DAGs). The authors propose to exploit a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to mitigate this problem. They demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low-rank assumption. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML). The method performs co-optimization of the neural architectures, training hyper-parameters and data augmentation policies in an end to end fashion without the need of model retraining. Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet and achieves superior performance compared with multi-stage AutoML algorithms."
SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends prior networks to regression tasks by considering the Normal-Wishart distribution over the parameters of multivariate normal distributions. The authors derive all measures of uncertainty, reverse KL-divergence training objective, and the Ensemble Distribution Distillation objective in closed form, and derive a set of general, efficient, interpretable uncertainty estimation approaches for regression models. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets, and two monocular depth estimation tasks, where they yield performance competitive with ensemble approaches."
SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The framework incorporates MAML into a Bayes online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed framework can effectively prevent catastrophic forgetting.
SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper studies the expressive power of higher-order graph neural networks (GNNs). Specifically, the authors propose a new class of GNNs, called Recursive Neighborhood Neighborhood Pooling Neural Graph Neural Networks (RNP-GNN), which represent each vertex by a representation of its neighborhood of a specific radius. Importantly, this representation is recursively computed. The authors show that this model can count subgraphs of size $k$ and overcomes a known limitation of low-order GNN. Moreover, they prove that, in several cases, RNP-GANs can greatly reduce the computational complexity compared to the existing k-GAN and LRP networks."
SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the behavior of two-player zero-sum games in the context of evolutionary game theory (EGT). In particular, the authors show that if the agents employ one of the well-known learning algorithms, Multiplicative Weights Update (MWU) or Follow-the-Regularized-Leader (FTRL), then Lyapunov chaos occurs everywhere in the cumulative payoff space. The authors derive the characterizations by extending the volume-expansion argument of Cheung & Piliouras (2019; 2020) via the canonical game decomposition. They also introduce a new notion of “matrix domination” to compare the strengths of the components against each other."
SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"This paper proposes a new class of adaptive algorithms that achieves marginal optimality, but can also potentially converge much faster than any existing adaptive algorithms in the long term. The authors propose a new motivation for designing the proximal function for adaptive algorithms, named as marginal regret bound minimization. They show the superiority of the proposed algorithms both theoretically and empirically using experiments in deep learning. "
SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The proposed framework is based on the expected quadratic utility maximization (EQUM), which is a common objective of risk management in finance and economics. The authors claim that the computation of the EQUM is easier than that of existing MVRL methods, which require double sampling. Empirical results demonstrate the effectiveness of the proposed method in benchmark settings of RL and financial data."
SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes a framework for multi-task learning based on implicit differentiation. The main idea is to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. The proposed method is evaluated on several tasks and domains, including image segmentation and learning with attributes in the low data regime."
SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"This paper proposes a new measure of epistemic uncertainty (EI) for neural machine translation (NMT) models, which is a measure of uncertainty for long sequences of discrete random variables (i.e. words in the output sentence). The measure is based on dropout approximate inference (Dropout Uncertainty). The authors show that their measure is able to detect out-of-distribution sentences in Neural Machine Translation using the Bayesian Deep Learning (BDL) equivalent of Transformer models. In particular, they show that with dropout uncertainty, their measure can identify when Dutch source sentences are given to the model instead of German."
SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the performance of pruning neural networks at initialization. It compares SNIP (Lee et al., 2019), GraSP (Wang et al. 2020), SynFlow (Tanaka et al, 2020), and magnitude pruning (Renda et al 2020) and finds that these methods perform better than random pruning after training. The authors also show that the pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune, which suggests broader challenges with the underlying pruning heuristics."
SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper proposes FedLearning, a federated learning protocol that is able to defend against both a semi-honest server and Byzantine malicious clients. The main challenge stems from the incompatibility between the server and secure aggregation. To solve this issue, the authors propose to split the clients into shards, securely aggregate each shard’s updates and run FilterL2 on the updates from different shards. The evaluation shows that FED-Learning consistently achieves optimal performance under three attacks among five robust FL protocols."
SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper proposes a benchmark for offline black-box model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function using only previously collected data. The proposed benchmark includes a suite of offline MBO tasks with a unified evaluation protocol and reference implementations of recent methods. The benchmark tasks are derived from real-world problems in biology, material science, and robotics. The authors systematically evaluate several existing MBO methods and baselines on all the proposed benchmark tasks and evaluate them on all of the proposed tasks and benchmark tasks. "
SP:073958946c266bf760d1ad66bd39bc28a24c8521,This paper proposes a new multimodal ELBO formulation for self-supervised generative models. The proposed method combines the benefits of Mixture-of-Experts and MMVAE as special cases and combines their benefits without considerable trade-offs. The experimental results show the advantage of the proposed method compared to the existing methods. 
SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a Bayesian optimization (BO) method for hyperparameter tuning that leverages the prior knowledge of domain experts. The prior knowledge is in the form of priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). The proposed method combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. The authors show that the proposed method is around 12x faster than state-of-the-art methods without user priors and 10,000 times faster than random search on a common suite of benchmarks."
SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes to use binary weights and activations in deep generative models to reduce the computational cost of the models. The authors develop a new class of binary weight normalization, and provide insights for architecture designs of these binarized generative model. They demonstrate that the binary models achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time."
SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning. The main idea is to obtain models of natural variation, which vary the data over a range of natural conditions, and then to use these models to improve the robustness of DL with respect to natural variation. The proposed method is evaluated on a variety of challenging, challenging, and challenging natural conditions."
SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the training of convolutional neural networks (CNNs) with ReLU activations and introduces exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, number of neurons, and data dimension. The authors show that two-layer CNNs can be globally optimized via an `2 norm regularized convex program. They then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\ell_1$-norm regularization. They also extend these results to three-layer networks with two ReLU layers. "
SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,This paper proposes a method for interpretable learning from demonstration (LfD) that uses a probabilistic generative model with a high-capacity neural network to model the high-level notions and concepts that are manifested in a set of demonstrations. The latent variables in the model are explicitly aligned with the human concepts and concepts. The method is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot – that of dabbing liquids with a sponge and moving it along a surface.
SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting. The authors also show that the VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. The paper is well-written and easy to follow."
SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover the 3D shape of an object from a single 2D image using a pre-trained 2D generative adversarial network (GAN). The main idea is to generate pseudo-images with different viewpoints and lighting conditions, which are then used to guide the generator of the original image towards the sampled pseudo images. The proposed method does not require any 3D annotations or strong assumptions on object shapes, yet it successfully recovers 3D shapes with high precision. "
SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tailed classifier called RIDE, which reduces the model variance with multiple experts, reduces model bias with a distribution-aware diversity loss, and reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. The proposed method can be applied to various re-balancing or re-weighting methods."
SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper studies the effectiveness of different pruning criteria for channel pruning. The authors claim that there are two blind spots: (1) Similarity: There are some strong similarities among the criteria and the ranks of filters’ Importance Score are almost identical, resulting in similar pruned structures. (2) Applicability: The filters‘ importance scores measured by some criteria are too close to distinguish the network redundancy well. Based on the assumption (Convolutional Weight Distribution Assumption) that the well-trained convolutional filters in each layer approximately follow a Gaussian-alike distribution, the authors propose a new criterion (Norm-based) that uses the Fermat point (i.e., geometric median) to calculate the relative importance scores of the filters. "
SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for programming language that considers the inherent structure of code. Specifically, the authors use data flow in the pre-training stage, which is a semantic-level structure that encodes the relation of “where-the-value-comes-from” between variables. Two structure-aware tasks are introduced to predict code structure edges and align representations between source code and code structure. The proposed model is evaluated on four tasks, including code search, clone detection, code translation, and code refinement."
SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The method uses a semi-supervised learning framework with an adversarial network to force the distribution of the regression output to resemble the assumed true distribution. The forcing algorithm regularizes the regression results while keeping the information of the training data. The proposed approach is evaluated on four real-world datasets (pLogP, Diamond, House, Elevators). In all four datasets, the proposed approach reduced the root mean squared error by around 55 percent to 75 percent."
SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of out-of-distribution generalization of compositional generalization. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, the authors propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines."
SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper studies the problem of poisoning attacks on deep reinforcement learning (RL) agents. The authors propose a poisoning method that can be applied to any policy-based RL agent. The proposed method, called vulnerability aware adversarial critic poisoning (VA2C-P), is based on the idea of stability radius, which is a metric that measures the vulnerability of RL algorithms. The method is evaluated on several RL agents and multiple environments and shows that it successfully prevents agents from learning a good policy or learning a target policy."
SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper proposes a new checkpointing technique for deep learning models, called Dynamic Tensor re-materialization (DTR), which is a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. The authors prove that DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N}$)$ memory budget with only $O(N)$ tensor operations, which is within a constant factor of optimal and matches the offline bound of the Chen et al. (2016) static checkpointing. The paper also shows that the proposed DTR closely matches the performance of optimal static checkpoints in simulated experiments."
SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the input text. The proposed method is based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach."
SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a neural architecture search (NAS) algorithm to automatically design class-aware generators for conditional generative adversarial networks (cGANs). The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. The proposed Multi-Net NAS (MN-NAS) is the first method that can produce a number of generator architectures, one for each classes, through one search procedure."
SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a method to estimate the average causal effect of a treatment from observational data. The authors formalize unconfoundedness as an orthogonality constraint, which is used during the estimation of the model parameters to ensure that the outcomes are orthogonal to the treatment assignment. They prove sufficient conditions under which this estimator yields an asymptotically normal estimator, and show that its variance is strictly smaller than other estimators. Based on the proposed method, they develop a method called DONUT, which learns outcomes that are orthotonic to treatment assignment, and demonstrate that it outperforms the state-of-the-art substantially."
SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper investigates the expressive power of BatchNorm, a feature normalization technique that normalizes activations and applies a learned affine transform of features. The authors train only the $\gamma$ and $\beta$ parameters and freeze all weights at their random initializations. They show that training only these parameters leads to surprisingly high performance considering the significant limitations. "
SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a test entropy minimization method for fully test-time adaptation, where the model is only given the test data and its own parameters. The proposed method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and achieves a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS and semantic segmentation from GTA to Cityscapes."
SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate the uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, the proposed method offers several advantages in different settings. The proposed method can learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the explorationexploitation trade-off in reinforcement learning."
SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper proposes a stochastic differential equation principled residual perturbation for privacy-preserving DL, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms the state-of-the-art DPSGD in both membership privacy protection and maintaining the DL models' utility."
SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes an extension of PoWER-BERT, which gradually decreases the length of a sequence as it is passed through layers. The proposed extension enables us to train a large-scale transformer, called Length-Adaptive Transformer, once and use it for various inference scenarios without re-training it. To do so, the authors train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length at each layer. They then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget."
SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressive power of neighborhood aggregation in graph neural networks (GNNs). The authors propose two GNN layers: ExpandingConv and CombConv, and evaluate them on general graph classification and graph regression tasks. Theoretically, the authors analyze the requirements of the aggregation coefficient matrix for building more powerful aggregators and even injective aggregators. They also show the necessity of applying nonlinear units ahead of aggregation, which is different from most aggregation-based GNNs. Experimental results show that the proposed models significantly boost performance, especially for large and densely connected graphs."
SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,This paper proposes a method to evaluate the disentanglement of generative models by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. The authors empirically evaluate several state-of-the-art models across multiple datasets. They find that the method ranks models similarly to existing methods.
SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training examples ""unlearnable"", i.e., the model will not be able to learn from them. This is achieved by adding imperceptible ""error-minimizing"" noise to the training data, which is intentionally designed to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is “nothing” to learn. The proposed method can be applied in both sample-wise and class-wise forms. The authors empirically verify the effectiveness of the proposed method on both synthetic and real-world datasets."
SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper proposes an extension of MuZero for nondeterministic, two-player, zero-sum games of perfect information. It formalizes chance as a player in the game and incorporates the chance player into the MuZero network architecture and tree search. Experiments show that NDMZ is capable of learning effective strategies and an accurate model of the game."
SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes a data-efficient option learning algorithm, called Hindsight Off-policy Options (HO2), for hierarchical reinforcement learning (HRL). The main idea of HO2 is to use a critic-weighted maximum-likelihood (similar to Abdolmaleki et al. (2018b); Wulfmeier et al (2020)) and a dynamic programming procedure to infer option probabilities along trajectories and update all policy parts via backpropagation through the inference procedure. To stabilize the policy updates, HO2 uses adaptive trust-region constraints, demonstrating the importance of robust policy optimization for HRL in line with recent work (Zhang & Whiteson, 2019). Experiments show that HO2 outperforms existing option learning methods."
SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel functional form of the Bellman equation, called max-Bellman, to optimize for the maximum reward in an episode. The authors also introduce the corresponding evaluation and optimality operators, and prove the convergence of Q-learning with the max-bellman formulation. Finally, the authors demonstrate the effectiveness of the proposed method on the task of synthesizable molecule generation."
SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes a method for few-shot learning, where new features are added to the dataset with few or no associated observations. The proposed method, Contextual HyperNetwork (CHN), is an auxiliary model that generates parameters for extending the base model to a new feature, by utilizing both existing data as well as any observations and/or metadata associated with the new feature. The CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. The method is evaluated on recommender systems, e-learning, and healthcare tasks."
SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method for Bayesian deep learning that performs inference over only a small subset of the model parameters while keeping all others as point estimates. This enables the method to use expressive posterior approximations that would otherwise be intractable for the full model. In particular, the method first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The proposed method is evaluated on CIFAR-10 and ImageNet, and compared with point-estimated networks and methods that use less expressive posterior approximation."
SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a new approach for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the authors use a model of the environment to obtain embedding for state and action and present a generic architecture that uses these to learn a policy. In this way, the embedded representations obtained via the approach enable better generalization over both state and actions by capturing similarities in the embedding spaces. Evaluations of the approach on several gaming, robotic control, and recommender systems show it significantly outperforms state-of-the-art models."
SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a generative model for unsupervised learning of views for contrastive learning. The viewmaker network generates views by generating and then adding an l_p-bounded perturbation to the input, and is trained adversarially with respect to the main encoder network. The proposed method is evaluated on CIFAR-10 and SimCLR, and on speech and wearable sensor data. The results show that the proposed method can achieve comparable performance to state-of-the-art contrastive methods while being more robust to common corruptions."
SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. The authors demonstrate how different existing detection approaches fail to detect certain types of outliers. They utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outlier. The results include experiments on CIFAR10, SVHN and MNIST."
SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model that is able to match the performance of autoregressive models on CIFAR-10, ImageNet, and FFHQ datasets. The authors show that VAEs with more stochastic layers outperform the PixelCNN in log-likelihood on all natural image benchmarks. They also provide theoretical justification for why greater depth (up to the data dimension D, but also as low as some value K) could improve VAE performance. Finally, they introduce an architecture capable of scaling past 70 layers."
SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a method to improve contrastive representation learning by sampling harder negative samples. The proposed method is based on the noise-contrastive estimation (NCE) method, which is an estimator of the mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. This paper introduces a family of mutual information estimators that sample negatives conditionally – in a “ring” around each positive. The authors prove that these estimators remain lower-bounds of mutual-information, with higher bias but lower variance than NCE. This motivates its value for representation learning. The paper also shows that the naive strategy of sampling hard negatives throughout training can be detrimental."
SP:613a0e2d8cbe703f37c182553801be7537333f64,"This paper studies the problem of large-batch data leakage in federated learning (FL). The authors propose an attack to recover batch data from the shared aggregated gradients. The attack is based on the idea of data index alignment and internal representation alignment in FL, which can significantly improve the recovery performance. Experimental results on vertical and horizontal FL settings have validated the effectiveness of the proposed attack. "
SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for inferring multi-agent interactions from trajectories. The model is based on a variational auto-encoder, where the latent code represents the interaction graph and the reconstruction is done using graph neural networks. The proposed model is evaluated on a simulated physics system, where it is shown to be able to recover ground-truth interactions in an unsupervised manner, and on real-world motion capture and sports tracking data. "
SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes an inductive collaborative filtering framework that learns a hidden relational graph among users from the rating matrix. The key advantage of the proposed method is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that the proposed model achieves state-of-the-art performance for inductive learning on several matrix completion benchmarks."
SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage approach for learning disentangled representations of images. In particular, the disentanglement factors are first learned using a preexisting representation learning method (beta-TCVAE) and then, a generative model is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. The proposed approach is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit models, generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. The authors demonstrate that the proposed approach has much higher reconstruction quality than current state-of-the-art methods with equivalent disenanglement performance across multiple standard benchmarks."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of representations learned by maximizing mutual information (MI) between random variables in the context of deep reinforcement learning (RL). In particular, the authors focus on two commonly used objectives for representation learning based on mutual information maximization, i.e., maximizing the mutual information between states, actions, and rewards at different time-steps. They show that two of these objectives are insufficient for the general class of MDPs, in the most general case, and prove that another typical objective is sufficient. The authors also provide some theoretical results to support their findings. "
SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. In particular, the authors show that the non-convex neural networks training problem is equivalent to a finite-dimensional convex copositive program. The authors also provide the first algorithms for provably finding the global minimum of the vector output convex program, which are polynomial in the number of samples for a fixed data rank."
SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The proposed method, Language-mediated, Object-centric Representation Learning (LORL), builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. LORL enables these algorithms to learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. Experiments show that the integration of the proposed method improves the performance of MONet on two datasets."
SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes EM-RBR (embedding and rule-based reasoning), which combines the advantages of reasoning based on rules and the state-of-the-art models of embedding for knowledge graph completion (KGC). In particular, it uses the relational background knowledge contained in rules to conduct multi-relation reasoning link prediction. In this way, it can find the most reasonable explanation for a given triplet to obtain higher prediction accuracy. Empirical results on FB15k, WN18 and a new dataset FB15K-R show the effectiveness of the proposed method."
SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a new recurrent neural network architecture that combines declarative and procedural knowledge. The proposed architecture consists of active modules called object files that maintain the state of a single object and passive external knowledge sources called schemata that prescribe state updates. The authors propose to use attention to determine which object files to update, the selection of schematas, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
SP:42a3c0453ab136537b5944a577d63412f3c22560,This paper proposes a neural module network (NMN) approach for visual question answering (VQA) on video-grounded language tasks. The approach first decomposes the language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. VilNMN is also trained to resolve any entities and take actions from the detected actions to extract the steps using detected actions of these actions. The proposed approach is evaluated on two tasks: video QA and video grounding tasks.
SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variations of the Policy-Space Response Oracles (PSRO) framework, which is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). The proposed methods modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy. The first method, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy, and the second method, called Mixed-Opponents, constructs a single pure opponent policy that represents an aggregate of the mixed strategy and learns a BR to this policy. Both of these methods employ the machinery of Q-Mixing (Smith et al., 2020), which constructs policies based on an aggregation of policies corresponding to an Q-functions corresponding to policies of a mixture of a set of policies."
SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper presents a non-attentive variant of Tacotron, which replaces the attention mechanism with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The authors also propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
SP:ab9532306d294f85db84b9419ce826f046a7d95e,This paper proposes a method for stereo bird’s eye view (BEV) layout estimation from a pair of stereo images. The proposed method first generates a disparity feature volume using the features of the stereo images and then project it to the bird's eye view coordinates. The inverse perspective mapping (IPM) is used to map the input images and their features to the birds eye view. The method is evaluated on the KITTI (Geiger et al. 2013) dataset and a synthetically generated CARLA simulator.
SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,"This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup. Experiments on synthetic tasks show that popular GNNs are bad at learning long range patterns, and are outperformed by the proposed method (GRGNN)."
SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for model explanation based on the Satisfiability Modulo Theory (SMT) solvers. The main idea is to use Integrated Gradients (IG) to focus on a subset of neurons in the first layer, which allows the proposed method to scale to large networks. The proposed method is evaluated on three datasets MNIST, ImageNet, and Beer Reviews. "
SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a method for 3D pose estimation that is robust to partial occlusion and unseen pose. The proposed method, called NeMo, consists of a generative model of neural feature activations at each vertex on a dense 3D mesh and a differentiable rendering network that estimates the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, the feature extractor is trained to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to partially occlusions and unseen poses compared to standard deep networks, while retaining competitive performance on regular data."
SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes an approach for feature compatible learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning (FCL). The approach requires only features extracted by the old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for FCL, and extend it to handle the case where the old models is a black-box. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the generalization performance of deep neural networks (DNNs) for hyper-parameter optimization. The authors propose to use accelerated approximation (Goodfellow, 2015) of gradient norm to compute the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200\�20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyperparameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). The results also show that the bandit-based or population-based algorithms such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalisation error is not always consistent across phases of the training process."
SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method to retrieve entities from large knowledge bases (KBs) by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. The proposed method is evaluated on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. "
SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. On each time step t, the algorithm receives a routing request and must select a valid path for each edge e in the selected path, incurs a cost ce = fe(x t e) + \eta t e, where x t e is the flow on edge e at time t, fe is the congestion function, and \eta e is a noise sample drawn from an unknown distribution. The algorithm observes ce, and can use this observation in future routing decisions. The routing requests are supplied adversarially. The authors present an algorithm with cumulative regret Á(|E|t), where the regret on each time steps is defined as the difference between the total cost incurred by our chosen path and the minimum cost among all valid paths. "
SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a new masking strategy, called PMI-Masking, to improve the pretraining of Masked Language Models (MLMs) such as BERT. The proposed method is based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. The authors show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. PMIMasking motivates, unifies, and improves upon more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masking and entity/phrase masking."
SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the amortised variational inference (AI) for generative models with the evidence lower bound (ELBO). The authors show that the ELBO objective forces partially-conditioned amortized variational posteriors to approximate products of smoothing posteriors instead of the true posteriors. The authors demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. The performance of the learned generative model is compared with the true posterior."
SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the authors first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(|D|) memory and O(\|D^2) time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, they derive the learning rates in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance and validate the theoretical bounds via numerical experiments."
SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address the drawbacks of RandomNAS. Specifically, the authors first introduce a proxy search space (PS) that is only a small subset of the global search space(GS) to improve RandomNAS’s search efficiency while at the same time keeping a good correlation for the top-performing architectures. The experiments on NASBench-201 show that the proposed approach can achieve near-optimal NAS performance and surpass all existing state-of-the-art."
SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a method to combine imitation learning and meta-RL to enable an agent to quickly adapt to new tasks at test time. The proposed method is called Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). The method is able to interpolate from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of Meta-RL benchmarks under sparse rewards. The authors show how PERIL is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties."
SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the generalization properties of over-parameterized convolutional neural networks (e.g. CNNs) with stochastic gradient descent (SGD). The authors consider the setting where the images contain orthogonal patches and the classification task is binary. They empirically identify a phenomenon of SGD in this setting, where the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. They call this phenomenon Pattern Statistics Inductive Bias (PSI) and empirically verify it in a large number of instances. They prove that if a learning algorithm satisfies PSI then its sample complexity is O(d log(d)) where d is the filter dimension. In contrast, they show a VC dimension lower bound which is exponential in d."
SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,"This paper studies the problem of document classification in the presence of latent structure in the form of contrastive learning. In particular, the authors consider the setting where the corpus is generated by a topic model and the goal is to distinguish between true and fake documents. The authors propose two ways of learning the representation of the documents: (1) landmark embedding, and (2) creating the embedding using a function of the predictions of the linear model. The former is based on the fact that under certain conditions, the topic model is polynomial in the posterior of the document, so that linear functions of the topic posterior correspond to polynomials of the embeddings. The latter is a linear transformation of the underlying topic model, and the authors show that this transformation can be used to learn a representation of documents that reveals their underlying topic posterior information."
SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multimodal generative model learning. The main idea is to train the model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multimodality data. The proposed method is evaluated on a variety of datasets, including CIFAR-10, Fashion MNIST, Fashion-MNIST with CelebA, CelebA-HQ, and Fashion-HQ-C, and it is shown that the proposed method outperforms the baselines."
SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes a method to improve the generative quality of VAEs by adding a reweighting term to the prior, which is the product of an energy-based prior and a re-weighting factor. The re-weighing factor is trained by contrastive learning, and the reweighted prior is learned by comparing samples from the prior with samples from a set of latent variables drawn from the variational posterior. The proposed method is applied to VAEs trained on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. The authors show that the proposed method outperforms the baselines on these datasets."
SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper considers the problem of regularized inverse reinforcement learning (IRL) in the setting of strongly convex MDPs, where the reward function is defined by a set of reward functions that explain the expert’s decisions. The authors propose tractable solutions for regularized IRL and propose a sample-based method for policy imitation and reward learning based on adversarial IRL (AIRL, Fu et al. 2018). Theoretical results show that the proposed method is equivalent to a specific instance of imitation learning (i.e., one that minimizes the Bregman-regman divergence associated with policy regularizers). Empirical results on both discrete and continuous control tasks demonstrate the effectiveness of the proposed approach."
SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many to one translation."
SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,This paper proposes a novel Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The authors split the data into uncertain and certain samples based on small loss criteria. They impose geometric constraints on the uncertain samples by normalizing them into the Wassersteins ball centered on certain samples. Experimental results demonstrate that our WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets with diverse noisy labels.
SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of convolutional neural networks (CNNs) for image classification. The main idea is to use conformal prediction (Vovk et al., 2005) for the purpose of modern image classification in order to make it more stable in the presence of noisy small probability estimates. Conformal predictors like this one can modify any black-box classifier to output predictive sets that are rigorously guaranteed to satisfy the desired coverage property shown in Eq. (1). For evaluations, the authors focus on Imagenet classification. "
SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper proposes a method to compute Wasserstein-2 barycenters of continuous distributions based on a novel regularized dual formulation where the convex potentials are parameterized by input convex neural networks (Amos et al., 2017). The algorithm is straightforward without introducing bias (e.g. Li et al. (2020)) or requiring minimax optimization. This is made possible by combining a new congruence regularizing term combined with cycle-consistency regularization (Korotin et al, 2019a). The paper provides theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach in low-dimensional qualitative scenarios and high-dimensional quantitative experiments."
SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of binary classification on two disjoint manifolds of the unit sphere, where the classifier is a deep fully-connected ReLU network of depth L and width n trained on N i.i.d. samples from a distribution supported on one of the two manifolds. The authors prove that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L, and the number of samples from the manifolds is polynomials in L. The main result is an analysis of the one-dimensional case of the multiple manifold problem, which reduces the analysis of gradient descent dynamics to the construction of a certificate—showing that a certain deterministic integral equation involving the network architecture and the structure of data admits a solution of small norm. "
SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple off-policy reinforcement learning algorithm, called advantage-weighted regression (AWR), which consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of existing supervised learning methods. The paper provides a theoretical motivation for AWR and analyze its properties when incorporated with experience replay. Experimental results show that AWR achieves competitive performance compared to a number of state-of-the-art RL algorithms."
SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method for deep quantization of neural networks with heterogeneous bitwidths. The main idea is to use a sinusoidal regularizer (sin2) to push the weights to the quantized levels, while simultaneously learning the bitwidth of each layer separately. The sin2 regularizer is parametrized so that it can be used as a differentiable regularizer in the form of a regularizer. The method is applied to quantized training algorithms (DoReFa and WRPN) and achieves state-of-the-art performance."
SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper proposes a data augmentation method for neural machine translation (NMT) by using only the original training data without extra data. More accurately, it randomly replaces words or mixup with their aligned alternatives in another language when training NMT models. It is simple yet effective and can be extremely useful when extra in-domain monolingual data is limited. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,This paper proposes a real-time contribution measurement method for federated learning. The method defines the impact of each agent and comprehensively considers the current round and the previous round to obtain the contribution rate. The paper conducts pseudo-distributed training and an experiment on the Penn Treebank dataset. The comparative experiment result shows that the proposed method is more sensitive to both data quantity and data quality.
SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,This paper studies the problem of learning Bayesian networks where an adversarially corrupted fraction of the samples are available. The authors propose a robust mean estimation algorithm that is nearly-linear in the number of nonzeros in the input samples. The algorithm and analysis are simpler than those in previous work. The main contribution of this paper is to establish a direct connection between robust learning of Bayesian Networks and robust Mean Estimation.
SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-horizon planning in model-based reinforcement learning (RL). The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. The proposed method is evaluated on challenging visual control tasks with long-term goals and long-range goals."
SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper studies the effect of data curation on the performance of Bayesian neural networks (BNNs) for image classification. The authors propose a generative model to model the process of data-curation and argue that BNNs use the wrong likelihood. They show that standard image benchmark datasets such as CIFAR-10 are carefully curated, and that it is important to consider this curation as part of the generative models. They also show that the likelihood under this new model closely matches the tempered likelihoods used in past work. Finally, they show that cold posteriors are not helpful when the original underlying labels from all labellers are available."
SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the speed-accuracy trade-off between autoregressive and non-autoregressive machine translation (AR) models. The authors argue that the speed disadvantage of AR models has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. They provide extensive speed-quality comparisons between iterative NAR models and AR models with varying numbers of encoder and decoder layers. They empirically demonstrate that given a sufficiently deep encoder, a single-layer auto-regressive decoder can substantially outperform strong NAR with comparable inference speed."
SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper investigates epoch-wise double descent, i.e., the test error of a DNN also shows double descent as the number of training epoches increases. Specifically, they extend the bias-variance analysis to epoch-dependent double descent and reveal that the variance also contributes the most to the zero-one loss. Inspired by this result, they propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the unknown test error."
SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies adversarial robustness in model-agnostic meta-learning (MAML), a bi-leveled learning procedure in which the outer loop optimizes a task-specific meta-initialization of the model parameters, while the inner loop learns a task specific meta-model. The authors show that robustifying the meta-update stage is sufficient to make robustness adapted to the task specific fine-tuning stage even if the latter uses a standard training protocol. They also investigate how robust regularization can be efficiently designed in MAML."
SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the generalization properties of the learning-to-learn (LTL) approach for tuning the step size for quadratic loss. The authors show that there is a way to design the meta-objective so that the metagradient remain polynomially bounded, but computing the meta gradient directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. They also characterize when it is necessary to compute meta-gradient on a separate validation set instead of the original training set."
SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a graph view-consistent learning network (GVCLN), which constructs a node classification network based on the consistency between two views. The two views have different viewing angles, but their observation objects are the same, so their observation representations need to be consistent. To achieve this, two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while the two loss are applied to the two views to obtain the consistent representation. The pseudo-label loss is designed by using the common high-confidence predictions. Experiments are conducted on three citation network datasets of Cora, Citeseer, and PubMed."
SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper proposes a method for learning the parameters of a Hamiltonian neural network (HNN) that takes into account the underlying physical system's symmetries, i.e., the rotation, angular momentum, and splitting into decoupled subsystems. The authors show that the HNN can be trained to identify the cyclic coordinates of the system's phase space, which can then be used as the input to the network's Hamiltonian. The proposed method is evaluated on both synthetic and real-world data sets, where it is shown that the proposed method outperforms the baseline HNNs."
SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes to combine gradient boosted decision trees (GBDT) and graph neural networks (GNN) for graph representation learning. GBDT deals with heterogeneous features, while GNN accounts for the graph structure. The proposed model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. The experimental results show that the proposed model outperforms GBDT and GNN on a variety of graphs with tabular features."
SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper provides a theoretical analysis of the training-validation split in meta-learning for linear centroid problems. It shows that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. It also shows that if the data are generated from linear models (the realizable regime), both the splitting and non-Splitting methods converge to the best prior."
SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard confident examples from the noisy training data for learning with noisy labels. The proposed method is built on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. The extracted confident examples in the previous round can be exploited to learn a better classifier and that the better classifiers will help identify better (and hard) confident examples. "
SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the intrinsic certified robustness of kNN and rNN against data poisoning attacks. In particular, the authors show that the intrinsic majority vote mechanism in kNN/rNN provides certified accuracy guarantees against general data poisoning attack. The authors also provide empirical evaluation results on MNIST and CIFAR10 to show the superiority of intrinsic certified accuracy of the proposed methods."
SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. To reduce the training time while keeping a decent model performance, the authors propose a metric that combines both the variance of gradients and compute time for each mini-batch. They theoretically analyze how batch-size influence such a metric and propose the formula to evaluate some rough range of optimal batch size. The experimental results show that in contrast to conventional deep learning models, GNNs benefit from large batch sizes."
SP:30d97322709cd292a49f936c767099f11b0e2913,"This paper proposes a method for detecting misclassification errors in neural network classifiers. The proposed method, RED, calibrates the classifier’s inherent confidence indicators and estimates uncertainty of the calibrated confidence scores using Gaussian Processes. Empirical comparisons with other confidence estimation methods on 125 UCI datasets demonstrate that this approach is effective. "
SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. The approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. The authors evaluate their method on question answering, obtaining state-of-the-art results."
SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper proposes a method for specifying Markov decision processes (MDPs) in formal languages to enable the use of safety methods from software engineering and controller synthesis in reinforcement learning. Constraint states are used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. The proposed method is evaluated on the Safety Gym, MuJoCo, and Atari environments."
SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model for binary classification. The idea is to build several smaller decision subtrees and cascade them in sequence. Each subtree is designed to identify the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples. After every subtree sequentially follows another subtree, the cascading waterfall sequence ends when the subtree does not contain any leaves describing positively. The authors evaluate their algorithm on standard datasets, as well as new real-world applications and find that their model shortens the explanation depth by over 40.8% for positive classifications compared to the classic decision tree models."
SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of the width of the neural network on the performance of neural networks. In particular, the authors compare different ways of increasing the network width while keeping the number of parameters constant. They show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance. The number of weights is secondary, as long as the model achieves high training accuarcy. "
SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text, while the language module generates context-aware initial embedding for entities and relations in the graph. The design enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains. The proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes an unsupervised learner that can be trained in a fully-parallelizable manner to discover high-quality autoregressive orders in a data driven way without a domain-specific prior. The learner is a neural network that performs variational inference with the order as a latent variable. The authors develop a practical algorithm for end-to-end optimization using policy gradients. The proposed method is evaluated on image captioning, code generation, and machine translation tasks."
SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction scheme to accelerate sampling-based sampling methods for training GCNs. The main idea is to decompose node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance (first-order variances) during backward propagation. Theoretical convergence analysis is provided to show that the proposed scheme enjoys an O(1/T) convergence rate. Empirical results are also provided to demonstrate the effectiveness of the proposed method.
SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a method for training conditional generators from a single training image based on thin-plate-spline augmentations. The proposed method learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. The method is evaluated extensively and displays remarkable results."
SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method to detect the maximum common subgraph (MCS) between two input graphs. The main idea is to replace the node selection heuristics used by the state-of-the-art MCS solvers with a novel task-specific Deep Q-Network (DQN) that is trained to make the best decision at each step in order to quickly find the best solution for an input graph pair. To enhance the training of DQN, the authors leverage the search process to provide supervision in a pre-training stage and guide our agent during an imitation learning stage. Experiments on synthetic and real-world large graph pairs demonstrate that the proposed method outperforms the baseline methods in terms of effectiveness."
SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes an end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments. The architecture gradually builds up the model: It starts by encoding the points into feature vectors, it identifies a pool of candidate vertices, it prunes those candidates to a final set of corners vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe."
SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies stochastic optimization under weaker assumptions on the distribution of noise than those used in usual analysis. The assumptions are motivated by empirical observations in training neural networks. The authors address this nonstationary behavior of noise by analyzing the convergence rates of stochastically gradient methods subject to changing second moment (or variance). When the noise variation is known, the authors show that it is always beneficial to adapt the step-size and exploit the noise variability. The results reveal why adaptive step size methods can outperform SGD, while still enjoying theoretical guarantees."
SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. The paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. The method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper presents a benchmark for Reinforcement Learning (RL) algorithms with various dimensions of hardness that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. It considers and allows control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. The authors define a parameterised collection of fast-to-run toy benchmarks in OpenAI Gym by varying these dimensions. They show that these benchmarks present substantial challenges to current RL algorithms."
SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes a new calibration method for regression models. The main idea is to use the idea of quantile calibration (Kuleshov et al. 2018) and recast it as entropy estimation, and leverage the new formulation to construct a novel quantile regularizer, which can be used as a blackbox to calibrate any probabilistic regression model. The authors provide a detailed formal analysis of the side-effects of Isotonic Regression when used for regression calibration. The proposed method is trainable in an end-to-end fashion, without requiring an additional dataset."
SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a deep generative probabilistic framework for learning representations of spatial environments. The proposed approach is based on variational inference, neural networks, and a differentiable raycaster. The approach is evaluated on realistic unmanned aerial vehicle flight data, and achieves performance close to that of state-of-the-art visual-inertial odometry systems. The authors also demonstrate the applicability of the model to generative prediction and planning."
SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method to leverage textual descriptions to improve generalization of control policies to new scenarios. The method uses a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. The model is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. The experiments demonstrate that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines. The grounding acquired by EMMA is also robust to noisy descriptions and linguistic variation."
SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes an actor-critic approach for training the critic in the actor critic framework. The critic uses a new state-value function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor critic. The paper provides the theoretical consistency of the new gradient estimator and shows dramatic empirical improvement across a variety of continuous control tasks and algorithms.
SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the effect of depth on the generalization performance of deep neural networks in the over-parameterized regime. The authors introduce local and global labels as abstract but simple classification rules and show that the depth of the network plays a key role in the performance. They also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. The NTK does not correctly capture the depth dependence of the generalisation performance, which indicates the importance of the feature learning, rather than the lazy learning."
SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies representation learning for few-shot learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2(n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. They provide a risk bound of $\tilde{O}(\delta^{(dk n1T + k n2)$ for the linear representation class, where d is the ambient input dimension and k(d) is the dimension of the representation. They also extend this result to handle a general representation function class and obtain a similar result. Finally, they demonstrate the advantage of representation learning in both high-dimensional linear regression and neural networks and show that representation learning can fully utilize all n1$ samples from source tasks."
SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,This paper studies the robustness of deep neural networks (DNNs) to perturbations of high-level input features. The authors propose a black-box approach to determine features for which a network is robust or weak. They leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features.
SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes an approach to automatically cluster together similar tasks during training for multi-task reinforcement learning. The approach is inspired by the expectation-maximization algorithm, which finds clusters of related tasks and uses these to improve sample complexity. The proposed approach is intuitive, simple to implement and orthogonal to other multi- task learning algorithms. The authors show the generality of the approach by evaluating on simple discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games."
SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised framework for learning representations for non-stationary time series. The proposed Temporal Neighborhood Coding (TNC) takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. The motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients’ latent states in settings where labeling data is practically impossible. The authors compare their method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets."
SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a novel approach for learning modular networks. In particular, the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in case of multi-task learning, transfer learning and domain adaptation while achieving competitive results."
SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is the phenomenon that the learned latent space becomes uninformative due to a fixed hyperparameter resembling the data variance. The authors suggest that this variance parameter regularizes the VAE and affects its smoothness, which affects its magnitude of its gradient. The paper proposes AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter, and thus avoids oversmoothing the model. Theoretical analysis on the linear approximated objective function and empirically in general cases are provided."
SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes an unsupervised variational autoencoder (VAE) model that combines a clustering inducing mixture model prior in the local space and a global latent space with a Gaussian prior. The authors show that the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). The model performs domain alignment to find correlations and interpolate between different databases. Finally, the model can discriminate between groups of observations with non-trivial underlying structures."
SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes to use human interaction and attention cues to improve the performance of self-supervised representation learning. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. They record the movements of the body parts by Inertial Movement Units (IMUs) and gaze to monitor the center of attention. They also introduce a new dataset of more than 4,500 minutes of interaction by 35 participants in everyday scenarios with the corresponding body movements and attention. The experiments show that the proposed method outperforms a visual-only state-of-the-art method MoCo (He et al., 2020), on a variety of target tasks: scene classification, action recognition, depth estimation, dynamics prediction, and walkable surface estimation."
SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the problem of negative pretraining, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch on a target task. The authors propose three interventions to remove and fix it. They show that increasing the learning rate after pretraining can yield better results than training directly on the target task, increasing the discretization of data distribution changes from start to target task instead of “jumping” to the target tasks, and resetting the network biases to larger values can also remove the negative preraining effect."
SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper proposes a method to improve the robustness of adversarially trained classifiers against adversarial perturbations by searching for safe spots in the input space that are resistant to adversarial attacks given a pre-trained classifier. The authors propose a bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images on CIFAR-10 and ImageNet datasets. They also show that safe spots can be used to improve both the empirical and certified robustness on smoothed classifiers.
SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. A spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolutions is used to model the dynamics of the spatial regions along the time dimension. Furthermore, it is incorporated the proposed PST convolution into a deep network, namely PSTNet, to extract features of point clouds sequences in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequence."
SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes an adaptive TTS system for high-quality and efficient customization of new voices. The authors design several techniques in AdaSpeech to address the two challenges in custom voice: 1) to handle different acoustic conditions, they model the acoustic information in both utterance and phoneme level. 2) to better trade off the adaptation parameters and voice quality, they introduce conditional layer normalization in the mel-spectrogram decoder. The experiments on VCTK and LJSpeech datasets show the effectiveness of the proposed method."
SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors propose a new metric called effective gradient flow (EGF) to measure the flow of gradients in sparse networks. They show that the default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks, and show that gradient flow can be improved by reconsidering aspects of the architecture design and the training regime. The paper also shows that initialization is only one piece of the puzzle and a wider view of tailoring optimization to sparse networks yields promising results."
SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. This paper studies the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where it analyzes how the feature/labels of one node is spread over its neighbors; and (2) feature and label influence of how much the initial feature and the final feature influence the final label of another node. Based on the theoretical analysis, this paper proposes an end-to-end model that unifies GCN and LPA for node classification. The edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. The proposed model can be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models."
SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper considers the problem of generalization error bounds for supervised classification, where the ground truth labels are generated by a function within another function space, which they call the generator space. They show that the generalization gap depends on the R-Complexity of both the classifier and the generator function spaces, and propose a joint entropy-like measure of complexity between function spaces (classifier and generator), which leads to tighter bounds on the generalisation error in this setting. Theoretical results are supported by empirical validation on the CNN architecture and its transformation-equivariant extensions."
SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a method for post-training quantization (PTQ) of neural networks, i.e. quantization of the weights of the network without retraining. The main idea is to reconstruct the basic building blocks in neural networks and reconstruct them one-by-one. The authors analyze the second-order error of quantization at the block granularity and show that the reconstruction at the granularity arrives at a good balance of cross-layer dependency and generalization error. The mixed precision technique is incorporated in the framework by approximating the inter-layer and intra-layer sensitivity. The proposed method is compatible with mixed precision and can reduce the search cost."
SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of dataset properties, rather than network architecture, on the calibration of deep neural networks. Specifically, the authors focus on the problem of class imbalanced datasets and show that the calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. The authors also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which the authors motive via results on network expressivity. "
SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication in referential games with embodied agents that learn to communicate via actuating their joints in a 3D environment. The authors consider the setting of ZS coordination (ZS communication) with non-uniform distribution of intents and a common knowledge energy cost. They show that under realistic assumptions, the agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice."
SP:5ba686e2eef369fa49b10ba3f41f102740836859,This paper proposes a meta-modeling framework for uncertainty quantification in sequential regression tasks. The proposed method is based on the meta-learning framework proposed in [1]. The authors propose a new evaluation methodology for SRT. They also propose a method for generating asymmetric and symmetric uncertainty bounds. The experimental results show that the proposed method outperforms competitive baselines on both drift and non-d drift scenarios.
SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,This paper proposes two unbalanced Gromov-Wasserstein (UGW) formulations for the comparison of metric measure spaces (i.e. metric spaces endowed with a probability distribution) up to isometry. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. It is shown that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is an unbalanced distance between mm-spaces based on the conic lifting. Numerical experiments on synthetic examples and domain adaptation data with a Positive-Unlabeled learning task highlight the salient features of the unbalanced divergence and its potential applications in ML.
SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a novel pruning method, called all-alive pruning (AAP), that produces the pruned networks with only trainable weights. This method is applicable to various saliency-based network pruning methods and model architectures. The authors show that dead connections do not contribute to model capacity, and hence, dead connections are the nodes that are not trainable. The proposed method can be applied to iterative pruning, one-shot pruning and dynamic pruning. The effectiveness of the proposed method is demonstrated on CIFAR-10, Cifar-100 and MNIST datasets."
SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for semantic image editing that uses a generative adversarial network (GAN) to learn latent space directions for image-to-image transformation. The proposed method is based on a regressor that predicts the attributes that an image exhibits, and a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. The method is evaluated on both synthetic and real-world datasets."
SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes a feature alignment regularization method for Gumbel-Softmax-based GANs for discrete sequence generation. Specifically, the proposed method embeds the latent feature representations in a finite feature space and force the generated samples of generated samples to resemble the real data distribution by minimizing the distance between their respective feature representation centroids. Experiments on synthetic and real-world datasets show the superior performance in quantitative evaluation and demonstrate the effectiveness of the proposed approach."
SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. The proposed method is evaluated on two domains with extreme reward progressivity, where standard value-based methods struggle significantly, and also evaluated on a set of six standard Atari games that do not overtly favour the approach."
SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper introduces a notion of “usable information” (i.e., the amount of information that can be extracted from the representation learned by a learned decoder) and studies how this information is represented across layers of the network throughout the training process, and how this is affected by the optimization algorithms and the network pretraining. The authors propose a simple task inspired by decision-making tasks in neuroscience, where inputs and outputs are carefully designed to probe specific information processing phenomena. They then extend their findings to standard image classification tasks trained with state-of-the-art models. They find that training with SGD is critical to bias the learning of minimal representations in intermediate layers."
SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper proposes a new algorithm for nonconvex-strongly-concave min-max optimization. The proposed algorithm is based on the SREDA algorithm proposed by Luo et al. (2020), which achieves the optimal complexity dependence on the required accuracy level. However, the convergence guarantee requires stringent initialization accuracy and an accuracy-dependent step-size, which makes the proposed algorithm very slow in practice. The authors propose an improved algorithm that has less restrictive initialization requirement and is accuracy-independent (and much bigger) stepsize. In addition, the authors also propose a zeroth-order variance reduction algorithm named ZO-SREDA-Boost for the scenario that has access only to the information about function values not gradients, and show that it outperforms the best known complexity dependency on."
SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of one-shot object detection. The authors show that increasing the number of object categories used during training can improve the generalization from seen to unseen classes from 45% to 89% and improve the state-of-the-art on COCO by 5.4 %AP50 (from 22.0 to 27.5). They verify that the effect is caused by the number categories and not the training samples, and that it holds for different models, backbones and datasets. This result suggests that the key to strong few-shot detection models may not lie in sophisticated metric learning approaches, but instead simply in scaling the number category."
SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes to use the spatial gradients of the occupancy field and signed distance field (SDF) as a source of supervision for single-view implicit surface reconstruction. The authors propose a novel closed-form Differentiable Gradient Sampling (DGS) solution that enables backpropagation of the spatial gradient to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. They demonstrate single view implicit surface reconstructions on real-world scenes via learning directly from a scanned dataset. Their model performs well when generalizing to unseen images from Pix3D or downloaded from the internet."
SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a training strategy called “Pseudo-to-real” for extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. The authors demonstrate a practice of pretraining an unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 NVIDIA-V100 GPUs and lasted around 10 days. Besides, the authors also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities."
SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper considers the problem of training energy-based generative models (EBMs) with shallow overparametrized neural network energies. The authors derive variational principles dual to maximum likelihood EBMs with shallow neural networks energies, both in the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. A variant of this algorithm is also considered in which the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies the lower bounds of differentially private empirical risk minimization (ERM) for general convex functions. In particular, the paper considers the constrained and unconstrained cases of DP-ERM, where the lower bound is $\Omega(\sqrt{p}log(1/\delta)$ for the constrained case and $Omega(p^{-\sqrt(n))$ lower bound for the unconstraint case. The main contributions of the paper are:  1. The paper proposes a novel $\ell_2$ loss function for the convex case, which is a generalization of the well-known $\ell_{2}$ loss used by Bassily et al. (2014). 2. It introduces an auxiliary dimension to simplify the computation brought by the `2 loss. 3. It provides a lower bound of $\tilde{\Omega}(p^n)$ lower bounds for both the constrained (constrained) and the unregularized (unregularized) cases. "
SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key of the method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The framework covers the heat equation and the porous medium equation. Numerical experiments demonstrate the performance and scalability of the algorithm."
SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed meta-feature with the space of distributions on the hyperparameter configurations. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta features boosts the performance of state-of-the-art AutoML systems, AutoSkLearn and Probabilistic Matrix Factorization."
SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a method for federated learning (FL) with heterogeneous participants that provides in-situ customization of model sizes and robustness. Specifically, the proposed method learns a set of base sub-networks of different sizes and adversarial robustness levels, which are then aggregated on-demand according to inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate that the method provides better performance than the existing FL methods."
SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. It also converges globally even in settings where the underlying operator exhibits limit cycles. Moreover, a variant with stochastic oracles is proposed."
SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies neural contextual bandits, a general class of contextual bandits where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves $O(\sqrt{T})$ regret, where $T$ is the learning time horizon."
SP:a9a2c21110e00f19882d27bef0063c422a15e576,This paper proposes a Shapley-inspired methodology for training action space categorization and ranking for reinforcement learning (RL). The methodology includes a Monte Carlo simulation to reduce the search space by 80% and categorizes the training action sets into dispensable and indispensable groups. It also ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study.
SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"This paper proposes a novel approach to construct probably approximately correct (PAC) prediction sets in the presence of covariate shift. The main idea is to use importance weights to capture the likelihood of a source example under the target domain. When the importance weights are known, it uses rejection sampling (von Neumann, 1951) to construct the prediction sets. When only given confidence intervals around importance weights, it constructs prediction sets that are robust to this uncertainty. The proposed approach is evaluated on covariate shifts based on DomainNet and ImageNet."
SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount of unlabelled data to progressively refine the model parameters. In particular, the authors seek to understand the behaviour of the generalisation error of iterative SSL algorithms using information-theoretic principles. The authors first work with a simple model—namely, the binary Gaussian mixture model. Their theoretical results suggest that when the class conditional variances are not too large, the upper bound on the generalizability error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR datasets."
SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method (GPM) for model-free reinforcement learning (RL) that can generate actions not only for the current step, but also for a number of future steps. The authors claim that GPM is trained by maximizing value and can be regarded as intentional action sequences for reaching high-value regions. GPM can leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Experiments are conducted on several benchmark environments and demonstrated its effectiveness compared with several baseline methods."
SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorizations to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix/tensor completion problems and prove that the proposed methods have tighter generalization error bounds than conventional matrix & tensor decomposition methods. The experiments on synthetic data and real datasets showed that the methods have much higher recovery accuracy than many baselines.
SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes a method to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. The paper assumes an arbitrary structured output model is available as a black box and argues how considering the correlations between output variables can improve the explanation performance. The goal is to train a function as an interpreter for the target output variable."
SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a method for risk-sensitive reinforcement learning based on policy gradients. The main idea is to use the cumulative distribution function (CDF) as the objective function of the policy gradient. The authors show how to achieve an asymptotically consistent policy gradient for a broad class of CDF-based objectives via sampling, subsequently incorporating variance reduction measures to facilitate effective on-policy learning. They use the resulting algorithm to train agents with different risk profiles in penalty-based formulations of six OpenAI Safety Gym environments, observing that moderate emphasis on improvement in training scenarios where the agent performs poorly both increases the accumulation of positive rewards and decreases the frequency of incurred penalties."
SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning framework to proactively learn a deep learning surrogate model and accelerate simulation of large-scale, spatiotemporal, age-structured epidemic models. The proposed method is based on the integration of neural process, deep sequence model, and active learning. The model automatically infers the latent process which describes the intrinsic uncertainty of the simulator. A new acquisition function, Latent Information Gain (LIG), is designed to select the parameters with the highest LIG, queries the simulator to generate new simulation data, and continuously updates our model. Theoretical analysis and empirical results demonstrate the efficacy of the proposed method."
SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"This paper proposes to improve the performance of DP-SGD for fine-tuning large pretrained NLP models by (1) using larger pretrained models, (2) hyperparameters that suit DP optimization, and (3) fine tuning objectives aligned with the pretraining procedure. The authors also propose a memory saving technique called ""ghost clipping"" that allows clipping to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead."
SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method to optimize the skeletal structure and joint attributes of a robotic agent. The main idea is to learn a conditional policy that first applies a sequence of transform actions to modify an agent’s skeletal structure, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods significantly in terms of convergence speed and final performance."
SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a method to accelerate the training and inference of coordinate-based implicit neural representations (coordinate-based MLPs) by proposing a new split MLP architecture, called CoordX, which splits the initial layers to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training/inference, while achieving similar accuracy as the baseline MLP. The proposed architecture can be used for many implicit neural representation tasks with no additional memory overheads."
SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to infer object-centric representations of visual scenes from a single image. The method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The authors empirically show that INFERNO discovers objects in a scene without supervision. They also validate the interpretability of the learned representations by manipulating inferred scenes and showing the corresponding effect in the rendered output."
SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method to evaluate the subgraph importance of subgraphs to explainability of graph neural networks (GNNs). The authors argue that a distribution shift exists between the full graph and the sub-graphs, causing the out-of-distribution problem. The authors propose Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an explanatory subgraph on the model prediction. DSE uses the front-door adjustment and introduces a surrogate variable of the sub graphs. Specifically, the authors devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of sub graph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity."
SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning in the context of few-shot learning, where the goal is to select a subset of training data points that are informative (i.e., those that are consistent with the provided examples) for the task at hand. The authors propose to use a pretrained model to select these points, and show that this approach can achieve better performance than random sampling. In particular, they show that pretrained models are able to select more informative points than unpretrained models, suggesting that the ability to actively learn is an emergent property of the pretraining process."
SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper presents a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The authors propose a novel pre-training strategy for GRAPHIX, namely deleted sub-tree reconstruction, to enrich the model with implicit knowledge of program structures from unlabeled source code. They evaluate the model on the Patches in The Wild Java benchmark, using both abstract and concrete code. Experimental results show that the model significantly outperforms a wide range of baselines including CodeBERT and BART and is as competitive as state-of-the-art pretrained Transformer models despite using fewer parameters."
SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes a new algorithm for federated adversarial training (FAT), which relaxes the inner-maximization optimization of Adversarial Training into a lower bound friendly to Federated Learning (FL). Specifically, the authors propose an alpha-weighted weighting mechanism, which weights the adversarial examples based on their adversarial losses. Then, the heterogeneous update can be downweighted with this weighting, facilitating a friendly optimization in the combination of adversarial learning and FL. Theoretical analysis on the convergence of the proposed algorithm is provided, and extensive experiments are conducted to comprehensively understand the characteristics of α-WFAT."
SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a method for semi-supervised lifelong learning. The main idea is to use data programming to label new data under the supervision of a set of weak labeling functions trained from a limited number of labeled data points. The proposed method is evaluated on various partially labeled LML task sequences generated from commonly used datasets including MNIST, Cifar-10 and CifAR-100 (LeCun & Cortes, 2010; Krizhevsky, 2009), while mounting on a diversity of supervised LML frameworks such as Deconvolutional Factorized CNN (DF-CNN), ORDisCo, and Tensor Expandable Network (TF)."
SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two methods to generate adversarial examples to evade detection defenses. The first method, called Selective Projected Gradient Descent (PGD), is a modification of the standard gradient descent (MMS+17) algorithm. The second one, called Orthogonal PGD, is a variant of PGD that orthogonalizes the gradients when running standard gradient-based attacks. The authors show that PGD can be used to evade four state-of-the-art detection methods, reducing their accuracy to 0% while maintaining a 0% detection rate."
SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes a probabilistic approach to the value function learning in cooperative games. The proposed approach is motivated by the maximum entropy principle, which states that maximizing the entropy minimizes the amount of prior information built into the distribution, i.e., it amounts to assume nothing about what is unknown. The authors propose to use mean-field variational inference of the energy-based model to recover classical game-theoretic valuation criteria through conducting one-step fixed point iteration for maximizing the ELBO objective. Theoretical analysis shows that under uniform initializations, the proposed variational valuations satisfy a set of game theoretic axioms. Empirical results show that the proposed Variational Index enjoys lower decoupling error and better valuation performance on some synthetic and real-world valuation problems."
SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method to directly estimate epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator is useful in interactive learning environments arising in active learning or reinforcement learning. The authors demonstrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning, and evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and estimating uncertainty about synergistic drug combinations."
SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes a method for learning rotation matrices for end-to-end PQ based embedding indexes. The proposed method is based on geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n). The authors propose a family of block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. They also provide complexity analysis and prove convergence of the proposed algorithm under convexity and other mild assumptions."
SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The framework uses (1) a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and (2) a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The authors claim that the proposed approach (NSM) isolates the relational structure from source domain with high accuracy and (b) successfully utilizes this structure for analogical reasoning in the targets domain."
SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a contrastive self-supervised method, namely Self-GenomeNet, for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force our framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the proposed method with self/semi-supervision settings outperforms state-of-the-art deep learning methods."
SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how our model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,This paper conducts an empirical study of the performance of pretrained language models (PLMs) and continual learning (CL) methods in the context of NLP. The authors compare the performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in two typical incremental settings. They also conduct a layer-wise and task-wise analysis to dissect the performance characteristics of different PLMs. 
SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,This paper proposes a defense against untargeted model poisoning attacks in federated learning. The proposed defense is based on the intuition that certain patterns of gradient flips are indicative of an attack. The paper proposes to assign reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. The authors show that TESSERACT provides robustness against even a white-box version of the attack.
SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a debiasing method for estimating linear functionals of high-dimensional regression functions. The proposed method is based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. The method solely requires value query oracle access to the linear function. The authors also propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined RiesZ representer and regression loss, while sharing representation layers for the two functions. "
SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. The authors consider a practical MARL setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. They propose a mini-batch Markovian sampled fully decentralized actor critic algorithm and analyze its finite-time convergence and sample complexity. They show that the sample complexity of this algorithm is O(N/\varepsilon^2/\ell_\infty) and matches that of the state-of-the-art single-agent actor critic algorithms."
SP:8475e89f143c727e33147b652c2d0b3cdb420382,This paper provides a theoretical understanding of contrastive learning (CL) for self-supervised representation learning. The main idea is to show that CL can be viewed as an instance discrimination task (differing each image from others) instead of a classification task (clustering images from the same class together and differing with other classes). The paper provides upper and lower bounds for its downstream performance and show how the two asptymptotically converge by supporting the effect of data augmentation on the augmentations. 
SP:b491314336c503b276e34e410cf461cb81294890,This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors also propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The experimental results show that the proposed method achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model.
SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a method to model the long-term dependencies in the variable space of multivariate time series. The main idea is to use a tensor network based on the idea of low-rank approximation to model variable space. The tensor components are shared to ensure the translation invariance of the network. The series variable encoder is designed to improve the quality of variable space, and the skip-connection layer is used to achieve the dissemination of information such as scale."
SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"This paper proposes a variant of SCO algorithms with sparse moving averages for GNN training. By storing the moving averages in the most recent iterations, the proposed algorithm only requires a fixed size buffer, regardless of the graph size. Theoretical results show that the proposed method preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. Empirical results validate the theoretical results and show the algorithm outperforms the traditional Adam SGD."
SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash), a variant of MinHash that uses two independent random permutations in a circulant manner, and provides the surprising theoretical results that using only two independent permutations leads to uniformly smaller Jaccard estimation variance than that of the classical MinHash with K independent permutation. Experiments are conducted to show the effectiveness of the proposed method."
SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a simple and efficient training scheme to achieve adversarial robustness against the union of lp-threat models. The proposed E-AT scheme is based on geometric considerations of the different Lp-balls and costs as much as normal adversarial training against a single threat model. Moreover, it can fine-tune with just 3 epochs any lp robust model (for p \in {1,2,\infty}) and achieve multiple norm robustness. In this way, the proposed method can boost the state-of-the-art for multiple-norm robustness to more than 51% on CIFAR-10 and report up to our knowledge the first ImageNet models with multiple norms robustness for the first time."
SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed to allow the gate to choose either the public Encoder or the corresponding private Encoder. Moreover, a variant ofSMTL to place all the gates after the decoder of all the task is proposed. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper argues that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions, which makes model selection difficult and undecidable. Specifically, the authors show that there is no good deterministic or randomized estimates of the partition function, and hence no useful importance sampling estimates can guarantee to have variance bounded below a rational number. As alternatives, they consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness."
SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), which are constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurface would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. The ASWD significantly outperforms other SWD variants for both synthetic and real-world problems."
SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a framework for improving coordination and performance of multi-agent reinforcement learners (MARL). The framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator, that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS is a combination of reinforcement learning (RL) and switching controls, which determines the best states to learn to add intrinsic rewards which leads to a highly efficient learning process. The authors demonstrate the superior performance of the proposed framework in challenging tasks in Foraging and StarCraft II."
SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a hierarchical attention mechanism for multi-hop question answering. The proposed model, called DOCHOPPER, iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The model is able to retrieve either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. The method is evaluated on four different QA tasks and achieves state-of-the-art results."
SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a semi-supervised learning method for character-based characteristic extraction for voice casting for audiovisual productions. The proposed method is based on Gresse et al. (2020), which defines the p-vector, a neural-network-based representation of the voice that is dedicated to the character aspects in acted voices. The authors propose to use a representation extractor based on the initial labels, then compute refined labels using a clustering algorithm to finally train a refined representation. The method is validated by applying Label Refining on recordings from the MassEffect 3 video game."
SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks, allowing clients to learn multiple tasks with their private data. The key idea is to disentangle representation of local and non-local features using task-agnostic Vision Transformer and a task-specific head/tail. Each client learns a translation from its own task to a common representation, while the Transformer body learns global attention between the features embedded in the representation. The proposed method is evaluated on various low-level and high-level computer vision including medical image data."
SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking, where RL agents exploit gaps in the misspecified reward functions. The authors construct four RL environments with reward misspecifications and investigate how reward hacking arises as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They also propose an anomaly detection task for aberrant policies and offer several baseline detectors for it."
SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective (f-TVO) that generalizes the TVO by replacing Kullback-Leibler (KL) divergence with arbitary differeitiable f divergence. In particular, it approximates dual function of model evidence f(p(x)) rather than the log model evidence log p(x) in TVO. It is derived from a deformed χ-geometry perspective. Experiments on VAE and Bayesian neural network show that the proposed f -TVO performs better than cooresponding baseline f-Divergence Variational inference."
SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of the interplay between different components of the deep reinforcement learning (RL) toolbox in the continuous control setting. The authors propose an ensemble-based off-policy RL algorithm, ED2, which addresses several issues found during the empirical study. They show how existing tools can be brought together in a novel way, giving rise to the Ensemble Deep Deterministic Policy Gradients (ED2) method, to yield state-of-the-art results on the OpenAI Gym MuJoCo."
SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for the Equalized Loss (EL) constraint, which is a fairness notion that requires the prediction error/loss to be equalized across different demographic groups. The authors propose two algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of this non-convex problem. Experiments on real-world data show the effectiveness of the proposed algorithms."
SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper revisits systematic generalization from the perspective of meaningful learning, i.e., the ability of humans to learn new concepts by connecting them with other previously known knowledge. The authors propose to reassess models’ compositional skills conditioned on the semantic connections between new and old concepts. They augment a training dataset in either an inductive or deductive manner to exposure such semantic links to models. Their observations on SCAN, as well as two real-world datasets on semantic parsing, suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking."
SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a novel method for 3D shape representation learning using multi-scale wavelet decomposition. The proposed method firstly generates approximation or detail wavelet coefficients per point, classifies each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, it exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes to combine the benefits of full and lightweight finetuning, achieving strong performance both ID and OOD. The authors show that an ensemble of the lightweight and full finetuned models achieves the best of both worlds: performance matching the better of full/lightweight fine-tuning, both in-distribution and out of distribution. Moreover, the authors also show that they can achieve similar improvements using a single model instead of two with their proposed cocktail Finetuning. "
SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models (WARM), a method to improve weakly supervised models via active learning. Specifically, at each iteration WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets reveal that WARM can substantially improve the accuracy of probablistic labels used to train downstream classifiers with as few as 30 queries to experts."
SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper considers the problem of training a classification model with group annotated training data. The authors propose a new algorithm that explicitly encourages learning of features that are shared across various groups. The key insight is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by GroupDRO. Theoretically, the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, it matches or achieves better performance compared to strong contemporary baselines on standard benchmarks on both minority groups and across all groups."
SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for black-box models that can capture feature interactions in a higher-order. The proposed method is based on Covert et al. (2020a), which provides a unifying mathematical framework capturing a broad array of explainability techniques, termed Removal-based Explanation methods. The authors propose a method to extend any given univariate removal-based explanation to capture asymmetrical feature interactions, represented as a directed graph. They also provide theoretical justification for these definitions in the context of SHAP, the Shapley value explanation map."
SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a novel framework for interpretable policy learning based on probabilistic tree policies. The proposed method is compatible with fully-offline and partially-observable clinical decision environments. The authors propose to learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This method outperforms the state-of-the-art on real and synthetic medical datasets."
SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a multi-agent reinforcement learning (MARL) approach for fine-grained data augmentation. The main idea is to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. Each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal effect of the entire image by sharing a team reward. Extensive experiments demonstrate the effectiveness of the proposed method on multiple benchmark datasets.
SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes an adversarial distribution alignment method to mitigate the adversarial vulnerability of deep neural networks (DNNs). The authors construct a causal graph to model the generation process of adversarial examples and define the adversary distribution as the difference between natural and adversarial distributions. The proposed method is evaluated on MNIST, CIFAR-10 and Fashion MNIST datasets."
SP:9f09449a47464efb5458d0732df7664865558e6f,"This paper proposes a method for continual learning (CL) that learns a low-rank filter subspace for each convolutional layer of a neural network. The subspace is constructed by decomposing convolution filters within each network layer over a small set of filter atoms. Then, the authors propose to perform continual learning by simply swapping filter atoms for each task. The proposed method can be applied to a wide range of optimization schemes and CNN structures. The effectiveness of the proposed method is illustrated both empirically and theoretically."
SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse in Stein Variational Gradient Descent (SVGD), a particle-based variational inference algorithm for high-dimensional Gaussian distributions. The authors show that SVGD underestimates the variance of the target distribution in high-dimensions, and connect SVGD with MMD-descent, a kernel-based particle inference algorithm that performs maximum mean discrepancy (MMD) objective, and show that despite their similar updates, SVGD does not collapse the variance. The variance collapse phenomenon is attributed to the bias from deterministic updates present in the “driving force” of SVGD, and the authors empirically verify that removing such bias leads to more accurate variance estimation."
SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies adversarial training (AT) and label correction (AT with noisy labels) in the presence of label noise. The authors show that AT with strong smoothing effects (i.e., smoothing out the small clusters around incorrect data and letting incorrect data alone) is more robust to label noise than standard training (Zhang et al. 2017). In addition, AT with NL is helpful for improving even the natural accuracy, which illustrates the superiority of AT as a general-purpose robust learning criterion."
SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method allows us to provide formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The approach can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods."
SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) that learns representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized generative model and demonstrate that HCM can benefit from having learned structures with shared components, leading to flexible transfer and generalization."
SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems. The authors show that reparametrizing the optimization variables as the output of a neural network can lead to significant speedup. They also show that to obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization. They show the utility of their method on two optimization problems: network synchronization and persistent homology optimization, and find an impressive speedup, with our method being 4 \times \times faster."
SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method for image classification based on a layered structure of Support Vector Machine (SVM) ensembles for non-parametric image classification. By utilizing the quick learning of SVMs compared to neural networks, the proposed method can reach higher accuracy than DCNNs when the training set is small. Experimental results show that while “conventional” DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples is smaller."
SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG) to reduce the communication and memory overhead. Each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. A cascade of “exits” is attached to the model to make multiple predictions and direct further computation. The authors redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. The proposed approach is evaluated on semantic segmentation and human pose estimation."
SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new bootstrapping neural process (NP) method, called NeuBANP, which is an extension of Bootstrapping Neural Processes (B(A)NP) by injecting multiple random weights into the encoder and the loss function to generate the bootstrap distribution of random functions. The proposed method is evaluated on Bayesian optimization and contextual multi-armed bandit tasks. The results show that the proposed method achieves the best performance in the sequential decision-making tasks. "
SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a method for pre-training large-scale genome data in a multi-modal and self-supervised manner, which the authors call GeneBERT. Specifically, they simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input to improve the robustness and generalizability of the model. They pre-train their model on the ATAC-seq dataset with 17 million genome sequences. They evaluate their GeneBERTs on regulatory downstream tasks including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction."
SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a method for detecting out-of-distribution (OOD) samples. The method is based on the geodesic (FisherRao) distance between the underlying data distributions. The discriminator is able to combine confidence scores from the logits outputs and the learned features of a deep neural network. Empirically, the authors show that the proposed method outperforms the existing methods on various network architectures and datasets."
SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper provides a generalization of Cover’s Function Counting Theorem, which quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. The authors show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. Finally, they test their theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a method to explain the misclassification of a pre-trained classifier in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The proposed method is based on two ideas: counterfactual explanations and concept activation vectors, and validate its approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a method to improve the efficiency and quality of Kernel Point Convolution (KPConv) for 3D point cloud classification and segmentation. KPConv is a kernel convolution method that maps the input points to the influence of kernel points via linear correlation, and then convolves the mapped features over the kernel points to compute weighted features. In this paper, the authors propose a depthwise kernel to reduce resource consumption, and re-calibrates the contribution of kernel point towards each neighbor point via Neighbor-Kernel attention to improve representation power. In addition, they utilize Inverted Residual Bottleneck (IRB) to craft a design space and employ a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on the proposed method."
SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of robust overfitting, robustness overestimation, and robustness-accuracy trade-off in adversarial training. The authors propose a method to measure the data quality based on the learning behaviors of the data during training and find that low-quality data may not be useful and even detrimental to the adversarial robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in training adversarial neural networks. The experiments show that when low quality data is removed from the dataset, robust over-fitting and overestimation can be largely alleviated, and the robustness accuracy tradeoff becomes less significant."
SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters required to approximate multivariate functions of bounded second mixed derivatives (Korobov functions). The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU. They also show that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate Korobov function, showing that neural networks are near-optimal function approximation."
SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on language emergence in the Lewis Game, a reinforcement learning task where a speaker describes a hidden object to a listener, which must then reconstruct object properties. The population size of the agents is randomly chosen. The goal is to observe whether increasing the number of agents improves the communication protocol qualities, e.g. success rate, compositionality, generalization etc. The authors first investigate how speaker-listener asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. They find that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. Then, they leverage this observation to control population heterogeneity without introducing confounding factors, and show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages."
SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a new polynomial filter-based graph neural network (GNN) model for heterophilic graphs. The proposed model is inspired by Chien et al. (2021), which proposes to learn a sum of polynomials over different subsets of eigenvalues of the graph. However, this paper points out that the proposed method is ineffective in learning higher-order polynoms because the model attempts to overcome over-smoothing by giving smaller coefficient values. To mitigate this issue, the authors propose to perform an eigendecomposition of the graphs and propose to learn multiple adaptive polynometric filters acting on different subset of the spectrum. Theoretical and empirical results are provided to show the effectiveness of the proposed model."
SP:903545b1b340ec5c13070e0f25f550c444de4124,This paper proposes a novel graph shortest distance embedding method called Betweenness Centrality-based Distance Resampling (BCDR) for shortest distance query (SDQ) in graphs. BCDR is based on truncated random walk followed by PMI-based optimization to embed local structural features into a dense vector on each node and integrates with a subsequent predictor for global extraction of nodes’ mutual shortest distance. Theoretical analysis shows that BCDR can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure.
SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL), where the goal is to maintain the invariant knowledge of the pre-trained language model (LM) on a corpus of new knowledge. The authors categorize world knowledge into three categories and make benchmark datasets to measure each of them during CKL. They also propose a metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new information. Through extensive experiments, they show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously."
SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a stochastic approach for the criterion minimization of decision trees. The splitting of a leaf node is cast as selecting the best feature and threshold that minimizes a desired criterion. The criterion is an impurity function, such as cross-entropy, uniform piece-wise constant densities, Gaussian differential, etc. The authors propose an iterative algorithm that selects the best features and threshold at each iteration. The algorithm is faster than the exhaustive search algorithm by several orders of magnitude. The proposed algorithm is compared with several other related state-of-the-art decision tree learning methods, including the baseline non-stochastic approach."
SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper proposes a new learning rate schedule for SGD on quadratic objectives. The proposed schedule is based on the assumption that the eigenvalue distribution of the underlying Hessian matrix is skewed, which is quite common in practice. Theoretical analysis shows that the proposed schedule can achieve minimax optimal convergence rates (up to a constant). The paper also proposes two simple learning rate schedulers for practical applications that can approximate eigencurve. Experimental results on image classification tasks on CIFAR-10 demonstrate the effectiveness of the proposed method."
SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"Offline reinforcement learning (RL) has been a hot topic in the last few years due to its ability to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. In this paper, the authors compare a variety of uncertainty heuristics for offline model-based RL, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. They also explore the interaction with a series of other hyper-parameters such as rollout horizon, model size, and model complexity. Finally, they show that selecting these parameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods and result in drastically stronger performance."
SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a new prioritized experience replay method for off-policy model-free reinforcement learning (MfRL). The proposed method, called model-augmented priority experience replay (MaPER), employs new learnable features driven from components in model-based RL (MbRL) to calculate the scores on experiences. The proposed MaPER brings the effect of curriculum learning for predicting Q-values better by the critic network with negligible memory and computational overhead compared to the vanilla PER. "
SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method for human-in-the-loop reinforcement learning (HACO) that learns to minimize the human intervention budget and adjust the level of automation during the training to adaptively adaptively adjust the learning agent’s level of adaptation during the learning process. HACO learns to extract proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that the proposed method achieves a substantially high sample efficiency in the safe driving benchmark. It can train agents to drive in unseen traffic scenes with a handful of human intervention budgets and achieve high safety and generalizability.
SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a new hierarchical meta-learning method, called Dual Meta Imitation Learning (DMIL), which uses the likelihood of state-action pairs from each sub-skill as the supervision for the high-level network adaptation, and use the adapted high level network to determine different data set for each sub skill adaptation. The authors theoretically prove the convergence of the iterative training process of DMIL, and establish the connection between DMIL and the Expectation-Maximization algorithm. Empirically, the proposed method achieves state-of-the-art performance on the meta-world benchmark and comparable results on the Kitchen environment."
SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method to compress the node embeddings of graph neural networks (GNNs). The proposed method is based on the ALONE method proposed by Takase & Kobayashi (2020), which computes the compositional code vectors for each node in the graph using a randomly generated compositional vector. Then, a decoder model is used to convert the vector into a floating-point vector, which is then compressed into a bit vector. The authors show that the proposed method can be used in combination with GNNs and achieves better performance than the alternatives."
SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper proposes to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta solvers) for domain adversarial learning (DAL). The authors show that gradient descent in DAL can violate the asymptotic convergence guarantees of the optimizer, which hinders the transfer performance. Based on this, the authors propose to replace the gradient descent algorithm with a more stable one, which allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers."
SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularizer, called individual Flood (denoted as iFlood), which encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The authors also theoretically show that the design of iFlOOD can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. The experimental results on both image classification and language understanding tasks confirm that models learned with Flooding can stably converge to solutions with better generalization ability, and behave consistently at instance-level."
SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a hierarchical reinforcement learning (HRL) approach for long-horizon tasks, where the agent must be able to effectively reason over long horizons, and to parse high-dimensional observations to infer the contents of a scene and its affordances. The authors propose a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that the approach improves long-term performance and enables better zero-shot generalization."
SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. The proposed method can replace i.i.d. generation in any VAE or GAN – it is easier to train and better captures complex dependencies in the data. Experiments on SetMNIST and QM9 demonstrate the effectiveness of the proposed method."
SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, the authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. The authors also propose a modified version of the proposed method and show that it can achieve minimax optimal bounds over Sobolev spaces. Empirically, following recent work which has shown that the deep model accuracy will improve with growing training sets according to a power law, they supply computational experiments to show the dimension dependent power law for deep PDE solvers."
SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper provides a theoretical analysis of the relationship between robustness and generalization in the context of domain generalization. The authors propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of the domain generalizability. The analysis, for the first time, shows that “robustness” is actually not the causation for domain generalisation; rather, robustness induced by adversarial training is a by-product of such function-class regularization."
SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the memorization effect in meta-learning, where the meta-knowledge simply memorizes all meta-training tasks discourages task-specific adaptation and poorly generalizes. The authors claim that the universal label space as a confounder to be the causing factor of memorization and frame the two lines of prevailing methods as different deconfounder approaches. The proposed causal perspective not only brings in the two deconfounder algorithms that surpass previous works in four benchmark datasets towards combating memorization, but also opens a promising direction for meta learning."
SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper tackles the problem of online ad hoc teamwork, i.e., when agents need to collaborate with previously unknown teammates on the fly. The authors propose a new reinforcement learning framework called ODITS, which allows the agent to adapt to arbitrary teammates in an online fashion. ODITS automatically learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the authors introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc team tasks."
SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a method for imputation of missing values in high-dimensional data. The proposed method, EMFlow, is based on an online version of Expectation-Maximization (EM) algorithm by using a normalizing flow (NF) model which maps the data space to a latent space. The method is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results are presented to illustrate the superior performance of EMFlow compared to a couple of recently available methods in terms of both predictive accuracy and speed of algorithmic convergence."
SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a novel interpretable deep linear gated networks (DLGN) for deep neural networks (DNNs) with rectified linear units (ReLUs). The authors extend the recently developed dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates are characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. In this paper, the authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN is not an alternative architecture per se, but a disentanglement and an interpretable re-arrangement of the computations in a DNN with ReLUs."
SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalizer for vision transformers, termed dynamic token normalization (DTN), which is designed to capture inductive bias such as the positional context in an image with LN. The authors claim that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. To tackle this problem, DTN is proposed to perform normalization both within each tokens (intra-token) and across different tokens (intertoken). Experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead."
SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper investigates the effect of regularization and data augmentation on the spectral behavior of deep neural networks (DNNs) trained with SGD. The authors propose two methods to measure the spectral bias of DNNs: (1) label noise, and (2) linear interpolation between test examples. They show that regularization methods such as weight augmentation, mixup, and label noise can affect the frequency content of the learned function in different ways. They also show that larger models learn high frequencies more readily than smaller ones."
SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the question of when to switch between exploration and exploitation modes in RL. The authors propose a set of algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. They also provide a promising analysis on Atari, using two-mode exploration and switching at sub-episodic time-scales."
SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median clustering problem in the general metric space, based on the construction of metric embedding tree structure of the data. The authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Theoretically, the authors show that the initial centers from HST initialized can achieve lower error than those from another popular initialization method, k-Median++, in the non-DP setting. Moreover, with privacy constraint, the error of applying DP local search followed by our private HST initial centers improves previous results, and approaches the known lower bound within a small factor."
SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a new video prediction model, named FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and how it is mitigated using existing image augmentation techniques. The proposed model outperforms the current SOTA models across four different video prediction benchmarks on four different metrics."
SP:6eb5ce1d85928a3af759d75016089c535941d0b0,This paper studies the influence of data structure on the test loss dynamics of stochastic gradient descent (SGD) on neural networks trained on features with arbitrary covariance structure. The authors propose an exactly solveable model of SGD which predicts test loss when training on features of the form f(x) = w · \phi(x). The authors show that the simpler Gaussian model can accurately predict test loss of nonlinear random-feature models and deep neural network trained with SGD on real datasets such as MNIST and CIFAR-10. 
SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the setting of non-convex, non-linear optimization problems. The authors consider the setting where the learning rate is fixed and the model is a neural network. They show that SGD converges to local maxima, but not to saddle points. They also show that AMSGrad may not converge to local minima, and that it may prefer sharp minima."
SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a novel hyperparameter optimization method for meta-learning. The proposed method approximates the second-order term with knowledge distillation by parameterizing a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The method allows online optimization and also is scalable to the hyper parameter dimension and the horizon length. The authors demonstrate the effectiveness of the proposed method on two different meta learning methods and three benchmark datasets.
SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a clustered task aware meta-learning (CTML) framework with task representation learned from both features and learning path. CTML first conduct rehearsed task learning from the common initialization, and collect a set of geometric quantities that adequately describes this learning path, by inputting this set of values into a meta path learner, we automatically abstract path representation optimized for the downstream clustering and modulation. To further save the computational cost incurred by the additional rehearsed learning, we devise a shortcut tunnel to directly map between the path and feature cluster assignments. Extensive experiments on two real-world application domains: few-shot image classification and cold-start recommendation demonstrate the superiority of CTML compared to state-of-the-art baselines."
SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near-OOD data. The proposed method relies on regularization to promote diversity on the OOD data while preserving agreement on ID data. Extensive comparisons of the proposed method with state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical images data sets reveal significant gains with negligible increase in computational cost.
SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,This paper proposes an encoder-decoder architecture for multi-agent trajectory prediction. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal and Social dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model is evaluated on the nuScenes vehicle motion prediction leaderboard and Argoverse vehicle prediction challenge.
SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,This paper proposes a baseline explanation technique for bias discovery using only the model’s output: input images are arranged in a grid grouped by the model's logit predictions. This design allows a user to inspect all attributes that potentially predict the target class. The authors also propose a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. The results show that the baseline outperformed concept-based explanations.
SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an approach to defend against backdoor data poisoning attacks on deep neural networks. The approach consists of two steps. First, it trains an ensemble of weak learners to automatically discover distinct subpopulations in the training set, and then leverage a boosting framework to exclude the poisoned data and recover the clean data. The algorithm is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. Empirically, the paper shows that the proposed approach significantly outperforms previous defenses."
SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, and maps the contextual encodings of these latent labels to the actual labels cooperatively. The correlations between labels, and between labels and the text are modeled indirectly through these latent-label embeddings and their correlations. The proposed method improves the state-of-the-art results on two widely used benchmark datasets by a large margin."
SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of deep convolutional kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers. The authors show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents regularities."
SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of computing the Neural Tangent Kernel (NTK) matrix for finite-width neural networks. The NTK is the outer product of the Jacobians of the neural network Jacobians, and it is useful for understanding training and generalization of NN architectures in the infinite-width limit. However, computing the NTK remains intractable for many architectures, and finite width corrections can be important to describe actual NNs used in practice. This paper provides the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. It further proposes two novel algorithms that change the exponent of the finite width NTK, dramatically improving efficiency."
SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper considers the problem of offline constrained reinforcement learning, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. The authors propose an algorithm, COptiDICE, that directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experiments show that the proposed algorithm outperforms baseline algorithms in terms of constraint satisfaction and return-maximization."
SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a novel parallel training scheme (called parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into multiple shorter sub-sequences and distributes each sub-sequence to different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach.
SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a manifold-regularized multi-decoder autoencoder (MRMD-AE) that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. The authors show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, as well as improves cross-subject translation of fMRI signal. The framework can be used for many downstream applications such as guided brain-computer interface (BCI) training."
SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper introduces new benchmarks for large-scale multi-class, multi-label, and multi-object anomaly detection. It also introduces a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. The authors conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large scale multi-classes, multi labels, and segmentation tasks."
SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,This paper studies the representation of tournaments in d-dimensions. It shows that tournaments can be represented in d dimensions if there exists a skew symmetric matrix M \in R^n of rank d such that a directed edge from i to j is present in T if and only if Mij > 0. It also shows that the flip class associated with a coned-doubly regular tournament of size O(d) must be a forbidden configuration. The paper also shows a lower bound of O(sqrt(n) on the minimum dimension needed to represent all tournaments on n nodes.
SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method to improve the performance of Neural Processes (NP) by adjusting the stochasticity of the attention weights. The authors claim that the proposed method encourages the context embedding to be differentiated from the target dataset, allowing the NPs to consider features in a target dataset and context embeddings independently. The proposed method is evaluated on 1D regression, predator-prey model, and image completion tasks, and is also validated on MovieLens-10k dataset."
SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"This paper proposes a method to improve interpretability of transformer language models (LMs) by incorporating prototype networks directly into the model architecture and hence explain the reasoning process behind the network’s decisions. The proposed method, Proto-Transformer Explanation (Proto-Trex) Networks, provides an explanation as a prototypical example for a specific model prediction, which is similar to (training-)samples with the label. To further enhance the prototypical network with human supervision, the authors propose an interactive learning setting (iProtoTrex), which allows users of any knowledge to give feedback and improve the model (Ribeiro et al., 2016), moving beyond a purely data-driven approach."
SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes a method for continual learning (CL) based on a notion of ‘trust region’ to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to reuse the frozen weights of the selected old tasks in the trust region. Experiments show that the proposed method achieves significant improvement over related state-of-the-art methods."
SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper studies the connection between optimization and generalization in machine learning. The authors propose a framework to connect optimization with generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. They show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalization bound, showing that short optimization paths after convergence indicate good generalization. Their framework can be applied to broad settings. They use it to obtain generalization estimates on three distinct machine learning models: underdetermined lp linear regression, kernel regression, and overparameterized two-layer ReLU neural networks."
SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, supported by theoretical and empirical findings. The analysis shows that adversarial example are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. Particularly, it highlights the glaring disparities between models trained on CIFAR-10 and ImageNet-derived datasets. Utilizing this framework, the authors analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency explanation for the commonly observed accuracy vs robustness trade-off."
SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper proposes a method to improve the performance of graph neural networks (GNNs) by addressing heterophily. The authors first show that not all cases of heterphily are harmful for GNNs with aggregation operation. Then, they propose a new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. Based on the metrics and the observations, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophilies. They validate the ACM-augmented baselines with 10 real-world node classification tasks."
SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a method to improve the generalizability of deep reinforcement learning (RL) solvers for solving the traveling salesman problem (TSP). The main idea is to use equivariance to facilitate training, and to use local search as a tool to smooth the objective function optimized during RL training. The proposed method is evaluated on random and realistic TSP problems against relevant state-of-the-art deep RL methods."
SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a framework for federated learning (FL) that aims to address the non-IID (independent and identically distributed) data distribution among clients. Specifically, each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation. To ensure privacy, the local GANs are trained with differential privacy by adding artificial noise to the local model gradients, and the PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold. Extensive experiments demonstrate the effectiveness of the proposed framework."
SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper studies the trade-off between accuracy and adversarial robustness for smoothed classifiers, i.e., increasing the robustness of a classifier for an input can be at the expense of decreased accuracy for some other inputs. In this paper, the authors propose a simple training method leveraging this tradeoff for obtaining more robust and sample-wise control of robustness over the training samples. Specifically, they propose to use the “accuracy under Gaussian noise” as an easy-to-compute proxy of adversarial perturbations for each input and differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method exhibits improved certified robustness upon existing state-of-the-art training methods."
SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper proposes two packing algorithms for speeding up BERT pretraining. The first algorithm, shortest-pack-first histogram packing (SPFHP), is based on the assumption that sequences are interchangeable and therefore packing can be performed on the histogram of sequence lengths, rather than per sample. The second algorithm, non-negative least-squares histogram-packing (NNLSHP) is more depth efficient and achieves near optimal packing by combining a maximum of 3 sequences in one sample. Both algorithms are fast and have linear complexity in dataset size."
SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, the proposed algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in auto-regressive models. Empirically, the authors show that the proposed method finds outputs with substantially better model scores compared to beam search and reranking techniques in models whose scores do not decompose additively."
SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method for anomaly detection based on energy based generative models (EBM). The main idea is to learn to associate low energy to correct values and high energy to incorrect values, which alleviates the intractable problem of modeling the world of anomalies. An adaptive sparse coding layer is also proposed to avoid training an anomaly detector for every task, which is a plug and play feature that can be used to quickly update what is normal during inference time."
SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of Question Answering (QA) in the presence of both human-written and machine-generated misinformation in context documents. The authors create a large-scale dataset, CONTRAQA, which contains over 10,000 contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. They also propose a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation learning that uses the distance between states between the different spaces of the agents to align and compare states. The authors provide a theoretical analysis of the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. They demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformation of state-action space."
SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,This paper proposes a method for self-supervised learning based on hierarchical cross-level contrastive learning (HCCL) to further distill the information mismatched by the conventional contrastive loss. HCCL uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. The experimental results show the effectiveness of the proposed method in improving the robustness of the pretraining model for downstream tasks.
SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a multi-agent deep reinforcement learning (RL) framework to learn Nash equilibria for dynamic general equilibrium models (DGE) with heterogeneous, interacting strategic agents of various agent types, such as consumers, firms, and governments. The authors propose to use structured learning curricula and efficient GPU-only simulation and training. The approach is more flexible and does not need unrealistic assumptions, e.g., market clearing, that are commonly used for analytical tractability. The paper demonstrates the approach in real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms and a government who taxes and redistributes. "
SP:f885c992df9c685f806a653398736432ba38bd80,This paper proposes a method to prevent model stealing by requiring users to complete a proof-of-work before they can read the model’s predictions. The method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen. The key idea is to wrap model predictions in a proof of work (PoW) problem to force users to expend additional compute on solving a PoW puzzle before being able to read the output predicted by the model. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction.
SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a multi-resolution variant of Continuous Normalizing Flows (CNFs) for generative models of images. CNFs have been shown to be capable of modelling complex distributions such as those associated with images. This paper characterizes the conditional distribution over the additional information required to generate a fine image that is consistent with the coarse image. The authors introduce a transformation between resolutions that allows for no change in the log likelihood. They show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time, using only one GPU."
SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect label noise in deep neural networks (DNNs). The main idea is to first identify “neighbor” instances for each training example whose representation extractor can be fully independent of noisy supervisions by referring to unsupervised learning (e.g., self-supervised, self-learning, etc.). Based on the neighborhood information, the authors propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations."
SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a new adversarial attack method called Policy Adversarial Director (PA-AD) for adversarial reinforcement learning (DRL). The main idea is to train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in high-dimensional state spaces. The proposed algorithm is theoretically optimal and significantly more efficient than prior RL- based works in environments with large state spaces, and the proposed method outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments."
SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy seeking method for multi-agent reinforcement learning (MARL) based on constrained optimization. The authors propose a novel metric to measure the distance between policies, and then propose a constrained optimization method to solve the constrained optimization problem. The proposed method is evaluated on several continuous control benchmarks, showing that it can generate diverse and well-performing policies. "
SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes a method for removing reverberation effects from audio and visual observations of the environment surrounding a speaker. The method is based on the idea that the visual environment surrounding the speaker reveals important cues about the room geometry, materials, and speaker location, which influence the precise reverberation effect in the audio stream. The authors develop a large-scale dataset that uses realistic acoustic renderings of speech in real-world 3D scans of homes offering a variety of room acoustics. The proposed method is evaluated on both simulated and real imagery for speech enhancement, speech recognition and speaker identification, and achieves state-of-the-art performance."
SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a simple and efficient position method, Attention with Linear Biases (ALiBi), which does not add positional embeddings to word embedding; instead, it biases query-key attention scores with a penalty that is proportional to their distance. The authors show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to inputs of length 2048 but training 11% faster and using 11% less memory. The inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark."
SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the multi-objective online convex optimization (MO-OCO) problem. The authors propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or an L1-regularized solver. They derive regret bounds of both variants and show that the L1 regularized variant enjoys a lower bound. Extensive experiments demonstrate the effectiveness of the proposed algorithm."
SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative negative replay approach for continual learning. The approach is based on the idea that the generated data can be used as negative examples (or antagonists) to learn the new classes, especially when the learning experiences are small and contain examples of just one or a few classes. The proposed approach is validated on two datasets (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences, a setup where existing generative replay approaches usually fail."
SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework, which is applied to the problem of node partitioning across multiple associated graphs of a system or scenario. IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. The trained model is then generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines."
SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a multiresolution equivariant message passing graph variational autoencoder (MGVAE) to learn and generate graphs in a multireferential manner. The key idea of MGVAE is to construct a series of coarsened graphs along with a hierarchy of graphs with a higher order message passing. The proposed framework is end-to-end permutation-equivariant with respect to node ordering. Experiments on several graph generation tasks including general graph generation, molecular generation, unsupervised molecular representation learning to predict molecular properties, link prediction on citation graphs, and graph-based image generation."
SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper proposes a framework for nonlinear independent component analysis (ICA), in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. The authors provide an insightful proof of the identifiability of the proposed framework, and verify the theory by experiments on artificial data and synthesized images. Moreover, the framework can disentangle interpretable features."
SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional operator, termed BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. It decomposes multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture. The authors validate the proposed architecture through extensive experiments on various graph classification tasks."
SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or look, to improve the transferability of supervised pretraining methods to downstream tasks. The authors argue that the worse transferability is due to the neglect of valuable intra-class semantic difference. To alleviate this problem, the authors propose a new method that only requires each image to share its class label with most of its k nearest-neighbor, thus allowing each class to exhibit a multi-mode distribution and preserving part of intra- class difference for better transferring to the downstream tasks, which alleviates the overfit of upstream tasks. Extensive empirical studies on multiple downstream tasks show that look outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method to generate facial details that are consistent with any desired target expression from a single image. The facial details are represented as a vertex displacement map and used by a Neural Renderer to photo-realistically render novel images of any single image in any desired expression and view. Given a single input image, a target expression (in this case ‘Happy’), and an initial detailed geometry extracted using FDS (Chen et al., 2019) as input, the proposed method hallucinates facial geometric details consistent with the target expression. The hallucinated details are added to the smooth proxy geometry (marked as ‘Proxy Shading’, also extracted from Chen et al. 2019) to give a detailed geometry with facial details. The detailed geometry is then rendered using Neural Rendering to give the final image."
SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) for disentanglement of syntactic roles. The model is trained on the SNLI dataset (Schmidt et al. 2020) and evaluated on English text. The authors show that the model is able to generate disentangled representations of sentences where different syntactic role correspond to clearly identified different latent variables. In addition, the authors also show that realizations of syntactical roles can be separately modified in sentences by controlling the lexical realization."
SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a method for improving coordination among agents in multi-agent reinforcement learning (MARL). In particular, the authors propose an intrinsic reward function that encourages agents to maximize the expected sum of extrinsic rewards. The proposed method is evaluated on the StarCraft Multi-Agent Challenge (SMAC) benchmark. The results show that the proposed method outperforms the baselines."
SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of large pre-trained neural networks. The proposed method, Model Editor Networks with Gradient Decomposition (MEND), is a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre- trained model’s behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. Experiments on T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters."
SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper introduces a physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in a compositional manner. Results on both one- and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents an analysis of the brain representations of code generated by unsupervised machine learning (ML) models and representations of computer programs in the human brain. The authors analyze recordings—brain representations—from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They discover brain representations that encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also map brain representations to representations of a suite of ML models that vary in their complexity. They find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties, as well as machine learned representations."
SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model, which consists of a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The authors also propose a new method for retaining the source speaker’s voice in the translated speech. Experiments show that the proposed model outperforms the original Translatron by a large margin in terms of translation quality and predicted speech naturalness."
SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of few-shot attribute learning, i.e., the task of learning new attributes that are previously not labeled in the dataset. The authors propose an attribute-based learning framework, where the attribute space is split into training and test attributes, and the task is to predict the predictability of a model’s ability to generalize to the new attributes. The proposed method is evaluated on three datasets, Celeb-A, Zappos-50K and ImageNet-with- Attributes, and it is shown that supervised learning with training attributes does not generalize well to new attributes, whereas self-supervised pre-training brings significant improvement."
SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The proposed method is based on the Wasserstein gradient flow of relative entropy, which is characterized by an ODE system with velocity fields depending on the density ratios of the evolving particles and the target density. The authors propose a novel nonparametric approach to estimate the logarithmic density ratio using neural networks. Extensive simulation studies on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate the effectiveness of the proposed method."
SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a framework to classify larger, realistic images using quantum systems. The approach relies on a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. The framework is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop, and obtains accuracy comparable to classical neural networks with the same number of learnable parameters. The authors also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance."
SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated face recognition framework called PrivacyFace, which improves the federated learning face recognition by communicating auxiliary and privacy-agnostic information among clients. The proposed framework consists of two components: a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which results in more discriminative features. Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of the method."
SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a method for transfer learning, where the goal is to improve performance in a data-scarce target domain using a model pretrained on the data-rich source domain. The authors propose a method, Head-to-Toe probing (HEAD2TOE), that selects features from all layers of the source model to train a classification head for the target-domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows that it outperforms fine-tuning on average, but critically, for out-of-distribution transfer, for the out of distribution transfer, Head2Toe outperforms finetuning. "
SP:d6f11fb32851f97af287f962f83220d27a8bc76a,This paper proposes an object-oriented text dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. The model identifies the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. The authors develop variational objectives under the object-supervised and self-supervision settings to model the stochasticity of predicted dynamics. Empirical results show that the proposed model significantly outperforms model-free baselines in terms of sample efficiency and running scores.
SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification, where a label taxonomy has a cost function that represents the cost of (wrong) predictions at different levels of the hierarchy. In this paper, the authors propose a method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. They show that there is a bijective mapping between the original hierarchical loss and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learning to abstain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems."
SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks to process multi-channel sound waveforms that have limited time-frequency resolution capability. By alternating the periodicity term, the proposed method can obtain a set of synperiodical filter banks, where each bank differs in its temporal length and frequency resolution. The proposed method is evaluated on both direction of arrival estimation task and physical location estimation task."
SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of unsupervised domain adaptation, i.e., shifting the model towards the target distribution rather than learning domain invariant representations. The authors propose a method called “Gradual Interpolation of Features toward Target” (GIFT) that creates virtual samples from intermediate distributions by interpolating representations of examples from source and target domains. They show that under two assumptions: (i) access to intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self-training can be successfully applied on gradually shifted samples to adapt the model toward the target distributions."
SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for zero-shot video-text retrieval, where the goal is to retrieve the correct class label from a video. The authors propose a method that learns representations from videos, titles and comments, which are abundant on the internet. They introduce an attention-based mechanism that allows the model to disregard text with irrelevant content. They demonstrate that, by using comments, their method is able to learn better, more contextualised, representations, while also achieving competitive results on standard benchmarks."
SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a new intrinsic reward for unsupervised skill discovery. The authors argue that the exploration problem is intrinsic to the entire class of variational skill discovery algorithms, which can inhibit discovery of new skills. To solve this problem, the authors propose an ensemble of discriminators and reward the policy for their disagreement. The proposed method is evaluated on a tabular grid world and the Atari suite."
SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a method for generating molecules using deep neural networks (DNNs). The proposed method is based on the construction of a spanning tree and the residual edges. The authors also design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Fréchet ChemNet distance, and fragment similarity."
SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper provides a spectral analysis of the Neural Tangent Kernel (NTK) of Polynomial Neural Networks (PNNs), a recently proposed parametrization of two-layer polynomial neural networks. The authors show that the NTK of PNNs has a spectral bias towards low-frequency functions, which leads to faster learning of high-frequency components during training. They show this bias through extensive experiments on synthetic and real-world datasets."
SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method for finding sparse subnetworks in randomly initialized neural networks. The method is based on the idea of “disguised subnetwork”, i.e., a sparse subnetwork that is not only “hidden” in the random networks but also can only be “unmasked” with certain transformations on the weights. The authors propose a two-stage algorithm that plays a Peek-a-boo (PaB) game to identify the disguised subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the remaining weights. Extensive experiments on CIFAR-10/100 datasets demonstrate the competency of the proposed method."
SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,This paper proposes a method to defend against adversarial attacks on graph neural networks (GNNs) by jointly cleaning the graph and denoising features. The authors propose a General Unified Graph Neural Network (GUGNN) framework to jointly clean the graphs and denoise features of data. They further extend it by introducing two operations and develop a robust GNN model(R-GNN). Experiments on four real-world datasets demonstrate the effectiveness of the proposed method.
SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns surface parameterization by learning a continuous bijective mapping between 3d surface positions and 2D texture-space coordinates. The surface paramization network can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. The paper shows that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,This paper proposes an adaptive region pooling (ARP) method for fine-grained recognition. The main idea is to combine the benefits of strided convolution and bilinear downsampling operations to achieve a trade-off between the scale of receptive field and the granularity of feature. The proposed method is evaluated on image classification and image retrieval tasks. The results show that the proposed method outperforms the baselines.
SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper studies the problem of out-of-distribution (OOD) generalization for node-level prediction on graphs. The authors propose a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that they design multiple context explorers (specified as graph editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node level prediction. They prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution."
SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning approach that adaptively selects optimal data augmentation strategies for contrastive representation learning. The proposed approach is based on two criteria: high fidelity and variety. The paper also provides a theoretical analysis on the criteria for selecting feasible data augmentations. Experiments on various datasets show that InfoTS outperforms the baselines. "
SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the lottery ticket hypothesis (LTH) for deep neural networks (DNNs) and the universality of lottery tickets. The authors use renormalization group theory to show that iterative magnitude pruning (IMP), the method used for discovering winning tickets, is an RG scheme. They show that IMP satisfies the properties required to be an RG operator. They also show that transfer experiments for winning tickets allow successful transfer of winning tickets."
SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the question of whether the performance gap between cross-attention (CA) and dual-encoder (DE) models is due to a limitation in the capacity of DE models or in the training of such models. The authors show theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. They also show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, they propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural re-ranking datasets."
SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the relationship between importance sampling (IS) and variance minimization (VM) in policy optimization. In particular, the authors consider the case where the goal is to estimate the performance of a target policy, given samples collected with a different behavioral policy. In this setting, IS is used as a what-if analysis tool, and VAE is used to find an optimal policy. The authors show that VAE can be used as an inner loop in the policy optimization algorithm, and propose a new algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that uses VAE as the inner loop. POPE is evaluated on Mujoco continuous control tasks, with a focus on the robustness to small batch sizes."
SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes a graph parallelism method to scale up graph neural networks (GNNs) for predicting per-atom forces and energies on the Open Catalyst 2020 (OC20) dataset. The authors propose a method to partition the input graphs across multiple GPUs, enabling them to train very large GNNs with hundreds of millions or billions of parameters. The proposed method is evaluated by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. The results show that the proposed method leads to relative improvements of 15% and 21% on the force MAE metric on the S2EF and AFbT metrics on the IS2RS tasks."
SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a method for generating long-range text by learning a representation that maps the dynamics of how text changes in a document to dynamics of a stochastic process of interest. This representation is used to generate text by first implicitly generating a document plan, and then generating text that is consistent with this latent plan. The proposed method is evaluated on a variety of text domains and compared to domain-specific methods and fine-tuning GPT2. "
SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning (OCRL) by learning to predict future scenes in the presence of moving objects. It treats objects as latent causes whose function to an agent is to facilitate efficient prediction of the coherent motion of their parts in visual input. The model learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together."
SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,"This paper proposes a disentangled representation learning method for mobility forecasting. The authors propose a VAE-based architecture for learning the disentanglement representation from real spatio-temporal data. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. The method is able to achieve state-of-the-art performance across multiple spatiotemporal datasets. Moreover, the authors investigate the effectiveness of eliminating the non-informative features from the learnt representations and show that models can benefit from this operation."
SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"This paper proposes a framework for probabilistic interpolation of irregularly sampled time series. The proposed method is based on the Multi-Time Attention Network (mTAN) model, a variational autoencoder (VAE) architecture for continuous-time interpolation. The main contribution of this paper is the introduction of a heteroscedastic output layer to enable variable uncertainty in output interpolations. The experiments show that the proposed model is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models."
SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two measures of modularity, i.e., importance and coherence, to measure the modularity of subsets of neurons in a neural network. The authors propose a clustering method based on spectral clustering of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights, reveal groups of neurons that are important and coherent."
SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the possibility of using lottery ticket hypothesis to discover lightweight speech recognition models that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. They obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights."
SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes a deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones entirely. The authors augment the standard ResNet architectures with a few extra skip connections and Hadamard transforms, which allows us to start the training from zero and one entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. The experiments show that ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet."
SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. The Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data.
SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD. The authors first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist permutations that offer exponentially faster convergence compared to random. Then, they show that there are easy-to-construct permutations for quadratic, strongly-convex functions that lead to accelerated convergence. Finally, they suggest that a general characterization of optimal permutations cannot capture the nuances of individual function classes, and can mistakenly indicate that one cannot do much better than random."
SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes an automated normalizing flow (NF) architecture search method. The method aims to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes to measure the expressiveness and learnability of representations learned by self-supervised learning (SSL) methods by using the Intrinsic Dimension (ID) of the dataset in representation space. To measure expressiveness, the authors introduce Cluster Learnability (CL), defined in terms of the learning speed of a KNN classifier trained to predict K-means cluster labels for held-out representations. The authors show that high ID and CL correlate with high downstream performance for 30 different pretrained checkpoints across different architectures. To further demonstrate the utility of the framework, they propose modifying DeepCluster (Caron et al., 2018) to improve the learningability of the representations. KNN-DeepCluster outperforms the state-of-the-art on both STL10 and ImageNet benchmarks."
SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box adversarial attacks such that the perturbations are limited in the salient region. The authors show that state-of-the-art blackbox attacks equipped with segmentation pretors can achieve much better imperceptibility performance with little reduction in query efficiency and success rate. They further propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptible performance by refining perturbation in the saliency region. "
SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a graph-to-sequence architecture called Graph2SMILES to solve the tasks of retrosynthetic analysis and reaction outcome prediction. The proposed architecture is a drop-in replacement for the Transformer in any task involving molecule(s)-to-molecule(s) transformations. The authors first design a sequential graph encoder with an attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding. The experimental results show that the proposed architecture improves the top-1 accuracy of Transformer baselines by 1.7% and 1.9% on the USPTO_480k and USPPTO_STEREO datasets respectively."
SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical reinforcement learning approach for disease diagnosis in task-oriented dialogues setting. The high level policy consists of a master model that is responsible for triggering a low level model, which consists of several symptom checkers and a disease classifier. The proposed framework imitates a group of doctors from different departments to diagnose a patient together, among them, each worker acts like a doctor from a specific department. When workers are sufficient, the master would activate separate disease classifiers to make the final diagnosis based on information collected by workers. "
SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing the challenges of label deficiency at the edge and data heterogeneity. The authors also develop a distributed training system and related evaluation protocol for SSFL. Experiments on a synthetic non-I.I.D. dataset based on CIFAR-10 and an intrinsically non IID dataset GLD-23K show that the performance gap of evaluation accuracy between supervised learning and unsupervised learning in FL is small and reasonable."
SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper attempts to derive a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, the adjustment operator is the solution operator of the PDE. Theoretically, the robustness of DNN trained with our method is certifiable and our training method reduces the generalization gap for DNN."
SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper investigates the emergence of emergent languages in referential games, where agents interact and develop an emergent language to solve a task. The authors propose to use contrastive loss to improve the generalization performance of the speaker and listener agents. They also propose a new message type collapse loss to alleviate the problem of message collapse. The results show that the proposed method is able to generalize better than the existing referentially-based language games."
SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper extends the Sample Average Uncertainty (SAU) developed in Rigotti & Zhu (2021) for bandit problems to the general sequential RL setting. In particular, the authors propose a new exploration strategy called $\delta$-exploration that extends SAU from bandits to general sequential Reinforcement Learning (RL) scenarios. This extension results in a class of exploration strategies for RL that share the simplicity of SAU bandits as well as its effectiveness. The authors empirically study the effectiveness of the proposed strategy in tabular and deep Q-learning settings."
SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes an implicit generative adversarial network (DIGAN) for video generation. DIGAN is based on implicit neural representations (INRs) that encodes a continuous signal into a parameterized neural network. The authors propose two modifications to the video generator: (1) an INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently and (2) a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. The proposed method is evaluated on UCF-101 and CIFAR-10 datasets, and it is shown that the proposed method outperforms the baselines."
SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of differentially private multi-label classification, i.e. the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. This task has been understudied in the machine learning literature despite its prevalence in many domains such as healthcare. The authors propose three new mechanisms: Binary, $\tilde{\phi}$ and Powerset voting. Binary voting operates independently per label through composition, $\phi$ voting operates over the entire binary vector by viewing the possible outcomes as a power set, and powerset voting is a mechanism that reveals the result for all k candidates jointly. The paper theoretically analyzes the tradeoffs between Binary voting and $\phi$, and shows that the former requires strong correlations between labels to outperform Binary voting."
SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a technique of learning rate grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Besides, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning."
SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes an algorithm for online reinforcement learning in sparse-reward settings, where the reward function can only indicate whether the task is completed partially or fully. The authors propose an algorithm called Learning Online with Guidance Offline (LOGO) that merges a policy improvement step with an additional policy guidance step by using the offline demonstration data. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimal policy, while being able to learn beyond and approach optimality. Theoretical analysis of the algorithm is provided, and a lower bound on the performance improvement in each learning episode is also provided."
SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a two-stage Pareto solver for multi-objective optimization (MOO) problems with non-convex functions and constraints. The proposed method is based on two stages: Stage-1 and Stage-2. The first stage uses a neural network to extract the weak points, and the second stage extracts the strong points from the weak ones. The paper provides a theoretical analysis of the proposed method, and conducts experiments on synthetic and real-world datasets."
SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of representation consolidation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors propose a simple multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from task specific teacher(s). The proposed method improves downstream performance, outperforming the teacher (or best of all teachers) as well as the strong baseline of ImageNet pre-trained features. The method almost reaches the performance of a multi- task joint training oracle."
SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. The authors develop a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches."
SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes an approach to query object localization, where an agent is trained to localize objects of interest specified by a small exemplary set. The proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class."
SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a transformer-based feature matching method for image classification, stereo matching, object detection, and object detection. The proposed method builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top-K patches. Experiments show that the proposed method achieves state-of-the-art performance in various vision tasks, e.g. 2.7% improvement in feature matching on ScanNet, 50% reduction in stereo matching."
SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper considers the problem of learning reusable temporally extended actions, or options, in reinforcement learning. The authors propose a method for learning termination conditions of options by maximizing mutual information (MI) between options and corresponding state transitions. They derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards, combined with intrinsic rewards."
SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a novel approach for open-world object detection, i.e. a detector that can not only identify known objects, but also discover object instances of unknown categories, and incrementally learn to categorize them when their annotations progressively arrive. The proposed approach is based on semantic topology, where all object instances from the same category are assigned to their corresponding pre-defined node in the semantic tree, including the ‘unknown’ category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed approach outperforms the current state-of-the-art open world object detectors."
SP:97f618558f4add834e5930fd177f012a753247dc,"This paper studies the problem of selecting a subset of the unlabeled data for training deep neural networks. The authors propose a greedy algorithm based on maximization of a submodular function, which can combine different selection objectives without sacrificing the approximation guarantees. They propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees, and show that it can identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset."
SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper studies the problem of semantic segmentation from synthetic data. The authors propose an adaptive inference strategy that adjusts the model to the test sample before producing the final prediction. They achieve this by using Instance-adaptive Batch Normalization (IaBN) to modify normalization layers by combining the feature statistics acquired at training time with those of the test samples. They also introduce a test-time training (TTT) approach, Seg-TTT, which adapts the model parameters to the tested samples using a self-supervised loss. The results show that the proposed method achieves a new state-of-the-art in segmentation accuracy."
SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper considers the task of finding out-of-distribution samples in tabular data, where little can be assumed on the structure of the data. The authors assume that the way in which a subset of variables in the feature vector is related to the rest of the variables is class-dependent. In this manner, for a given input sample xi \in Rd, we obtain a set of pairs {(aji, b j i )}mj=1, where a j i is a vector of k consecutive features from xi, and b j is the vector of all other feature values, and m = d-k + 1. All vectors aji have the same length and vary according to the first coordinate of xi. From which the subsets are collected. "
SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a method to learn low-dimensional embeddings of neural correlates that preserve the diagnostic attributes of represented disorders. The proposed method leverages the available diagnostic information in learning the optimal embedding space by proposing a novel type of conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The method is evaluated on two empirical neuropsychiatric neuroimaging datasets and discovers a reliable nosological relation among autism spectrum disorder, major depressive disorder, and schizophrenia."
SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding (Tensor product encoding). Theoretical analysis is provided to characterize the representation power of input features and the proposed QTN enables an end-to-end parametric model pipeline, from the generation of quantum embeddings to the output measurement. Experiments on the MNIST dataset demonstrate the advantages of the proposed method over other quantumembedding approaches."
SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method to construct low-dimensional manifolds of the space of neural network models, where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The proposed method, called DYNAMO, takes as input a collection of pre-trained neural networks and outputs a meta-model that emulates the dynamics of the hidden states as well as the outputs of any model in the collection. The specific model to be emulated is determined by a model embedding vector that the meta-models take as input; these model embeddings constitute a manifold corresponding to the given population of models. The authors apply the proposed method to both RNNs and CNNs, and find that the resulting manifold enables novel applications: clustering of neural networks, model averaging, and semi-supervised learning."
SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a framework for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The authors implement their method using a graph neural network as the constraint function and gradient descent as the constraints solver. They test the model on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids. The model achieves better or comparable performance to top learned simulators."
SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes an evolutionary diversity optimization algorithm with clustering-based selection. In each iteration, the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on deceptive and multi-modal continuous control tasks show the superior performance of EDO-CS over previous methods."
SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of distributed linear stochastic approximation algorithms driven by Markovian noise and general consensus-type interaction. The interconnection structure among the agents is described by a time-varying directed graph. The paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. The equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication."
SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes Graph Mechanics Network (GMN) which is equivariant, constraint-aware, and constraint-sensitive. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Moreover, the authors developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Extensive experiments support the advantages of the proposed GMN compared to the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency."
SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper studies federated learning with partial model personalization. The authors propose two algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. They give convergence analyses of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalisation with a small fraction of personalized parameters, and the alternating update algorithm often outperforms the simultaneous update algorithm."
SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a framework for unsupervised learning of object representations, called Contrastive Learning Through Time (CLTT). The main idea is to replace the contrastive augmentation operations in contrastive learning (e.g. scaling, flipping, cropping, rotation, etc) with successive views in an unsegmented viewing sequence, which allows perfect control over the temporal structure of the input and allows us to ask the following two questions. First, can CLTT approach the performance of fully supervised learning? Second, if so, what are the required conditions on the temporal structures of input? To answer these questions, the authors develop a new data set using a near-photorealistic training environment based on ThreeDWorld (TDW). They consider several state-of-the-art contrastive methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object, which leads to increased representational similarity between these objects, reminiscent of classic neurobiological findings."
SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes an extension of AlphaZero with Hindsight experience replay (HER) to tackle goal-directed planning tasks. HER is a reinforcement learning approach that aims to improve the performance of a parametrized policy and value network by learning a policy that performs well in the whole state space. In particular, the authors propose to use a reward function based on the state distance between the current state and the goal state to guide the exploration. HER can be used with any policy-based RL algorithm to extend the AlphaZero algorithm. The authors demonstrate the effectiveness of the proposed approach through an extensive empirical evaluation in several simulated domains, including a novel application to a quantum domain."
SP:e2d33c7331db7f52b84ad1018152564d91a9f126,This paper proposes Recursive Gradient Optimization (RGO) for continual learning. RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer (FEL) that represents different network structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines.
SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper provides an in-depth study of the properties and applications of aligned generative models for image-to-image translation. The authors empirically analyze aligned models and provide answers to important questions regarding their nature. They show that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Then, equipped with this better understanding, they leverage aligned models to solve a diverse set of tasks such as zero-shot vision tasks, cross-domain image morphing and fine-tuning."
SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a new Gromov-Wasserstein (GW) distance based on Optimal Transport (OT) distance, which relaxes the coupling between all the nodes from the two considered graphs. The authors argue that this property can be detrimental for tasks such as graph dictionary or partition learning, and propose a new semi-relaxed GW distance to alleviate this issue. The proposed GW distance is shown to be computationally efficient, and it can be used for efficient graph dictionary learning algorithms. Experiments are conducted to demonstrate its relevance for complex tasks on graphs such as partitioning."
SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method to model systematic suboptimality, i.e. systematic deviations from optimal behavior that are not independent, but instead consistent over time. The authors introduce the Boltzmann policy distribution (BPD) as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free post-training quantization method for convolutional neural networks. The proposed method is based on minimizing Constrained Absolute Sum of Error (CASE) of weights as the rounding metric. CASE is derived from the second-order information of DNN task loss and decomposes the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, progressively compose sub- items and propose a novel objective in the discrete domain, which does not need any dataset and is not aware of network architecture. "
SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation, i.e., splitting time series into segments that correspond to given categories. The proposed method, SegTime, is based on a bi-pass neural network architecture with several structures that can process information in a multi-scale fashion. The authors claim that SegTime obviates sliding windows, handles long-term dependencies, and is insensitive to the label changing frequency. Experiments are conducted to evaluate the effectiveness of SegTime."
SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a new explanation method for graph neural networks (GNNs) by decomposing the information generation and aggregation mechanism of GNNs. The proposed method, called DEGREE (Decomposition based Explanation for GRaph nEural nEtworks), provides a faithful explanation for GNN predictions by tracking the contributions of specific components of the input graph to the final prediction. The authors further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of the proposed method can be further improved by utilizing GNN characteristics. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the method."
SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper introduces DiffStride, a new method for learning the stride of pooling layers in convolutional neural networks. The main idea is to learn the strides of the strided convolution layers, which is a critical hyperparameter of the pooling layer. The authors propose a learnable stride that is learned in the Fourier domain. The proposed method is evaluated on CIFAR-10, Cifar-100, and ImageNet. "
SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each prediction is associated with a small receptive field. The certificate is based on randomized smoothing, where the random perturbation strength for different input regions is proportional to their importance for the outputs. The resulting locally smoothed model yields strong collective guarantees on both image segmentation and node classification tasks."
SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases into normalizing flows. In particular, the authors propose a method to convert differentiable probabilistic models into equivalent bijective transformations and introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. They demonstrate that EMFs can be used to induce desirable properties such as multimodality, hierarchical coupling and continuity, and enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. They also show that this approach outperforms a large number of alternative methods in common structured inference problems."
SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of large pre-trained language models (GPT-2 and GPT-3) to learn to map an entire conceptual domain (e.g., direction or color) onto a grounded world representation given only a small number of examples. Specifically, the authors show a model what the word “left” means using a textual depiction of a grid world, and assess how well it can generalize to related concepts, for example, “right”, in a similar grid world. They find that although the small language models that contain an order of 100M) cannot perform either generalization generalization or prediction errors, the largest model (a GPT3 model containing 175B) can indeed learn groundings in the conceptual worlds we build."
SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the effect of shaped rewards in emergent language research. In particular, the authors show that shaped rewards can bias the semantics of the learned language, significantly change the entropy of the language, and mask the potential effects of other environmental variables of interest. The authors introduce a mathematical model for understanding the role of experience buffer size in the emergent experience size of the environment, and demonstrate the impact of the size on the entropy and the semantics. The experiments were performed with a novel emergent-based environment with a sender-receiver navigation task."
SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. Experiments show that IWDA outperforms the more popular semi-supervised learning methods under large prior shifts, and can be additionally combined with them for further performance gains. "
SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a meta-learning algorithm that first bootstraps a target from the metalearner, then optimizes the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors establish conditions that guarantee performance improvements and show that the metric can control meta-optimization. Meanwhile, the bootstrapping mechanism can extend the effective meta learning horizon without requiring backpropagation through all updates. The proposed algorithm achieves a new state-of-the-art for model-free agents on the Atari ALE benchmark and yields both performance and efficiency gains in multi-task meta learning."
SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper investigates the generalization ability of model-based agents in comparison to their model-free counterparts. The authors evaluate the performance of MuZero on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, MuZero achieves state-of-the-art generalization performance and data efficiency on Procgen and the multi-task Meta-World benchmarks (ML-1 and ML-45 train-train). They also show that these factors do not always provide the same benefits for task generalisation in Meta-world, indicating that transfer remains a challenge."
SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph deconvolution network (GDN) to learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. The authors propose a graph convolutional relationship between the observed and latent graphs, and formulate the graph learning task as a network inverse (deconvolution) problem. The architecture is derived from the principle of algorithm unrolling used to learn fast approximate solutions to inverse problems (Gregor & LeCun, 2010; Sprechmann et al. 2015; Monga et al., 2021), an idea that is yet to be explored in the context of graph structure identification. The experiments demonstrate the applicability of GDNs to infer brain structural connectivity from functional networks obtained from the Human Connectome Project-Young Adult neuroimaging dataset."
SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a framework for reward shaping in reinforcement learning. The proposed framework is based on a two-player nonzero-sum Markov game, where one agent (Shaper) learns to add and calibrate shaping rewards, and the other agent (controller) learns the optimal policy using these shaped rewards. The authors prove that the proposed framework, called Reinforcement Learning Optimising Shaping Algorithm (ROSA), learns to construct a shaping-reward function that is tailored to the task thus ensuring efficient convergence to high performance policies. Experiments are conducted on three Mujoco environments, where the proposed method outperforms the baselines."
SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a method to improve the robustness of vertical federated learning (VFL) against backdoor attacks and adversarial attacks. In particular, the authors propose a robust feature subspace recovery (RVFR) training and inference framework that can recover the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. The authors conduct extensive experiments on NUS-WIDE and CIFAR-10 datasets and show that RVFR outperforms different baselines in terms of robustness against diverse types of attacks."
SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes to use contrastive learning as a pretext task to train unsupervised dense retrievers for document retrieval. The paper shows that the proposed method outperforms BM25 on 8 out of 14 BEIR datasets. It also shows that fine-tuning the model on the few-shot MS MARCO dataset with a few thousand examples improves the performance.
SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the trade-off between fine-tuning (updating all the model parameters) and linear probing when transferring a pretrained model to a downstream task, especially when the pretrained features are good and distribution shift is large. On six distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CifAR10.1, FMoW), the authors show that fine-tuneing obtains an average 2% higher accuracy ID but 6% lower accuracy OOD than linear probing. The authors theoretically analyze the tradeoffs arising in fine- tuning overparameterized two-layer linear networks, characterizing how fine-tanning can distort high-quality pre-trained features which leads to low OOD accuracy. The analysis suggests that the simple two-step strategy of linear probing then full fine tuning can achieve better ID and OOD performance than fine tuning."
SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning to discover novel classes (L2DNC), where the goal is to cluster novel classes together with known classes. The authors argue that the problem is well-defined if the novel classes and known classes share high-level semantic features. Then, the authors propose a meta-learning approach to solve the problem, which is based on the assumption that seen and unseen classes share semantic features, and show that it is theoretically solvable under certain assumptions. Finally, the proposed approach is empirically evaluated on synthetic and real-world datasets."
SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper studies the problem of combining offline and online data in the Partially-Observable Markov Decision Process (POMDP) setting. In this setting, the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data) and also has access to a large collection of offline experiences obtained by observing another agent (observational data). The key ingredient, which makes this situation non-trivial, is that we allow the observed agent to act based on privileged information, hidden from the learner. This paper proposes a general yet simple methodology for safely leveraging offline data during learning. The method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard POMDP transition model via deconfounding using the recovered latent variable. The authors prove that the method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and empirically on a series of synthetic toy problems."
SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes a new approach for open-ended question answering and abstract generation tasks. The authors propose to use an additional guide retriever that is allowed to use the target output and “in hindsight” retrieve relevant passages during training. The proposed approach is based on the posterior distribution Q of passages given the input and the output. The retriever is trained jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. Experiments on the Wizard of Wikipedia dataset show that the retriever finds passages with higher relevance in the top-10, the generator’s responses are more grounded in the retrieved passages, and the end-to-end system produces better overall output."
SP:bec15075409c71f98f3698bc35e34eeb4862d94f,This paper proposes a graph neural network-based approach for influence estimation and influence maximization. The main idea is to use a GNN to parameterize the upper bound of the influence function and train it on small simulated graphs and then use it to estimate the influence of a given seed set on the target graph. The paper shows that the proposed approach is able to outperform the state-of-the-art influence estimation algorithms on real-world graphs. 
SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper considers the problem of active learning for general loss functions under domain shift. The authors propose a distribution matching approach based on discrepancy minimization (Mansour et al. 2009) and derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy. A practical K-medoids algorithm that can address the case of large data set is inferred from the theoretical bounds. Numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning methods in the context of domain adaptation.
SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks. The approximation relies on a central result from singular learning theory according to which the posterior distribution over the parameters of a singular model, following an algebraic-geometrical transformation known as a desingularization map, is asymptotically a mixture of standard forms. From here, the authors demonstrate that a generalized gamma mean-field variational family, following desedularization, can recover the leading order term of the model evidence. Affine coupling layers are employed to learn the unknown desedularity map, effectively rendering the proposed methodology a normalizing flow with the generalized gamma as the source distribution."
SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper presents a learning-theoretic generalization bound for domain generalization (DG) that bounds novel DG performance in terms of the model’s Rademacher complexity. The authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. The analysis suggests that domain generalisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective."
SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper introduces the maximum n-times coverage problem, which is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors introduce NTIMES-ILP and MARGINALGREEDY, efficient algorithms for solving the n-time coverage problem on both synthetic data and real-world vaccine design. They show that the proposed methods can be used to augment existing vaccine designs with predicted population coverage by initializing the predicted peptides that are present in an existing design. The proposed methods outperform 29 other published vaccines for COVID-19 with less than 150."
SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes a weight initialization method for spiking neural networks (SNNs) based on the slant asymptotic formula for the response curve of spiking neurons. The authors also propose an initialization method to overcome gradient vanishing. The proposed method is evaluated on the MNIST and CIFAR-10 datasets. 
SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper studies the problem of distribution shift in federated learning. The authors propose FedTEM, a federated expectation-maximization algorithm enhanced by temporal priors of the shifting distribution (FedTEM), which jointly learns a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments for image classification on EMNIST and CIFAR datasets and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of the distribution shift and significantly improve the final model performance."
SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a new approach to pruning deep neural networks (DNs) based on the analysis of the spline of the input-output mapping. The authors show that the splines of DNs exhibit an early-bird (EB) phenomenon, which is similar to the lottery ticket hypothesis of deep networks. Based on this insight, the authors propose a new pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate that the proposed approach reduces training FLOPs by up to 3.5x while maintaining the performance."
SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning in machine learning, where the goal is to learn representations that are invariant with respect to different subgroups. The authors formulate the problem as a bi-level optimization problem where the representation is learned in the outer-level and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost, the authors propose the implicit path alignment algorithm, which relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. Moreover, the proposed method is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in fair representations learning."
SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper investigates the importance of different design choices in reinforcement learning via supervised learning (RvS) methods, such as policy architectures and how the conditioning variable is constructed, and empirically investigates the design decisions, performance, and limits of such methods. The authors claim that the most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments show that more complex design choices such as the large sequence models and value-based weighting schemes used in prior work, are often not necessary."
SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to predict exploration behavior of humans by inferring the structure of unobserved spaces based on previously explored spatial information collected from previously explored spaces. The authors model this cognitive process computationally through “Map Induction”, which involves the compositional formation of proposed maps of complex space based on already-seen spaces through program induction in a Hierarchical Bayesian framework. The model thus explicitly reasons about unseen spaces through a distribution of strong spatial priors. The paper introduces a new behavioral Map Induction Task, and compares human performance with that of state-of-the-art existing models as well as our MapInduction framework. "
SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The method learns to maintain a set of marginal posterior samples using end-to-end training. The authors evaluate the proposed method on a series of articulated pose tracking tasks and compare performance with learned baselines. Results demonstrate the effectiveness of using learned factors for tracking and suggest practical advantage over hand-crafted approaches.
SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based generative model for molecule optimization. The authors propose a generative method that supports scaffolds as the initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from. "
SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies the problem of imitation learning in the setting of deterministic experts. The authors show that, for deterministic agents, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments confirm that the reduction works well in practice for continuous control tasks."
SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the over-parameterized worst-group performance of various reweighting algorithms for fairness. It shows that almost all reweighted algorithms converges to the same ERM interpolator that fits all training samples, which means that the implicit biases of these algorithms are equivalent to that of ERM. Then, the authors analyze whether adding regularization helps fix the issue, and prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a multi-agent reinforcement learning (MARL) framework that incorporates an established model of human-irrationality, the Rational Inattention (RI) model, which models the cost of cognitive information processing using mutual information. The proposed RIRL framework generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs. The authors evaluate RirL in Principal-Agent (specifically manager-employee relations) problem settings of varying complexity where RI models information asymmetry (e.g. it may be costly for the manager to observe certain information about the employees). They show that using RIRLU yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. For instance, some forms of a Principal’s inattention can increase Agent welfare due to increased compensation, while other forms of inattentive can decrease Agent welfare by encouraging extra work effort."
SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a phase-oriented approach for generating imperceptible adversarial perturbations for ASR systems. Specifically, the authors leverage spectrogram consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbation to adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, they propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate that the proposed method can generate full-sentence imperceptable audio adversarial examples and achieve a 6.64x generation speed-up over the state-of-the-art imperceptibly counterparts. "
SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper studies the representation learning in non-contrastive self-supervised learning (nc-SSL). In particular, the authors analyze a generalized version of DirectPred, called DirectSet(alpha), and show that it provably learns a desirable projection matrix and reduces the sample complexity on downstream tasks. Inspired by the theory, they simplify DirectPred by removing the expensive eigen-decomposition step and propose a simpler and more computationally efficient algorithm, DirectCopy."
SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a gradient-based method for learning long-term dependencies in sequential tasks. The proposed method is based on a time-discretization of a multiscale ordinary differential equations (ODEs). The authors derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem. The authors also prove that LEM can approximate a large class of dynamical systems to high accuracy. The empirical results on image and time-series classification, dynamical system prediction, keyword spotting and language modeling demonstrate the effectiveness of the proposed method."
SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a geometric deep learning framework that is rotation and permutation-equivariant. The proposed method is composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology."
SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the generalized assignment problem for real-time order fulfillment. The proposed method is based on the edge-feature-embedded graph attention mechanism, which considers the high-dimensional edge features and accounts for the heterogeneous information, which are important characteristics of the studied optimization problem. The model is also size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality."
SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a framework to infer concepts out of the dialogue context (CODC) in the dialogue summarization task. The proposed framework consists of a CODC inference module leveraging external knowledge from WordNet and a knowledge attention module aggregating the inferred knowledge into a neural summarization model. The authors also propose a new evaluation metric based on the inference capability of different methods. Experiments suggest that current automatic evaluation metrics of natural language generation may not be enough to understand the quality of out-of-context inference in generation results, and the proposed summarisation model can provide statistically significant improvements on both CODCs inference and traditional automatic evaluation metric, e.g., CIDEr."
SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper considers the question of whether entity embedding models can capture logical dependencies in a less demanding sense. Specifically, the authors consider the setting where the embedding of an entity has to be learned by pooling the embeddings of its known attributes. They first show that some of the most popular embedding model are not able to capture even basic logical rules. However, they also find that some embedding strategies are capable, in principle, of modelling both monotonic and non-monotonic attribute dependencies."
SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based language models on three reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. The authors show that when the models are explicitly given all the information required to perform deductive reasoning, such as facts and rules, the models can easily learn logical reasoning, but when this information is only implicitly in the text or in the supervision, they struggle. They also show that transformer models have a certain degree of commonsense knowledge learned during pre-training, but they are also limited in logical reasoning over logical events and physical commonsense."
SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper introduces the compositional problem graph (CPG) as a framework for learning compositional generalization. CPG is a set of problems with shared subproblems. The authors propose a curriculum to train the learner to generalize to new problems by testing how readily old knowledge can be reused and hence built upon. They also introduce a compositional recursive learner (CRL) framework to learn algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. They show on symbolic and high-dimensional domains that their compositional approach can generalize better than baselines that are not explicitly compositional."
SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP) technique, which integrates the activation pruning with the weight pruning. Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore improves execution efficiency. The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets. With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ∼ 96.35% of computation cost."
SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a new method for feature selection based on GANs. The proposed method builds on the knockoff framework and adapts the GAN framework to generate knockoffs with no assumptions on the feature distribution. The model consists of 4 networks, a generator, a discriminator, a stability network, and a power network. The authors demonstrate the capability of their model to perform feature selection, showing that it performs as well as the originally proposed knockoff generation model in the Gaussian setting and outperforms the original model in non-Gaussian settings, including on a real-world dataset."
SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes a new pruning method for Winograd convolution. In particular, the authors propose a spatial-Winograd pruning, which is a combination of spatial structured pruning (i.e., the spatial-domain weights are pruned in a structured way) and spatial-winograd retraining (which is a direct pruning). The authors claim that the proposed method can achieve higher sparsity in the Winograddomain domain than the previous methods."
SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed method is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available.
SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes an unbiased estimator for Monte Carlo Monte Carlo integration that is unbiased, exhibits low variance, and has low computational complexity. The proposed estimator is a combination of variable augmentation, REINFORCE, and reparameterization. The variance-reduction mechanism of the estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric “self-control” baseline function. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation for discrete latent variable models."
SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a modular probabilistic programming language (MXFusion) which includes a new type of re-usable building blocks, called probablistic modules, which consists of a set of random variables with associated probabilistics distributions and dedicated inference methods. The authors propose a framework based on variational inference, where the pre-specified inference methods can be transparently used for inference of the whole probabilistically model. Experimental results show the power and convenience of the proposed approach."
SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method for pruning large neural networks at initialization. The proposed method, called SNIP, identifies important connections in the network that are important to the given task based on their influence on the loss function at a variance scaling initialization. Given the desired sparsity level, redundant connections are pruned once prior to training (i.e., single-shot), and then the sparse pruned network is trained in the standard way. The method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks."
SP:986b9781534ffec84619872cd269ad48d235f869,This paper studies the performance degradation of beam search for neural sequence synthesis tasks. The authors find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. They also propose two methods to constrain the search and show that constrained beam search effectively eliminates the problem of performance degradation.
SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes a method to improve the sample efficiency of RL agents when they have access to expert demonstrations. The proposed method, Backplay, uses a single demonstration to construct a curriculum for a given task. The method starts the agent near the end of the demonstration and moves the starting point backwards during the course of training until we reach the initial state. The paper analytically characterizes the types of environments where Backplay can improve the training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency."
SP:426c98718b2dbad640380ec4ccb2b656958389bc,"This paper proposes an automated multi-layer pruning method (MLPrune) to compress deep neural networks. The authors use an efficient approximation of the Hessian as the pruning criterion, based on a Kronecker-factored Approximate Curvature method. The proposed method achieves state-of-the-art results on several model architectures and benchmarks."
SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper presents an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. They first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, they evaluate the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. They show several practical applications enabled by their framework, from comparing internal representations across different layers, models, and datasets, to improving GAN models by locating and removing artifact-causing units."
SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper proposes a metric to measure the distance between two neural networks in function space. The metric is based on the L2 distance between the outputs of two networks when given the same inputs. The paper shows that the metric is useful for two applications: 1) preventing catastrophic forgetting in multitask learning, and 2) constraining the distance a network can travel through L-space in any one update."
SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,This paper proposes a deep generative Markov model (Dynamics Modeling Network or DyMoN) that is trained to predict the near-future state of a given cell given its current state. The model is trained using a recurrent neural network (RNN) that takes as input the current state and outputs a probability distribution of the next state. This probability distribution is then used to train a generative model of the cell dynamics. The authors show that the model is able to generate new trajectories that are similar to the observed trajectories and can be used to extract the features of the dynamics.
SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a method for unsupervised image classification based on graph Laplacian. Unlike the widely used classification method, this architecture does not require the labels of data and the number of classes. The key idea is to introduce a approximate linear map and a spectral clustering theory on the dimension reduced spaces into generative adversarial networks (GANs). Inspired by the human visual recognition system, the proposed framework can classify and also generate images as the human brains do."
SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning representations of sets that are permutation-invariant, i.e. representations that are learnable and differentiable. The authors propose to learn the permutation in the forward pass of a neural network using pairwise comparisons. They demonstrate their model’s ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which they achieve state-of-the-art results."
SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper introduces Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. The embedding in PSN deems samples of a given class to form an affine subspace. The authors show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, the PSN approach has the ability of end-to-end learning."
SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes a meta-learning method for fast adaptation. The authors propose to split the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), the proposed method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,This paper proposes a framework where the user controls what characteristics of the data (utility) and what they want to keep private (secret) without necessarily asking the utility provider to change its existing machine learning algorithms. The authors first analyze the space of privacy-preserving representations and derive natural informationtheoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of data X. They present explicit learning architectures to learn privacy preserving representations that approach this bound in a data-driven fashion. They describe important use-case scenarios where the utility providers are willing to collaborate with the sanitization process.
SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation of GANs (PAGAN) to improve the training stability. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. The proposed method is evaluated on the MNIST, Fashion-MNIST, CIFAR-10 and CELEBA datasets. The experimental results show that the proposed method preserves the original GAN objective, does not bias the optimality and encourages the healthy competition between the generator and discriminator."
SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a rotation-equivariant convolutional neural network (R-CNN) to predict neural responses to natural stimuli in the primary visual cortex (V1) of mice. The R-CNN is trained to be rotation equivariant, i.e., it is able to predict the responses of a population of 6000 neurons to natural images recorded in mouse V1 using two-photon imaging. The authors show that the R-RCNN outperforms a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many V1 neurons and are pooled sparsely to predict Neural activity."
SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper proposes a distributed algorithm for robust, communication-efficient learning, called SIGNSGD, which aggregates gradients by majority vote. This algorithm uses 32x less communication per iteration than full-precision distributed SGD. Theoretical results show that the algorithm converges in the large and mini-batch settings, establishing convergence for a parameter regime of ADAM as a byproduct. The authors also prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially."
SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a conditional generative adversarial network (WGAN) approach to generate realistic and high-fidelity stock market data. The authors model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. They test their approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data. However, there are open issues that provide for further research material."
SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper studies the problem of learning in noisy reinforcement learning, where the rewards are generated by a reward confusion matrix. The authors propose an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. The proposed framework builds upon existing approaches for supervised learning with noisy data. The core ideas of the proposed framework include estimating the reward confusion matrices and defining a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that policies based on the estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines."
SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network width and depth on the convergence of gradient descent on the weight space traversal of RNNs. The authors conduct a series of experiments to characterize the impact of width on the error surface. They show that the convergence rate is a function of direct distance from initial weights to final weights, average step size, and the average angle between gradient vectors and the path that connects current weights to the final wights. They also provide a simple theoretical analysis for a simplified LNN and show that direct distance is expected to shrink as models get wider."
SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper studies the problem of learning optimal algorithms for online optimization problems. The authors introduce a novel learning framework, the online primal-dual framework, and introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst-case. They test their new ideas on the AdWords problem, online knapsack problem, and the secretary problem. The results indicate that the models have learned behaviors that are consistent with the optimal algorithms derived using the online-paleo-policies framework."
SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper re-examines several domain-specific components that modify the agent’s objective and environment interface. The authors investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature. They found that performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes adaptive components performed better. They then investigated the main benefit of having fewer domain specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. As hypothesized, the system with adaptive components performs better on many of the tasks."
SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for vision-and-language navigation (VLN) task. The proposed method consists of two complementary modules: visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images, and progress monitor module to ensure the grounded instruction correctly reflects the navigation progress. Experiments on the VLN benchmark show that the proposed method outperforms the state-of-the-art by a significant margin."
SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes two methods to improve the performance of encoder-decoder-style neural program synthesis approaches. The first method, execution-guided synthesis, is based on the view of the program execution as a sequence of manipulations to transform each input state into the corresponding output state. The second method, synthesizer ensemble, leverages the semantic information to ensemble multiple neural program synthesizers. Experiments on the Karel dataset show that the proposed methods can boost the accuracy from around 77% to more than 90%."
SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the effect of batch normalization (BN) on the stability, convergence and acceleration properties of gradient descent (GD) in the setting of ordinary least squares (OLS) regression. In particular, the authors analyze the convergence and stability properties of GD with BN on a simplified model of the least-squares regression problem. They show that BNGD converges for any constant learning rate $\epsilon$ regardless of the conditioning of the regression problem, which is in stark contrast with GD, where the condition number of the problem adversely affects stability and convergence. The authors also show that there exist regions of $\eps^2$ such that the performance of BN is insensitive to changes in $\eps$ and the learning rate."
SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"This paper presents a new theoretical perspective of data noising in recurrent neural network language models (RNNs) based on variational inference. The authors show that each variant of data-noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). Based on this insight, the authors propose a more principled method to apply at prediction time and propose natural extensions to data noizing under the variational framework. In particular, they propose variational smoothing with tied input and output embedding matrices and an element-wise variational method."
SP:f4a914d3df1a5a21a7365ba78279420f39210884,"This paper proposes a method for extracting classifier-agnostic saliency maps for object localization. The proposed method is based on the idea that the saliency map should be classifier agnostic, i.e., it should be able to identify all the parts of the image that any classifier could use, not just one given in advance. The method is evaluated on the ImageNet dataset, and it is shown that it outperforms existing weakly-supervised localization techniques."
SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a knowledge exchange method to train a student model from a set of teacher models. The student model is independent of the teacher models and can be trained on different tasks with different output spaces. The proposed method is evaluated on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other knowledge exchange methods."
SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes to perform soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. The authors propose an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. The model construction is cast as a maximization of the compression of the state variables such that the predictive ability and causal interdependence constraints between the original data streams and the compact model are closely bounded. The paper provides theoretical guarantees concerning the convergence of the proposed learning algorithm.
SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a variational autoencoder with arbitrary conditioning (VAEAC) model for the problem of learning all conditional distributions of the form p(xI |xU\I), where U is the set of all features and I is its arbitrary subset. The model is trained using stochastic gradient variational Bayes (SGD) with Gaussian latent variables. Experiments on synthetic data, feature imputation and image inpainting problems show the effectiveness of the proposed approach."
SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes a method to reduce the memory footprint of deep neural networks (DNNs) by replacing the full-precision activations in the forward pass of back-propagation with approximations during the backward pass. The proposed method is motivated by distributed training algorithms that succeed despite working with approximate and noisy gradients aggregated across multiple devices (Recht et. al., 2011; Dean et al., 2012; Seide et al, 2014; Wen et al. 2017). The authors show that the proposed method can significantly reduce a network’s memory footprint during training but has negligible effect on training performance and computational expense."
SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a method for high-resolution face completion. The authors propose a coarse-to-fine attentive module network architecture. The model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components. The proposed method can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x 1024 resolution."
SP:a300122021e93d695af85e158f2b402d21525bc8,"This paper presents a theoretical analysis of the impact of the choice of accumulation precision for partial sum accumulations in deep learning training. The authors show that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, and derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. They also show that reducing accumulation precision further degrades the quality of the trained network, proving that their equations produce tight bounds."
SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic weight matrix alignment of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. The authors show that for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized ith weight matrix is aligned across layers, meaning |v>i+1ui| → 1. In the case of the logistic loss (binary cross entropy), the linear function induced by the network — the product of its weight matrices — converges the same direction as the maximum margin solution."
SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper proposes to extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona based HRED generator (PHRED) and a conditional discriminator. Experiments on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends."
SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a goal-driven collaborative task between two agents, called CoDraw, which is a Collaborative image-Drawing game. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art objects. The two players communicate via two-way communication using natural language. The authors collect the CoDraw dataset of ∼10K dialogs consisting of ∼138K messages exchanged between human agents. They also define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition."
SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learning neural random fields (NRFs) with an auxiliary generator, called ""inclusive-NRF"", for continuous data (e.g. images). The proposed approach is based on the idea of minimizing the KL divergence between the target random field and the auxiliary generator. Theoretical results show that the proposed approach can be used in both unsupervised/supervised image generation and semi-supervised classification tasks, and is empirically shown to represent the best-performed random fields in these tasks."
SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper proposes an algorithm for training-time adversarial attacks on graph neural networks for node classification that perturb the discrete graph structure. The core principle is to use meta-gradients to solve the bilevel problem, essentially treating the graph as a hyperparameter to optimize. The experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings."
SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model, called Cramer-wold autoencoder (CWAE), which is a combination of sliced-Wasserstein autoencoders (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). CWAE cost function is based upon a characteristic kernel, which has a simple closed-form in the case of normal prior. CWAE performance matches quantitatively and qualitatively that of SWAE and often improves upon SWAE."
SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper studies the problem of many-class few-shot (MCFS) classification in both supervised learning and meta-learning scenarios. The authors propose MahiNet, a hierarchical classification network that explores the class hierarchy by exploring the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine classes and is cheaper to obtain. The paper also proposes two novel benchmark datasets “mcfsImageNet” and “McfsOmniglot” for MCFS problem."
SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes Structural-Jump-LSTM, a recurrent neural network for speed reading. The proposed method is inspired by human speed reading and can skip and skip irrelevant words in important sections, while also being able to jump past unimportant sections of the input. The model consists of a standard LSTM and two agents: one capable of skipping single words and the other capable of exploiting punctuation structure (sub-sentence separators, sentence end symbols, or end of text markers) to jump ahead after reading a word. An extensive experimental evaluation against all state-of-the-art neural reading models (Seo et al. 2018; Yu et al., 2017; Huang et al 2018; and Fu & Ma, 2018), shows that the proposed method leads to large FLOP reductions while maintaining the same accuracy or better accuracy."
SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a nonlinear radial basis convolutional feature transformation by learning the Mahalanobis distance function. The proposed method maps the input features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary. The authors evaluate the robustness of the proposed method to different gradient (targeted and untargeted) and nongradient based attacks and compare it to several non-gradient masking defense strategies."
SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a method for on-policy temporally consistent exploration for deep reinforcement learning agents. The proposed method is based on the idea of dropout, which is used as a global random variable for conditional distribution. The dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy’s stable improvement. The experiments demonstrate the effectiveness of the proposed method."
SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes the nonlinearity coefficient (NLC) as a gradient-based metric that can be computed before training and can robustly predict the performance of the network after training is complete. The NLC combines the Frobenius norm of the Jacobian of the neural network with the input data and global variability of the outputs into a single metric, which is tied to many important properties. The authors show that the NLC is a powerful predictor of test error and that attaining a right-sized NLC can be essential for attaining an optimal test error, at least in fully-connected feedforward networks."
SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. The authors demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification – a previously unreported form of bias that can be traced back to the features of a trained model. To mitigate bias amplification, the authors propose two feature selection algorithms that are designed to mitigate bias and demonstrate their effectiveness on both linear models and deep neural networks."
SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the generalization properties of deep neural networks with relu activations. The authors show that the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, and that increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks. They also show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time."
SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes a GAN model that allows to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. Experiments on the Multi-MNIST, CLEVR, and the more complex MSCOCO data set show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations."
SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a representation learning and model-based reinforcement learning method, SOLAR, which jointly optimizes a latent representation and model such that inference produces local linear models that provide good gradient directions for policy improvement. The main idea is to learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. The proposed method is evaluated on a suite of robotics tasks, including a manipulation task on a real Sawyer robotic arm directly from camera images."
SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"This paper proposes a method for counterfactual reinforcement learning. The proposed method, Counterfactually-Guided Policy Search (CF-GPS), leverages structural causal models (SCMs) to model the environment with two ingredients: (1) deterministic transition functions (also called causal mechanisms) that predict the outcome of the agent’s actions, and (2) scenarios that are inferred in hindsight from given off-policy data, and then evaluate and improve the agent on these scenarios. The authors show that the proposed method can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions."
SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"This paper studies the relationship between adversarial robustness and the geometric properties of the decision surface in parameter space and input space. It shows that the geometry property of decision surface correlates well with the adversarial generalization, and proposes a robust training method guided by it. The paper also shows that adversarial attack trajectory on decision surfaces in input space can be visualized, which reveals that various adversarial attacks are all utilizing decision surface geometry properties to cross the decision boundary within least distance. "
SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The authors formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. They integrate the quantitative DNN energy estimation into the training process to assist the constrained optimization. Compared to the best prior energy-saving methods, their framework trains DNNs that provide higher accuracies under same or lower energy budgets."
SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a Bayesian policy optimization method for continuous Bayes-Adaptive Markov Decision Process (BAMDP) problems. The proposed method builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, the authors propose a new policy network architecture that encodes the belief distribution independently from the observable state. The method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with POMDP solvers."
SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for building unsupervised representations of entities and their compositions, by viewing each entity as a histogram (or distribution) over its contexts. This enables them to take advantage of optimal transport and construct representations that effectively harness the geometry of the underlying space containing the contexts. The method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. The key tools at the core of this framework are Wasserstein distances and Wassersteins barycenters. Empirical results show strong advantages gained through the proposed framework."
SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper investigates the performance gap between model-based and model-free reinforcement learning (MBRL) methods in two MuJoCo environments. The authors propose a dynamics model that directly predicts distant states, based on current state and a long sequence of actions. This avoids the need for many recursions during long-range planning, and thus is able to yield more accurate state estimates. These predictions allow the authors to uncover the relationship between model accuracy and performance, and translate to higher task reward that matches or exceeds current state-of-the-art model free approaches."
SP:da14205470819495a3aad69d64de4033749d4d3e,"This paper proposes a method for ultra-low-precision quantization in deep neural networks. The proposed method is based on the idea of precision highway, which is an end-to-end high precision information flow to reduce the accumulated quantization error. The method is applied to both convolutional and recurrent neural networks, and the experiments show that the proposed method outperforms the state-of-the-art quantization methods while offering 3-bit weight/activation quantization and 2-bit quantization with no accuracy loss."
SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a general method for image restoration. The proposed method is formulated as a constrained optimization problem, where the goal is to maximize the posterior probability of latent variables and the constraint is that the image generated by these latent variables must be the same as the degraded image. The authors use a GAN as the density estimation model. The method is evaluated on the MNIST dataset."
SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a method for knowledge distillation from few samples. The authors assume that both ""teacher"" and ""student"" have the same feature map sizes at each corresponding block, and add a 1x1 conv-layer at the end of each block in the student-net, and align the block-level outputs between the teacher and student by estimating the parameters of the added layer with limited samples. Experiments verify that the proposed method is very efficient and effective to distill knowledge from teacher to student."
SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a transferability metric, H-score, to evaluate the performance of transferred representations from one task to another in classification problems. Inspired by a principled information theoretic approach, the proposed metric has a direct connection to the asymptotic error probability of the decision function based on the transferred feature. This formulation of transferability can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments using both synthetic and real image data show that the proposed transferability is meaningful in practice, but also can generalize to inference problems beyond classification."
SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes to add a planning phase in neural machine translation (NMT) models to control the global sentence structure ahead of translation. The proposed approach learns discrete structural representations to encode syntactic information of target sentences. During translation, it can either let beam search to choose the structural codes automatically or specify the codes manually. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations."
SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,This paper proposes a new loss function for generative adversarial networks (GANs) based on the Relativistic discriminator (RGAN). The RGAN is a discriminator that estimates the probability that the given real data is more realistic than a randomly sampled fake data. The generator (G) is trained to increase the probability of the real data being generated by the RGAN and decrease the probability for the fake data by the GAN. The authors claim that the proposed loss function is more similar to integral probability metric (IPM) GANs than standard SGANs. They also propose a variant in which the discriminator is also trained to estimate the average probability of real vs. fake samples.
SP:8df1599919dcb3329553e75ffb19059f192542ea,This paper proposes a method to tackle the catastrophic forgetting problem in continual learning. The proposed method learns to build a model with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach for the proposed method.
SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper introduces Relational Forward Models (RFM) for multi-agent reinforcement learning, which is an extension of Graph Neural Networks (GN) to predict the future behavior of agents in MARL environments. The authors claim that RFM is able to capture the rich dynamics dynamics of the MARL environment and can provide interpretable intermediate representations which offer insights into what drives agents’ behavior, and what events mediate the intensity and valence of social interactions. Finally, the authors show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines."
SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL) that learns reward functions from demonstrations of tasks. The main idea is to learn a set of tasks, along with demonstrations of the desired behaviors of those tasks, and then to use these tasks as a meta-training set to learn reward functions for a new task. The proposed method is evaluated on a few-shot RL task, where it is shown that it is able to recover the reward function from a few demonstrations. "
SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a general framework for meta-learning approximate probabilistic inference for prediction (ML-PIP), which extends existing probabilism interpretations of meta learning to cover a broad class of methods. The proposed method, called VERSA, is an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. The method is evaluated on standard benchmarks and a challenging ShapeNet view reconstruction task."
SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper presents a novel deep learning architecture for classifying structured objects in ultrafine-grained datasets, where classes may not be clearly distinguishable by their appearance but rather by their context. The authors model sequences of images as linear-chain CRFs, and jointly learn the parameters from both local visual features and neighboring class information. The visual features are learned by convolutional layers, whereas the class-structure information is reparametrized by factorizing the CRF pairwise potential matrix. The performance of the proposed method is illustrated on a huge dataset that contains images of retail-store product displays, and shows significantly improved results compared to existing methods."
SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) to generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. The authors demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts. This work also opens up possible avenues for domain transfer and other exciting applications."
SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper proposes a mixed integer linear programming (MILP) verifier for verifying the robustness of piecewise-linear neural networks to adversarial perturbations. The main contributions of this paper are two-fold: (1) a tight formulation for nonlinearities, and (2) a novel presolve algorithm that makes full use of all information available. The proposed verifier is shown to be two to three orders of magnitude faster than the state-of-the-art verifier, Reluplex (Katz et al., 2017) on a representative task of finding minimum adversarial distortions, and is able to verify properties on convolutional and residual networks with over 100,000 ReLUs. "
SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper proposes a method to regularize learning in reinforcement learning by learning a default policy that can be used alongside the agent policy. The main idea is to restrict the amount of information the default policy receives, forcing it to learn reusable behaviours that help the policy learn faster. The authors formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. They present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, the proposed method can significantly speed up and improve learning."
SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a novel FLOW component for conversational machine comprehension (CM) tasks. The proposed FLOWQA is a multi-turn model that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous questions/answers as input, FLOW integrates the latent semantics of the conversation history more deeply. The model shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC)."
SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper tackles the problem of predicting the edits that software developers make to source code files. The authors develop several neural networks and use synthetic data to test their ability to learn challenging edit patterns that require strong generalization. They then collect and train their models on a large-scale dataset consisting of millions of fine-grained edits from thousands of Python developers. The main conclusion is that a new composition of attentional and pointer network components provides the best overall performance and scalability.
SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning strategy for multi-class classification. The proposed method, meta-classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi- class classifier as a sub-module. The authors formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. The same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings."
SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the interpretability of deep learning models for visual question answering (VQA). The authors design experiments to replicate experiments from psycholinguistics where the same question was investigated for humans. The experiments show that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber’s law. Moreover, the authors identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system."
SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic extension of the DistMult and ComplEx embedding models for link prediction in relational knowledge graphs. The authors argue that knowledge graphs should be treated within a Bayesian framework because even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. The proposed approach uses variational inference to estimate a lower bound on the marginal likelihood of the data. The main benefit of the Bayesian approach is that it allows for efficient, gradient based optimization over hyperparameters, which would lead to divergences in a non-Bayesian treatment. Experiments on several benchmarks demonstrate the effectiveness of the proposed approach."
SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"This paper proposes a new online learning approach for supervised dimension reduction. The proposed approach builds on top of the sliced inverse regression (SIR) algorithm, which is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in. The authors also refine the algorithm by using an overlapping technique and develop an incremental overlapping sliced inverse regressions algorithm. The effectiveness and efficiency of both algorithms are verified by simulations and real data applications."
SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"This paper proposes a multimodal factorization model (MFM) that decomposes multimodal representations into two sets of independent factors: discriminative and modality-specific generative factors, which are shared across all modalities and contain joint multimodality features required for the task of classification. The authors propose to optimize for a joint generative-discriminative objective with respect to the labels. The proposed MFM achieves state-of-the-art or competitive performance on six multi-modal datasets. "
SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, a multi-speaker model is learned using a shared conditional WaveNet core and independent learned embeddings for each speaker. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNetcore fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker encodings with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-Speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few audio data from new speakers."
SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper provides an interesting connection between f-GANs and various depth functions through the lens of f-Learning. Similar to the derivation of f GANs, they show that these depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning. They show that some appropriate structures of discriminator networks with hidden layers in GAN can lead to statistical optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist."
SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper presents a method for inferring scene programs, representing a scene via a symbolic program for its objects, attributes, and their relations. The authors also propose a model that infers such scene programs by exploiting a hierarchical, object-based scene representation. Experiments demonstrate that the model works well on synthetic data and transfers to real images with compositional structure. The use of scene programs has enabled a number of applications, such as complex visual analogy-making and scene extrapolation."
SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a time-gated LSTM RNN model called the g-LSTM for reducing state updates. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. The authors also propose a computational budget term to the training loss, which further reduces the number of computes by at least 10x. Finally, the authors propose a temporal curriculum learning schedule that helps speed up the convergence time of the equivalent LSTMs on long sequences."
SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a plug-and-play method for detecting out-of-distribution (OOD) samples in deep neural networks (DNNs). The proposed method is based on the observation that the mean and standard deviation within feature maps differ greatly between in-dist distribution and OOD samples. Based on this observation, the authors propose a simple and efficient OOD detection procedure that does not require re-training, pre-processing or changes to the model. The method is evaluated on CIFAR-100 and TinyImageNet."
SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper studies the problem of model recovery for one-hidden-layer fully-connected neural networks with sigmoid activations, where the goal is to recover the weight vectors of the neural network. The authors prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground-truth without requiring a fresh set of samples at each iteration. "
SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolution layers. It preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. Experiments show that FBS can provide 5x and 2x savings in compute on VGG-16 and ResNet-18 with less than 0.6% accuracy loss."
SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the problem of training GANs from the mixed Nash equilibrium perspective. The authors propose to use the classical prox methods to solve the infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE. Then, they propose a principled procedure to reduce the proposed methods to simple sampling routines, leading to practically efficient algorithms. Finally, they provide experimental evidence that their approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality."
SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method for parameter-efficient transfer and multitask learning with deep neural networks. The basic approach is to learn a model patch a small set of parameters that will specialize to each task, instead of fine-tuning the last layer or the entire network. The approach allows both simultaneous (multi-task) and sequential transfer learning. In the multi-task learning setting, the authors show that the proposed method can achieve better performance than traditional logits-only fine-tuneing. The authors also show that re-learning existing low-parameter layers improves transfer learning accuracy."
SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method, called mode normalization (MN), which extends the normalization to more than a single mean and variance. The proposed method first assigns samples in a mini-batch to different modes via a gating network, and then normalizes each sample with estimators for its corresponding mode (Figure 1). The proposed methods can be implemented as layers in standard deep learning libraries, and their parameters are learned jointly with the other parameters of the network in an end-to-end manner."
SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper investigates the lottery ticket hypothesis, which states that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation—reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the lottery: their connections have initial weights that make training particularly effective. The authors find that a standard pruning technique automatically uncovers trainable subnetwork from fully-connected and convolutional networks. They identify a winning ticket by a network and pruning its smallest-magnitude connections."
SP:08c662296c7cf346f027e462d29184275fd6a102,"This paper proposes a method for learning a representation of the state space of a reinforcement learning agent that can be used for count-based exploration in Atari games. The proposed method is based on an attentive dynamics model (ADM) that is trained to predict the actions taken by the agent. The ADM is trained in a self-supervised fashion, and the learned representation is used as a part of the actor-critic algorithm for exploration purposes. The method achieves state-of-the-art results on a set of challenging Atari games without using expert demonstrations, explicit high-level information. "
SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes a generative model for generating the parameters of deep neural networks. The proposed method, HyperGAN, first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. The generator is trained with conventional maximum likelihood (classification/regression) on the parameters, and an adversarial regularization keeps it from collapsing onto only one mode. HyperGAN is applied to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters."
SP:230b3e008e687e03a8b914084b93fc81609051c0,"This paper proposes an approach to train variational autoencoders (VAEs) with discrete valued latent variables. The authors propose a differentiable estimator for the ELBO which is based on importance sampling. Theoretical results show that the variance of the estimator approaches zero close to the optimal parameter configuration, which is a very desirable property for training VAEs. Experiments are conducted on two different VAEs architectures with Bernoulli and Categorically distributed latent representations."
SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes to train a feed forward neural network with increased robustness against adversarial attacks. This is achieved using a novel pre-trained building block based on a mean field description of a Boltzmann machine. On the MNIST dataset the method achieves strong adversarial resistance without data augmentation or adversarial training. The authors show that the increased adversarial performance is correlated with the generative performance of the underlying Boltzmani machine.
SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"This paper studies the effect of small changes in the size and location of the visible region of minimal images on the performance of deep neural networks (DNNs). The authors introduce the concept of fragile recognition images (FRIs) for DNNs, which are more general than the minimal images for humans. They show that a slight adjustment of a one-pixel shift or two-pixel shrink of the region produces a drop in DNN recognition accuracy in many image regions. They also show that this phenomenon is independent of previous works that have reported lack of invariance to minor modifications in the object location in the minimal image. The results thus reveal a new failure mode of DNN that also affects humans to a much lesser degree."
SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper proposes a framework for multi-agent reinforcement learning (MARL) where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, the authors train a super agent to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The authors have evaluated their approach in two environments, Resource Collection and Crafting, to simulate various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents’ minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation."
SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a new recurrent neural network (RNN) model for asynchronous time series. The authors propose a unified RNN that handles five different feature types, i.e., sparse, dense, static, time features, static decay and time features at the sequential level. The sparse features are those that are present frequently, called dense features, and are updated only at time steps when that feature is present. The static features are the ones that are not related to time and are combined with the encoder output. The experiments show that the proposed model achieves better performance than standard RNNs."
SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a top-down approach to represent formulae by neural networks and proposes a novel neural model that tries to answer the question whether the formula has the given property, for example whether a propositional formula is always true. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a topdown manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of the proposed model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper studies the problem of curriculum learning in deep neural networks. The authors propose a method to sample the mini-batches from the training set in a non-uniform way, in order to improve the speed of learning and the final accuracy of the trained network. Specifically, they decompose the problem into two parts: (1) sort the data by some difficulty measure, and (2) sample mini-batch with a gradually increasing level of difficulty. They show that this method improves the learning speed and final accuracy for both small and competitive networks, using the CIFAR-10 and CifAR-100 datasets."
SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to analyze the generalization properties of deterministic and uncompressed deep neural networks. The main idea is to show that if on the training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions generalize to the interaction between the matrices on the test data, thereby implying a wide test loss minimum. The authors then apply their general framework in a setup where the pre-activation values of the network are not too small. In this setup, they provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with the product of the spectral norms of the weights matrices."
SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes a new approach to training discrete latent variable models for machine translation. The approach builds on the vector quantized variational autoencoder (VQ-VAE) method proposed by van den Oord et al. (2017), which uses a learned code-book combined with nearest neighbor search to train the discrete latent variables. The authors propose an alternate training technique inspired by its connection to the Expectation Maximization (EM) algorithm. They achieve a BLEU score of 22.4 on English-to-German translation, outperforming Kaiser et al (2018) by 2.6 BU."
SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes two multi-view learning methods for learning sentence representations in an unsupervised fashion. One framework uses a generative objective and the other one uses a discriminative one. In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN) and other view is a simple linear model. The authors show that, after learning, the vectors produced by the models provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view."
SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes an online distributed optimization method called Anytime Minibatch (AMB) to mitigate the impact of stragglers. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper provides a convergence analysis and analyze the wall time performance. The numerical results show that AMB is up to 1.5 times faster in Amazon EC2 and up to 5 times faster when there is greater variability in compute node performance."
SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. The authors argue that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. They test this approach in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.
SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper proposes a method to improve Neural Processes (NP) by adding an attention mechanism to the encoder and decoder of NP. The authors claim that the main drawback of NPs is the underfitting of the context set, which results in inaccurate predictions at the inputs of the observed data. To address this issue, the authors propose to use attention to attend to the relevant context points for the prediction. The proposed method is evaluated on 1D function regression and 2D image regression tasks. "
SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,"This paper provides a theoretical analysis of credit assignment in gradient-based meta-reinforcement learning (meta-RL) and proposes ProMP, a new meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients by controlling the statistical distance of both pre-adaptation and adapted policies. The paper also provides the low variance curvature (LVC) surrogate objective that yields gradient estimates with a favorable bias-variance trade-off. The proposed ProMP outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance."
SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes a message-passing neural network (MPNN) to predict satisfiability of SAT problems. The model is trained as a classifier on a dataset of randomly generated SAT problems, and at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs. The authors show that the model is able to generalize to novel distributions. The main contribution of this paper is that the proposed model can be used to help construct proofs for unsatisfiable problems."
SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. The main idea is to augment the imitation loss with additional losses that penalize undesirable events and encourage progress – the perturbations then provide an important signal for these losses and lead to robustness of the learned model. The authors show that the model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of their proposed changes. Finally, they demonstrate the model driving a car in the real world."
SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method to select a subset of training data to achieve faster training with no loss in model predictive performance. The authors first train a small proxy model to estimate the utility of individual training data points, and then select the most informative ones for training the large target model. Extensive experiments show that the proposed method leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data."
SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,This paper proposes a method for planning in continuous domains by treating planning as a probabilistic inference problem over future optimal trajectories. The proposed method is based on Sequential Monte Carlo (SMC) and Bayesian smoothing in the context of control as inference. The authors also propose a way to combine model-free and model-based reinforcement learning for planning based on the SMC perspective. 
SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper studies the relationship between adversarial robustness and the input data distribution. The main finding is that adversarial training is sensitive to the data distribution, unlike clean accuracy. The paper provides theoretical analyses to show the significance of input data distributions in adversarial adversarial trained robustness, and conducts experiments on MNIST and CIFAR-10 variants to confirm the finding. The authors also discuss the practical implications of the findings."
SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique that exhibits the fast convergence properties of batch normalization using a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. The authors validate the method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet."
SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper proposes a method for robust data denoising for large-scale training of deep neural networks. The proposed method uses the implicit regularization effect of stochastic gradient descent with large learning rates to identify mislabeled examples. Then, the proposed method can be used to discard the mislabelled examples on the fly and continue training with the rest of the examples. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeling."
SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a theoretical analysis framework to link the pretraining and downstream tasks with an underlying latent variable generative model of text — the downstream classifier must recover a function of the posterior distribution over the latent variables. The authors analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. They show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task. They also show that prompt tuning obtains downstream guarantees with weaker conditions. Experiments on synthetically generated data from HMMs back their theoretical findings."
SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization (OOD) in deep learning. In particular, the authors propose a new transferability metric, i.e., the ability of a model to generalize to a target domain that is at least as good as the target domain. The transferability is defined in terms of total variation and Wasserstein distance between the source and target domains. The authors show that the transferability can be estimated with enough samples and give a new upper bound for the target error based on our transferability. Empirically, the paper shows that existing methods for OOD generalization are not quite learning transferable features, although few could still survive. "
SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) an ordering over trajectories. The main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. Then, the authors provide a polynomial-time algorithms that construct a reward function that can be used to optimize tasks of each of these three types and correctly determine when no such reward function exists."
SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization to unseen test conditions from a limited number of training conditions in reinforcement learning. The authors show that the sequential structure of the RL problem necessitates new approaches to generalize beyond the well-studied techniques used in supervised learning. In particular, the authors recast the generalization in RL as solving the induced partially observed Markov decision process, which they call the epistemic POMDP. They demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, they demonstrate that their simple algorithm derived from the empirically derived epistemic MDP achieves significant gains in generalization over current methods on the Procgen benchmark suite."
SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a unified framework for estimating higher-order derivatives of value functions in meta-reinforcement learning (meta-RL) based on the concept of off-policy evaluation. The framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper proposes a bidirectional compression algorithm for distributed convex optimization with a central server. The main idea is to compress the downlink communication from the central server to the local workers, while preserving the global model. The proposed algorithm, called MCM, achieves the same convergence rate as algorithms using only uplink compression. The authors also propose a new algorithm that combines model compression with a memory mechanism. "
SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper introduces counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. The authors also provide practical schemes for learning (approximately) counterfactually invariant predictors. The means and implications depend on the true underlying causal structure of the data, in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactuality."
SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a novel strategy called Adaptive Pseudo Augmentation (APA) to encourage healthy competition between the generator and the discriminator. APA alleviates overfitting by employing the generator itself to augment the real data distribution with generated images, which deceives the discriminators adaptively. Extensive experiments demonstrate the effectiveness of APA in improving synthesis quality in the low-data regime."
SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a method for causal inference between pairs of event variables in multivariate recurrent event streams by extending Rubin’s framework for the average treatment effect (ATE) and propensity scores to multivariate point processes. The setting is similar to the joint probability distribution representing i.i.d. data, but with irregularly spaced occurrences of various types of events over a common timeline. The authors theoretically justify their point process causal framework and show how to obtain unbiased estimates of the proposed measure. They conduct an experimental investigation using synthetic and real-world event datasets, where their proposed causal inference framework is shown to exhibit superior performance against a set of baseline pairwise causal association scores."
SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper investigates the role of residual connections in the message passing of GNNs. The authors claim that the residual connections amplify the vulnerability of the GNN to abnormal node features, which is undesirable because in real-world applications, node features in graphs could often be abnormal such as being naturally noisy or adversarially manipulated. Based on this observation, the authors propose and derive a simple, efficient, interpretable, and adaptive message passing scheme, leading to a novel GNN with Adaptive residual, named as AirGNN. Extensive experiments under various abnormal feature scenarios demonstrate the superiority of the proposed algorithm."
SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper proposes a variational Bayesian Optimistic Sampling (VBOS) algorithm for the stochastic multi-armed bandit problem, which is an extension of the Thompson sampling (TS) algorithm to bilinear saddle-point problems. The authors show that VBOS can be viewed as a variant of Thompson Sampling in the bandit setting, and show that it enjoys $\tilde{O}(\sqrt{AT})$ Bayesian regret for a problem with A actions after T rounds. They also extend the regret analysis for optimistic policies to zero-sum matrix games and constrained bandits as special cases. "
SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper improves the convergence analysis and rates of variance reduction under without-replacement sampling orders for composite finite-sum minimization. The authors develop a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. The rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without replacement sampling with variance-reduction. Moreover, the authors also propose a practical method to discover the optimal cyclic ordering numerically."
SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper provides theoretical guarantees on the sub-optimality of a policy learned using first-order optimization methods applied to the relative entropy policy search (REPS) objective. In particular, the authors show that the REPS objective is near-optimal in the setting of exact gradients, and then provide a stochastic gradient descent (SGD) algorithm that maintains favorable convergence to the optimal regularized policy. The main technical contributions of the paper are:  1. Theorems 1.2 and 1.3.  2. Theorem 4.4. 3. 4. 5. "
SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper studies the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. The authors propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, they also propose metrics to evaluate the spatial smoothness of encoding 3D structure, and the representation complexity of the DNN. The experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes an extension of RegretNet, an existing neural network-based auction mechanism, to encode constraints using (potentially human-provided) exemplars of desirable allocations. They also introduce a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that their proposed method is competitive with current state-of-the-art neural-network based auction designs. They validate their approach through human subject research and show that they are able to effectively capture real human preferences."
SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of model personalization in the setting of user-level, joint differential privacy. In this setting, there are n users, each of whom has a training data set drawn from their own distribution, and the goal is to learn a model that generalizes well to unseen examples from the other users' data sets. The setting is a special case of multi-task learning, in which users train a model on their own data and share information about it with other users in order to improve the performance of the model. The paper provides algorithms that exploit popular non-private approaches in this domain, such as the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach. The algorithms are shown to satisfy nearly optimal estimation error guarantees. "
SP:3925fc528de17b8b2e93808f5440ea0503895b75,"This paper introduces AdVQA (Adversarial VQA), a large evaluation dataset of 46,807 examples in total, all of which fooled the MoViE+MCAN model. The dataset is designed for accelerating progress on Visual Question Answering. The authors evaluate a wide range of state-of-the-art models and find that their performance is significantly lower than on the commonly used VZA v2 dataset. They also conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions."
SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in medial entorhinal cortex (MEC) in navigation and memory tasks. The authors first identify the similarity transform between neural populations in different animals, and then evaluate the ability of various neural network models to explain the response variance of MEC neurons, treating each candidate model as a potential “source animal” and measuring how well it maps each animal’s response variance. They then evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. Finally, they introduce a new MEC model that performs reward-modulated path integration, and find that this unified model matches neural recordings across all variable-reward conditions."
SP:57f9812fa5e7d0c66d412beb035301684d760746,This paper studies the problem of KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors show that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. The pathology can be remedied by non-parametric behavior reference policies and this allows the proposed method to outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks.
SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the relationship between the compositionality of the data and the learning curve of convolutional neural networks (CNNs). Specifically, the authors propose a teacher-student framework for kernel regression, where the function to be learned is one of the following: fLC(x) = \sum_i \in P g_i(xi), f CN(x), f_CN(x). The student kernel corresponds to a prior on the true function of the form described by the neural tangent kernel (NTK), except that the filter size and its smoothness on the smoothness of the target function differ from those of the student kernel. The teacher kernel can be expressed as a sum of constituent functions each of which depends on a smaller number of variables depending on the number of training samples. The learning curve exponent of the teacher kernel depends on the size of the training set. The authors show that in the ridgeless case, if the teacher is smaller than the student, then the translation invariance is not preserved. In contrast, in the case where the student is larger than the teacher, the translational invariance of the kernel is preserved. The paper also shows that under a natural universality assumption, performing kernel regression with a ridge that decreases with the sizes of training set leads to similar learning curve exponents to those we obtain in the ridge-less case. "
SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a deterministic autoencoding framework for variational autoencoders that is applicable to expressive priors and overcomes the necessity of ex-post-deterministic density estimation step for deterministic training. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. Experimental results show the expressiveness and sample quality of the proposed model.
SP:6232d8738592c9728feddec4462e61903a17d131,"This paper proposes a method for self-supervised adversarial detection based on autoencoders. The proposed method is based on disentangling the input images as class features and semantic features, and then training an autoencoder, assisted by a discriminator network, over correctly paired class/semantic features to reconstruct benign and counterexamples. Experiments are conducted on CIFAR-10 under various adversarial attacks and victim models (30 attack settings)."
SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper proposes a method to model the brain representations of syntactic syntactical information. The authors propose to use syntactic structure-based features and fMRI recordings of participants reading a natural text to predict the brain activity of various parts of the language system. They show that syntactic structures explain additional variance in brain activity, even after controlling for complexity metrics that capture processing load. They also show that regions well-predicted by syntactic features are distributed in the language systems and are not distinguishable from those processing semantics."
SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for generating controllable images by using energy-based models (EBMs) in the latent space of a pre-trained generative model such as StyleGAN. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The proposed method is simple, fast to train, and efficient to sample. The quality is robust to hyperparameters, and it outperforms previous methods in both conditional sampling and sequential editing."
SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper presents a federated linear contextual bandits model, where individual clients face different K-armed stochastic bandits coupled through common global parameters. By leveraging the geometric structure of the linear rewards, a collaborative algorithm called Fed-PE is proposed to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a new model-based offline RL algorithm, COMBO, that trains a value function using both the offline dataset and data generated using rollouts under the model while also additionally regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the authors show that COMBO satisfies a policy improvement guarantee in the offline setting. Empirically, it is shown that the proposed method outperforms prior offline RL methods in benchmark tasks."
SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the impact of backpropagation on the feature space of deep neural networks during training. The authors propose to model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. The main finding is that if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, then the features of the training data become linearly separable, meaning vanishing training loss; otherwise, the features are not separable. This result provides convincing evidence for the presence of local elasticity in deep learning."
SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a framework that learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embeddings space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and generalizable policies."
SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper analyzes the physics-informed neural network (PINN) methodologies for learning PDEs, and shows that the PINN methodologies can learn good models for relatively trivial problems, but fail to learn relevant physical phenomena for even slightly more complex problems. The authors demonstrate that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They also show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that PINN’s setup makes the loss landscape very hard to optimize. They then describe two promising solutions to address these failure modes. One approach is to use curriculum regularization, where the PINNs’ loss term starts from a simple PDE regularization and becomes progressively more complex as the NNs gets trained. The second approach is a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that these methods can achieve up to 1-2 orders of magnitude lower error as compared to regular PINN training."
SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, CST generates target pseudo-label with a source-trained classifier. The reverse step trains a target classifier using target pseudo labels, and then updates the shared representations to make the target classifiers perform well on the source data. The authors introduce the Tsallis entropy as a confidence-friendly regularization to improve the quality of target pseudo label."
SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured network pruning method, called Discriminative Masking (DAM), which is a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. The proposed approach has remarkably good performance over a diverse range of applications, including dimensionality reduction, recommendation system, graph representation learning, matrix factorization, and structured pruning for image classification."
SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a modular self-attention network architecture that allows compositional reasoning and reuse of knowledge. In particular, the proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments on image classification and visual abstract reasoning on Raven Progressive Matrices show that Neural Interpreters perform on par with the vision transformer, while being transferrable to a new task in a sample efficient manner."
SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes a technique called Behavior Transfer (BT) that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. The authors show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre- trained policies can then be leveraged by BT to discover better solutions than without pre- training, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration."
SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a new class of differentiable surrogates for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divide-and-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, the authors demonstrate the role of larger list sizes during training and show that piRank significantly improves over comparable approaches on publicly available learning-to-rank benchmarks."
SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,This paper proposes a reinforcement learning approach to improve the performance of Variational Quantum Eigensolvers (VQEs) for estimating the ground-state energy of a given Hamiltonian Hamiltonian. The authors propose to use reinforcement learning (RL) to explore the space of possible variational quantum circuits to identify economic circuits that yield accurate ground energy estimates. The RL algorithm uses a feedback-driven curriculum learning method that autonomously adapts the complexity of the learning problem to the current performance of the RL algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. Experiments on the LiH benchmark show that the proposed approach achieves state-of-the-art results in terms of circuit depth and accuracy.
SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper proposes a new method for transductive few-shot learning. The authors argue that the existing methods assume that the marginal label probability of the testing samples is known and fixed to the uniform distribution. In fact, in realistic scenarios, the unlabeled query sets come with arbitrary and unknown label marginals. To address this issue, the authors propose to model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. The proposed method is evaluated on three widely used data sets, and shows substantial performance drops, even below inductive methods."
SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a method to reduce the memory consumption of convolutional neural networks (CNNs) on tiny microcontroller units (MCUs). The authors propose a patch-by-patch execution order for the initial memory-intensive stage of CNNs and propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. The proposed method, called MCUNetV2, achieves state-of-the-art results on ImageNet and Pascal VOC."
SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent and takes actions based on the strategic agent's report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal's utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. For environments with large time horizons, the authors show that the principal’s optimal utility is hard to approximate within a certain constant factor, complementing their algorithmic result."
SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,"This paper investigates the question of how neural architecture search (NAS) is able to select the desired GNN architectures. The authors propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is capable of discovering graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. Extensive experiments on real-world graph datasets demonstrate that the proposed GASSO model can achieve the state-of-the-art performance compared with existing baselines."
SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. In particular, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. They derive lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. They also derive impossibility results for other natural fairness objectives. They conclude with experimental results on real-world datasets. "
SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper investigates the effect of the choice of ReLU(0) = 0 in the [0,1] for a neural network on the backpropagation and training. The authors investigate the importance of the ReLU$=0$ for various precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet, and datasets) and datasets (MNIST, CIFAR10, SVHN, and ImageNet). They observe considerable variations of backprop outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU($0$ = 0$) seems to be the most efficient. For ImageNet, the gain in test accuracy over ReLU-$0$=1$ was more than 10 points (two runs). The authors also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of reLU$$0$."
SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. This method brings together ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck."
SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"This paper proposes a Transformer-based graph representation learning method, called Spectral Attention Network (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. Theoretical analysis shows that the proposed method is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Empirical results on several standard datasets show that the model performs on par or better than state-of-the-art GNNs."
SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper considers two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. The authors present an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium (where no coalition of voters can deviate and receive a better outcome)."
SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the Hessian rank of deep linear networks. The authors develop theoretical tools to analyze the range of Hessian map, which provide a precise understanding of its rank deficiency and the structural reasons behind it. This yields exact formulas and tight upper bounds for the Hessians, which allow for an elegant interpretation in terms of rank deficiency. Moreover, the authors demonstrate that the bounds remain faithful as an estimate of the numerical Hessians for rectified and hyperbolic tangent networks."
SP:24cdcb12fca34680d8b34bc61c51b9003368228a,This paper proposes a new metric to quantify linear disentanglement based representation learning (LSBD). The metric is based on the notion of linearly disentangled representations (LSDB). The authors also propose a method to learn LSBD representations using a VAE-based representation learning method. The authors show that the proposed metric can be used to evaluate the performance of various disentangling methods. 
SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,This paper proposes a framework for learning deep state-space models (DSSMs). The authors propose a constrained optimisation framework that combines variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The proposed method is evaluated on the pendulum on the reacher task and the context-based reinforcement learning task. 
SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method to generate counterfactual explanations for a given query image. The proposed method, called DISC (Deep Inversion for Synthesizing Counterfactuals), improves upon deep inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective, and (c) adopting a progressive optimization strategy. The method is evaluated on CIFAR-10 and ImageNet datasets and compared with DeepInversion, LSO, and DeepDream. "
SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a learning algorithm for identifying regions of heterogeneity in decision-making, i.e., regions where the assignment of decision-maker has a large causal effect on the decision. The authors formalize this as a causal inference problem, seeking a region where the decision assignment of the agent has a high causal effect. The algorithm finds such a region by maximizing an empirical objective, and the authors give a generalization bound for its performance. In a semi-synthetic experiment, they show that their algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, they apply their algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based style-based generative model (TokenGAN) for image synthesis. The key idea is to model the image synthesis task as a visual token generation problem, where the visual tokens are semantically different, i.e., the learned constant content tokens and the style tokens from the latent space. Given a sequence of style tokens, the tokenGAN is able to control image synthesis by assigning the styles to the content tokens by attention mechanism with a Transformer. Experiments on FFHQ and LSUN Church show that the proposed TokenGAN has achieved state-of-the-art results on several widelyused image synthesis benchmarks, including FFHQ."
SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the robust overfitting in the presence of noiseless data. The authors show that even in the absence of noise, avoiding interpolation through ridge regularization can significantly improve generalization. They prove this phenomenon for the robust risk of both linear regression and classification, and hence provide the first theoretical result on robust over fitting. "
SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The CPAE performs two key functions: (a) encoding an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to the original input instance shape. The primitive plays a key role to map all the unordered point clouds on the canonical surface and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the model performs favorably against state-of-the-art correspondence learning methods."
SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper studies the domain generalization (DG) problem and proposes a new method, called Stochastic Weight Averaging Densely (SWAD), to find flat minima. SWAD is based on a dense and overfit-aware stochastic weight sampling strategy. Theoretically, the authors show that finding flatter minima results in a smaller generalization gap, and SWAD achieves state-of-the-art performance on five DG benchmarks. The authors also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods."
SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper provides a large-scale study of performance predictors for neural architecture search (NAS) by analyzing 31 techniques ranging from learning curve extrapolation, weight-sharing, to weight sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. The results act as recommendations for the best predictors to use in different settings, and show that certain families of predictors can be combined to achieve even better predictive power."
SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the inherent privacy of sampling from a Dirichlet posterior distribution. The main contribution of this paper is the development of the notion of truncated concentrated differential privacy (tCDP), which allows to derive a simple privacy guarantee of the Dirichlett posterior sampling. The privacy guarantee allows to analyze its utility in various settings. Specifically, the paper provides accuracy guarantees of the posterior sampling in Multinomial Dirichle sampling and private normalized histogram publishing."
SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for computing random walks in the massively parallel computation model (MPC). The main idea is to build random walks efficiently and locally at the same time. Theoretical analysis shows that the proposed algorithm is both memory and round efficient, and in particular yields an efficient parallel local clustering algorithm. Experimental results show that the algorithm is significantly more scalable than previous approaches."
SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper provides a unified analysis of the typical learning performance of the L1-regularized linear regression (`1-LinR) for Ising model selection on typical random regular graphs in the paramagnetic phase. The main contribution of this paper is to provide an accurate estimate of the sample complexity of `1-LinR, which is consistent with the same order of sample complexity as the logistic regression (L1-LogR) and interaction screening (IS) estimators. Moreover, the authors provide an efficient method to accurately predict the nonasymptotic behavior of L1LinR for moderate M,N, such as precision and recall."
SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means clustering problem, which extends the clustering capability of the well-known k-mean clustering to datasets that are uncertain, vague and otherwise hard to cluster. The authors propose a semisupervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert) to ask for the similarity between a certain set of chosen items. They prove that having a few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynemic time-complexity, where n is the number of items. "
SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method for augmenting model-based reinforcement learning by encouraging a learned model and value function to be jointly self-consistent. The proposed method differs from classic planning methods such as Dyna, which only update values to be consistent with the model. The authors propose multiple methods to achieve this goal, and evaluate these in both tabular and function approximation settings, and find that, with appropriate choices, the proposed method can improve both policy evaluation and control."
SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper studies the effect of sampling schemes on the performance of few-shot learning. The authors propose a method to approximate the episode sampling distributions based on the difficulty of the episodes. They show that sampling uniformly over episode difficulty outperforms other sampling schemes, including curriculum and easy-/hard-mining. They also propose simple and universally applicable modifications to the episodic sampling pipeline to approximate any sampling scheme."
SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of generalized linear bandits (GLBs), a special case of logistic bandits (logistic bandits) that are used in problems with binary rewards. In this paper, the authors consider the case where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’, ‘show me later’ vs ‘never show again’). They use multinomial logit (MNL) to model the probability of each one of K+1-2 possible outcomes and propose an upper confidence bound (UCB)-based algorithm that achieves regret $\tilde{O}(\frac{dK p T}{\sqrt{T})$ with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds."
SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The proposed method aims to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed based on disentanglement of exploration and exploitation. Numerical results show that the proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of learning the time evolution of discrete sets of items (e.g., genetic mutations) from continuous-time Markov chains. The authors show that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. They propose an approximate likelihood maximization method that can scale to hundreds of items and is orders of magnitude faster than previous methods. They demonstrate the effectiveness of their approach on synthetic and real cancer data."
SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,This paper proposes a unified pretraining framework for document understanding. It extends the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. The model integrates image information in the pretraining stage by taking advantage of cross-modal interactions between visual and textual information. The proposed method is evaluated on several benchmark datasets.
SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies clustering problems with lp-norm objectives (e.g. k-center, k-median and k-means) in the context of individual fairness. In particular, the authors propose to use linear programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. "
SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper proposes a polynomial-time Gaussian sampling-based algorithms for the MAX-K-CUT and MAX-AGREE clustering problems. The main idea is to use the Gaussian-sampling-based approach of [27] to generate an approximate solution with provable approximation guarantees to the two problems using only O(|V |+ |E|) memory. For dense graphs arriving in a stream, the authors eliminate the dependence on|E| in the storage complexity at the cost of a slightly worse approximation ratio. "
SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a Motion-Aware Unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. Based on the learned attention map, the historical temporal states are aggregated to an augmented motion information (AMI). In this way, the predictive unit can perceive more temporal dynamics from a wider receptive field. Moreover, an information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions."
SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the sample complexity of two-layer neural networks with ReLU and polynomial activation functions for deep reinforcement learning. The authors consider two settings: (1) the generative model setting, where the agent can simulate the MDP at any state-action pair, and (2) the online setting, in which it can only start at an initial state and interact with the environment step-by-step. In both settings, the authors propose algorithms that are computationally and statistically efficient. The main contributions of this paper are two folds:  1) The authors show that under certain assumptions on the completeness of the neural network function class and on the realizability of the function class, one can obtain a sample complexity that scales linearly in the algebraic dimension. 2) When the transition dynamics is deterministic, they also present algorithms that can be shown to be sample efficient under certain conditions. "
SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization ability of deep metric learning (DML) models under out-of-distribution (OOD) distribution shifts. The benchmark is constructed by systematically constructing train-test splits of increasing difficulty and presents the ooDML benchmark to characterize generalization under OOD shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of the state of the art DML methods. They find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, they propose few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in the new benchmarks."
SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a data-driven approach for unsupervised outlier model selection (UOMS) based on meta-learning to automatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model) for a new dataset. The UOMS problem is notoriously challenging, as compared to model selection for classification and clustering, because evaluation is infeasible due to the lack of hold-out data with labels, and model comparison is impossible due to lack of a universal objective function. The proposed method, called METAOD, is based on the prior performances of a large collection of existing detection models on an extensive corpora of historical Outlier detection benchmark datasets. Extensive experiments show that selecting a model by METAOVER significantly outperforms no model selection. Moreover upon (meta-)training, the proposed method is extremely efficient at test time; selecting from a large pool of 300+ models takes less than 1 second for new task."
SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning, demonstrating that their method outperforms traditional two-staged methods and other decision-focused approaches."
SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper proposes DropGNN, a new approach that aims to overcome the limitations of standard GNN frameworks. In particular, it executes multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, it combines the results from these runs to obtain the final result. Theoretically, the authors prove that DropGnns can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. They also derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and they prove several properties regarding the expressive capabilities and limits of the proposed method."
SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper presents a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. The proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves SOTA performance over other Vision Transformers and ResNets."
SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a framework for visual reasoning with differentiable physics (VRDP) that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a physics engine. The visual perception modules parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learners learns visual concepts (e.g., color, shape, and material) based on the language. The physics engine is implemented as an impulse-based differentiable rigid-body simulator that performs differentiable physical simulation based on grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. "
SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The proposed method is able to work well in realistic scenarios such as imbalance or rare classes, out-of-distribution data in the unlabeled set, and redundancy. Empirically, it outperforms existing active learning algorithms by as much as ≈5%-18% in the case of rare classes and ≈ 5%-10% for the out of distribution data."
SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper proposes identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. First, the authors consider the case when the central ranking is known, and devise two algorithms for testing the spread parameter of the Mallows models. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test and then converting it into a sample-optimal identity test. The second one is derived from an optimal learning algorithm and is easy to compute and is sample optimal for a wide range of parameters. The authors also consider the unknown central ranking case. This case can be tackled by introducing a bias that exponentially decays with the sample size. The experiments show that the proposed tests scale gracefully with the number of items to be ranked."
SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,This paper proposes a generalizable neural radiance field (NHP) method for free-viewpoint human performance rendering that generalizes across different human performers and requires only sparse camera views. The proposed method is a combination of temporal and multi-view transformers that aggregate spatio-temporal observations to robustly compute the density and color of a query point. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses.
SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method to search the search space of vision transformers by gradually evolving different search dimensions guided by their E-T Error computed using a weight-sharing supernet. The search space is decomposed by multiple dimensions, including depth, embedding dimension, MLP ratio, window size, number of heads, and Q-K-V dimension, and progressively evolve each dimension to compose a better space. Moreover, the authors provide design guidelines of general vision transformer with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. "
SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this setting, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. The authors provide an algorithm that efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. The main result provides evidence that these algorithmic bounds cannot be significantly improved, even for learning monotone ORs using LTFs."
SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a method to create surrogate neural architecture search (NAS) benchmarks that contain the full training information for each architecture, rather than just the final validation accuracy. The authors propose a method using singular value decomposition and noise modeling to create the surrogate benchmarks. They also propose a learning curve extrapolation framework to modify single-fidelity algorithms, and show that it leads to improvements over popular single fidelity algorithms."
SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a post-hoc explanation method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space by answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. To answer these questions, SimplEx reconstructs the test latent representation as a mixture of corpus latent representations. The authors propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture."
SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based visual embedding for vision-language pre-training (VLP) to better learn visual relation and further promote inter-modal alignment. Specifically, the authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., Inter-modality). They also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the intermodality learning. Experiments are conducted on Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment and Visual Reasoning."
SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the dynamics of differential privacy of noisy gradient descent (GD) algorithms with private internal state. In particular, the authors propose a novel analysis for privacy dynamics of noisy GD with smooth and strongly convex loss functions. The analysis traces a provably tight bound on the Rényi divergence between the pair of probability distributions over parameters of models trained on neighboring datasets. The authors prove that the privacy loss converges exponentially fast, which is a significant improvement over composition theorems (which over-estimate the privacy risk by upper-bounding its total value over all intermediate gradient computations). For Lipschitz, smooth, and L-smooth loss functions, they prove optimal utility with a small gradient complexity for noisy GD algorithms. "
SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes RLQP, a reinforcement learning approach to accelerate the optimization of operator splitting quadratic programs (QPs) using ADMM (alternating direction method of multipliers). The approach is based on RL to learn a policy to tune the parameters of ADMM to improve the convergence of the algorithm. The policy is either trained on a set of QPs or a subset of them, or it is trained to specialize to a specific class of problems. The authors show that the RL approach outperforms the state-of-the-art OSQP solvers on QPLIB, Netlib LP, and Maros-Mészáros problems."
SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the order in which convolutional neural networks learn to classify images in the same order. The authors study the over-parameterized deep linear network model and show that the convergence rate of this model’s parameters is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They term this convergence pattern the Principal Components bias (PC-bias). The authors show how the PC-Bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the results to the spectral bias, showing that both biases can be seen independently, and affect the orders of learning in different ways."
SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper studies the problem of detecting backdoored data poisoning attacks in deep neural networks (DNNs) during training. The authors propose a two-stage gradient ascent mechanism for standard training to isolate backdoor examples at an early training stage, and to break the correlation between backdoor examples and the target class at a later training stage. They empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data."
SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a novel shading-guided generative implicit model (ShadeGAN) that is able to learn a sharper shape representation. The key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. The multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions during rendering. To compensate for the additional computational burden of calculating surface normals, the authors devise an efficient volume rendering strategy via surface tracking, which reduces the training and inference time by 24% and 48% respectively. The experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes."
SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a quasi-Bayesian approach for instrumental variable (IV) regression. The proposed approach builds upon the recent development in kernelized IV models. The authors use a Gaussian process (GP) prior and construct a conditional kernel-like conditional expectation estimator (CIE) to estimate the quasi-posterior, which is a generalized method-of-moments (GMM) procedure for IV estimation. Theoretical properties of the proposed approach are analyzed, and the empirical performance of the approach is evaluated. "
SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,This paper proposes a cross-lingual open-retrieval answer generation (CORA) model for multilingual open question answering (QA) that can answer questions in any target language by retrieving evidence from any language and generating answers in the target language. The authors propose a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question and a multilingual autoregressive generation model (mGEN) that generates answers in target language conditioned on the retrieved multilingual passages. The model is trained using an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. The results show that CORA substantially outperforms the previous state-of-the-art on the XOR-TYDI QA and MKQA datasets.
SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the generalization properties of ERM-based domain generalization methods. The authors consider the setting of unsupervised domain adaptation, where the source-domain and target-domain data are assumed to be indistinguishable, and the goal is to study when ERM methods generalize well out-of-distribution (OD). The authors propose three measures of OOD generalization: Fisher information, predictive entropy, and maximum mean discrepancy, and show that these measures are good predictors of the OOD performance of ERMs. "
SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper presents a theoretical framework for studying backdoor data poisoning attacks for classification problems. The authors identify a parameter they call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack, which allows them to argue about the robustness of several natural learning problems to backdoor attacks. They show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. They then show that backdoor filtering and robust generalization are nearly equivalent."
SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of width and depth on the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In particular, the authors aim to understand how width affects (standard) neural networks once they have sufficient capacity for a given modeling task. They show that depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian. They also show that there is a “sweet spot” that maximizes test performance before the limiting GP behavior prevents adaptability, occurring at width = 1 or width = 2."
SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper considers the setting of federated learning (FL) where a group of clients periodically coordinate with a central server to train a statistical model. The authors develop a general algorithmic framework called FedLin to tackle some of the key challenges intrinsic to FL, namely objective heterogeneity, systems heterogeneity, and infrequent and imprecise communication. They show that under these challenges, existing FL algorithms suffer from a fundamental speed-accuracy conflict: they either guarantee linear convergence but to an incorrect point, or convergence to the global minimum but at a sub-linear rate, i.e., fast convergence comes at the expense of accuracy. In contrast, when the clients’ local loss functions are smooth and strongly convex, they show that FedLin guarantees linear convergence to global minimum, and establish matching upper and lower bounds on the convergence rate of FedLin that highlight the effects of infrequent, periodic communication. "
SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the Sliced-Wasserstein (SW) distance, which is an alternative to the Wasserstein distance in machine learning applications. The authors propose a new perspective to approximate SW by making use of the one-dimensional projections of a high-dimensional random vector are approximately Gaussian. Based on this observation, the authors develop a simple deterministic approximation for SW. The method does not require sampling a number of random projections, and is therefore both accurate and easy to use compared to the usual Monte Carlo approximation. "
SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to map the low-dimensional structure in the space of language representations extracted from various NLP models. The authors use an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dim structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embedding. They also show that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI."
SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes Diffusion-Decoding models with Contrastive representations (D2C) for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. It can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, it achieves superior performance over state-of-the-art VAEs and diffusion models."
SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper provides a theoretical analysis of contrastive self-supervised learning. The authors propose a loss function that performs spectral decomposition on the population augmentation graph and can be written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. It follows up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. They further show that this result can be strengthened to a localized version of the feedback edge sets, and provide corresponding lower bounds that complement previous results to provide a complexity classification of the problem. "
SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification in the streaming setting. The algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a set of labeled and weak-labeled points. Theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate the algorithm outperforms standard baselines."
SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov growth (KG), which is used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on the bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised image representation learning that avoids the collapse problem by two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, and (2) another term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. The variance regularization term stabilizes the training of other methods and leads to performance improvements."
SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes a general active-reward learning method for reinforcement learning (RL) where the agent obtains information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. The proposed method, called Information Directed Reward Learning (IDRL), uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In particular, IDRL can use arbitrary Bayesian reward models and arbitrary types of queries (Section 4), making it more general than existing methods. The authors also describe an exact and efficient implementation of IDRL using Gaussian process (GP) reward models (Section 5) and different types of query types (Section 6). "
SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method to predict the parameters of unseen neural network architectures by leveraging the past knowledge of training other networks. The authors introduce a large-scale dataset of diverse computational graphs of neural architectures and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, the authors propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second. The proposed model achieves surprisingly good performance on unseen and diverse networks."
SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the distortion-perception (DP) function for the mean squared error (MSE) distortion and the Wasserstein-2 perception index. It shows that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, it further provides a closed form expression for such estimators. For general distributions, it shows how these estimators can be constructed from the estimators at the two extremes of the tradeoff: the global MSE minimizer and a minimizer of the MSE under a perfect perceptual quality constraint."
SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a new model architecture for the representation learning on textual graph. The main idea is to use GNN components nested alongside the transformer blocks of pretrained language models, where the text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. The proposed model is evaluated on three large-scale benchmark datasets, where it outperforms the baselines with comparable running efficiency."
SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of learning with user-level differential privacy in the central model of DP. In particular, the authors propose algorithms and analyses for the tasks of mean estimation, empirical risk minimization (ERM), stochastic convex optimization (SCO), and learning hypothesis classes with finite metric entropy. The algorithms rely on novel techniques for private mean estimation in arbitrary dimension with error scaling as the concentration radius of the distribution rather than the entire range. The utility analyses assume that all users draw their samples i.i.d. from related distributions, a setting referred to as limited heterogeneity. "
SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper provides a theoretical framework to study the effect of feature learning in deep neural networks (DNNs) trained with noisy gradient descent on a large training set and derive a self-consistent Gaussian process theory to account for strong finite-DNN and feature learning effects. The authors also identify a sharp transition between a feature learning regime and a lazy learning regime in the two-layer fully-connected and non-linear two-layered CNN models. They show that the assumptions required for their theory hold true in more realistic settings (Myrtle5 CNN trained on CIFAR-10).
SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the phenomenon of compositionality in signaling games, where agents communicate over a noisy channel. The authors theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, they prove that compositionality spontaneously arises in the signaling games. They experimentally confirm that a range of noise levels, which depends on the model and data, indeed promotes compositionality."
SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per 4 patch. The proposed model is trained with a modern training strategy using heavy data-augmentation 5 and optionally distillation, and attains surprisingly good accuracy/complexity trade-offs on ImageNet."
SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of online multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, this paper minimizes the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e., the label of a point is given by which of k centers it is closest to in Euclidean distance – this paper shows that one can achieve a loss that is independent of the total number of queries. The paper also shows that learning general convex sets requires an almost linear loss per query."
SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. The experiments also demonstrate the effectiveness of this approach experimentally on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training."
SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the problem of simulating Turing Machines (UTM) using RNNs. Previous works have shown that RNN-based UTM simulations are computationally feasible, but previous works assumed unbounded-precision neurons, which is neither practical in implementation nor biologically plausible. To remove this assumption, the authors propose a dynamically growing memory module made of neurons of fixed precision. The memory module dynamically recruits new neurons when more memories are needed, and releases them when memories become irrelevant. The authors prove that a RNN with growing memory modules can simulate a Universal Turing Machine, with time complexity linear in the simulated machine’s time and independent of the memory size. The result is extendable to various stack-augmented RNN."
SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper provides a theoretical analysis of the under-coverage bias of quantile regression. The main result is that under the assumption that the number of samples $n$ is under-parameterized and the ratio $d/n$ small, the quantile function $\alpha$ has an inherent under-cover bias. The authors show that this bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories. The paper also shows that the bias can be disentangled from other factors such as sample size and model capacity. "
SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the context of class-incremental learning (CIL). In particular, the authors propose a dynamic memory allocation strategy that is optimized for the incremental phases and different object classes. The proposed method, called Reinforced Memory Management (RMM), leverages two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each specific class. RMM is trained on pseudo CIL tasks, e.g., the tasks built on the data of the 0-th phase, and then applied to the target tasks."
SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper considers the problem of speeding up SGD by parallelizing stochastic gradient descent (SGD) in the presence of multiple workers. In particular, it considers the setting where the data set is shared among N workers, who can take SGD steps and coordinate with a central server. While it is possible to obtain a linear reduction in the variance of the variance by averaging all the stochastically gradients at every step, this requires a lot of communication between the workers and the server, which can dramatically reduce the gains from parallelism. The Local SGD method, proposed and analyzed in the earlier literature, suggests machines should make many local steps between such communications. In this paper, the authors suggest a local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. The analysis shows that this can achieve an error that scales as 1/(NT ) with a number of communications that is completely independent of T. The authors also show that $\Omega(N)$ communications are sufficient. Empirical evidence suggests this bound is close to tight as it further shows that $\sqrt{N}$ communications fail to achieve linear speed-up in simulations."
SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the Online Lazy Gradient Descent for linear optimisation on strongly convex domains. The algorithm is known to achieve $O(\sqrt{N} \log N)$ regret against adversarial opponents; here they show it is universal in the sense that it also achieves $O(logN)$ expected regret against i.i.d opponents. This improves upon the more complex meta-algorithm of Huang et al [20] that only gets $\sqrt{\log N}$ regret. The authors also show that unlike for the simplex, the order bounds for pseudo-regret and expected regret are equivalent for strongly-convex domains, which is nice."
SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper analyzes the Bures-Wasserstein (BW) geometry with the Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors show that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. They also show that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesiﬁc convex  under the BW geometry."
SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a framework to evaluate NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which have traditionally been absent from leaderboards. On each task, models are ranked according to a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset."
SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a multi-modal text-to-speech (TTS) model for automatic video dubbing (AVD) task: synthesizing human speech synchronized with the given video from the text. The proposed model uses the lip movement in the video to control the prosody of the generated speech. An image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments are conducted on the chemistry lecture dataset and LRS2 dataset. Results show that the proposed model can generate speech audios on par with state-of-the-art TTS models in terms of speech quality."
SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a novel federated learning framework for collaborative learning of deep neural networks. The proposed framework is based on the recently proposed Vision Transformer (ViT) architecture, which is able to decompose the entire network into parts: the head for extracting features from the input image, the Transformer body to model the dependency between features, and the tail used for mapping features to task-specific output. The authors show that this configuration is optimal for split learning, where a network should be split into the clients and servers. They also show that the proposed framework along with heterogeneous multi-task clients also improves individual task performances including the diagnosis of COVID-19, eliminating the need for sharing large weights with innumerable parameters."
SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable Poisson surface reconstruction (PSR) layer to bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, the proposed Shape-As-Points (SAP) model is more interpretable, lightweight, and yields high-quality watertight surfaces at low inference times."
SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper introduces a new family of inverse problems for recovering representations of corrupted data. The authors assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. They propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking."
SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks. The authors first formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. They introduce an off-Policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents."
SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper proposes a self-supervised predictive loss function (contrastive predictive coding) to model both the ventral and dorsal pathways of the visual system of mice. The authors use the Allen Brain Observatory (ABO) to collect fMRI data from the mouse visual system and train a network architecture with two parallel pathways using a contrastive predictive loss. They show that the proposed method outperforms other models in fitting mouse visual cortex. Moreover, they can model both dorsal and ventral pathways. "
SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper introduces TopicNet, a deep hierarchical topic model that can incorporate prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes a self-supervised pretraining method specifically designed for the task of object detection. The proposed method, called Selective Object COntrastive learning (SoCo), introduces object-level representations via selective search bounding boxes as object proposals; the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN); and pretraining is equipped with object detection properties such as object level translation invariance and scale invariance. SoCo achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework."
SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,This paper proposes a learning-augmented local search framework to solve large-scale vehicle routing problems (VRPs). The method iteratively improves the solution by identifying appropriate subproblems and delegating their improvement to a black box subsolver. The proposed method accelerates state-of-the-art VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000. The results generalize to a variety of VRP distributions and solvers.
SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on Bayesian continual learning and active forgetting. Specifically, the authors propose to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. The proposed method dynamically expands parameters to learn each new task and then selectively combines them, which is formally consistent with the underlying mechanism of biological active forgetting, through regulating the learning-triggered synaptic expansion and synaptic convergence. The method is evaluated on CIFAR-10 regression tasks, a variety of visual classification tasks, and Atari reinforcement tasks, where it achieves the state-of-the-art performance."
SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. It provides a convergence guarantee for the prior guided random gradient-free (PRGF) algorithms. Moreover, it presents a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. Theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides a theoretical analysis of the lottery ticket hypothesis (LTH) by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. It shows that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, it is shown that the number of samples required for achieving zero generalisation error is proportional to number of the non-pruned weights in the hidden layer."
SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper studies the problem of differentially private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. The authors propose two new methods. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Experimental results show that GEM and PEP outperform existing algorithms."
SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a simple mask classification model that predicts a set of binary masks, each associated with a single global class label prediction, for semantic segmentation and instance-level segmentation tasks. The proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentations tasks and shows excellent empirical results. In particular, the proposed method outperforms per-pixel classification baselines when the number of classes is large."
SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies the problem of adversarial perturbations of random two-layer ReLU neural networks with smooth and Lipschitz activation functions. In particular, the authors show that for any sub-exponential width random neural network with smooth activation function, a single gradient step on the weights of the network suffices to find adversarial examples with high probability. The main result is that, while |f(x)| = O(1) with probability, a perturbation of the form = ⌘rf(x), for some $\mathcal{O}(\frac{2}{2} \log(x^2)$ and $O(k)$ weights, is approximately a centered Gaussian with variance $E_N(0,1)$ for large $k$, where $N$ is the number of neurons and $K$ the dimension. "
SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (SGM) approach that trains SGMs in a latent space, relying on the variational autoencoder (VAE) framework. The authors propose a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset."
SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper studies the role of convolutional neural networks (CNNs) and neural tangent kernels (NTKs) in the context of image classification. Specifically, the authors show that CNNs are able to learn to find a sparse signal in the presence of high-variance noise, while NTKs are unable to adapt to the signal in this manner. They further show empirically that as the background noise increases in intensity, a CNN’s performance stays relatively robust, whereas the corresponding NTK sees a notable drop in performance."
SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper provides a tighter analysis of the gradient tracking (GT) method in the stochastic strongly convex, convex and non-convex settings. The analysis improves over all existing results that analyze the GT algorithm. Specifically, they prove a weaker dependence on the connectivity of the network (spectral gap) which is commonly incorporated into the convergence rates via standard parameter p. This improvement was possible due to a new proof technique which could be of independent interest."
SP:24d637e8c3489bfe50b17bf684097776ad6ee485,This paper studies the upper confidence bound (UCB) algorithm for the stochastic multi-armed bandit (MAB) problem. The authors show that the arm-sampling rate of UCB is asymptotically deterministic regardless of the problem complexity. They also show that UCB achieves a near-optimal $O(\log n)$ minimax regret when the gap between the top two arms is arbitrarily small. The paper also provides the first complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling.
SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper focuses on the cross-domain cold-start recommendation (CDR) problem, i.e., how to leverage the information from a source domain, where items are ‘warm’, to improve the recommendation performance of a target domain. Specifically, the authors propose DisAlign, a framework for the CDCSR problem, which utilizes both rating and auxiliary representations from the source domain. The authors first propose Stein path alignment for aligning the latent embedding distributions across domains, and then further propose its improved version, called proxy Stein path, which can reduce the operation consumption and improve efficiency. The empirical study on Douban and Amazon datasets demonstrates the effectiveness of the proposed method."
SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a simple yet computationally efficient architecture, called Global Filter Network (GFNet), that learns long-term spatial dependencies in the frequency domain with log-linear complexity. The architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2d inverse Fourier transforms. The experiments on ImageNet and downstream tasks demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness."
SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of predicting the trustworthiness of a classifier on a large-scale dataset, i.e., ImageNet. The authors propose a new loss function named the steep slope loss to improve the generalizability of trustworthiness predictors. The proposed loss consists of two slide-like curves that face in opposite directions to separate the features w.r.t. correct predictions from the ones that are untrustworthy. Experiments on ImageNet show that the proposed loss improves the generalization ability of the predictors trained with the cross entropy loss, focal loss, and true class probability confidence loss."
SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then initialized with the output of a reconstruction network and solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge as compared to variational methods."
SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a new approach for vision-and-language pre-training, ALBEF, which uses a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. The authors propose a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. They also propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model."
SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. They also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper studies the convergence of stochastic convex optimization methods with non-sub-Gaussian (heavy-tailed) noise. The main contribution of this paper is to derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-smooth convex problems with heavy-tailed noise. In particular, the authors propose novel stepsize rules for two methods with gradient clipping. Moreover, the analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, they provide an extension for strongly convex problem. "
SP:a22a893e25ce739dc757861741014764e78aa820,This paper proposes a Transformer-based long-term forecasting model for time-series forecasting. The authors propose a novel decomposition architecture with an Auto-Correlation mechanism. This design empowers Autoformer with progressive decomposition capacities for complex time series. The proposed method outperforms the state-of-the-art in both efficiency and accuracy.
SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset of cryptic crossword clues for NLP systems that seek to process compositional language in more creative, human-like ways. The authors also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies. Finally, the authors propose a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words."
SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the role of self-attention and residual connections in the internal representation structure of Vision Transformers (ViTs) and CNNs on image classification tasks. The authors find that ViTs have more uniform representations across all layers, and that skip connections in ViTs are even more influential than in ResNets, having strong effects on representation similarity and representation similarity. They also study the effect of (pretraining) dataset scale on intermediate features and transfer learning."
SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) algorithms for combinatorial multi-armed bandit (CMAB) problems under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) optimization problems. The authors provide a problem-dependent regret lower bound of order $\Omega(\log T/2)$ to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper considers the problem of federated learning, i.e. the distributed learning setting where each agent has access to a set of parameters and the goal is to learn a model that minimizes the average error of all the other agents. In this setting, each agent may not wish to federate with every other potential agent, and instead it must determine which of the exponentially many combinations of players it would prefer. The paper provides an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyzes the relationship between the stability and optimality of an arrangement. "
SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. It computes capsule decompositions of objects through permutation-equivariant attention, and learns the process by training with pairs of randomly rotated objects. The proposed method outperforms the state-of-the-art on 3d point cloud reconstruction, canonicalization, and unsupervised classification."
SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches."
SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the generalization properties of feature averaging in kernel ridge regression (KRR) with invariance enforced by feature averaging. In particular, the authors derive a generalization bound on the gap between the test error of two predictors f and the difference in their test errors. The generalization gap is defined as the difference between the effective dimension of the feature space of the target and the space of functions on which the target is invariant to the action of a compact group. The authors show that this bound is strictly non-zero when the target function is a function of the group and the group action induces an orthogonal decomposition of the reproducing kernel Hilbert space and its kernel. "
SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a method for composing neural networks with itself to create an ensemble-like model with uncertainty that is useful for out-of-distribution detection, adversarial example detection, and model calibration. The authors propose to train a neural network classifier jointly with a generative model whose role is to map the classifier’s activations back to the input, approximating invertibility. They further explore this compositionality by combining DecNN with pretrained models, where they show promising results that neural networks can be regularized from using protected features."
SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators for optimal transport (OT) maps. The main contribution is a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general OT estimators. The authors first provide rates of convergence for the natural discretediscrete and semi-discrete estimators of OT maps and then use the same stability estimate to show that, under additional smoothness assumption of Sobolev type or Besov type, kernel smoothed or wavelet based plug in estimators respectively speed up the rates of convergences and significantly mitigate the curse of dimensionality suffered by the natural discrete/semi discrete/kernel smoothed/wavelet-based estimators in terms of the rate of convergence. As a by-product of the analysis, the authors also obtain faster rates for the Wasserstein distance between two probability distributions, thereby complementing recent results in Chizat et al."
SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed meta-learning framework for dataset distillation using infinitely wide convolutional neural networks. The proposed method extends the methods of Nguyen et al. (2021) to obtain new state-of-the-art (SOTA) results. The distilled datasets are effective for both kernel ridge-regression and neural network training and achieve an impressively wide margin over prior art, including over 25% and 37% accuracy gain in accuracy on CIFAR-10 and SVHN image classification."
SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an approach for semi-supervised learning (SSL) based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. The paper also proposes an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. The proposed approach achieves state-of-the-art performance on three datasets and outperforms a fully supervised model on CIFAR10."
SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper presents Latent Explorer Achiever (LEXA), an agent for unsupervised RL that explores its environment, learns to achieve the discovered goals, and solves image-based tasks in a zero-shot way. LEXA learns a world model from image inputs and uses it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. The proposed method is able to solve challenging downstream tasks specified as images without any supervision such as rewards or demonstrations."
SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper proposes a low-rank factorized representation of matrices that underpin dense, embedding, and self-attention layers to reduce the number of trainable parameters in transformers. Specifically, the authors propose to reshape and rearrange the reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical results show that stacking such low rank layers increases their expressiveness, providing theoretical understanding for their effectiveness in deep networks. In Transformer models, the proposed approach leads to more than tenfold reduction in the total number of parameters, including embedding and feed-forward layers, with little degradation in performance."
SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes to incorporate path encoding into the attention module of Transformer. Specifically, the authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. The authors also explore the interaction between these two kinds of paths by integrating them into the unified Transformer framework. Experiments on code summarization across four different languages demonstrate the effectiveness of the proposed approach."
SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a Transformer-based generator for high-fidelity image generation for GANs. The authors propose to replace the standard global self-attention in low-resolution stages of the generative process with the proposed multi-axis blocked self attention, which allows efficient mixing of local and global attention. In high-resolution, the proposed HiT proposes to drop the global attention while only keeping the multi-layer perceptrons. To further improve the performance, the authors introduce an additional self-modulation component based on cross-att attention. The proposed model has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. "
SP:41a6753bc56eb16040600666a859294ae36cfa9c,"This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors assume that the vertices are geodesic convex, i.e., for every pair of vertices with the same label it holds that every vertex on every shortest path between them also has that label. This assumption is used, for example, by biologists on gene similarity networks [Zhou et al., 2002] and cancer-related protein-protein-interaction networks [Li et al, 2012]. It also typically holds for connected subgraphs of collaboration networks [Marc and Šubelj, 2018]. The authors derive bounds on the number of queries required to determine all labels, that is, the queries complexity, using concepts from convexity theory and Geodesic Convexity Spaces (GC). "
SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL). The proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoders and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Experiments show that the proposed method can significantly enhance the performance of existing TAL methods."
SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The main contributions are three-fold. First, it provides general formulae for the derivatives of regularized M-stimators where differentiation is taken with respect to both y and X. Second, it characterizes the distribution of the residual ri = yi-xi \beta^T in the intermediate high-dimensional regime where dimension and sample size are of the same order. Third, it proposes a novel adaptive criterion to select the tuning parameters of regularizer."
SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a calibrated beam-based summarization algorithm with global awareness of the global attention distribution for neural abstractive summarization. Specifically, a global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a transformer-based attention mechanism that is equivariant to the orientation of local coordinate systems (i.e., gauge-equivariant) and rotation invariant. The proposed method employs multi-head self-attention to jointly incorporate both position-based and content-based information. To enhance expressive ability, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields. Experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes an approach for unsupervised learning of finite mixture models by combining the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of stochastically sampled components, thus substantially reducing the computational cost. The authors put emphasis on generality of the method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations. The performance of the proposed approach is illustrated in a variety of synthetic and real-data contexts, considering deep models, such as mixtures of normalizing flows and sum-product (transform) networks."
SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes an efficient sparse training method with completely sparse forward and backward passes. It first formulate the training process as a continuous minimization problem under global sparsity constraint. Then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, it uses the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, it proposes a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, the proposed algorithm is much more effective in accelerating training process."
SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper proposes a novel non-equilibrium importance sampling (NEO-IS) and Markov chain Monte Carlo (MCMC) samplers for estimating the normalizing constant of a complex distribution $p(x) = \sum_i \in \mathbb{R}^d \mathbf{x}$. The proposed method is based on the idea of non-Equilibrium orbits, which is a family of deterministic mappings from $p$ to $R$ that does not leave the target $\pi$ invariant, hence the name NEO-IS. The proposed methods are unbiased estimators of $Z$ and self-normalized IS and MCMC estimators, respectively. Theoretical results are also provided for the proposed methods."
SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a method for large-scale mini-batch set encoding of sets, where the set size is too large to fit in the memory or the data arrives in a stream. The authors introduce a new property termed Mini-Batch Consistency (MBC) that is required for large scale set encoding. They also propose a scalable and efficient attention-based set encoding mechanism that is amenable to mini- batch processing of sets and capable of updating set representations as data arrives. The proposed method adheres to the required symmetries of permutation invariance and equivariance as well as maintaining MBC for any partition of the input set."
SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to perform value iteration while learning a policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, the authors train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. Additionally, they extend their methods to full-scale no-press Diplomacy. They show that this agent plays a strategy that is incompatible with human-data bootstrapped agents."
SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors propose two strategies to select the attention heads for different languages or domains. The first strategy is based on the similarity between languages (or domains) and the second strategy is to learn to share heads among a subset of languages and domains. Experiments are conducted on various tasks including speech recognition, text-to-text and speech to text translation. Results show that the proposed attention sharing strategies improve the performance of the model."
SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the generalization error of random feature regression in the setting of linear regression. The authors derive exact bounds for the test error, bias, and variance of the random feature kernel under covariate shifts. They also provide an exact linear relationship between the in-distribution and out-of distribution generalization performance, which provides an explanation for the recent observation that overparameterized models are more robust to distribution shifts."
SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling (TS) and other Bayesian sequential decision-making algorithms to misspecified priors in the context of Bayesian meta-learning. The authors prove that the expected reward of TS with a misspecification of the prior is $\tilde{O}(\epsilon^{-1}(H^2)$, where H is the learning horizon and $H$ is the total-variation distance between the priors and the action space. They show that this result is tight up to universal constants in the worst case. They also establish generic PAC guarantees for algorithms in the recently studied Bayesian Meta-learning setting and derive corollaries for various families of priors."
SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and Equivalence-Query-learning (EQ-learning) models. The authors prove an exponential separation for the sample/query complexity between the two models. They also discuss how their result relates to adversarial robustness and generalization. 
SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection, i.e., given some data and a source model, can you quickly predict the model’s accuracy after fine-tuning. The authors formalize this setting as “Scalable Diverse Model Selection” and propose several benchmarks for evaluating on this task. They find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. They then introduce simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,This paper proposes a method for learning low-dimensional binary codes (LLC) for both instances and classes. The proposed method learns extremely low dimensional binary codes for both classes and instances. The learned codes are super-efficient while ensuring nearly optimal classification accuracy for ResNet50 on ImageNet-1K. The method is applied to efficient image retrieval and out-of-distribution (OOD) detection problems.
SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a method of Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. GDWS is scalable to large problem sizes since it operates on pre-trained models and doesn’t require any additional training. The paper also provides exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. Experiments on CIFAR-10, SVHN, and ImageNet datasets demonstrate the effectiveness of GDWS."
SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a method for retrosynthesis prediction for the USPTO-50k dataset, which consists of 50000 reactions across 10 reaction classes. The authors propose a graph-based approach that capitalizes on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. The proposed method achieves a top-1 accuracy of 53.7%, outperforming previous template-free and semi-template-based methods."
SP:772277d969c95924755113c86663fb0e009f24cc,"This paper considers the problem of statistical downscaling of low-resolution (LR) spatial fields with high-resolution information. The authors propose a Bayesian formulation of deconditioning, which naturally recovers the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1]. They extend decongitioning to a downscaled setup and devise an efficient conditional mean embedding estimator for multiresolution data. They show that this solution can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. "
SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,This paper proposes a neural architecture search (NAS) method for deep sparse networks (DSNs). The authors propose a distilled search space to cover the desired architectures with fewer parameters and develop a progressive search algorithm for efficient search on the space. Experiments on three real-world benchmark datasets show promising results of PROFIT in both accuracy and efficiency. 
SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data. In particular, the authors analyze the generalization and robustness properties of finetuning and propose regularized self-labeling, which is a combination of layer-wise regularization, label correction, and label reweighting. The authors derive a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the finetuned model, and empirically measure these quantities. The proposed method outperforms baseline methods on several image and text classification tasks."
SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the multi-armed bandit best-arm identification problem, where the goal is to identify the arm with the smallest CVaR, VaR, or weighted sum of CVaRs and VaRs. The main contribution is an optimal $\delta$-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically. The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The authors develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, a vision transformer architecture that incorporates two types of intrinsic inductive bias (IB) into transformers, i.e., locality and scale-invariance. The authors propose to use spatial pyramid reduction modules to embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, it has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model over baseline transformer and concurrent works."
SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper studies the problem of adversarial fine-tuning of pre-trained language models. The authors claim that adversarial training, the prevalent defense technique, does not directly fit a conventional fine- tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-training model. To address this problem, the authors propose Robust Informative Fine-Tuning (RIFT), a novel adversarial learning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to keep the features learned from the pretrained model throughout the entire fine tuning process. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks."
SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper proposes a second-order method based on Anderson mixing (AM) for nonconvex stochastic optimization problems. In particular, the authors introduce damped projection and adaptive regularization to the classical AM, and propose a Stochastic Anderson Mixing (SAM) scheme to solve the non convex optimization problem. Under mild assumptions, the convergence theory of SAM is established, including the almost sure convergence to stationary points and the worst-case iteration complexity. Moreover, the complexity bound can be improved when randomly choosing an iterate as the output. The authors also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of our method."
SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper introduces a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing."
SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a framework named MetaHG to automatically detect drug traffickers on social media (i.e., Instagram), by tackling the following two new challenges: (1) different from existing works which merely focus on analyzing post content, the proposed framework is capable of jointly modeling multimodal content and relational structured information, and (2) it addresses the issue of requiring sufficient data for model training. More specifically, in this paper, the authors first build a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on Instagram. Then, they employ a relation-based graph convolutional neural network to learn node representations over the built HG, in which they introduce graph structure refinement to compensate the sparse connection among entities in the HG for more robust node representation learning. Afterwards, they propose a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model."
SP:242da1384f48260d58a0e7949438611c05079197,"This paper investigates the question of whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on the number of layers) in a neural network with ReLU activations and a given architecture. The authors use techniques from mixed-integer optimization, polyhedral theory, and tropical geometry to provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. They also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper studies the use of min-max optimization beyond adversarial training (AT) for adversarial attack generation. The authors propose a general framework for reformulating the worst-case attack loss as a min-min optimization problem, which can be reformulated for three types of adversarial attacks: model ensemble attack, universal attack, and robust attack over data transformations. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method. "
SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SSTM), which is a generalization of both sparse PCA (in its Wigner form) and tensor PCA. The authors propose a polynomial-time algorithm and an exponential-time exhaustive search algorithm for recovering the k-sparse unit vector x in the highly sparse regime of $k \leq \sqrt{n}$. They also present a lower bound against low-degree polynomials, which extends the known lower bounds for both the sparse and the tensor versions of the problem. The results extend to the case of r distinct $k\leq r$-s sparse signals with disjoint supports."
SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. Experiments on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes."
SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of finding the quantal response equilibrium (QRE) of a two-player matrix game with entropy regularization. The authors propose two algorithms, PU and OMWU, for solving entropy-regularized zero-sum Markov games, whose last iterates are guaranteed to converge linearly to the QRE at a linear rate. The rates are nearly dimension-free, which are independent of the size of the state and action spaces up to logarithm factors. In addition, the authors show that the algorithms can also be used to locate Nash equilibria of the unregularized matrix game at a sublinear rate without assuming the Nash equilibrium to be unique."
SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes a method for super-resolution of low-resolution (LR) screen content images (SCIs) at arbitrary resolutions. The authors propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for high-quality continuous SR at arbitrary ratios. The pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.
SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning interventional distributions using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The method is motivated and illustrated by a structural causal model themed around personal health. The empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate. This shows that the dream of tractable causal models is not insurmountable."
SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a deep Markov factor analysis (DMFA) method for high-dimensional functional magnetic resonance imaging (fMRI) data. The authors propose a generative model that employs Markov property in a chain of low dimensional temporal embeddings together with spatial inductive assumptions, all related through neural networks, to capture temporal dynamics in fMRI data, and tackle their high spatial dimensionality, respectively. The proposed method is able to cluster the data in its low-dimensional temporal embedding with regard to subject and cognitive state variability, and enables validation of a variety of fMRI-driven neuroscientific hypotheses."
SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper introduces a new class of generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. The authors introduce Moser Flows (MF), a new generative model within the family of continuous normalizing flows (CNF). The model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity."
SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-linear independent component analysis (ICA) and blind source separation (BSS) in the context of unsupervised representation learning. In particular, the authors consider the setting where additional, typically observed variables are included in the generative process. The authors propose an approach based on the principle of independent causal mechanisms (ICM), which is motivated by thinking of each source as independently influencing the mixing process. They provide theoretical and empirical evidence that their approach circumvents a number of nonidentifiability issues arising in nonlinear Blind Source Separation. "
SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a differentiable variant of Annealed Importance Sampling (AIS) with Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing (UHA). The main idea is to use an AIS-like procedure with an uncorrected MCMC instead of the accept-reject steps in Hamiltonian AIS, which is motivated by the fact that Hamiltonian dynamics sometimes have high acceptance rates. The authors show that UHA leads to tight and differentiable lower bounds on the log-normalization constant of the unnormalized target distribution. They also show empirically that their method yields better performances than other competing approaches."
SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes an efficient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, they eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. They also propose to clip activation functions with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local LPschitz bound."
SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a scalable method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees for Bayesian posterior predictive distributions. Conformal inference provides finite sample frequentist guarantees on predictive confidence intervals without the requirement of model fidelity. The proposed method is based on ‘add-one-in’ importance sampling, which can construct conformal predictive intervals from any Bayesian model given model parameter values from the posterior values. The authors demonstrate the utility on a range of examples including extensions to partially exchangeable settings such as hierarchical models."
SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoising-by-denoising (RED) and plug-and-play priors (PnP) methods. The authors introduce image denoisers derived as the gradients of smooth scalar-valued deep neural networks, acting as potentials, which ensures two things: (1) they display symmetric Jacobians, allowing for MAP and MMSE estimators interpretation; (2) they can be integrated into RED and PnP schemes with backtracking step size, removing the need for enforcing their Lipschitz constant. Theoretically, the authors show that the proposed method converges to stationary points of an underlying objective function consisting of the learned potentials. Numerical experiments on various imaging tasks show improved results compared to the state-of-the-art methods."
SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music that is in-sync with the human body movements in videos. The proposed method, called RhythmicNet, takes as an input a video with human movements and generates a soundtrack for it. The method first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Next, it implements a transformer-based model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and offsets of the instruments. Additional types of instruments are added to the soundtrack by further conditioning on generated drum sounds. "
SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"This paper proposes a simple one-step baseline for offline reinforcement learning (RL) that only performs one step of policy improvement using an on-policy Q estimate of the behavior policy. The proposed algorithm is shown to outperform the iterative approaches on a large portion of the D4RL benchmark. The authors argue that the performance is due to the high variance inherent in doing off-policy evaluation and magnified by the repeated optimization of policies against those estimates. They also hypothesize that the strong performance of the one-stage algorithm is due a combination of favorable structure in the environment and behavior policy, and suggest that if the behaviour policy is already fairly good but does not have full coverage of the state-action space, then one step algorithms are often preferable."
SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies convex relaxations for low-rank and nonsmooth matrix optimization problems. The authors prove that under a generalized strict complementarity condition, the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate $O(1/t)$ while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method. They support their theoretical results with empirical experiments on several nonsmoothing low rank matrix recovery tasks, demonstrating that using simple initializations, the method produces exactly the same iterates when the full-rank SVD is replaced with SVD of rank matching the low rank."
SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a method to estimate the conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts) from observational data. The authors propose a generalized Robinson decomposition (GRD) to isolate the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and possesses a quasi-oracle convergence guarantee under mild assumptions. In experiments with small-world and molecular graphs, the authors demonstrate that their approach outperforms prior work in CATE estimation."
SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper proposes a new method for causal effect identification, which uses both graphical criteria and matrix equations. Specifically, the authors first characterize the relationships between certain graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a causal effect ID algorithm, which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach."
SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper proposes a new approach to unsupervised environment design (UED) based on prioritized level replay (PLR). The authors argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD), which includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. The authors also provide a novel theory for PLR with a robustness guarantee at Nash equilibria. "
SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper considers the multi-armed bandit (MAB) problem in the shuffle model, where the reward distribution is unknown, and the goal is to minimize the expected regret. Differentially private versions of the MAB problem have been considered in various settings, such as the centralized model, the local model and the distribution-dependent model. This paper provides an algorithm with a (\epsilon, \delta)-differentially private algorithm, which has a distribution-independent regret of $O(\sqrt{k\log T} + k \frac{k}{\delta}log T + \kappa^2}$ where $k$ is the number of arms and $\delta$ is a constant. The authors show that their algorithm matches the regret upper bound of the best known algorithms for the centralized and local models, and significantly outperforms the upper bound for the distribution independent case."
SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration, called threshold calibration, to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm that takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated Forecaster. The procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. The proposed method is evaluated on two real-world scenarios: hospital scheduling decisions and resource allocation decisions."
SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The method optimizes a set of centroid points to compactly approximate the argmin distribution with a simple objective function. Theoretically, the argmax centroid method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth arg max distribution and the centroids approximation under proper conditions. The authors demonstrate the applicability and effectiveness of the method on a variety of real-world multitask learning applications, including few-shot image classification, personalized dialogue systems and multi-target domain adaptation."
SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers multi-objective reinforcement learning (MORL) where the objectives are balanced using preferences. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The authors provide a model-based algorithm that achieves a nearly minimax optimal regret bound. This result partly resolves an open problem raised by Jin et al [2020]."
SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a local explanation of response generation (LERG) method for dialog response generation. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. The authors show that LERG adheres to desired properties of explanation for text generation, including unbiased approximation, consistency, and cause identification. Empirically, the method consistently improves other widely used methods on proposed automaticand humanevaluation metrics for this new task by 4.4-12.8%."
SP:965413b1726617006317bbbec55673dd5d21812a,"This paper proposes a method to accelerate the convergence of gradient compression in distributed learning. The main idea is to apply a lossy compression transformation to the messages before they are communicated so as to save on communication time. In particular, they propose and study the error compensated loopless Katyusha method and establish an accelerated linear convergence rate under standard assumptions. They show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a new method for tuning the weights of a liquid state machine (LSM) that is inspired by astrocytes, a long-neglected non-neuronal brain cell that modulates synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. The proposed method, called neuron-astrocyte LSM (NALSM), integrates neuronal activity and provides global feedback to spike-timing-dependent plasticity (STDP), which self-organizes NALSM dynamics around a critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST, N-MNIST, and Fashion MNIST datasets show that the proposed method achieves comparable performance to current fully connected multi-layer spiking neural networks trained via backpropagation."
SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,This paper studies the problem of graph topology imbalance (TINL) in semi-supervised node classification. The authors propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. They further increase the training weights of nodes with small conflict that are highly likely to be close to top class centers to make them play a more pivotal role during training and vice versa. Empirical results demonstrate the effectiveness and generalizability of the proposed method in relieving the TINL issue and promoting node classification performance.
SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper considers the problem of recovering the partition of the lattice induced by the constancy regions of the unknown signal, using the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k\sqrt{log(N)/kappa, where k is the minimal number of rectangular sub-graphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, $\kappa$ is the smallest magnitude of the signal difference among contiguous elements, and N is the size of lattice. The authors further extend to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,This paper proposes a method for counterfactual maximum likelihood estimation (CMLE) based on the Structural Causal Model (SCM) to reduce the spurious correlations caused by observed confounders. Theoretical analysis on the underlying general SCM is provided and two algorithms are proposed to perform MLE on the interventional distribution instead of the observational distribution. Experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning show that the proposed method outperforms the regular MLE method in terms of out-of-domain generalization performance.
SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper proposes a new method for multi-task learning, CAGrad, which minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. The authors show that vanilla gradient descent (GD) and the multiple gradient descent algorithm (MGDA) are special cases of the proposed method. The proposed method is evaluated on a series of challenging multi-supervised and reinforcement learning problems and shows improved performance over prior state-of-the-art methods."
SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning, i.e., the task of identifying algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small set of examples. They explore how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. They also investigate whether some strong pre-training priors can be distilled from data, making learning from a few examples possible."
SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method to explicitly distill feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, it injects noise variation to each feature unit and evaluates the information flow in the feature representation to dichotomize feature units either robust or brittle, based on the noise variation magnitude. Through comprehensive experiments, it shows that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, it presents an attack mechanism intensifying the gradient of non-Robust features that is directly related to the model prediction."
SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the support vector proliferation (SVP) phenomenon, which is a connection between support vector machine (SVM) and ordinary least squares (OLS). The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for SVP in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and bound the width of this transition. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the classical active pure exploration problem in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. They also provide the first algorithm with an instance-specific sample complexity in this setting."
SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model, ConE, for multi-hop reasoning over knowledge graphs. ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations. The authors also design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper proposes a novel numerical scheme for implementation of the corresponding value iteration (VI) algorithm in the conjugate domain. In particular, with a discretization of size X and U for the state and input spaces respectively, the proposed approach reduces the time complexity of each iteration in the VI algorithm from $O(XU)$ to $O(\X + U)$ by replacing the minimization operation in the primal domain with a simple addition in the dual domain. Detailed analyses of the convergence, time complexity, and error of the proposed algorithm are provided."
SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a video-audio-text (VATT) framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. VATT is trained end-to-end from scratch and evaluates its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text to video retrieval. The authors also study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. The results show that the convolution free VATT outperforms state-of-the-art ConvNet-based architectures in downstream tasks."
SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes to replace the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. Experiments on the Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or better performance than the full self-attention while requiring fewer computation resources.
SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data-efficient learning from parametric experts. Specifically, the authors propose an augmented policy cloning (APC) approach, which combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. They show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and they also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes an approach to improve the robustness of computer vision models against corruptions and corruptions of the input images. The authors propose a framework that leverages the capability of deep neural networks to design “robust objects,” i.e., objects that are explicitly optimized to be confidently classified. The framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments. "
SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the problem of data augmentation in off-policy Reinforcement Learning (RL) from visual observations. The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique to stabilize this class of algorithms under augmentation. The proposed method, SVEA, improves stability and sample efficiency of ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. "
SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the Federated Learning (FL) problem, where multiple worker nodes (WNs) collaborate with the goal of learning a joint model, by only using local data. The authors propose the Stochastic Two-Sided Momentum (STEM) algorithm, which utilizes certain momentum-assisted stochastic gradient directions for both the WN and the SN updates. They show that there exists an optimal trade-off between the minibatch sizes and the number of local updates, such that the WNs use the minimum number of samples and communication rounds to achieve the desired solution. This is the first FL algorithm that achieves such near-optimal sample and communication complexities simultaneously."
SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the generalization properties of SGLD with isotropic noise. The authors show that the noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. Theoretically, the authors prove that the optimal noise is quite close to the empirical gradient. Then, they develop a new information-theoretical bound that enables such an optimization analysis. "
SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a video compression framework that uses one model to support all possible prediction modes. The motion compensation module applies multiple 3D motion vector fields for weighted trilinear warping in spatial-temporal space. The voxel flows convey the information of temporal reference position that helps to decouple inter prediction modes away from framework designing. The flow prediction module can predict accurate motion trajectories with a unified polynomial function.
SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper introduces and analyzes MT-OMD, a multitask generalization of Online Mirror 1 Descent (OMD) that operates by sharing updates between tasks. Theoretical results show that the regret is of order $\sqrt{\frac{1}{\epsilon} + \sqrt{2}(N-1) \times T$, where $N$ is the number of tasks, $T$ the time horizon, and $\sigma$ the task variance. This improves upon the $O(N^{-1})$ bound obtained by running independent OMDs on each task. Experiments on several real-world datasets support the theoretical findings."
SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the problem of sampling from a high-dimensional strongly log-concave underdamped Langevin diffusion (ULD) with sum-decomposable potential function. The authors propose an efficient discretization method, called Acceerated ULD-MCMC (ALUM), which requires O(N + d 1 3N^2 3 / \epsilon^3 ) gradient evaluations to achieve $\epsilons$-error for approximating d-dimensional ULD. The paper also proves a lower bound of gradient complexity as $\Omega(N+d 1 3 N^2^3)$, which indicates that the proposed method is optimal in dependence of N, \varepsilon, and d. In addition, the authors also propose VRALUM, which uses two gradient evaluations at each iteration to achieve constant speedup. Experimental results on both synthetic and real-world data show that ALUM outperforms existing ULD approaches."
SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"This paper proposes a method for continual learning of feedforward and recurrent neural networks. The proposed method is a combination of two existing methods: weight regularization and projected gradient descent. In particular, the proposed method uses Bayesian weight regularizer to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. Experiments on MNIST and CIFAR-10 datasets show that the method outperforms the existing methods."
SP:26de056be14962312c759be5d284ef235d660f9c,This paper proposes two methods to backpropagate through the volume term arising from the injective change-of-variable formula in Normalizing flows. The first method involves exact evaluation of this term and its gradient which incurs higher memory cost. The second method uses conjugate gradients and Hutchinson’s traceator to obtain unbiased estimates. Both methods perform end-to-end nonlinear manifold learning and density estimation for data projected onto this manifold. The authors show the trade-offs between the proposed methods and empirically verify that they outperform approaches ignoring the volume-change term by more accurately learning manifolds and the corresponding distributions on them.
SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a framework for inference and learning in neural networks with slow components by harnessing the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. The proposed model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. The authors demonstrate successful learning of standard benchmark datasets, achieving competitive performance."
SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new graph representation learning method, called Nested Graph Neural Networks (NGNNs), which is based on the Weisfeiler-Lehman (1-WL) algorithm. The key idea of NGNN is to learn a subgraph around each node and apply a base GNN to each subgraph to learn the subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis is provided to show that NGNNs is strictly more powerful than 1-wl. In particular, it is proved that NGN can discriminate almost all r-regular graphs, where 1-Wl always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant factor higher time complexity."
SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes nested variational inference (NVI), a family of methods that learns proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model, and (c) perform amortized inference in hierarchical deep probabilistic models. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristic to guide the sampler. Experiments demonstrate that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size."
SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of zeroth-order black-box optimization of a Lipschitz function f on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral $\max(f) - f(x) + \epsilon$. This result was only (and partially) known in dimension d = 1, and solves an open problem dating back to 1991. The authors also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes a new type of attack on confidence estimation methods for deep neural networks (DNNs). The attack is based on perturbations of the network parameters (softmax score, deep ensemble, MC-dropout) and selective classification. The attack can be applied in both black-box and white-box settings, where the attacker can only query the attacked model for predicted labels and has no knowledge of the model itself. The proposed attack is shown to be able to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimations."
SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the community detection problem in dynamic networks, where nodes are revealed one at a time in random order. The authors introduce a simple model for networks growing over time which they refer to as streaming stochastic block model (StSBM). Within this model, they prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. "
SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. They also study the reverse problem, identifying which regularizers (e.g., group quasi-norms, the k-support-norm, elastic net) can be the regularizers induced by simple l2-regularization and designing parameterizations that do so."
SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper presents a method for reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Given a source entity and a binary relation (eq,rq), the proposed method first retrieves k entities that are similar to eq and rq, and then finds a set of reasoning paths that connect the retrieved entities to the query entities. The retrieved paths are connected via the query relation rq. The proposed method achieves new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122."
SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,This paper presents an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links. The model achieves the state-of-the-art performance on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The authors also present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data. 
SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes to use ensemble Active Learning (AL) methods to perform data subset acquisition at a large scale (10k to 500k samples at a time) with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows them to automatically and efficiently perform a training data subset search for large labeled datasets. They observe that their approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset."
SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. The routing mechanism is designed via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routes. The proposed method improves performance on benchmark datasets such as CIFAR-10 and CifAR-100 and it performs at-par with a powerful CNN (ResNet-18) with 4x fewer parameters."
SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a novel approach to hyperparameter optimization for deep learning models. The main idea is to use the parallel tempering technique of statistical physics to allow the hyperparameters to exchange during the training process. The authors argue that this is motivated by the fact that the final trained model is not characterized by a particular point of the hyper parameter space, but rather by a path or history in the joint hyper parameter/model-parameter space. The proposed approach is evaluated on dropout and learning rate optimization tasks, and the authors show that the proposed approach outperforms the baselines."
SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper provides a theoretical analysis of Reinforcement Learning (RL) methods for machine translation (MT). The authors show that Reinforce (REINFORCE) does not optimize the expected reward, and that other methods take an infeasibly long time to converge. They also show that the performance gains observed in the literature likely stem not from making target tokens the most probable, but from unrelated effects, such as increasing the peakiness of the output distribution (i.e., the probability mass of most probable tokens)."
SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the large-sample behaviors of the Q-value estimates with closed-form characterizations of the asymptotic variances. This allows the authors to efficiently construct confidence regions for Q-values and optimal value functions, and to develop policies to minimize their estimation errors. The authors propose an exploration strategy that relies on estimating the relative discrepancies among the Q estimates. Numerical experiments show superior performances of the exploration strategy than other benchmark approaches."
SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"This paper proposes a method to improve the efficiency of Top-k recommendation by representing users and items as binary codes. The proposed method, called collaborative generated hashing (CGH), is designed to learn hash functions through the Minimum Description Length (MDL) principle; thus, it can deal with various recommendation settings. In addition, CGH initiates a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages for recommendations in various settings over competing baselines."
SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"This paper proposes Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. The main application is pharmacogenomics where the goal is to predict drug response in patients using their genomic information. The proposed method outperforms state-of-the-art pharmacogenomic and transfer learning baselines and may guide precision oncology."
SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward. The proposed approach is tested on a collection of benchmark tests including simulated robotic locomotion.
SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a method for detecting adversarial examples by using capsule networks. Capsule networks allow a model to reconstruct the pose parameters of the winning capsule and then detect adversarial images by comparing the difference between the reconstruction distributions for natural and adversarial (or otherwise corrupted) images. The authors extend this detection mechanism to standard convolutional neural networks and show its effectiveness against black box and white box attacks on three image datasets; MNIST, FashionMNIST and SVHN. They show that capsule models achieve the strongest attack detection rates and accuracy on these attacks."
SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the effect of initialization and activation function on the Neural Tangent Kernel (NTK) of deep neural networks. In particular, the authors show that only the Edge of Chaos (EOC) initialization leads to an NTK that is invertible for deep networks. They also show that the smoothness of the activation function plays a major role in the behavior of NTK, and that a class of smooth activation functions discussed in (Hayou et al., 2019) also improves the training dynamics compared to ReLU-like activation functions (see also Clevert et al 2016)."
SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. The proposed method is based on contrastive framework, which aims at mapping close input sentences to close representations while separating unrelated sentences. By contrasting different linguistic views, the proposed method aims at building embeddings which better capture semantic and are less sensitive to the sentence outward form."
SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper proposes a transfer learning approach for financial sentiment classification. The authors further pretrain a language model on a domain specific unlabeled corpus and fine-tune it for sentiment analysis (FinBERT). This work is the first application of BERT for finance to the best of my knowledge and one of the few that experimented with further pre-training on the domain-specific corpus. On both datasets, FinBERT achieved state-of-the-art results by a significant margin."
SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a new task of singing voice generation without pre-assigned scores and lyrics, in both training and inference time. The proposed tasks are either unconditioned or weakly conditioned. The authors outline the associated challenges and propose a pipeline to tackle these new tasks. This involves the development of source separation and transcription models for data preparation, adversarial networks for audio generation, and customized metrics for evaluation. "
SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"This paper proposes a novel adversarial defense technique that leverages a latent high order factorization of the network. Randomization is applied in the latent subspace, therefore resulting in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization. The approach can be easily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. The authors empirically demonstrate the effectiveness of the approach on standard image classification benchmarks."
SP:762729b64c1c1494de0f7410ea3662da61e93b6d,"This paper proposes a clustered graph transformer framework that integrates graph attention network and transformer under an encoder-decoder architecture to address the unsmoothness issue of urban data. Specifically, the authors propose two novel structural components to refine the architectures of those existing deep learning models. In spatial domain, they propose a gradient-based clustering method to distribute different feature extractors to regions in different contexts. In temporal domain, it proposes to use multi-view position encoding to address periodicity and closeness of urban time series data. Experiments on real datasets obtained from a ride-hailing business show that the proposed method can achieve 10%-25% improvement than many state-of-the-art baselines."
SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper analyzes the deep Q-network (DQN) algorithm from both algorithmic and statistical perspectives. In particular, it focuses on the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network used in DQNs. Under mild assumptions, the authors show that the rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI. The statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques such as experience replay, which are crucial to the empirical success of the proposed algorithm."
SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes a new attention architecture called PhraseTransformer to represent the semantic composition of words in natural language. The authors argue that the phrases play an important role in attention. Besides representing the words of the sentence, the proposed architecture introduces hypernodes to represent candidate phrases in attention, which can represent the semantics of the candidate phrases. The experimental results on WMT16 English-German translation task show the effectiveness of the proposed method. "
SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper proposes a modular neural network architecture for learning algorithms given a set of input-output examples. The architecture consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, MAIN uses a general domain-agnostic mechanism for selection of modules and their arguments. The authors evaluate MAIN on five algorithmic tasks and show that it can learn policies that generalizes perfectly to inputs of much longer lengths than the ones used for training."
SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy. This paper proposes a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCDA), for determining the sensitivity of deep neural networks to quantization in floating point arithmetic. The method applies Monte Carlo Arithmetic to the inference computation and analyzes the relative standard deviation of the neural network loss. The authors evaluate our method on pre-trained image classification models on the CIFAR-10 and ImageNet datasets."
SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). In particular, the authors identify a fundamental issue of the standard MBRL framework – what they call the objective mismatch issue, which arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, this mismatch is between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. The authors propose an initial method to mitigate the mismatch issue by reweighting dynamics model training."
SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper presents a new targeted blackbox transfer-based adversarial attack methodology that achieves state-of-the-art success rates for ImageNet classifiers. The presented attacks leverage learned class-wise and layer-wise intermediate feature distributions of modern DNNs. The attack transfer layers have feature distributions that are class-specific and highly-separable, but are not overly-correlated with the white-box model output. They also find that highly transferable attacks induce large disruptions in the intermediate feature space of the blackbox models."
SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper introduces a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. The authors show that by utilizing the dual formulation of the WD, we can learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the authors devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. Two on-policy algorithms, Behavior Guided Policy Gradients (BGPG) and Behavior-Guided Evolution Strategies (BGES) are proposed, which replace the KL-based trust region from Schulman et al. (2015) with a WD-based PPE space. In addition, they also demonstrate a way to harness our methodology for imitation learning and repulsion learning."
SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper proposes an adaptive learning rate for interpolating with gradients (ALI-G) algorithm for training deep neural networks. The main idea is to use the interpolation property to compute the learning rate in closed form at each iteration. The learning rate is computed using a single constant hyper-parameter and does not require a decay schedule, which makes it easier to tune. The authors provide convergence guarantees for the stochastic convex setting and show that the algorithm converges up to some tolerance. The proposed algorithm is evaluated on a variety of tasks and shows comparable performance with SGD."
SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper investigates the role of bottom-up, horizontal, and top-down connections for perceptual grouping in the brain. The authors systematically evaluate neural network architectures with combinations of these connections on two synthetic visual tasks, which stress low-level “Gestalt” vs. high-level object cues for perceptual groupings. They show that increasing the difficulty of either task strains learning for networks that rely solely on bottom up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-up connections rescue learning on tasks involving high level object cues by modifying coarse predictions about the target object."
SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. The proposed regularizers are inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems. The experiments show that enforcing the regularizers can produce even sparser neural network models than previous works, under the same accuracy level. "
SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam that achieves a data-dependent $O(\log T)$ regret bound for strongly convex functions. The main idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. The proposed algorithm reduces to SC-RMSprop, which is a recently proposed variant of RMSprop. The paper provides an alternative proof for the proposed algorithm and establishes the first data-dependency regret bound."
SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. The model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short or long-range contextual information. The authors conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%."
SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the effect of pruning on the generalization of deep neural networks (DNNs). The authors introduce the notion of pruned instability, which is the size of the drop in network accuracy caused by a pruning iteration. They show that even the pruning of unimportant parameters can lead to instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks. They also derive a batch-normalized pruning algorithm to control pruning stability."
SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"This paper proposes a method for neural architecture search (NAS) with reinforcement learning (RL). The main idea is to search in an embedding space by using architecture encoders and decoders. The architecture simulator simulates the origin architecture space, which assists the real architecture encoder learning, and the decoder realizes the relationship between origin architecture and architecture-embedding, which maps the architecture embedding to the origin network. The proposed method is evaluated on CIFAR-10 classification task, and it is shown that the performance of the final architecture network is comparable with that of other popular NAS approaches."
SP:e2e5bebccc76a51df3cb8b64572720da97174604,This paper proposes a homotopy training algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low-dimensional structure and tracks the solution to the high-dimensional coupled system. The authors have proved the convergence of HTA for the non-convex case and existence of the solution path for the convex case. HTA has provided a better accuracy on several examples including VGG models on CIFAR-10.
SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,"This paper introduces the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning. The experiments show that the simplicial agent confers an advantage over the relational agent."
SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"This paper proposes a method for semi-supervised object pose estimation with limited labeled data. The proposed method is based on conditional variational autoencoders (CVAEs) with circular latent representations to estimate the 2D rotations of an object. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing. Moreover, the model’s performance increases with the amount of supervision provided and the angle coverage."
SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a method to scale up multi-agent reinforcement learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. The proposed method maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets that have the best adaptability to the next stage. The authors also propose an evolutionary approach to fix an objective misalignment issue: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations."
SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,"This paper introduces a multi-layer multiplex graph neural network architecture for abstract diagram reasoning. The proposed model is built on top of WReN, a state-of-the-art neural network for RPM-style tasks. The model encodes subsets of diagram panels into multiplex graphs, and combines the feature embeddings to predict the correct candidate answer. Experimental results show that the proposed model outperforms the previous state of the art model by a considerable margin."
SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood.
SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper investigates the existence of early-bird (EB) tickets (small but critical subnetworks) for dense, randomly initialized networks. EB tickets are identified at a very early training stage, which they term as Early-Bird tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. They also propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, they propose an efficient DNN training scheme, which is achieved by identifying EB tickets and the proposed mask distance, and then continuing to train merely the EB tickets towards the target accuracy. Experiments are conducted on various deep networks and datasets."
SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"This paper proposes a new framework, Embedding Regularized Classifier (ER-Classifier), which improves the adversarial robustness of the classifier through embedding regularization. The authors propose to embed high-dimensional input images into a low-dimensional space to perform classification. The proposed method achieves state-of-the-art performance against strong adversarial attack methods on several benchmark datasets."
SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and relations and integrates a large, pre-trained language model. Because the bulk of the model’s parameters are pretrained and it eschew recurrence for self-attention, the model is fast to train. On 5 datasets across 3 domains, the proposed model matches or exceeds state-of-the-art performance."
SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of learning a low-level feature representation for items, i.e., RGB values of images in the image context. In this setting, the authors assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? The authors provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answer to the above-mentioned triplet comparison. They demonstrate that their proposed approach is significantly faster than existing methods, and can scale to real-world large datasets."
SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"This paper proposes a meta-learning framework to learn the data values jointly with the target task predictor model. The data value estimator (modeled by a deep neural network) is used to learn how likely each datum is used in training of the predictor model, and it is trained using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target target task. The proposed method is evaluated on a wide range of use cases, including domain adaptation, corrupted sample discovery and robust learning, and shows significant improvements compared to permutation-based strategies."
SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the statistical properties of the gradient in random fully-connected ReLU networks through the study of the the per-layer Jacobian in finite-sized networks. The authors compare three types of architectures: vanilla networks, ResNets and DenseNets. They show that while the variance of Jacobian squared norm is exponential in depth for vanilla networks and polynomial for dense networks, there exists an initialization strategy for both, such that the norm is preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. They also show that the statistics of the per layer Jacobian norm is a function of the architecture and the layer size, but surprisingly, not the layer's depth."
SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes an approach to reduce the compilation time for neural network code optimization. The main idea is to use reinforcement learning to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. The authors propose an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experiments on real hardware shows that CHAMELEON provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of modern deep networks by 5.6%."
SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper studies the problem of certified robustness for top-k predictions, i.e., given an example x and a classifier f and a perturbation $\delta$, the classifier g(x + \delta) makes predictions for x + \epsilon as long as the $\ell_2$-norm of the adversarial perturbations is less than a threshold (called certified radius). The certified radius is the unique solution to the certified radius equation, which depends on σ and the largest probabilities of the largest faces’s for the example x. This paper proposes to use randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. The proposed method is scalable to large-scale neural networks and applicable to any base classifier. The main theoretical result is a tight certified radius bound in $\ell_{2}$ norm for topk predictions when using randomized Gaussian noise."
SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the relationship between the gradient signal to noise ratio (GSNR) of parameters during training of deep neural networks (DNNs) and the generalization performance. The GSNR is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. The authors establish a quantitative relationship between model parameters’ GSNRs and generalization gap. This relationship indicates that larger GSNr during training process leads to better generalisation performance. Moreover, they show that the gradient descent optimization dynamics of DNNs naturally produces large GSNrs during training."
SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a method for answering logical queries on large-scale incomplete knowledge graphs (KGs) by embedding KG entities as well as the query into a low-dimensional vector space, such that entities that answer the query are embedded close to the query. The authors propose to represent queries as boxes (i.e., hyper-rectangles), where the set of points inside the box corresponds to a set of answer entities. They show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KGs entities. However, they show that by transforming queries into a Disjunctive Normal Form, QUERY2BOX is capable of handling arbitrary logical queries with $\�, $\�, and $\�$ operators in a scalable manner."
SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper considers the problem of efficient SGD for convex and non-convex loss functions. The authors consider the setting of strongly convex, convex-concave, and non convex objectives, where the objective function is a convex function of a finite graph. They show that the convergence of SGD with a consistent gradient estimator is the same as that of the full gradient. They also show the convergence rate of the gradient with respect to the empirical risk minimization (ERM) of the loss function."
SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. It can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, it proposes a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently."
SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper extends the Neural Module Networks (NMNs) to answer compositional questions against a paragraph of text. Specifically, the authors introduce modules that reason over a paragraph, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner, and propose an unsupervised auxiliary loss to help extract arguments associated with the events in text. The proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by the modules."
SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper provides a theoretical analysis of the effect of initialization on the connection sensitivity of pruning at initialization. The authors show that the initial weights have a critical impact on connection sensitivity, and therefore, pruning results. They also propose a simple, data-free method to recover the layer-wise orthogonality, which improves the training performance of the pruned network significantly."
SP:d5899cba36329d863513b91c2db57675086abc49,"This paper studies the problem of training sparse neural networks. The authors propose a new initialization scheme for sparse networks that allows to train very deep sparse networks without suffering from the vanishing gradient effect. They also develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. They then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them."
SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"This paper proposes a novel initialization scheme for training recurrent neural networks (RNNs) on long sequence tasks. The authors develop a mean field theory of signal propagation in LSTMs and GRUs that enables them to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, the authors derive a new initialization scheme that eliminates or reduces training instabilities. They demonstrate the efficacy of their initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower."
SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"This paper proposes a method to improve the prediction accuracy of deep neural networks on remote sensing data by leveraging the semantic similarity between neighboring scene images. The proposed method consists of a siamese network, a similarity metric learning component, a convolutional network, and a decision layer. The siamee network is used to extract features from the two neighboring images, and the decision layer is used for classification or regression. The method is evaluated on a disease density estimation task, and it is shown that the proposed method outperforms the baseline."
SP:99c10e038939aa88fc112db10fe801b42360c8dc,"This paper proposes a self-supervised monocular depth estimation method that leverages semantic information from a fixed pretrained semantic segmentation network to guide the generation of multi-level depth features via pixel-adaptive convolutions. The proposed method improves upon the state-of-the-art on the standard KITTI benchmark over pixels, fine-grained details, and per semantic categories. The authors also propose a two-stage training process to overcome a common semantic bias on dynamic objects."
SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper proposes a method to build linear classifiers based on deep features that are certifiably robust against a strong variant of label-flipping attacks, where the adversary can target each test example independently. The approach leverages randomized smoothing, a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a classifier. In this paper, for each test point, the classifier makes a prediction and includes a certification that its prediction would be the same had some number of training labels been changed adversarially. The certified bounds are obtained with no additional runtime cost over standard classification."
SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper proposes to use differential privacy to improve anomaly detection and backdoor attack detection. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset or to intermediate results of the aggregation mechanism. In this paper, the authors demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks."
SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. The authors show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions. They propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. They also propose a simple scaling-based calibration that preforms well in our experimental tests."
SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a method for disentangling BERT representations into Tensor-Product Representations (TPRs), which is a collection of constituents that is the binding of a filler to a structural role. Specifically, it adds a TPR layer on top of BERT, which takes the final-layer BERT embedding of each input token and transforms it into the tensor tensor. This factorization effectively untangles the fillers from their roles, these two dimensions having been fully entangled in the BERT encoding itself. Experiments on the GLUE benchmark and HANS dataset show that untangling data-specific semantics from general language structure is key for better transfer among NLP tasks."
SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper proposes a method for multi-agent reinforcement learning that combines deep reinforcement learning (DRL) and hierarchical RL (HRL) to train heterogeneous humanoid agents that can both navigate and interact in dynamic simulation. Specifically, the authors propose a partial parameter sharing approach wherein the lower level of the hierarchy is shared enabling learning using decentralized methods. This drastically reduces the overall parameter space in the MARL problem and introduces structure in the optimization problem. The authors also introduce a set of reward functions to encourage human-like behaviour while navigating with other agents."
SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"This paper proposes a method to improve the performance of graph neural networks (GNNs) by leveraging a spatial representation of the graph. The proposed method is based on a graph embedding method, which is equivalent to a point-cloud representation of a graph. In this way, the local feature extractor of the GNN distinguishes similar local structures in different locations. The GNN infers the topological structure from the spatial distribution of the locally extracted feature vectors. A new graph pooling method is proposed and it is shown that the proposed method achieves competitive or better results in comparison with the state-of-the-art methods."
SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,"This paper provides a formal definition of shift-equivalent when down sampling is used in convolutional neural networks (CNNs) and proposes a method called frequency pooling (F-pooling) to achieve the shift equivalent and anti-aliasing pooling in theory. The proposed method is achieved by (inverse) Discrete Fourier Transform (DFT) to transform the signal into frequency domain via discretizing its low frequencies, i.e. the frequencies which are smaller than Nyquist sampling rate. Finally, it transforms the low frequencies back into time domain via inverse DFT. Experiments on CIFAR-100 and a subset of ImageNet demonstrate the effectiveness of the proposed method. "
SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a simple technique to improve the generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. The authors also provide an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. They demonstrate the superiority of their method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks."
SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation approach for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. The authors propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match. The explanation method predicts which attributes are important for the similarity score predicted by a model. The proposed method is evaluated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2."
SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes a flow-based generative model for the synthesis of high-fidelity speech. The proposed method, called WaveFlow, is trained with maximum likelihood without density distillation and auxiliary losses, which is similar to the approach used in parallel waveform generative models such as Parallel WaveNet and ClariNet. The authors demonstrate that WaveFlow is able to synthesize high fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms."
SP:963e85369978dddcd9e3130bc11453696066bbf3,This paper proposes a graph translation-generative-adversarial-network (GT-GAN) framework for graph generation. The proposed framework consists of a graph translator equipped with graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A new conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate that the proposed GT-GAN significantly outperforms other baseline methods in terms of both effectiveness and scalability.
SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence modeling unit called METAGROSS (Meta Gated Recursive Controller). The proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. The authors conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach."
SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,This paper proposes a self-supervised learning framework for speech recognition. The key idea is to leverage a strong language model to provide learning signal given unlabeled speech. The proposed method is evaluated on the LibriSpeech corpus. The results show that the proposed method reduces the word error rate (WER) by 54% and 73% WER relative to a fully supervised model on the same 360 hours with labels and 500 hours with additional labels.
SP:e6af249608633f1776b608852a00946a5c09a357,"This paper considers the problem of fair and robust model training in the presence of data poisoning. The authors propose a generative adversarial network (GAN) framework that combines two discriminators: a fairness discriminator that predicts the sensitive attribute from classification results and a robustness discriminator which distinguishes examples and predictions from a clean validation set. The proposed framework respects all the prominent fairness measures: disparate impact, equalized odds, and equal opportunity. In the experiments, the authors show that the proposed framework shows almost no decrease in fairness and accuracy in the case of data poisoned data."
SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. The learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, and using moving particles. The authors demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme."
SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper proposes a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to mitigate the effects of noise, since one does not overly trust any single sample. The authors prove that for the common problem of label noise in classification, standard gradient clipping does not in general provide robustness. On the other hand, they show that a simple variant of gradient clipping is robust and is equivalent to suitably modifying the underlying loss function."
SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state-based imitation learning method that learns to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and is combined with a reinforcement learning framework by a regularized policy update objective. The authors show the superiority of the proposed method on standard imitation learning settings and imitation learning with different dynamics models.
SP:91761d68086330ce378507c152e72218ed7b2196,This paper proposes an extension of SGD called Deep Gradient Boosting (DGB) where the back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. At each layer of a neural network the weight update is calculated by solving the corresponding boosting problem using a linear base learner. The resulting weight update formula can also be seen as a normalization procedure of the data that arrives at each layer during the forward pass. Experiments on CIFAR10 and ImageNet classification tasks show that the proposed method outperforms other methods.
SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"This paper proposes a novel approach to differentiable architecture search (DARTS) by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, it performs operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges, which is alleviated by edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, the proposed method can be trained with a larger batch size and enjoys both faster speed and higher training stability."
SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes an actor-critic algorithm for deep reinforcement learning (DRL) based on deep deterministic policy gradients (DDPG). The key idea is to use advantage estimation (AE) gradients from a differentiable simulator to improve the convergence of both the actor and the critic. The authors show that the AE gradients improve the performance of DDPG and improve the robustness to poor local minima. 
SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a method to learn a task-agnostic representation of the environment in the form of a world graph. The world graph is constructed by training a binary recurrent variational autoencoder (VAE) with binary latent variables and a curiosity-driven goal-conditioned policy. The nodes of the world graph are important waypoint states and edges represent feasible traversals between them. The paper also proposes a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. The method is evaluated on a suite of challenging maze tasks and shows that using world graphs significantly accelerates RL, achieving higher reward and faster learning."
SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering neural networks. The main idea is to construct a target function by discretizing the layers of a neural network and then reconstruct the target function from the discretized layers. The method is motivated by the question of whether we can benefit from knowing the functions that are used in certain fields, e.g., in a programmable logic controller (PLC). The proposed method is evaluated on the PLC data and verified whether it can obtain the target logic."
SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for self-supervised imitation learning of goal-conditioned policies by maximizing the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. The main idea is that in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. The authors show that this is the case in the setting where the tasks correspond to different goals, and propose a simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. The proposed method is evaluated on a range of challenging goal reaching problems."
SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper studies the problem of Byzantine tolerance in distributed asynchronous stochastic gradient descent (SGD). In particular, the authors propose Zeno++, a new robust asynchronous SGD procedure which tolerates Byzantine failures of the workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. The authors prove the convergence of the proposed method for non-convex problems under Byzantine failures. Experimental results show that the proposed approach outperforms existing approaches."
SP:d16ed9bd4193d99774840783347137e938955b87,"This paper proposes to generate adversarial examples by manipulating semantically meaningful image-based visual descriptors (color and texture) in order to generate effective and photorealistic adversarial images. The proposed method is able to generate semantically aware perturbations that are effective against JPEG compression, feature squeezing, adversarially trained model, and can be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. The authors conduct comprehensive user studies to show that the generated generated semantic adversarial samples are more realistic to humans than other attacks."
SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,"This paper proposes a method for few-shot learning of entity recognition in the presence of a few observations per class. The proposed method is based on the idea of learning to control (LTC), which is an RL-based approach that disentangles the representation learning of a neural encoder from its memory management role. The authors propose a novel architecture that uses a trainale controller to manipulate latent representations in an external memory (Section 3). They also introduce a reward signal that is proportional to the average reduction of entropy when attending the memory entries (Section 4). This enables them to propose a policy that learns a policy based on interactions with the external memory."
SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"This paper proposes an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. The approach is based on jointly learning both the underlying motor primitive and recomposing these primitives to form the original demonstration. The proposed approach is able to learn a diverse set of motor primitive primitives, as well as a coherent latent representation for the primitives. The learned primitives capture semantically meaningful aspects of a demonstration. This allows us to compose these primitive in a hierarchical reinforcement learning setup to efficiently solve robotic manipulation tasks like reaching and pushing."
SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for single episode transfer in reinforcement learning (RL). The authors propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational inference or RL. Moreover, the approach does not require access to rewards at test time, allowing it to perform in settings where existing adaptive approaches cannot. In diverse experimental domains with a single episode test constraint, the proposed method shows favorable performance against baselines for robust transfer."
SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper proposes a three-head network architecture for AlphaZero, a reinforcement learning algorithm for two-player zero-sum zero-dimensional games. The paper shows that the proposed architecture outperforms the two-head architecture in zero-style iterative learning and Monte Carlo tree search (MCTS) on the game of Hex. The main contribution of the paper is the introduction of the three-headed network architecture and its application to AlphaZero."
SP:89d6d55107b6180109affe7522265c751640ad96,This paper proposes a method for transfer learning in reinforcement learning. The main idea is to combine the reward of the source policy with the environment reward to improve the performance of the learned policy in the target domain. The source policy is learned in the source domain and is used to train the target policy. The target policy is then used to learn to solve a target task with significant transition differences and uncertainties. The proposed method is evaluated on a set of Mujoco tasks. 
SP:626021101836a635ad2d896bd66951aff31aa846,This paper introduces a general theory for building scale-equivariant convolutional networks with steerable filters. The authors develop scale-convolution and generalize other common blocks to be scale equivariant. They demonstrate the computational efficiency and numerical stability of the proposed method. They compare the proposed models to the previously developed methods for scale-invariance and local scale invariance.
SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper proposes a point-based unpaired 3D scan completion method that works directly on input point clouds, does not require paired training data, and hence can directly be applied to real scans for scan completion. The proposed method consists of an adaptation network that transforms latent code encodings of the raw point clouds and complete scans to obtain clean and complete point clouds. The authors evaluate the approach qualitatively on several real-world datasets (ScanNet, Matterport3D, KITTI), quantitatively on 3D-EPN shape completion dataset, and demonstrate realistic completions under varying levels of incompleteness."
SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data using generative adversarial networks (GANs). The authors cast the problem as a maximin game between the attacker and the authenticator, and characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. The analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights, the authors design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data."
SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper studies the trade-off between robustness and accuracy in adversarial learning. The authors propose a framework called “sensible adversary”, which is a combination of adversarial robustness (i.e. adversarial perturbations that do not cross the constraint boundary of the Bayes rule) and the standard robustness, i.e., the robustness that does not change the true class. Theoretically, the authors show that the most robust multi-class classifier is the one with the 0-1 loss under sensible adversary. Then, they propose a novel and efficient algorithm that trains a robust model with sensible adversarial examples and achieves state-of-the-art results against various attacks."
SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish feature map distributions of different networks. The authors have applied their method to various network architectures on the classification task and discovered a significant improvement of performance.
SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a sentiment word embedding model for semantic and sentiment analysis tasks. The proposed method is based on GloVe and C&W, and the parameters of the proposed model are determined by using both the maximum likelihood estimation and the Bayesian estimation. Experimental results show the proposed method significantly outperforms the baseline methods in sentiment analysis for low-frequency words and sentences. Besides, it is also effective in conventional semantic/sentiment analysis tasks"
SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method, called Prestopping, for noise-free training under any type of label noise for practical use. First, it proposes to early stop training a deep neural network before the noisy labels are severely memorized. Then, it resumes training the early stopped network using a “maximal safe set,” which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Extensive experiments using four image benchmark data sets verify that the method significantly outperforms four state-of-the-art methods in test error by 0.4–8.2 percent points under existence of real-world noise."
SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a framework for action space generalization in reinforcement learning. The main idea is to use an autoencoder-based representation learning method to learn representations of actions, which are then used to train a policy to generalize to unseen actions. The paper also proposes a regularization term to encourage generalization. The proposed method is evaluated on four Mujoco tasks, where it is shown that it generalizes better than other baselines."
SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes two methods for storing word embedding matrix during training and inference in a highly efficient way. The authors use approaches inspired by quantum computing to propose two related methods, word2ket and word_2ketXS, for storing and accessing embedding vectors for all words in a dictionary. Their approach achieves a 100-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks."
SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning (IL) that learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The proposed method, called Behavioral Repertoire Imitation Learning (BRIL), learns a single neural network policy conditioned on a behavior description that can be precisely modulated. The authors apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors."
SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis of Frankle & Carbin (2018), which suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. This paper provides an in-depth investigation of the structure of winning lottery tickets. The authors discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. Based on these insights, the authors identify the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning."
SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to detect unknowns. The key idea is to enhance existing CNNs to support a product operation that models the product relationship among the features produced by convolutional layers. UDN uses a learned ensemble of these product operations, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting knowns. To further improve the performance of UDN at detecting unknowns, the authors propose an information-theoretic regularization strategy that incorporates the objective of rejecting unknowns into the learning process. The experiments on benchmark image datasets including MNIST, CIFAR-10, CifAR-100, and SVHN, show that UDN consistently outperforms state-of-the-art methods at rejecting unknown."
SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for variational inference (VI) for deep neural networks. The main idea is to start with a coarse mean-field approximation and iteratively refine the approximation by sampling the values of auxiliary variables and conditioning on the newly sampled values. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. The authors demonstrate theoretically that their method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. They show that the refinement steps are guaranteed to improve the quality of the variational distribution."
SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a policy gradient estimator for contextual categorical sequence generation. The main idea is to use a set of correlated Monte Carlo (MC) rollouts for variance control, where the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. The proposed methods yield lower gradient variance and consistent improvement over related baselines. Experiments are conducted on both neural program synthesis and image captioning."
SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) problem with two main stages: (1) deceptive opponent modeling based on maximum entropy regularized Markov decision processes (MDPs) and (2) goal recognition under proactively static interdiction. The main idea is to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, the task of S-GRF is to interdict a set of actions that improve or reduce the wcd. The authors empirically demonstrate that the proposed approach control the goal recognition process based on opponent’s deceptive behavior."
SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batches for training GANs that are effectively large though actually small. The method is inspired by the use of Coreset-selection in active learning. The authors draw a large batch of samples from the prior and then compress that batch using Coreset selection. To create effectively large batches of ‘real’ images, the authors create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Core-set selection on those projected activations at training time. They conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows GAN-based anomaly detection."
SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper investigates if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. The authors show that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. They show how constraining past information by injecting noise into the hidden state can improve the ability of RNN to extract predictive information for both maximum likelihood and contrastive loss training."
SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate the problem of ""delusional bias"" in Q-learning, i.e. the condition that the Q-values used to update the state-action pairs should be consistent with the expressible policy class. The authors propose two methods to achieve this goal: (1) training Q-approximators with labels that are “consistent” with the underlying greedy policy class, (2) using a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent, and (3) a search framework that allows multiple Q approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experiments on the Atari suite demonstrate that the proposed methods can improve the performance of Q-learners."
SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. SPACE also resolves the scalability problems of existing spatial attention methods by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations. The experiments on Atari and 3D-Rooms show that SPACE achieves the above properties consistently."
SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a depthwise separable convolution-based compression method for compressing convolutional neural networks. The proposed method is derived by interpreting existing convolution methods based on depthwise convolution using EHP, which is a mathematical formulation to approximate the standard convolution kernel. Based on the interpretation, a generalized version of FALCON is proposed, which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. Experiments on CIFAR-10/100 and ImageNet show that the proposed method outperforms existing methods."
SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper identifies four improvements to Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. The contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy, recognizing and validating the powerful regularization effect of Ghost Batch normalization for small and medium batch sizes, examining the effect of weight decay regularization on the scaling and shifting parameters, and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. Experiments are conducted on CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet."
SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper studies the problem of data inspection in the setting of federated learning, i.e., when the training data is privacy-sensitive and the modeler cannot access the raw data. The authors propose to use generative models trained with federated methods and with formal differential privacy guarantees as an alternative to manual data inspection. The proposed approach is evaluated on text and image data. "
SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the motion material and modifies it to become natural."
SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper studies the problem of providing interpretable explanations for NLP models. The authors propose two methods to quantify the importance of each word and phrase: sampling and contextual decomposition (SCD) and sampling and occlusion (SOC) algorithms. The proposed methods are evaluated on both LSTM models and BERT Transformer models on multiple datasets, and the results show that they outperform prior hierarchical explanation algorithms."
SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,This paper proposes a saliency map method for deep convolutional neural networks (CNN) that is much more efficient than popular gradient methods. The technique works by measuring information at the end of each network scale which is then combined into a single saliency maps. The saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. The authors visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network.
SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a non-autoregressive transformer (NAT) model that explicitly models the position of the output words as a latent variable in the text generation process. The authors propose a heuristic search process to guide the position learning, and max sampling is adopted to inference the latent model. The proposed PNAT is motivated by learning syntax position (also called syntax distance). Experimental results show that PNAT achieves top results on machine translation and paraphrase generation tasks."
SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a new generative adversarial network (GAN) model, called Random Path Generative Adversarial Network (RPGAN), which is an alternative to traditional GANs. The main idea of RPGAN is to use random paths as the source of stochasticity in the forward pass of the GAN, which allows natural interpretability of the generator network. The proposed method is evaluated on CIFAR-10/100 and Fashion-MNIST datasets, and it is shown that it is able to generate high-quality images compared to other GAN models."
SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolutional neural network based on a graph representation of the sampled sphere. The proposed method is based on graph convolution with respect to the number of vertices and neighbors. The authors study both theoretically and empirically how equivariance is affected by the underlying graph. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation."
SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of domain adaptation (DA) from the perspective of representation invariance. In particular, the authors show that the search for invariance favors the compression of representations, which may have a bad impact on the adaptability of representations. The authors also show that weighting representations can align representation distributions without impacting their adaptability. The paper also provides a bound on the target risk that reveals a trade-off between compression and invariance of learned representations."
SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper presents a method for learning to infer user interface attributes from images. Specifically, given an input image created by a designer, the method learns to infer its implementation which when rendered, looks visually the same as the input image. To achieve this, the authors take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and train specialized neural models to predict each of the attribute values. The authors also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values."
SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, in which the goal is to transfer not only performance but also robustness from a source model to a target domain. The authors first show that robust networks contain robust feature extractors. By training classifiers on top of these extractors, the new models inherit the robustness of their parent networks. Then, the authors consider the case of “fine-tuning” a network by re-training end-to-end in the target domain, which preserves the robusts of the source network while achieving high accuracy. The proposed methods improve the generalization of a robust robust CIFAR100 model by roughly 2% while preserving its robustness."
SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"This paper proposes a neural iterated learning (NIL) algorithm to encourage the emergence of a more structured type of language, i.e., highly compositional languages, in the context of communication agents playing referential games. The authors claim that these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL. They provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. The experiments confirm their analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication."
SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The proposed method is a minimax optimization problem, which is solved by stochastic gradient descent (SGD). It is proven convergent under certain conditions."
SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple reward function for goal-conditioned reinforcement learning. The reward function is a simple indicator reward function, which is used when the robot’s observation exactly matches the target goal observation. Theoretical analysis shows that the optimal policy under the indicator reward has a bounded sub-optimality. The paper also proposes two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. The experiments show that the proposed method can perform complex tasks in continuous state spaces."
SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper considers the robustness verification problem for Transformers with complex self-attention layers, which pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. To resolve these challenges, the authors develop the first robustness verify algorithm for Transformers. The certified robustness bounds computed by their method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis."
SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining objective that explicitly forces the model to incorporate knowledge about real-world entities. The proposed objective replaces the entity mentions in the original documents with names of other entities of the same type and train the models to distinguish the correct entity mention from randomly chosen ones. Experiments on four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset with 5.7 accuracy gains."
SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper tackles the problem of discovering novel classes in an image collection given labelled examples of other classes. The challenge is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. The authors address this problem by combining three ideas: (1) they suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabeled data; (2) they use rank statistics to transfer the model’s knowledge of the labelled classes to the task of clustering the unllabeled images; and (3) they train the data representation by optimizing a joint objective function on the labelled/unlabelled subsets of the data."
SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a method for visual planning based on the semi-parametric topological memory (SPTM) method. The authors argue that SPTM is constricted in its ability to generalize to changes in the domain, as its graph is constructed from direct observations and thus requires collecting new samples for planning. To address this issue, the authors propose a method called Hallucinative Topological Memory (HTM), which overcomes these shortcomings. The main idea is to learn a conditional VAE model that generates samples given a context image of the domain and use these hallucinated samples for building the connectivity graph, allowing for zero-shot generalization to domain changes. The method is evaluated on simulated domains and shows superior performance compared to SPTM and visual foresight methods."
SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a method to reduce the spurious biases in large-scale benchmark datasets. The authors argue that the real world problems consist of a great deal of long-tail problems, and the existing benchmarks are overly populated with similar (thus non-tail) problems, which leads to a major overestimation of true AI performance. To address this challenge, the authors propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify and filter artifact-prone instances in the remainder of the dataset to yield a reduced dataset with more realistic problem distributions and considerably less spurious biases."
SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a method for few-shot classification based on prototypical networks. In particular, the authors propose to use batch normalization to enforce zero centering in the latent space of the prototypical network for the null class. The authors also propose a novel Gaussian layer for distance calculation in the prototype network, which takes the support examples’ distribution rather than just their centroid into account. Experiments on the Omniglot and MiniImageNet datasets show that the proposed method outperforms the two-way matching and prototypical models in terms of classification accuracy."
SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. It first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. Then, it repeatedly coarsens into much smaller graphs by merging nodes with high spectral similarities. Finally, it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. The proposed method is evaluated on several popular graph datasets for both transductive and inductive tasks."
SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relatively, by predicting new pixels relative to previously generated pixels (or pixels from the conditioning context, when available). The authors show that this form of prediction fare favorably to its absolute counterpart when used independently, but their coordination under an unified probabilistic model yields optimal performance. Experiments on multiple benchmarks for unconditional image generation, image colorization, and super-resolution indicate that the presented mechanism leads to improvements in terms of likelihood compared to the absolute prediction counterparts."
SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper studies the problem of estimating the stationary distribution of a Markov chain in the setting where access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. The authors show that consistent estimation remains possible in this challenging scenario, and that effective estimation can still be achieved in important applications. The approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective."
SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper proposes a method for training NLP models that are less sensitive to spurious patterns. The authors propose to use a human-in-the-loop approach to generate counterfactual versions of documents, where the goal is to improve the performance of the model trained on the original data and the counterfactually generated data. The proposed approach is based on the idea that spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. The paper proposes to use the language of causality to define the notion of spurious associations and proposes a framework for training models to distinguish between spurious and non-spurious associations."
SP:b720eb5b6e44473a9392cc572af89270019d4c42,"This paper studies the perceptual quality of pre-trained deep convolutional neural networks (CNNs) by measuring the frequency and orientation tuning of channels in trained image classification deep CNNs by applying grating stimuli of different spatial frequencies and orientations. The authors claim that the behavior of CNN channels as spatial frequency/orientation selective filters can be used to link basic human visual perception models to their characteristics, and develop a theory to get more insight into deep CNN representations as perceptual quality features. They conclude that sensitivity to spatial frequencies that have lower contrast masking thresholds and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality."
SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to explicitly model link type correlations. The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrated that GENN is superior to many baselines without consideration of link type correlation and achieved 13.77% and 5.01% PR-AUC improvement on the two datasets."
SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a self-supervised method to learn discrete representations of audio segments through a wav2vec-style context prediction task. The proposed method uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.
SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning (ARL) approach to improve the performance of latent variable models (LVMs) in recommender systems. The main idea is to train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. The proposed approach amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. Experiments on three large-scale datasets demonstrate the effectiveness of the proposed approach."
SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a multi-step TD-learning approach for off-policy reinforcement learning. The proposed approach is based on the idea of truncated Q-functions, which represent the return for the first n steps of a target-policy rollout. The authors also propose a farsighted Q-function to represent the farsightsed return after this truncated rollout. They prove that the combination of these short and long-term predictions is a representation of the full return, leading to the composite Q-learning algorithm. They show the efficacy of their approach in the function-approximation setting and compare it with TD3, Model-based value expansion and TD3(∆)."
SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a method for continuous reinforcement learning for dexterous manipulation. The main idea is to design a system that is able to learn from raw sensory inputs, learn reward functions from easily available supervision, and learn continuously in non-episodic settings without requiring human intervention to manually reset the environment. The proposed method is evaluated on a set of dexterous robotic manipulation tasks, where it is shown that it outperforms baselines."
SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies adversarially robust generalization of deep neural networks. The authors show that adversarial robustness is upper bounded by the sum of two terms: the stability term, which measures the prediction stability in the presence of perturbations, and the accuracy part, which evaluates the standard classification accuracy. They further show that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarial generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabeled data is provided. Finally, the authors propose a practical adversarial training algorithm that leverages the theoretical findings."
SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a method to estimate the advantage of an agent in reinforcement learning (RL) based on the order statistics over the path ensemble. The authors propose a family of estimates based on order statistics that allows one to flexibly drive the learning process in a promotion focus or a prevention focus. They systematically study the impacts of different regulatory focuses. The findings reveal that regulatory focus, when chosen appropriately, can result in significant benefits. For the environments with sparse rewards, promotion focus would lead to more efficient exploration of the policy space; while for those where individual actions can have critical impacts, prevention focus is preferable."
SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks, which computes most features in a low precision and only a small proportion of important features in high precision to preserve accuracy. The proposed approach significantly reduces the computational cost of DNN execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs, including static compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4x less compute on ImageNet."
SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a new unsupervised domain adaptation (UDA) setting, where the source and target domain have noisy labeled data, and the target domain has unlabeled data. The authors claim that the existing UDA methods share an implicit assumption that there are no noisy source data. Therefore, these methods cannot well handle the WUDA setting. To this end, the authors propose a new UDA framework, called ""Butterfly"", which maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled to unlabeled, and SD to-TD-distributional) and the other two can focus on classification in TD. The experiments demonstrate that the proposed method outperforms existing baseline methods."
SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,This paper proposes a model-free off-policy reinforcement learning algorithm that combines an autoencoder with an actor-critic algorithm. The key idea is to use a pixel reconstruction loss as an auxiliary task to improve the sample efficiency. The proposed method is evaluated on Atari and DeepMind Control tasks and shows comparable performance to the state-of-the-art methods.
SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for both accelerating training and improving generalization over the original dropout. The proposed dropout creates multiple dropout samples. The loss is calculated for each sample, and then the sample losses are averaged to obtain the final loss. This technique can be easily implemented without implementing a new operator by duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experiments show that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks."
SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a label sensitive gate (LSG) structure to enable the model to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization, which imposes each filter’s attention to just one or few classes, namely class-specific. Extensive experiments demonstrate the performance of the method in generating sparse and highly class-related representation of the input."
SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"This paper proposes a method for learning nearly decomposable Q-functions (NDQ) in cooperative multi-agent reinforcement learning (MARL) via communication minimization, where agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. Experiments on the StarCraft unit micromanagement benchmark show that the proposed method significantly outperforms baseline methods and allows agents to cut off more than 80% of communication without sacrificing the performance."
SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes to use puzzles as a domain for teaching computers programming. A programming puzzle is a short program for a Boolean function f(x) with the goal of finding an input that makes f return True. The authors propose a GAN-like algorithm called “Troublemaker” which can generate puzzles adaptively targeted at any given puzzle-solver. It generates a diverse set of puzzles that are difficult for the solver. In experiments, it learns to generate challenging problems for a variety of state-of-the-art puzzles-solving techniques."
SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a method for hierarchical reinforcement learning (HRL) that decomposes the policy into primitives, i.e., each primitive can decide for themselves whether they wish to act in the current state. The authors use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current states to make a decision, and the primitive that requests the most information is the one that acts in the environment. The paper shows that this approach leads to natural competition and specialization, and is compatible with any RL method."
SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-based planning framework for deep reinforcement learning (DRL) that learns a latent dynamics model directly from rewards. The model is learned exclusively from multi-step reward prediction which is shown to be the only necessary information for successful planning. The proposed method is able to benefit from the concise model-free representation, while still enjoying the data-efficiency of model based algorithms. Experiments on multi-pendulum and multi-cheetah environments demonstrate the effectiveness of the proposed method."
SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). The authors also propose a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentences inputs. The proposed method achieves new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large."
SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"This paper proposes an extension of the Transformer architecture for multi-modal tasks. Specifically, the authors propose a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning. The authors demonstrate that a single instance of OmniNet can concurrently learn to perform the tasks of part-of-speech tagging, image captioning, visual question answering and video activity recognition."
SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"This paper proposes a new method for quantifying the predictive uncertainty of deep learning models. The proposed method, called discriminative jackknife (DJ), constructs predictive confidence intervals for a wide range of regression models, is easy to implement, and provides rigorous theoretical guarantees on (1) and (2). The DJ procedure uses higher-order influence functions (HOIFs) of the trained model parameters to construct a leave-one-out estimator of predictive confidence interval. Experiments demonstrate that DJ performs competitively compared to existing Bayesian and non-Bayesian baselines."
SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a GAN-based method for video synthesis and video prediction. The proposed method, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. It achieves new state-of-the-art Fréchet Inception Distance and Inception Score for Kinetics-600 and UCF-101 datasets, alongside establishing a strong baseline for synthesis on Kinetics."
SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. The proposed method adapts the representation in two stages, namely on the few base data if available and on the even fewer data of new tasks. In doing so, the authors obtain a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base classes are few, the network cannot learn where to focus implicitly."
SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a generative adversarial network (GAN) framework that injects constraints into the model during training by penalizing the generator whenever it outputs invalid structures. The constraints are assumed to satisfy structural requirements (e.g., molecules must be chemically valid, game maps must guarantee reachability of the end goal) that are difficult to capture with examples alone. The proposed method, called Constrained Adversarial Networks (CANs), inject the constraints directly into the learned model, avoiding the need for costly sampling or optimization steps. The authors also extend the semantic loss (Xu et al., 2018) to deal with complex constraints that would normally lead to intractably large circuits."
SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which gives equal expressive power to both the positive and negative halves on the input space and is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power. Finally, they show that the use of the proposed activation functions can significantly increase the robustness of deep neural networks to adversarial attacks."
SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes to use a Dirichlet layer to enrich the output of a classification black-box with a measure of uncertainty. The proposed method is model agnostic and can be used on top of any algorithm as long as it satisfies some desideratum. The authors propose a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. Experiments on two real-world datasets demonstrate the effectiveness of the method."
SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes to use blockwise adaptive stepsize in Adagrad, which is a variant of coordinate-wise adaptive gradient descent (CAD). The authors argue that blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance between adaptivity and generalization. Theoretically, the authors show that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex loss functions, but is better up to some constant. The authors also show its uniform stability and show its lower generalization error than SGD."
SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new dataset called CATER, which is a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. The dataset is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatio-temporal video architectures."
SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes a method to improve the model compatibility of GANs by adding a boundary calibration loss to the generator. The main idea is to use a set of pre-trained classifiers to obtain multiple decision boundaries, which are then used to guide the generator to generate samples that are close to the boundary of the classifiers. The proposed method is evaluated on the CIFAR-10 dataset and compared with the original GAN and WGAN."
SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the authors learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate that the L2L outperforms existing adversarial defense methods in both classification accuracy and computational efficiency. Moreover, the L_2L framework can be extended to the generative adversarial imitation learning and stabilize the training."
SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper studies the problem of constraint learning in inverse reinforcement learning (IRL), where the goal is to learn a set of constraints that maximizes the likelihood of observing an expert agent’s demonstrations. The authors propose a method based on the Maximum Entropy IRL framework, which allows them to reason about the likelihood given a state, action, and feature representation of the MDP. They propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and they evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper introduces a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). The authors show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces the accuracy of the proposed network. Overall, this paper suggests that the orientation tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour discovery."
SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,This paper proposes a context-aware object detection method that leverages conditional random field (CRF) for object detection. The main idea is to use CRF as a probabilistic graphical model to model the semantic context of the objects in the scene. The proposed method can be plugged into any existing region-based object detection methods. Experiments show that the proposed method improves the AP of object detection by 2 percentage points. 
SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper investigates the adversarial vulnerability of the BatchNorm layer. The authors hypothesize that the use of different normalization statistics during training and inference (tracking and moving average of these values at inference) is the main cause of this adversarial vulnerabilities. They empirically proved this by experiments on various neural network architectures and datasets. Furthermore, they introduce RobustNorm (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of Batchnorm."
SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper proposes two regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, the authors prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. The generalization analysis relies on the connection between wide neural network and neural tangent kernel (NTK). The authors show that the generalization bound is independent of the network size, and is comparable to the bound one can get when there is no label noise."
SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes to incorporate data augmentation into the convolutional neural tangent kernel (CNTK) and CNN-GP. Specifically, the authors propose to use a local average pooling (LAP) operation, which preserves efficient computability of the kernel and inherits the spirit of standard data augmentations using pixel shifts. The authors also propose to represent the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolution layer composed of random image patches. On CIFAR-10, the proposed method achieves 89% accuracy, matching the performance of AlexNet."
SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes an approach for combining model-free Q-learning and model-based Monte-Carlo Tree Search (MCTS) for reinforcement learning (RL). In particular, the authors propose a method called Search with Amortized Value Estimates (SAVE) which uses a combination of real experience as well as the results of past searches to improve overall performance and reduce planning cost. During training, SAVE uses MCTS to estimate the Q-values at encountered states. These estimates are used along with real experience to fit a Q-function, thus amortizing the computation required to estimate values during search. The Q-functions are then used as a prior for subsequent searches, resulting in a symbiotic relationship between model free learning and model based search. At test time, the proposed method achieves higher rewards with fewer training steps, and achieves strong performance with very small search budgets."
SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a deep 3D pan pipeline for monocular-to-stereo view synthesis. The proposed pipeline is based on a t-shaped adaptive kernel with globally and locally adaptive dilations, which can handle global camera shift and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB STEREO indoors dataset to prove the efficacy of the proposed method."
SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the variational auto-encoders (VAEs) from an information theoretic perspective. The authors propose a variational lower bound of the capacity-constrained InfoMax (CCIM) and show that the optimal generative model is the one optimising the CCIM. The paper also shows that the WAE (Tolstikhin et al. 2017) and InfoVAE (Zhao et al 2017) models are actually maximising a lower bound on the mutual information to a generator belonging to a certain family of generative adversarial networks (GANs) belonging to the family of variational variational autoencoders (VINs). The authors also show that in the case of WAEs, the capacity of the network is the entropy of the prior p(z|x) and the prior of the generative network p(x|z) is the maximum mutual information between the output of the generator and the input x. "
SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes two new node embedding methods, AE-USAE and AE-EGO, which are based on the Skip-gram approach. The authors prove that the embeddings approximately factorize the node-feature pointwise mutual information (PMI) matrix, which is the product of the adjacency matrix power and the node feature matrix. Theoretical results show that the PMI matrices are implicitly factorized by the embedding. Experiments on social, web and citation network datasets show the effectiveness of the proposed methods."
SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the phase transitions in the Information Bottleneck (IB) objective, i.e., the transition from a trivial global minimum to learning a nontrivial representation. The authors introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phases transitions, and draw its connection with the Fisher information matrix for parameterized models. They provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, they present an algorithm for discovering phase transition points."
SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method to estimate the advantage function of a policy and a state-action pair (s,a) in RL. The advantage function is estimated by subtracting an estimate of the value function V π(s) from the estimate of Q-value Q_pi(s, a) and the dependency factor C_pi, which is the contribution level of each future reward to the estimated advantage function. The authors propose to use the independence property between current action and future states in the environment to reduce the variance of the advantage estimation. They further propose to combine the proposed advantage estimator with the Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the proposed method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"This paper proposes a new estimator for infinite horizon off-policy policy evaluation based on importance sampling. The proposed estimator is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that the proposed method yields significant advantages over previous methods."
SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a two-stage framework for few-shot classification. The first stage learns task-agnostic feature on base data with a novel Metric-Softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. In the second stage, the authors design a task-adaptive transformation which adapts the classifier to each novel setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the state-of-the-arts."
SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper proposes a data-driven approach to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data, then employs a neural network that infers a correction to move a coarse result closer to the reference data. The paper provides insights into the targeted learning problem with two learning approaches: fully supervised learning methods with a naive and an optimized data acquisition and an unsupervised learning method with a differentiable Navier-Stokes solver."
SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper studies the problem of online continual compression, where one attempts to learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. This problem is highly relevant for downstream online continual learning tasks, as well as standard learning methods under resource constrained data collection. To address this, the authors propose a new architecture which Stacks Quantization Modules (SQM), consisting of a series of discrete autoencoders, each equipped with their own memory. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, our approach does not require pretraining, even on challenging datasets."
SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes an extension of the k nearest neighbor (kNN) method for multi-label metric learning. The proposed method is based on a bidirectional representation learning where the label dependency is also integrated and deep convolutional networks that handle image data. The model scales linearly in the number of instances and trains deep neural networks that encode both input data and output labels, then obtains a metric space for testing data. Experiments on a number of data sets demonstrate that the proposed model can achieve good performance."
SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the generalization performance of A-SGD (asynchronous asynchronous stochastic gradient descent) in terms of the learning rate and batch-size. The main idea is to study how the delay in the update of the parameters of the parameter server (PS) affects the stability of the algorithm, and how it affects the set of minima that can be reached by the algorithm. The authors derive closed-form rules on how the learning rates can be changed, while keeping the accessible set the same. They also extend this analysis to include momentum, and show that momentum can be either turned off or modified to improve the stability."
SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes an approach for model-free reinforcement learning (ML) that learns a special value function in hindsight that receives future observations as an additional input. This learning process reveals features of future observations that are most useful for value prediction (e.g. flight path of the ball), if provided in advance. These important features are then predicted, in advance, using only information available at test time (at the time of releasing the ball and given to the batter, the type of throw and the spin). Learning these-relevant features can help representation learning for an agent and provide an input to its value function. The authors show how this can help dramatically even in simple policy evaluation settings."
SP:6388fb91f2eaac02d9406672760a237f78735452,"This paper proposes a method for adversarial attacks on graph neural networks (GNNs) by rewiring three nodes (vfir, vsec, vthi) by adding/deleting an edge between vfir and vsec and adding one between v_i and v_thi. The attack strategy is learned using reinforcement learning. The proposed method is evaluated on three real-world graph classification tasks (node classification, graph classification, and semi-supervised node classification)."
SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper considers the problem of estimating the prediction error of encoder-decoder convolutional neural networks for denoising problems. The authors propose to use Stein’s unbiased risk estimator (SURE) as an unbiased estimator of prediction error for denoisings. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. This paper proposes a novel bootstrap and aggregation scheme to prevent the neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems."
SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"This paper proposes a meta-learning approach for few-shot classification, where the number of instances per task and classes can vary across tasks. The authors propose a Bayesian approach to balance the effect of meta- and task-specific learning within each task. Specifically, they first obtain set-representations for each task, which are learned to convey useful statistics about the task or class distribution, such as mean, variance, and cardinality (the number of elements in the set), and then learn the distribution of three balancing variables a function of the set: 1) task-dependent learning rate multiplier, which decides how far away to deviate from the meta-gradient, when performing task specific learning. 2) class-dependent learn rate, to decide how much information to use from each class, to automatically handle class imbalance, and 3) task dependent initialization for initial model, which modifies the shared initial model parameter. They validate their approach on CIFAR-FS-ImageNet and miniImageNet datasets, and show the effectiveness of each balancing component."
SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning based on reinforcement learning. The main idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. The proposed method can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, the authors show that the proposed method is a regularized variant of behavioral cloning (BC) that uses a sparsity prior to encourage long-horizon imitation. Empirically, it outperforms BC and achieves competitive results compared to GAIL."
SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. They combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. They show that their method works for large, deforming point sets from different sources to demonstrate the flexibility of our approach."
SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer (OT-SI) for optimal transport (OT) that learns the transport cost function using a small amount of side information which is often available. The side information captures subset correspondence, i.e. certain subsets of points in the two data sets are known to be related. The authors develop an OT-SI that differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. Experiments on image, marriage-matching and single-cell RNA-seq datasets demonstrate the effectiveness of the proposed method."
SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"This paper proposes a novel method to detect anomalies in large datasets under a fully unsupervised setting. The key idea is to learn the representation underlying normal data. To this end, the authors leverage the latest clustering technique suitable for handling high dimensional data. They train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset based on clustering and representation learning. The reconstruction error serves as a scoring function to assess the normality of the data. The proposed framework does not rely on any training labels. Instead, it iteratively distills out anomalous data and improves the learned representation of normal data by incorporating clustering techniques."
SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper proposes a new algorithm, Projection-Based Constrained Policy Optimization (PCPO), which is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. The authors theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. The empirical results over several control tasks demonstrate that PCPO achieves superior performance."
SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper studies the role of the parameter $\alpha$ in word embeddings. The authors show that the embedding can be viewed as a low-rank transformation from the word-context co-occurrence space to the word embedding space, which preserves the relative distances among words. They also provide a theoretical explanation for this behavior, and derive a method to automatically find its optimal value. The experiments on real datasets verify their analysis."
SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"This paper proposes a method for clustering data using a group of approximate Random Projection Trees (RP Trees). The authors claim that the proposed method achieves high accuracy, high efficiency in terms of speed, and independence from prior knowledge. The proposed method is evaluated on three real-world datasets, and the results show that it achieves an efficiency of up to 3.5 times higher than RP Trees."
SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method between Markov chain Monte Carlo (MCMC) and variational inference (VI). The proposed method is based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. The effectiveness of EI was verified on synthetic examples and on popular generative models."
SP:64f2744e938bd62cd47c1066dc404a42134953da,"This paper proposes a method for causal inference with missing data, which generalizes and extends the work of Kallus et al. (2018) in different aspects: (i) instead of linear factor analysis models with missing values, they consider non-linear versions using deep variable models (Kingma & Welling, 2014; Rezende et al., 2014); (ii) they rely on the missing at random assumption for the missing data at random (MAR) assumption, and (iii) they consider latent confounders whose distribution is learned through variational autoencoders adapted to missing values. They can be used either as a pre-processing step prior to causal inference or embed them in a multiple imputation strategy to take into account the variability due to missing data. Numerical experiments demonstrate the effectiveness of the proposed methodology especially for non- linear models."
SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning policies, by combining ENAS (Efficient Neural Architecture Search) and ES (Salimans et al., 2017) in a highly scalable and intuitive way. The authors define the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, and represent compact architectures via efficient learned edge partitionings. For several RL tasks, the authors manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices (Choromanski et al. 2018)."
SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a generalization of the Wavelet Transform (WT) and Short-Time Fourier Transform (STFT) and Continuous Wavelet transform (CWFT) by introducing the Learnable Group Transform (LGT) framework, which generalizes the affine transformations of a mother filter leading to the wavelet filter-bank by introducing non-linear transformations. The authors propose a parameterization of such a nonlinear map such that its sampling can be optimized for a specific loss and signal. The learnable group transform can thus be cast into a Deep Neural Network. The experiments on diverse time-series datasets demonstrate the expressivity of this framework."
SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCNs) to (products of) constant curvature spaces. The authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvatures, leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Their class of models smoothly recover the Euclideans when the curvature goes to zero from either side. Empirically, the authors show that the proposed method outperforms Euclideen GCNs in the tasks of node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior, according to their discrete curvature."
