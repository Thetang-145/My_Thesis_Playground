paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"This paper proposes a method for multi-agent multi-task learning (MARL) by decomposing joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. A bi-level learning hierarchy: the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. The proposed method outperforms the current state-of-the-art MARL algorithms on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and achieves rapid transfer to new environments with three times the number of agents."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the behavior of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, the authors prove that SSGD converges with rates $O(1/\epsilon^2)$ for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the SGD methods applied to smooth problems that also satisfy the same condition. The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmoothed machine learning models."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper proposes a method to improve the efficiency and performance of transformer-based machine learning models by replacing the regular transformer layers with ""reservoir"" layers. These are non-linear layers that are randomly initialized and never updated. The authors propose a new metric, AUCC, to measure the performance-efficiency trade-off between the regular and reservoir layers. The proposed method is evaluated on a variety of machine translation and masked language modeling tasks."
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,This paper studies the connection between group representation theory and the filter transform technique for steerable convolutional networks. The authors show that the kernel constructed by filter transform can be interpreted in the group representations theory and that filter transformed kernels can be used to convolve input/output features in different group representations. This interpretation helps complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable operators. Experiments are executed on multiple datasets to verify the feasibility of the proposed approach.
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while maximizing the program’s score with respect to a neural model. Specifically, the authors focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of the method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but also allows the authors to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs. The experimental results show that the proposed method substantially outperforms prior state-of-the-art techniques in terms of accuracy."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper proposes a protein graph convolutional neural network (PGCN) to predict the substrate specificity landscape of a protease enzyme, which is the set of all sequence motifs that are recognized/cut/not recognized by the enzyme. The proposed method is based on a structure-based molecular interaction graph generated using the Rosetta energy function that describes the topology and energetic features, to determine substrate specificity. The authors use the PGCN to recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C virus and show that its performance in classification tasks is equivalent or better."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the underestimation bias of double Q-learning, a classical method for reducing overestimation bias, which is caused by taking maximum estimated values in the Bellman operation. In this paper, the authors show that under the effects of approximation error, double Q learning may have multiple non-optimal fixed points. To address this issue, they propose a simple but effective approach as a partial fix, which leverages an approximate dynamic programming to bound the target value. The experiments show that the proposed method has shown great promise in improving both sample efficiency and convergence performance."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"This paper proposes NOT-SO-BIG-GAN (NSB-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, it generates images in low-frequency bands by training a sampler in the wavelet domain. Then, it super-resolves these images back to the pixel-space with a novel wavelet super-resolution decoder network. Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512x512, the proposed method achieves a FID of 10.59 – beating the baseline BigGAN model – at half the compute (256 TPU-v3 cores)."
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper provides a theoretical analysis of self-supervised learning (SSL) for few-shot learning (FSL). In particular, the authors analyze why SSL is suitable for FSL and the main difference between supervised training and SSL on FSL. They also provide a bound for the gap between the self supervised loss and the supervised loss. Finally, they propose potential ways to improve the test accuracy under the setting of SSL."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper studies the convergence properties of two-layer teacher-student neural networks with ReLU activation and Gaussian inputs. The authors show that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for “not-too-wide” neural networks that there might not be any local minima near the initialization. The paper also shows that under the most basic settings, all student neurons must align with the teacher neuron at any local minimum. "
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper proposes a framework for combining deep neural networks (DNNs) with commonsense knowledge to improve the performance of DNNs. The framework is based on the idea of adapting semantic logic (ASL), which is an extension of the neuro-symbolic approach proposed in [1]. The authors provide a formal grounding for the correctness and generality of DASL for the representation of declarative knowledge in first order logic, including correctness of mini-batch sampling for arbitrary domains. The authors demonstrate the effectiveness of the proposed method on a toy problem and a visual relationship detection task. "
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper proposes two regularizations to make feedforward residual neural networks (ResNets) more iterative and convergent. In particular, the authors define three indices of iterative convergence to measure the degree to which ResNets learn iterative solutions and introduce a regularization approach that encourages the networks to learn to approximate iterative recurrent computations. The authors also introduce a Lipschitz constraint to make the networks more convergent using spectral normalization. Experiments are conducted on MNIST, CIFAR-10/100 and ImageNet to demonstrate the effectiveness of the proposed regularizations. "
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"This paper proposes two normalization techniques, SelfNorm and CrossNorm, to improve the robustness of deep neural networks to out-of-distribution (OOD) data. SelfNorm uses attention to recalibrate statistics (channel-wise mean and variance), while CrossNorm exchanges the statistics between feature maps. Extensive experiments on different domains (vision and language), tasks (classification and segmentation), and settings (supervised and semi-supervised) show the effectiveness of the proposed methods."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper proposes and studies the effectiveness of augmenting a simple attention module in the convolutional encoder of an RL agent. The proposed module can extract interpretable task-relevant information such as agent locations and movements without the need for data augmentations or contrastive losses. Experiments on the widely benchmarked DeepMind Control Suite environments demonstrate that the proposed module (R-LAtte) is able to achieve similar performance compared with representation learning and data augmentation methods, which are currently the best-performing ones in the literature."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"This paper proposes an extension to GradNorm, a widely used gradient-based approach for training multitask networks, by homogenizing not only the gradient magnitudes but also their directions across tasks. Specifically, it adds a layer of task-specific rotation matrices that aligns all the task gradients. The authors also analyze the proposed method through the lens of game theory, providing theoretical guarantees on the algorithm stability and convergence. Experiments on several real-world datasets and network architectures show that Rotograd outperforms previous approaches."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper proposes a novel I2I translation constraint, called Minimal Geometry-Distortion Constraint (MGC), which promotes the consistency of geometry structures and reduce the unwanted distortions in translation by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of MGC, the authors propose an approximate representation of mutual information called relative Squared-loss Mutual Information (rSMI) that can be efficiently estimated analytically. The experimental results demonstrate that MGC achieves high quality translation to maintain the geometry of images in the original domain."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the sampling sensitivity of discriminators in point cloud GANs. The authors propose a sampling spectrum to depict the different sampling sensitivities of the discriminators. They further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics. They discover a middle-point sampling-aware baseline discriminator, which improves all existing point cloud generators by a large margin on sampling-related metrics."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper investigates the adversarial robustness of capsule neural networks (CapsNets). CapsNets have been shown to be more robust to white-box attacks than CNNs under popular attack protocols. In this paper, the authors propose a novel vote attack where they attack the output capsules directly. The vote attack is not only effective but also efficient by circumventing the routing process. Extensive experiments demonstrate the superior attack performance of the proposed method."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"This paper proposes a meta-reinforcement learning algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. The method learns an informed policy (i.e., a policy receiving as input the description of the current task) that is used to both construct task embeddings from the descriptors, and to regularize the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task and by exploiting them efficiently. It is able to learn better strategies than Thompson Sampling approaches, and faster than recurrent neural network policies and Task Inference approaches."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper proposes a method to learn an RL policy from offline data in the real-world sequential recommendation system (SRS). Instead of increasing the fidelity of models for policy learning, the authors propose to learn to adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable to real world environments where dynamics are changing and have stochasticity in the offline setting. Experiments are conducted in synthetic environments and a real-life ride-hailing platform."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper proposes a method for learning goal-reaching policies from scratch, without the need for expert demonstrations or a value function. They leverage the property that any trajectory is a successful demonstration for reaching the final state in that trajectory. They propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal- reaching behaviors from scratch. The agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. They formally show that this iterated supervised learning procedure optimizes a bound on the RL objective and derive performance bounds of the learned policy. The authors empirically demonstrate improved performance and robustness over current RL algorithms."
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper proposes FastSpeech 2, a fast and high-quality non-autoregressive text-to-speech (TTS) model that addresses the issues in FastSpech (Ren et al., 2019) by directly training the model with ground-truth target instead of the simplified output from teacher, and introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. In addition, the authors also introduce a continuous wavelet transform to improve the duration accuracy and introduce more variance information including pitch and energy to ease the one to many mapping problem, and improve the voice quality. Experimental results show that the proposed method achieves a 3x training speed-up and faster inference speed."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"This paper proposes to reformulate the unsupervised dimension reduction problem (UDR) in the language of tempered distributions, i.e. the problem of approximating an empirical probability density function pemp(x) by another tempered distribution q(x), whose support is in a k-dimensional subspace. This reformulation allows to establish a connection with another classical problem of data science — the sufficient dimension reduction (SDR) problem. The authors introduce a nonnegative penalty function R(f) that “forces the support of f to be k-dimensions” and present an algorithm for minimization of I(f), which is based on the idea of two-step iterative computation. The algorithm performs quite stably when we vary most of the hyperparameters."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,"This paper proposes Feature Contrastive Learning (FCL) to balance robustness and sensitivity in deep neural network training. Unlike previous work that only enforces robustness, FCL aims to promote sensitivity to high utility features and inhibit sensitivity to low utility features. The performance of FCL is validated on both synthetic and real image classification datasets."
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper proposes a new method for imitation learning based on adversarial learning. The main idea is to use a discriminator network to learn a latent representation of the expert's state and action space, which is then used to train an agent to imitate the expert. The discriminator is trained by minimizing the mutual information between the latent representation and the action space. The proposed method is evaluated on a variety of tasks, including balancing, manipulation, and locomotion."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides a theoretical analysis of the lottery ticket hypothesis (LTH) which states that learning on a properly pruned network (the winning ticket) improves test accuracy over the original unpruned network. This paper characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. It shows that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, it shows that when the algorithm is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to the total number of non-pruned weights in the hidden layer. The paper also provides extensive numerical validations of the theoretical findings."
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes AutoLabel to automatically learn the labels for augmented data, based on the distance between the clean distribution and augmented distribution. The proposed method is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. The authors show that AutoLabel is a generic framework that can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on CIFAR-10, CifAR-100 and ImageNet show that the proposed method can improve models’ accuracy and calibration performance."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper studies the problem of representation learning in self-supervised learning, i.e. pretraining representations only using unlabeled data. The authors propose a novel objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, the authors generalize contrastive learning and provide an alternative theoretical explanation for the success of these methods. Empirically, RELIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization."
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper proposes a visual transformer network (VTNet) for object goal navigation. The main idea of the proposed method is to leverage the relationships among all the object instances in a scene and the spatial locations of objects and image regions so that directional navigation signals can be learned. The authors also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. Experiments in the artificial environment AI2-Thor demonstrate that the proposed VTNet significantly outperforms state-of-the-art methods in unseen testing environments."
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,This paper proposes a new secure aggregation method for federated learning. The main idea is to design the topology of the secret-sharing nodes (denoted by the assignment graph G) as sparse random graphs instead of the complete graph corresponding to the existing solution. Theoretical guarantees on the reliability/privacy of the proposed scheme are also provided. Extensive experiments on real-world datasets show that the proposed method achieves the same levels of reliability and data privacy as the conventional solution.
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,This paper proposes a new neural network architecture for the problem of finding optimal auctions in deep learning. The proposed approach builds on top of the work of Duetting et al. (2019) which proposes to use a time-independent Lagrangian for the optimization of the optimal auction. The authors also propose to amortize the optimization process by introducing an additional neural network. Experiments are conducted to show the effectiveness of the proposed approach. 
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper proposes a bi-tuning approach to fine-tune pre-trained representations for supervised and unsupervised representation learning. The proposed approach consists of two heads: a classifier head with an improved contrastive cross-entropy loss to leverage the label information in an instance contrastive way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. Comprehensive experiments confirm that the proposed approach achieves state-of-the-art results on CUB and CIFAR-10 datasets."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper studies the problem of adversarial robustness and adversarial training. The authors argue that standard adversarial accuracy fails to properly measure the robustness of classifiers. They introduce a new measure for robustness, called ""genuine adversarial accuracies"", which measures the adversarial perturbation on clean data and adversarially perturbed samples. They show that a single nearest neighbor (1-NN) classifier is the most robust classifier according to the proposed measure. "
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper studies the problem of dyadic fairness in graph-structured data. Specifically, the authors propose an algorithm, FairAdj, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that the proposed method delivers effective dyadic fairness and at the same time enjoys a favorable fairness-utility tradeoff."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes a disentangled exploration autoencoder (DEAE), which uses disentanglement representation and regularization to guarantee the validity of exploration in latent space and achieve controllable synthesis. The encoder of DEAE first turns the input sample into disentangling latent code, then explores the latent code space through directed interpolation. To aid the interpolated latent code in successfully outputting a meaningful sample, after the decoder, DEAE regularizes the output by ’reusing’ the encoder to force the obtained latent representation to maintain perfect disentangle. Experiments demonstrate that DEAE can improve the performance of downstream tasks by synthesizing attribute-controllable augmented samples and eliminate dataset bias."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a new Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. The authors take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. They simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. This allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values on binarized MNIST."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper studies the sample complexity and loss landscape of attention-based neural networks. Theoretical results show that, under mild assumptions, every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Experiments on various datasets validate the theoretical findings."
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper extends the concept of expected free energy (EFE), which is a core quantity in active inference, and claims that EFE can be treated as a negative value function. Motivated by this connection, the authors propose a simple but novel method for learning a prior preference from experts. Experimental results show the possibility of active inference with EFE-based rewards and its application to an inverse RL problem. "
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. The authors theoretically show how to improve the generalization theoretically using OOD data in each learning scenario and complement their theoretical analysis with experiments on CIFAR-10, Cifar-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper proposes a new meta-RL method, Fast Linearized Adaptive Policy (FLAP), which is able to extrapolate well to out-of-distribution tasks without the need to reuse data from training, and adapt almost instantaneously with the need of only a few samples during testing. FLAP builds upon the idea of learning a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods fail to generalize to."
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a federated kernel-k-means algorithm to solve the optimization problem of kernel $k$ means in a distributed setting. The authors propose a distributed stochastic proximal gradient descent (DSPGD) algorithm to find an approximate solution to the optimization of the problem. They also propose a communication efficient mechanism (CEM) to reduce the communication cost. Theoretical analysis shows that DSPGD with CEM converges with an O(1/T) rate, where T is the number of iterations. The clustering quality of the proposed algorithm is evaluated on several real-world datasets."
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"This paper proposes CompOFA, a method to reduce the search space of Once-For-All (OFA) networks by using compound couplings between model dimensions, which speeds up the process of one-shot training and neural architecture search with hardware latency constraints. The authors show that intractably large architectural search spaces are unnecessary for both accuracy and diversity of models, and introduce a simple heuristic that vastly shrinked the search spaces without losing on Pareto optimality. This smaller search space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while reducing the complexity of the training and subsequent extraction algorithms."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper proposes a meta-learning algorithm, ADML (ADversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. ADML leads to the following desirable properties: 1) it turns out to be very effective even in the cases with only clean samples; 2) it is robust to adversarial attacks; 3) it sheds light on tackling the cases of limited and even contaminated samples."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper proposes a method to improve the decoding of error correction codes (ECC) using a self-attention mechanism. The proposed method, called perm2vec, selects a suitable permutation out of the code’s PG without actually trying all the permutation based decodings. The method pre-computes the permutations’ representations, thus allowing for fast and accurate permutation selection at the inference phase. The authors demonstrate the effectiveness of the proposed method by showing significant BER performance improvement compared to the baseline decoding algorithms for various code lengths."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper proposes to perform an unsupervised clustering task before fine-tuning BERT for a target text classification task. Specifically, the authors propose to train BERT on predicting the cluster labels and then fine-tune it on the target task. The authors show that this additional classification step can significantly reduce the demand for labeled examples mainly for topical classification tasks."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper studies the problem of model-based reinforcement learning (MBRL) in the context of generative models using a fixed (random shooting) control agent. The authors compare the performance of deterministic and probabilistic models on the Acrobot environment. They find that deterministic models consistently outperform their probablistic counterparts, and heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. "
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes a method for learning disentangled representations that explicitly disentangle affine transformations in a self-supervised and rigorous manner. The proposed method is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias. The affine transformation is derived by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum likelihood estimation. The features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, skew, and translation can be explicitly selected and learned."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,This paper proposes a contrastive learning method for representation learning. The authors propose to use the distributional divergence between the weakly and strongly augmented images over the representation bank to supervise the retrieval of strongly augmented queries from a pool of candidates. The proposed method achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned.
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"This paper proposes a method for de-identification of magnetic resonance imagery (MRI) that preserves privacy by remodeling privacy-sensitive facial features, instead of removing them. The authors propose a conditional, multi-scale, 3D GAN architecture that takes a patient’s MRI scan as input and generates a 3D volume in which the brain is not modified but the face has been de-identified. Compared to the classical removal-based techniques, the proposed method preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper proposes a multi-head attention based global pooling layer to capture the interaction between nodes according to their structural dependencies. The authors show that the proposed method satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. The experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks and obtains even larger performance gain on graph reconstruction and generation tasks."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper studies the problem of over-squashing in GNNs. The authors propose a new explanation for this problem, which they call the ""bottleneck"", which they claim is a bottleneck when aggregating messages across a long path. They show that this bottleneck causes the over-Squashing of exponentially growing information into fixed-size vectors, which results in the failure to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. They further show that GNN that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over- Squashing than GAT and GGNN. Finally, they show that prior work, which extensively tuned GNN models of long range problems, suffer from over- squashing and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights."
SP:90d8fa381446923902e42b259392e5e975e6caa1,This paper proposes a method for cross-domain sentiment analysis based on aligning two domain-specific distributions in a shared embedding space. The authors show that inducing larger margins between the classes in the source domain using an intermediate multimodal prototypical distribution can help mitigate the domain shift problem in the target domain. The experiments demonstrate that the algorithm is effective.
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper proposes an evaluation methodology to measure the gender bias in NLI systems by constructing a challenge task which involves pairing gender neutral premise against gender-specific hypothesis. The authors use this challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Their findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets are significantly prone to gender-induced prediction errors. They also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper revisits variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. The authors show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, the authors propose two methods respectively based on the transitional probability model and Gaussian mixture model."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the use of deep neural network ensembles for learning in the low-data regime. In particular, the authors focus on image classification problems with a few labeled examples per class and improve sample efficiency by using an ensemble of relatively small deep networks. They show that deep ensembling is a simple yet effective technique that outperforms current state-of-the-art approaches for learning from small datasets. They compare different ensemble configurations to their deeper and wider competitors given a total fixed computational budget and provide empirical evidence of their advantage."
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"This paper proposes a method to further compress Binary Neural Networks (BNNs) by introducing sparsity, while reducing their required computations. The approach is based on quantization of weights in the 0/1 binary domain and a highly sparse initialization of the network. The method has been evaluated on feed-forward linear and convolutional networks on MNIST and CIFAR-10 data sets. Experiments confirm that SBNNs can achieve high compression rates and good generalization, while further reducing the operations of BNNs."
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a simple post hoc calibration method for predictive uncertainty. The proposed method uses outlier exposure to calibrate the model probabilities. The method works by no longer requiring that model outputs approximate class probabilities. Instead, it adds a simple extra calibration step, and detect the level of corruption of data, which allows the use calibrations tuned to the corruption level of the data."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, they compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. They also investigate the impact of node attributes on the performance of the different models and kernels. Their results reveal interesting findings. For instance, they find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with GNNs."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a method for self-supervised image animation based on driving videos. The proposed method, called PriorityCut, is based on the observation that the occlusion masks of the foreground of the driving video can be used to regularize the discriminator for inpainting on the warped source image. The method is evaluated on the CIFAR-10 dataset, and the results show that the proposed method outperforms the baseline methods. "
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper proposes a method for learning disentangled representations of independent causal mechanisms (ICM), which directly model multiple data generation processes (mechanisms) in a coarse granularity. The authors aim to learn a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. They outline sufficient conditions under which the mechanisms can be learned using a single self-supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, they prove the identifiability of their model w.r.t. the mechanisms in the self supervised scenario. Experiments show that their approach is more robust to intervention, covariant shift and noise due to the disentanglement between the data generation process."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper proposes a graph aligning approach to predict molecular graph structures (W) given a 2D image of a chemical compound (U) and a graph representation (V) of the target domain (W). The proposed approach is based on the idea of fully mediating representation V, which is a planar embedding of the chemical graph structure we are predicting. The authors propose a graph alignment approach that generates rich or detailed labels given normal labels W. The approach is evaluated on two datasets (Indigo and Maybridge) and compared with the state of the art."
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper proposes a neural network-based method to solve the energy optimization problem of a chiller plant. The proposed method is based on the observation that the energy consumption of most chillers can be viewed as an input-output monotonic problem, and the authors design a Neural Network to mimic the physical behavior of the system. The authors verify the proposed method in a cooling system of a data center, and experimental results show the superiority of the framework in energy optimization compared to existing ones."
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a causal spatio-temporal fusion transformer (CTF) method for multi-task, multi-target and multitarget forecasting. The proposed method is based on the causal attention and the Conditional Average Treatment Effect (CATE) estimation method in causal inference. The authors also propose a novel and fast multi-head attention evolved from Taylor’s expansion instead of softmax, reducing time complexity from O(V) to O(\V) where V is the number of nodes in a graph. The spatial graph fusion mechanism is further designed to reduce the parameters’ scale."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes an unsupervised framework for joint learning of discrete and continuous factors of variability. The proposed method, called coupled mixture VAE (cpl-mixVAE), utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. The authors provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. The approach is evaluated on MNIST and dSprites, achieving state-of-the-art categorical assignment while preserving interpretability of the continuous factors."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper provides a general characterization of steerable kernel spaces for the group equivariant convolutional networks (GCNNs). The authors generalize the famous Wigner-Eckart theorem for spherical tensor operators and show that steerable kernels are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan coefficients, 3) harmonic basis functions on homogeneous spaces, and 4) a basis of endomorphisms for the space of G-steerable kernels. "
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper studies the effect of selective classification on the accuracy disparities between groups. The authors show that selective classification can improve average accuracies, but it can also magnify existing accuracy disparities, especially in the presence of spurious correlations. They also show that increasing abstentions can even decrease accuracies on some groups. To better understand this phenomenon, they study the margin distribution, which captures the model’s confidences over all predictions. For symmetric margin distributions, they prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage and whether the distribution satisfies a property we call left-log-concavity. Motivated by their analysis, they train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selectively classification uniformly improves each group on these models."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper proposes a hierarchical nonnegative CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. It utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD. The authors empirically demonstrate the promise of this method on both real and synthetic datasets."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"This paper proposes the first collective robustness certificate, which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. The proposed method combines many weak base certificates into a provably stronger collective certificate. It is agnostic towards network architectures and base certification procedures."
SP:cc93dd2f68e415e2457166e78627865dc1b44697,This paper proposes Quantile Regression GAN (QRGAN) to improve the performance of GANs by minimizing the 1-Wasserstein distance between real and generated data distribution. The proposed method is based on quantile regression and quantile normalization. The authors analyze the output space of discriminator and gradients of fake samples to see if the discriminator guides the generator well. QRGAN obtains an apparent improvement in the evaluation and comparison of generation performance.
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"This paper investigates relevance metrics for instance-based similarity-based explanation of machine learning models. The authors propose three tests to evaluate relevance metrics and show that cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. They also show that some metrics perform poorly in the proposed tests and analyze the reasons of their failure. "
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"This paper proposes a low-rank global attention (LRGA) module to improve the generalization power of GNNs. The authors show that the LRGA module can align with the powerful 2-FWL isomorphism test by learning simple monomial functions, which have a known sample complexity bound. Empirically, the authors demonstrate that augmenting GNN models with LRGA improves their performance significantly, often achieving SOTA performance."
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,This paper proposes an adaptive label smoothing method to improve the calibration performance of convolutional neural networks (CNNs) for object localization and calibration. The proposed method uses bounding box information pertaining to objects can be used to compute a smoothing factor adaptively during training to improve classification performance of CNNs. Experiments on ImageNet-1K and COCO demonstrate the effectiveness of the proposed method.
SP:5254658923e594294b69d124a8d004166852822a,"This paper proposes a convex duality framework for learning a two-layer fully-convolutional ReLU denoising network with ReLU activations. The main idea is to train the network with weight decay regularization, which induces path sparsity regularization for training, while the prediction is piece-wise linear filtering. Experiments on MNIST and fastMRI datasets confirm the efficacy of the proposed method."
SP:085cad6bc143c8713580bddfaa71f06496dac314,"This paper proposes an end-to-end speech synthesis model that is able to synthesize speech from normalised text or phonemes in an end to end manner. The proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models."
SP:01148cea55db606aa78d27e900818684a8bce9ab,This paper proposes a method for learning node representations for node classification and matrix completion in the presence of missing node attributes. The proposed method is based on matrix decomposition and Wasserstein graph diffusion for representation learning such that observed node features can be transformed into discrete distributions and diffused along the graph. The authors show that the proposed method can produce high-quality node representations with powerful ability to represent attribute and structure information. Extensive experiments on node classification under two missing settings verify the powerful representation capacity.
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes a meta-learning framework for generating a curriculum of intrinsic rewards as a form of intrinsic motivation for reinforcement learning (RL) agents. The proposed method, AMIGO, consists of two parts: a teacher and a student policy. The teacher is trained to generate goals that are challenging but achievable, and the student policy is conditioned on the teacher's goals to maximize an intrinsic reward. The paper shows that the proposed method outperforms other intrinsic motivation methods in procedurally-generated tasks. "
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"This paper studies the problem of information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. Specifically, a user requesting a file from the dataset should be able to reconstruct the requested file with a prescribed distortion, and in addition, the identity of the requested files should be kept private from the server. The proposed model can be seen as an extension of the well-known concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement. The authors show that the optimal rate-distortion-leakage tradeoff is convex and that in the limit of large file sizes this allows for a concise information-theoretical formulation in terms of mutual information. Moreover, the authors propose a new data-driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in term of download rate from the data itself."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"This paper proposes a decoupled greedy learning method for graph neural networks (DGL-GNN) that, instead of sampling the input graph, decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. This allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. The method achieves improved efficiency without significantly compromising model performances, which would be important for time or memory limited applications."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper proposes a framework for answering Existential Positive First-Order logical queries by answering each of its sub-queries, and aggregating the resulting scores via t-norms. The main idea is to translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor, and then analyze two solutions to the optimisation problem, including gradient-based and combinatorial search. Experimental results show that CQD produces significantly more accurate results than current state-of-the-art complex query answering methods on incomplete Knowledge Graphs."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,This paper proposes a method for verifying the local robustness of neural networks with piecewise-linear activation functions. Local robustness is defined as the ability of a neural network to classify all inputs within an $\ell_p$-ball consistently. The authors propose to evaluate the robustness by searching for decision boundaries within the regions around a given input. The proposed method is based on geometric projections of the region around a point in the input space. The method is shown to be computationally efficient and is able to scale up to much deeper networks.
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper presents an approach to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, based on text corpora. The authors propose an approach for creating an embedding of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions. They show that the learned dimensions are interpretable, and correspond to typical patterns of interaction with objects. Finally, the dimensions can be used to predict a state-of-the-art mental representation of objects, derived purely from human judgements of object similarity."
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for the emergence of individualism in multi-agent reinforcement learning (MARL). The proposed method learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, which makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. Two regularizers are introduced to increase the discriminability of the classifiers. Empirically, the authors show that EOI outperforms existing methods in a variety of multiagent cooperative scenarios."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper proposes a smoothed weighted ensemble (SWEEN) method to improve the certified robustness of randomized smoothed classifiers. SWEEN is based on weighted weighted ensembling, where the weights of each model are computed as a weighted sum of the parameters of the other models in the ensemble. Theoretical analysis is provided to show that under some mild assumptions, the optimal model can be obtained from training under mild assumptions. The paper also proposes an adaptive prediction algorithm to reduce the prediction and certification cost of the proposed method. Extensive experiments are conducted to demonstrate the effectiveness of the method."
SP:ea892e3d199ed6121279b20061a87f43afae8796,This paper proposes a method to learn the hierarchy of tasks in an unsupervised way. The method is based on the idea of learning the subtask hierarchy from the demonstration trajectory. The authors propose a novel ordered memory policy network (OMPN) to represent the hierarchy and leverage it to perform task decomposition. Experiments on Craft and Dial demonstrate the effectiveness of the proposed method. 
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) prediction. The proposed model is based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction.  Theoretically, the authors prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines."
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of robust online learning in the presence of adversarial corruptions. In particular, the authors study settings where an online algorithm makes a prediction at each time step, and receives a stochastic reward from the environment that can be arbitrarily corrupted with probability $\sqrt{[0,12]$. The authors propose algorithms with small regret over a period of time steps, while the algorithm observes corrupted rewards, and require its regret to be small with respect to the true uncorrupted reward distribution. They build upon recent advances in robust estimation for unsupervised learning problems to design robust online algorithms with near optimal regret in three different scenarios. "
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes Rewriter-Evaluator, which consists of a rewriter and an evaluator. The rewriter produces a new translation at every pass to improve the past translation and evaluates the quality of the new translation. The evaluation is done using a prioritized gradient descent (PGD) method. The authors apply the proposed framework to improve Transformer and RNNSearch on two translation tasks, Chinese-English and English-German."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"This paper proposes a method for learning a calibrated multimodal predictive distribution for semantic segmentation, where the empirical frequency of the sampled predictions closely reflects that of the corresponding labels in the training set. To this end, the authors propose a two-stage, cascaded strategy for calibrated adversarial refinement. In the first stage, they explicitly model the data with a categorical likelihood, and in the second, they train an adversarial network to sample an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings."
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new compressed communication with error feedback (EF) framework to deal with the error induced by unbiased compressors such as Top-K or PowerSGD. In particular, the authors propose a construction which can transform any contractive compressor into an induced unbiased compressor. Theoretical results show that this approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. The authors also extend their results to federated learning with partial participation following an arbitrary distribution over the nodes and demonstrate the benefits thereof."
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper proposes a new research framework, hyperparameter transfer across adjustments (HT-AA), to improve efficiency during the development of machine learning (ML) algorithms. Specifically, the authors propose four simple HT-AA baseline algorithms and eight benchmarks that can be used to improve the performance of an existing HPO algorithm. The authors also provide python packages for the baselines and benchmarks to facilitate future research."
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper investigates the role of label representation in training deep neural networks. The authors hypothesize that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. They support this hypothesis with evidence from various label representations including constant matrices, spectrograms, shuffled spectrogram, Gaussian mixtures, and uniform random matrices of various dimensionalities. The experiments reveal that high-dimensional, high-entropy labels achieve comparable accuracy to text (categorical) labels on the standard image classification task, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes a method to improve the robustness and uncertainty performance of ensemble neural networks by using a single model to train multiple subnetworks that independently learn the task at hand. The main idea is to use a multi-input multi-output (MIMO) configuration, where one can utilize the single model’s capacity to train several sub-networks. By ensembling the predictions made by the sub-nets, the proposed method improves model robustness without increasing the compute cost."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a method for knowledge distillation (KD) that uses sparse representation learning to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representations learning. The proposed method first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixel-level and image-level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plug-and-play manner. The experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a new policy similarity metric (PSM) and contrastive representation learning (PSE) to improve generalization in reinforcement learning. PSM measures the behavioral similarity between states, and PSE is a contrastive learning procedure to embed any state similarity metric to obtain policy similarity embeddings (PSEs). PSEs are evaluated on a variety of RL tasks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite."
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"This paper studies the disentanglement of affine transformations in deep neural networks. The authors show that a popular approach to disentangle natural factors of variation in data (e.g. object shape vs pose) introduces discontinuities in the encoder for a broad family of transformations acting on images. They then propose an alternative, more flexible approach that relies on distributed equivariant operators, which can potentially act on the entire latent space. Theoretical and empirical results demonstrate the effectiveness of the proposed approach."
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes an EM algorithm to estimate the maximum a posteriori (MAP) estimate for the SNMHP model, which is a variant of the Hawkes process. The authors propose to use three auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes and sparsity variables) to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. They demonstrate the accuracy and efficiency performance of their algorithm on synthetic and real neural recordings."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the gradient descent (GD) dynamics for training two-layer neural network models in the under- and over-parameterized regime. It shows that there are two distinctive phases in the GD dynamics: an early phase in which GD dynamics follows closely that of the corresponding random feature model, followed by a late phase where the neurons are divided into two groups: a group of a few (maybe none) “activated” neurons that dominate the dynamics and another group of “quenched’ neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper proposes a method for solving constrained constrained Markov decision process (CMDP) problems by decomposing the CMDP into a pair of MDPs; reconnaissance MDP (R-MDP) and planning MDP. In R-RDP, the threat function is used to determine whether a given state-action pair is safe or not. In P-PDP, a reward-seeking policy is trained while using a fixed threat function to determine the safeness of each action. The proposed method is evaluated on a benchmark dataset and two navigation tasks."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,This paper provides an empirical comparison of cross-entropy and square loss for training neural networks for classification tasks. The authors show that the square loss performs comparably or better when trained with the same hyper-parameter settings as reported in the literature. They also show that training with square loss appears to be less sensitive to the randomness in initialization. 
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes a self-supervised representation learning method for reinforcement learning. The proposed method, called Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future using an encoder which is an exponential moving average of the agent’s parameters and a learned transition model. It further improves performance by adding data augmentation to the future prediction loss, which forces the representation to be consistent across multiple views of an observation. The method achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper proposes a local node embedding method for learning embeddings for graphs. The proposed method is based on local PageRank computations. The authors provide theoretical guarantees on the locality of the computation, as well as the proof of the global consistency of the generated representations. They show empirically that their method is able to produce high-quality representations on par with state-of-the-art methods, with efficiency several orders of magnitude better in clock time and memory consumption."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"This paper proposes a method for graph coarsening based on graph neural networks. The authors propose a framework for measuring the quality of the proposed method and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graphs may be suboptimal, the authors parametrize the weight assignment map with graph neural network and train it to improve the quality in an unsupervised way. Experiments on both synthetic and real networks demonstrate that the method significantly improves the performance of the method."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper proposes a geometric deep learning algorithm to predict the scattering properties of 3D objects based on their shape. The proposed method is based on a point cloud approximation of each object, and each point is encoded in a high-dimensional latent space. The method can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce RTX 2080 Ti GPU. The authors also prove that the learning method is permutation and rotation invariant and demonstrate high accuracy on objects that are quite different from the training data."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper proposes Risk Extrapolation (REx) as a method for robust optimization over a perturbation set of extrapolated domains (MMREx), and propose a penalty on the variance of training risks (V-REx). The authors prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (“covariate shift”). By appropriately trading-off robustness of causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur."
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper introduces the Fourier neural operator, a neural network-based method for learning partial differential equations (PDEs) from data. The method is based on the idea of learning a mapping between functions in the space of functions, where the mapping is a mapping from a functional parametric dependence to the solution of the PDE. The authors propose to learn this mapping by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. The proposed method is evaluated on Burgers’ equation, Darcy flow, and Navier-Stokes equation, and is shown to be up to three orders of magnitude faster than traditional PDE solvers."
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the implicit bias of gradient flow (GD) on linear neural network training. The authors propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. They show that gradient flow on separable classification finds a stationary point of the `2/L max-margin problem in a “transformed” input space defined by the network. For underdetermined linear regression, they prove that GD finds a global minimum which minimizes a norm-like function that interpolates between weighted `1 and `2 norms in the transformed input space."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper proposes a method for optimizing slimmable neural networks by jointly learning the width-multipliers and the shared weights. The proposed method is motivated by the fact that different layers affect the network’s prediction accuracy differently and have different FLOP requirements. The authors propose a training algorithm, PareCO, which jointly learns both the channel configurations and the weights. Experiments are conducted on 15 network and dataset combinations and two cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper studies the problem of federated semi-supervised learning (FSSL) in the setting where each client may have either partially labeled or completely unlabeled data at the client-side. The authors propose a new method, FedMatch, which improves upon the naive combination of local federated learning (FL) and local semi supervised learning (SSL) approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabelled data. Experiments are conducted on CIFAR-10 and ImageNet datasets to validate the proposed method."
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,This paper proposes a self-supervised learning method for discrete event sequences. The key idea is to use contrastive learning (CL) to learn embeddings of event sequences in the low-dimensional fixed-length vector space. The paper provides a theoretical justification of the proposed method and shows that the augmentation method is sufficient to ensure that the learned representations are representative of the data. The method is evaluated on several public datasets and showed that it outperforms other methods on various downstream tasks.
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper proposes a method for unsupervised parsing of dependency and constituency grammars at the same time. The authors propose a new parsing framework that can jointly generates constituency tree and dependency graph. Then they integrate the induced dependency relations into transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. Experimental results show that the proposed method can achieve good results on the dependency parsing and masked language modeling tasks."
SP:078966ff62775bba6031e47d374bda95f4a7dde3,This paper proposes a method for learning the mapping between scene graph nodes and visual objects under weak supervision. The proposed method learns a metric among visual objects and scene graphs by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that the proposed method post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graphs parsing task verify the grounding found by our model can reinforce the performance of the existing method.
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"This paper proposes a new variant of sliced fused Gromov wasserstein (SFG) for relational regularized autoencoder (RAE). The main idea is to replace the uniform distribution over slicing direction in SFG by a von Mises-Fisher distribution that can cover the most informative area of directions. Two variants of SSFG are proposed to improve the performance and stability of SFFG: MSSFG, which uses a mixture of vMF distributions instead of a single vMF distribution to capture more informative areas of directions; and PSSSFG, which is a power spherical distribution that improves the sampling time in high-dimensional settings. Extensive experiments show that these new variants are more stable and achieve better generative performance than the previous autoencoders."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes a simple approach to speed up training for a particular kind of deep networks which contain repeated structures, such as the transformer module. In this paper, the authors first train such a deep network with the weights shared across all the repeated layers till some point, then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. The adaptive untying criterion is obtained from a theoretic analysis over deep linear networks. Empirical results show that the proposed method is able to reduce the training time of BERT by 50%."
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper studies the transferability of adversarial perturbations from the perspective of interactions based on game theory. The authors show that adversarial transferability is correlated with the interaction between perturbation units, and propose a new loss to penalize interactions during the attacking process, which improves the adversarial attack transferability. The paper is well-written and easy to follow. "
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper studies the role of higher layers in catastrophic forgetting in deep neural networks. The authors show that catastrophic forgetting is concentrated at the higher layers and erase earlier task subspaces through sequential training. They also show that mitigation methods all stabilize higher layer representations but vary on whether they enforce more feature reuse, or store tasks in orthogonal spaces. They formulate an analytic model to investigate the connections between forgetting and task semantic similarity and show that intermediate similarity in sequential tasks leads to maximal forgetting."
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes a method for efficient pre-training and fine-tuning of large language models such as BERT, XLNet and T5. The main idea is to prune the self-attention and fully-connected sub-layers inside a transformer to identify structured winning tickets in the early stage of BERT training, and apply those tickets towards efficient BERT pretraining. Experiments on GLUE and SQuAD downstream tasks show that the proposed method achieves comparable performance to standard BERT with much less training time."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper studies the robustness of f-divergences when used to train a classifier in the presence of label noise. The authors derive a nice decoupling property for a family of f -divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. They also propose fixes to make them robust."
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes an ensemble-based weighted Bellman backup method for off-policy deep reinforcement learning (RL). The proposed method re-weights the target Q-values based on uncertainty estimates from a Q-ensemble. The authors empirically show that the proposed method stabilizes and improves learning on both continuous and discrete control benchmarks. They also investigate the signal-to-noise aspect by studying environments with noisy rewards, and find that the method significantly outperforms standard Bellman backups."
SP:afc08f203562b841180811aef943bfb63a1659ea,"This paper proposes a method to predict the uncertainty of few-shot classification problems by measuring the distributional mismatch between support and query sets via class-wise similarities. The proposed method is algorithm-agnostic and readily expanded to include a range of meta-learning models. The authors conduct extensive experiments including dataset shift, and show that their training strategy helps the model avoid being indiscriminately confident and thereby, produce calibrated classification results without the loss of accuracy."
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a method for learning video-text representations for cross-model text-to-video retrieval. The authors propose to use a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of other support samples’ visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper proposes a method, seg tok, to re-build the vocabulary of Chinese BERT with the help of Chinese word segmentation (CWS) and subword tokenization. It also proposes three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that the proposed method improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency, and it can improve the downstream performance."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,This paper proposes a method for distributed training of graph convolutional neural networks (GCNs). The proposed method is based on partitioning the full-graph into sub-graphs and sampling the boundary nodes in each subgraph. The authors claim that the proposed method can reduce the memory and communication cost of GCN training while maintaining the accuracy. The method is evaluated on several benchmark datasets and shows improved performance.
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper presents a graph neural network (GNN) based model for estimating per-atom forces for quantum chemistry simulations. The model is based on graph neural networks and uses surrounding 3D molecular structure to estimate per-atomic forces. The authors show that the proposed model reduces the estimation error of atomic forces by 30% compared to existing ML models and generalizes well to out-of-distribution structures. Finally, the authors apply ForceNet to the large-scale catalyst dataset, OC20, where ForceNet is able to achieve a 4x higher success rate than other ML models."
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper investigates different regularisation methods for fine-tuning deep learning networks. The authors provide two new bounds on the generalization performance of neural networks based on the distance of the final weights from their initial values. They also show that MARS distance is a more appropriate metric in the parameter space of convolutional networks than Frobenius distance. The empirical results corroborate their theoretical investigation, demonstrating that constraining MARS distances is more effective than constraining Euclidean distance."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"This paper investigates the phenomenon of ""decoupled find-eval phenomenon"" where the hyperparameters for mask discovery (Hfind) and mask evaluation (Heval) are decoupled. The authors show that certain Hfind values lead to models which have lower performance, but generate masks with substantially higher eventual performance compared to using the same hyperparameter for both stages. They show that this phenomenon holds across a number of models, datasets, configurations, and also for one-shot structured pruning."
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper proposes a new metric (m-coherence) to study the alignment of per-example gradients during training. The metric is based on the number of examples in the sample that benefit from a small step along the gradient of any one example on average. This metric is more interpretable, cheaper to compute (O(m) instead of O(m)) and mathematically cleaner. The authors study the evolution of alignment of each example gradients in ResNet and EfficientNet models on ImageNet. "
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper considers the problem of learning sufficient statistics for likelihood-free generative models. The authors propose to learn the mutual information maximizing representations of the data with the help of deep neural networks. The learning procedure does not need to estimate any density or density ratio. They apply their approach to both traditional approximate approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks."
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper proposes an unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translate input images into the estimated domains. The main idea is to use a guiding network that provides pseudo labels and the image translation tasks. Experimental results show that the proposed model achieves comparable or even better performance than the set-level supervised model trained with full labels, generalizes well on various datasets, and is robust against the choice of hyperparameters."
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper studies the implicit bias of gradient descent training of ReLU networks for 1D regression. In particular, the authors show that the solution of training a width-n shallow ReLU network is within n-1/2 of the function which fits the training data and whose difference from initialization has smallest 2-norm of the weighted second derivative with respect to the input. The curvature penalty function is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and the authors compute it explicitly for various common initialization procedures. The solution function is the natural cubic spline interpolation of the data. "
SP:8b885142facbb3b8db41ec9d83822cee81324694,"This paper studies the problem of weight decay in deep learning. The authors firstly show that the L2 regularization is unstable weight decay for all optimizers that use momentum, such as stochastic gradient descent (SGD). Then, they propose the Stable Weight Decay (SWD) method to fix the unstable weights decay problem from a dynamical perspective. The proposed SWD method makes the weight decay rate more stable during training. The empirical results demonstrate that SWD makes significant improvements over the existing Adam variants."
SP:a3206dc71e32ba1830895bf442d3840f3331a532,This paper proposes a method to combine the strengths of both Translation Memory (TM) and Neural Machine Translation (NMT) by treating the matched sentence pair of TM as the additional signal and applying one encoder enhanced by the pre-trained language model (PLM) to encode the TM information and source sentence together. The authors extend the sentence level retrieval method to the n-gram retrieval method and explore new methods to manipulate the information flow from TM to the NMT decoder. Experiments on a mixed test set of multiple domains demonstrate that the proposed methods can significantly improve the translation quality.
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper attempts to interpret modern deep (convolutional) networks from the principles of rate reduction and (shift) invariant classification. It shows that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators (linear or nonlinear), and parameters of the network are all explicitly constructed layer-by-layer in a forward propagation fashion. All components of this “white box” network have precise optimization, statistical, and geometric interpretation. The experiments indicate that such a network can already learn a good discriminative deep representation without any back propagation training."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. The authors show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and the amount of acceleration depends on both the magnitude of that difference (which is fixed at initialization) and the spectrum of the data. "
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper proposes a method to improve the quality of LIME, a popular explanation method that aims to explain an opaque model’s behavior by training a surrogate interpretable model to be locally faithful on perturbed instances. The authors propose a theoretically sound framework based on uniform sampling of user-defined subspaces. Through logical constraints, they afford the end-user the flexibility to delineate the precise subspace of the input domain to be explained. This not only helps mitigate the problem of OOD sampling, but also allow experts to drill down and uncover bugs and biases hidden deep inside the model."
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"This paper proposes a novel pre-trained language model, called AMBERT (A Multi-grained BERT), which is an extension of BERT. The proposed model takes both the sequence of words (finegrained tokens) and sequence of phrases as input, and employs two encoders, one for processing the sequences of words and the other for processing sequences of phrases, with shared parameters, to create a sequence of contextualized representations. Experiments are conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD, and RACE. The results show the effectiveness of the proposed model."
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper proposes a new Transformer-based model for semantic parsing. The main idea is to incorporate Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS and Atis datasets."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes a method to improve adversarial robustness of deep neural networks (DNNs) against combinations of adversarial perturbations. In particular, the authors propose a new class of composite adversarial attacks that combine multiple perturbation attacks and penetrate the state-of-the-art defenses. They then propose a novel training method that improves DNNs robustness not only against individual perturbational attacks but also against their compositions. The proposed method is evaluated on several benchmark datasets and models."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"This paper proposes a method for learning abstract rules from high-dimensional sensory data. The proposed method is a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, the authors show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a generative approach for structured prediction tasks. The proposed approach is a translation task between augmented natural languages, where the task-relevant information is extracted from the augmented natural language. The approach is evaluated on several tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The results show that the proposed approach can match or outperform task-specific models on all tasks."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,This paper studies the impact of unlabeled entities on the performance of named entity recognition (NER) models and proposes a negative sampling approach that avoids training NER models with such entities. The authors claim that the reduction of annotated entities and the use of negative sampling are the main causes of performance degradation. The proposed approach is evaluated on both synthetic and real-world datasets. The results show that the proposed approach outperforms the existing methods. 
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel method for encoding speech and text in a vector space of fixed, reduced dimensions by adapting stochastic neighbor embedding (SNE) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained: an acoustic encoder that accepts speech signals in the form of frame-wise subword posterior probabilities obtained from an acoustic model and a text encoder network that accepts text in form of subword transcriptions. Experiments show that the proposed method is shown to have more effective gradients for neural network training."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper proposes a reinforcement learning algorithm for stationary mean-field games, where the goal is to learn a pair of mean- field state and stationary policy that constitutes the Nash equilibrium. The proposed algorithm is based on gradient descent and proximal policy optimization, respectively. Theoretical results show that the proposed algorithm converges to a Nash equilibrium at a sublinear rate. "
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a framework for approximate probabilistic inference on the joint distribution defined by a normalizing flow model. Specifically, it trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, it can efficiently train it using variational inference and also handle conditioning under arbitrary differentiable transformations. The resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper introduces a new Electron Microscopy (EM) dataset for cell membrane segmentation, U-RISC, the largest annotated EM dataset for the Cell membrane with multiple iterative annotations and uncompressed high-resolution raw data. The authors also propose a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of the segmentation results. Experiments on a small-scale dataset show that the new criterion is more consistent with human perception."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper proposes a new benchmark for continual learning (CL) to evaluate the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. The authors also propose a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. The learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks, and the experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper proposes a generative approach for unsupervised meta-learning. The proposed approach is based on generative models that generate pairs of in-class and out-of-class samples from the latent space in a principled way, which can be used to create synthetic classes forming the training and validation data of a meta-task. Experimental results show that the proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), outperforms or is competitive with existing methods on few-shot classification tasks on the most widely used benchmark datasets."
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies the injectivity of fully-connected and convolutional ReLU neural networks. It shows that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. The authors also characterize the stability of inverting an injective network via worst-case Lipschitz constants of the inverse. Finally, they show that an end-to-end—rather than layerwise—doubling of the dimension suffices for injectivity."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes the continuous conditional generative adversarial network (CcGAN), which is the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs are mainly designed for categorical conditions (e.g., class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems: (1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses often fails in practice; (2) Since regression labels are scalar and infinitely many, conventional label input methods are not applicable. The proposed CcGAN solves the above problems, respectively, by reformulating existing empirical losses to be appropriate for the continuous scenario and proposing a novel method to incorporate regression labels into the generator and the discriminator. Two empirical discriminator losses (HVDL and SVDL), a novel empirical generator loss and a novel label input method are proposed to overcome the two problems of existing cGANs."
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper proposes an active learning (AL) approach to semi-supervised learning (SSL) based on the convergence rate control (CRC). The main idea is to use active learning to control the rate of convergence of a classification network to the labeled set, and then use supervised learning to select the subset of unlabeled instances to query to be annotated by a human-in-the-loop (HIP) to improve the performance of the classification network. The proposed approach is evaluated on CIFAR-10 and ImageNet datasets, and compared to the existing AL and SSL approaches."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes FedDyn, a federated learning method for training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, this paper proposes a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. The authors demonstrate both through empirical results on real and synthetic data as well as analytical results that the scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper proposes a method for self-supervised learning based on contrastive learning. The main idea is to use intermediate contrastive losses as a surrogate for the final similarity between the input and output representations. The authors propose to truncate the back-propagation and update only a part of the parameters for each gradient descent update. Moreover, they do selection based on the intermediate losses to filter easy regions for each image, which further reduces the computational cost. They apply their method to recently proposed MOCO (He et al., 2020), SimCLR (Chen et al, 2020a), SwAV (Caron et al,. 2020) and show that it can reduce the cost with little loss on the performance of ImageNet linear classification and other downstream tasks."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. The authors focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, the authors consider two function approximation settings where both the agent and the critic are represented by linear or deep neural networks. For both cases, they prove that the actor sequence converges to a globally optimal policy at a sublinear O(K−1/2) rate, where K is the number of iterations."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes to represent log files at a few levels of abstraction including field level, log level, and sequence level. These representations are in vector format and serve as interfaces to downstream applications. The authors use a version of Transformer Networks (TNs) to encode numerical and textual information that is suitable for log embeddings. They show how a number of log processing applications can be readily solved with their representation."
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper proposes a method for understanding the properties of deep convolutional neural networks (CNNs). The main idea is to decompose the convolution layers into a succession of wavelet decompositions, which are modulated by freely-trained mixture weights. The authors evaluate their approach with three variants of wavelets with the AlexNet architecture for image classification as an example."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes a coach-player framework for multi-agent reinforcement learning, where the coach has a complete view of the environment and the players have a partial view, and the coach coordinates the players by distributing individual strategies. Specifically, the authors propose an attention mechanism for both the players and coach, incorporate a variational objective to regularize learning, and design an adaptive communication method to let the coach decide when to communicate with different players. The proposed method is evaluated on the resource collection task in the Multi-agent particle environment, and demonstrates zero-shot generalization to new team compositions."
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper provides an empirical study of influence functions in deep neural networks with non-convex loss functions. The authors conduct extensive experiments on a variety of datasets, including Iris, MNIST, CIFAR-10, Cifar-100, and ImageNet. They show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects on the accuracy of the influence functions. In particular, they show that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy can vary significantly depending on the examined test points."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"This paper investigates the connection between the pretraining task of next word prediction and text classification. The authors hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pre-training task. They show that language models that are -optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with O(\sqrt(log(p/s)) error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. They experimentally verify various assumptions and theoretical findings, and use insights from the analysis to design a new objective function that performs well on some classification tasks."
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper proposes a new method for membership inference attacks (MIA) on conditional image generation models. The main idea of the paper is to use the reconstruction error of the training set to discriminate between easy and hard images. The reconstruction error is used to compute a difficulty score for each image, which is then used as the membership error. The proposed method is shown to achieve high MIA accuracy on various benchmarks. "
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes Dirichlet Neural Architecture Search (DrNAS), a differentiable neural architecture search method by formulating it into a distribution learning problem. The authors treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlets, and propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of DrNAS."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper proposes a multiplicative filter network (FMN) to represent sinusoidal or Gabor wavelet functions as a linear function approximator over an exponential number of Fourier and Gabor basis functions. The authors show empirically that FMN can be used to represent low-dimensional-but-complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance functions or neural radiance fields. They also show that it outperforms or matches the performance of the baselines on these tasks."
SP:f5be855300f63c185a006834302bd4b033b56258,"This paper proposes a teacher-student scheme to enable the gradient-based meta-learning algorithms to explore long horizons by the inner loop. The key idea is to employ a student network to adequately explore the search space of task-specific models (e.g., by more than ten steps), and a teacher then takes a “leap” toward the regions probed by the student. The teacher not only arrives at a high-quality model but also defines a lightweight computation graph for meta-gradients. The approach is generic; it performs well when applied to four meta learning algorithms over three tasks: few-shot learning, long-tailed classification, and meta-attack."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,This paper proposes a new algorithm for offline reinforcement learning (RL) based on behavior regularization. The authors propose to use an analytical upper bound on KL divergence as the behavior regularizor to reduce the variance associated with sample-based estimations. They also employ state-dependent Lagrange multipliers for the regularization term to avoid distributing KL divergence penalty across all states of the sampled batch. The proposed method outperforms the state-of-the-art model-free and model-based approaches.
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,This paper introduces Adjoined networks as a training approach that can regularize and compress any CNN-based neural architecture. The one-shot learning paradigm trains both the original and the smaller networks together. The parameters of the smaller network are shared across both the architectures. The authors prove strong theoretical guarantees on the regularization behavior of the adjoint training paradigm. They complement their theoretical analysis by an extensive empirical evaluation of both the compression and regularization behaviour of adjoint networks.
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper proposes a simple extension of the greedy exploration algorithm, i.e., z-greedy, which simply repeats the sampled action for a random duration. The authors argue that the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. To address this limitation, the authors propose a simple instantiation of z-Greedy, where z is the number of sampled actions and the duration is the length of the sampled actions. Experiments on a variety of domains and RL algorithms are conducted to demonstrate the effectiveness of the proposed method. "
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper shows that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to Greedy Low-Rank Learning (GLRL) under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables them to construct counter-examples to refute the conjecture from Gunasekar et al. (2017). They also extend the results to the case where depth > 3 and show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimisation is more likely to take effect for initialization with practical scale."
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper proposes CAMEL, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. CAMEL first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate sub group features. The authors demonstrate CAMEL’s effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. "
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper proposes a method for learning interpretable non-fuzzy rules for data representation. The proposed method, called Rule-based Representation Learner (RRL), is based on the idea of learning non-differentiable rules that can automatically learn interpretable rules. RRL is trained by projecting the discrete model to a continuous space and using gradient descent to optimize the discrete parameters. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. The experimental results show that RRL enjoys both high classification performance and low model complexity."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a new regret minimization (RGM) algorithm and its extension for structured environments. RGM builds from Invariant Risk Minimization (IRM) by recasting simultaneous optimality condition in terms of predictive regret, finding a representation that enables the predictor to compete against an oracle with hindsight access to held-out environments. The structured extension adaptively highlights variation due to complex environments via specialized domain perturbations. Experiments on molecular property prediction, protein homology and stability prediction show that RGM significantly outperforms previous state-of-the-art."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes a cross-probe BERT model for text-to-text image retrieval. The proposed model is built on top of the BERT architecture, where the cross-modal attention is conducted on both the text and vision probes. The authors claim that the proposed method is much more efficient than the text-vision BERT, and achieves state-of-the-art performance on two public benchmarks."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new type of actor, named forward-looking actor or FORK, for Actor-Critic algorithms. It can be easily integrated into a model-free ActorCritic algorithm. The experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement. A variation of FORK can further solve BipedalWalkerHardcore in as few as four hours using a single GPU."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper proposes a new aggregation method for federated learning (FL) based on Bayesian inference. Specifically, the authors propose to sample higher-quality global models and combine them via Bayesian model ensemble, leading to much robust aggregation. They show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. The empirical studies validate FEDBE’s superior performance, especially when users’ data are not i.i.d. and when the neural networks go deeper."
SP:3ac5f437fc349a33810d0645664d1c448528af74,This paper proposes a method to analyze the influence patterns in BERT models. The authors show how to use multi-partite influence patterns to localize a DNN model’s handling of a concept of interest and how BERT handles subject-verb number agreement and reflexive anaphora. They also quantitatively validate the sufficieny and sparsity of influence patterns by way of compression experiments and the influence concentration of discovered patterns.
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper studies the training of deep neural networks from a control theory perspective. The authors formulate the supervised learning framework as a control problem where weights of the network are control inputs, and learning translates into a tracking problem. They propose a Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time. They also prove that the proposed method is robust to any perturbations in the input and convergence guarantees still hold true."
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper proposes to use the mutual information between each learned latent variables and the auxiliary variable to correctly identify informative latent variables. Theoretical analysis and extensive experiments on synthetic and real data show that the proposed method is a better criterion for informative latent variable selection than the previously proposed variance criterion. The authors also show that by selecting the correct features in the representation, the GIN model can perform better on the classification, outlier detection and adversarial defense tasks."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"This paper proposes a new pooling method for deep convolutional neural networks. The proposed method, called LiftPool, decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies, and then performs a corresponding up-pooling layer LiftUpPool, which is able to generate a refined upsampled feature map using the details of the down-sized feature map. Experiments show that the proposed method achieves better results on image classification and semantic segmentation tasks."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset T into binary sequences in the cube {±1}. When T consists of well-spread (i.e., non-sparse) vectors, the proposed method applies a stable noise-shaping quantization scheme to Ax where A is a sparse Gaussian random matrix. Moreover, the authors show that Euclidean distances among the elements of T are approximated by the l_1 norm on the images of the cube. The proposed method is both fast and memory efficient, with time complexity O(m) and space complexity $O(m^2)$ on well-sparsity data."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,The authors hypothesize that the plasticity rules of RNNs can be used as a proxy for gradient descent (GD) to learn the parameters of the RNN. The authors propose a genetic model that simulates the training of a plasticity rule and show that GD on the parameters can improve the performance of the network. They also show that gradient descent on the parameter of the last layer of an RNN can recover the results of the perceptron algorithm and the multiplicative weights method.
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new approach for visual question generation (VQG) task, which aims to generate visual questions with Double Hints textual answers and visual regions of interest. The authors propose a new double-hints guided Graph-to-Sequence learning framework that first models them as a dynamic graph and learns the implicit topology, and then utilize a graph to sequence model to generate the questions with double hints. The experiments on VQA2.0 and COCO-QA datasets demonstrate that the proposed model can significantly outperform existing state-of-the-art baselines."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper studies the double descent phenomenon in the context of optimal regularization. Theoretically, they prove that for certain linear regression models with isotropic data distribution, optimally-tuned regularization achieves monotonic test performance as we grow either the sample size or the model size. They also demonstrate empirically that optimally tuned regularization can mitigate double descent for more general models, including neural networks."
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"This paper proposes a spatial dependency network (SDN) to improve generative modeling by better exploiting spatial regularities and coherence in images. The proposed SDN is tailored to image generators that operate in a non-autoregressive way, i.e. synthesize all pixels ‘at once’. Spatial dependency layers improve upon convolutional layers by modeling spatial coherence and long-range spatial dependencies. SDN was shown to improve the performance of VAEs immensely."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper proposes a new offline RL algorithm based on the BCQ algorithm. The main contribution of this paper is the introduction of the Expect-Max Q-learning (EMaQ) operator, which explicitly considers the number of samples and the proposal distribution, allowing the authors to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting, EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks (Fu et al., 2020a) and in the online RL setting (Fujimoto et al, 2018a), it is competitive with Soft Actor Critic (SAC). "
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper proposes a bilevel optimization-based batch selection method for improving model fairness. The main idea is that the outer optimizer is SGD, and the inner optimizer performs adaptive batch selection to improve fairness. It is shown that the proposed method is compatible with existing batch selection techniques intended for different purposes, thus gracefully achieving multiple purposes. Experiments are conducted on both synthetic and real-world data sets. "
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the Lipschitz constants of monotone deep equilibrium (DEQ) networks, a recently proposed class of impicit-layer neural networks. The authors show that the monotonicity of the network can be controlled by the strong monotonic parameter $m$, which is a simple function of the input-output mapping and the weight output mapping. They also show how to use these bounds to develop PAC-Bayes generalization bounds that do not depend on any depth of the networks and avoid the exponential depth dependence of comparable DNN bounds."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper proposes a method for imitation learning and goal-conditioned reinforcement learning. The main idea is to use density estimation to estimate the probability of reaching a given state, and then use this estimate to train a neural network to predict the value function. The paper shows that this approach can be applied to both goal-conditional RL and imitation learning, and achieves state-of-the-art results on a common benchmark. "
SP:d57550b2f323b356d7e609acc35ee33039f376b4,This paper proposes a variational multi-task learning (VMTL) method for learning multi-tasks simultaneously. The authors cast the task relatedness as a probabilistic inference problem and introduce Gumbel-softmax priors to condition the prior of each task on related tasks. Each prior is represented as a mixture of variational posteriors of other related tasks and the mixing weights are learned in a data-driven manner for each individual task. The posteriors over representations and classifiers are inferred jointly for all tasks and individual tasks are able to improve their performance by using the shared inductive bias. Experimental results demonstrate that VMTL is able to tackle challenging tasks with limited training data well and achieves state-of-the-art performance on four benchmark datasets consistently surpassing previous methods.
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. The experimental results show that these tasks are very challenging even for long-range Transformer models."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper proposes a multilingual code summarization model that jointly learns on Context and Structure of source code. The model uses language-agnostic features, i.e., source code and features that can be computed directly from the AST. The authors show that training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Moreover, multilingual training only from Context does not lead to the same improvements."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper introduces a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. The authors demonstrate their approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Their model improves the state-of-the-art by a substantial margin."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper investigates the role of initial weights in the convergence of deep neural networks. Specifically, the authors train a convolutional neural network to predict the steps of the cellular automaton Conway’s Game of Life. They find that networks of this architecture trained on this task rarely converge. In particular, networks require substantially more parameters to consistently converge. They also find that the initialization parameters that gradient descent converges to a solution are sensitive to small perturbations, such as a single sign change."
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper proposes a semi-supervised learning (SSL) method based on consistency regularization. The proposed method considers not only the perturbed inputs but also the similarity among the inputs having the same label. The authors also introduce a new objective function, dubbed BatchMean Triplet loss, which has the advantage of computational efficiency while taking into account all input samples. The experiments show that the proposed method achieves state-of-the-art results across many standard SSL benchmarks with various labeled data amounts."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper studies the problem of meta-learning in the context of sequential learning, where the tasks are presented in a sequence and the goal is to minimize the total amount of supervision (i.e. the number of examples needed to learn a new task and the amount of data needed for metalearning). The authors propose an algorithm that can handle the variable-shot settings that naturally arise in sequential learning: from many-shot learning at the start, to zero shot learning towards the end. The proposed algorithm is based on a scaling rule for the learning rate that scales with the number-of-shot. The authors show that the proposed algorithm outperforms a strictly more expressive approach of learning individual learning rates for each number of shots."
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper proposes a series of probes to test the sensitivity of Transformer representations to syntactic structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. The authors experiment with three different perturbations: random permutations of n-grams of varying width, swapping of two spans which do or do not form a syntactic phrase, and swaps of two adjacent words. Results show that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process."
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper proposes a light-weight GAN structure for few-shot image synthesis. The authors propose a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. The model converges from scratch with just a few hours of training on a single GPU, and has a consistent performance, even with less than 100 training samples. The experiments are conducted on a wide variety of image domains."
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper proposes a novel dual algorithm for neural network bounding. The main idea is to use a specialised dual solver to improve the performance of the existing neural network bounds, which are often too loose to verify more challenging properties. In particular, the main contributions of this paper are the following:  1. The authors propose a new dual algorithm that combines the strengths of the new relaxation in the dual space: tightness and a linear separation oracle.  2. The proposed method shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time.  3. The method achieves better bounds than the existing ones in only a fraction of their running time and recovers the speed-accuracy trade-offs of looser dual solvers. "
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper proposes a method to augment pretrained language models with concept-centric commonsense knowledge. The authors propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). The authors also develop a joint pretraining framework to unify the pretraining objectives so that they can mutually reinforce each other. Extensive experimental results show that the proposed method, concept-aware language model (CALM), can pack more commonsens knowledge into the parameters of a pre-trained text-to-text transformer, yielding better performance on both NLU and NLG tasks."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper proposes a method for unsupervised physical object discovery. The proposed method is based on the idea that physics, especially object interactions, facilitates disentangling of 3D geometry and position of objects from video. The method uses both multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes and infer properties of those objects. The model reliably segments objects on both synthetic and real scenes. The discovered object properties can also be used to reason about physical events."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes a method to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. Specifically, the authors propose to increase the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The proposed method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attack, and results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data."
SP:276ffd59fbf49e3ee02756da8920218102214917,"This paper proposes a new knowledge distillation method, ProKT, which projects the supervision signals of a teacher model into the student’s parameter space by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show the proposed ProKT consistently achieves the state-of-the-art performance."
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a channel pruning method to solve the problem of compression and acceleration of Convolutional Neural Networks (CNNs). The authors propose a hyper-structure network to generate the architecture of the main network, which can be optimized by regular backpropagation. The authors also use a regularization term to specify the computational resource of the compact network. Extensive experimental results on CIFAR-10 and ImageNet show that the proposed method is competitive with state-of-the-art methods."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper proposes a method for theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. The authors propose to augment the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. They show that their theorem prover trained with this exploration mechanism but no human proofs, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs and approaches the performance of a model trained by a combination of imitation and reinforcement learning."
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes MODALS (Modality-agnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. The method is tested on text, tabular, time-series, and image data and can be readily integrated with popular deep learning models."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper provides a global convergence result for unregularized feedforward three-layer networks in the mean-field regime. The main idea is to define a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a convergence guarantee under suitable regularity and convergence mode assumptions, which does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time (not necessarily at convergence)."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes a method for learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “what if” outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these costbenefit tradeoffs associated with the expert’s actions, the authors integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making—where active experimentation is often impossible (e.g. in healthcare). The authors demonstrate the effectiveness of their approach in recovering accurate and interpretable descriptions of behavior."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper investigates the role of explicit morphological information in graph-based continous control. It ablated existing methods SMP and NERVENET, providing evidence against the belief that these methods improve performance by exploiting explicit morphology encoded in graph edges. Motivated by the findings, it presents AMORPHEUS, a transformer-based method for MTRL in incompatible environments. It obviates the need to propagate messages far away in the graph and can attend to different regions of the observations depending on the input and the particular input."
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper proposes a method for visual counting, which aims to predict the number of occurrences given a natural image and a query. Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, the authors propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, the method MoVie, short for Modulated conVolutional bottlenecks, reasons implicitly and holistically and only needs a single forward-pass during inference. It showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art  on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020VQA challenge when integrated as a module for ‘number’ related questions in generic VQa models."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,"This paper studies the problem of model-targeted poisoning attacks, in which an adversary with access to a small fraction of the training data attempts to select data in a way that induces a model that misbehaves in a particular way desired by the adversary, such as misclassifying certain inputs. The authors propose an efficient poisoning attack that can target a desired model based on online convex optimization. The attack comes with provable convergence to any achievable target classifier, and the distance from the induced classifier to the target model is inversely proportional to the square root of the number of poisoning points. The paper also provides a lower bound on the minimum number of poisoned points needed to achieve a given target model."
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper proposes a method for efficient deep learning on point clouds. The proposed method is based on the idea of aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. It introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that the proposed method outperforms existing binarization methods by convincing margins."
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper proposes and studies memory-augmented neural networks (MANNs), which extend traditional neural architectures with general-purpose memory for representations. The authors propose and study several extensions of the Transformer baseline by adding memory tokens to store non-local representations, creating memory bottleneck for the global information, and controlling memory update with dedicated layer. Experiments are conducted to evaluate the memory augmented Transformers and demonstrate that presence of memory improves the model performance for machine translation and language modeling tasks."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes an unsupervised representation learning method that bridges contrastive learning with clustering. Specifically, the authors introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. They iteratively perform E-step clustering as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive loss. They propose ProtoNCE loss, a generalized version of the InfoNCE, which encourages representations to be closer to their assigned prototypes. Experiments on multiple benchmarks demonstrate the advantage of the proposed method."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"This paper proposes an orthogonal multi-path (OMP) block to improve the robustness of neural networks against adversarial attacks. The proposed OMP block consists of a block containing multiple paths to learn robust features and the parameters of these paths are required to be orthogonality with each other. Via forward learning and backward correction, the neural networks learn features that are appropriate for all the paths and hence are expected to be robust. Experiments on both white-box and black-box attacks are conducted to verify the effectiveness of the proposed method."
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a self-supervised graph attention network (SuperGAT) for graph attention. Specifically, the authors exploit two attention forms compatible with edge prediction task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. Experiments on 17 real-world datasets demonstrates that the recipe generalizes across 15 datasets of them."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a method for solving the dialogue system for medical automatic diagnosis (DSMAD) problem, which aims to learn an agent that is able to produce reliable and convincing diagnosing processes and also is robust in making diagnosis facing noisy interaction with patients. The authors propose a novel DSMAD agent, INS-DS (Introspective Diagnosis System) comprising of two separate yet cooperative modules, i.e., an inquiry module for proposing symptom-inquiries and an introspective module for deciding when to inform a disease. They also propose two evaluation metrics to validate the reliability and robustness of DSMAD methods. Extensive experimental results demonstrate that the proposed method achieves the new state-of-the-art under various experimental settings."
SP:10ae09d90d465125433a9b4f15b1405ab017920d,"This paper proposes a batch-wise regularization method for fine-grained visual classification (FGVC) to address the long-tailed distribution of natural world distribution. The proposed method is based on the Batch Confusion Norm (BCN), which is an extension of the existing confusion energy-based framework to account for the inter-class similarity and intra-class variations. The BCN term can alleviate possible overfitting due to exploring image features of fine details. The experimental results show that the proposed method achieves state-of-the-art results on several benchmark FGVC datasets."
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,This paper proposes a method for inverse reinforcement learning (IRL) based on variational inference over the reward. The main idea is to jointly learn an approximate posterior distribution over the rewards and an appropriate policy in a completely offline manner through a variational approach to the latent reward. Experiments on real-world medical data and classic control simulations demonstrate the effectiveness of the proposed method. 
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper proposes a method for performing belief search in partially observable POMDPs. The key idea is to use an approximate auto-regressive counterfactual belief that is learned as a supervised task. The method is evaluated on Hanabi, and it is shown to outperform the state-of-the-art methods in Hanabi. It is also shown that the method can be used in multi-agent settings."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper proposes an extension of random shooting tree search (STS) that allows the user to control the bias-variance trade-off between depth and breadth of the search. The proposed method is an interpolation between two celebrated search mechanisms: MCTS and random shooting. The core improvement is multi-step expansion, which may be used to control depth of search and inject into planning more randomness. The experiments on challenging domains show that STS can get the best of both worlds consistently achieving higher scores."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,This paper proposes a method for pre-training neural networks to learn inductive bias in the form of synthetic tasks. The main idea is to design synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pretraining methodology called “LIME” (Learning Inductive bias for Mathematical rEasoning). The authors demonstrate that pretraining on these tasks (LIME) significantly improves the performances across three mathematical reasoning benchmarks.
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper analyzes the inductive bias of weight normalization (EWN) for smooth homogeneous neural nets. The authors show that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. The results can be extended to hold for gradient descent via an appropriate learning rate. Experimental results on simple data sets and architectures support the claim on sparse EWN solutions, even with SGD."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,This paper proposes a method to mitigate the mode collapse problem of GANs. The authors claim that catastrophic forgetting in the discriminator is the cause of mode collapse. They propose a training procedure that dynamically spawns additional discriminators to remember previous modes of generation. They show that the proposed method can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes an approach to regularize BERT by pruning its attention heads based on a proxy score for head importance. Instead of relying on heuristics or rule-based policies, AUBER learns a pruning policy that determines which attention heads should or should not be pruned for regularization. Experimental results show that the proposed method outperforms existing pruning methods by achieving up to 9.39% better accuracy."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,This paper proposes a framework to learn dynamics cycles that align dynamic robot behavior across two domains using a cycle-consistency constraint. The proposed method is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. The authors show the efficacy of the proposed method on multiple downstream applications in both simulation and on a real-world robot.
SP:006434d56992836ab9420d7d4215bc70664de304,"This paper studies the problem of Shapley explainability, i.e., the Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. The authors argue that the existing methods for Shapley values make an untenable assumption: that the model's features are uncorrelated. They demonstrate the drawbacks of this assumption and develop two solutions that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other one directly learns Shapley value-function, providing performance and stability."
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"This paper proposes a new opponent modelling approach, LIOM, which jointly trains a VAE-based opponent model with a decision policy for the agent under control, such that the resulting opponent model is conditioned only on the local observations of the controlled agent. LIOM is agnostic to the type of interactions in the environment (cooperative, competitive, mixed) and can model an arbitrary number of opponent policies simultaneously in the set $\mathcal{X}$. The authors show that LIOM can significantly improve the episodic return that the controlled agents achieves over a method that does not use opponent modelling."
SP:c239bc531bcf7293032748af29a1b786e9d893dd," contrastive learning has been adopted as a core method for unsupervised visual representation learning. The authors propose Consistent Contrast (CO2), which introduces a consistency regularization term into the current contrastive framework. The consistency term takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1%. It also transfers to image classification, object detection, and semantic segmentation."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies the last-iterate convergence of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) in bilinear games over the probability simplex. The authors extend the results of Daskalakis & Panageas (2019a) to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last iterate convergence rates with a constant learning rate whose value only depends on the smoothness of the objective function. The condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019)."
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes a framework for training user verification models in federated setup, where the constraints that each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. In FedUV, users jointly learn a set of vectors and maximize the correlation of their instance embedding with a secret user-defined linear combination of those vectors. The authors show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors. They present the experimental results for user verification with voice, face, and handwriting data."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper studies the geometry of the class manifolds (CMs) of deep neural network classifiers. The authors propose a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. They provide a theory for the technique and verify that their theoretical predictions agree with measurements on real neural networks. They show that well-performing, robust models have higher dimensional CMs than worse performing models. Moreover, they offer a unique perspective on ensembling via intersections of CM."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper proposes a method to improve the performance of the Soft Actor-Critic (SAC) algorithm by introducing a curiosity-aware entropy temperature (CAT-SAC). The proposed method is based on the idea that the agent should explore more in an unfamiliar state, while less in a familiar state, so as to understand the environment more efficiently. In particular, the proposed method uses the state prediction error to model curiosity and adds it to the target entropy to increase the entropy temperature for unfamiliar states and decrease it for familiar states. Experimental results on the MuJoCo benchmark show the effectiveness of the proposed approach. "
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. The proposed method is based on a simple insight that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. The algorithm can adapt the model first, relabeling all data from the meta-training tasks with this model, and then fine-tuning on that data using a standard off-Policy RL method. The authors show that combining these components leads to good results across a range of challenging meta-RL problems."
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper proposes a gradient-based meta-learning algorithm Eigen-Reptile, which updates the meta-parameters with the main direction of historical taskspecific parameters to alleviate gradient noise. The main direction is computed by a special mechanism for the parameter’s large size. Furthermore, the paper proposes Introspective Self-paced Learning (ISPL) that constructs a plurality of prior models to determine which sample should be abandoned. Theoretical and empirical results show that the proposed methods outperform or achieve highly competitive performance compared with the state-of-the-art methods."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper proposes a method to improve the robustness of image classification models to distributional shifts. The authors propose to perturb the mean and variance of deep image features during adversarial training by adversarially perturbing these feature statistics, rather than image pixels. The proposed method, Adversarial Batch Normalization (AdvBN), significantly improves the performance of ResNet-50 on ImageNet-C, Stylized-ImageNet, and ImageNetInstagram over standard training practices. In addition, AdvBN can improve generalization on semantic segmentation."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,This paper proposes Variance of Gradients (VoG) as a proxy metric for detecting outliers in the data distribution. The authors provide quantitative and qualitative support that VoG is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Data points with high VoG scores are far more difficult for the model to learn and over-index on corrupted or memorized examples.
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"This paper proposes a new technique to improve the quality of generated samples from deep generative models by using gradient flow of entropy-regularized f-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, this technique avoids wasteful sample rejection used by previous methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN variants, this paper shows that the proposed approach can be applied to GANs with vector-valued critics, VAEs and Normalizing Flows. Experimental results on multiple synthetic, image, and text datasets demonstrate that DGf low leads to significant improvement in the quality."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents a variable encoder-decoder (VECO) pre-training approach to unify the two mainstreams in both model architectures and pre- training tasks. VECO splits the standard Transformer block into several sub-modules trained with both inner-sequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub- modules for understanding and generation tasks during inference. Such a workflow ensures to train the most streamlined parameters necessary for two kinds of tasks, but also enables them to boost each other via sharing common sub-Module. The proposed method achieves new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval. For generation tasks, the proposed method also outperforms all existing Cross-Lingual models and state of the art Transformer variants on WMT14 English-to-German and English- to-French translation datasets."
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper proposes an intrinsic reward for reinforcement learning (RL) based on the prediction of auditory event clusters. The authors propose to use a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. The proposed method is evaluated on a set of Atari games, audio-visual exploration using the Habitat simulator and active learning using the ThreeDWorld (TDW) simulator."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper studies the problem of novel category discovery on single and multimodal data with labels from different but relevant categories. The authors present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, the authors take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabeled data. In particular, they propose using category discrimination on labelled data and cross-modal discrimination on multi- modal data to augment instance discrimination used in conventional contrastive learning approaches. They further employ Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabeling data to better predict cluster assignments. The proposed method is evaluated on large-scale multi-scale video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, CifAR100 and ImageNet."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"This paper proposes a method for weakly supervised segmentation, which is a semi-supervised metric learning problem, where pixels of the same semantics need to be mapped to the same (distinctive) features. The authors propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, feature affinity, and feature affinity. The proposed method can learn from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images can participate not only in data driven grouping within each image, but also in discriminative feature learning within and across images. "
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper proposes a new self-supervised distillation method, named BINGO, which bags related instances by matching embeddings of the teacher. The bag of instances indicates a set of similar samples constructed by the teacher and are grouped within a bag, and the goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. The proposed method achieves new state-of-the-art performance on small scale models, i.e., 65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet."
SP:328866aad6544c81ded8980934df31dc4472435f,"This paper proposes a generative adversarial approach to Simulation-based inference (SBI) based on variational variational inference (VVI) and adversarial training. The main idea is to reformulate the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. The proposed method is evaluated on two SBI benchmark problems and on two high dimensional simulators."
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,This paper proposes a generative prognostic model for the identification and estimation of treatment effects (TEs) under limited overlap. The model is learned as a new type of variational autoencoder (VAE). The authors derive the TE error bounds that enable representations balanced for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using (semi-)synthetic datasets.
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a framework for autonomous reinforcement learning (ARL) where the agent is not only learning through its own experience, but also contends with lack of human supervision to reset between trials. They introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. They show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"This paper investigates the state-of-the-art GNN-based QA systems for question answering (QA) by analyzing their reasoning capability. The authors claim that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. They also claim that GNN essentially works as a counter in the QA reasoning process and design soft/hard counter models, which achieve comparable or better experimental results than existing GNN based QA methods."
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a three-stage framework to enable DNN inference with near-optimal compression and much better performance during inference runtime. The key insight of this paper leverages the concept of Succinct Data Structures, which supports fast queries directly on compressed representation without decompression. The method first transforms DNN models as their proposed formulations in either Element-wise or Block-wise manner. Then, the method compresses the transformed DNN model using the succinct data structures. Finally, this method exploits the specialized execution pipelines for different model formulations, to retrieve relevant data for inference. The experimental results show that, our method keeps near optimal compression and achieves at least 8.7X/11.5X speedup on AlexNet/VGG-16 inference, compared with Huffman Coding."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes a new training method for improving the performance of tiny neural networks. The authors argue that training tiny models is different from training large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a generative adversarial network (GAN) for dynamic point cloud sequences without requiring point correspondence annotations. The proposed model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, the authors propose a learnable masking module to adapt upsampling ratio according to the point distribution. Extensive experiments on two different domains: particles in the fluid dynamical system and human action scanned data demonstrate the effectiveness of the proposed method."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,This paper proposes a novel method for fully pre-training an encoder-only transformer and fine-tuning it for object detection via a task adapter. The proposed method is inspired by the success of textual prompts in NLP and treats query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. Experiments on the challenging COCO dataset demonstrate that our PT-DETR achieves competitive performance.
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper proposes a new federated learning algorithm, FedPAGE, which further reduces the communication complexity by utilizing the recent optimal optimal PAGE method (Li et al., 2021). Theoretical results are provided for both convex and non-convex settings. The authors also conduct several numerical experiments showing the effectiveness of multiple local update steps in FedPAAGE and verifying the practical superiority."
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper studies the decision boundary geometry of ANN classifiers by utilizing adversarial perturbations. The authors define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundaries from any given input sample. The distance to the boundary grows smoothly and sub-linearly as one increases the dimensionality of the subspace. The boundary curvature is characterized to characterize the geometry of the boundary, which is more curved within the adversarial space than within a random subspace of equal dimensionality."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"This paper proposes a two-stage weakly-supervised contrastive learning approach. The first stage clusters data according to its auxiliary information. The second stage learns similar representations within the same cluster and dissimilar representations for data from different clusters. The authors claim that the auxiliary-information-infused representations bring the performance closer to supervised representations, which use direct downstream labels as supervision signals."
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper studies the problem of recovering sparse parameters from observational data. The authors propose an algorithm called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm) to learn algorithms automatically from data. PLISA is designed by unrolling a path-following algorithm, with some components being more flexible and learnable. With this structure, the authors theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, they analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a method for learning representations for discrete-continuous hybrid action spaces. The main idea is to learn a compact and decodable latent representation space for the original hybrid action space. The latent space is constructed by embedding the dependence between discrete action and continuous parameter via an embedding table and conditional Variational Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space via a decoder. The experiments demonstrate the superiority of the proposed method."
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper proposes SGEM, Stochastic Gradient with Energy and Momentum to solve a large class of general nonconvex stochastic optimization problems, based on the AEGD method that originated in the work [AEGD: Adaptive Gradient Descent with Energy. arXiv: 2010.05109]. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. The authors show that SGEM features an unconditional energy stability property, and derive energy-dependent convergence rates in the general nonconsvex setting, as well as a regret bound in the online convex setting. A lower threshold for the energy variable is also provided."
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a Conditional Masked Language Model with Correction (CMLMC) to improve the performance of non-autoregressive (NAR) machine translation models. The authors claim that the performance gap between NAR and autoregressive models is due to the indistinguishability of tokens, and mismatch between training and inference. They propose a dual strategy of revealing positional information and adding error correction mechanism, which improves NAR translation performance on both raw and distilled datasets."
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper proposes a spiking neural network (SNN) for the task of keyword-spotting. The proposed method is inspired by the WaveNet architecture, which uses neural dynamics, fixed time-constants and a simple feed-forward architecture, and hence is particularly well suited for a neuromorphic implementation. The authors test the capabilities of this model on several datasets for keyword-Spotting, and show that the proposed network beats the state-of-the-art of other SNNs and achieves near state of the art performance. "
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper studies the problem of demographic shift in machine learning, i.e., when the distribution of subgroups of the population becomes more or less probable in deployment (a phenomenon referred to as demographic shift), prior work’s fairness assurances are often invalid. The authors propose a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. They evaluate Shifty using a real-world dataset of university entrance exams and subsequent student success. They show that the learned models avoid bias in demographic shift, unlike existing methods."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes to extend the standard stochastic dual dynamic programming (SDDP) method by introducing a trainable neural model that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming (ν-SDDP), continually self-improves by solving successive problems. An empirical investigation demonstrates that the proposed method can significantly reduce problem solving cost without sacrificing solution quality in high-dimensional and long-horizon problems."
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a protocol for privacy-preserving next-token prediction for language models that were fine-tuned on a private corpus after pre-training on a public corpus. The authors show that the proposed method limits the leakage of information that is unique to any individual user in the private corpus via a relaxation of group differentially private prediction. They also propose a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing data-extraction attacks while maintaining the utility of the language model."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate with respect to a classification model’s intermediate activations on indistribution samples. The authors leverage a recent insight about label smoothing, which they call the Label Smoothed Embedding Hypothesis, and show that one of the implications is that the proposed method performs better as an OOD detection method both theoretically and empirically when the model is trained with label smoothed. Finally, they show that their proposal outperforms many OOD baselines and provide new finite-sample high-probability statistical results."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,This paper proposes a new method for representation learning based on diffusion-based methods. The main idea is to use a new formulation of the denoising score matching objective that encodes information needed for denoizing autoencoders. The paper also proposes to learn an infinite-dimensional latent code that achieves improvements of state-of-the-art on semi-supervised image classification.
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"This paper proposes a method for goal-conditioned reinforcement learning, where the goal is to learn a policy that can reach distant goals. The proposed method, C-Planning, is based on the idea of expectation maximization, which is to maximize the expected return of the learned policy to reach the goal. The main contribution of this paper is that it proposes to learn the expectation of the policy to visit a sequence of intermediate states (called waypoints) using graph planning, and then use the learned waypoints to train the policy. The method is evaluated on a range of navigation and manipulation tasks, where it is shown to outperform prior methods."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper extends mixup to k-mixup, i.e. perturbing k-batches of training points in the direction of other k-bits using displacement interpolation. The authors demonstrate theoretically and in simulations that k-Mixup preserves cluster and manifold structures, and they extend theory studying the efficacy of standard mixup training to the k+mixup case. The empirical results show that training with k- mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,"This paper proposes a nonlinear kernelized classification layer for deep networks to tackle the problem of finding nonlinear patterns in the embedding vector space. The classification layer classifies embeddings in a high dimensional RKHS while automatically learning the optimal kernel that enables this high-dimensional mapping. The authors show that a classification network with a lightweight representation learning backbone can be made more effective by replacing the usual softmax classifier with the kernelized classifier. The proposed method is evaluated on image classification, natural language understanding, distillation, and active learning settings."
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"This paper analyzes the sources of bias in node representations obtained via Graph Neural Networks (GNNs) and shows that both nodal features and graph structure lead to bias in the obtained representations. Based on the analysis, several fairness-aware data augmentation frameworks are developed to reduce the intrinsic bias. Extensive experiments on node classification and link prediction are carried out over real networks in the context of graph contrastive learning. The proposed strategies can improve fairness in terms of statistical parity and equal opportunity."
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating treatment effects from observational data in the presence of unmeasured confounders. In particular, the authors propose a Confounder Balanced IV Regression (CB-IV) algorithm to jointly remove the bias from the unmeasureable confoundsers with IV regression and achieve better bias-variance trade-off in imbalanced treatment distributions due to the observed confounder by balancing for treatment effect estimation. Specifically, CB-IV consists of three main modules: (1) treatment regression: regressing the treatment with IVs and confounds in the first stage; (2) confounding: learning a balanced representation of confounds to eliminate the bias induced by the observations; and (3) outcome regression: learning the outcome with the predicted treatment and the balanced representation for treatment effects estimation. Theoretical analysis is provided to show the effectiveness of the proposed method."
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper analyzes MAML in the setting of linear regression with a mixture of easy and hard tasks, where hardness is related to the rate that gradient descent converges on the task. The authors show that in order to achieve substantial gain over NAL, there must be some discrepancy in hardness among the tasks, and the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions. They also give numerical and analytical results suggesting that these insights apply to two-layer neural networks. Finally, they provide few-shot image classification experiments that support their insights for when MAMLU should be used and emphasize the importance of training MAMUL on hard tasks in practice."
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes an unrolled version of the Proximal Alternating Linearized Minimization (PALM) algorithm for sparse blind source separation (BSS). The main idea is to leverage the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameters and variables. The proposed LPALM algorithm enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. The authors demonstrate the algorithm's effectiveness in astrophysical multispectral imaging."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper introduces a Legendre Memory Unit-based model for language modeling, which is based on transformers and LSTMs. The authors introduce a novel attention module called implicit self-attention and construct a model that exhibits an O(n) and O(\n lnn) dependency for memory and computation respectively. They show that for the same amount of training, the proposed model improves the loss over transformers about as much as transformers improve over LSTM. "
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"This paper introduces LatentKeypointGAN, a two-stage GAN that is trained end-to-end on the classical GAN objective with internal conditioning on a set of space keypoints. These keypoints have associated appearance embeddings that respectively control the position and style of the generated objects and their parts. The paper demonstrates that the latent space provides an interpretable latent space that can be used to re-arrange the generated images by re-positioning and exchanging keypoint embedding, such as generating portraits by combining the eyes, and mouth from different images. In addition, the explicit generation of keypoints and matching images enables a new GAN-based method for unsupervised keypoint detection."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper studies the effect of increasing the depth of fully-connected neural networks with layer normalization on signal propagation. The authors show that increasing depth leads to gradient explosion or representation shrinkage. They also show that the appearance of at least one of these problems is not restricted to a specific initialization scheme or a choice of activation function, but rather is an inherent property of the architecture itself. Additionally, they show that many popular normalization techniques fail to mitigate these problems."
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper proposes a line search method to estimate the full-batch loss of SGD with a piece-wise constant learning rate schedule. The proposed method is based on the observation that the full batch loss behaves locally parabolically in the direction of the noisy update step directions, and the trend of the optimal update step size changes slowly. The learning rates are derived from the learning rates of the parabolas during training. Experiments show that the proposed method mostly outperforms SGD tuned with constant learning rates and other line search approaches for Deep Learning across models, datasets, and batch sizes on validation and test accuracy."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper provides a theoretical analysis of the difficulties that arise when optimizing the NCE objective with an uninformative noise distribution, stemming from an ill-behaved loss landscape. The authors show that even on the simple task of Gaussian mean estimation, even assuming access to the population gradient, gradient descent and Newton’s method with standard step size choice still require an exponential number of steps to reach a good solution. They then propose a new loss called eNCE, which can be efficiently optimized using normalized gradient descent, and empirically outperforms existing methods."
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper studies the problem of how to combine DP and Byzantine resilience in distributed SGD. The authors show that the integration of standard practices in DP and BR is not straightforward and that many existing results on the convergence of distributed SGd under Byzantine faults, especially those relying on (alpha, f)-Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, the authors revisit the theory of Byzantine resilience to obtain an approximate convergence guarantee. The analysis provides key insights on how to improve this guarantee through hyperparameter optimization."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper considers the problem of code editing with few exemplars, i.e., the editing exemplar contains the original and modified support code snippets, showcases a certain editorial pattern, and code editing adapts the common pattern derived from few support exemplars to a query code snippet. The authors propose a novel deep learning approach to solve this code editing problem automatically. Specifically, they parse the support and query code snippets using language-specific grammar into abstract syntax trees. They apply the similarities measurement in multiple extents from individual nodes to collective tree representations for query and support sample matching, and ensemble the matching results through a similarity-ranking error estimator. They evaluate the proposed method on C# and Python datasets, and show up to 8.6% absolute accuracy improvements compared to non-composition baselines."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a generative model for generating sequences with high-level structure in the form of relational constraints between different subcomponents of an example (e.g., lines of a poem or measures of music). The authors propose a program synthesis algorithm that infers the relational constraints present in the training data, and then learns to generate generative models based on the resulting constraint data. The experiments show that the proposed approach significantly improves over state-of-the-art in terms of capturing high level structure."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper studies the problem of set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. The authors address two common scaling problems encountered in set to hypergraph tasks that limit the size of the input set: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. They propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. They introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Finally, they combine both contributions in a single set tohypergraph model that enables them to address problems with larger input set sizes. They provide ablations for their model and show that their model outperforms prior state-of-the-art, especially for larger sets."
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a post-processing method to mitigate bias of state-of-the-art models. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre-trained model to give more representation power to the discriminated subgroups. Its training is supervised by the von Mises-Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper proposes a method to improve the performance of continual learning (CIL) methods by selecting high-quality (placebo) data from a free image stream (e.g., Google Images) and using them to improve KD between the models learned in adjacent phases, while not harming the learning of new classes in the new model. The authors also propose an RL algorithm to make the selection of placebos more adaptive in different phases. Extensive experiments on multiple baselines show that the method is general and efficient."
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes a path auxiliary algorithm for sampling from discrete energy-based models (EBMs). The proposed algorithm is based on MCMC with a composition of local moves to efficiently explore large neighborhoods. The authors also give a fast version of the algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirical results show that the proposed algorithm outperforms other generic samplers on various discrete models for sampling, inference, and learning."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper presents Variational Predictive Routing (VPR), a hierarchical generative model that is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future. The proposed approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,This paper proposes a new method for image retrieval that combines global and local features. The proposed method first learns homography transformation with CNN and then replaces the re-ranking process with information fusion to obtain more powerful features and overcome the low efficiency of local features in storage and matching. The authors also combine spatial and channel attention to improve accuracy and semantics in local attention. Experiments on the Revisited Oxford and Paris datasets validate the effectiveness of the approach.
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper introduces RotoGrad, an algorithm that homogenizes gradient magnitudes and directions for multi-task learning. The main idea is to homogenize task gradients in terms of magnitude and directions to avoid conflicting gradients. The proposed method is evaluated on the CelebA and NYUv2 datasets. "
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"This paper proposes a method to fuse heterogeneous neural networks by solving a cross-layer alignment problem, followed by a layer balancing step. The proposed method, called CLAFusion, is applied in the setting of heterogeneous data as a skill transfer method. Experiments are conducted on CIFAR-10 and ImageNet datasets. "
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper studies the effect of implicit regularization in deep reinforcement learning (RL). The authors show that the implicit regularizer is harmful in the offline RL setting, leading to poor generalization and degenerate feature representations. To address this issue, the authors propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularized regularizer. Experiments on Atari 2600 games, D4RL domains and robotic manipulation from images demonstrate the effectiveness of the proposed DR3."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes HyperDQN, a variant of DQN with a probabilistic hyper-parameterized base model and a meta-model that outputs the parameters of the base model. The hyper-model is used to generate approximate posterior samples of the Q-value function, which are then used to select action sequences for exploration. The proposed method is evaluated on the Atari suite and SuperMarioBros."
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper proposes to learn causal representation from observational data by regularizing the learning procedure with mutual information measures according to our hypothetical causal graph. The optimization involves a counterfactual loss, based on which the authors deduce a theoretical guarantee that the causality-inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by the approach is robust under adversarial attacks and distribution shift."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,This paper proposes a progressive training framework for efficient and effective federated learning. The proposed ProgFed reduces computation and two-way communication costs while maintaining the strong performance of the final models. Theoretical results are provided to show the convergence rate of the proposed method. Extensive experiments are conducted to demonstrate the effectiveness of the method. 
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,This paper studies the generalization of adversarial training through the lens of the adversarial Rademacher complexity of deep neural networks. The authors provide an upper bound for the adversarially trained weight norms and show that the product of weight norms is a key factor explaining why adversarial learning cannot generalize well. Experiments on CIFAR-10 and ImageNet show the proposed upper bounds. 
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a kernel-based differentiable estimator for differential entropy and mutual information. The proposed estimator is a general-purpose estimator and can be incorporated as part of any training objective, where differential entropy or mutual information estimation is desired. Experiments on visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of the proposed method."
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"This paper proposes a new soft-greedy operator, called resmax, which takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space like softmax, but focuses exploration more on potentially promising actions. The authors prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a nonexpansion (called mellowmax). The authors empirically validate that resmax is comparable to or outperforms softmax and \epsilon in both tabular and deep RL."
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper proposes a method to control the learnability of a specific dataset with a special key. The proposed method is based on adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become “unlearnable” by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnable of the dataset and train models normally using the corresponding key."
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper proposes a method for recovering missing node features in graph neural networks. The proposed method is based on minimizing the energy of the Dirichlet energy, which leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which is called Feature Propagation (FP). The proposed approach outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features."
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"This paper considers the problem of selecting a core subset of unlabeled data for active learning, i.e. selecting a subset of data that minimizes the discrete Wasserstein distance between the labeled data and the unlabeled data. The paper proposes a Generalized Benders Decomposition (GB) algorithm to solve the integer optimization problem for selecting the core set. The core set is defined as the set of data points that minimize the distance to the labeled set, and the goal is to find a core set that maximizes this distance. The authors show that this problem can be solved using a generalized Bender's decomposition algorithm. The proposed approach is evaluated on several benchmark datasets, and it is shown to outperform existing active learning approaches."
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a method for de novo genome assembly based on graph neural networks and finding a path through the assembly graph. A graph convolutional network is trained on a dataset generated from human genomic data (CHM13) to reconstruct the genome by finding the path through assembly graphs. The authors show that their model can compute scores from the lengths of the overlaps between the sequences and the graph topology which, when traversed with a greedy search algorithm, outperforms the greedy search over the overlap lengths only. Moreover, the method reconstructs the correct path in the fraction of time required for the state-of-the-art genome assemblers."
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a method to improve the representation learning in meta-continual learning by incorporating experience replay (ER) into meta-testing and meta-training. The authors propose to store the samples’ representations, instead of the samples themselves, into the replay buffer to ensure the batch nature of ER does not conflict with the online-aware nature of meta-learning. They also introduce a meta-learned Predictive Sample Selection to replace the widely used reservoir sampling to populate the replay buffers. The proposed method outperforms the state-of-the-art on a number of real-world continual learning benchmark data sets."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper studies the multi-agent joint Q-learning based on centralized training with decentralized execution (CTDE). The authors formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q-values to explicitly maximize the joint Q value. Theoretically, the authors give a gradient ascent solution for this problem. Empirically, they instantiate the core idea with deep neural networks and propose Explicit Credit Assignment Joint Q-Learning (ECAQ). Extensive experiments justify the superior performance of ECAQ."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper studies adversarial attacks against transductive learning-based adversarial defenses. The authors propose a new attack framework called Greedy Model Space Attack (GMSA) that can be used as a new baseline for evaluating adversarial robustness. GMSA is based on the principle of attacking model space for solving bilevel attack objectives, and the authors show that GMSA, even with weak instantiations, can break the previous Transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack. On the positive side, the authors report a somewhat surprising empirical result of “transductive adversarial training”: Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks."
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies batch normalization as an approximation of the limiting case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. The authors demonstrate an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation, as well as a fully per-example training procedure, which removes the extra computation at a cost of accuracy drop. They further use their insights to improve batch renormalization for very small minibatches."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper proposes a low-rank adaptation (LoRA) strategy to reduce the number of trainable parameters for downstream tasks. LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture to inject rank-deficiency matrices. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the total number of parameters by 10,000 times and the GPU memory requirement by 3 times. Experiments on RoBERTa and DeBERTa show that LoRA performs on-par or better than finetuning in model quality."
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a regular-constrained linear-chain conditional random field (RegCCRF) that can enforce a broad class of constraints, including non-local ones, by specifying the space of possible output structures as a regular language L. RegCCRF has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. The authors prove that constrained training is never worse than constrained decoding, and show empirically that it can substantially better in practice. Finally, the authors demonstrate a practical benefit on downstream tasks by incorporating a RegccRF into a deep neural model for semantic role labeling."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"This paper proposes two neural networks for camera-based physiological measurement, EfficientPhys, that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. The proposed models achieve state-of-the-art accuracy on three public datasets. The authors also evaluate the latency of the proposed networks and show that the most light-weight network achieves a 33% improvement in efficiency."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper proposes a new method for hardware-aware latency pruning (HALP) that focuses on structured pruning for underlying hardware towards latency budgets. The authors formulate structural pruning as a resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined latency budget. The latency lookup table is used to track latency reduction potential and global saliency score to gauge accuracy drop. This makes the problem solvable via an augmented knapsack solver enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes a method for molecular graph generation based on energy-based models (EBMs). The authors propose to learn the energy function by contrastive divergence and generate samples by Langevin dynamics. To generate molecules with a specific desirable property, the authors propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. The authors also explore to use the proposed method for generating molecules towards multiple objectives via compositional generation, which is practically desired in drug discovery."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"This paper proposes a neural-network-based approach for bottom-up program synthesis. The approach, called CROSSBEAM, uses a neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. The model is trained on-policy using data extracted from its own bottom up searches on training tasks. The proposed approach is evaluated on string manipulation and logic programming tasks, and it is shown that it learns to search efficiently."
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper proposes a method to improve the stability of deep reinforcement learning (DQN) methods by replacing the standard squared Bellman error with the FR Squared Bellman Error (FR). The main idea is to decouple the lagging set of parameters from the target Q-values, which allows the method to use up-to-date parameters and control the regularization. The method is evaluated on a range of Atari environments and shows improved performance over target-network based methods."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,This paper proposes a new method to improve the expressive power of message passing graph neural networks (GNNs) by injecting structural properties of graphs into the message-passing aggregation scheme of GNNs. The authors develop a new hierarchy of local isomorphism on neighborhood subgraphs and prove that the proposed method is strictly more expressive than the Weisfeiler Lehman test (1-WL) in distinguishing graph structures. Experiments on node classification and graph classification tasks demonstrate the effectiveness of the proposed approach.
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a new method for uncertainty quantification, named PI3NN, which is based on linear combinations of three neural networks, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. The authors theoretically prove that the proposed method avoids the crossing issue and does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, the authors address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes a meta-learning method for online continual learning, where the goal is to learn a model that can adapt to changing tasks and input distributions. The proposed method is based on MAML and is able to adapt to new tasks in a fully online fashion, where it does not need to know the task boundaries and does not require resetting the parameters of the model back to the meta-learned parameters for each task. The method is evaluated on two datasets, Rainbow-MNIST and CIFAR-100, and shows comparable or better performance compared to baselines."
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,This paper proposes a differentiable scaffolding tree (DST) to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). The authors claim that the proposed method is both effective and sample efficient.
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"This paper proposes a personalized lab test result prediction approach that learns a strong patient representation incorporating both patient information accumulated over the visits as well as information from other similar patients. The representation also captures fine-grained dosage information enabling us to adjust the prediction in response to changes in treatment regime, which is often needed in the management of chronic patient care. Experiments on two real-world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors by a significant margin."
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper proposes a new single-domain generalization (SDG) task, i.e., open-set single domain generalization, which aims to generalize models from a single source domain to unseen target domains and simultaneously deals with unknown classes. The authors propose a novel CrossMatch approach to tackle the OS-SDG task, which generates auxiliary samples for unknown classes and improves the capability of unknown classes identification with a novel consistency regularization. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of SDG methods in the new setting."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper proposes two extensions of TRPO and PPO, namely Wasserstein Policy Optimization (WPO) and Sinkhorn Policy Optimisation (SPO). WPO and SPO directly optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, the authors show that WPO guarantees a monotonic performance improvement, while SPO provably converges to WPO as the entropic regularizer diminishes. Experiments on tabular domains and robotic locomotion tasks demonstrate the performance improvement of both approaches."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper proposes a new paradigm of forgetting-and-recovery, where the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The proposed framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. The authors leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper investigates the offline-online batch RL setting, where the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. The authors show that standard RL agents trained in an offline-Online manner can outperform agents trained only offline or online, sometimes by a large margin."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper studies the problem of domain generalization (DG) via learning to reduce domain shift with an episodic training procedure. In particular, the authors propose a meta-learning algorithm that learns to optimize Y-discrepancy between the unseen target domain and source domains only using source-domain samples. Theoretically, they give a PAC-style generalization bound for discrepancy-optimal meta learning and further make comparisons with other DG bounds including ERM and domain-invariant learning. The theoretical analyses show that there is a tradeoff between classification performance and computational complexity for discrepancy optimal meta learning. They also shed light on a bilevel optimization algorithm for DG. Empirically, the algorithm achieves state-of-the-art results on two DG benchmarks."
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the interplay of the policy and value networks in DNN-based best-first search on the Sokoban domain and shows the surprising effectiveness of policy network, further enhanced by the value network, as a guiding heuristic for the search. The authors show the existence of left heavy tails and propose an abstract tree model that can empirically explain the appearance of these tails. The experiments show the critical role of a policy network as a powerful heuristic guiding the search, which can lead to left heavy tail with polynomial scaling by avoiding exploring exponentially sized sub-trees. The results also demonstrate the importance of random restart strategies, as are widely used in traditional combinatorial solvers, for DNN based search to avoid left and right heavy tails."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,This paper proposes a method for meta-imitation learning from videos of human demonstrations. The main idea is to use a generative model (A-CycleGAN) to translate human videos into robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data. The proposed method is evaluated on a set of vision-based tasks and shows comparable performance to the baseline.
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper proposes a simple and powerful method of modifying deep net training with a base optimizer to improve weight adaptivity and lead to improved generalization. The main idea is to switch from free to weight norm constrained training at an appropriate point to improve the adaptivity of the network. The proposed method is evaluated on a variety of tasks, optimizers and batch sizes. "
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper proposes a method for learning group equivariant convolutional neural networks (G-CNNs) that is able to learn both partial and full equivariance at each layer of the network. In particular, the authors propose to learn different levels of group equivariances at every layer, i.e., partial equivariances for low-level features and full ones for high-level ones. The proposed method is evaluated on the MNIST and CIFAR-10 datasets, and compared to G-CNN. The results show that the proposed method performs on par or slightly better than the baseline."
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes a method for efficient inference in deep latent variable models. The proposed method is based on the amortized Langevin dynamics (ALD), which replaces the traditional datapoint-wise MCMC iterations with updates of an inference model that maps observations into latent variables, which enables scalable inference from large-scale datasets. The authors prove that ALD has the target posterior as a stationary distribution under some assumptions, and extend ALD to sampling from an unconditional distribution such as an energy-based model, enabling more flexible generative modeling by applying it to the prior distribution of the latent variable. Based on ALD, the authors construct a new deep latent variables model named the Langevin autoencoder (LAE). LAE uses ALD for autoencoders-like posterior inference and sampling from the latent space EBM."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper proposes a framework for efficient training and inference of hypergraph reasoning networks. It uses sparse tensors to represent high-order relationships (hyperedges), and implements finited-domain quantification to infer new knowledge. A sparsification loss is introduced to regularize the number of hyperedges in intermediate layers of a SpaLoc model. To enable training on large-scale graphs such as real-world knowledge graphs, SpaLoc makes training- and inference-time sub-sampling of the input graphs. To remedy the information loss in sampled sub-graphs, it proposes a novel sampling and label calibration paradigm based on an information-theoretic measure information sufficiency."
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper proposes a new family of differentiable top-k cross-entropy classification losses, where k is a positive integer, such as 1 or 5, and k is assumed to be a probability distribution. The authors show that relaxing k leads to better top-5 accuracy and also makes models more robust, which leads to top-1 accuracy improvements. They also demonstrate the impact of relaxing k by fine-tuning publicly available ImageNet models with the proposed loss."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a new method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. The proposed method is built on the Douglas-Rachford splitting (DRF) technique and tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel."
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,This paper proposes a framework for disentangling performance gaps in federated learning (FL) based on out-of-sample (OoU) and participation gaps (P2P). The authors propose a semantic synthesis strategy for realistic simulation of FL without naturally-partitioned data. The proposed method is evaluated on synthetic and real-world datasets. 
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,This paper studies the few-shot learning methods on pretrained language models from the BERT family in the zero-shot setting. They find that simply introducing a few prompt [MASK]s can significantly improve the performance and robustness of the null prompt method and even exceed cherry-piked manual prompts. They also find that simple Multi-null Prompting strategy can yield very promising results on a few widely-used datasets.
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"This paper proposes a method to improve the performance of the attention mechanism. The proposed method is based on the idea that the attention module should be able to align the relevant parts of the encoded image with the target output. To achieve this, the authors propose to add a sharpener module to the current attention module. The sharpener is used to find the target in an image region and refines the representation to be target-specific. Experiments on both synthetic and real-world text recognition datasets show the effectiveness of the proposed method."
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper proposes a deep reinforcement learning-based approach to solve the vehicle routing problem (VRP), which is a combinatorial optimization problem with a fixed number of vehicles. The proposed approach is based on a reinforcement learning framework that constructs a complete tour plan from scratch while respecting an apriori fixed number (number of vehicles) of available vehicles. This approach is shown to be much faster and easier to train and achieves competitive results. "
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a method for link prediction based on the idea of counterfactual inference. Specifically, the authors propose a method to predict the existence of missing links between two nodes in a graph. The proposed method is based on a causal model that considers the global graph structure as the treatment and link existence as the outcome. The method is evaluated on several benchmark datasets and achieves state-of-the-art performance."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper proposes a two-stage method for unsupervised feature selection via knowledge contrastive disTillation (SOFT) that incorporates the second-order covariance matrix with the first-order data matrix for feature selection. In the first stage, the authors learn a sparse attention matrix that can represent second order relations between features, and in the second stage, they build a relational graph based on the learned attention matrix and perform graph segmentation. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state-of-the-art methods."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"This paper proposes a method for learning multimodal variational autoencoders (VAEs) by combining information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing—something that most existing approaches either cannot handle, or do so to a limited extent. The authors demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image-image) and CUB (image–text) datasets. They also compare the quality of representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data."
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes Deep Explore Options, an extension of Explore Options to tackle complex visual problems. Explore Options are an option that allows an extrinsically motivated Exploiter agent to call an intrinsically motivated Explorer agent for a set amount of time to observe and learn from interesting behaviors in the environment. They have only been conceptualized in a tabular setting, preventing generalization of the option. They revise them to Deep Explore options in the Dopamine framework, to learn from intrinsic reward in deep Reinforcement Learning. They introduce a new multi-agent buffer-selection algorithm, showing that agents can benefit from observing data that is interesting accoding to others. They also introduce an architecture to combine intrinsic and extrinsic agents, resulting in a new architecture that achieves a 50% faster wall-clock speed and a stronger representation."
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,This paper proposes a method for learning Hamiltonian dynamical systems from data. The authors propose a new metric SAI (stiffness-aware index) to classify the training data into stiff and non-stiff portions. This classification along with a resampling technique allows them to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. They evaluate SANN on complex physical systems including a three-body problem and billiard model and show that SANN is more stable and can better preserve energy when compared with other methods.
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper presents a method for training Transformer-based language models to perform multi-step computations. In particular, the authors train Transformers to emit intermediate computation steps into a “scratchpad” and then ask them to emit these steps into the scratchpad. The scratchpads are then used to improve the performance of the language models on a series of increasingly complex tasks, including long addition and execution of arbitrary programs."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"This paper proposes a method to generate adversarial perturbations that are interpretable, universal to any source image, and physically-realizable. The proposed method is based on deep image generators and a novel optimization objective. The authors show that they are versatile and use them to generate targeted feature-level attacks at the ImageNet scale. These attacks can reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of natural objects. They also guide the design of “copy/paste” adversaries in which one natural image is pasted into another to cause targeted misclassification."
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper proposes a reinforcement learning approach for simulated annealing (SA) based on reinforcement learning. In particular, the authors frame the proposal distribution as a policy, which can be optimised for higher solution quality given a fixed computational budget. The proposed approach is evaluated on Rosenbrock’s function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. The results show that the proposed approach outperforms the baselines."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper proposes a novel notion, the \delta-stationarity measurement, to explicitly measure the non-stationary of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. The authors propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition (MAMT) is established by adjusting the trust-regions of the local policies adaptively in an end-to-end manner. Experiments show that MAMT can bring noticeable and stable performance improvement and more suitable for large-scale scenarios with complex coordination relationships."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper proposes a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. The key idea is to leverage the strong correlation between the audio and lip movement streams for self supervised speech representation learning. Experiments on visual speech recognition show that AV-HuBERT achieves SOTA using 433 hours of text transcriptions, two orders of magnitude less than the amount of labeled data used in the prior best approach."
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"This paper proposes a new RL algorithm for the Maximum Cut problem. The main idea is to restrict the GNN to a single pre-processing step, before entering a fast-acting exploratory phase directed by a recurrent unit. The proposed algorithm achieves a new SOTA for RL algorithms on the maximum cut problem, while also providing orders of magnitude improvement in speed and scalability."
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper proposes a variational autoencoder (VAE) model with discrete latent variables, where the latent variables are sampled from a discrete distribution. The authors propose to train the VAE by optimizing the variational posterior of the encoder and decoder in a way that preserves the discrete nature of the latents. This is achieved by using evolutionary algorithms and truncated posterior truncation. The proposed approach is shown to be more efficient than amortized variational variational inference (AMI) and sampling-based VAE (sampling approximation)."
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"This paper proposes an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. The proposed method is evaluated in a wide range of environments showing that it can accurately identify controlled effects. The authors also demonstrate its capabilities as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a structure-regularized pruning (SRP) method to improve the performance of lightweight image super-resolution (SR) networks. The proposed method is based on the idea of structure regularization to align the locations of pruned filters across different layers. It can transfer the expressive power in the unimportant filters to the rest of the network. The authors apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-L and a very deep one SRPN. The experimental results show that the proposed method outperforms the state-of-the-art."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a framework for few-shot learning that tackles large domain shift between base and novel categories. The first step of the framework trains a feature extracting backbone with the contrastive loss on the base category data. For the second step, it trains a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones."
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"This paper studies the generalization properties of neural networks trained with Bayesian inference and gradient descent. The authors show that gradient descent can improve generalization by selecting networks with a large margin. They also show that minimum a posteriori functions can generalise best, and gradient descents can select for those functions. "
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a cross-lingual manifold mixup (X-Mixup) method, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show X-mixup achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the cross-Lingual representation discrepancy significantly. "
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of Byzantine robust distributed learning, i.e. the setting where a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages to the central server. The authors propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. They also theoretically and empirically validate their approach, showing that combining bucketing with existing robust methods is effective against challenging attacks."
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper investigates the relationship between disentanglement and multi-task learning based on hard parameter sharing. The authors perform a thorough empirical study of the representations obtained by neural networks trained on automatically generated supervised tasks. Using a set of standard metrics, the authors show that disentangled representations appear naturally during the process of multi- task neural network training. Next, they study the effects of hard-parameter sharing on representation learning and find that nontrivial disentangling appears in the representations learned in a multi-Task setting. Finally, they investigate the question of whether disentangler representation is needed for multitask learning, the results however are not conclusive."
SP:9851adb72e2918780f661f83f7da06eb866787be,"This paper presents a framework of certifying robust policies (CROP) for reinforcement learning against adversarial state perturbations, which provides state level robustness certification and the first certification for cumulative rewards. The authors propose two types of robustness criteria: robustness of per-state actions and lower bound of cumulative rewards, and develop a local smoothing algorithm that uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness guarantees of actions taken along this trajectory. They also develop a global smoothing algorithms for certifying the robusts of a finite-horizon cumulative reward under adversarial attacks. The proposed method is evaluated on three representative Atari games. "
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a new approach to conformal prediction, which aims to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. Conformal prediction provides the ability to adapt to model uncertainty by constructing a calibrated candidate set in place of a single prediction, with guarantees that the set contains the correct answer with high probability. This paper proposes to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets is bounded according to a user-specified tolerance. Subject to this constraint, the proposed algorithm optimizes for a generalized notion of set coverage (i.e., the true positive rate) that allows for any number of true answers for a given query (including zero). The proposed approach is evaluated on a number of classification tasks in natural language processing, computer vision, and computational chemistry."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper studies the complexity of deep ReLU networks with random initialization. The authors show that the expected length distortion does not grow with depth, and indeed shrinks slightly. They also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. The theoretical results are corroborated by the empirical results."
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes SAFEty skill pRiors (SAFER), a behavioral prior learning algorithm that accelerates policy learning on complex control tasks, under safety constraints. SAFER learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. In the inference stage, SAFER composes a safe and successful policy from the safety skills according to the inferred safety variable and abstract action. The authors demonstrate its effectiveness on several complex safety-critical robotic grasping tasks inspired by the game Operation."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a multi-branch neural network architecture for image restoration, inspired from the Retinal Ganglion Cells (RGCs) to achieve multiple restoration tasks in a general framework. In particular, the authors propose a new loss function for general restoration tasks and an MSC to replace the traditional skip connection. The experiments show that the proposed model achieves competitive performance results on four datasets, including image dehazing, deraindrop, deblurring, and rain removal."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes a new federated learning setup, Inference-Time PFL (IT-PFL), where a model trained on a set of clients, needs to be later evaluated on novel unlabeled clients at inference time. The proposed approach is based on a hypernetwork module and an encoder module. Specifically, the encoder network learns a representation for a client given its unlabelled data. The client representation is fed to the hypernetwork that generates a personalized model for that client. The authors also analyze and bound the generalization error for the novel clients and show that the approach can guarantee differential privacy."
SP:960d0a63a82593f6e72275b65f0501f0469d1924,"This paper proposes to use a conditional generative model (RCDM) to visualize the representations learned by self-supervised learning (SSL) models. The authors claim that the proposed method can reveal the invariances of SSL representations to data augmentation, and that SSL representations are more robust to small adversarial perturbation of their inputs. They also claim that there is an inherent structure learned with SSL model that can be used for image manipulation."
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"This paper proves that Fp sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when p \in (0,1] and p\in (1,2). This is an important step towards narrowing the gap of space complexity between private and non-private frequency estimation algorithms. The paper is well-written and easy to follow. "
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes a method for learning policies that are both locally optimal and sufficiently different from existing ones. The authors propose to use a trajectory-based novelty measurement to encourage the learning policy to consistently converge towards a previously undiscovered local optimum. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, the proposed method utilizes an intrinsic diversity reward to promote exploration. The proposed method is able to discover a wide spectrum of strategies in a variety of domains."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"This paper proposes a method for finding a few-step sampler for Denoising Diffusion Probabilistic Models (DDPMs) by optimizing a perceptual loss over a space of diffusion processes that makes use of a pre-trained DDPM’s samples by leveraging the reparametrization trick and gradient rematerialization. The authors propose Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. They show that optimizing the degrees of freedom of GGDM by maximizing sample quality scores via gradient descent leads to improved sample quality."
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"This paper proposes P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, they investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (“experts”) and select one to query. They require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. They perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations."
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes a method for continuous classification of time series (CCTS). The authors define CCTS as a continual learning task with the unclear distribution division. The authors propose a novel Adaptive model training policy ACCTS, which extracts data distributions adaptive to the time series evolution and the model change. Instead of reviewing all old distributions, ACCTS only replays the important samples adapted to the contribution of data to the model. Experiments on four real-world datasets show that the method can classify more accurately than all baselines at every time."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes an extension to the transformer architecture, called kNN-augmented attention, which dramatically increases the length of the context that a language model can attend to by using k-nearest-neighbor lookup into a large external memory. The authors demonstrate the effectiveness of external memory in a series of language modeling experiments over a variety of long-document datasets, including LaTeX documents, source code, formal proofs, and books. They also find that the model is capable of making use of newly defined functions and theorems during test time."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes a sampling scheme based on Metropolis-Hastings (MH) algorithm to sample from masked language modeling (MLM) models. The authors interpret MLMs as energy-based sequence models and propose two energy parametrizations for MLMs. The proposed sampling scheme uses the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. They validate the effectiveness of the proposed parameteretrization by exploring the quality of samples drawn from these energy based models for open-ended unconditional generation and a conditional generation task of machine translation."
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper proposes a method for learning data augmentation policy for NLP tasks. Specifically, the authors propose a reward function for learning the augmentation strategy to construct difficult but not too different samples (DND). They jointly optimize a data augmentation policy while training the model, to construct the augmented samples with low confidence but a high semantic similarity with original ones. They also introduce a sample re-weighting scheme to focus on difficult augmented samples after the original ones are learned confidently for more effective learning from the augmented ones."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,This paper proposes a method for meta-learning for offline reinforcement learning (OMRL) that improves upon FOCAL by incorporating intra-task attention mechanism and inter-task contrastive learning objectives to robustify task representation learning against sparse reward and distribution shift. Theoretical analysis and experiments are presented to demonstrate the superior performance and robustness of the proposed method compared to prior algorithms. 
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"This paper proposes a method to improve the performance of a belief model for partially observable Markov systems by fine-tuning the model parameters at inference time. Specifically, the authors propose to fine-tune the parameters of the belief model at each time step of the inference process. The authors show that the proposed method can improve the accuracy of the model at test time. The method is evaluated on Hanabi, where it is shown that the method improves the performance in terms of both the accuracy and the search performance."
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper proposes a method for training sparse neural networks. The authors propose to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. Specifically, the authors propose a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). They empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy-efficiency tradeoffs."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper proposes a conditional diffusion probabilistic model (ST-DDPM) for score-based generative models, which learns to recover the data distribution based on score matching. Inspired by the class clustering phenomenon, the authors propose to explicitly model the class center in the forward and reverse process, and make an elegant modification to the original formulation, which enables controllable generation and gets interpretability. The authors conduct extensive experiments on multiple tasks, and achieve competitive results compared with the state-of-the-art methods."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper proposes a method for domain generalization (DG) that explores diverse latent sub-spaces and learns individual hypotheses on those sub-space. The proposed method is motivated by the label-informative features captured in source domains, which allow us to project source and target examples onto appropriate subspaces while preserving crucial label information for label prediction. Experiments on benchmark datasets verify that LASSO achieves competitive and even state-of-the-art performances."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper improves upon the kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth squareroot kernel. The authors show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RkHS. They also show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square root kernel. In addition, they prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn, that do not have square-roots."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents an open-source benchmark suite for the max-min independence set (MIS) problem. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. The authors also conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. (2018), testing various configurations on small and large synthetic and real-world graphs. They show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper proposes Wavelet compressed convolution (WCC) for activation maps compression for 1x1 convolutions. WCC achieves compression ratios and computational savings that are equivalent to low bit quantization rates at a relatively minimal loss of accuracy. The authors use a hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. By combining WCC with light quantization, they achieve compression rates equal to 2-bit and 1-bit with minimal degradation in image-to-image tasks."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper studies the learning dynamics for extensive form correlated equilibrium (EFCE) in multiplayer general-sum imperfect-information extensive-form games. The main contribution is to develop faster no-regret learning dynamics in EFCE for multiplayer general sum games. In particular, it is shown that when all agents play T repetitions of the game according to the accelerated dynamics, the correlated distribution of play is an $O(T^3/4)$-approximate equilibrium. This significantly improves over the best prior rate of $O(\T 1/2)$. "
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes a method to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. By discretizing the action space, the proposed method can apply any discrete action deep RL algorithm to the continuous control problem. The proposed method is evaluated on three different setups: RL with demonstrations, RL with play data, and Imitation Learning. The results show that the method outperforms state-of-the-art continuous control methods."
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper proposes an adversarial style augmentation (AdvStyle) approach for domain generalization in semantic segmentation. AdvStyle dynamically generates hard stylized images by learning adversarial image-level style feature, which can encourage the model learning with more diverse samples. With AdvStyle, the model can refrain from the problem of overfitting on the source domain and thus can be more robust to the style variations of unseen domains. Experiments on two synthetic-to-real settings show that AdvStyle can largely improve the generalization performance and achieve state-of-the-art performance."
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper proposes a neuromorphic gesture analysis system which encodes event-based gesture data at high-resolution into a latent space representation suitable to compute the similarity of mid-air gesture data. The proposed method is based on an event guided variational autoencoder (Guided-VAE) which is jointly trained by two classifiers such that the latent space disentangles and accurately represents target features. The authors also implement the encoder component of the model on neuromorphic hardware and discuss the potential for the algorithm to enable real-time, self-supervised learning of natural mid- air gestures."
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper proposes a method for sparse hierarchical table ensemble (S-HTE) for tabular data classification and regression. The main idea is to use ferns (oblivious decision trees) instead of neurons to perform the inference. The inference is dense at the beginning of the training process and becomes sparse using annealing mechanism, leading to an efficient final predictor. The method is evaluated on CIFAR-10/100 and Fashion MNIST datasets and shows comparable performance to the state-of-the-art."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper proposes a method for learning value functions from undirected state-only experience (i.e. state transitions without action labels). The authors first theoretically characterize the applicability of tabular Q-learning in discrete Markov decision processes (MDPs) and show that it can learn the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-Learning (LAQ), an offline RL method that can learn effective value functions. LAQ learns value functions on discrete latent actions obtained through a latent-variable future prediction model. The authors show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. The experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper proposes SWARM parallelism, a model parallelism algorithm for training large models in a distributed setting. The main idea of SWARM is to create temporary randomized pipelines between available nodes that are rebalanced in case of failure. To further reduce the network usage of the approach, the authors develop several compression-aware architecture modifications and evaluate their tradeoffs. Experimental results show that SWARM can train a large Transformer language model on a swarm of preemptible T4 GPUs with less than 400Mb/s network throughput."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a method to improve the performance of decentralized multi-agent reinforcement learning by bridging the gap between offline training and online fine-tuning. The transition dynamics in offline training are not aligned with the transition dynamics of the online training, leading to sub-optimal policies. The authors propose to use two types of distances, i.e., embedding-based and value-based distance, to measure the similarity between transitions, and further propose an adaptive rank-based prioritization to sample transitions according to the transition similarity. Experimental results show that the proposed method outperforms baselines in a variety of tasks."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a logarithmic unbiased quantization (LUQ) method to quantize both the forward and backward phase to 4-bit, achieving state-of-the-art results. The authors analyze the difference between two rounding schemes: round-to-nearest and stochastic-rounding. They show that the former has lower MSE and works better for the quantization of the forward phase (weights and activations), while the latter is an unbiased approximation of the original data and is better for quantizing the backward phase (specifically, the neural gradients). Based on these conclusions, the authors propose to use the proposed method to format the gradients to format FP4 [1,3,0]. Combined with a known method for quantization to INT4, they achieve, without overhead, sota in 4- bit training in all the models."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper studies the problem of few-shot meta-learning in the setting of polythetic classification, where the goal is to learn a classifier that is able to classify the set of features. The authors propose a self-attention feature-selection mechanism that adaptively dilutes non-discriminative features. They demonstrate the effectiveness of their approach in several synthetic and real-world learning tasks. "
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes an environment and training methodology for emergent communication between agents using reinforcement learning. The communication environment consists of two agents, speaker and listener. The speaker is equipped with a vocoder that maps symbols to a continuous waveform, and the listener needs to map the continuous signal to the concept. They use deep Q-learning to show that basic compositionality emerges in the learned language representations. They also show that noise is essential in the communication channel when conveying unseen concept combinations."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"This paper proposes BadPre, a novel method for backdoor attacks on pre-trained NLP models. BadPre is task-agnostic and does not require prior information about the downstream tasks when implanting the backdoor to the pretrained model. The authors also design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that BadPre can compromise a wide range of downstream NLP tasks in an effective and stealthy way."
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper proposes an incremental skill discovery method, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn’t forget a learned skill. Experiments on HalfCheetah and Pong show that incremental skills significantly outperform current state-of-the-art skill discovery methods."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a new convolutional layer, called log-polar space convolution (LPSC), where the convolution kernel is elliptical and adaptively divides its local receptive field into different regions according to the relative directions and logarithmic distances. The proposed LPSC not only naturally encodes local spatial structures, but also greatly increases the single-layer receptive field while maintaining the number of parameters. Experiments on different tasks and datasets demonstrate the effectiveness of the proposed LpsC."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper studies the generalization ability of neural networks (NNs). The authors propose an information bottleneck (IIW) based on the trade-off between accuracy and information complexity of NNs, and propose an algorithm for the efficient approximation of IIW. They empirically demonstrate the fitting to compressing phase transition during NNs’ training and the connection between the IIW compression and generalization. Besides, they verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization and noisy labels."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"This paper investigates the working principle of indiscriminate data poisoning attacks, which add imperceptible perturbations to training data to maximize the test error. The authors find that the perturbation of advanced poisoning attacks are almost linear separable when assigned with the target labels of the corresponding samples. They further confirm that linear separability is indeed the workhorse for recent attacks and show that pre-trained feature extractors can be a powerful defense."
SP:7b50be406138ad01db3ee112899f622637896fe9,"This paper studies importance sampling and its variants in offline policy evaluation. The authors identify an important overfitting phenomenon in optimizing the importance weighted return, and propose an algorithm to avoid this overfitting. They provide a theoretical justification of the proposed algorithm through a better per-state-neighborhood normalization condition and show the limitation of previous attempts to this approach through an illustrative example. They further test the proposed method with less overfitting and better test performance compared with state-of-the-art batch reinforcement learning algorithms."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents a method for continual learning of how language is grounded in vision. Given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case CLIP by OpenAI), CoLLIE learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. Unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use and can efficiently learn and generalize from only a few examples, with little interference with the model's original zero-shot performance."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper proposes a framework for novel object captioning (NOC) based on BERT and CLIP. In particular, the authors propose VLAF2, for learning Visual-Linguistic Adequacy, Fidelity, and Fluency, which utilizes linguistics observed from captions for describing visual information of images with novel objects. Guided by BERT, the model learns to refine wordings of novel objects and uses reinforcement learning to evaluate the correctness of visual content described in the generated caption. Experiments on the nocaps dataset demonstrate the effectiveness of the proposed method."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. The authors show that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. They demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and – more importantly – to new classes as well, allowing the feature maps to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper proposes a deep point cloud reconstruction network consisting of a 3D sparse stacked-hourglass network as for the initial densification and denoising, and a refinement via transformers converting the discrete voxels into 3D points. In particular, a newly proposed module called amplified positional encoding is designed to differently amplify the magnitude of positional encoding vectors based on the points’ distances for adaptive refinements. Extensive experiments demonstrate that the network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"This paper proposes a new method, PipeGCN, for efficient full-graph GCN training. The main idea is to pipeline inter-partition communication with intra-partitions computation to hide the substantial communication overhead. The paper also provides a theoretical analysis to show the convergence of the proposed method. Extensive experiments show the effectiveness of the method. "
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper studies the problem of test time robustification, i.e., using the test input to improve model robustness. The authors propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations and maintaining confidence in its predictions. The experiments show that this approach achieves accuracy gains of 1-8% over standard model evaluation and generally outperforms prior augmentation and adaptation strategies."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper proposes a model-based reinforcement learning algorithm that jointly optimizes the dynamics model and the policy, such that updates to either component increase a lower bound on expected return. The proposed algorithm is conceptually similar to a GAN, where a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policies are updated to avoid states where the model predictions are unrealistic. The main contribution of this paper is an approach to jointly optimize the policy and dynamics model using the same objective. Unlike prior work, the objective is a global lower-bound on the standard expected return objective. The approach not only tells users how to train their dynamics model, but also guarantees to them that updating their model will result in a better policy."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a method to combine the advantages of behavioral cloning (BC) and observation history cloning (OH) methods. BC-SO and BC-OH have been shown to have their advantages and drawbacks, and the authors propose to combine them optimally to achieve the best of both worlds. The main idea is to first compute a coarse action based on the instantaneous observation, and then refine it into a final action using historical information. The proposed method is evaluated on the CARLA autonomous driving and MuJoCo continuous control tasks."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a model-based meta-learning method, DyAd to forecast physical dynamics. It uses an encoder to infer the parameters of the task and a prediction network to adapt and forecast given the inferred task. The model can also leverage any weak supervision signals that can help distinguish different tasks, allowing the incorporation of additional domain knowledge. On challenging turbulent flow prediction and real-world ocean temperature and currents forecasting tasks, the authors observe superior performance of their model across heterogeneous dynamics."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method first detects 2D boxes on the image, then uses the generated 2D box labels to select the corresponding RoI LiDAR points as weak supervision, and finally, a network is trained to predict 3D boxes which can tightly align with associated RoI points. This network is learned by minimizing the newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RL points. "
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The authors also introduce a deep Transformer model that integrates GBST and operates on the byte level. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed method outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper proposes a black-box hard-label backdoor detection method based on adversarial extreme value analysis (AEVA) to detect backdoors in neural networks. The authors show that the objective of backdoor detection is bounded by an adversarial objective, which leads to a solution with highly skewed distribution. Based on this observation, the authors propose the adversarial singularity phenomenon, which is an observation that adversarial map of a backdoorinfected example has a singularity. The proposed AEVA is based on the monte-carlo gradient estimation, computed from the extreme-value analysis. Experimental results demonstrate the efficacy of AEVA across a set of popular tasks and attacks."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new uncertainty measure, Kullback-leibler divergence criterion (KLoS), which is a class-wise divergence measure built from in-distribution samples and to not require OOD training data, in contrast to current second-order uncertainty measures. The proposed KLoS captures class confusion and lack of evidence in a single score. An auxiliary neural network is also designed to learn the refined criterion directly aligned with the evidential training objective. Experiments are conducted on CIFAR-10 and ImageNet datasets to demonstrate the effectiveness of the proposed method."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper studies a two-step semi-supervised learning algorithm that learns a linear classifier over datadependent features from unlabeled data. The authors show that the algorithm provably learns CNNs, under some natural distributional assumptions. Specifically, it efficiently learns convolutional networks, assuming the distribution of patches in the input images has low-dimensional structure. The dependence of the algorithm on the dimension of the patch distribution is essentially optimal."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"This paper proposes a new approach for face clustering based on graph convolutional networks (GCNs). The proposed approach first transforms the features of each face into a structure space, then uses an adaptive neighbour discovery strategy to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods."
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a new method for Generalizable Person Re-Identification (DG ReID) that aims to learn ready-to-use cross-domain representations for direct cross-data evaluation. The proposed method is based on distributionally robust optimization (DRO) for learning robust models that are able to perform well on a collection of possible data distributions (the uncertainty set) without demographics. However, the convex condition of DRO may not hold for overparameterized neural networks and applying KL DRO fails to generalize under distribution shifts in real scenarios. To address this issue, the authors propose a simple yet efficient approach, Unit DRO, which minimizes the loss over a reweighted dataset where important samples (i.e., samples on which models perform poorly) will be upweighted and others will be downweighted. Empirical results show that the proposed method outperforms previous DG ReID methods that even require demographics."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"This paper proposes a new regularizer for GNNs, called Noisy Nodes, which is a simple technique for improving the training of GNN. The idea is to corrupt the input graph with noise, and add a noise correcting node-level loss. The authors claim that adding noise helps overfitting, and the noise correction loss helps ameliorate oversmoothing by encouraging diverse node latents. The proposed regulariser applies well-studied methods in simple, straightforward ways which allows generic architectures not designed for quantum chemistry to achieve state-of-the-art results."
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper proposes a differentiable EM model for set representation learning. The model is built from the perspective of fitting a Gaussian mixture model to the set data that are viewed as i.i.d. samples, which offers more flexibility and prior-induced model regularization in a principled Bayesian manner. The proposed model is also shown to generalize the recent set embedding models based on optimal transport and attention, leading to a computationally efficient model with superb performance on tasks in bioinformatics and NLP."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes a contrastive feature selection method for feature selection in the contrastive analysis (CA) setting, where the goal is to select a small number of informative features for use in unknown downstream tasks. The proposed method, CFS (Contrastive Feature Selection), is a method for performing feature selection for the CA setting. The method is evaluated on a semi-synthetic dataset and four real-world biomedical datasets, and it consistently outperforms previous state-of-the-art methods."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the relationship between the optimal early stopping time and the model size and the sample size of the dataset for linear regression models. The authors propose a new model of overparametrization to capture the phenomenon that model size usually exceeds the number of features in practice. They show that the theoretical results on optimal early-stopping time corresponds to the training process of deep neural network. Moreover, they show the effect of early stopping on generalization and demonstrate that early stopping can help mitigate “double descent”."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper proposes a quasi-Newton method for the policy gradient algorithm with entropy regularization. The main idea is to replace the standard natural policy gradient (NPG) algorithm with the entropy-regularized NPG algorithm. The authors show that the resulting algorithm can be viewed as a natural policy gradients algorithm with a regularization term that encourages the policy to explore and improve the stability. In the case of Shannon entropy, the authors show the convergence of the proposed algorithm. For other entropy functions, this method results in new policy gradient algorithms that enjoy the Newton-type quadratic convergence near the optimal policy."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper proposes a general method inspired by case-based reasoning to train agents and generalize out of the training distribution in text-based games (TBGs). The method collects instances of positive experiences from the agent’s interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on-policy neural agent in the literature for TBGs. The experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state of the art results on widely used environments."
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,This paper proposes a two-stage method to distill multiple word senses from a pre-trained contextual language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. The authors demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embedding of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multimodal embedding methods on multiple benchmark data sets.
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper proposes to use the same architecture and pretrained weights of a neural net model to understand both images and point-clouds. Specifically, the authors transfer the image-pretrained model to a 3D convolutional filters and finetune the inflated imagepretrained models (FIP). They find that models with minimal finetuning efforts — only on input, output, and batch normalization layers — can achieve competitive performance on 3D point cloud classification, beating a wide range of point cloud models that adopt task-specific architectures and use a variety of tricks. Meanwhile, FIP improves data efficiency, reaching up to 10.0 points top-1 accuracy gain on few-shot classification. It also speeds up training of point- cloud models by up to 11.1x."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper proposes a method for training the autoregressive generative model that takes advantage of a well-designed energy-based learning objective. They show that the method is capable of alleviating the exposure bias problem and increase temporal coherence by imposing a constraint which fits joint distributions at each time step. Besides, they estimate energy scores based on the underlying auto-regressive network itself, which does not require any extra network. Finally, they can train the entire model efficiently without requiring an MCMC process."
SP:51e748c55bd4134047098559577fa3f37aa7433a,"This paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art adversarial training (AT) methods, including PGD-AT, TRADES, MART, and AWP. The authors introduce a new cost function and show that standard AT methods are special cases of their counterparts in the framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distribution-based AT-based algorithms."
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a method for unsupervised representation learning for multivariate time-series data. The proposed method is based on instance-level data augmentation and iterative bilinear temporal-spectral fusion. The method is evaluated on three tasks: classification, forecasting and anomaly detection. The results show that the proposed method outperforms the existing methods. "
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent. The learning rate is optimized via a simple extra gradient descent step, justified by an analysis that exploits the structure of neural networks. The authors formulate first and second-order gradients with respect to learning rate as functions of consecutive weight gradients, leading to a cost-effective implementation. Extensive experimental evaluation is conducted, validating the effectiveness of the proposed method for a plethora of different settings."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta-learning method for multi-task learning, where the agent’s goal is to achieve high reward over any sequence of tasks quickly. The authors propose a method, continual meta-policy search (CoMPS), that meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta learning to prepare for subsequent task learning. The RL process keeps track of the set of trajectories that achieve the highest sum of rewards. The M step uses this experience to perform metaRL via training a model to learn how to reproduce the best policies achieved from previous tasks. This meta-RL training process can accelerate the RL process even in fully offline settings, allowing the agent to train a metaRL model without collecting additional experience."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a new threat model for poisoned classifiers, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, the authors propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial trigger and the training data. They construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images with human interaction. They demonstrate the effectiveness of their attack through extensive experiments on high-resolution datasets."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper studies the problem of unconditional GAN distillation. The authors claim that the main challenge lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. Based on this finding, the authors propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent. A latent-direction-based distillation loss is also proposed, which preserves the semantic relations in latent space. Extensive experiments demonstrate the effectiveness of the proposed method."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a method for learning online approximations of offline algorithms. The main idea is to train a multi-task learning model to simultaneously detect behavioral structures which have already occurred and predict those that may come next. The authors demonstrate the methodology on both synthetic data and historical stock market data, where the contrast between explanation and prediction is particularly stark. "
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper proposes a method to improve the scalability of Gaussian processes (GP) by amortizing the computation of the inducing points locations, as well as the parameters of the variational posterior approximation. Specifically, a deep neural network (DNN) is used to output inducing points for each point at which the predictive distribution of the GP needs to be computed. The DNN also outputs the parameters for the corresponding variational approximation on the inducing values associated to inducing points. The proposed method is evaluated on several regression and binary classification problems from the UCI repository."
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"This paper proposes a protocol for distributed training of deep neural networks that is robust to both Byzantine and Sybil attacks. The proposed method, called BTARD-SGD, is a distributed training algorithm for large neural networks. Theoretical analysis is provided to show the robustness and effectiveness of the proposed method. Experiments on image classification and language modeling are conducted to demonstrate the effectiveness and robustness."
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper proposes a method for learning SPH-informed fluid dynamics models. The method is based on smoothed particle hydrodynamics (SPH), which is a mesh-free Lagrangian method for obtaining approximate numerical solutions of the equations of fluid dynamics. The authors propose a learn-able hierarchy of parameterized and physics-explainable SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators. The learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. The proposed method is capable of: (a) solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; (b) learning Lagrangians statistics of turbulence; (c) combining Lagrange trajectory based, probabilistic, and Eulerian field based loss functions; and (d) extrapolating beyond training sets into more complex regimes of interest."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"This paper proposes Mix-MaxEnt, a simple approach to regularize a single deterministic neural network to obtain improved accuracy and reliable uncertainty estimates. The approach simply puts an entropy maximization regularizer corresponding to the predictive distribution in the regions of the embedding space between the class clusters. This is achieved by synthetically generating between-cluster samples via the convex combination of two images from different classes and maximizing the entropy on these samples. Such a data-dependent regularization guides the maximum likelihood estimation to prefer a solution that maps out-of-distribution samples to high entropy regions (creating an entropy barrier); and (2) is more robust to the superficial input perturbations."
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a method for animating images by learning a set of orthogonal motion directions in the latent space of an auto-encoder. The proposed method is based on the idea of linear displacement of codes in the space of latent codes, which is used to represent the motion directions. The method is evaluated on three datasets (VoxCeleb, Taichi, TED-TALK) and compared with several state-of-the-art methods. "
SP:86f9f89f84e117c86478b9afaf087f65524f5472,"This paper proposes a meta-learning method called task interpolation (MLTI) to generate additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. The proposed method can be applied to both label-sharing and non-label-sharing scenarios. Theoretical analysis shows that MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification demonstrate the effectiveness of the proposed method."
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper considers the problem of fair representation learning, i.e. ensuring that no adversary can predict sensitive attributes at the cost of a small accuracy decrease. The authors propose a new method, called Fair Normalizing Flows (FNF), which is based on normalizing flows. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. The experimental evaluation on several datasets shows that FNF effectively enforces fairness without significantly sacrificing utility, while simultaneously allowing interpretation of the representations and transferring to unseen tasks."
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a graph neural network (GNN) for sub-graph isomorphism counting. The proposed method is based on edge-centric message passing and query-conditioned graph modulation. In particular, the edge level is treated as first-class citizens, and the graph level is modulated to adapt the input graph representation conditioned on the query. Extensive experiments are conducted to show the effectiveness of the proposed method. "
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper studies the problem of personalized federated learning, where each client has their own personalized labels, which might not be compatible with others (even for the same class), and can be also possibly from a variety of multiple domains. The authors propose a novel method, namely Similarity Matching and Kernel Factorization (SimFed), which measures task-level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. Furthermore, the authors factorize the model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge."
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper proposes an object dynamics distillation network (ODDN) which distills explicit object dynamic representations (e.g., velocity) from raw video input. The authors also build a relation module that calculates object-pair interactions and applies it to the corresponding dynamic representations of objects. The results show that visual representations of ODDN perform better in answering reasoning questions around physical events in a video compared to representaions of the previous scene representation methods, and the model could generate reasonable future frames given two input frames."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"This paper proposes a new graph neural network (GNN) layer that uses positional encoding (PE) techniques to improve the performance of GNNs that allow using positional features of nodes given by positional encoding techniques such as Laplacian Eigenmap, Deepwalk, etc. The proposed PEG layer uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original nodes features and rotation equivariant w.rt. the positional features simultaneously. Extensive experiments on link prediction demonstrate the effectiveness of PEG."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer and a newly proposed challenging task (political stance transfer), LaMer achieves qualitative advances in transfer accuracy, content preservation, and fluency. "
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper proposes a method for multi-hop logical reasoning on hyper-relational knowledge graphs (KGs). The proposed method is based on Graph Neural Networks (GNNs) and query embedding techniques. The main idea is to embed and answer queries in the form of queries, i.e. embed the query embeddings into the node features of the GNNs. The proposed approach is evaluated on a synthetic dataset and the results show that the proposed approach outperforms the baselines."
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper proposes a new Bayesian optimization method for hyperparameter optimization (HPO) in the gray-box setting. The authors propose a new surrogate for Gaussian Processes that embeds the learning curve dynamics and a new acquisition function that incorporates multi-budget information. The proposed method is evaluated on 50 datasets (Tabular, Image, NLP) and diverse neural networks (MLP, CNN/NAS, RNN)."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper proposes a method to improve the performance of learned image compression methods. The proposed method is based on quantization and makes the model inference integer-arithmetic-only, which is much simpler than the existing training and fine-tuning based approaches. The authors further improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. "
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes an unsupervised noise reconstruction and removal network for denoising scanning electron microscopy images. The architecture is inspired by gated recurrent units, which reconstructs and removes the noise by synthesizing the sequential data. The proposed method is able to distinguish true signal from noise and gives comparable/even better results than supervised approaches on the FIB-SEM data set."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"This paper provides a theoretical analysis of the label trick in graph neural networks (GNNs). In particular, the authors show that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. The authors then motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"This paper proposes a framework to analyze machine theory of mind in a symmetric multi-agent symmetric setting, which is a more realistic setup than the tasks currently used in the community. Based on the four properties needed for symmetric theory to arise, the authors provide a simplified setup on which to test the problem, and they show that SymmToM proves algorithmically difficult for current multiagent deep reinforcement learning models, even when tailoring them to our specific task. They also show that the best agents fail to achieve performance comparable to agents with access to the gold-standard mental state of other agents."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper proposes a zero-shot object detection method for detecting daily objects in indoor scenes. The main idea is to use a modified YOLOv5 neural network to perform generalized zeroshot detection on seen and unseen objects. The authors also propose a novel splitting method for YCB Video dataset to train and test gZSD algorithms. By changing the final detection layers, the proposed method significantly improves its performance on the YCB video dataset split with the proposal."
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper proposes a method to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, it scales up prior models by employing a high-Fidelity image generator (VQ-GAN) with a causal transformer model, and introduces additional techniques of top-k sampling and data augmentation to further improve video prediction quality. The proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high resolution video prediction on complex and large-scale datasets."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"This paper proposes to use vision transformers (ViTs) to improve generative adversarial networks (GANs). Specifically, ViTs are used to train discriminators and generators in GANs, and the paper proposes several regularization techniques to improve the stability and convergence of ViT-GANs. Experiments on CIFAR-10, CelebA, and LSUN bedroom show that the proposed method achieves comparable performance to the state-of-the-art GAN models. "
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper studies the question of why variational autoencoders (VAEs) tend to exhibit good likelihoods but poor sample quality for image generative modeling. The authors hypothesize that much of the entropy in natural image distributions is attributable to visually imperceptible information, which makes it easy for VAEs to achieve competitive likelihoods without successful modeling of the visually perceptible bits. Based on this hypothesis, the authors propose a two-stage training procedure that prioritizes the modeling the perceptible information. By doing so, they can train VAEs with good sample quality while still achieving ELBOs comparable to conventionally-trained VAEs."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"This paper studies the analytic forms of the reverse variance and the optimal KL divergence of diffusion probabilistic models (DPMs). In particular, the authors show that the optimal reverse variance of a DPM has analytic forms w.r.t. its score function. Then, they propose a training-free inference framework that estimates these two quantities using Monte Carlo method and a pretrained score-based model. They also provide lower and upper bounds of the optimal variance and clip the estimate for a better result."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper investigates whether it is feasible to switch to transformers for medical image classification as well, or if we should keep working with CNNs – can we trivially replace CNNs with transformers? The authors consider this question in a series of experiments on several standard medical image benchmark datasets and tasks. Their findings show that, while CNNs perform better if trained from scratch, off-the-shelf vision transformers can perform on par with convolutional neural networks when pretrained on ImageNet. The best overall performance on medical imaging tasks is achieved using in-domain self-supervised pretraining, where ViTs show a small advantage over CNNs."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper shows that the pretrained NLM can model stronger dependencies between text segments that appeared in the same training example, than it can between different training examples. This result has two roles: 1) it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. 2) it indicates further improvements to be made in NLM pretraining for the benefit of natural language understanding tasks."
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"This paper proposes a method to improve the interpretability and scalability of learning-to-optimize (L2O) models by introducing the powerful tool of symbolic regression to L2O. In particular, the authors introduce a holistic symbolic representation and analysis framework for learning to optimize, which yields a series of insights for learnable optimizers. The authors further propose a lightweight L2o model that can be meta-trained on large-scale problems and outperforms human-designed and tuned optimizers in terms of SOTA performance."
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper proposes a method to make RL models provably robust under adversarial perturbations. In particular, the authors propose to use randomized smoothing to add Gaussian noise to the input of the policy to defend it against norm-bounded adversarial attacks of its input. They show that by adding Gaussian smoothing noise, one can make any reinforcement learning agent provably defend against adversarial attack without significantly increasing the complexity of the agent’s policy. They also show that their method provides meaningful guarantees on the robustness of the defended policies and the total reward achieved."
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper studies the problem of estimating the accuracy of a classifier on the target domain using only labeled source data and unlabeled target data. The authors propose a method called Average Thresholded Confidence (ATC) that learns a threshold on the model’s confidence, predicting accuracy as the fraction of examples for which model confidence exceeds that threshold. ATC outperforms previous methods across model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). The authors also explore the theoretical foundations of the problem, proving that in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift."
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper studies the problem of partial distribution matching (PDM) for point set registration, where the goal is to recover a transformation that matches one set to the other. The authors formulate the registration problem as a PDM problem, where point sets are regarded as discrete distributions and are only required to be partially matched. They propose a method for large scale PDM problems by utilizing the partial Wasserstein-1 (PW) discrepancy, which they show can be efficiently optimized. Specifically, they theoretically derive the Kantorovich–Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these theoretical results, they propose a PWAN, which approximates the discrepancy by a neural network, and learns the transformation adversarially with the network. It also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations. They evaluate the proposed method on practical point sets registration tasks and show that the proposed PWAN is robust, scalable and scalable."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"This paper proposes a transfer learning method for hyperparameter optimization (HPO) based on landmark meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. DKLM is designed to capture the similarity between hyper-parameter configurations with an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, the proposed method can learn dataset-specific similarity representations for hyper parameter configurations. The authors experimentally validate the performance of DKLM in a wide range of HPO meta-datasets from OpenML and demonstrate the empirical superiority of our method against a series of state-of-the-art baselines."
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"This paper proposes a method for responsible disclosure of deep generative models. The proposed method is based on the idea of fingerprinting, i.e. generating a set of distinct latent codes for each generative model, which can be used to identify the source of the generated samples. The key idea is to use a 128-bit latent code to generate a population of models with distinct fingerprints, which are then used for identification and attribution. The method is evaluated on two tasks: deep fake detection and deep attribution. "
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"This paper proposes a method to provide post-hoc explanations for black box models that predict similarity between two inputs. The authors first provide feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. Then, they propose analogies as a new form of explanation in machine learning. The goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model’s prediction. The selection of analogies can be done by leveraging feature attribution. The proposed method is evaluated on tabular and text data."
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper studies the certified robustness of ensembles of deep neural networks (DNNs) against adversarial attacks. In particular, the authors show that ensemble models are shown to be more robust than single models empirically, but not in terms of the certified L2-robustness. Based on the theoretical findings, they propose the lightweight Diversity Regularized Training (DRT) to train certifiably robust ensemble ML models. Extensive experiments show that DRT-enhanced ensemble models can achieve the highest certified L_2-Robustness compared with existing baselines. "
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper studies the expressive power of the message passing graph neural networks (GNNs) in terms of the number of subgraphs. The authors propose a new pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. In particular, they show that this model can count subgraph of size k, and thereby overcomes a known limitation of low-order GNNs. They also show how recursive pooling can exploit sparsity to reduce the computational complexity compared to the existing higher-order GCNs."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper proposes a simple probe model called Graph Convolutional Simulator (GCS) for interpreting knowledge-enhanced LMs and exposing what kind of knowledge is integrated into these models. The authors conduct experiments to verify that our GCS model can indeed be used to correctly interpret the KI process, and use it to analyze two typical knowledge-enriched LMs: K-Adapter and ERNIE. They find that only a small amount of factual knowledge is captured in these models during integration. They further find that while K- adapter struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities and relations."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the effect of the adaptation learning rate in meta-learning with mixed linear regression. The authors first give a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. Second, they interpret the underlying dependence between the optimal learning rate and the input data. Finally, they prove that compared with empirical risk minimization (ERM), MAMl produces an initialization with a smaller average distance to the task optima."
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a method for source-free domain adaptation (SFDA), which aims to adapt a model trained on labelled data in a source domain to unlabelled data in the target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in target domain. The authors address these issues for a particularly pervasive type of domain shift called measurement shift, characterized by a change in measurement system, which can be resolved by restoring the source features. In the source domain, they store a lightweight and flexible approximation of the feature distribution under the source data and adapt the feature-extractor such that the approximate feature distributions under the target data realigns with that saved on the original source data. They call this method Feature Restoration (FR) as it seeks to extract features with the same semantics from the target domains as were previously extracted from the source, rather than extracting new ones. They additionally propose a bottom-up training scheme for FR which boosts performance by preserving learnt structure in the later layers of a network. They demonstrate that BUFR outperforms existing SFDA methods on real and synthetic data in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source models in the source."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,This paper studies the problem of adversarial robustness propagation in federated learning. The authors propose a simple yet effective propagation approach that transfers robustness through carefully designed batch-normalization statistics. They demonstrate the rationality and effectiveness of their method through extensive experiments. The proposed method is shown to grant FL remarkable robustness.
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper proposes a framework to infer the network structure behind the games from their equilibrium actions. Unlike existing methods, it achieves so by learning a mapping from the actions to the network structures without knowing the utility function of the game. This is especially beneficial in real-world scenarios where the nature of strategic interactions between players of the games remains hidden or may evolve over time. The proposed method is evaluated on three different types of network games using both synthetic and real-life data."
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes a relation prediction framework that predicts relations between each node pair by checking whether the subgraph containing the pair are similar to other subgraphs containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. The model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper studies the few-shot learning problem for histology images. The authors propose to combine contrastive learning (CL) with latent augmentation (LA) to build a system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. The experiments show that models learned by CL generalize better than supervised learning and LA brings consistent gains over baselines."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"This paper proposes a method for learning RNNs with continuous-time hidden states. The authors show that similar to standard RNN, the underlying reason for this issue is the vanishing or exploding of the gradient during training. This phenomenon is expressed by the ordinary differential equation (ODE) representation of the hidden state, regardless of the ODE solver’s choice. They provide a solution by equipping arbitrary continuous time networks with a memory compartment separated from its time-continuous state. This way, the continuous time dynamical flow within the RNN can be encoded to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path. They call these models Mixed-Memory-RNNs (mmRNN)."
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper proposes a method to perform full binarization of BERT, i.e., 1-bit quantization of the weights, embedding, and activation of the model. The proposed method, BiBERT, consists of two components: a bi-attention and a direction matching distillation (DMD) scheme to optimize the binarized BERT accurately. Theoretical analysis shows that the performance drop is mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose Bi-Attention and DMD to eliminate the performance bottlenecks. Extensive experiments show that the proposed method outperforms existing SOTA quantization methods with ultra-low bit activation, giving an impressive 56.3x FLOPs and 31.2x model size saving."
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper presents a method to solve keypoint detection and instance association by using Transformer. It supervises the inherent characteristics of self-attention – the feature similarity between any pair of positions – to solve the grouping problem of the keypoints or pixels. Unlike a typical CNN-based bottom-up model, it no longer requires a pre-defined vector field or embedding as the associative reference, thus reducing the model redundancy and simplifying the pipeline. The authors demonstrate the effectiveness and simplicity of the proposed method on the challenging COCO keypoints detection and person instance segmentation tasks."
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper proposes a method for finding Pareto-efficient policies in reinforcement learning (RL) for the mean-variance (MV) trade-off. The proposed method is based on maximizing the expected quadratic utility function, which corresponds to the policy that maximizes the expected utility. The method is computationally friendly, as it does not require gradient estimation of the variance term. The paper also includes various interpretations, such as targeting optimization and regularization, which expands the scope of applications of the method. "
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper studies the problem of test-time domain adaptation for an autoencoder system whose channel is generatively-modeled using a mixture density network (MDN). Different from the setting of conventional training-time (unsupervised or semi-supervised) domain adaptation, here the authors propose a fast and sample-efficient method for adapting only the MDN channel model. The method utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present to the decoders samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt theMDN channel using very limited number of samples, and improve or maintain the error rate of the auto-encoder under changing channel conditions."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new model for the abductive natural language inference task (αNLI), where the goal is to infer the most plausible explanation between the cause and the event. The authors propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” to penalize the inference network uniformly. The experimental results show that the proposed model achieves the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,This paper proposes a method for certifiable adversarially robust out-of-distribution detection (ProoD) that combines a binary discriminator (binary discriminator) and a standard neural network (ReLU) for OOD detection and classification. The main idea is to use the standard ReLU as the discriminator and then add a negative bias term to the loss function of the binary classifier to prevent the overconfidence issue of standard neural networks on OOD data. The proposed method is evaluated on CIFAR-10 and ImageNet datasets and shows comparable performance to previous work.
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper proposes a new transferable adversarial attack method called Image Classification Eraser (ICE) to erase classification information for any encountered images from arbitrary dataset. Specifically, the attacker has a set of surrogate models trained on different datasets (with different label sets and image sizes), and none of them is equal to the dataset used by the victim model. The proposed method is evaluated on Cifar-10/100/TiedredImageNet and shows better performance than existing transferable attack methods."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper studies the problem of false negative issue in discriminative pretrained language models (PrLMs). In particular, the authors claim that the existing PrLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false-negative issue where training is carried out on wrong data and leads to less efficiency and less robustness. To counteract the intrinsic and critical issue, they employ extra pre-training objectives to correct or prune the harmful gradient update after detecting the false negative predictions. Experimental results on GLUE and SQuAD benchmarks show that the counter-false-negative pretraining methods indeed bring about better performance together with stronger robustness and better performance."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes a novel semi-supervised learning (SSL) setting in which novel classes may appear in the unlabeled test data. In this setting, the goal is to solve the class distribution mismatch problem where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class must be initialized and the instance assigned to it. To tackle this challenging problem, the authors propose ORCA, an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. Extensive experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes a second-order quasi-Newton method for training large-scale deep neural networks (DNNs). The proposed method uses the BFGS update rule to approximate the Hessian inverse using past parameters and gradients, without computing the inverse. To achieve stable convergence, the authors introduce momentum in Hessian updates together with an adaptive damping mechanism. The authors provide rigorous theoretical results on the convergence of SLIM-QN in a stochastic setting and demonstrate that the proposed method has much less compute and memory overhead compared to existing second order methods."
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes a method called Locality-Sensitive hashing (LSP) for graph pruning based on locality-sensitive hashing. LSP aims to sparsify a graph so that similar local environments of the original graph result in similar environments in the resulting sparsified graph, which is an essential feature for graph-related tasks. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of LSP."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper proposes IDAA, a data augmentation method for contrastive self-supervised learning. The main idea is to add adversarial perturbations to the latent space of the variational auto-encoder (VAE) to improve the identity-preservation of the original latent space. Specifically, the authors propose to perturb the output of the VAE and add the perturbed latent space back to the original space, which preserves the original identity of the samples. The proposed method is evaluated on a variety of contrastive learning methods and shows improved efficiency and generalization."
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper proposes a method to detect distribution shifts in the training and test data distribution. The method is based on the observation that the difference between the training distribution and the test distribution can lead to a significant increase in the risk function of interest, like accuracy or calibration. The proposed method is able to detect harmful distribution shifts while ignoring benign ones, and allows continuous monitoring of model performance without increasing the false alarm rate. The authors demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"This paper presents a method for learning a physical model from a short video of a physical phenomenon. The authors propose to use neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) in order to obtain interpretable physical models directly from visual observations. The proposed method is able to identify physical parameters from only a single video, and the use of neural implicit representation enables the processing of high-resolution videos and the synthesis of photo-realistic imagery. Moreover, the embedded neural ODE has a known parametric form that allows for the identification of interpretable parameters and long-term prediction in state space."
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper considers the context-dependent Reinforcement Learning (C-MDP) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. The authors propose a variational inference algorithm for model learning using a sticky Hierarchical Dirichlet Process (HDP) prior and a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. They then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, they demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that their approach succeeds where state-of the art methods of other frameworks fail and elaborate on the reasons for such failures."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"This paper presents a framework to pretrain knowledge-based multilingual language models (KMLMs). The authors first generate a large amount of code-switched synthetic sentences and multilingual training data using the Wikidata knowledge graphs. Then based on the intra-and inter-sentence structures of the generated data, they design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. The pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by the authors, namely, logic reasoning."
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"This paper proposes an approach to learn an agent that maximizes the number of states that another agent can reach in its future, in order to improve the performance of the other agent. The agent is trained to maximize the choice of other agents, which is motivated by instrumental convergence theory, which shows that for a large proportion of rational agents this will be a useful subgoal, and thus can be leveraged to design generally altruistic agents. The authors evaluate their approach on three different multi-agent environments where another agent's success depends on the altruistic agent’s behaviour, and show that their unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them."
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the phenomenon of double descent in finite-width neural networks. Double descent is the generalization behavior of models depending on the regime they belong to: under or over-parameterized. The main contribution of this paper is the derivation of the lower bound of the population loss and its lower bound in terms of influence functions. The lower bound is based on the spectrum of the Hessian at the optimum, and it is shown to exhibit double descent behaviour at the interpolation threshold. The authors further investigate how the loss function affects double descent — and uncover interesting properties of neural networks and their Hessian spectra near the interpolated threshold."
SP:b485114712055f39a7afb951dbc3db482ff523fd,"This paper studies the asymptotic behavior of GNTK to measure the trainability of deep and wide GCNs in the large depth. The authors show that trainability drops at an exponential rate due to the aggregation operation. They further propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, inspired by the theoretical insights on trainability. The experimental results confirm that the proposed method can address the exponential decay problems of deep GCNs."
SP:25a92b3583afdc6892e59f1e769125d52c8011af,"This paper studies the problem of video-based cardiac measurement, where the goal is to estimate the left ventricle ejection time (LVET) intervals. The authors propose to use the second derivative of both the input frames and the target vital sign signals into the training procedure, and show that this improves the performance of the model. They also show that adding second-derivative inputs also improves performance when estimating second-order dynamics."
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper proposes a new communication architecture, called symbolic mapping, to help agents learn a compositional and symmetric language in complex settings like dialog games. The authors claim that a process from simplicity to complexity is a natural way to help multi-agent language learning and propose symbolic mapping as a basic component of the communication system of agent. They find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks and explore vocabulary expansion, and show that agents can easily learn to use new symbols when the environment becomes more complex."
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper proposes a hierarchical modular approach to learn agents that navigate and manipulate objects in a divide-and-conquer manner. Specifically, the authors first infer a sequence of subgoals to be executed based on language instructions by high-level policy composition controller (PCC). They then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies. Finally, they infer manipulation actions with the corresponding object masks using the appropriate interaction policy. The proposed method achieves the state-of-the-art performance on the challenging ALFRED benchmark."
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes Nuisance-randomized distillation (NURD) to build predictive models that perform well regardless of the nuisance-label relationship. NURD first defines the so-called nuisance-varying family, which is a set of distributions that differ only in the nuisance/label relationship, and then introduces the nuisance randomized distribution, a distribution where the nuisance and the label are independent. The authors prove that the representations in this set always perform better than chance, while representations outside of this set may not. The proposed method is evaluated on several tasks including chest X-ray classification."
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper proposes OTTER (Optimal TransporT distillation for Efficient zero-shot recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. The proposed method is based on pretrained image and text encoders, and models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero- shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images."
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper proposes a simple and generic framework for object detection. It casts object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. It achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"This paper proposes a method to distill the knowledge of a policy network into a symbolic policy, which is composed of geometric and numerical symbols and operators. A policy regression algorithm called RoundTourMix is proposed to learn the symbolic rules as teacher-student. The symbolic policy can be treated as discrete and abstracted representations of the policy network, but are found to be more interpretable, robust and transferable. The proposed symbolic distillation approach is experimentally demonstrated to maintain the performance and “denoise” the CNN policy."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a StyleGAN2-style method for image-to-image translation. The main idea is to disentangle the coarse-level object arrangements (posture) and the fine-grained level styling (identity) of the generated image from two exemplar sources. To achieve this goal, the authors propose a Vector-Quantized Spatial Normalization (VQSN) module for the generator for better pose-identity disentanglement. The VQSN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training-set images. The authors also design a joint-training scheme with self-supervision methods for the GANInversion encoder and the generator. Specifically, they let the encoders and generators reconstruct images from two differently augmented variants of the original ones, one defining the pose and the other for identity. Comprehensive experiments on various datasets show better synthesis image quality and disentangling scores of the proposed method."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a simple MLP-based model for extracting information from speech signals. The model splits feature channels into non-overlapped chunks and processes each chunk individually. These chunks are then merged together and further processed to consolidate the output. The proposed model is evaluated on two tasks: keyword spotting and speech enhancement. In all experiments, the proposed model outperforms the transformer-based solutions."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,This paper provides a lower bound on the generalization error that can be achieved by any transfer learning algorithm (regardless of its computational complexity) as a function of the amount of source and target samples. The lower bound depends on a natural notion of distance that can easily computed on real world data sets. It applies to any arbitrary source/target data distributions and requires minimal assumptions that enables it application to a broad range of problems. The authors also consider a more general setting where there are more than one source domains for knowledge transfer to the target task and develop new bounds on generalization errors in this setting.
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. Experiments show that the model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper proposes a non-Markovian generalization of behavioral priors for guiding exploration in reinforcement learning. The authors focus on state-independent temporal priors, which exploit the idea of temporal consistency and are generally applicable and capable of transferring across a wide range of tasks. They show how dynamically sampling actions from a probabilistic mixture of policy and temporal prior can accelerate off-policy reinforcement learning in unseen downstream tasks and provide empirical evidence that their approach improves upon strong baselines in long-horizon continuous control tasks under sparse reward settings."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a method for learning learning rate scheduling for training neural networks. The main idea is to construct a directed graph for the underlying neural network of the target optimization problem, which is then used to encode the current dynamics with a graph message passing network and train an agent to control the learning rate accordingly via reinforcement learning. The proposed method is evaluated on Fashion-MNIST and CIFAR10 for image classification and GLUE for language understanding."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper proposes a method for unsupervised object-centric learning from point clouds. The proposed method is based on the Chamfer Mixture Loss, which is a variational loss on point clouds, and the object-specification scheme that describes each object’s location relative to its local voxel grid cell. The method is evaluated on the UOR and UOT datasets, and it is shown that it can generalize well to previously unseen scenes with a large number of objects."
SP:3c57e921c1bf23e482551ceb71702931a7f07439,This paper proposes a method for grounding high-level tasks expressed in natural language to a set of actionable steps (e.g. open the fridge) using a pre-trained language model (LLM). The main idea is to condition on demonstrations and semantically translate the plans to admissible actions. Experiments on the VirtualHome environment show that the proposed method improves executability over the LLM baseline.
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a new generative model based on variational autoencoders (VAEs). The authors show that VAEs naturally unveil a Riemannian structure of the learned latent space, and propose a new way to generate samples consisting in sampling from the uniform distribution deriving intrinsically from the Riemmanian manifold learned by a VAE. The authors also stress the proposed method’s robustness in the low data regime which is known as very challenging for deep generative models. "
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper proposes Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, this approach accelerates training and inference, has fewer parameters, and requires less FLOPs to compute while achieving comparable or better accuracy across tasks."
SP:82731dcce233e748f63382e09b6224a513fe9689,"This paper proposes a method to integrate proprioceptive and external signals for navigation in a two-dimensional continuous environment. The proposed method is based on a recurrent neural network (RNN) with a resetting path integration module. The RPN is trained to update its internal state using the (possibly noisy) self-motion signal, and occasionally resets it when the image signal is present. The internal state exhibits strong correlation with position in the environment due to the direct-inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions in the environments through integration of past movement."
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the role of structure in feature learning in neural networks. Specifically, the authors consider the setting where the labels are determined by a set of class-relevant patterns and the inputs are generated from these along with some background patterns. They prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no Polynomial algorithm in the Statistical Query model can learn even weakly."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper studies the robustness of feature extractors to adversarial perturbations. In particular, the authors propose a methodology to analyze the robustity of fixed feature extractor, which in turn provides bounds on the robusts of any classifier trained on top of it. The tightness of these bounds relies on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers. The authors also propose a bespoke algorithm based on the iterative solution of a convex program that provably finds collisions. "
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper proposes Expectile V-Learning (EVL), a novel offline RL method, based on a new V-learning algorithm, EVL. EVL avoids actions outside the dataset and provides a smooth tradeoff between generalization and conversation for offline learning. It enables effective implicit planning along offline trajectories to accelerate the convergence of EVL and achieve better advantage estimation. The experimental results demonstrate that VEM achieves superior performance in most D4RL tasks and learns the accurate values to guide policy learning."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. Inspired by MAML-based approaches, the authors formulate weighted adversarial learning as a bilevel optimization problem where the upper-level task corresponds to learning a robust classifier, and the lower-level tasks corresponds to learn a parametric function that maps from a sample’s multi-class margin to an importance weight. Extensive experiments demonstrate that the approach improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines."
SP:3ad36be6b6900aabe43da043461cf178ce977082,"This paper proposes a new type of equivariant graph neural networks (SEGNNs) that can incorporate geometric and physical information in both the message and update functions. Specifically, the node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. The proposed model is composed of steerable MLPs, which are able to incorporate the node attributes into the message function and update function. Experiments on several tasks in computational physics and chemistry demonstrate the effectiveness of the proposed method."
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"This paper proposes a new differentiable physics model for composite materials such as cloths, where the model is able to model individual yarn physics and force interactions. The authors propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces are applied to cloths and are ubiquitous in various physical systems. Through comprehensive evaluation and comparison, the model can effectively solve inverse problems, provide high data efficiency and facilitate control."
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a method for transfer learning in reinforcement learning based on logical composition. In particular, the authors propose a method to determine whether a new task with possibly many desirable goals can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. The authors provide theoretical results on the performance of the transferred policy and the number of tasks that need to be learned throughout an agent’s lifetime to generalize over a distribution. They also demonstrate that as a side effect of their approach, an agent can produce an interpretable Boolean expression of its understanding of the current task."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper proposes a distributed solution for multivariate time series classification (MTSC) based on a wavelet scattering transformation of the time series and distributed feature selection. The proposed method, called LightWaveS, employs just 2,5% of the features of the popular rocket feature selection method (MINI)ROCKET while achieving comparable accuracy. The method scales well with more nodes and large numbers of channels and significantly reduces the input size and also provides insight to an MTSC problem by keeping only the most useful channels."
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a method for pretraining ELECTRA-style text encoders with an adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. The main idea is to jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. The weights of the mixtures are learned to maximize the training signals difficulty for the discriminator, by backpropagating the reversed gradient from the discriminators through Gumbel-Softmax. The experiments on the GLUE and SQuAD benchmarks demonstrate the empirical advantages of AMOS. "
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper proposes an adaptive fine-tuning method for cloze-style relational fact extraction. The idea is to fine-tune a pre-trained language model on the standard fill-mask task using a small training dataset of existing facts from a knowledge graph. The proposed method is evaluated on the LAMA probe. The results show that the proposed method outperforms baselines, even with significantly fewer training facts."
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper proposes to learn knowledge base embeddings in different geometric spaces and apply manifold alignment to align the shared entities. The proposed method is evaluated on the out-of-taxonomy entity typing task, where the goal is to predict the types of the entities from the knowledge graph. Experimental results on two datasets based on YAGO3 demonstrate that the approach has significantly good performances, especially in low dimensions and on small training rates."
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a one-shot learning framework for link prediction in temporal knowledge graphs. The proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. The experiments show that the proposed algorithm outperforms existing state-of-the-art baselines in predicting new events for infrequent relations."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes Progressive Module Networks (PMN) that learns task modules in a compositional manner, by exploiting previously learned lower-level task modules. PMN can produce queries to call other modules and make use of the returned information to solve the current task. The model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. The authors test the model in learning a set of visual reasoning tasks."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper proposes a method to improve the efficiency of convolutional neural networks (CNNs) by introducing channel-selectivity, i.e., redistributing its computations to important channels. In particular, the authors propose Selective Convolutional Unit (SCU), a widely-applicable architectural unit that improves parameter efficiency of various modern CNNs with bottlenecks. During training, SCU gradually learns the channel selectivity on-the-fly via the alternative usage of (a) pruning unimportant channels, and (b) rewiring the pruned parameters to the important channels during training. Experiments on CIFAR-10 and ImageNet show that the SCU-based models achieve both model compression and accuracy improvement compared to the baselines."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper considers the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). The authors propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, the method outperforms previous methods for PU learning on various real-world datasets."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes to use cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss. They show that their approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and RL on Atari games."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric framework to analyze the high-dimensional geometry of adversarial examples. The main idea is that for low-dimensional data manifolds embedded in high dimensional space, there are many directions off the manifold in which to construct adversarial attacks. Adversarial examples are a natural consequence of learning a decision boundary that classifies the data manifold well, but classifies points near the manifold incorrectly. The proposed framework shows that there is a tradeoff between robustness under different norms, adversarial training in balls around the data is sample inefficient, and nearest neighbor classifiers and ball-based adversarial learning are robust."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes a new representation learning framework for learning discrete representations of time series. The proposed framework is built on ideas from interpretable discrete dimensionality reduction and deep generative modeling. The authors propose a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of the method, the authors integrate a Markov model in the representation space, which uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper investigates the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. The authors show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. They prove that there is a trade-off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. They also provide a general method that is easily applicable to a large family of commonly used latent distributions."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes to learn the parameters of shallow neural networks in hyperbolic space. The main idea is to use the geometry of embedding of object representations, instead of the Euclidean space, to improve the efficiency of the model. The proposed method is evaluated on a variety of tasks, including machine translation, link prediction, shortest path prediction, and visual question answering. "
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper studies the problem of deep neural network (DNN) fingerprinting attacks that exploit cache side-channels. Specifically, the authors propose two attacks: DeepRecon, which reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload, a cache-side-channel technique, and a meta-model, which predicts the architecture and family of the pretrained model in a transfer learning setting. The authors also propose and evaluate new framework-level defense techniques that obfuscate the attacker’s observations."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a hierarchical network model for long-range video prediction, inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. The model contains a feed-forward path that computes and encodes spatiotemporal features of successive complexity and a feedback path that projects interpretation from a higher level to the level below. The network learns by comparing the incoming signals with its prediction, updating its internal model of the world by minimizing the prediction errors at each level of the hierarchy in the style of predictive self-supervised learning. This allows it to learn relationships among movement patterns, yielding state-of-the-art performance in long range video sequence predictions in benchmark datasets."
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper proposes a method to compute continuous embeddings for kmers from raw RNA-seq data, in a reference-free fashion. The authors show that their model captures information of both DNA sequence similarity as well as DNA sequence abundance in the embedding latent space. They confirm the quality of these vectors by comparing them to known gene sub-structures and report that the latent space recovers exon information. They also show that this latent space allows the detection of genomic abnormalities such as translocations and patient-specific mutations."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes a method to compress CNNs by first training a representation of the architecture and then performing gradient descent to determine an optimal architecture for the given task. The proposed method first encodes the network and then perform gradient descent in continuous space to optimize a compression objective function that maximizes accuracy and minimizes parameter count. The final continuous feature is then mapped to a discrete architecture using the decoder. Experiments on CIFAR-10/100, FMNIST and SVHN demonstrate the effectiveness of the proposed method."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper proposes a framework for planning online and learning offline in the setting where an agent with an internal model needs to continually act and learn in the world. The proposed framework builds on the synergistic relationship between local model-based control, global value function learning, and exploration. The authors study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value-function learning. Conversely, they also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, they demonstrate how trajectory optimization is used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper proposes an approach to zero-shot machine translation between language pairs for which there is no aligned data. It builds upon a multilingual NMT system (Johnson et al., 2016) by applying reinforcement learning, using only monolingual data on the zero shot translation pairs, inspired by dual learning. Experiments on the UN corpus show that the proposed approach outperforms the multilingual baseline model for unsupervised language pairs, on in-domain evaluations in the UN dataset. In out-of-domain evaluation, the proposed method performs as well or better than the state of the art."
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper criticizes the IRGAN, a generative adversarial network (GAN) framework that attempts to learn the conditional probability distribution p(d|q) over the documents (d), given the query (q). The authors claim that optimizing their minimax loss function will result in a generator which can learn the distribution, but their setup and baseline term steer the model away from an exact adversarial formulation, and this work attempts to point out certain inaccuracies in their formulation. The authors also propose a co-training like setup, where two models are trained in a cooperative manner rather than an adversarial fashion."
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"This paper proposes a variational auto-encoders (VAEs) that explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution. The authors derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efficient as in the standard VAE case. With the new approach, they are able to infer truly sparse representations with generally intractable non-linear probabilistic models. They show that these sparse representations are advantageous over standard VAEs representations on two benchmark classification tasks (MNIST and Fashion-MNIST) by demonstrating improved classification accuracy and significantly increased robustness to the number of latent dimensions."
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper proposes a non-adversarial feature matching-based approach to train generative models. The approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. The experimental results demonstrate that the proposed approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"This paper analyzes the expressive power of GNNs to capture different graph structures. It shows that the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, cannot learn to distinguish certain simple graph structures, and then develops a simple architecture that is provably the most expressive among the class of Graph Neural Networks and is as powerful as the Weisfeiler-Lehman graph isomorphism test. Experiments on a number of graph classification benchmarks demonstrate that the proposed model achieves state-of-the-art performance."
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"This paper proposes a framework for interpretable continual learning (ICL) that uses saliency maps to provide explanations of performed tasks and proposes a new metric to assess the quality of the explanations. Experiments show that the proposed ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy and also the explanations, which are assessed qualitatively and quantitatively using the proposed metric. This framework demonstrates that interpretability is not only useful for increasing the understanding of the obtained results, but can also improve the performance of a sequential learning procedure."
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a new evaluation framework for adversarial attacks on neural sequence-to-sequence (seq2seq) models taking the meaning preservation into account. It also proposes new constraints for attacks on word-based MT systems and shows, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. The paper also shows that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper proposes a new approach to improve the performance of reinforcement learning algorithms by combining the policies using original rewards and inverse (negative) rewards. The inverse policies and hybrid policies in this paper improve the original deep Q-learning, double-Q-learning and on-policy actor-critic algorithms significantly. The experiments for some games in OpenAI gym show that the hybrid polices obtained the rewards up to 63.8%, 97.8% and 54.7% more than the original algorithms."
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper presents a method for learning hierarchical representations of object parts and their hierarchical structure from videos. The method is based on the idea of learning a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. It learns to recognize the object parts via a layered image representation, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure, and predict the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that the proposed method works well on all three tasks: segmenting object parts, building their hierarchical structures, and capturing their motion distributions."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"This paper proposes Deep Deterministical Generative Classifier (DDGC), a generative classifier that can be trained on top of any softmax neural classifier pre-trained on noisy datasets. The main idea of DDGC is to define the parameters of the classifier using the minimum covariance determinant estimator, which can improve the classification accuracy with no re-training of the deep model nor changing its architectures. The authors show that DDGC can generalize well from noisy labels, and is robust against adversarial perturbations due to its large margin property."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes a method for hierarchical reinforcement learning (HRL) based on incremental unsupervised learning over a small memory of the most recent experiences of the agent. The method learns subgoals and skills together, based on experiences in the environment. The proposed method is evaluated on two RL problems: a variant of the rooms environment and ATARI 2600 game called Montezuma’s Revenge."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a neural framework for learning to solve the Circuit Satisfiability problem (SAT), which is a fundamental NP-complete problem in Computer Science. The proposed method is built upon two contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of the framework compared to the recently developed NeuroSAT method."
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes to combine the cross-entropy method (CEM) and Twin Delayed Deep Deterministic Policy Gradient (TD3) to improve the sample efficiency of off-policy deep RL algorithms. CEM-RL is mainly an evolutionary method, and is mainly used for goal-exploration. The authors show that the proposed method is competitive to the state-of-the-art even when considering sample efficiency. "
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes an interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) for time series with exogenous variables. The proposed model is equipped with hidden state matrix and update process, so as to learn variableswise hidden states. The authors also develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Extensive experiments using real datasets demonstrate the prediction performance and interpretability of the proposed model in comparison to a variety of baselines."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes feature smoothing, a simple data augmentation method for adversarial defense. The proposed method is based on the intuition of generating virtual data points as close as adversarial examples to avoid the computational burden of generating data during training. The experiments on MNIST and CIFAR-10 datasets explore different combinations of known regularization and data-augmentation methods and show that the proposed method with logit squeezing performs best for both adversarial and clean accuracy. "
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for deep and locally connected neural networks with ReLU nonlinearity. The framework bridges data distribution with gradient descent rules, favors disentangled representations and is compatible with common regularization techniques such as Batch Norm, after a novel discovery of its projection nature. It is built upon teacher-student setting, by projecting the student’s forward/backward pass onto the teacher's computational graph. "
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper proposes a modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy. This approach allows efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents and recommendation tasks, as allows learning personalized representations of different user states). In the experiment with video games playing, the proposed method allows separation of main task’s objectives and behaviors between different BMs. The experiments also show network extendability through independent learning of new behavior patterns."
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper introduces a biologically-inspired method for training networks to self-modify their weights. Building upon the differentiable plasticity framework, which already improved performance (sometimes dramatically) over non-plastic architectures on various supervised and RL tasks (Miconi, 2016; Miconi et al., 2018), here the authors introduce neuromodulated plasticity to let the network control its own weight changes. As a result, for the first time, the authors show that the proposed method can be trained with gradient descent and can improve the performance of neural networks on both reinforcement learning and supervised learning tasks. The authors conclude that differentiable neurmodulation of plasticity offers a powerful new framework for training neural networks."
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper proposes a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. The authors also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a method for open-ended recombination of style and content of one image with the content of another. The method is based on a variational autoencoder (VAE) and a to-be-trained style encoder. The authors propose an auxiliary loss, leakage filtering, to ensure that no style information remaining in the content representation is used for reconstruction and vice versa. The proposed method is evaluated on few-shot learning tasks and achieves state-of-the-art performance."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a new method to speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of states are defined under SAP. The first type says that after an action at is performed in a state and the agent reaches the new state st+1, the agent can decide whether the action is permissible or not permissible in state st. The second type says even without performing the action at in st, an agent can already decide whether at is permitted or not permitted in st. An action is not permitted if the action can never lead to an optimal solution and thus should not be tried. The authors incorporate the proposed SAP property into two state-of-the-art deep RL algorithms to guide their state-actions exploration. Results show that the proposed method can markedly speed up training."
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper studies the behavior of weight-tied autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, the analysis reveals interesting phase transition phenomena when the depth becomes large. This provides quantitative answers and insights to three questions that were yet fully understood in the literature. It also provides insights on pitfalls in training initialization practice, and demonstrates experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to layer-wise pre-training or batch normalization."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes SimBA, a black-box adversarial attack method that can be used for targeted and untargeted attacks. SimBA randomly selects a low-frequency component of the discrete cosine transform (DCT) and either adds or subtracts it to the target image. The proposed method is able to produce adversarial images that are imperceptibly different from the target model. The paper shows that SimBA can achieve better query efficiency than previous methods for both targeted attacks and targeted attacks."
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"This paper proposes a new method for option discovery in hierarchical reinforcement learning. The method is based on the idea of discovering “landmark sub-goals”, which are prototypical states of well connected regions. The authors propose a new model called Successor options that leverages Successor representations to achieve the same. They also design a novel pseudo-reward for learning the intra-option policies. Finally, they describe an Incremental Successor Options model that iteratively builds options and explores in environments where exploration through primitive actions is inadequate to form the successor representations."
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper studies the problem of domain division, which aims to segment instances drawn from different probabilistic distributions. The authors propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, are introduced to uncover and fine-tune the decision boundary of each domain, and the uncertain domain is newly introduced in the framework to adopt those instances whose domain labels cannot be predicted confidently. Extensive experiments demonstrate that the approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes polar prototype networks, a class of networks that explicitly states the structure, i.e., the layout, of the output. The structure is defined by polar prototypes. For classification, each class is described by a single polar prototype and they are a priori distributed with maximal separation and equal shares on the hypersphere. For regression, the authors show that training can be performed as a polar interpolation between two prototypes, arriving at a regression with higher-dimensional outputs."
SP:d1034342785d133cf8372b8624897963cc2ee83a,"This paper proposes a method for learning a reward function for reinforcement learning (RL) based on the observation that the state of the environment is already optimized for what humans want, and can be used to infer implicit preference information from the state to fill in the blanks. The authors develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. They find that the algorithm can infer both side effects that should be avoided as well as preferences for how the environment should be organized."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed method combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, they express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variables values."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,This paper proposes a dynamical neural network (DNN) to learn dictionaries for sparse representations. The proposed method is based on the idea of top-down feedback and contrastive learning. The authors prove that the true gradients for learning are provably computable by individual neurons. Experiments on several dictionary learning problems demonstrate the effectiveness of the proposed method. 
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"This paper proposes a novel convolutional neural network for the task of lane detection. The main contributions are as follows. First, the authors propose to use multiple encoder-decoders module in end-to-end ways and show the promising results for the lane detection task. Second, they analyze different configurations of multiple encoders nets. Third, they make an attempt to rethink the evaluation methods for the limitation of the popular methods based on IoU. The experiments on CUlane dataset demonstrate the effectiveness of the proposed net."
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper proposes a new method for batch contextual bandit learning based on Maximum Likelihood Inverse Propensity Scoring (MLIPS). The main idea is to estimate a maximum likelihood surrogate policy based on the logged action-context pairs, and then use this surrogate policy as the proposal. Theoretical analysis shows that the proposed MLIPS estimator is asymptotically unbiased, and moreover, has a smaller mean squared error than the classical IPS estimator. Experimental results on multi-label classification problems and a large-scale ad placement dataset demonstrate the empirical effectiveness of MLIPS."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"This paper proposes a meta-learning method for few-shot classification. The authors propose to learn how to create an individualized feature embedding specific to a given query image for better classifying, i.e., given a query image, a feature space tailored for its characteristics is created accordingly, leading to an individualised feature space in which the query image can be more accurately classified. Specifically, the authors introduce a kernel generator as meta-learner to learn to construct feature embeddings for query images. The kernel generator acquires meta-knowledge of generating adequate convolutional kernels for different query images during training, which can generalize to unseen categories without fine-tuning."
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper proposes a population-based genetic algorithm (GA) to train deep neural networks with evolutionary algorithms (ES) on deep reinforcement learning (RL) problems. The authors show that the proposed algorithm, called Deep GA, is able to outperform ES, A3C, and DQN on Atari and humanoid locomotion tasks. In addition, the authors also show that Deep GA is faster than ES, and can be used in combination with other neuroevolution techniques to improve performance. "
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a new curiosity module for reinforcement learning. The novelty bonus is computed by comparing the current observation with the observations in memory. This is done based on how many environment steps it takes to reach the observation from those in memory, which incorporates rich information about the environment dynamics. The proposed method is evaluated on VizDoom, DMLab and MuJoCo. "
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper presents a method for learning a transition model for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a feature selection method for instance-wise feature selection. The proposed method consists of 3 neural networks, a selector network, a predictor network, and a baseline network, which are used to train the selector network using the actor-critic methodology. The authors claim that the proposed method is capable of flexibly discovering feature subsets of a different size for each instance, which is a key limitation of existing state-of-the-art methods."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a domain adaptation method for semantic segmentation. The authors propose to learn discriminative feature representations of patches based on label histograms in the source domain, through the construction of a disentangled space. They then use an adversarial learning scheme to push the feature representations in target patches to the closer distributions in source ones. In addition, they show that their framework can integrate a global alignment process with the proposed patch-level alignment and achieve state-of-the-art performance."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper proposes two new optimistic algorithms for AMSGrad and Adam, respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in OPTIMISTIC ONLINE LEARNING, which leads to speed up in training deep neural nets in practice. Theoretical results are provided in Appendix F."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper introduces two new benchmarks to evaluate the robustness of image classifiers against common corruption and perturbations. The first one, called IMAGENET-C, is a standard corruption robustness benchmark, which evaluates the performance of a classifier against a set of corruptions and adversarial attacks. The second one is a perturbation robustness dataset, which is an extension of the ImageNet-P dataset. The authors also propose several methods to improve robustness, including histogram equalization, multiscale architectures, and larger models."
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper shows that dropout training can be understood as performing MAP estimation concurrently for an entire family of conditional models whose objectives are lower bounded by the usual dropout objective. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully-stochastic dropout objectives. The deterministic subvariant’s bound is equal to its objective, and the highest among these models. It also exhibits the best model fit in the experiments."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a global soft filter pruning (GSFP) scheme to prune redundant filters of convolutional neural networks (CNNs). Specifically, it adopts a robust pruning method, which measures the global redundancy of the filter in the whole model by using the soft pruning strategy. In addition, in the model recovery process after pruning, it uses the cumulative saliency strategy to improve the accuracy of pruning. Experimental results show that GSFP is effective on many classic CNN architectures and different data sets."
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a cross-lingual document classification framework (CACO) between related language pairs. CACO trains a document classifier with labeled training documents from a related language and optionally a small dictionary, pre-trained embeddings, or parallel text. The method exploits subword similarities between related languages to generalize from source language data. This technique is particularly useful in low-resource settings where unlabeled monolingual or parallel texts are limited. The authors provide empirical evaluation on multiple related languages and show that the proposed method matches high resource CLWE-based methods."
SP:544e421f9c747640d949f433e3091763508b7237,"This paper proposes a method for weakly supervised temporal action localization. The proposed method is based on the marginalized average attention (MAAN) module, which learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to the learned latent distributions and takes the expectation over all the averaged subset features. Theoretically, the authors prove that MAA reduces the gap between the most discriminant regions in the video to the others, and thus MAAN generates better class activation sequences to infer the action locations."
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper proposes a method to learn disentangled representations of word-level and chunk-level representations using the framework of holographic reduced representation (HRR). The authors claim that the proposed method is able to discover crude linguistic roles, which roughly resembles a classic division between syntax and semantics. The authors also claim that by introducing an inductive bias, their models can learn representations that roughly corresponds to syntax and semantic roles."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,This paper studies the problem of joint active perception and planning in partially observable Markov decision processes (POMDPs). The authors propose a greedy strategy for observation selection that aims to minimize the uncertainty in state. They also develop a novel point-based value iteration algorithm that incorporates the greedy strategy to achieve near-optimal uncertainty reduction for sampled belief points. This enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning.
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"This paper proposes a new curriculum loss for training deep neural networks. The main idea is to train top layers on “good” samples to reduce large shifting, and encourage ”bad’ samples to learn from ”good’ sample”. The authors also propose a representation loss for low-weighted data to encourage their training. Experimental results show that the proposed algorithm has an consistent performance improvement over several benchmark datasets."
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper proposes a model for learning heuristics for combinatorial optimization problems. The proposed model is based on attention layers and is trained using REINFORCE with a simple baseline based on a deterministic greedy rollout, which the authors claim is more efficient than using a value function. The model is evaluated on the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP)."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,This paper proposes a differentiable neural architecture search (DNAS) framework to solve the mixed-precision quantization of neural networks. The authors formulate this problem as a neural network architecture search problem and propose a novel differentiable NAS framework to efficiently explore its exponential search space with gradient-based optimization. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,This paper proposes a new attention model for sequence prediction. The main idea is to decompose the joint distribution of the attention and output variables into a posterior distribution conditioned on the output. The output token distribution is obtained by aggregating predictions across all attention. The authors also propose a principled way to incorporate task-specific structural biases and prior knowledge into attention. Experiments are conducted on translation and morphological inflection tasks.
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper proposes a new method for the unpaired image-to-image translation problem. The proposed method, called HarmonicGAN, aims to learn bi-directional translations between the source and the target domains. The main idea is to introduce a smoothness term over the sample graph to attain harmonic functions to enforce consistent mappings during the translation. The method is evaluated on medical imaging, object transfiguration, and semantic labeling."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"This paper proposes a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing the exploding and vanishing gradient problem (EVGP). Specifically, the gradient components through the linear path (cell state) get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which the authors show empirically), their suppression can prevent LSTMs from capturing them. This algorithm1 prevents gradients flowing through this path from getting suppressed, thus allowing the L STM to capture such dependencies better. The authors show significant improvements over vanilla gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization on various benchmark datasets."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a method for training binary weight neural networks from scratch under the Bayesian deep learning perspective. The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, the authors generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. The proposed method is evaluated with several visual recognition tasks including ImageNet."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through the framework. The authors then develop an inference approach that allows them to synthesize a more expressive global network without additional supervision or data pooling. They then demonstrate the efficacy of their approach on two popular image classification datasets."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper studies the problem of learning in differentiable games. The authors propose a new algorithm called SOS, which interpolates between LOLA and a stable variant called LookAhead. They show that the proposed algorithm inherits the essential guarantees of LOLA while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally. Moreover, SOS fully preserves opponent shaping."
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper presents a VAE based alarm system for segmentation algorithms which predicts the qualities of the segmentation results without using ground truth. The shape feature of a segmentation result is captured using the value of loss function when the segmentations result is tested using a Variational Auto-Encoder(VAE). The VAE is trained using only the ground truth masks, therefore the bad segmentation Results with bad shapes become the rare events for VAE and will result in large loss value. By utilizing this fact, the authors are able to detect all kinds of shapes that are out of the distribution of normal shapes in ground truth (GT). Finally, they learn the representation in the one-dimensional feature space to predict the representations in the feature space. The proposed method outperforms the uncertainty based methods and direct regression method, and the proposed method has better transferability to other datasets."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper proposes an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The proposed model is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper presents an end-to-end neural network architecture for program synthesis from natural language (NL) specifications. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTMs, for which the authors propose novel signal propagation schemes and soft attention mechanism."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper studies the adversarial robustness of deep neural networks on MNIST. The authors propose a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. They derive bounds on the robustness and go to great length to empirically evaluate the model using maximally effective adversarial attacks. They show that even the widely recognized and by far most successful L∞ defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbation that make little sense to humans."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weights matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, the authors show that the spectrum control improves the generalization ability of GAN. The experiments on CIFAR-10, STL-10 and ImgaeNet datasets confirm that compared to other methods, the proposed method is capable of generating images with competitive quality by utilizing spectral normalization and encouraging the slow singular value decay."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"This paper proposes Anderson accelerated value iteration (A2VI) and Anderson accelerated Q-learning (DA2Q) algorithms for reinforcement learning. A2VI is an extension of the modified policy iteration (MPI) algorithm, which is a classical approximate method for policy evaluation. The authors provide convergence analysis for the proposed algorithms and conduct experiments on both toy problems and Atari games. "
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper proposes a novel method, SupportNet, to solve the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines the strength of deep learning and support vector machine (SVM) to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training so that the model can review the essential information of old data when learning the new information. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The experimental results show that SupportNet outperforms the state-of-the-art incremental learning methods and even reaches similar performance as the deep neural network trained from scratch on both old and new data."
SP:d228d213f79716774043cea253305fecece659ec,"This paper compares the performance of different measures of unit selectivity in AlexNet, including precision, mean activity selectivity, activation maximization, and top-class selectivity. It shows that AlexNet does not have any 100% selective localist units, and that the precision and CCMAS measures provide a much higher level of selectivity than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. In addition, the interpretable images in the hidden layers are not associated with highly selective units."
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper studies the problem of node-wise community detection in the setting of graph neural networks (GNNs). In particular, the authors propose to use the non-backtracking operator on the line graph of edge adjacency matrices to improve the performance of the belief propagation algorithm on binary and multiclass stochastic block models. They also provide a theoretical analysis of the optimization landscape of simplified linear GNNs for community detection and show that the gap between the loss value at local and global minima are bounded by quantities related to the concentration of certain random matricies."
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper studies the dictionary learning problem, where the goal is to learn a linear combination of a few columns of a matrix known as a dictionary where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This paper proposes a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the Dictionary and coefficients exactly at a geometric rate, when initialized appropriately. The algorithm is scalable and amenable for large scale distributed implementations in neural architectures, by which they mean that it only involves simple linear and non-linear operations."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,This paper proposes a new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. The loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. The training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. The authors demonstrate that these techniques provide large improvements to a similarity search tasks.
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,This paper proposes a method for neural architecture search (NAS) that is based on hypernetworks and graph neural networks. The main idea is to compute the weights of any architecture by running inference on a graph neural network (GNN) that predicts the topology of the architecture. The authors propose to use the validation accuracy of networks with the generated weights as the surrogate search signal for NAS. The proposed method achieves competitive results on CIFAR-10 and ImageNet mobile with nearly 10x faster speed compared to other random search methods.
SP:65ccf43cd4e033d22239069057f5200d49f33724,"This paper proposes a method to improve generative adversarial imitation learning (GAIL) by using additional information from non-expert demonstrations as an extra class in discriminator learning. The idea is to perform multiclass classification to learn discriminator functions where the demonstrations are regarded as being drawn from an additional class. Compared to related methods that use an additional dataset for IL, the proposed method M-GAIL relies on a less restrictive assumption on the dataset and can efficiently train deep neural networks. Experiments in continuous control tasks demonstrate that the method learns better policies than the baseline GAIL."
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper proposes a new neural network architecture, called Invertible Neural Networks (INNs), for the inverse problem, i.e. the problem of inferring the posterior distribution of the parameters of a system from a set of measurements, conditioned on the observed measurements. The authors argue that INNs are well-suited for this task, as they focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. They prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INN are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters. "
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a new ensemble method for uncertainty quantification in neural networks. The proposed method is based on the idea of mixture density networks (MDNs), which is a finite mixture model with uniform mixing weights. The authors propose to replace the fixed mixing weights by an adaptive, input-dependent distribution represented by an NN, and by considering uncountably many mixture components. They empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper proposes a method to compress the weights of a neural network by using a variational distribution over weights. The proposed method is based on the bits-back argument, where the weights are encoded using a random sample and the compression rate is controlled by the number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distributions and the encoding distribution. The method is shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. "
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper proposes ProxylessNAS, a differentiable neural architecture search (NAS) algorithm that can directly learn the architectures for large-scale target tasks and target hardware platforms. It addresses the high memory consumption issue of differentiable NAS and reduces the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper proposes to modify the linear penalties to second-order ones, and argues that this results in a more practical training procedure in non-convex, large-data settings. For one, the use of second order penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding the instability and potential lack of convergence associated with two-player min-max games. Secondly, the authors derive a method for efficiently computing the gradients associated with the second- order penalties in stochastic mini-batch settings. The resulting algorithm performs well empirically."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"This paper proposes to use the reweighted wake-sleep (RWS) algorithm for learning discrete latent-variable models. The authors show that RWS outperforms the existing state-of-the-art methods in discrete latent variable learning. They also show that, unlike the importance weighted autoencoder, RWS learns better models and inference networks with increasing numbers of particles and that its benefits extend to continuous latent variable models as well."
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper introduces SG-SPEN, a method for learning structured prediction energy networks (SPENs) using a scalar reward function. The reward function is constructed from human-written functions or complex non-differentiable pipelines. The authors propose to use efficient truncated randomized search in this reward function to train SPENs, which provides efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction. The key ingredient of the training algorithm is sampling from reward function through randomized search, which is used to generate informative optimization constraints. These constraints gradually guide gradient-descent inference toward finding better prediction according to the reward function, leading to better performance. The method also enjoys a simpler training algorithm and rich representation over output variables."
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper studies the problem of robust policy search, i.e. learning policies that do not degrade in performance when subject to unseen environment model parameters. The authors propose an active learning based framework, EffAcTS, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset. They apply this framework to an existing method, namely EPOpt, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control tasks."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper proposes a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. The authors prove convergence for TTN, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation. They empirically demonstrate the benefits of TTNs, both for policy evaluation and control."
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target to execute, and updates the semantic models based on new observations. The proposed method is evaluated in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a method to improve the generalization and explanation ability of end-to-end deep learning driving models. The proposed method is composed of two modules: a perception module for see and think and a driving module for behave. The driving module is trained with multi-task perception-related basic knowledge and driving knowledge stepwisely. Specifically, segmentation map and depth map (pixel level understanding of images) are considered as what & where and how far knowledge for tackling easier driving-related perception problems before generating final control commands for difficult driving task. The results of experiments demonstrated the effectiveness of multitask perception knowledge for better generalisation and accident explanation ability."
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the trade-off between adversarial robustness and standard classification. The authors show that there exists an inherent tension between the goal of adversarially robustness (i.e., robustness to adversarial perturbations) and that of standard generalization. They also argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to better with salient data characteristics and human perception."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"This paper proposes a method for gradient-based training of neural networks that uses only local learning rules and does not rely on neurons having a mechanism for back-propagating an error gradient. The authors propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, the authors can simply use this initializing network for inference, resulting in a learned feed forward network. The experiments show that this network appears to work as well or better than the original version of Equilibrium propagate while requiring fewer steps to converge."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper proposes a zeroth-order stochastic optimization algorithm, ZO-signSGD, which enjoys the dual advantages of gradient-free operations and signSGD. The convergence rate is proved to be $O(\sqrt{\frac{d}{\sqrt{d}}/T}$ under some mild conditions, where $d$ is the number of optimization variables and $T$ is number of iterations. The authors also analyze the effects of different gradient estimators on the convergence rate and propose several variants of the proposed algorithm. Experiments on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of the algorithm."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,This paper proposes a method to reduce the computation efforts of convolutional neural networks. The method takes advantage of the fact that some convolution operations are actually wasteful since their outputs are pruned by the following activation or pooling layers. The proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example model and Network in Network.
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper proposes to exploit the temporal dependency property in audio data to characterize audio adversarial examples. The authors show that while four primitive input transformations on audio fail to withstand adaptive adversarial attacks, temporal dependency is shown to be resistant to these attacks. The proposed method is easy to operate and does not require model retraining. They believe their results shed new lights in exploiting unique data properties toward adversarial robustness."
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes a compositionality-aware generative model for generative models of images. The main idea is to learn to generate images by means of compositionality, i.e. the generator is encouraged to consider objects and their relations explicitly, and generate images that are more faithful to the reference distribution. The authors evaluate their approach on several multi-object image datasets, and find that the generator learns to identify and disentangle information corresponding to different objects at a representational level."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper introduces a learning setting called “reference-based disentangling”, where the goal is to learn a representation where a set of target factors are disentangled from others. In this setting, the only supervision comes from an auxiliary ""reference set"" that contains images where the factors of interest are constant. In order to address this problem, the authors propose reference-based variational autoencoders, a novel deep generative model designed to exploit the weak supervisory signal provided by the reference set. During training, they use the variational inference framework where adversarial learning is used to minimize the objective function. By addressing tasks such as feature learning, conditional image generation or attribute transfer, they validate the ability of the proposed model to learn disentanglement representations."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper proposes a method for continual online learning of deep neural network models. The proposed method is based on meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control. The authors demonstrate that MOLe outperforms alternative prior methods and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances."
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper studies the problem of training RNN-based RL agents from prioritized experience replay. The authors propose a new training strategy, Recurrent Replay Distributed DQN (R2D2), to improve the performance of distributed RL agents. The proposed method is evaluated on Atari-57 and DMLab-30, and it is the first agent to exceed human-level performance in 52 of the 57 Atari games. "
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper proposes a hierarchical generative model for modeling multi-agent trajectories in basketball. The model is based on the idea of using weak labels, which is extended to the spatiotemporal regime. The authors propose a hierarchical framework that can capture long-term coordination using intermediate variables. The proposed model is evaluated on synthetic and real-world datasets. "
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper presents a method that learns to integrate temporal information with partially observed visual evidence, based on graph-structured variational recurrent neural network (Graph-VRNN) to infer the current state of the (partially observed) world, as well as to forecast future states. The proposed method is trained end-to-end to learn the dynamics model, which is then used to predict the future state. The authors show that their method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"This paper proposes a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. The method is based on approximating the functions with a differentiable neural network in a way that drives the base network to comply with the function interface during the optimization process. At inference time, the method replaces the differentiable estimator with its external blackbox non-differentiable counterpart such that the output of the neural network matches the input arguments of the black box function. Experiments show that the integrated model generalizes better than a fully differentiable model and learns more efficiently."
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,"This paper proposes a method for meta-learning, where the goal is to learn a model that generalizes well to new tasks. The authors propose to use a mixture of hierarchical Bayesian models over the parameters of an arbitrary function approximator such as a neural network. They propose a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. They also derive a novel and scalable non-parametric variant of their method that captures the evolution of a task distribution over time as demonstrated on a set of few-shot regression tasks."
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes Meta Auxiliary Learning (MAXL), a self-supervised auxiliary learning method for the task of image classification, where the auxiliary task is hierarchical sub-class image classification. The proposed method learns to generate optimum auxiliary tasks which, when trained alongside a principal task in a multi-task setup, maximise the generalization of the principal task across a validation dataset. The experiments on three different CIFAR datasets show that MAXL outperforms baseline auxiliary learning methods and is competitive even with a method which uses human-defined sub-classes hierarchies."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,This paper proposes a neural network-based approach for open set recognition. The proposed approach is based on the idea that instances from the same class should be close to each other while instances from different classes should be further apart. The authors propose a simple threshold estimation technique to estimate the threshold. The approach is evaluated on two datasets of malware and images.
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of training low-precision deep neural networks. The authors propose two approaches: (1) starting with pretrained fp32 precision baseline networks and fine-tuning, and (2) combatting gradient noise introduced by quantizing weights and activations during training, by using larger batches along with matched learning rate annealing. They demonstrate the effectiveness of their approach on the ImageNet classification benchmark."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"This paper proposes an approach to model surface properties governing bounces in everyday scenes. The model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing restitution and effective collision normals. To achieve this, the authors introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials. The proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations."
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper studies the relationship between adversarial vulnerability and the gradient norm of the loss function and the input dimension. The authors show that the gradient norms of various neural network architectures are related to the square root of the input size, and that the network becomes increasingly vulnerable to adversarial attacks with increasing input size. They also show that this relationship holds for differentiable classifiers and losses."
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper proposes an interactive agent modeling scheme enabled by encouraging an agent to learn to probe. In particular, the probing agent (i.e., a learner) learns to interact with the environment and with a demonstrator to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used for enhancing the agent modeling to learn a more accurate mind model of the target agent. The framework consists of two learning processes: i) imitation learning for an approximated agent model and ii) pure curiosity-driven RL for an efficient probing policy to discover new behaviors that otherwise can not be observed. The experimental results suggest that the approach generalizes better in novel scenarios than the ones learned by passive observation, random probing, and other curiositydriven approaches."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to traditional artificial neural networks (ANNs) inspired by biological neuromodulators. The authors introduce a new type of ANN nodes, termed modulators, which mimic the function of biological neurmodulators and enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns. In this manner, they enable the slope of the activation function to be context dependent. Experimental results show that the modulated models consistently outperform their original versions."
SP:287a577834fd2820a939a1113b39146a22727491,"This paper presents a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. NANSY does not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training. Experiments on zero-shot voice conversion, pitch shift, and time-scale modification show the effectiveness of the proposed method."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"This paper studies the generalization properties of gradient-based bilevel programming algorithms for hyperparameter optimization. In particular, the authors propose an expectation bound for the unrolled differentiation algorithm based on a notion of uniform stability on validation. They also present an expectation bounds of the classical cross-validation algorithm. Theoretical results suggest that unrolled gradient descent algorithms can be better than cross validation algorithms under certain conditions. In experiments on feature learning and data reweighting for noisy labels, the paper corroborates the theoretical findings."
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper proposes a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. The main idea is to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of the approach lies in training teacher models and the procedure is straightforward, most of the existing methods can adopt this technique."
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies generalization to out-of-distribution (OOD) data. The authors propose a theoretical framework to characterize the learnability of OOD generalization problems. They also introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. Based on these, the authors prove generalization bounds and give guarantees for the generalization error. Inspired by the framework, they design a model selection criterion to check the model’s variation and validation accuracy simultaneously."
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper proposes a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm for online multi-task learning with non-stationary task distributions. The proposed algorithm maintains a Dynamic Gaussian Mixture Model (DGM) for meta-parameters, with the number of component distributions determined by a Chinese Restaurant Process (CRP). To infer the posteriors of model parameters, the authors develop a more robust posterior approximation method – structured variational inference for the sake of avoiding forgetting knowledge. The experimental results show that the proposed method is superior in transferring knowledge among diverse tasks and alleviating catastrophic forgetting in an online setting."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a probabilistic method for solving ODE boundary value problems (BVPs), which are ordinary differential equations subject to boundary conditions. The authors propose a Gauss–Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of wellestablished, non-probabilistic methods. The model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. The proposed method is compatible with other statistical modelling tool-chain."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper studies the problem of learning a near optimal policy for two reward-mixing Markov decision processes (RM-MDPs), where a reward function is drawn from one of multiple reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. The authors provide the first polynomial-time algorithm that finds an -optimal policy after exploring poly(H, S, A) episodes, where H is time-horizon and A is the number of states and actions respectively. This is the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes a method to estimate the conditional average treatment effect (CATE) in the multi-causal setting, i.e. the setting where there are multiple interventions on multiple variables. The proposed method, called Single-cause perturbation (SCP), consists of two steps: (1) augmenting the observational dataset with the estimated potential outcomes under single-cause interventions, and (2) performing covariate adjustment on the augmented dataset to obtain the estimator. The authors show that the proposed method is agnostic to the exact choice of algorithm in either step. Experimental results on synthetic and semi-synthetic data demonstrate the effectiveness of the method."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"This paper proposes a multi-wavelet-based neural operator learning scheme that compresses the associated operator’s kernel using fine-grained wavelets. The projected kernel is trained at multiple scales derived from using repeated computation of multiwavelet transform. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. Compared with the existing neural operator approaches, the proposed model shows significantly higher accuracy and achieves state-of-the-art in a range of datasets."
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper proposes a frequency domain approximation (FDA) method to approximate the gradient of the sign function in the Fourier frequency domain using the combination of sine functions for training BNNs. The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. The experiments on several benchmark datasets and neural architectures illustrate that the proposed method achieves the state-of-the-art accuracy."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,The authors propose to use multi-area RNNs with neuroscience-inspired architecture constraints to derive key features of multi- area computation. They show that incorporating multiple areas and Dale’s Law is critical for biasing the networks to learn biologically plausible solutions. They also show that output-relevant information is preferentially propagated between areas. These results suggest that cortex uses modular computation to generate minimal sufficient representations of task information.
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"This paper proposes a method to search for multiple saliency maps for explaining the decisions of convolutional neural networks (CNNs) for image classification. The authors argue that a single saliency map provides an incomplete understanding since there are often many other maps that can explain a classification equally well. The proposed method is based on a beam search algorithm that systematically searches for multiple explanations for each image. Results show that there are indeed multiple relatively localized explanations for many images. However, naively showing multiple explanations to users can be overwhelming and does not reveal their common and distinct structures. Therefore, the authors introduce structured attention graphs (SAGs), which compactly represent sets of attention maps for an image by visualizing how different combinations of image regions impact the confidence of a classifier. An approach to compute a compact and representative SAG for visualization is proposed via diverse sampling."
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. The authors show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks. The choice of loss has little effect when networks are fully fine-tuned on the new tasks, and the difference among loss functions are apparent only in the last few layers of the network. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks, the authors claim."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper proposes a method to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. The proposed method, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. They test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper proposes a hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where the source and target trees are treated as latent and induced during training. The authors develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. They apply this latent neural grammar to various domains—a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation—and find that it performs better than standard baselines."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a new efficient algorithm for solving the Group Elastic Net (GNET) problem. The main idea is to use the Augmented Lagrangian (AGL) to solve the GNN problem, and then extend it to the function-on-scalar regression (FOS) problem, where a functional response is modeled against a very large number of scalar predictors. The proposed algorithm, called fgen, is based on the SsNAL algorithm, which is then extended to the FOS problem using the Functional Principal Components (FPC) representation. Theoretical results are provided to show that fgen can reduce the computational cost by a factor of logarithmic factors. Empirical results on synthetic and real-world datasets demonstrate the effectiveness of fgen. "
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"This paper proposes a clustering method for structured point process data. Specifically, the authors propose a mixture model of multi-level marked point processes for identifying potential heterogeneity in the observed data. An efficient semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) is proposed for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and real data analyses."
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper proposes a meta-algorithm for multi-task adaptive nonlinear control, where the goal is to control a nonlinear system subject to adversarial disturbance and unknown environment dependent nonlinear dynamics, under the assumption that the environment-dependent dynamics can be well captured with some shared representation. The authors provide instantiations of their approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for adaptive non-linear control and integration with deep learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches in inverted pendulum and 6-DoF drone control tasks under varying wind conditions."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"Certified robust training methods have been proposed for training neural networks with certifiable robustness guarantees. The state-of-the-art (SOTA) methods including interval bound propagation (IBP) and CROWN-IBP have per-batch training complexity similar to standard neural network training, but they usually use a long warmup schedule with hundreds or thousands epochs to reach SOTA performance and are thus still costly. In this paper, the authors identify two important issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improve IBP training. To mitigate these issues and conduct faster certified training with shorter warmup, they propose three improvements: 1) They derive a new weight initialization method, 2) They propose to fully add Batch Normalization (BN) to each layer in the model, and 3) They design regularization to explicitly tighten certified bounds and balance ReLU activations during wamrup."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection in the presence of adversarial perturbations to the data. In particular, the authors propose an extension of the Huber-contamination framework, which allows the contamination distributions to be different at each time point. They show that the detection boundary is a function of the contamination proportion and is the first time shown in the literature. They also derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of contamination proportion. Finally, they propose a computationally-feasible method, matching the lower bound under certain conditions, saving for logarithmic factors."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the power of learning via mini-batch stochastic gradient descent (SGD) and batch Gradient Descent (GD) on the empirical loss of a differentiable model and asks what learning problems can be learned using these paradigms. The authors show that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision of the gradient calculations relative to the minibatch size b and sample size m (for SGD). The authors also show that with polynomially many bits of precision (i.e. when ρ is exponentially small), SGD can both simulate PAC learning regardless of the mini batch size. On the other hand, when bρ is large enough, the power is equivalent to that of SQ learning."
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"This paper considers the problem of generating a discrete probability distribution over a set of N points, which minimizes the Wasserstein distance to the model distribution. This minimization problem, where the unknowns are the positions of the atoms, is non-convex. The authors show that, in most cases, a suitably adjusted version of Lloyd’s algorithm — in which Voronoi cells are replaced by Power cells — leads to configurations with small Wassersteins error. This is surprising because, again, of the non-Convex nature of the problem, as well as the existence of spurious critical points. They provide explicit estimates for the convergence of this Lloyd-type algorithm."
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"This paper introduces a relational feature transform, dubbed the relational self-attention (RSA), that leverages the rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. The proposed RSA network substantially outperforms convolution and self attention counterparts on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1&V2, Diving48, and FineGym. "
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the second-order mean field limit of multilayer neural networks. The authors derive a system of dynamical equations that captures the limiting fluctuation distribution. They demonstrate through the framework the interaction among neurons in this second order limit, the stochasticity with cross-layer dependency and the nonlinear time evolution inherent in the limiting fluctuations. They apply the result to show a stability property of gradient descent mean field training: in the large-width regime, along the training trajectory, it progressively biases towards a solution with minimal fluctuation."
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"This paper proposes a method to learn dissipative brackets from metriplectic dynamical systems for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. In the case of added thermal noise, the authors guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. The authors provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either ""black-box"" or penalty-based approaches."
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection-based algorithm for fair and robust training. The authors formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption, and propose a greedy algorithm that is efficient and effective in practice. Experiments show that the proposed algorithm obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper proposes periodic activation functions in Bayesian neural networks (BNNs) as a way to introduce inductive biases in the function space. The authors show that these activation functions establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. They show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. They also show that for BNNs with a periodic activation function, placing a Student-t distribution on the weights corresponds to a prior in function space with Matérn covariance structure."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,This paper studies the problem of providing feedback to an agent that is tasked with learning to classify Markov Decision Processes (MDPs) from interactive student programs. The authors propose a cooperative objective between an agent and an autoregressive model that allows the agent to sample differential trajectories from the input MDP that allows a classifier to determine membership: Play to Grade. The proposed method enables an automatic feedback system for interactive code assignments.
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,This paper proposes a method for interpretable deep reinforcement learning (DRL) models. The main idea is to learn a disentangled representation of the DRL state space and a mimic tree that extracts the causal impact of the latent features on DRL action values. The proposed method is based on the idea of minimum description length (MDL) and the information bottleneck (IB) principle. The paper also proposes a Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to find the IB-optimal mimic tree. Experiments show the effectiveness of the proposed method. 
SP:84560de78af979354fff83d1370d8675c1e9191f,"This paper introduces a Gaussian latent information martingale (GLIM) framework for modeling the structure of dynamic predictions over time. The authors model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. They show that GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics. "
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration in stochastic bandit problems, where the goal is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. The authors devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. They apply FWS to various pure exploration tasks, including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits, and show that FWS is competitive compared to state-of-art algorithms."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper proposes a new method for Bayesian optimization (BO) for combinatorial spaces. The main idea is to use a surrogate model to learn a structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to SOTA methods."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper studies the role of the representation of state-action value functions in regret minimization in finite-horizon Markov Decision Processes (MDPs) with linear structure. The authors first derive a necessary condition on the representation, called universally spanning optimal features (UNISOFT), to achieve constant regret in any MDP with linear reward function. They then demonstrate that this condition is also sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms (LSVI-UCB and ELEANOR). Finally, they propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper introduces a differentiable contact model, which can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. The authors demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a simulator for downstream optimization tasks, such as planning and control."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper studies the training dynamics of deep neural networks (NNs) in terms of the Lipschitz constant of the network parameters. The authors show that the network’s training dynamics can be decomposed into two parts: (1) the bias of the first layer, and (2) the number of parameters. They show that networks with a smaller bias have a shorter bias trajectory and their bias will vary less. They also show that steady training with dropout implies a training and datadependent generalization bound that grows poly-logarithmically with the number parameters. "
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper studies the problem of distribution independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. In particular, the authors show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. The main application of this result is to obtain a new algorithm with sample complexity that scales polynomially with the sample complexity. The sample complexity and running-time of this algorithm depends polynomeially on the size of the largest outlier in the distribution."
SP:e5229305af00067ae2dbabd903e585964aec8928,"This paper proposes a Bayesian optimisation-based adversarial attack method for graph classification. The proposed method is black-box, query-efficient and parsimonious with respect to the perturbation applied. The authors empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, the authors analyze common interpretable patterns behind the adversarial samples produced."
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies online label shift adaptation in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose two adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of label shift scenarios."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper considers the problem of detecting and localization of gradual changes in the distribution of time-ordered observations. The authors propose a general method for detecting and localizing gradual changes that does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. The proposed method possesses proven theoretical guarantees for both detection and localization. "
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper proposes a biologically plausible neural network architecture for blind source separation (BSS) problems. The authors propose a novel objective function for ICA, which they use to derive biologically plausible ICA neural networks, including both the neural architecture and the synaptic learning rules. The proposed algorithm relies on modulating synaptic plasticity by the total activity of the output neurons. "
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper proposes a method to characterize the space of solutions associated with various tasks in recurrent neural networks (RNNs). Specifically, the authors study a simple two-neuron network on a task that leads to multiple solutions. They trace the nature of the final solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. They then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time Reproduction. For each task, they find a rich set of solutions. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks’ ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, they relate extrapolation patterns to specific dynamical objects and effective algorithms. They introduce a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics with regards to behavioral inputs and outputs. Using this representation, they can partition the solutions to each task into a handful of types and show that neural features can partially predict them."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper proposes a method for estimating the conditional density of a set of covariates. The proposed method, called Arbitrary Conditioning with Energy (ACE), can simultaneously estimate the distribution p(xu | xo) for all possible subsets of unobserved features xu and observed features xo. ACE is designed to avoid unnecessary bias and complexity — it specifies densities with a highly expressive energy function and reduces the problem to only learning one-dimensional conditionals. This results in an approach that is both simpler and higher-performing than prior methods. ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks."
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new adaptive weighted loss for SISR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, the authors introduce variance estimation characterizing the uncertainty on a pixel-by-pixel basis into the proposed loss function, so that the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, the uncertainty estimation allows the authors to leverage conventional wisdom such as sparsity prior for regularizing the solutions. The authors demonstrate that such uncertainty-driven loss can achieve better results than MSE or L1 loss for a wide range of network architectures."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"This paper proposes a general PAC-Bayesian generalization bound for adversarial robustness, i.e., how much a model will be invariant to imperceptible perturbations in the input at test time. The main idea is to study the averaged risk on the perturbation for majority votes (over the whole class of hypotheses) and derive bounds that provide general bounds that are valid for any kind of attacks (i.e. adversarial attacks) and tight thanks to the PAC-bayesian framework, that can be directly minimized during the learning phase to obtain a robust model on different attacks.   The main contribution is mainly theoretical and does not appear to directly lead to potentially negative social impact."
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper proposes a probabilistic entity representation model (PERM) for logical reasoning over Knowledge Graphs (KGs). The proposed model encodes entities as a multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. It also defines the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. On a COVID-19 drugrepurposing case study, it is able to recommend drugs with substantially better F1 than current methods."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"This paper proposes a new gradient-based hyperparameter optimization method for few-shot meta-learning. The proposed method is based on forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward- mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. The authors provide theoretical guarantees about the noise reduction properties of the algorithm, and demonstrate its efficiency empirically by differentiating through 10 gradient steps of unrolled optimization on CIFAR-10."
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper proposes a method to improve the coherence and consistency of neural sequence models by adding a symbolic reasoning module that can either accept or reject the generations of the neural sequence model. The authors propose to use neural inference to mediate between the neural System 1 and the logical System 2. Experiments on two tasks, robust story generation and grounded instruction following, show that the proposed method can improve the accuracy and coherence of neurally-based generations."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper considers the problem of off-policy evaluation (OPE) in continuous treatment settings, where one aims to estimate the mean outcome under a new treatment decision rule using historical data generated by a different decision rule. The authors develop a novel estimation method for OPE using deep jump learning. The key ingredient of the method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multiscale change point detection. The method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing."
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a variational inference framework for continuous-time hybrid dynamical systems. The proposed method is based on a Markov jump process modulating a subordinated diffusion process. The authors provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are computationally intractable. Therefore, they develop a new continuous time inference algorithm, which combines a Gaussian process approximation on the diffusion level with posterior inference for Markov Jump Processes. By minimizing the path-wise Kullback-Leibler divergence, they obtain (i) Bayesian latent state estimates for arbitrary points on the real axis and (ii) point estimates of unknown system parameters, utilizing variational expectation maximization. "
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the effect of the spectrum of the sensing matrices on the difficulty of recovering x from y in the expectation propagation (EP) algorithm. The authors define a notion for the spikiness of A and show the importance of this measure in the performance of the EP. Based on this framework, they are able to show that for instance, in phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries."
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper proposes a method to address the domain shift problem of zero-shot learning (GZSL) by progressively improving cross-domain transferability and category discriminability of visual representations. The proposed method constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. The attribute prototypes alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. The category prototypes are further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts the category discrimination ability. Experiments on four benchmarks prove the effectiveness of the proposed method."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus blurring kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network (DNN) is developed by unrolling a fixed-point iteration of the GKM-based deblurring. Extensive experiments show that the proposed method can achieve state-of-the-art results on existing datasets."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper proposes a method for self-supervised video representation learning. The proposed method is based on motion vectors and cross-guidance contrastive learning, where the motion vectors can take supervision signals from RGB frames and vice versa. The motion vectors are decoded on-the-fly from compressed videos and cross guidance contrastive loss is used to enhance the representation ability of the motion vector. The method is evaluated on two downstream tasks and achieves state-of-the art performance. "
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"This paper proposes an extension of the cubic spline kernel for Bayesian neural networks (BNNs) to deal with the issue of asymptotic overconfidence of BNNs far from the training data. In particular, the authors extend finite ReLU BNN with infinite ReLU features via the GP and show that the resulting model is maximally uncertain far away from the data while the BNN’s predictive power is unaffected near the data. They also show how the proposed method can be applied post-hoc to any pre-trained BNN and causes only a small overhead during prediction."
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of selecting a causal quantity of interest among a set of available formulas. The authors assume an sequential setting in which the investigator may alter the data collection mechanism in a data-dependent way with the aim of identifying the formula with lowest asymptotic variance in as few samples as possible. They formalize this setting by using the best-arm-identification bandit framework, where the standard goal of learning the arm with the lowest loss is replaced with the goal to learn the arm that will produce the best estimate. They introduce new tools for constructing finite-sample confidence bounds on estimates of the variance that account for the estimation of potentially complex nuisance functions. They validate their method by providing upper bounds on the sample complexity and an empirical study on artificially generated data."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper proposes a new compression scheme for variance reduced stochastic gradient descent (SGD) algorithms. The compression scheme is based on the compression error of the last two steps of SGD. The authors show that adding back the previous step’s compression error does not fully compensate the previous two steps’ compression error. Instead, the authors propose to use the compressed gradient from the first two steps. Theoretical results show that the proposed method achieves the same asymptotic convergence rate with the training without compression. Numerical experiments are also provided to show the performance of the proposed algorithm."
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper proposes a generative probabilistic model, ReFine, to generate multi-grained explanations for graph neural networks (GNNs). Specifically, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classification."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,This paper proposes a method to generate robust counterfactual explanations on Graph Neural Networks (GNNs) by explicitly modelling the common decision logic of GNNs on similar input graphs. The proposed method extracts decision boundaries from the given GNN model to formulate an intuitive and effective and effective Counterfactual loss function. The authors optimize this loss to train a neural network to produce explanations with strong counterfactuality characteristics. Experiments on synthetic and real-life benchmark datasets demonstrate the efficacy of the proposed method.
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper proposes a method to improve the disentanglement and transfer performance of voice conversion systems. The proposed method is based on self-supervised representation learning and adversarial feedback. The authors propose to decompose the content and voice style of the converted speech into content and style information, which is then used to train a discriminator. The discriminator is decomposed into a content discriminator and a style discriminator, which enables the model to achieve better generalization to the voice style. The experimental results show the superiority of the proposed method. "
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,This paper proposes a Siamese voxel-to-BEV (V2B) tracker for 3D single object tracking on sparse point clouds. V2B consists of two parts: 1) Shape-aware feature learning network to learn the discriminative features of the object; 2) Target localization network to regress the target’s 2D center and the z-axis center from the dense bird's eye view (BEV) feature map in an anchor-free manner. Extensive experiments on the KITTI and nuScenes datasets show the effectiveness of the proposed method.
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a positional encoding method based on learnable Fourier features for multi-dimensional positional encoding. In particular, the authors propose to represent each position in an image as a learnable feature mapping, which is modulated with a multi-layer perceptron. The proposed method is evaluated on image generation, object detection, image classification, and sparse spatial structure modeling in user interfaces. "
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of learning the causal structure of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. This paper proposes a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of the approach is that at each iteration a specific type of variable is identified and removed. This technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests."
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"This paper studies the problem of online decision making in stochastic multi-armed bandit and linear contextual bandit. The authors propose a batch Thompson Sampling (BTS) algorithm for online decision-making problems, which is based on Thompson sampling. BTS is a variant of Thompson sampling, and the authors show that it achieves the same (asymptotic) regret bound as a fully sequential one while carrying out only O(log T ) batch queries. To achieve this exponential reduction, the authors propose to dynamically decide the duration of each batch in order to balance the exploration-exploitation trade-off. They also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines."
SP:653a519e3c799c25e0d0b4240322642040b121a3,"This paper studies the problem of domain adaptation (DA) and domain generalization (DG) in the setting of multiple source DA and multiple target DA. The authors develop theoretical upper bounds for the target general loss and define two kinds of domain-invariant representations: general DA representation for learning invariant classifier which works on all source domains and compressed DI representation motivated from reducing inter-domain representation discrepancy. They further characterize the properties of these representations, and develop a lower bound on the target loss which governs the trade-off between learning them. They conduct experiments on Colored MNIST dataset and real dataset to illustrate their theoretical claims."
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes an aligned structured sparsity learning (ASSL) strategy to train efficient image super-resolution (SR) networks with smaller model size and lower computation than state-of-the-art methods. ASSL introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. To align the pruned filter locations across different layers, the authors propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. The proposed ASSL achieves superior performance gains over recent methods quantitatively and visually."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper proposes a novel exploration method for cooperative multi-agent reinforcement learning (MARL). The authors leverage the insight of factorized MARL algorithms that the “induced” individual Q-values, i.e., the individual utility functions used for local execution, are the embeddings of local action-observation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, they use prediction errors of individual Q values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. The authors demonstrate the advantages of their method by didactic examples, and demonstrate its significant outperformance over state-of-the-art MARL baselines on challenging tasks in the StarCraft II micromanagement benchmark."
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"This paper studies the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. The main result is a Statistical Query (SQ) lower bound of d for this problem. The SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a method to incorporate expert domain knowledge into ML models for predicting patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. The proposed method, called latent hybridisation model (LHM), integrates a system of expert-designed ODEs with machine-learned Neural ODE to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. The authors evaluated LHM on synthetic data and real-world intensive care data of COVID-19 patients."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper studies the problem of few-shot representation learning in the context of meta-learning. In particular, the authors consider the setting where there are $n$ tasks and the goal is to learn a representation for each of them. In this setting, they assume that there is a shared structure between the MDPs, i.e., there is an initial representation $x_i$ that is shared across all tasks, and that the task-specific representation $y_i(x_j)$ is learned by finetuning via gradient descent. The authors provide theoretical analysis of the performance of the proposed method, MAML, and show that it can be shown to outperform other methods that do not consider the shared structure assumption. They also show that in the worst-case, any such algorithm cannot outperform directly learning the target task."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of the approach is a collection of lexicon entries, which map each word to a tuple of syntactic type and a neuro-symbolic semantic program. To facilitate learning in an exponentially-growing compositional space, the authors introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time. Experiments on visual reasoning and language-driven navigation domains demonstrate the data efficiency and compositional generalization capability of the proposed approach."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper studies distributed stochastic convex optimization in the setting of quasi-self-concordant objectives. In this setting, each machine can have access to both independent and Hessian-vector products of the population objective. The authors propose a distributed algorithm for solving the convex quadratic optimization problem, which they call Stochastic Newton. They show that the proposed algorithm can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance."
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes a new similarity measure named Density-aware Chamfer Distance (DCD), which is derived from Chamfer distance (CD) and is more computationally efficient than Earth Mover’s Distance (EMD). The authors claim that DCD can detect disparity of density distributions and is thus a more intensive measure of similarity compared to CD. The authors also propose a novel point discriminator module that estimates the priority for another guided downsampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper studies the effect of knowledge distillation (KD) on the generalization ability of a student network trained to imitate a teacher network. The authors show that, while KD can improve generalization, it does not typically work as commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. They identify difficulties in optimization as a key reason for why the student is unable to match the student. They also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalization."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper studies the problem of computing a coreset for k-decision trees, i.e. a tree of matrices of size k for which the loss function is the sum of squared differences over all matrices in the matrix and the labels of each of the matrices. The paper provides an algorithm for computing the coreset, which is a summarization of the decision tree of the matrix. The coreset size is polynomial in k log(N/\epsilon) and the construction time is O(N^k). The paper shows that the optimal k-tree of C is a (1 + \eps)-approximation of the optimal tree of matrix D. "
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the problem of identifying the $m$ arms with the largest means under a fixed error rate $\delta$ (fixed-confidence Top-m identification) for misspecified linear bandit models. This problem is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms, but in which data inevitably deviates from linearity. In this work, the authors first derive a tractable lower bound on the sample complexity of any \delta-correct algorithm for the general Top-M identification problem. They then describe the first algorithm for this setting, which is both practical and adapts to the amount of misspecification. The authors derive an upper bound to its sample complexity which confirms this adaptivity and that matches the lower bound when \� → 0. Finally, they evaluate their algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a disentangled graph representation learning method for self-supervised learning of graph neural networks (GNNs). The key idea is to disentangle the latent factors of the input graph and derive its factorized representations. Each of the representations describes a latent and disentanglement aspect pertinent to a specific latent factor of the graph. Then, the authors propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method."
SP:0a7edbbdabab11273689c40c517001eb46491113,This paper proposes a statistical simulation to assess the robustness of a neural network to input uncertainty. The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The procedure is based on an Importance Splitting simulation generating samples of rare events. The authors derive theoretical guarantees that are nonasymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of the method making a low number of calls to the network function.
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper introduces a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto-and cross-correlations. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges to image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper proposes a neural tangent kernel (NTK) based two-sample test for Maximum Mean Discrepancy (MMD) statistic. Theoretically, the authors show the connection between NTK and MMD and show that the NTK-MMD statistic has the same computational complexity as the MMD statistic, but is computationally efficient and memory-efficient. Experiments on both synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed method."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"This paper proposes a variational autoencoder-based approach to decompose an input image x into x = G(x)+R(x), where R(x) captures the essential information for classification and G covers all the class-redundant information. The authors further propose an objective to jointly train the VAE and classifier to gain such class-disentanglement capability. Experiments show that the proposed approach substantially improves the detection and defense against different types of adversarial attacks."
SP:2789874561620ba7894c4672f935056bb911e919,"This paper proposes a federated learning algorithm that is differentially private (DP-DP) for Bayesian optimization (BO) with Thompson Sampling (FTS) in the federated setting. The proposed algorithm is built on top of the recent work of [1] that integrates differential privacy (DP) into FTS to preserve user-level privacy. The authors also leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of the proposed algorithm through distributed exploration (DE). The resulting DP-FTS-DE algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off."
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"This paper proposes a Gaussian process-Bayesian Bernoulli Mixture model (GP-BM) for multi-label active learning (ML-AL). The BM encodes label correlations using a BayesianBernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. To tackle highly sparse labels under AL, the BM is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. A novel auxiliary variable based variational inference algorithm is developed to tackle the non-conjugacy introduced along with the mapping process for efficient end-to-end posterior inference. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. A principled sampling function is designed accordingly to capture both the feature uncertainty (through GP) and label correlations (through BM) for effective data sampling."
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"This paper proposes a streaming method for 3D object detection, segmentation and segmentation on the nuScenes dataset. The proposed method is based on the polar coordinate system, which is a more compact representation for lidar sectors compared to previous methods. The authors propose to use multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following one from the past scan. They also introduce feature undistortion and range stratified convolutions to improve the core polar convolutional architecture. Experimental results show that the proposed method can achieve comparable results with lower latencies."
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper studies the problem of learning with structured latent variables. The authors propose to use score function-based gradient estimators instead of differentiable surrogates to estimate the gradient of the latent variable. The main contribution of this paper is to extend the Gumbel-Max trick to the case of structured random variables. In particular, the authors propose a family of recursive algorithms with a common feature they call stochastic invariant. The feature allows them to construct reliable gradient estimates and control variates without additional constraints on the model. In the experiments, they consider various structured latent variable models and achieve results competitive with relaxation-based counterparts."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper proposes a method for adaptive denoising of deep convolutional neural networks (CNNs) based on a single multiplicative scaling parameter (the ""Gain"") of each channel in the convolution layers of the CNN. The proposed method is evaluated on standard image-denoising benchmarks, where it is shown to improve performance on nearly every image in a held-out test set. The adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type. The authors also demonstrate the potential of adaptive GainTuning in a scientific application to transmission electron microscopy images, using a CNN that is pre-trained on synthetic data."
SP:90afa1102683b456bc72a54abef466326827546a,This paper proposes a fully differentiable architecture for semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multi-way cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a label. The formulation allows to directly maximize a smooth surrogate of the panopti-quality metric by backpropagating the gradient through the optimization problem. Experimental results on Cityscapes and COCO datasets demonstrate the effectiveness of the proposed method.
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes Recursive Bayesian Networks (RBNs), which generalizes and unifies PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. This paper provides two solutions: 1) generalizing inside and outside probabilities from PCFG to the mixed discrete-continuous case, and 2) providing an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference."
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a constrained backpropagation (CBP) algorithm based on the pseudo-Lagrange multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. The defining characteristic of the proposed CBP algorithm is the utilization of a Lagrangian function (loss function plus constraint function) as its objective function. The authors considered various types of constraints — binary, ternary, one-bit shift, and two bit shift weight constraints. As a post-training method, CBP applied to AlexNet, Resnet-18, ResNet-50, and GoogLeNet on ImageNet. For most cases, the proposed algorithm outperforms the existing methods."
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper studies the problem of active learning for Gaussian process classification (GPC). In particular, the authors propose efficient algorithms for EER-based active learning with GPC, which estimate the error reduction by querying instances based on the joint distribution of label pairs. They derive the joint predictive distribution as a one-dimensional integral, which allows them to avoid retraining the GPC for each query, reducing the computational overhead. They also derive the gradient chain rule to efficiently calculate the gradient of the acquisition function, which leads to the first query synthesis active learning algorithm implementing EER based strategies."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper studies the effect of unbounded gradients on the regularization of variational autoencoders (VAE) on the risk of over- and under-regularization. The authors prove that unbounded gradient is necessary for guaranteeing that global minima of canonical AE architectures will coincide with optimal sparse representations, meaning high fidelity reconstruction of the training data using the minimal number of informative latent dimensions. They also show that heuristic modifications to or constraints on the VAE energy function may be ill-advised, and large gradients should be accommodated to the extent possible."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the problem of bandit problem with graph feedback. The authors propose the notions of fractional weak domination number and the k-packing independence number to capture upper bound and lower bound for the regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual — the fractional vertex packing set respectively. Based on this connection, they utilize the strong duality theorem to prove a general regret upper bound O(\delta,\log V) and a lower bound $\Omega(\log V/\log T)$ where $\log V$ is the integrality gap of the dual linear program. The upper bound is tight up to a (log |V|) 1.3 factor for the vertex packing problem including trees and graphs with bounded degree."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper proposes a new neighbourhood Shapley value (Nashley value) based on the Nadaraya-Watson estimator, which is a well-studied kernel regressor that can be expressed as a self-normalised importance sampling estimator. The authors show that this formulation improves the local interpretability of Shapley values. The paper also shows that the proposed neighbourhood shapley values can identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, and increase on-manifold explainability."
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes a method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, it augments the actions to generate a large amount of virtual state-action trajectories. The proposed method achieves the state-of-the-art performance on the Atari and DeepMind Control Suite benchmarks."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,This paper investigates the question of how the network’s architecture impacts its robustness to noisy labels. The authors provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. They hypothesize that a network is more robust if its architecture is more aligned with the target function than the noise. They provide both theoretical and empirical evidence across various neural network architectures and different domains.
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes a data-driven approach to control, where examples of success states are used in place of a reward function. The proposed method estimates the probability of reaching a success example in the future and optimizes a policy to maximize this probability of success. Unlike prior imitation learning methods, this approach is end-to-end and does not require learning a reward functions. The method is simpler, with fewer hyperparameters and fewer lines of code to debug. Experiments show that this approach outperforms prior methods that learn explicit rewards functions."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization in convex and non-convex settings. In the convex case, the authors focus on the family of non-smooth generalized linear losses (GLLs). The algorithm for the l2 setting achieves optimal excess population risk in near-linear time, while the best known algorithms for general convex losses run in superlinear time. For the l1-case with smooth losses and polyhedral constraint, they provide the first nearly dimension independent rate, O(\log d (n\epsilon)1/3) in linear time for the constrained l2-case. They also extend all their results above for the non- convex l2 settings to the lp setting with only polylogarithmic overhead in the rates."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies the cooperative bandit problem in three settings: stochastic time-varying networks, instantaneous reward sharing over a network with random delays, and adversarially corrupted rewards. The authors propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well. In the setting with perfect communication, the authors present an improved delayed-update algorithm that outperforms the existing state-of-the-art on various network topologies. Finally, they present tight network-dependent minimax lower bounds on the group regret."
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a post-training quantization method for vision transformers to reduce the memory storage and computational cost of the network. To preserve the functionality of the attention mechanism, the authors introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, they thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the convergence rate of double Q-learning, which is an extension of the work of Xiong et al. (2020) where the polynomial learning rate was adopted in the analysis typically yields a slower convergence rate. This paper tackles the more challenging case of a constant learning rate, and develops new analytical tools that improve the existing convergence rate by orders of magnitude. Specifically, the authors show that synchronous double-Q-learning attains an accurate global optimum with a time complexity of $\tilde{O}(\ln(d^2\log(1/\epsilon^2)^{-\gamma)^7/2)$, where $\gamma$ is the cardinality of the state-action space, $D$, is the discount factor, and $L$ is a parameter related to the sampling strategy for asynchronous double-q-learning. "
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper proposes a new semi-supervised learning setting for out-of-distribution (OOD) detection. In this setting, only limited labeled data and many mixed unlabeled data is used for OOD detection. The proposed method, called Structure-Keep Unzipping (Steps) approach, learns a new representation space in which OOD samples could be separated well. An efficient optimization algorithm is derived to solve the objective. Comprehensive experiments are conducted to demonstrate the effectiveness of the proposed method."
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper proposes a transformer-based model for referring expression comprehension (REC) and segmentation (RES) tasks. The main idea is to fuse the visual-lingual encoder and the decoder of ViLBEERT, where the visual encoder is used to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. Experiments are conducted on both REC and RES datasets and show the effectiveness of the proposed method."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of multiclass boosting, where the weak learner is an agnostic PAC learner for a base class, and the goal is to learn a combination of weak and moderately inaccurate hypotheses to a strong and accurate one. The weak learners are assumed to belong to an “easy-to-learn” base class. The goal of the overall boosting algorithm is then to learn by repeatedly calling the weak learners. The paper shows the resources required for boosting, especially how they depend on the number of classes k, for both the booster and weak learners, and find that the boosting algorithm itself only requires O(log k) samples, as they show by analyzing a variant of AdaBoost for our setting. In stark contrast, assuming typical limits on the numbers of weak-learner calls, they prove that the total number of samples required by a weak learners is at least polynomial in k. "
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper proposes a method for unsupervised object segmentation and object-centric scene generation. The proposed method is based on an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative refining. Experiments on the Sketchy and APC datasets show that the proposed method outperforms MONET-G and other baselines."
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper proposes an adaptive approach to conformal inference, where the data generating distribution is allowed to vary over time in an unknown fashion. The authors model the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. They show that their adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. They test their method on two real world datasets and find that its predictions are robust to visible and significant distribution shifts."
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"This paper proposes a Pose-level Inference Network (PINet) for multi-person pose estimation in crowded scenes. PINet first applies the Part-based Pose Generation (PPG) to infer multiple coarse poses for each person from his/her body parts. Those coarse poses are refined by the Pose Refinement module through incorporating pose priors, and finally are fused in the Pose Fusion module. Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet."
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,"This paper studies the problem of solving robust Markov decision processes (RMDPs) with L_infty-constrained rectangular ambiguity sets. The authors propose a homotopy continuation method and a bisection method to solve RMDPs with S-rectangular ambiguity in quasi-linear time in the number of states and actions. Theoretical analysis shows that the worst-case time complexity is O(SA log(S)). The algorithms also perform well in practice, outperforming a leading LP solver by several orders of magnitude."
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper studies the online knapsack problem with very weak predictions, in the form of knowing an upper and lower bound for the number of items of each value. The authors systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction. They also extend the results to more general settings such as generalized one-way trading and two-stage Online Knapsack. "
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes a model-based episodic memory of trajectories to improve the sample efficiency of episodic control. The proposed method stores distributed trajectories produced by a trajectory model, uses memory-based planning with fast value-propagating memory writing and refining, and consolidates episodic values to parametric value function. Experiments show that the proposed method improves the performance in various RL tasks."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"This paper proposes a data programming scheme to generate probabilistic labels for unlabeled data in semi-supervised learning (SSL). The authors propose a label model to resolve the conflict and overlap among the noisy labels, and then infer the label for unlabelled data. The label model consists of two parts: (1) a label generator, and (2) an end model that predicts the labels. The labeling functions are learned using a multi-choice learning (MCL) approach. The proposed method is evaluated on four standard SSL benchmarks and achieves better classification performance than existing SSL methods."
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,This paper proposes a multi-view pose transformer (MVPT) for estimating multi-person 3D poses from multiple views. MVPT represents skeleton joints as learnable query embeddings and let them progressively attend to and reason over the multi- view information from the input images to directly regress the actual 3D joint locations. The paper also introduces a RayConv operation to integrate the view-dependent camera geometry into the feature representations for augmenting the projective attention. Experiments on the challenging Panoptic dataset show that the proposed approach achieves better performance than the existing methods.
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of support recovery and approximate recovery of unknown sparse vectors from a mixture of sign responses. In the first problem, the goal is to learn the supports of all vectors from the family using a sequence of noisy responses, where each response shows the sign of the inner product between a randomly chosen vector and the query vector. The second problem is to design queries such that all sparse vectors can be approximately reconstructed based on the error-free responses. The main contribution of the paper is to prove the existence of learning algorithms for both problems, which work without any assumptions."
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper considers the problem of changepoint detection, i.e. detecting abrupt changes in temporal behavior patterns. The authors propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. They derive expected delay bounds for the proposed scheme and show that these bounds match the information-theoretic lower bounds at low false alarm rates, establishing optimality of the proposed method. They then perform a number of experiments on synthetic and real datasets demonstrating the effectiveness of their method."
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper studies the problem of solving stochastic nested optimization problems, i.e. problems that have a nested structure. In particular, this paper unifies several SGD-type updates into a single SGD approach that they term ALternating Stochastic Gradient dEscenT (ALSET) method. The main contribution of this paper is to provide a tighter analysis of ALSET for nested problems. Under the new analysis, to achieve an -stationary point of the nested problem, it requires O( −2) samples in total. Under certain regularity conditions, applying the proposed method to the min-max, compositional and reinforcement learning problems either improves or matches the best-known sample complexity."
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper proposes a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy consists of two modules: (1) siamesed knowledge generation to learn the inter-relationship among clips; (2) Siameses knowledge reasoning to produce the refined soft label by propagating the weights of inter-relation to the predicted candidates of all clips. Extensive experiments demonstrate that the proposed method achieves state-ofthe-art performance on five VideoQA benchmarks."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a low-rank factorization approach to reduce the computational and memory complexity of large class of structured probabilistic models. The authors show that by viewing the central inference step as a matrix-vector product, they can trade off model expressivity and speed via the rank. Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that the approach matches the accuracy of standard models at large state spaces while providing practical speedups."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper introduces Sample Average Uncertainty (SAU), a simple and efficient uncertainty measure for contextual bandits. SAU is a frequentist approach that directly estimates the uncertainty of the outcomes based on the value predictions. The authors show theoretically that the uncertainty measure estimated by SAU asymptotically matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. In addition, the authors show empirically that SAU-based exploration outperforms current state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost."
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper proposes a method to disentangle dynamic behavioral factors from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder to learn robust behavioral embeddings from unlabeled, multi-view, high-resolution behavioral videos across different animals and multiple sessions. They further combine DBE with a stochastic temporal model to propose Variational Disentangled Behavior Embedding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks such as fine-grained behavioral motif generation and behavior decoding."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,This paper proposes a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. The core of DMTET includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh.
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper proposes sliced mutual information (SMI) as a surrogate measure of statistical dependence. SMI is defined as an average of MI terms between one-dimensional random projections. The authors show that SMI preserves many of the structural properties of classic MI, while gaining scalable computation and efficient estimation from samples. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof. Numerical experiments demonstrate the potential gains SMI offers over classic MI for high-dimensional inference."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) for both sequential and batch settings. The main idea is to use a likelihood-ratio-based unbiased estimator of the gradient of the optimal acquisition function instead of using the reparameterization trick for more efficient derivative-based optimization of non-myopic acquisition functions in the unconstrained setting, like sample average approximation and infinitesimal perturbation analysis. The proposed method substantially improves both query efficiency and computation time over the one previous method focused on this class of problems. The numerical experiments show that the proposed method improves query efficiency by 2x or more over previous methods, and in some cases by 10x or 10x."
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper introduces Multi-Dimensional Distributional DQN (MD3QN) which extends distributional RL to model the joint return distribution from multiple reward sources. The authors prove the convergence for the joint distributional Bellman operator and build an empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distributions and its Bellman target. In experiments, the authors show that the proposed method accurately models the joint reward distribution in environments with richly correlated reward functions and outperforms previous RL methods in the control setting."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper introduces a geometric deep learning model for efficiently reconstructing high-resolution, accurate, and regular triangular meshes from volumetric images. The authors develop a lightweight neural network to predict a dense 3D flow vector field from a 3D image. Then, they describe a new Diffeomorphic Mesh Deformation (DMD) module, which is parameterized by a set of diffeomorphic mappings. This includes the derivation of numerical conditions for recasting the continuous flow ODE problem into an efficient discrete solver. "
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper studies the problem of data deletion in the non-convex setting, where the goal is to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. In this paper, the authors provide a general reduction from deletion guarantees against adaptive sequences to deletion guarantees for non-adaptive sequences, using differential privacy and its connection to max information. They also provide a practical attack against the SISA algorithm of Bourtoule et al."
SP:7150006590e268ab732c9be6c9048f67a377f956,This paper studies the problem of CVaR optimisation in Bayes-adaptive Markov decision processes (MDPs). The authors reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search (MCTS) and Bayesian optimisation. They show that a policy optimising the conditional value at risk (CVaR) in this setting is risk-averse to both the epistemic uncertainty due to the prior distribution over MDPs and the aleatoric uncertainty of the environment. The authors also show that the proposed algorithm outperforms baseline approaches.
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model."
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"This paper proposes a method to detect coordinated groups on social media based on neural temporal point process (NTP) with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, it jointly learns a Gibbs distribution of group assignment based on how consistent an assignment is to the account embedding space and the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, the authors design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of the proposed method compared to state-of-the-art model in both unsupervised and semi-supervised settings."
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper studies the problem of binary classification in the neural tangent kernel (NTK) regime, where the network depth plays the role of a fitting resource in solving the classification problem. In particular, the authors show that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization. This is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties."
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes two improvements to the auxiliary classifier GAN (ACGAN) framework, i.e., the Data-to-Data Cross-Entropy loss (D2D-CE) and the Rebooted Auxiliary Classifier Generative Adversarial Network (ReACGAN). The first contribution is to identify that gradient exploding in the classifier can cause an undesirable collapse in early training, and projecting input vectors onto a unit hypersphere can resolve the problem. Second, the authors propose the Data to data cross-entropy loss to exploit relational information in the dataset. The experimental results show that ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets."
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper proposes two extensions of the policy space response oracles (PSRO) algorithm for two-player zero-sum games. The first one, Extensive-form Double Oracle (XDO), is an extensive-form double oracle algorithm that is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. The second one, Neural XDO (NXDO) is a deep reinforcement learning (RL) algorithm that learns the best response at every infostate. The authors show that XDO and NXDO outperform PSRO and NFSP on a sequential multidimensional continuous-action game."
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper proposes a permutation-invariant variational autoencoder for graph structured data. The proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. The authors demonstrate the effectiveness of the proposed model for graph reconstruction, generation and interpolation."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"This paper proposes to decouple the depth and scope of GNNs by extracting a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into “white noise”. Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, the design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,This paper studies the universal approximation properties of affine-coupling normalizing flows. The authors show that any log-concave distribution can be approximated using well-conditioned affine coupling flows. They also show that affine couplings can be used to approximate a padded version of the input distribution with iid Gaussians.
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper studies the problem of online coupon allocation in the online e-commerce market. The authors propose a method based on reinforcement learning and evaluation with $\lambda$-generalization (BCORLE(\lambda)$ framework to solve the problem. Specifically, the proposed method can help enterprises develop a coupons allocation policy which greatly improves users’ retention rate on the platform while ensuring the cost does not exceed the budget. Experimental results on the simulation platform and real-world market validate the effectiveness of the proposed approach."
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a method for source-free domain adaptation (SFDA), where the source pretrained model is adapted to the target domain in the absence of source data. The method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. To capture this intrinsic structure, the authors define local affinity of the target data and encourage label consistency among data with high local affinity. The authors observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. The experimental results verify that the inherent structure of target features is an important source of information for domain adaptation."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a method for pooling features from sets into fixed-dimensional embeddings based on the sliced-Wasserstein distance. The proposed method treats elements of a set as samples from a probability distribution and proposes an end-to-end trainable Euclidean embedding for sliced-wasserstein distances to learn from set-structured data effectively. Experiments on point cloud, graph, and image classification tasks demonstrate that the proposed method provides superior performance over existing set representation learning approaches."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper proposes a new family of RNNs, called SBO-RNN, that can be formulated using stochastic bilevel optimization (SBO), where the lower-level optimization is mapped to the sub-network for computing hidden features and the upper- level optimization defines the predictor based on the computed features. The authors prove that under mild conditions, there is no vanishing/exploding gradient in the training of our new RNN, and thus our training is easy and stabilized. They demonstrate the superior performance of their approach on several benchmark datasets with faster convergence."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"This paper studies the online problem of minimizing power consumption in systems with multiple power-saving states. During idle periods of unknown lengths, an algorithm has to choose between power- saving states of different energy consumption and wake-up costs. The authors develop a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods. The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem."
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper proposes a mathematical framework for quantifying the transferability in multi-source transfer learning problems, with both the task similarities and the sample complexity of learning models taken into account. In particular, the authors consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure the transferable. Then, they demonstrate the analytical expression of this transferability measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. Furthermore, they apply their analyses for practical learning tasks, and establish a quantifiable transferability measures by exploiting a parameterized model. In addition, they develop an alternating iterative algorithm to implement their theoretical results for training deep neural networks in multi source transfer learning tasks."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"This paper proposes a neural network-based method for visual search. The proposed method is based on the observation that humans exhibit search asymmetry, i.e. finding a target among distractors can be easier than finding the target among other distractors. The authors propose a method that takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The model integrates eccentricity-dependent visual recognition with target-dependent top-down cues and is trained on augmented versions of ImageNet. "
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"This paper studies the problem of certifiable robustness, i.e. training a classifier to be robust to adversarial perturbations. In particular, the authors study the loss landscape of linear relaxation-based methods, and they find that the current state-of-the-arts method often has a landscape with favorable optimization properties. They then propose a new method that satisfies the two criteria: tightness of the upper bound on the worst-case loss and smoothness of loss landscape. The proposed method can achieve a decent performance for a wide range of perturbation."
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper considers the problem of online linear regression in the stochastic setting. The authors derive high probability regret bounds for online ridge regression and the forward algorithm. This enables them to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. Moreover, they explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. Last, they provide numerical experiments to illustrate their results and endorse their intuitions."
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper proposes a two-time-scale and anchored extragradient method, named FEG, for smooth structured nonconvex-nonconcave problems. The proposed FEG has an accelerated O(1/k2) rate, with respect to the squared gradient norm, for the Lipschitz continuous and negative comonotone operators for the first time. The FEG also has value for smooth convex-concaves problems, compared to existing works. "
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies the problem of uniformity testing for ranking data, where the alternative class is restricted to Mallows models. The authors show that uniform distribution can be distinguished from Mallows model with O(m 1/2) samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if m is large enough. They also consider uniforming testing with central and local differential privacy (DP) constraints. They present a central DP algorithm that requires O(max{1/\�0, 1/ p m}), where $\�0$ is the privacy budget parameter. Interestingly, the uniforming algorithm is straightforward to apply to the local DP scenario, since it works with binary statistics that are extracted from the ranking data."
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"This paper studies a general greedy score-based algorithm for learning directed acyclic graphs (DAGs). In particular, the authors show that recent polynomial-time algorithms for learning DAG models are a special case of this algorithm, thereby illustrating how these order-based algorithms can be interpreted as score based algorithms. This observation suggests new score functions and optimality conditions based on the duality between Bregman divergences and exponential families, which the authors explore in detail. The authors also provide extensive experiments suggesting that this algorithm indeed optimizes the score in a variety of settings."
SP:b60989706296b963b6671c01f22384978a334be1,This paper proposes a method to improve the adversarial robustness of convolutional neural networks (CNNs) by dilating the architecture of CNNs. The main idea is to dilate the architecture to increase the robustness while maintaining a competitive standard accuracy with a straightforward constraint. The framework is called Neural Architecture Dilation for Adversarial Robustness (NADAR). Experiments on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance accuracy and robustness.
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs), where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. The authors propose a new provably efficient algorithm, called UCRL-RFE under the linear Mixture MDP assumption, where the agent works in two phases. In the exploration phase, the agent interacts with the environment and collects samples without the reward, and in the planning phase, it uses the samples collected from exploration phase to learn a good policy. They show that to obtain an $\epsilon$-optimal policy for arbitrary reward function, the proposed algorithm needs to sample at most $\tilde{O}(H^5d^2^2)$ episodes during the Exploration phase. They also propose a variant of UCRLRFE using Bernstein-type bonus and show that the sample complexity is $\Omega(\H^d(H + d)^{+ d)$ for this algorithm. The sample complexity matches the lower bound in terms of the dependence on the accuracy and feature dimension."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a method for forecasting future activities in data streams with seasonal patterns that evolve over time. The proposed method, Shifting Seasonal Matrix Factorization (SSMF), can adaptively learn multiple seasonal patterns (called regimes) and switch between them. The method is based on a lossless data compression scheme. The authors demonstrate that the proposed method outperforms state-of-the-art baseline methods on three real-world data streams."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper proposes a novel neural network architecture for solving assignment problems. The core module, feature weaving layer, is stacked to model frequent communication between elements in a parameter-efficient way for solving the combinatorial problem of assignment. The experimental results showed its impressive performance among the learning-based baselines."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper systematically studies the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, they study MLP-based (PointNet), convolution-based neural network (DGCNN), and transformer-based network (PCT) 3D architectures. Through extensive experimentation, they demonstrate that appropriate applications of self supervision can significantly enhance the robustness of 3D deep learning models against adversarial attacks. The analysis reveals that local feature learning is desirable for adversarial robustness in point clouds since it limits the adversarial propagation between the point-level input perturbations and the model’s final output."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,This paper considers the problem of computing iterative projections of close-by points over submodular base polytopes. The authors propose a toolkit to speed up the computation of projections using both discrete and continuous perspectives. They also adapt the away-step Frank-Wolfe algorithm to use this information and enable early termination. Theoretical results show orders of magnitude reduction in runtime.
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,This paper considers the problem of learning the natural parameters of a k-parameter minimal exponential family from i.i.d. samples in a computationally and statistically efficient manner. The authors propose a novel estimator via minimizing a convex loss function and obtain consistency and asymptotic normality of the same. They also provide a finite sample analysis to achieve an α-approximation to the true natural parameters with O(poly(k/\alpha) samples and O(k / \alpha) computations. 
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a hybrid differentiable renderer, DIBR++, which combines the advantages of rasterization and ray-tracing. The renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. The proposed renderer is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth. The authors demonstrate that the approach achieves superior material and lighting disentanglement on synthetic and real-world data."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes sampling-argmax, a differentiable training method that imposes implicit constraints to the shape of the probability map by minimizing the expectation of the localization error. To approximate the expectation, the authors introduce a continuous formulation of the output distribution and develop a differentiability sampling process. The authors show that the proposed method is effective and flexible by conducting comprehensive experiments on various localization tasks."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,This paper proposes a directed graph data augmentation method called Laplacian perturbation to generate contrastive views for graph contrastive learning (GCL). The authors also propose a curriculum learning approach to progressively learn from easy-to-difficult contrastive view. The proposed method is evaluated on several benchmarks and shows superior performance compared to other GCL methods. 
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes the Symbolic Interactive Language Grounding benchmark (SILG), which unifies a collection of diverse grounded language learning environments under a common interface. The authors propose the first shared model architecture for RL on these environments and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG. The shared architecture achieves comparable performance to environment-specific architectures. Moreover, many recent modelling advances do not result in significant gains on environments other than the one they were designed for."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper presents a sparse version of the Vision Transformer, called Vision MoE (V-MoE), that is scalable and competitive with the largest dense networks in computer vision. The authors propose a batch-prioritized routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. Experiments on ImageNet and CIFAR-10 show the effectiveness of the proposed method."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper studies the expressivity and trainability of training narrow neural networks. In particular, the authors show that as long as the width m > 2n/d, there exists at least one global minimizer with zero training loss, and identify a nice local region with no local-min or saddle points. The authors also consider a constrained optimization formulation where the feasible region is the ""nice local region"" and prove that every KKT point is a nearly-global minimizer. Numerical results show that projected gradient methods outperform SGD on this constrained formulation."
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper proposes a continuous mean-covariance bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent’s objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, the authors consider three feedback settings, i.e., full-information, semi-bandit and full-bandits feedback. They propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of the algorithms."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper considers the problem of computing Positive Semidefinite (PSD) matrices for nonnegative matrix factorization (NMF) problems. The authors propose a non-commutative extension of Lee-Seung’s Multiplicative Update (MMU) algorithm for computing PSD matrices, which ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSd matrices. They also show that under their update scheme the squared loss objective is non-increasing and fixed points correspond to critical points. They demonstrate the utility of their method on real and synthetic data."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes a meta-learning framework for domain generalization (DG) by disentangling features in the latent space and jointly learning both domain-invariant and domainspecific features in a unified framework. The domain-specific representation is optimized through the meta learning framework to adapt from source domains, targeting a robust generalization on unseen domains. The authors empirically show that mDSDI provides competitive results with state-of-the-art techniques in DG."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,This paper proposes a method to improve the sample quality of diffusion models for conditional image synthesis. The main idea is to use classifier guidance to trade off diversity for fidelity using gradients from a classifier. The proposed method achieves FID of 2.97 on ImageNet 128/128 and 4.59/4.72 for ImageNet 256/256 and 7.72/7.85 on the ImageNet 512/512. The authors also show that the proposed method can be combined with upsampling diffusion models.
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes to leverage out-of-distribution samples, i.e., unlabeled samples coming from outside target classes, for improving the performance of few-shot learning. Specifically, the proposed method maximizes the distance from prototypes to out- of distribution samples while minimizing that to in-dist distribution samples. The proposed method is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed approach consistently improves the performance."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper studies the problem of prioritized sampling in reinforcement learning. In particular, the authors propose two new methods, namely ReMERN and ReMERT, to compute the prioritization weight for the Bellman update of the policy. Theoretical analysis is provided to show the importance of the weighting of the data with higher hindsight TD error, better on-policiness and more accurate Q value during sampling. The authors also provide theoretical justifications for the previous criteria of prioritization, such as TD error and recentness, which are mostly heuristically designed. "
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,This paper considers the problem of sequential prediction with expert advice in a nonstationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. The authors give a linear-time algorithm that improves on the best known regret bounds [27]. This algorithm incorporates a relative entropy projection step. This projection is advantageous over previous weight-sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization. The authors also give an algorithm to compute this projection step in linear time.
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper studies the problem of contextual linear bandits, which is motivated by routing applications in navigational engines and recommendation systems. In this setting, the learner is presented with a subset of actions, and the goal is to select the action that maximizes the utility of the set of actions. The authors consider two variants of this problem: (1) the setting where the action is a recommendation, and (2) the case where the recommendation is a list of several actions. In both settings, the authors provide algorithms that achieve regret bounds of $O(\log T)$ and $O(d^log d)$ in terms of the total distance between the true point and the hyperplanes the separation oracle returns."
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper introduces a small set of orthogonal combinators for composing machine learning operators into pipelines. It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers. The paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper studies the problem of meta-learning, where the goal is to learn a weight initialization such that a small number of weight changes results in low generalization error. The authors show that this form of meta learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. They find that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. They also show that sparse learning also emerges in a more expressive model where learning rates are meta-learned."
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"This paper proposes a new method for multi-view learning, called ShICA, which models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. The authors show that this model is identifiable if the components are either non-Gaussian or have enough diversity in noise variances. They also show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset CCA fail. To solve this problem, the authors propose to use joint diagonalization after MultiSet CCA, leading to ShICA-J, which is based on second-order statistics. They further propose to leverage non-gaussianity of the components using a maximum-likelihood method, ShICAML, that is both more accurate and more costly."
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper studies the problem of training multi-agent reinforcement learning agents that perform well with human co-players in a two-player collaborative cooking simulator. The authors propose a simple yet effective method, called Fictitious Co-Play (FCP), that trains an agent to be the best response to a population of self-play agents and their past checkpoints taken throughout training. They show that FCP outperforms existing methods, such as self play (SP), population play (PP), and behavioral cloning play (BCP). They also show that humans report a preference to partner with FCP agents."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes a method for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. The proposed method learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX. The centralised policy gradient estimator optimizes over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG. This allows for more coordinated policy changes and fully reaps the benefits of the centralised critic."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper proposes a biologically plausible model of long-term memory using a combination of biologically plausible three-factor plasticity rules. The authors propose a key-value mechanism to store and read out memories in a single step. The proposed method can be applied to continual recall, hetero-associative memory, and sequence learning. The results suggest a compelling alternative to the classical Hopfield network as a model of biological long term memory. "
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper studies the problem of pairwise learning, i.e., learning tasks where the loss function depends on a pair of instances. The authors propose simple stochastic and online gradient descent methods for the problem. The main difference from the existing studies is that they only pair the current instance with the previous one in building a gradient direction, which is efficient in both the storage and computational complexity. They develop novel stability results, optimization, and generalization error bounds for both convex and nonconvex as well as both smooth and nonsmooth problems. They also extend their algorithms and stability analysis to develop differentially private SGD algorithms that significantly improves the existing results."
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper presents REDO, a class-agnostic framework to REconstruct the Dynamic Objects from RGBD or calibrated videos. REDO is implemented as a canonical 4D implicit function which captures the precise shape and dynamics and deals with partial visibility. The authors show the efficacy of REDO in extensive experiments on synthetic RGBD video datasets SAIL-VOS and DeformingThings4D++, and on real-world video data 3DPW."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. The authors derive high-probability bounds on the performance of LSA under weaker conditions on the sequence {(An,bn) : n 2 N⇤}. However, in contrast, they establish polynomial concentration bounds with order depending on the stepsize and show that their conclusions cannot be improved without additional assumptions."
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper extends the options framework for temporal abstraction in reinforcement learning from discounted to average-reward MDPs. The authors propose general convergent off-policy learning algorithms, intra-option algorithms for learning values and models, and sample-based planning variants of the learning algorithms. They also extend the notion of option-interrupting behavior from the discounted to the average reward formulation. The experiments on a continuing version of the Four-Room domain demonstrate the efficacy of the proposed algorithms."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper proposes an auxiliary self-supervised task to improve the performance of visual transformers (VTs). Specifically, the authors propose a dense localization loss to encourage the VT to learn spatial information. Experiments show that the proposed method can improve the final accuracy of the VTs."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"This paper proposes a method for label-free alignment of hierarchical data in hyperbolic spaces. The proposed method is based on the Riemannian geometry of the Lorentz model and consists of three components: translation, scaling, and rotation. The authors analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. "
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper considers the problem of producing privacy-preserving micro-data for differentially private query answering systems. The authors show that there is a statistical price for this kind of convenience. The uncertainty principle governs the trade-off between accuracy for a population of interest (sum query) and accuracy for its component sub-populations (point queries). The accuracy can degrade by a logarithmic factor for point queries and an extra O(d) factor for the sum query. The paper also provides lower bounds for pure, approximate, and concentrated differential privacy."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"This paper proposes a framework for learning to plan and goal-conditioned RL in the context of long-horizon tasks. The authors propose to learn a curriculum of tree-structured sub-tasks for the planner and an RL agent. The planner is trained to minimize the RL agent’s cost of completing the sequence in each layer from top to bottom layers of the tree, which gradually increases the sub-task complexity and thus forms an easy-to-hard curriculum for the planning policy. The bottom-up traversal of the trees is used to guide the RL policy from easier tasks with denser rewards on bottom layers to harder ones on top layers and collects its cost to train the planner in the next episode. Experiments on navigation and continuous control tasks demonstrate the effectiveness of the proposed framework."
SP:9911693a04a300b5a93634fb0267ef83e5489d77,"This paper proposes a Bayesian framework for generating local explanations for black-box classifiers. The main idea is to use BayesLIME and BayesSHAP to generate local explanations that are consistent, stable, and uncertain about the correctness and reliability of the explanations. The authors also propose a new sampling technique called focused sampling that leverages the uncertainty estimates to determine how to sample perturbations for faster convergence. Experiments on real-world datasets and user studies demonstrate the efficacy of the proposed framework."
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper proposes a method to improve the performance of Adder neural networks (ANNs) by pre-defining the distribution of the weights of the feature maps. The authors argue that the heavy-tailed feature distributions in ANNs could lead to worse classification and propose to pre-define ANN features to follow a mixture of Multivariate Skew Laplace distributions, with which the heavy tails can be better controlled with high order moment skewness. They introduce an angle-based constraint on distribution parameters to incorporate high diversity of distribution tails in angle space so that the overlapping can be eliminated. Experiments on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance."
SP:cbccb65457564992d534504c0d060da44cafce8c,"This paper provides a theoretical explanation for the emergence of such feature imbalances in neural networks. The authors identify and formalize a fundamental gradient descent phenomenon leading to a learning proclivity in over-parameterized neural networks, Gradient Starvation (GS), which arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. In this paper, the authors identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, they develop guarantees for a novel but simple but simple regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper presents a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, the authors also quantify subjective measures of the human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. They find that humans have a clear preference toward a rule based AI teammate (SmartBot) over a learning based teammate (Other-Play) across nearly all subjective metrics, and generally view the learning based agent negatively, despite no statistical difference in the game scores."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper proposes a method for visual question generation (VQG) based on double-hints. In particular, the authors propose a DoubleHints guided Generative Adversarial Networks (DH-GAN), which consists of a question generator and a question-answer-aware discriminator. The generator and discriminator undergo an adversarial process to encourage the generator to identify the salient visual regions of interest so that the high-quality question can be generated, which are indistinguishable from the ground truth questions by the discriminator, which is a weakly supervised learning problem with noises. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed model."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes Generalized Data Weighting (GDW) to mitigate label noise and class imbalance by manipulating gradients at the class level. To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. In this way, it achieves remarkable performance improvement on both issues."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper introduces a spatio-temporal language grounding task where the goal is to learn the meaning of temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatiotemporal references to objects in the scene. To study the role of architectural biases in this task, the authors train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. The experiments show that maintaining object identity in the attention computation of the Transformers is instrumental to achieving good performance on generalization overall."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper proposes a method for online multiple object tracking and segmentation in videos. The proposed method first distills the space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, the proposed method adopts a prototypical appearance module to learn the set of contrastive foreground and background prototypes, which are propagated over time. Experiments on Youtube-VIS and BDD100K datasets demonstrate the effectiveness of the proposed approach."
SP:1175ad16382b349ab1a39895150172d266abe571,"This paper studies the connection between gradient flow and gradient descent in deep learning. In particular, the authors show that gradient descent can be viewed as an approximate numerical solution to the initial value problem of gradient flow, and that the degree of approximation depends on the curvature around the gradient flow trajectory. They then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows them to translate an analysis of gradient flows over deep linear neural networks into a guarantee that gradient descents efficiently converges to global minimum almost surely under random initialization. Experiments suggest that over simple neural networks, gradient descent is indeed close to gradient flow."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers a stochastic multi-armed bandit problem with delayed impact of actions. In this setting, actions taken in the past impact the arm rewards in the subsequent future. The authors generalize the bandit setting to encode the dependency of this “bias” due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. They propose an algorithm that achieves a regret of $\tilde{O}(\KT^2/3)$ and show a matching regret lower bound of $\Omega(\KT^{-2})$ where K is the number of arms and T is the learning horizon."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"This paper proposes a video instance segmentation network using Inter-frame Communication Transformers (IFC) to reduce the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, it utilizes concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. The proposed method achieves state-of-the-art performance on the YouTube-VIS dataset."
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a graph embedding method that can debias structural biases in graphs by using random graphs. The proposed method, called residual2vec, is based on the idea that random walks can be biased due to the structural properties of graphs. In particular, random walks are biased by the degree of each node, where a node is sampled proportionally to its degree. The authors show that this debiasing not only improves link prediction and clustering performance but also allows us to explicitly model salient structural properties in graph embeddings."
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies the problem of estimating the power sum functional of discrete distributions in the context of local differential privacy. In particular, the authors consider the setting where the data are i.i.d. and distributed according to an unknown discrete distribution p = (p1,..., pK). Only α-locally differentially private (LDP) samples z1,..., zn are publicly available, where the term ‘local’ means that each zi is produced using one individual attribute xi. The authors exhibit privacy mechanisms (PM) that are sequentially interactive (i.e. they are allowed to use already published confidential data) or non-interactive. They give lower bounds results over all α-LDP mechanisms and all estimators using the private samples. "
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper studies the problem of online multiclass classification in a setting where the learner’s feedback is determined by an arbitrary directed graph. In this setting, feedback graphs allow a much richer set of applications, including filtering and label efficient classification. The authors introduce GAPPLETRON, a new algorithm that works with arbitrary feedback graphs, and prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. They also prove a general lower bound of order max {BK, \sqrt{BK} } showing that their upper bounds are not significantly improvable. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of explainable clustering in the setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). A k-clustering is said to be explainable if it is given by a decision tree where each internal node splits data points with a threshold cut in a single dimension (feature), and each of the k leaves corresponds to a cluster. The authors give an algorithm that outputs an explainable cluster that loses at most a factor of $O(\log k)$ compared to an optimal (not necessarily explainable) clustering for the k-medians objective, and a factor $O(k)$ for thek-means objective. This improves over the previous best upper bounds of O(\k) and O(k), respectively. "
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper proposes a multilingual pre-trained language model (PrLM) that supports both explicit universal dependency parsing and implicit language modeling. The universal dependency parses are used as the universal structure knowledge in the multilingual PrLM. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in the model, which brings unprecedented PrLM interpretability and convenience in downstream task use. The model outperforms m-BERT and XLM-R in all tasks and achieves state-of-the-art results on syntactic parsing."
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper proposes a new Transformer-based model for solving vehicle routing problems (VRPs). The proposed model learns node and positional embeddings separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. Moreover, the positional features are embedded through a novel cyclic positional encoding (CPE) method to allow Transformer to effectively capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). The authors train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. They apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Results show that DACT outperforms existing Transformer based improvement models and exhibits much better generalization performance across different problem sizes on synthetic and benchmark instances, respectively."
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"This paper proposes a method to estimate the exact Bayes error of generative models learned using normalizing flows. The method relies on a fundamental result, which states that the Bayesian error is invariant under invertible transformation. The authors then show that they can compute exact Bayesian errors for the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, they show that by varying the temperature of the learned flows, they can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes errors. They use their approach to conduct a thorough investigation of state-of-the-art classification models and find that in some — but not all — cases, these models are capable of obtaining accuracy very near optimal."
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper proposes a gradient-based initialization method for neural networks. The method is based on a simple heuristic: the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients."
SP:f69731403592fa5bdd4ca327708582d615aa131c,This paper proposes a method for learning the push-forward of the Euclidean metric by a diffeomorphism to improve the prediction of disease progression in linear mixed-effect models. The proposed method is based on the idea of embedding the data in a Riemannian manifold and learning patient-specific trajectories distributed around a central geodesic. The method is evaluated on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset and shows promising results.
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, it introduces parallel Procedural Units (PUs). A PU consists of a memory head and a procedure. The memory head maintains a summary of a type of features. For an intermediate feature, it searches its closest memory and forward it to the corresponding procedure."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper studies the equivariance of polynomial functions under symmetries of physical laws. The authors show that it is possible to parametrize equivariant polynomials in terms of scalar products and scalar contractions of the scalar, vector, and tensor inputs. They also provide numerical experiments to support their theoretical results. "
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper proposes a generalization of the Intersection over Union (IoU) loss to a new family of power IoU losses, which have a power i.i.d. IoU term and an additional power regularization term with a single power parameter alpha. The authors analyze properties such as order preservingness and loss/gradient reweighting properties of the proposed loss function, and show that it can improve bbox regression accuracy through up-weighting the loss and gradient of high IoU objects. The proposed loss functions can help train high-performance object detectors for impactful applications."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper studies Distributionally Robust Imitation Learning (DROIL), which is a framework that maximizes a generalized concept of entropy. The authors develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. The approach lets us optimize both stationary and non-stationary policies and, unlike prevalent previous methods, it does not require solving an inner reinforcement learning problem."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper studies the problem of post-processing in algorithmic fairness. In particular, the authors consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals guiding the desired fairness constraints. The authors cast the problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired “treat similar individuals similarly” interpretation. Theoretical results demonstrate the connection of the new objective function to a local relaxation of original individual fairness. Empirically, the proposed algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a structure-aware Dual Graph Aggregation Network (SADGA) for cross-domain Text-to-SQL task. SADGA introduces a unified dual graph encoding for both natural language question and database schema, and devises a structure aware aggregation mechanism to take advantage of the global and local structure information of the dual graph in the question-schema linking. Experimental results show that both the dual-graph encoding and the structure aware dual graph aggregation method are able to improve the generalization ability of the proposed method."
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of learning end-to-end learnable discrete-continuous models. The authors propose two strategies to overcome the challenges of learning complex stochastic computations graphs with multiple sequential discrete components. They show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. They propose to increase the scale parameter of the Gumbel noise perturbations during training and propose dropout residual connections specifically tailored to stochastically-discrete computation graphs. They also propose a new temperature matching scheme for mitigating these problems. They demonstrate that their methods enable the training of complex discrete-stochastic models which could not be learned before."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"This paper studies the robustness of Bayesian neural networks (BNNs) with approximate inference via full-batch Hamiltonian Monte Carlo (HMC) to covariate shift. The authors show that BNNs with high-fidelity approximate inference with HMC achieve poor generalization, even under-performing classical estimation. They also show that the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, the authors propose novel priors that improve BNN robustness to many sources of covariate shifts."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper categorizes meta-learning evaluation into two settings: in-distribution (ID), in which the train and test tasks are sampled iid from the same underlying task distribution, and out-of- distribution (OOD). The authors identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. The authors also provide suggestions on how to construct FSL benchmarks to allow for ID evaluation as well as more reliable OOD evaluations."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper revisits the differences between KB-based and LM-based rule induction methods for rule generation. The authors argue that the current LM methods are “learning rules from rules”, which limits these methods to only produce “canned” rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. Therefore, the authors propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, they propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision of annotations."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,This paper proposes Implicit Constraint Q-learning (ICQ) for multi-agent offline reinforcement learning. The main idea of ICQ is to use the state-action pairs given in the dataset to estimate the Q-values of the joint-policy. The authors show that the extrapolation error is controlled within a reasonable range and insensitive to the number of agents. ICQ achieves the state of the art performance in the StarCraft II offline learning task.
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper studies adversarial robustness against evasion attacks, with a focus on applications where input features have to comply with certain domain constraints. The key idea of this paper is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. The authors propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. The experimental results on malware classification, credit risk prediction, and spam detection demonstrate the effectiveness of the proposed method."
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper extends the theory of Tikhonov regularization to generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. The authors show that fast and optimal rates can be achieved for GSC by using the iterated Tikhonshine scheme, which is intrinsically related to the proximal point method in optimization, and overcomes the limitation of the classical Tikhonishine regularization scheme. They also prove that iterated tikhonsshine has optimal learning rates and higher qualification than the classical one."
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper introduces a new kind of linear transform named Deformable Butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. The authors apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy."
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes a method to address the catastrophic forgetting (CF) problem in continual learning, where the weights of a network are overwritten during the training of a new task causing forgetting of old information. The proposed method, called MetA Reusable Knowledge (MARK), keeps a set of shared weights among tasks and uses them as a common knowledge base (KB) that is not only used to learn new tasks, but also enriched with new knowledge as the model learns new tasks. A set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. Experiments show that the proposed method achieves state-of-the-art results in several popular benchmarks, surpassing the best performing methods."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data-driven framework for scheduling primal heuristics in exact MIP solvers. The proposed approach is based on learning from data describing the performance of primal heuristic algorithms and learning a problem-specific schedule that collectively finds many solutions at minimal cost. The authors formalize the learning task and propose an efficient algorithm for computing such a schedule. Compared to the default settings, the proposed approach can reduce the average primal integral by up to 49% on two classes of challenging instances."
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper studies reinforcement learning in the setting where the learner receives binary feedback only once at the end of an episode. In this setting, the trajectory labels are generated by an unknown parametric model, and the authors provide a statistically and computationally efficient algorithm that achieves sublinear regret. The main contribution of this paper is to show that learning is possible in this more challenging setting, and to provide a theoretical analysis of the proposed algorithm."
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows the authors to apply message-passing techniques for node representations to edges. The authors also propose two edge pooling methods to obtain a holistic edge representation for a given graph, where one clusters similar edges into a single edge for graph reconstruction and the other drops unnecessary edges for graph classification. Experiments on graph reconstruction, generation and classification tasks show the effectiveness of the proposed method."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations for learning and representing the optimal policy in the context of mutual information maximization (MI) based RL methods. The authors consider the problem of learning representations that are sufficient for RL from a theoretical perspective, and study several popular MI based objectives through this lens. They find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. They corroborate their theoretical results with empirical experiments on a simulated game environment with visual observations."
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper proposes a novel steerable convolution (SS-Conv) method for 3D semantic analysis. The main idea is to use a feature-steering module to improve the efficiency of the convolutional layers. The proposed method is evaluated on 3D object semantic analysis tasks, namely instance-level 6D pose estimation, category-level pose and size estimation, and category level 6D tracking. The results show that the proposed method outperforms the existing methods."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, a lightweight prediction module is added to different layers to estimate the importance score of each token given the current features. An attention masking strategy is also proposed to differentiably prune a token by blocking its interactions with other tokens. By hierarchically pruning 66% of the input tokens, the method greatly reduces the FLOPs and improves the throughput by over 40%."
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper considers the problem of obtaining distribution-free inference guarantees for the conditional mean E [Y |X]. The authors consider the setting where the features X are continuously distributed, and they show that any confidence interval for E [y |X] must have non-vanishing width, even as sample size tends to infinity. At the other extreme, if X takes only a small number of possible values, then inference is trivial to achieve. The authors show that there are several distinct regimes in between the finite setting and the continuous setting, where vanishing-width confidence intervals are achievable if and only if the effective support size of the distribution of X is smaller than the square of the sample size."
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes Representation Neutralization for Fairness (RNF), a new mitigation technique that achieves fairness by debiasing only the task-specific classification head of DNN models. The key idea of RNF is to discourage the classification head from capturing undesirable correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, the authors leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate that the proposed RNF framework can effectively reduce discrimination with minimal degradation in task specific performance."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper proposes a new convolutional layer that takes advantage of Bessel functions, well known in physics, to build Bessel-CNNs (B-CNN) that are invariant to all the continuous set of possible rotation angles by design. B-CNN is a new type of convolution layer that is built on top of CNNs and achieves global rotational invariance by a succession of local invariance provided by the multiplication between the image and the filters. The performance of the proposed method is deeply dependent on the choice of $\�max$ and $jmax$."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper proposes a new algorithm for large-scale kernel ridge regression. The proposed algorithm combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, the authors promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. The authors characterize the statistical-computational tradeoff of our model, and demonstrate the effectiveness of our method by numerical experiments."
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper proposes a method for learning to communicate among agents via discrete tokens. The proposed method is based on word embedding techniques from natural language processing. The authors show in a decision theoretic framework that their technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, the authors validate that their trained agents learn to cluster tokens in semantically-meaningful ways, allowing them to communicate in noisy environments where other techniques fail."
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper proposes to combine the strengths of convolutional and transformers for better generalization and efficiency. The authors propose to combine depthwise convolution and self-attention layers, and propose to stack convolution layers and attention layers in a principled way to improve the generalization, capacity and efficiency of the model. The proposed model achieves state-of-the-art performance on ImageNet classification tasks."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper presents a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality, which is amenable to efficient minimization. The authors also derive a new concentration of measure inequality that combines PAC-Bayesian bounding with Bennett’s inequality for empirical estimation."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a weakly supervised method for audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, the method explores event co-occurrence across audio, visual, and audio- visual streams. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations. Extensive experimental results show that the proposed method substantially improves several baselines and performs favorably against the state-of-the-art methods."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes a personalized federated learning (FL) algorithm called QuPeD, which allows clients to learn compressed personalized models with different quantization parameters and model dimensions/structures. The authors propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. They formulate a compressed personalization framework by introducing knowledge distillation loss for local client objectives collaborating through a global model. They develop an alternating proximal gradient update for solving this compressed personalisation problem, and analyze its convergence properties. Numerically, the authors validate that QuPED outperforms competing personalized FL methods, FedAvg, and local training of clients."
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"This paper proposes a novel framework for constrained clustering based on deep generative models. The proposed DC-GMM uncovers the underlying distribution of data conditioned on prior clustering preferences, expressed as pairwise constraints. These constraints guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. The authors provide extensive experiments to demonstrate the superior clustering performances and robustness."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper studies the Neural Tangent Kernel (NTK) and the convolutional NTK (CNTK). The authors propose a near input-sparsity time approximation algorithm for NTK, by sketching the polynomial expansions of arc-cosine kernels. The authors also prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) with a sketching algorithm. The proposed methods are evaluated on various large-scale regression and classification tasks and show that a linear regressor trained on our CNTK features matches the accuracy of exact NTK on CIFAR-10 dataset while achieving 150x speedup."
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,This paper proposes a multi-person 3D motion trajectory prediction framework for long-term motion prediction. The key observation is that a human’s action and behaviors may highly depend on the other persons around. The proposed Multi-Range Transformers (MRT) model contains of a local-range encoder for individual motion and a global-range decoder for social interactions. MRT is able to predict 15-person motion simultaneously by automatically dividing the persons into different interaction groups.
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a method to automatically generate the guiding programs for long-horizon planning problems, where the goal is to find a strategy that correctly, and optimally, handles every possible configuration of the hidden regions of the environment. The authors propose to use a generative model to predict the unobserved parts of the world, and then synthesize a program based on samples from this model in a way that is robust to its uncertainty. The proposed method is evaluated on a set of challenging benchmarks, including a 2D Minecraft-inspired environment, and achieves a similar performance as using handcrafted programs to guide the agent."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper studies the problem of causal imitation learning in sequential settings, where the imitator must make multiple decisions per episode. The authors develop a graphical criterion that is necessary and sufficient for determining the feasibility of causal imitation learning, providing conditions when an imitators performance can match a demonstrator’s performance despite differing capabilities. They also provide an efficient algorithm for determining imitability and corroborate their theory with simulations."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper presents a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to a slotwise object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. The proposed model is able to accurately predict the reappearance of objects, even after long-term occlusion."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"This paper studies the generalization properties of a weighted ERM algorithm for risk minimization (ERM) using adaptively collected data to minimize the average of a loss function over a hypothesis class and provides generalization generalization guarantees and fast convergence rates. The results are based on a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the good dependence on the exploration rate in the data. For regression, the authors provide fast rates that leverage the strong convexity of squared-error loss. For policy learning, they provide regret guarantees that close an open gap in the existing literature whenever exploration decays to zero."
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper proposes a novel approach for kernel-reweighted regression. The authors propose to reparametrize the sample weights using a doubly non-negative matrix. The proposed approach is based on the idea of adversarially reweighting the weights of the regression model. In particular, the authors show that when the weighting matrix is confined in an uncertainty set using either the log-determinant divergence or the Bures-Wasserstein distance, they show that the adversarial reweighted estimate can be solved efficiently using first-order methods. Numerical experiments are provided to demonstrate the effectiveness of the proposed approach."
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper proposes two gradient estimators for training models with discrete latent variables based on importance sampling and statistical couplings. The proposed estimators are based on reparameterizing categorical variables as sequences of binary variables and Rao-Blackwellization. The authors also introduce a novel derivation of the DisARM/U2G gradient estimator, which extends it to the categorical setting and is called DisARM-IW. In the experiments, the authors demonstrate that the proposed estimator provide state-of-the-art performance."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a new predictor-based NAS method that progressively shrinks the sampling space, by learning a series of weak predictors that can progressively evolve to sample towards the subspace of best architectures, thus greatly simplifying the learning task of each predictor. The proposed method is evaluated on NAS-Bench-101 and NAS-bench-201 and achieves state-of-the-art performance. "
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes a new intrinsic control method, Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), which is based on the idea of maximizing the number of latent codes that can be discriminated from future states under some short time horizon (e.g. maximising the effective number of states it can reliably reach). The authors show that EDDICT’s globally consistent codes allow it to be far more exploratory, as demonstrated by improved state coverage and increased unsupervised performance on hard exploration games such as Montezuma's Revenge."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,This paper proposes a fragment-based generative generative RL with Explorative Experience replay for Drug design (FREED) to generate pharmacochemically acceptable molecules with large docking scores. The proposed method is based on a fragment based generative model and a novel error-prioritized experience replay (PER). The authors show that the proposed method produces molecules of higher quality than existing methods while achieving state-of-the-art performance on two of three targets in terms of the docking scores of the generated molecules.
SP:b938bca513e7de1231212064caf8877a78d8b612,"This paper studies the problem of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. The approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. The authors show that for certain graph ensembles, a simple forward greedy search algorithm suffices to learn the Markov boundaries of each node. This substantially improves the sample complexity, which is at most polynomial in the number of nodes. This is then applied to learn  the entire graph under a novel identifiability condition that generalizes existing conditions from the literature."
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper considers the setting where each user has m samples and the privacy protection is enforced at the level of each user’s data. In this setting, the authors show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an ("", )DP algorithm using only O(log(1/\epsilon^2/\sqrt{(d})/\log^2) users. The main contribution is a generalization of global stability [BLM20] that allows the use of public randomness. Under this relaxed notion, the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples. "
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper studies the implicit representation of value iteration networks (VINs) in the context of reinforcement learning. In particular, the authors study the convergence of VINs with the implicit parameterization of a linear function and show that, for a linear parametrization, gradient descent converges to global optima despite nonlinearity and non-convexity introduced by the implicit representations. They also show conditions under which SGD with this implicit representation converges substantially faster than its explicit counterpart. Finally, they provide empirical results in some simple domains that illustrate the theoretical findings."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes a method to improve the performance of Knowledge Graph (KG) embeddings for downstream tasks such as KG-based question answering. The proposed method is based on a combination of PSL-KGI and KG embedding methods such as ComplEx and ConvE. In particular, the authors propose to use PSL to refine the KG, and then use the refined KG as input for the embedding method. The method is evaluated on a variety of KG datasets and shows improved performance over the baselines."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a new benchmark dataset for evaluation of knowledge base completion (KBC) methods. The proposed dataset consists of binary predictions, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. The authors randomly remove some of these correct answers from the data set, simulating the realistic scenario of real-world entities missing from a KB. This way, we can measure a model’s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answers."
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes an approach to improve the performance of pre-trained language models for dialog generation. The approach is based on the idea of alternating roles, i.e., each speaker is modeled as a separate task, and the model is trained with a large pretrained language model. The model is evaluated on two task-oriented dialog datasets, CamRest676 and MultiWOZ, and it is shown to outperform or on-par with the state-of-the-art methods that use human annotations, such as belief states and dialog acts."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"This paper studies the problem of estimating the probability that the classification predicted by a deep neural network is correct (or in the top 5) on the test set. The authors propose to use the notion of implied loss and prove that if an uncertainty measure is an implied loss, then low uncertainty means high probability of correct (/top-k) classification. The proposed method is simple to use on existing networks: they proposed confidence measures for Top-k which can be evaluated by binning values on thetest set."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the spectrum of the neural tangent kernel (NTK) of deep neural networks. The authors show that the NTK is related to the Neural Network Gaussian Process (NNGP) kernel (NNNGP) in the wide network limit. The spectrum of NTK simplifies in much the same way as that of the NNGP kernel. By analyzing this spectrum, the authors arrive at a precise characterization of trainability and a necessary condition for generalization across a range of architectures including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs)."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper presents a method for protein quality assessment (QA) based on graph convolutional neural networks (GCNs). The authors propose a method to estimate the quality of protein models, which they call GRAPHQA. The method is based on the Graph Convolutional Neural Network (GCN), which is a graph-based neural network that is able to capture the structure of the protein. The authors evaluate the performance of the proposed method on a variety of metrics and datasets, and show that it outperforms existing methods."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper studies the landscape of loss functions of linear neural networks with different loss functions and different parameterizations. In particular, the functional space is either the set of all linear maps from input to output space or a determinantal variety, i.e., a set of linear maps with bounded rank. The authors introduce a distinction between pure critical points and spurious critical points, which only depend on the function space, and arise from the parameterization. The analysis clearly illustrates that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear linear maps (filling architectures), but it holds only for the quadratic loss when the functional spaces is a deterministic variety (“non-filling architecture”)."
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper proposes a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Given an input graph, the proposed framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub graph vectors, and uses the embedding of the sub graph vector distribution as the output vector representation for the input graph. Theoretical analysis shows the close connection between SEED and graph isomorphism. The empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods."
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper proposes a new algorithm for counterfactual regret minimization (CFR) based on a lazy update strategy, which avoids traversing the whole game tree in each round. The authors prove that the regret of Lazy-CFR is almost the same as that of the vanilla CFR and only needs to visit a small portion of the game tree. They also propose a new framework to develop efficient variants of CFR with an analysis that shows that the proposed algorithm is provably faster than vanilla CFR. The final algorithm runs fast in practice, but with extra cost on space complexity."
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper proposes a method for Unsupervised Domain Adaptation (UDA) by learning transferable features by minimizing the feature distribution discrepancy between the source and target domains. The main idea is to model the deep features from each domain as Gaussian mixture distributions. The authors propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distances between the corresponding Gaussian component means of the source data and the target data. The second one is a pseudo negative log likelihood of generating the target features from the source feature distribution. The proposed method is trained by minimizing classification loss on the labeled source data, the domain discrepancy loss, and the domain invariant loss. Extensive experiments are conducted over two UDA tasks. "
SP:40be996e8bb86e887077b762b87c7c34a786ac98,This paper proposes a conditional continuous normalizing flow (CNF) model that partitions the latent space into a class-specific supervised code and an unsupervised code that is shared among all classes for efficient use of labeled information. It also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. Experiments on CIFAR-10 show that the proposed method improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs.
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm in the lazy training regime. In this regime the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling arises naturally, implicit in the initialization of their parameters. Both in the under-and over-parametrized frameworks, the authors prove exponential convergence to local and global minimizers of the above algorithm. They then give examples of such convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the cases of neural network."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper proposes a method to train an agent that can take actions to generate observations that can help predict whether the hypothesis is true or false. The agent is trained to solve the problem of hypothesis verification as a reinforcement learning problem. Specifically, given a hypothesis about the dynamics of the world, the agent is asked to take actions that maximize the probability of the observed observations being correct or incorrect. The authors propose to use the structure of the hypothesis to train the agent to learn to solve this problem. They show that agents trained end-to-end with the reward fail to learn the problem. "
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper proposes a method to predict the embeddings of mathematical formulas in latent space based on the set of rewrites that can be successfully performed on a statement. The paper proposes to use a graph neural network (GNN) to predict whether a statement can be rewritten by other theorems. The GNN predicts the embedding of a formula generated by some rewrite rule, which is viewed as approximate reasoning in the latent space. The proposed method is evaluated on a corpus of 9 mathematical formulas from various fields."
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"This paper proposes a method for learning depth from images and very sparse depth measurements. The method is motivated by the way natural agents learn to predict depth, and proposes an approach for training a dense depth estimator from two unconstrained images given only very sparse supervision at training time and without the explicit use of geometry. Experiments on several datasets show that the proposed model can learn monocular dense depth estimation when trained with very sparse ground truth, even a single pixel per image. The global parameters extracted by the network are predictive of the metric agent motion."
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper extends the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. The authors show that by applying a multi-layer Transformer to these Bloom filter digests, they are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models trained using sampled softmax with the same computational budget."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper tackles the problem of zero-shot 3D shape part discovery. The authors propose a learning-based agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of the approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. The proposed method achieves the best performance over all four baseline methods on the PartNet dataset."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a technique called “neuron editing” that learns how neurons encode an edit for a particular transformation in a latent space. They use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in the latent space, they encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron’s activations. Their technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications. "
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper proposes a meta-learning approach for few-shot image segmentation. The authors propose an extension of FOMAML and Reptile to the task of segmentation, a novel neural network architecture for parameter efficiency and fast learning called EfficientLab, a formalization of the generalization error of meta learning algorithms, and a small benchmark dataset, FP-k, for the empirical study of how meta learning systems perform in both few and many-shot settings. They show that meta-learned initializations for segmentation outperform random and ImageNet-trained initializations on FSS-1000 dataset and larger datasets."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a new method for semi-supervised few-shot learning (SS-FSL) based on Prototypical Networks (PN). The proposed method is based on the random walk notion of prototypical random walk (PRW) and is built on top of the prototypical networks (PN) framework. The authors propose a global consistency loss and a distractor filter to improve the performance of the proposed method. Experiments on mini-Imagenet, Omniglot and CIFAR-10 datasets show the effectiveness of the method. "
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a new self-supervised training objective, Contrastive Sensor Fusion (CSF), which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. The proposed method outperforms fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper proposes two methods for retraining after neural network pruning: (1) weight rewinding, which rewinds the weights to their values from earlier in training and retrains them from there using the original training schedule, and (2) learning rate re-winding, which trains the unpruned weights from their final values using the same learning rate schedule as weight re-inding. Both methods outperform fine-tuning in terms of accuracy and compression ratio. The paper is well-written and easy to follow."
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper studies the relationship between output margin and generalization in deep neural networks. The authors propose to analyze a new notion of margin, which they call the “all-layer margin”, and show that it has a clear and direct relationship with generalization for deep models. They also propose a theoretically inspired training algorithm for increasing the all-layer margins, which improves both clean and adversarially robust test performance in practice."
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper studies the problem of knowledge-grounded dialogue generation in a low-resource setting. The authors propose a disentangled response decoder in order to isolate parameters that depend on knowledge grounded dialogues from the entire generation model. By this, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only 1/8 training data, the proposed model can achieve the state-of-the-art performance and generalize well on out of-domain knowledge."
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a new approach for training neural machine translation models (NMT) that makes use of non-parallel data. In particular, the authors propose a mirror-generative NMT (MGNMT) model that jointly learns bidirectional translation models as well as source and target language models in a latent space of the shared bilingual semantics. Experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of language pairs and scenarios."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper studies the role of the entropy term in Soft Actor Critic (SAC) for the Mujoco benchmark. The authors first show that the primary role of entropy in SAC is to maintain satisfactory exploration in the presence of bounded action spaces, and then propose a new algorithm that does not employ entropy maximization but nevertheless matches the sampling efficiency and robustness performance of SAC. The experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper studies the vulnerability of industrial copyright detection systems to adversarial attacks. The authors propose to use a well-known music identification method and implement this system in the form of a neural net, and then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube’s Content ID system. The goal is to raise awareness of the threats posed by adversarial examples in this space and highlight the importance of hardening copyright detection system to attacks."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper proposes a method to generate point-to-point activation intensity between two images so that the relationship between different regions of the two images can be uncovered. The proposed method can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. The experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper studies the problem of online lifelong learning in the context of model-based planning and model-free learning. The authors propose a new algorithm, Adaptive Online Planning (AOP), that combines model based planning (MBP) and model free learning (FML) to improve the performance of the planner and the uncertainty of the model free components. In particular, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times. The proposed method is evaluated on a set of Mujoco environments."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes to replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and total-variation Sparse Attention (TVMAX). With sparsemax, we obtain sparse attention weights, selecting relevant features. With TVMAX, we propose to promote sparsity and encourage fusing of the related adjacent spatial locations. By selecting relevant groups of features, the TVMAX transformation improves interpretability."
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes a generative model to predict the evolution of dynamic graphs. Specifically, the authors use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graph topology. The generative models predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology using a recurrent neural network. The authors evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets."
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The proposed method consists of a generator network that generates imputations that a discriminator network is tasked to distinguish. Then, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The experimental results show the effectiveness of the proposed method in generating imputations and providing estimates for the class uncertainties in a classification task when faced with missing values."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper proposes a novel approach for off-policy estimation in the long-horizon setting. The authors formulate the problem as solving for the fixed point of a certain operator, and use tools from Reproducing Kernel Hilbert Hilbert Spaces (RKHSs) to develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the data are collected. They analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our approach."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper proposes a method for learning the Gaussian mixture model (GMM), which is a probabilistic framework that allows us to define a dataset containing K different modes when each of the modes is associated with a Gaussian distribution. In the traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional likelihood p(x|k, θ), as well as responsibility probability p(k|x, \theta) which describes the distribution index corresponds to the data. However, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it’s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, the authors utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data's latent space z instead of x. They devise a modified GAN to allow to define the distribution using p(z|k,.\theta), where z is the corresponding latent representation of x and $\theta$ through an additional classification network which is trained with the GAN in an “end-to-end” fashion. They demonstrate through experiments that the proposed method surpasses previous baselines in terms of image generation performance with only minor growth on the size of the network."
SP:2da1608209058d214f8671062cc9eb0833ba4831,"This paper presents a method for training large capacity neural networks with significantly improved accuracy and lower dynamic computational cost by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, the authors introduce a new residual block architecture that gates convolution channels and introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. The authors present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation."
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30," method proposes a probabilistic importance inference approach for pruning DNNs. Specifically, they test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemetric scoring test and keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a method for learning hierarchical reinforcement learning by iteratively compressing action trajectories to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary length. The learned temporally extended actions provide new action primitives that can participate in deeper hierarchies as the agent learns. The authors demonstrate the relevance of this approach for tasks with non-trivial hierarchical structure and show that the approach can be used to accelerate learning in recursively more complex tasks through transfer."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"This paper proposes a probabilistic generative autoencoder (HBAE) model with an EBM-based multimodal decoder. The model is trained using variational inference (VAE) to recover latent codes conditioned on inputs, and the decoder is trained with an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The proposed model is also capable of modeling sets, by inferring latent codes for a set of examples, and sampling set members through the multi-modal decoders. In both single image and set cases, the model generates plausible variations consistent with the input data, and generates realistic unconditional samples."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper proposes a normalization technique for deep off-policy temporal difference (DDPG) and TD3 based on a mixture of on-and-off-policy transitions, which the authors call cross-normalization. The authors show that this technique improves the performance of DDPG/TD3 on a range of MuJoCo tasks. They also show that mean-only normalization is sufficient to stabilize training."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper proposes a method to learn discriminative features that are unbiased and invariant to the confounder(s). The proposed method is based on adversarial training strategies by encouraging vanished correlation to learn features for the prediction task while being unbiased to the confounding variables in the study. Experiments are conducted on synthetic data, medical images, and a gender classification dataset. The results show that the learned features are uncorrelated with the bias or confounders variables."
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper proposes a lightweight Transformer-based model for character-level language modeling. The authors propose a lightweight model, called GroupTransformer, that factorizes the calculation paths by grouped embedding operators. Additionally, Group-Transformer employs inter-group linear operators to prevent performance degradation from the group strategy. Experiments on enwik8 and text8 show the effectiveness of the proposed method."
SP:946c26d371297c88d0ac246257104099b4585edc,"This paper proposes a novel objective function for training generative models with deep hierarchies of latent variables using Optimal Transport. The approach recursively applies the Wasserstein distance as the regularisation divergence, allowing the stacking of WAEs for arbitrarily deep-latent hierarchies. The authors show that this approach enables the learning of smooth latent distributions even in deep latent hierarchies, which otherwise requires extensive model design and tweaking of the optimisation procedure to train. They also show that their approach is significantly more effective at learning smooth hierarchical latents than the standard WAE."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,This paper proposes an autoregressive video generation model based on a three-dimensional self-attention mechanism. The proposed model is able to generate high-quality video continuations and achieves state-of-the-art results across a range of video generation benchmarks. The authors also present results from training their models on a large scale action recognition dataset called Kinetics.
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"This paper proposes a latent feature generation framework for generalized zero-shot ICD coding, where they aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. The framework generates semantically meaningful features by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. To the best of the knowledge, this is the first adversarial generative model for the generalized zero shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of the approach."
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,This paper proposes a method for self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). The authors propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. They demonstrate that the action embedding alone improves the sample efficiency and peak performance of model-free RL on control from low-dimensional states. 
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the task-specific meta-learner. Experiments on toy regression and few-shot image classification demonstrate the superiority of ARML over state-of-the-art baselines."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a method for controlling attributes of the generated language (e.g. switching topic or sentiment) without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. The proposed method combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. The attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM, and the sampling entails a forward and backward pass in which gradients from the attribute model push the LM’s hidden activations and thus guide the generation."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes to learn data representations with a novel type of denoising autoencoder, where the noisy input data is generated by corrupting clean data in the gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption. Furthermore, the learned representations perform well when transferring to other vision tasks."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper studies the problem of under-sensitivity to input text deletions in natural language inference. The authors propose to use the interval bound propagation (IBP) approach to verify the correctness of the decoder of the decomposable attention (DA) model. They show that the IBP approach is able to verify a relatively small fraction of data points, but that IBP-training in particular is capable of improving verified accuracy."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper studies the problem of model-free off-policy deep reinforcement learning (RL) in continuous state and action spaces. The authors propose a new Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in – resulting in a QGRAPH. They show that the Q-value for each transition in the simplified MDP is a lower bound of the q-value of the original continuous Q-learning problem. By using these lower bounds in TD learning, the proposed method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper studies the problem of unsupervised domain adaptation (UDA), i.e., generalization of a hypothesis trained in a source domain to an unlabeled target domain. The authors study the effect of the embedding complexity on the generalization to the target domain, and show that the complexity affects the upper bound on the target risk. They further propose a strategy that mitigates sensitivity to the embeddings complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper studies generalization error bounds for learning general non-convex objectives. The authors develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalisation error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. They obtain new data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Their result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al (2018)."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper investigates the role of the hippocampus in continual learning in the context of two different spatial navigation tasks. Specifically, the authors analyze population-level activity of 612 hippocampal CA1 neurons of rodents learning to perform allocentric and egocentric spatial tasks. The components uncovered using dPCA from the firing activity reveal that hippocampal neurons encode relevant task variables such as decisions, navigational strategies and reward location. The authors compare this hippocampal features with standard reinforcement learning algorithms, highlighting similarities and differences. They demonstrate that a standard deep reinforcement learning model achieves similar average performance when compared to animal learning, but fails to mimic animals during task switching."
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,This paper proposes a method to improve the performance of Monte Carlo Tree Search (MCTS) in continuous action spaces. The proposed method is based on bootstrapping the MCTS tree search with a pre-trained policy and using a limited number of action samples from the policy distribution and a new loss function based on the trajectories’ mean and standard deviations. The results show that the proposed method outperforms PPO (a baseline policy optimization method) in most of the environments.
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper studies the lottery ticket hypothesis, which claims that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, the authors aim to answer the following open questions: can we find winning tickets with few data samples or few labels? Can we even obtain “good” tickets without supervision? They find that winning tickets found in these scenarios are, perhaps surprisingly, competitive with winning tickets generated on the full ImageNet dataset when evaluated on ImageNet classification task."
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"This paper focuses on the complementary problem of excessive prediction undersensitivity where input text is meaningfully changed, and the model’s prediction does not change when it should. The authors formulate a noisy adversarial attack which searches among semantic variations of comprehension questions for which a model still erroneously produces the same answer as the original question – and with an even higher probability. They show that – despite comprising unanswerable questions – SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarially generated questions. This indicates that current models—even where they can correctly predict the answer—rely on spurious surface patterns and are not necessarily aware of all information provided in a given comprehension question. They experiment with both data augmentation and adversarial training as defense strategies: both are able to substantially decrease a model's vulnerability to undersensitivity attacks on held out evaluation data."
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. It learns the transition dynamics of the environment and generates a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing it to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define), is taken as a human input. The imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Experiments on two gridworld environments and a self-driving car simulator show that the proposed approach visits unsafe states significantly less frequently than a baseline."
SP:c2796f28fb067138303df8d424d646f4ada31558,"This paper proposes a method for learning the dynamics of PDEs in the context of graph neural networks. In particular, the authors propose a method that leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. The proposed method, Physics-aware Difference Graph Networks (PA-DGN), exploits neighboring information to learn finite differences inspired by physics equations. The authors demonstrate the superiority of PA-D GN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations."
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper considers the problem of training structured neural networks with nonsmooth regularization and constraints. The authors formulate training as a constrained nonsmoothed nonconvex optimization problem, and propose a convergent proximal-type stochastic gradient descent (ProxSGD) algorithm. They show that under properly selected learning rates, with probability 1, every limit point of the sequence generated by the proposed Prox SGD algorithm is a stationary point. Numerical experiments are conducted to show the effectiveness of the proposed algorithm. "
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper proposes an end-to-end differentiable compression framework for lossy image compression, which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bits-back efficient. The authors apply it to training Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art on low bitrates."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper proposes a new method for super resolution (SR) based on C-JPG images. The proposed method consists of two components: (1) a functional sub-model to recover the information from the compressed images, and (2) a cycle loss to improve the performance of the SR solver. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the existing methods."
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,This paper presents a deep learning approach to estimate the probability of passes in a soccer game. The approach is based on a convolutional neural network that is trained to predict the likelihood of a given pass from a set of low-level features extracted from the high-frequency data. The proposed approach is evaluated on a dataset of high-resolution images from the 2014 MLS season. It is shown that the proposed approach outperforms the baselines in terms of both the number of predicted passes and the overall performance.
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion model without using side information. It trains a graph neural network (GNN) based on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps them to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper considers the unconstrained minimization of a smooth objective function in the setting where only function evaluations are possible. The authors propose and analyze stochastic zeroth-order method with heavy ball momentum. They show new complexity results for non-convex, convex and strongly convex functions. They test their method on a collection of learning to continuous control tasks on several MuJoCo environments."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes a network architecture, Action Semantics Network (ASN), that explicitly represents the action semantics between agents in multi-agent systems (MASs). The proposed ASN characterizes different actions’ influence on other agents using neural networks and can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance of state-of-the-art DRL approaches."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper proposes to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep reinforcement learning (RL). Specifically, the authors investigate the low-rank structure, which widely exists for big data matrices. They verify empirically the existence of low rank Q functions in the context of control and deep RL tasks, and propose a general framework to exploit these structures. This leads to a more efficient planning procedure for classical control, and a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on “low-rank tasks”."
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes a new algorithm, Best-Action Imitation Learning (BAIL), which selects from the batch the actions it believes to be high-performing actions for their corresponding states; it then uses those state-action pairs to train a policy network using imitation learning. The authors demonstrate that BAIL achieves state-of-the-art performance on the Mujoco benchmark."
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper proposes DeepXML, an algorithm to jointly learn representations for extreme multilabel learning on text data. The proposed algorithm addresses the key issues of scalability and low accuracy (especially on tail labels and very short documents) with existing approaches such as Slice, AttentionXML and XML-CNN, and improves on them substantively. Experiments show that the proposed method can lead to a 1.0–4.3 percentage point gain in performance while being 33–42x faster at training than AttentionxML. "
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper proposes a new method for collaborative filtering based on hashing-based collaborative filtering. The proposed method is based on the idea of self-masking, i.e., the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. The authors also provide an efficient implementation that yields <4% runtime overhead."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper investigates the mode collapse issue of GANs. The authors propose a set of statistical tools to quantify the general mode collapse via statistical tools, discuss and verify possible causes, and propose two black-box calibration approaches for the first time to alleviate the mode collapses. The two calibration approaches only handle one worst-case dense mode."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper studies the optimization and generalization of over-parameterized neural networks that are beyond the NTK regime and are still governed by the Taylor expansion of the network. The authors propose the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. They show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. They also prove concrete generalization and expressivity results on these randomized networks."
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"This paper proposes a novel tool for evaluating the effectiveness of graph convolutional filters for node classification. The proposed tool, called Graph Filter Discriminant Score (GFD), is based on the graph filter discriminative score (GFD) and is applied to analyze a family of existing filters as a case study. It shows that there is no single filter as a “silver bullet” that performs the best on all possible graphs, and graphs with different properties are in favor of different graph filters. Based on these findings, the authors develop Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn data-specific filters. Experiments on both synthetic and real-world benchmark datasets have demonstrated that the proposed model has the flexibility in learning an appropriate filter and consistently provides state-of-the-art performance."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies the problem of distributionally robust optimization (DRO) for overparameterized neural networks, where the goal is to minimize the worst-case training loss over a set of pre-defined groups. The authors show that naively applying group DRO to overparametrized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst case training loss. Instead, the poor worst case performance arises from poor generalization on some groups. To address this issue, the authors propose to use a combination of DRO models with increased regularization—a stronger-than-typical `2 penalty or early stopping—and achieve substantially higher worst-group accuracies, with 10–40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. "
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper proposes a local explanation method for black-box classifiers. The proposed method is based on the idea of distribution controller on relevance scores and integrate it with a trainable mask generator to directly guide the relevance scores. In addition, the proposed predictor is optimized under the classification loss, aiming to better mimic the classifiers and enables discriminative scores over supporting features and facilitates the setting of involved hyperparameters. The experimental results demonstrate that the proposed method outperforms others in terms of faithfulness and explainability."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"This paper proposes a method for multi-instance image reconstruction and classification. The proposed method is based on the idea of lifting the training optimization problem by treating the result of top-K selection as a slack variable, resulting in a simple, yet effective, multi-stage training. The method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and it outperforms the state-of-the-art."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,"This paper proposes a reinforcement learning approach for neural program synthesis, where the goal is to generate assembly code that can be executed to match a state change in the CPU and RAM. The main idea is to use reinforcement learning to learn a policy network and a value network to reduce the breadth and depth of the Monte Carlo Tree Search (MCTS). The authors also propose a multi-entropy policy sampling technique to alleviate the online update correlations. The proposed approach is evaluated on a few tasks and shows better performance than the baselines."
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper studies the effect of model architecture on the speed of training in the context of gradient descent optimization. The authors use the ideas from prior work that shows gradient descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H to characterize the convergence rate. They introduce a simple analysis technique that enumerates H in terms of all possible “paths” in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the time of convergence."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper studies the connection between strongly overparametrized neural networks (NNs) and kernel methods. In particular, the authors prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: the first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows the authors to transfer generalization bounds from minimum complexity kernel methods to NNs; (b) in the opposite regime, the test performance of wide NNs increases significantly with the initialization variance."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"This paper proposes to improve the depth estimation and stereo-based 3D object detection using pseudo-LiDAR. The authors propose to use cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. They propose a depthpropagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. The proposed method is evaluated on the KITTI object detection benchmark."
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a principled adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, they train K base detectors where the i-th detector is trained to discriminate natural data of class i from adversarial examples perturbed from other classes. At inference time, they first get the predicted label (say k) of the input, and then use the kth detector to identify whether the input is a natural sample (of class k) or an adversarial sample (perturbed from the other classes). They further devise a generative approach to detect/classify adversarial samples by interpreting each base detector as an unnormalized density model of the classconditional data."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper proposes an intrinsic reward for model-free reinforcement learning that encourages the agent to take actions that lead to significant changes in its learned state representation. The authors propose to use the Euclidean distance between the predicted next state representations and the actual next state representation as intrinsic reward. The proposed method is evaluated on procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work."
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper studies the problem of large-scale query-document retrieval, where the goal is to retrieve the set of documents that contain the answer to a question from a large document corpus. The retrieval phase consists of two steps: (1) reducing the solution space, (2) returning a subset of candidate documents, and (3) re-ranks the documents. This paper proposes a set of pretraining tasks to improve the performance of embedding-based Transformer-based retrieval models. The paper shows that pretraining with these tasks improves the performance over the baselines. "
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper introduces a new graph convolution operation, called bipartite graph convolutions, which is a parameterized transformation between different input and output graphs. The authors claim that it is general enough to subsume conventional graph convolve and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. The proposed method is evaluated on some common applications to show that it can generate comparable or better performance."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,This paper proposes a feature-wise transformation layer for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. It further applies a learning-to-learn approach to search for the hyper-parameters of the feature transformation layers. The proposed method is applicable to various metric-based few-shot classification algorithms and provides consistent improvements on the performance under domain shift. The authors conduct extensive experiments and ablation studies under the domain generalization setting on five benchmark datasets.
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper presents an approach to Lagrangian fluid simulation with a new type of convolutional network. The networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, this paper does not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. The authors show that their network architecture can simulate different materials, generalize to arbitrary collision geometries, and can be used for inverse problems."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes a new ensemble method, called BatchEnsemble, to reduce the computational and memory cost for training and testing neural networks. The proposed method is based on the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. The method is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Experiments on CIFAR-10/100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks demonstrate the effectiveness of the proposed method."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network-based solver for solving high-order PDEs. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. The resulting solution is an explicit smooth differentiable function with a known analytical form. The proposed method is grid free, mesh free and shape free, and the solution is approximated by a neural networks. The authors demonstrate the method on several free shape 2D systems with application to Electrical Impedance Tomography (EIT), diffusion and wave equations."
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper analyzes the architecture and training procedure of binarized neural networks (BNNs), a class of neural networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. The authors propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solver without sacrificing accuracy on the primary task. The experimental results demonstrate that the approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets."
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNnmp can lose a significant portion of their power when their depth and width is restricted."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a localised generative flow (LGF) method for density estimation, which is composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. The method is a generalization of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but the authors propose a simple variational scheme that performs well in practice."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper studies the performance gap between seen and unseen environments in vision-and-language navigation (VLN) tasks, trying to find where and why this performance gap exists and provide possible initial solutions. By designing the diagnosis experiments of environment re-splitting and feature replacement, the authors locate the environment bias to be in the low-level visual appearance; and they discuss semantic features that decrease performance gap in VLN tasks and achieve state-of-the-art results."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper studies the problem of using implicit human feedback to accelerate and optimize the training of deep reinforcement learning (DRL) agents. In particular, the authors propose a system to obtain and accurately decode human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. They demonstrate the feasibility of capturing error-potentials of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Then, they propose two different frameworks to integrate human into the training loop of RL agent based on active learning, while the second is to learn a reward function from imperfect demonstrations labeled by ErrP. Finally, they scale the implicit feedback (via ErrP) based RL to reasonably complex environments (games)."
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper proposes laconic classification as a way to compare and compare the performance of diverse image classifiers. The goal is to minimize the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, they compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows combining and comparing the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction, etc) on classification performance. The proposed method generalizes similar methods explored in previous works. "
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,This paper provides a comprehensive analysis of the robustness of perturbation-based adversarial defense techniques. The authors identify a family of defense techniques that are based on the instability assumption. The defenses include deterministic lossy compression algorithms and randomized perturbations to the input that all lead to similar gains in robustness. The results are consistent across both deterministic and stochastic channels that degrade the input example.
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper investigates the role of view prediction in the development of 3D visual recognition. The authors propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3d feature maps to novel viewpoints, to predict and match against target views, and propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. The proposed model learns visual representations useful for semi-supervised and unsupervised learning of 3d moving object detectors, both in simulation and in real world."
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised domain translation (UDT), i.e. finding meaningful correspondences between two domains, without access to explicit pairings between them. The authors define UDT in a rigorous, non-ambiguous manner, explore the implicit biases present in the approach and demonstrate the limits of theses approaches. Specifically, they show that mappings produced by these methods are biased towards low energy transformations, leading them to cast UDT into an Optimal Transport (OT) framework by making this implicit bias explicit. This allows them to provide theoretical guarantees for existing methods, and also to solve UDT problems where previous methods fail. Finally, making the link between the dynamic formulation of OT and CycleGAN, they propose a simple approach to solve the UDT, and illustrate its properties in two distinct settings."
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a novel regularization method, RotationOut, for neural networks. Different from Dropout that handles each neuron/channel independently, the proposed method regards its input layer as an entire vector and introduces regularization by randomly rotating the vector. The proposed method can also be used in convolutional layers and recurrent layers with small modifications. The authors further use a noise analysis method to interpret the difference between Rotationout and Dropout in co-adaptation reduction. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed methods."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,This paper proposes a method to create universal adversarial perturbations (UAP) for a given CNN in a data-free manner. The proposed method is based on finding the first singular vector of the linearly approximated neural network. The approximation is achieved by optimizing with the proposed dilate loss. Extensive experiments and ablations demonstrate that the proposed method achieves superior data free fooling performance.
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a transferable neural architecture search (NAS) method based on meta-learning. Specifically, it learns a meta-architecture that is able to adapt to a new task easily and quickly through a few gradient steps, which is more flexible than the existing NAS methods. In addition, it proposes an efficient first-order approximation algorithm. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. The proposed model is related to VIB and variational dropout, but provides a simpler and more direct realization via neuron regularization by a non-informative activation prior. The experiments show that this simple framework has diverse benefits for network pruning, adversarial defense and label noise robust learning."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta-learning approach to generate curiosity mechanisms for reinforcement learning (RL) agents. The core idea is to search over a space of curiosity mechanisms that dynamically adapt the agent’s reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. The outer loop will search over these mechanisms, and the inner loop performs standard RL. The inner loop is composed of two parts: (1) a curiosity module that predicts the reward, and (2) a reinforcement learning module that learns the parameters of the reward module. The paper proposes to meta-learn the meta-programs, which are pieces of code similar to those designed by humans in ML papers. The proposed approach is evaluated on grid-navigation, acrobot, lunar lander, ant, and hopper tasks."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"This paper proposes a new approach for Any-Code-to-Code Generation (AnyC2C) that leverages the strict syntax of programming languages to model a code snippet as a tree – structural language modeling (SLM). SLM estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. The authors present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous structural techniques that have severely restricted the kinds of expressions that can be generated in this task, the approach can generate arbitrary expressions in any programming language. The model significantly outperforms both seq2seq and a variety of existing structured approaches in generating Java and C# code."
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper studies the problem of learning large-scale neural networks (NN). The authors introduce a tool called canonical space and show that the objective functions in learning NNs are convex in the canonical model space. They further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, they show that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"This paper proposes interactive graph-based segmentation algorithms that enforce connectivity. The proposed method is based on a discrete Potts model and a class-aware integer linear programming (ILP) formulation that ensures global optimum. Both algorithms can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. The authors present competitive semantic (and panoptic) segmentation results on the PASCAL VOC 2012 and Cityscapes dataset given initial scribbles. They also demonstrate that the interactive approach can reach 90.6% mIoU on VOC validation set with an overhead of just 3 correction scribbles and can be used for interactive annotation on new or existing datasets."
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper proposes to use learned saliency models to detect adversarial perturbations in the salient features of an image. The saliency maps of adversarial images differ from those of natural images, and the proposed method is able to distinguish between adversarial and natural images using salient pixels as its input. The method is evaluated on MNIST, CIFAR-10, and ASSIRA, and can detect various adversarial attacks."
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper studies the problem of adversarial robustness, i.e., the probability that a trained model is susceptible to adversarial attacks at any point sampled from the (unknown) input distribution. In particular, the authors consider the setting where the model is trained via stochastic gradient descent and with iterative pruning techniques. They show that the concentration inequalities can be used to compute global robustness with estimation error upper-bounded by, for any > 0 selected a priori. They then use these techniques to provide statistically sound analysis of the robustness/accuracy trade-off for a variety of neural networks architectures and training methods on MNIST, Fashion-MNIST and CIFAR."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the problem of robust reinforcement learning, i.e. finding the optimal policy with some extent of robustness to environmental dynamics. Existing learning algorithms usually enable the robustness though disturbing the current state or simulated environmental parameters in a heuristic way. This paper proposes to leverage Wasserstein distance to measure the disturbance to the reference transition kernel. The authors show the existence of optimal robust policies, provide a sensitivity analysis for the perturbations, and then design a novel robust learning algorithm—Wasserstein Robust Advantage Actor-Critic algorithm (WRAAC). The proposed algorithm is verified in the Cart-Pole environment."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method to approximate mixed strategy Nash equilibria in multi-player continuous games, which always exist and include the pure ones as a special case. The authors use the pushforward measure technique to represent a mixed strategy in continuous spaces and generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players’ joint strategy profile and a Nash equilibrium. The proposed method is shown to converge to a stationary Nash equilibrium under the convexity assumption on payoff functions, the same popular setting as in previous studies."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper proposes a neural execution tree (NExT) framework for natural language (NL) explanations to augment training data for text classification using NL explanations. NExT transforms NL explanations into executable logical forms by semantic parsing and generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper extends the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper studies the problem of generalization to visually diverse environments in deep reinforcement learning. They formalize the problem, illustrated the inefficiencies of standard domain randomization, and proposed a theoretically grounded method that leads to robust, low-variance policies that generalize well. They conducted several experiments in different environments of differing complexities using both on-policy and off-policy algorithms to support their claims."
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper casts deep metric learning (DML) as a pairwise binary classification problem and proposes a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows the authors to recover the state-of-the-art complicated losses and also to induce novel variants. Experiments on several benchmark data sets demonstrate that the proposed method outperforms the existing methods."
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,This paper studies the problem of finding a local minimum in non-convex finite-sum minimization. The authors propose a sample-efficient stochastic trust region (STR) algorithm that finds an approximate local minimum within $\tilde{O}(\sqrt{n/\epsilon})$ stochastically Hessian oracle queries. They also develop Hessian-free STR algorithms that achieve the lowest runtime complexity.
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper proposes a method for training deep neural networks that ensures at least one neuron is active at a given layer. The method is based on the interaction of the geometry of the weights with the data: by simply adding one linearly dependent row to the weight matrix, the proposed method ensures that no neurons are dead. The authors empirically demonstrate that training with larger learning rates is possible even in the absence of batch normalization and with default initialization."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper studies the problem of computing exact robustness certificates for deep classifiers with differentiable activation functions in two steps. First, it shows that if the eigenvalues of the Hessian of the network are bounded, we can compute a robustness certificate in the l2 norm efficiently using convex optimization. Second, it derives a computationally-efficient upper bound on the curvature of a deep network. It also proposes a curvature-based regularization term to boost the certified robustness against adversarial examples. Numerical results show that the proposed method outperforms the CROWN certificate."
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a method for compressed sensing recovery using untrained deep generative models. The method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. The authors show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. They also introduce a novel learned regularization technique, which incorporates prior information on the network weights."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes a general HRL framework TAIC for learning temporal abstraction from action sequences. The authors formulate the temporal abstraction problem as learning latent representations of action sequences and present a novel approach of regularizing the latent space by adding information-theoretic constraints. Specifically, the authors maximize the mutual information between the latent variables and the state changes. The learned abstraction allows us to learn new tasks on higher levels more efficiently."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper proposes a layer-wise sampling strategy for training graph convolutional networks (GCNs). The proposed sampling strategy samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, the proposed method can potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-irectional influence (correlation). Further, the authors apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper presents STOVE, a state-space model for unsupervised video modeling and planning. It is constructed by combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. The authors also demonstrate the strength of the model as a simulator for sample efficient model-based control in a task with heavily interacting objects."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"This paper proposes a VAE-GAN fusion model that combines the best of both VAE and GAN, i.e., sharp and coherent samples and low-dimensional representations. The authors propose to use an implicit VAE model with an adversarially trained discriminator to train the VAE with an implicit likelihood by an adversarial training. They provide a theoretical analysis of their objective and show that it is equivalent to the Jeffreys divergence. In experiments, they demonstrate that their model achieves a good balance between generation and reconstruction quality."
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper shows that adversarial attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. The authors present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low-dimensions. They introduce new datasets of realistic images of faces and digits where the optimal Bayesian classifier can be calculated efficiently. They show that for some of these datasets the optimal classes are robust and for others it is vulnerable to adversarial examples. In systematic experiments, they find that standard CNN training consistently finds a vulnerable classifier."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the impact of magnitude pruning on ImageNet classification and finds that certain examples, which they term pruning identified exemplars (PIEs), and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both sparse and non-sparse models. These hard-to-generalize-to images tend to be mislabelled, of lower image quality, entail abstract representations, atypical examples or require fine-grained classification."
SP:4b17edaa7ec6201891433320d85f9a415656b763,This paper proposes a method to tackle the challenge of large natural language action spaces in interactive fiction games. The proposed method builds a dynamic knowledge graph (KG-A2C) that is used to reason about the game state and to constrain natural language generation. The authors claim that the dual uses of the KG and the template-based action space are the keys to scalable exploration of combinatorially large text-based natural language actions. Experiments on a wide variety of IF games show that the proposed method outperforms current IF agents.
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper proposes a data-dependent Gaussian prior (D2GPo) that is poles apart from the data-independent Gaussian regularization (L2 regularization) commonly adopted in smoothing the training of maximum likelihood estimation (MLE). The proposed method makes effective use of a more detailed prior in the data and has improved performance in typical language generation tasks, including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning. "
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper proposes to replace the widely used cross-entropy loss with focal loss to improve the calibration of deep neural networks. The authors provide a thorough analysis of the factors causing miscalibration, and use the insights to theoretically justify the empirically excellent performance of focal loss. They perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (SST, 20 Newsgroups/SST) datasets and show that the proposed approach achieves state-of-the-art accuracy and calibration in almost all cases."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper introduces LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The authors show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. They conduct experiments on networks with random weights as well as networks trained on MNIST, showing that their approach yields superior estimates, compared to baselines available in the literature."
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. The method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). The authors also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition)."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper proposes a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network. Both the associated selection masks as well as the neural network are trained simultaneously such that a good model performance is achieved while, at the same time, only a minimal amount of data is selected. During the inference phase, only the parts selected by the masks have to be transferred between the server and the client. The experiments show that it is often possible to achieve a good accuracy with significantly less input data."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a method for simultaneous classification and out-of-distribution detection. The proposed loss function includes two regularization terms where the first minimizes the l1 norm between the output distribution of the softmax layer of a DNN and the uniform distribution, while the second minimises the Euclidean distance between the training accuracy of aDNN and its average confidence in its predictions on the training set. Experimental results showed that the proposed method achieves state-of the-art results in OOD detection with OE in both image and text classification tasks. "
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, the authors demonstrate the superior performance of the proposed method."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper studies the problem of learning a collective policy that can be applied to the real-world environment. In particular, the authors propose a setting where agents have different biases, which results in imperfect evidence collected for taking optimal actions. The agents take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents’ biases vary, the collective policy developed while sequentially visiting the internal simulations can complement one another’s shortcomings. The collective policies consistently achieve significantly higher returns than the best individually trained policies."
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper proposes a dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that the proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light-and-shadows in nature. GLAS provides a useful coarseto-fine control benefiting from scalability of Gaussian mask. The authors also devised the ability to identify multiple instances through recursive GLAS. They prove the effectiveness of GLAS for fine-grained classification using the fine-Grained classification dataset."
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"This paper proposes a method to remove pixel-wise and channel-wise correlations before the data is fed into each layer of a convolutional neural network. The proposed method, called network deconvolution, is motivated by the center-surround structure found in biological neurons in the visual regions of the brain. The method is evaluated on CIFAR-10, Cifar-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"This paper proposes a novel quantization method for GANs based on EM algorithms, named as QGAN. It also proposes a multi-precision algorithm to help find an appropriate quantization precision of GAN given image qualities requirements. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GAN to even 1-bit or 2-bit representations with results of quality comparable to original models."
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies the global last-iterate convergence of Hamiltonian gradient descent (HGD) for convex-concave min-max optimization problems. The main result is that HGD converges globally to the Nash equilibrium of the Dirac-GAN (Mescheder et al. 2018) when $f(t) = t$ for some function $f$ with non-zero derivative. This result is a generalization of the result of Balduzzi et. al. (2018), which shows global convergence of HGD to the NE of DiracGAN. The authors also show convergence rates for stochastic HGD and Consensus Optimization (COO)."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of learning ResNet. Specifically, the authors consider the ResNet block hl = φ(hl−1) + \tau(hl^{1/\sqrt{L}) where $\tau$ is a scalar and $L$ is the number of residual blocks. They show that for standard initialization $h_l$ is sharp value in characterizing stability of forward/backward process of ResNet, where stability is guaranteed for $tau \leq 1/L$. Moreover, if ResNet is properly over-parameterized, they show that gradient descent is guaranteed to find the global minima 1, which significantly enlarges the range of $\tilde{O}(L)$ that admits global convergence in previous work."
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. The authors show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"This paper proposes a method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. The method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations such as translation, zoom or color variations. The authors demonstrate the effectiveness of their method qualitatively and quantitatively, both for GANs and variational auto-encoders."
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a physics-as-inverse-graphics (PIGS) framework to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled state and velocity representations are not available. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. The authors demonstrate that PIGS significantly outperforms related methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), and demonstrate the value of this integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"This paper considers the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the “clean” probability is exploited as a relevance measure. Each noisy example is weighted by its relevance when learning the classifier for the end task. Experimental results show that using noisy images weighted by this relevance measure significantly improves the classification accuracy."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"This paper proposes a new objective function for GNNs, called Edge Information Maximized Graph Neural Network (EIGNN), that maximizes the mutual information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The authors theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper proposes a method for verifying the properties of generative models. The proposed method is based on a relaxation of the latent space of a generative model, and is able to capture sufficient non-convexity so as to be able to produce precise bounds on the output. The method can be used to verify both deterministic and probabilistic abstract interpretation and captures infinite sets of outputs of the generative network. It is shown that the method is faster and more precise than previous methods."
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the problem of “suspended animation” in graph neural networks (GNNs). In particular, the authors identify that when the model depth reaches the “suspended animation limit”, the model will not respond to the training data any more and become not learnable. To resolve the problem, this paper introduces the GRESNET (Graph Residual Network) framework, which creates extensively connected highways to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. The authors prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node’s representations between sequential layers."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper proposes a semi-supervised approach for the reconstruction of 3D geometry shape, albedo, and lighting from a single face image. The proposed method is based on a convolutional neural network (CNN) trained on a dataset generated from a linear 3D morphable model (3DMM). The proposed model disentangles identity, expression, pose, lighting, and expression representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that the proposed method produces high-quality reconstruction."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper proposes a method for imitation learning in the setting where the transition model is unknown. The authors propose to replace the known transition kernel with a synthetic kernel that simulates the transition of state components for which the transition kernel is known (s) and extracts from demonstrations the state components (su) for which kernel is unknown (su). The next state is then stitched from the two components: s = {sr, su}. The authors describe in detail the recipe for building an eMDP and analyze the errors caused by its synthetic kernel. They show that combining a policy gradient algorithm with our model achieves superior performance than the simulation-free alternative."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self-supervised reinforcement learning approach for learning to control states of interest (i.e., states of an object) by maximizing the mutual information between the context states and the states of the state of interest. The proposed approach is evaluated on two simulated robotic manipulation tasks (reaching, pushing, picking up, and sliding) and a navigation task (navigation). The results show that the proposed approach outperforms the baselines. "
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a trojan attack method for neural network (NN) trojaning attacks, which is an emerging and important attack that can broadly damage the system deployed with NN models. The attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim’s deployment. The trojan can be inserted into large-scale models, which provides well-learned general features, and can affect a large scope of applications. The proposed method outperforms existing studies in capability, generality, and stealthiness."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"This paper proposes a novel few-shot regression (FSR) algorithm for drug discovery. The proposed algorithm is based on deep kernel learning, where a neural network is used to predict a kernel function and a differentiable kernel algorithm is used for learning the kernel function. The choice of kernel function is important and the proposed algorithm learns to find the appropriate kernel for each task during inference. Experiments are conducted on both toy and real-world drug discovery datasets. "
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper proposes a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. The authors formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new metric for evaluating conditional generative adversarial networks (cGANs). The proposed metric, called Fréchet Joint Distance (FJD), is defined as the distance between joint distributions of images and conditioning, allowing it to implicitly capture the aforementioned properties in a single metric. The authors conduct experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to existing metrics. Moreover, they use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities (e.g. class labels, object masks, bounding boxes, images, and text captions)."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper proposes a method to identify decision states, i.e. the set of states where decisions made by an agent affect the future states it can reach in an environment. The authors use the VIC framework (Gregor et al., 2016), which maximizes an agent’s ‘empowerment’ and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work (Goyal et al. 2019), the decision states are discovered without extrinsic rewards – simply by interacting with the world."
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. The method SEFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. The authors extensively compare their method to competitors on multiple healthcare time series datasets and show that it performs competitively while significantly reducing runtime."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper proposes a new convolution operation, called Harmonic convolution, to improve the performance of deep convolutional neural networks (CNNs) for the task of audio prior modeling. The authors propose to use sets of harmonic series (Harmonical series) instead of local neighborhoods (LN) kernels as convolution kernels for the convolution operations. They show that the proposed method improves the performance on unsupervised audio restoration and sound separation tasks. "
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper proposes a data-augmentation technique called “data echoing” to reduce the total computation used by earlier pipeline stages and speed up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipelines in order to reclaim idle capacity. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. In all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes a variant of successor features (successor features) and variational intrinsic features (VIPS) to tackle the generalization problem in few-step reinforcement learning (RL). In particular, VIPS and VIPS can be viewed as extensions of each other, where VIPS is used to learn the successor features for VIPS, and VIPs are used for learning successor features. The main contribution of this paper is to combine VIPS with successor features to improve generalization of VIPS. The proposed method is evaluated on the full Atari suite and achieves human-level performance. "
SP:83500230586a9134f910ad067b7233dc563dc1ba,"This paper investigates the properties of deep neural networks (DNNs) from the theoretical and empirical perspective. The authors propose a functional view of DNNs, which is based on the notion of functional approximation. Theoretical and empirical results are provided to support the claim that the smoothness of the functional approximation, combined with a flat initial approximation, explains why massively overparameterized networks are able to generalize well."
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper proposes a new method for image-to-image translation based on attention mechanism. Specifically, the discriminator is equipped with an attention mechanism so that it estimates the probability that its input is real, but also does it create an attention map that highlights the critical features for such prediction. This attention map then assists the generator to produce more plausible and realistic images. The proposed method is evaluated on a number of image transfer tasks."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper investigates the role of multiplicative interaction layers as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions. The authors conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required, and argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, the authors demonstrate the potential of such layers by applying them in large-scale complex RL and sequence modelling tasks, where their use allows them to deliver state-of-the-art results, and thereby provides new evidence in support of multiplication playing a more prominent role when designing new neural network architectures."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper proposes a new algorithm for batch active learning with deep neural network models. The algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters and consistently performs as well or better than other baselines."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"This paper proposes a method to improve the interpretability of deep neural networks (DNNs). In particular, the authors claim that existing DNNs are hard to interpret because each hidden layer carries a mix of low level features and high level features. The authors propose a novel feature leveling architecture that isolates low-level features from high- level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that the modified models are able to achieve competitive results on standard datasets while being more self-explainable."
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper proposes a method for extreme classification, i.e. training a classifier over a large number of classes, known as ‘extreme classification’. The proposed method is based on a scalable approximation to the softmax loss function via a generalized form of negative sampling. By generating adversarial negative samples from an auxiliary model, they prove that they maximize the signal-to-noise ratio of the stochastic gradient estimate. They further show that, while the auxiliary model introduces a bias, they can remove the bias at test time."
SP:29b52fee83309268d9864f3b1fc3617948577d41,"This paper proposes a new approach for efficient exploration that leverages a low dimensional encoding of the environment learned with a combination of model-based and model-free objectives. The approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty and leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational spaces. One key element of the approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy. The authors test their approach on a number of maze tasks, as well as a control problem and show that their exploration approach is more sample efficient compared to strong baselines."
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper studies the problem of out-of-distribution detection in the few-shot setting. The authors propose two new methods for this task, namely -MinDist and LCBO, and investigate their performance. They show that existing confidence scores developed in the supervised setting (i.e., setting with a fixed number of classes) are not suitable when used with popular Few-shot classifiers. The proposed confidence scores substantially outperform the baselines on both tasks."
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper proposes a generalized model of sequence generation that unifies decoding in directed and undirected neural sequence models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, the authors derive various neural sequences models as special cases, such as autoregressive, semi-autoregressive and refinement-based non-autorgressive models. This unification enables them to adapt decoding algorithms originally developed for directed sequence models to generate sequences directly from the encoded data. They demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019) on WMT’14 English-German translation. They also demonstrate that the proposed approach enables constant time translation with similar performance to linear-time translation."
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper proposes a two-stage approach for the recognition of mathematical expressions (MEs). In the first stage, this method locates and recognizes the math symbols of input image by object detection algorithm. In the second stage, it translates math symbols with position information into LaTeX sequences by seq2seq model equipped with attention mechanism. The authors claim that the detection of mathematical symbols and the structural analysis of mathematical formulas are carried out separately in two steps, which effectively improves the recognition accuracy and enhances the generalization ability."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,This paper proposes a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. They validate their approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and compressing a Mask R-CNN with a 26x factor.
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper introduces a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The authors claim that this attention mechanism improves representation-building and resolves ambiguities introduced by multiple layers of standard attention. They also claim that their attention maps give better insights into how it is capable of solving the Mathematics Dataset’s challenging problems."
SP:d319df820c6630c409fab32097652a083e8f53ea,This paper studies the problem of generalization in deep learning. The authors propose to learn a more general classification function by concatenating encodings of the input features and then train the classifier on the extended features. Theoretical results show that a learned classification function must be sufficiently complex for a classification task in order to be closer to the true classification function. Experiments demonstrate that a model trained on arbitrarily encoded input features is more robust to common corruptions and adversarial perturbations.
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper proposes a new graph pooling operation based on compressive Haar transforms, called HaarPooling. The proposed method is computed by following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressed Haar basis of the corresponding clustering. It operates in the frequency domain by the synthesis of nodes in the same cluster and filters out fine detail information by compressive haar transforms. Such transforms provide an effective characterization of the data and preserve the structure information. By the sparsity of the Haar transform, the computation is of linear complexity."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper studies sample-based point-cloud decoders that map a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. The authors develop three sample based decoder architectures and compare their performance to each other and show their improved effectiveness over feedforward architectures. In addition, they investigate the learned distributions to gain insight into the output transformation."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper presents a benchmark of real-world noisy labels at 10 controlled noise levels. The authors conduct a large-scale study across a variety of noise levels and types, architectures, methods, and training settings. They show that: (1) Deep Neural Networks (DNNs) generalize much better on real- world noise. (2) DNNs may not learn patterns first on real world noisy data. (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (5) Robust learning methods that work well on synthetic noise may not work as well on real data and vice versa."
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. The authors propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,This paper proposes a memory-based graph neural network (MemGNN) and graph memory network (GMN) for hierarchical graph representation learning. The MemGNN is an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. The graph diffusion is used to define the topological embeddings and concatenate the node features and pass them through a few memory layers. Experiments show that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks.
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper studies the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. They show that for deep networks, the width needed for efficient convergence to a global minimum is independent of the depth. The width needed with Gaussian initializations scales linearly in the depth, and the benefits of a good initialization can persist throughout learning."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,This paper studies the problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. The authors propose a Lagrangian formulation to solve the optimization problem via Lagrangians. The proposed method obtains excellent results on deep neural networks. It can compress deep CNN ResNet-50 down to 2 bits with only 0.7% accuracy loss.
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,This paper proposes a method for training Wasserstein Generative Adversarial Networks (GANs) using an autoencoder and a generative model (iWGAN). The main idea is to fuse the encoder and generative models in an iterative primal-dual fashion. The authors provide a generalization error bound for iWGAN and provide a rigorous probabilistic interpretation of the model under the framework of maximum likelihood estimation. The proposed method is evaluated on both synthetic and real-world datasets and compared with other GAN methods.
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper proposes an extension of the mention pair model of anaphoric annotation (MPA) to alleviate the effects of sparsity inherent in some crowdsourcing environments. Specifically, the authors use a nonparametric partially pooled structure (based on a stick breaking process), fitting jointly with the ability of the annotators hierarchical community profiles. The individual estimates can thus be improved using information about the community when the data is scarce. The authors show, using a recently published large-scale crowdsourced anaphora dataset, that the proposed model performs better than its unpooled counterpart in conditions of sparse data, and on par when enough observations are available."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes a method to combine intrinsic and extrinsic rewards for exploration in sparse reward reinforcement learning (RL). The authors propose a hierarchical intrinsic reward, successor feature control (SFC), which is general and not task-specific. The intrinsic reward is based on statistics over complete trajectories, which differs from previous methods that only use local information to evaluate intrinsic motivation. The proposed method is evaluated on VizDoom, DeepMind Lab and DeepMind Control Suite. "
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper proposes a method for weakly supervised video moment retrieval, where the goal is to locate the video segment which is described by the sentence without having access to temporal annotations during training. To facilitate this alignment, the authors propose a multi-level co-attention mechanism to learn richer multimodal representations. The proposed method is comprised of a Frame-By-Word interaction module as well as a novel Word-Conditioned Visual Graph (WCVG) to learn visual-semantic representations that contain contextual information of their relative positions in the temporal sequence through iterative message-passing. Comprehensive experiments on the DiDeMo and Charades-STA datasets demonstrate the effectiveness of the learned representations."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a method for image-guided re-rendering of reconstructed objects for virtual and augmented reality applications. The main idea is to train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry via multi-view stereo. Based on this proxy geometry, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. In the target view, the pipeline reinserts the new view-independent effects. To composite multiple reprojected images to a final output, the authors learn a composition network that outputs photo-realistic results."
SP:257d124367b1da9a595dc11a9df750d6bade298e,"This paper presents a sparse representation of model uncertainty for deep neural networks (DNNs) that relies on an inverse formulation of Multivariate Normal Distribution (MND): an information form. The authors show that the model uncertainty can be estimated in this form using a scalable Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. They further devise a novel low-rank approximation of this eigen-basis that exploits spectral sparsity of DNNs. "
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes a load-balanced hashing method to improve the accuracy of One Permutation Hashing (OPH) and reduce the number of empty bins in Minwise Hashing. The proposed method, called AHash, aims to balance the load of the bins (the number of elements in a bin) so as to generate as few empty bins as possible. Experiments on real datasets validate the effectiveness of the proposed method."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,This paper proposes a feature extraction method for periodic signals. The proposed method is based on adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. Simulation and experimental results illustrate the effectiveness of the proposed method. 
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"This paper proposes a method to improve the fidelity of conditional text generation. The authors propose a confidence-oriented decoder that assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data-to-text dataset – WikiBio (Lebret et al., 2016) show that the approach is more faithful to the source than existing state-of-the-art approaches."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"This paper proposes a new method for pruning neural networks based on magnitude-based pruning. The proposed method is motivated from the perspective of minimizing the Frobenius distortion of a linear operator corresponding to a single layer, and extends the single layer optimization to a multi-layer optimization. Theoretical results show that the proposed method can achieve better performance than the vanilla magnitude based pruning (MP) method on VGG and ResNet."
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper proposes Moniqua, a decentralized stochastic gradient descent (SGD) algorithm that allows decentralized SGD to use quantized communication. Theoretical results show that the proposed algorithm can communicate a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Empirically, the authors demonstrate empirically that moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper introduces a general family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations. They show that partial models can be causally incorrect: they are confounded by the observations they don’t model, and can therefore lead to incorrect planning. They propose a simple modification to partial models so that they can still make correct predictions under changes in the behavior policy, which they validate theoretically and experimentally."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a new variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed model is based on two information theoretic problems—distributed simulation and channel synthesis—in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. The authors decompose a pair of data variables into their common representation and local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images."
