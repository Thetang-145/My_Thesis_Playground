,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,Federated learning ( FL ) USED-FOR machine learning models. decentralized data sources USED-FOR machine learning models. local differential privacy constraints USED-FOR FL. communication efficiency CONJUNCTION highdimensional compatibility. highdimensional compatibility CONJUNCTION communication efficiency. sqSGD ( selective quantized stochastic gradient descent ) HYPONYM-OF gradient - based learning algorithm. privacy - preserving quantization scheme USED-FOR algorithm. training performance CONJUNCTION communication costs. communication costs CONJUNCTION training performance. fixed privacy budget USED-FOR gradient subsampling strategy. communication costs EVALUATE-FOR gradient subsampling strategy. training performance EVALUATE-FOR gradient subsampling strategy. randomized rotation USED-FOR quantization error. quantization CONJUNCTION perturbation. perturbation CONJUNCTION quantization. perturbation USED-FOR FL algorithm. quantization USED-FOR FL algorithm. privacy and communication constraints FEATURE-OF FL algorithm. benchmark datasets EVALUATE-FOR framework. LeNet CONJUNCTION ResNet. ResNet CONJUNCTION LeNet. sqSGD USED-FOR large models. local privacy constraints FEATURE-OF large models. ResNet HYPONYM-OF large models. LeNet HYPONYM-OF large models. sqSGD COMPARE baseline algorithms. baseline algorithms COMPARE sqSGD. fixed privacy and communication level FEATURE-OF sqSGD. OtherScientificTerm is sensitive data disclosures. Method is privacy - preserving FL algorithms. Generic is base algorithm. ,"This paper proposes a new gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) based on the privacy-preserving quantization scheme in federated learning (FL) for machine learning models trained on decentralized data sources. The FL algorithm is motivated by the local differential privacy constraints of FL with respect to sensitive data disclosures. The authors propose a fixed privacy budget for the gradient subsampling strategy, which is based on randomized rotation to reduce the quantization error. The proposed FL algorithm uses quantization and perturbation to improve the privacy and communication constraints of the FL algorithm. The experimental results show the effectiveness of the proposed framework on several benchmark datasets, including ResNet and LeNet.","This paper proposes a federated learning (FL) algorithm for machine learning models with decentralized data sources. The algorithm is based on sqSGD (selective quantized stochastic gradient descent), a gradient-based learning algorithm with local differential privacy constraints. The authors propose a privacy-preserving quantization scheme to improve the privacy of the FL algorithm. The proposed gradient subsampling strategy uses a fixed privacy budget and a randomized rotation to reduce the quantization error. The FL algorithm uses quantization and perturbation at the fixed privacy and communication level. The paper shows that the proposed framework outperforms the baseline algorithms on a number of benchmark datasets, including LeNet, ResNet, and large models with local privacy constraints and highdimensional compatibility. "
9,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,Self - attention networks ( SANs ) USED-FOR natural language processing tasks. it USED-FOR language representation. language knowledge USED-FOR it. prior knowledge USED-FOR language representation. prior knowledge USED-FOR general representation method. method USED-FOR SANs. prior knowledge USED-FOR SANs. it USED-FOR language representation. it USED-FOR prior word frequency knowledge. prior word frequency knowledge CONJUNCTION prior translation lexicon knowledge. prior translation lexicon knowledge CONJUNCTION prior word frequency knowledge. it USED-FOR prior translation lexicon knowledge. prior translation lexicon knowledge USED-FOR bilingual data. prior word frequency knowledge CONJUNCTION monolingual data. monolingual data CONJUNCTION prior word frequency knowledge. method COMPARE Transformer - based baseline. Transformer - based baseline COMPARE method. Method is neural networks. ,"This paper proposes a general representation method for self-attention networks (SANs) for natural language processing tasks. The proposed method is based on prior knowledge for SANs, and it uses language knowledge to learn a language representation based on the prior knowledge. The authors use prior word frequency knowledge, prior translation lexicon knowledge, and monolingual data to train the SANs. They show that the proposed method outperforms the Transformer-based baseline. ","This paper proposes a general representation method for natural language processing tasks, which is based on self-attention networks (SANs). The authors propose a new method for learning SANs with prior knowledge. The authors show that it is able to learn a language representation from language knowledge, and it can be combined with prior word frequency knowledge and prior translation lexicon knowledge to learn bilingual data and monolingual data. The proposed method is evaluated on a Transformer-based baseline and compared to other neural networks."
18,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"cat - and - mouse game USED-FOR cybersecurity. Moving Target Defense ( MTD ) HYPONYM-OF proactive defense methods. leader - follower games USED-FOR MTD. models USED-FOR sequential settings. incomplete information FEATURE-OF rational adversary. learning defense policies USED-FOR cyber - security. learning defense policies USED-FOR sequential settings. sequential settings USED-FOR cyber - security. optimal movement policy USED-FOR BSMGs. interaction USED-FOR optimal movement policy. Bayesian Stackelberg Markov Games ( BSMGs ) HYPONYM-OF game - theoretic model. BSMGs PART-OF landscape of incomplete - information Markov games. Strong Stackelberg Equilibrium ( SSE ) FEATURE-OF them. learning approach USED-FOR SSE. learning approach USED-FOR BSMG. MTD USED-FOR web - application security. optimal policy USED-FOR MTD domains. movement policy USED-FOR optimal policy. SSE FEATURE-OF BSMG. incomplete information FEATURE-OF MTD domains. MTD EVALUATE-FOR movement policy. OtherScientificTerm are reconnaissance, sub - optimal movement strategies, incomplete - information Markov games, and prior information. Method are movement strategies, defense policies, single - agent reinforcement learning techniques, and MTD system. Generic is they. ","This paper studies the problem of cybersecurity in a cat-and-mouse game. The authors propose a new proactive defense method, Moving Target Defense (MTD), which is based on the idea that a rational adversary with incomplete information can exploit the incomplete information of the rational adversary in order to attack the agent. The proposed MTD uses a game-theoretic model, Bayesian Stackelberg Markov Games (BSMGs), to learn the optimal movement policy for BSMGs. The models are trained in sequential settings, where the goal is to find the sub-optimal movement strategies that are the best for the agent in the current state. The defense policies are learned using single-agent reinforcement learning techniques, and the MTD system is trained using a learning approach that learns the SSE of the BSMG and the optimal policy for MTD domains that have incomplete information. MTD is shown to be effective in web-application security.","This paper proposes a novel method for improving the security of a cat-and-mouse game. The proposed method, Moving Target Defense (MTD), is an extension of proactive defense methods such as moving target defense (MDR) and Moving Target Attack (MTA). The main difference is that MTD is based on leader-following games, where the goal is to defend against an adversary that has access to information about the current state of the game. In contrast to MDR, MTD uses a single-agent reinforcement learning approach to learn the optimal movement policy for a game-theoretic model. The authors show that the proposed MTD system is more robust to adversarial attacks than MDR. "
27,SP:97911e02bf06b34d022e7548beb5169a1d825903,"unsupervised disentangled representation learning EVALUATE-FOR Variational Autoencoder ( VAE ) based frameworks. VAE im3 plementation choices USED-FOR PCA - like behavior. PCA - like behavior FEATURE-OF data sam4 ples. local orthogonality CONJUNCTION data re6 construction. data re6 construction CONJUNCTION local orthogonality. models USED-FOR entangled representations. architecture CONJUNCTION hyperparameter 7 setting. hyperparameter 7 setting CONJUNCTION architecture. architecture USED-FOR models. hyperparameter 7 setting FEATURE-OF models. multi9 ple VAEs PART-OF VAE ensemble framework. VAE ensemble 15 objective USED-FOR linear transformations. approach COMPARE unsupervised disen18 tangled representation learning approaches. unsupervised disen18 tangled representation learning approaches COMPARE approach. OtherScientificTerm are model identifiability, disentangled representations, signed permutation transformation, pair - wise linear transformations, VAEs, triv16 ial transformations, and latent representations. Method are VAE based disentanglement 5 frameworks, and VAE ensemble. Generic is It. ","This paper studies the problem of unsupervised disentangled representation learning in Variational Autoencoder (VAE) based frameworks. The authors propose a VAE based disentanglement 5 frameworks, where the goal is to achieve model identifiability. The VAE ensemble framework is based on multi9 ple VAEs, where each VAE represents a set of data sam4 ples, and the VAE im3 plementation choices are used to predict PCA-like behavior of the PCA of the data sam2 ples. The models are trained to generate entangled representations with local orthogonality and data re6 construction. The architecture and hyperparameter 7 setting are also used to train the models. The proposed approach is shown to achieve better performance than the state-of-the-art in terms of performance compared to the state of the art in the hyperparameters. ","This paper proposes a new VAE based disentanglement 5 frameworks for learning disentangled representation learning. The authors propose Variational Autoencoder (VAE) based frameworks, which are based on VAE im3 plementation choices. The VAE ensemble framework is based on multi9 ple VAEs, where each VAE is a signed permutation transformation, and the authors show that the PCA-like behavior of data sam4 ples can be modeled as PCA of the VAEs. The paper also shows that the models can learn entangled representations in the hyperparameter 7 setting, and that the model identifiability can be improved. It is also shown that the proposed approach outperforms unsupervised disen18 tangled representation learning approaches in terms of the number of latent representations. The main contribution of the paper is to propose a new architecture for learning models for disentangling representations, and to propose pair-wise linear transformations that can be represented as VAEs with triv16 ial transformations. This is achieved by using the VAE ensembles 15 objective for linear transformations."
36,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,zero - shot approach USED-FOR automated machine learning ( AutoML ). model USED-FOR supervised learning task. zero - shot approach USED-FOR model. approach USED-FOR AutoML. meta - feature extractor USED-FOR data. free - text descriptions CONJUNCTION meta - feature extractor. meta - feature extractor CONJUNCTION free - text descriptions. transformer - based language embedding USED-FOR algorithms. meta - feature extractor USED-FOR method. free - text descriptions USED-FOR algorithms. transformer - based language embedding USED-FOR method. meta - feature extractor USED-FOR algorithms. graph neural network USED-FOR machine learning pipeline. approach USED-FOR AutoML. unsupervised representation learning USED-FOR AutoML. unsupervised representation learning USED-FOR natural language processing. unsupervised representation learning USED-FOR approach. Method is AutoML systems. Metric is running time. OtherScientificTerm is prediction time. ,"This paper proposes a zero-shot approach for automated machine learning (AutoML). AutoML is a model that can be used for any supervised learning task. The proposed approach is based on the zero-shooting approach in AutoML, where the model is trained using a graph neural network. The authors propose two algorithms based on transformer-based language embedding and a meta-feature extractor to extract data from the data using free-text descriptions and meta-features extractor. The approach is applied to AutoML with unsupervised representation learning for natural language processing. Experimental results show that the proposed approach outperforms the state-of-the-art AutoML systems. ",This paper proposes a zero-shot approach for automated machine learning (AutoML). AutoML is a model for a supervised learning task where the goal is to predict the output of the model. The proposed approach is based on the idea of unsupervised representation learning for natural language processing. The authors propose two algorithms based on transformer-based language embedding and meta-feature extractor to extract the data from free-text descriptions. The paper also proposes a graph neural network to guide the machine learning pipeline. The running time of AutoML systems is measured by prediction time.
45,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,optimization process USED-FOR non - compositional solutions. compositionality learning approaches USED-FOR compositionality. model architecture design USED-FOR compositionality learning approaches. compositional learning CONJUNCTION gradient descent. gradient descent CONJUNCTION compositional learning. machine learning models USED-FOR human - level intelligence. Method is neural network optimization. Task is compositional generalization. ,This paper proposes a new optimization process for non-compositional solutions in neural network optimization. The authors propose a new model architecture design for compositionality learning approaches to improve the compositionality in the optimization process. The compositional learning and gradient descent can be combined to improve compositional generalization. The proposed machine learning models can be used to improve human-level intelligence.,This paper proposes a novel optimization process for non-compositional solutions. The compositionality learning approaches for compositionality are based on model architecture design. The compositional learning can be combined with gradient descent. The authors show that compositional generalization can be achieved using machine learning models for human-level intelligence.
54,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"Knowledge graph ( KG ) representation learning USED-FOR entity alignment. machine translation CONJUNCTION feature extraction. feature extraction CONJUNCTION machine translation. methods COMPARE embeddingbased ones. embeddingbased ones COMPARE methods. embedding spaces PART-OF KGs. pre - aligned entities USED-FOR embedding spaces. scoring function USED-FOR embedding learning. margin FEATURE-OF scoring function. margin USED-FOR representation discrepancy. approach USED-FOR KG - invariant and principled entity representations. feature distribution CONJUNCTION ontology knowledge. ontology knowledge CONJUNCTION feature distribution. neural ontologies PART-OF KGs. state - of - the - art ones HYPONYM-OF embedding - based entity alignment methods. Generic are they, paradigm, and model. Method is alignment learning. OtherScientificTerm is geometric distance. ","This paper proposes a new framework for entity alignment based on Knowledge Graph (KG) representation learning. The key idea is to use pre-aligned entities as embedding spaces in KGs, and then use a scoring function for embedding learning based on the margin between the embedding space and the geometric distance between the pre-altered entities. The authors show that the proposed framework can achieve state-of-the-art performance on a variety of tasks.",This paper proposes a new approach for entity alignment learning based on Knowledge graph (KG) representation learning. The key idea is to learn embedding spaces in KGs that are invariant to changes in the feature distribution and ontology knowledge. The authors propose a scoring function for embedding learning that is based on the margin of the representation discrepancy between the embedding space and the pre-aligned entities. They show that they are able to achieve state-of-the-art results in terms of geometric distance. They also show that the proposed approach is able to learn KG-invariant and principled entity representations. 
63,SP:0e42de72d10040289283516ec1bd324788f7d371,"Convolutional Neural Networks ( CNNs ) powered functionalities USED-FOR ubiquitous intelligent “ IoT cameras ”. medicineand wearable - related ones HYPONYM-OF applications. CNNs COMPARE IoT devices. IoT devices COMPARE CNNs. limited resources FEATURE-OF IoT devices. storage and energy cost USED-FOR CNNs. form factor FEATURE-OF PhlatCam. compression techniques USED-FOR storage and energy reduction. Sensor Algorithm Co - Design framework USED-FOR CNN - powered PhlatCam. SACoD USED-FOR CNN - powered PhlatCam. SACoD HYPONYM-OF Sensor Algorithm Co - Design framework. mask CONJUNCTION backend CNN model. backend CNN model CONJUNCTION mask. PhlatCam sensor CONJUNCTION backend CNN model. backend CNN model CONJUNCTION PhlatCam sensor. mask PART-OF PhlatCam sensor. differential neural architecture search USED-FOR mask. model compression CONJUNCTION energy savings. energy savings CONJUNCTION model compression. energy savings EVALUATE-FOR SACoD framework. model compression EVALUATE-FOR SACoD framework. task accuracy EVALUATE-FOR SACoD framework. PhlatCam imaging system EVALUATE-FOR SACoD. Method are IoT systems, CNN algorithm, and SOTA ) designs. Metric is camera form factor. OtherScientificTerm is model parameters. Generic is tasks. ","This paper studies the problem of ubiquitous intelligent “IoT cameras” with convolutional Neural Networks (CNNs) powered functionalities. The authors propose a new sensor Algorithm Co-Design framework, SACoD, for the CNN-powered PhlatCam, which is a combination of a mask, a backend CNN model, and a Phlatcam sensor. The main contribution of the paper is a new camera form factor, which can be applied to a variety of applications such as medicine and wearable-related ones. The storage and energy cost of existing CNNs have limited resources, and the proposed compression techniques are able to reduce the storage and the energy reduction. The paper also proposes a new CNN algorithm, called SOTA, which uses differential neural architecture search (DNN) to learn the mask and the backbones of the model parameters. The SACOD framework is shown to achieve better model compression, energy savings, and task accuracy compared to the SACon-based CNNs. ","This paper proposes a new architecture for image generation from images. The proposed architecture, called SACoD, is based on the sensor Algorithm Co-Design framework. The main idea of the proposed architecture is to reduce the size of the camera form factor of the sensor. The paper also proposes to use differential neural architecture search to learn the mask of the PhlatCam sensor and the backend CNN model. Experiments show that the proposed method is able to achieve state-of-the-art performance on a variety of tasks."
72,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"Honey bees USED-FOR complex social systems. dataset USED-FOR temporal matrix factorization model. temporal matrix factorization model USED-FOR average developmental path. lifetime trajectories PART-OF dataset. social sciences CONJUNCTION neuroscience. neuroscience CONJUNCTION social sciences. behavioral biology CONJUNCTION social sciences. social sciences CONJUNCTION behavioral biology. neuroscience CONJUNCTION information science. information science CONJUNCTION neuroscience. method USED-FOR behavioral heterogeneity. behavioral heterogeneity FEATURE-OF complex social systems. information science HYPONYM-OF fields. behavioral biology HYPONYM-OF fields. social sciences HYPONYM-OF fields. neuroscience HYPONYM-OF fields. OtherScientificTerm are global behavior, and social network. Material is honey bee colonies. ","This paper proposes a new dataset for learning the average developmental path of a temporal matrix factorization model for complex social systems. The dataset is composed of lifetime trajectories in the form of a social network. The authors show that this dataset can be used to learn the global behavior of a complex social system. They also show that the proposed method is able to capture behavioral heterogeneity in complex socials. They show that their method can be applied to a variety of fields such as social sciences, neuroscience, and information science.","This paper proposes a new dataset for learning complex social systems from the data of honey bees. The dataset consists of a temporal matrix factorization model that predicts the average developmental path of a set of lifetime trajectories. The authors also propose a method to measure the behavioral heterogeneity of complex social system. The method is based on the observation that the global behavior of a social network is similar to that of a honey bee colonies. Experiments are conducted on three fields: information science, neuroscience, and behavioral biology."
81,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"Deep neural networks USED-FOR image restoration and reconstruction tasks. noisy or corrupted measurement USED-FOR networks. pipeline USED-FOR data augmentation. Data Augmentation ( DA ) USED-FOR classification problems. data augmentation USED-FOR image reconstruction tasks. medical imaging FEATURE-OF image reconstruction tasks. invariances FEATURE-OF medical imaging measurements. naive DA strategies USED-FOR DA pipeline. invariances USED-FOR DA pipeline. problem regimes EVALUATE-FOR DA. fastMRI dataset EVALUATE-FOR DA. training data USED-FOR single - coil reconstruction. training data USED-FOR multi - coil reconstruction. multi - coil reconstruction CONJUNCTION single - coil reconstruction. single - coil reconstruction CONJUNCTION multi - coil reconstruction. training data CONJUNCTION training data. training data CONJUNCTION training data. Task is accelerated magnetic resonance imaging. OtherScientificTerm are under - sampled linear measurements, and high - data regime. Method is data augmentation pipeline. ","This paper studies the problem of accelerated magnetic resonance imaging. The authors propose a new data augmentation pipeline, called Data Augmentation (DA), to improve the performance of deep neural networks for image restoration and reconstruction tasks with noisy or corrupted measurement. The proposed pipeline is based on the naive DA strategies and is able to achieve state-of-the-art performance on a variety of image reconstruction tasks in medical imaging, including multi-coil reconstruction, single- coil reconstruction, and training data. The DA pipeline uses invariances from various medical imaging measurements, including under-sampled linear measurements. The paper shows that the DA pipeline can achieve state of the art performance on the fastMRI dataset, and is also able to perform well on other problem regimes. ","This paper proposes a new data augmentation pipeline for image restoration and reconstruction tasks. Data Augmentation (DA) is an important problem in classification problems where the network is trained with noisy or corrupted measurement. The authors propose a pipeline to perform the training of the DA augmentation. The proposed DA pipeline is based on naive DA strategies and is evaluated on medical imaging measurements with invariances, including accelerated magnetic resonance imaging. The paper shows that the DA pipeline can be applied to a wide range of problem regimes, including under-sampled linear measurements, single-coil reconstruction, multi-collision reconstruction, and training data. The DA is also evaluated on the fastMRI dataset. "
90,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"deep repulsive clustering ( DRC ) algorithm USED-FOR order learning. ordered data USED-FOR deep repulsive clustering ( DRC ) algorithm. order - related feature CONJUNCTION identity feature. identity feature CONJUNCTION order - related feature. facial age estimation CONJUNCTION aesthetic score regression. aesthetic score regression CONJUNCTION facial age estimation. aesthetic score regression CONJUNCTION historical color image classification. historical color image classification CONJUNCTION aesthetic score regression. algorithm USED-FOR ordered data. historical color image classification EVALUATE-FOR algorithm. facial age estimation EVALUATE-FOR algorithm. rank estimation EVALUATE-FOR algorithm. Method is order - identity decomposition ( ORID ) network. OtherScientificTerm are identity features, repulsive term, and rank. ","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning on ordered data. The proposed algorithm is based on order-identity decomposition (ORID) network, where the order-related feature is replaced by the identity feature and the repulsive term is replaced with the rank. The algorithm is evaluated on facial age estimation, aesthetic score regression, and historical color image classification.","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning on ordered data. The key idea is to decompose the order-identity decomposition (ORID) network into two parts: (1) order-related feature and (2) identity feature. The repulsive term is defined as the difference between the order and identity features. The authors show that the proposed algorithm can be applied to ordered data with respect to the repulsive terms. The algorithm is evaluated on facial age estimation, aesthetic score regression, and historical color image classification. The proposed algorithm is shown to outperform rank estimation."
99,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"Exploration PART-OF model - free reinforcement learning. sparse reward USED-FOR Exploration. intrinsic rewards USED-FOR state - of - the - art methods. methods USED-FOR procedurally - generated environments. intrinsic rewards USED-FOR methods. episode - level exploration method USED-FOR procedurally - generated environments. RAPID HYPONYM-OF episode - level exploration method. per - episode and long - term views FEATURE-OF episodic exploration score. episodic exploration score EVALUATE-FOR RAPID. sparse MuJoCo tasks EVALUATE-FOR method. procedurally - generated MiniGrid environments EVALUATE-FOR method. RAPID COMPARE intrinsic reward strategies. intrinsic reward strategies COMPARE RAPID. sample efficiency EVALUATE-FOR intrinsic reward strategies. sample efficiency EVALUATE-FOR RAPID. OtherScientificTerm are uncertain environment dynamics, and ranking buffer. Material is MiniWorld. ","This paper proposes a new episode-level exploration method for procedurally-generated environments, called RAPID. The authors propose a sparse reward for exploration in model-free reinforcement learning, where the goal is to explore the environment with uncertain environment dynamics. They show that the intrinsic rewards in state-of-the-art methods can be used to improve the sample efficiency of these methods. They demonstrate that the episodic exploration score of RAPid is better than the intrinsic reward strategies on sparse MuJoCo tasks. ","This paper proposes an extension of exploration in model-free reinforcement learning to the context of uncertain environment dynamics. Exploration is a sparse reward, where the goal is to find a good sparse reward for a given state-of-the-art methods. The authors propose an episode-level exploration method called RAPID, which can be applied to procedurally-generated environments with intrinsic rewards. The method is evaluated on sparse MuJoCo tasks with per-episode and long-term views, and the episodic exploration score is computed using a ranking buffer. The proposed method is shown to outperform intrinsic reward strategies in terms of sample efficiency, and is also shown to be effective in procedurally generated MiniGrid environments. "
108,SP:30024ac5aef153ae24c893a53bad93ead2526476,"semantic space of class attributes CONJUNCTION visual space of images. visual space of images CONJUNCTION semantic space of class attributes. Isometric Propagation Network ( IPN ) USED-FOR class dependency. IPN USED-FOR class representations. auto - generated graph USED-FOR class representations. ZSL benchmarks EVALUATE-FOR IPN. them USED-FOR IPN. Method are Zero - shot learning ( ZSL ), static representation, and dynamic propagation procedures. OtherScientificTerm are class attributes, imbalanced supervision, and semantic and the visual space. Generic are representations, and mapping. Task is ZSL settings. Material is seen - class data. Metric is consistency loss. ","This paper studies the problem of Zero-shot learning (ZSL), where the goal is to learn a static representation from seen-class data without imbalanced supervision. The authors propose an Isometric Propagation Network (IPN) to learn the class dependency between the semantic space of class attributes and the visual space of images. The IPN learns the class representations from an auto-generated graph, which is then used to map the representations to the semantic and visual space.  The authors show that the IPN outperforms the state-of-the-art on several ZSL benchmarks. ","This paper proposes a new method for Zero-shot learning (ZSL), where the goal is to learn a static representation of an image. The key idea is to use an Isometric Propagation Network (IPN) to model the class dependency between the semantic space of class attributes and the visual space of images. The IPN can be used to learn class representations from an auto-generated graph. The authors show that the IPN outperforms the state-of-the-art on several ZSL benchmarks. "
117,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"multi - task model USED-FOR tasks. HyperGrid Transformers HYPONYM-OF Transformer architecture. task - conditioned hyper networks USED-FOR feed - forward layers. task - conditioned hyper networks USED-FOR Transformer architecture. decomposable hypernetwork USED-FOR grid - wise projections. global ( task - agnostic ) state CONJUNCTION local task - specific state. local task - specific state CONJUNCTION global ( task - agnostic ) state. method USED-FOR hypernetwork. SuperGLUE test set EVALUATE-FOR state - of - the - art. fine - tuning CONJUNCTION multi - task learning approaches. multi - task learning approaches CONJUNCTION fine - tuning. method USED-FOR fine - tuning. method USED-FOR multi - task learning approaches. Task is natural language understanding tasks. Generic are model, and approach. OtherScientificTerm is weight matrices. Material is GLUE / SuperGLUE. ","This paper proposes HyperGrid Transformers, a Transformer architecture based on task-conditioned hyper networks with feed-forward layers. The proposed model learns a multi-task model to learn tasks from multiple tasks. The model is trained using a decomposable hypernetwork with grid-wise projections. The global (task-agnostic) state and the local task-specific state are learned by the hypernetwork. The method is evaluated on the GLUE/SuperGLUE test set and shows state-of-the-art performance. The authors also show that the proposed method can be used for fine-tuning and multi-tasks learning approaches. ","This paper proposes HyperGrid Transformers, a Transformer architecture based on task-conditioned hyper networks for feed-forward layers of a multi-task model for tasks where the model is trained on natural language understanding tasks. The proposed approach is based on a decomposable hypernetwork for grid-wise projections, where the global (task-agnostic) state and the local task-specific state are represented by weight matrices. The method is evaluated on the SuperGLUE test set on the state-of-the-art GLUE/SuperGLUE  and is shown to outperform other fine-tuning and multi-tasking approaches."
126,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"lighting CONJUNCTION weather. weather CONJUNCTION lighting. weather CONJUNCTION visibility conditions. visibility conditions CONJUNCTION weather. image input USED-FOR autonomous driving. image input USED-FOR learning algorithm. algorithm USED-FOR task. sensitivity analysis USED-FOR algorithm. sensitivity analysis USED-FOR task. approach USED-FOR learning outcomes. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. neural network training USED-FOR self - driving cars. approach COMPARE techniques. techniques COMPARE approach. algorithm USED-FOR neural network training. robustness EVALUATE-FOR algorithm. adversarial training HYPONYM-OF techniques. data augmentation HYPONYM-OF techniques. OtherScientificTerm are external and environmental factors, and sensors. Task is perceptual data processing. ","This paper proposes a new algorithm for autonomous driving based on image input from external and environmental factors. The proposed algorithm is based on a sensitivity analysis of the task. The authors show that the proposed approach can improve the learning outcomes of self-driving cars by using data augmentation, adversarial training, and adversarial learning. Empirical results show the effectiveness of the proposed algorithm in terms of robustness.","This paper proposes a novel approach for learning the learning outcomes of self-driving cars from perceptual data processing. The proposed approach is based on a sensitivity analysis of the external and environmental factors that affect the performance of autonomous driving, including lighting, weather, and visibility conditions. The authors propose a learning algorithm based on the image input for autonomous driving. The algorithm is evaluated on the task using sensitivity analysis and is shown to improve robustness compared to other techniques such as adversarial training and data augmentation. "
135,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"deep networks USED-FOR approximate solvers. hard constraints FEATURE-OF Large optimization problems. hard constraints FEATURE-OF problems. method USED-FOR feasibility. gradient - based corrections USED-FOR inequality constraints. differentiable procedure USED-FOR method. differentiable procedure USED-FOR feasibility. DC3 USED-FOR AC optimal power flow. DC3 USED-FOR synthetic optimization tasks. synthetic optimization tasks CONJUNCTION AC optimal power flow. AC optimal power flow CONJUNCTION synthetic optimization tasks. hard constraints FEATURE-OF physics of the electrical grid. DC3 USED-FOR near - optimal objective values. feasibility FEATURE-OF DC3. Method are classical solvers, deep learning approaches, and Deep Constraint Completion and Correction ( DC3 ). OtherScientificTerm are infeasible solutions, and equality constraints. Generic is algorithm. ","This paper studies the problem of approximate solvers in deep networks. Large optimization problems with hard constraints on the physics of the electrical grid are hard constraints for classical solvers. The authors propose Deep Constraint Completion and Correction (DC3), a method that uses gradient-based corrections to overcome the inequality constraints. DC3 is able to achieve near-optimal objective values in a variety of synthetic optimization tasks and AC optimal power flow. The proposed algorithm is well-motivated and well-written. The empirical results show the feasibility of DC3.","The paper proposes a method to improve the feasibility of approximate solvers in the presence of hard constraints in the setting of Large optimization problems with hard constraints on the physics of the electrical grid. The authors propose Deep Constraint Completion and Correction (DC3), which is a differentiable procedure for improving the feasibility. The proposed algorithm is based on gradient-based corrections to reduce the inequality constraints in classical solvers. DC3 is evaluated on synthetic optimization tasks and AC optimal power flow, and is shown to achieve near-optimal objective values. "
144,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"Regularization USED-FOR sparsity. Regularization USED-FOR deep neural network pruning. pruning schedule CONJUNCTION weight importance scoring. weight importance scoring CONJUNCTION pruning schedule. problems PART-OF pruning. weight importance scoring HYPONYM-OF pruning. pruning schedule HYPONYM-OF pruning. weight importance scoring HYPONYM-OF problems. pruning schedule HYPONYM-OF problems. it COMPARE one - shot counterpart. one - shot counterpart COMPARE it. L2 regularization variant COMPARE one - shot counterpart. one - shot counterpart COMPARE L2 regularization variant. rising penalty factors FEATURE-OF L2 regularization variant. Hessian information USED-FOR pruning. growing penalty scheme USED-FOR approach. approach USED-FOR Hessian information. networks USED-FOR structured and unstructured pruning. algorithms USED-FOR large datasets. large datasets CONJUNCTION networks. networks CONJUNCTION large datasets. networks EVALUATE-FOR algorithms. CIFAR and ImageNet datasets EVALUATE-FOR deep neural networks. OtherScientificTerm are small penalty strength regime, and regularization. Task is Hessian approximation problems. Generic is state - of - the - art algorithms. ","This paper studies the problem of sparsity in deep neural network pruning. Regularization is an important problem in sparsity, and it is important to consider the small penalty strength regime. The authors consider two problems in pruning: weight importance scoring and pruning schedule. The L2 regularization variant is a variant of the one-shot counterpart with rising penalty factors. The proposed approach is based on a growing penalty scheme, where the Hessian information is used to guide the pruning, and the authors show that the proposed approach outperforms the state-of-the-art algorithms. The algorithms are tested on large datasets and networks for structured and unstructured pruning on CIFAR and ImageNet datasets.","This paper proposes a new regularization for sparsity in deep neural network pruning. Regularization is an important problem in pruning, and the authors propose two problems: pruning schedule and weight importance scoring. The main idea is to use Hessian information to guide the pruning in the small penalty strength regime. The proposed approach is based on a growing penalty scheme. The authors show that it outperforms the one-shot counterpart and the L2 regularization variant with rising penalty factors. They also show that the proposed algorithms can be applied to large datasets and networks for structured and unstructured pruning on CIFAR and ImageNet datasets. "
153,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"Model - based planning USED-FOR deep, careful reasoning. deep, careful reasoning CONJUNCTION generalization. generalization CONJUNCTION deep, careful reasoning. Model - based planning USED-FOR generalization. generalization USED-FOR artificial agents. deep, careful reasoning USED-FOR artificial agents. deep function approximation USED-FOR model - based reinforcement learning ( MBRL ). planning USED-FOR MBRL agents. planning USED-FOR generalization. MuZero HYPONYM-OF MBRL algorithm. MuZero COMPARE MBRL algorithms. MBRL algorithms COMPARE MuZero. overlapping components CONJUNCTION MBRL algorithms. MBRL algorithms CONJUNCTION overlapping components. MBRL algorithm COMPARE MBRL algorithms. MBRL algorithms COMPARE MBRL algorithm. overlapping components PART-OF MuZero. overlapping components PART-OF MBRL algorithm. control tasks CONJUNCTION Atari. Atari CONJUNCTION control tasks. Atari CONJUNCTION 9x9 Go. 9x9 Go CONJUNCTION Atari. Planning USED-FOR learning process. Planning USED-FOR policy updates. Planning USED-FOR data distribution. Monte - Carlo rollouts USED-FOR shallow trees. Planning USED-FOR generalization. planning USED-FOR reinforcement learning settings. zeroand few - shot learning CONJUNCTION strategic thinking. strategic thinking CONJUNCTION zeroand few - shot learning. Model - based reinforcement learning ( MBRL ) COMPARE model - free methods. model - free methods COMPARE Model - based reinforcement learning ( MBRL ). data efficiency CONJUNCTION zeroand few - shot learning. zeroand few - shot learning CONJUNCTION data efficiency. zeroand few - shot learning EVALUATE-FOR model - free methods. data efficiency EVALUATE-FOR model - free methods. planning CONJUNCTION learning. learning CONJUNCTION planning. learning PART-OF methods. planning PART-OF methods. models USED-FOR intelligent artificial agents. models USED-FOR discrete search. planning PART-OF MBRL algorithm. MuZero HYPONYM-OF MBRL algorithm. value estimation CONJUNCTION policy optimization. policy optimization CONJUNCTION value estimation. learned model CONJUNCTION value estimation. value estimation CONJUNCTION learned model. search - based planning CONJUNCTION","This paper proposes a deep function approximation for model-based reinforcement learning (MBRL) based on Monte-Carlo rollouts. The authors show that the proposed MBRL algorithm, MuZero, can achieve better performance than previous MBRL algorithms with overlapping components and overlapping components in the learning process. They also show that MuZero performs better than MBRL with planning in a variety of reinforcement learning settings, including zeroand few-shot learning, strategic thinking, and strategic planning. ","This paper proposes a deep function approximation for model-based reinforcement learning (MBRL) based on Monte-Carlo rollouts. The authors show that the proposed MBRL algorithm outperforms MuZero and other MBRL algorithms in terms of the number of overlapping components and the generalization of deep, careful reasoning. Model-based planning is used to improve generalization in reinforcement learning settings. Planning is applied to the learning process and the data distribution. Planning can also be used for policy updates. Experiments are conducted on control tasks and 9x9 Go. The results show that model-free methods outperform model-by-planning and zeroand few-shot learning on both data efficiency and strategic thinking. "
162,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"long - range reasoning CONJUNCTION understanding of environment dynamics. understanding of environment dynamics CONJUNCTION long - range reasoning. Value Iteration Networks ( VINs ) USED-FOR implicit planning. long - range reasoning USED-FOR tasks. deep reinforcement learning USED-FOR implicit planning. understanding of environment dynamics USED-FOR tasks. graph representation learning CONJUNCTION neural algorithmic reasoning. neural algorithmic reasoning CONJUNCTION graph representation learning. contrastive self - supervised learning CONJUNCTION graph representation learning. graph representation learning CONJUNCTION contrastive self - supervised learning. generic environments USED-FOR VIN - style models. XLVINs COMPARE VIN - like models. VIN - like models COMPARE XLVINs. XLVINs COMPARE model - free baselines. model - free baselines COMPARE XLVINs. MDP USED-FOR VIN - like models. Generic is model. Task is planning computations. OtherScientificTerm are state space, and Markov decision process ( MDP ). Method is Latent Value Iteration Networks ( XLVINs ). ","This paper proposes a new model for planning computations. Value Iteration Networks (VINs) are used for implicit planning in deep reinforcement learning, where the state space is represented as a Markov decision process (MDP) and the goal is to maximize the return of the model. The authors propose to use long-range reasoning and understanding of environment dynamics to perform these tasks. The proposed model is evaluated on a variety of tasks, including graph representation learning, contrastive self-supervised learning, and neural algorithmic reasoning. The VIN-style models are shown to perform well in these generic environments, and are able to achieve better performance than the XLVIN-like models. ","This paper proposes a new model for planning computations. Value Iteration Networks (VINs) are used for implicit planning in deep reinforcement learning. The main idea is to use long-range reasoning and understanding of environment dynamics for tasks such as graph representation learning, contrastive self-supervised learning, and neural algorithmic reasoning. The proposed model is based on the Markov decision process (MDP). The authors show that the proposed VIN-style models outperform existing models in generic environments. The authors also show how the proposed model outperforms other model-free baselines."
171,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"Learning functions PART-OF machine learning. Boolean variables FEATURE-OF Learning functions. neural networks USED-FOR functions. distribution free setting FEATURE-OF functions. networks USED-FOR they. read - once DNFs HYPONYM-OF functions. convex neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION convex neural network. convex neural network USED-FOR functions. gradient descent USED-FOR functions. inductive bias FEATURE-OF learning process. ones HYPONYM-OF networks. networks USED-FOR risk. gradient descent USED-FOR compact representation. process USED-FOR DNF. it USED-FOR process. computer assisted proof USED-FOR inductive bias. inductive bias FEATURE-OF DNFs. computer assisted proof USED-FOR DNFs. network USED-FOR process. network USED-FOR DNF. optimization USED-FOR inductive bias. learning process CONJUNCTION optimization. optimization CONJUNCTION learning process. network USED-FOR l2 norm. network USED-FOR DNF terms. margin constraints FEATURE-OF l2 norm. OtherScientificTerm are uniform distribution, neurons, logical formulas, and high dimensional DNFs. Method is zero - error networks. Metric is population risk. Material is tabular datasets. ","This paper studies the problem of learning functions in machine learning with Boolean variables. Learning functions in the distribution free setting is an important problem in the context of zero-error networks, where the uniform distribution of the neurons is not known. The authors propose to use neural networks to learn functions in this setting, which they call read-once DNFs. The main idea is to use a convex neural network and gradient descent to learn the functions, which are then used to train a DNF. The proposed process is based on a computer assisted proof that shows the inductive bias of the learning process and the optimization of the DNF through the network. The paper also shows that the network is able to learn DNF terms in the l2 norm with margin constraints, and that it can be used to learn a compact representation of the data.   The paper is well-written and easy to follow. The theoretical results are clear and well-motivated. However, I have some concerns about the empirical performance of the paper. ","This paper studies the problem of learning functions in machine learning with Boolean variables. Learning functions in the distribution free setting is an important problem in the context of zero-error networks. The authors propose two new functions, read-once DNFs and ones with uniform distribution, and they show that they can be learned by neural networks. They show that these functions can be learnt by a convex neural network and gradient descent. They also show that it is possible to learn a compact representation by gradient descent, and that it can be done by learning a process with computer assisted proof. Finally, the authors show that the network can learn DNF terms that satisfy the l2 norm under margin constraints. The paper also shows that the inductive bias of the learning process and the optimization of the DNF can be reduced by the network. "
180,SP:6e600bedbf995375fd41cc0b517ddefb918318af,exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. sparse environment FEATURE-OF map. graph structure USED-FOR exploration directions. Graph Structured Reinforcement Learning ( GSRL ) USED-FOR value function estimation. graph structure USED-FOR value function estimation. graph structure PART-OF historical trajectories. graph structure USED-FOR Graph Structured Reinforcement Learning ( GSRL ). state transitions PART-OF replay buffer. GSRL USED-FOR dynamic graph. attention strategy USED-FOR map. state transitions USED-FOR dynamic graph. historical trajectories USED-FOR dynamic graph. graph structure USED-FOR value learning. GSRL COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE GSRL. sample efficiency EVALUATE-FOR state - of - the - art algorithms. sample efficiency EVALUATE-FOR GSRL. Task is reinforcement learning. OtherScientificTerm is sparse reward functions. ,"This paper proposes Graph Structured Reinforcement Learning (GSRL) for value function estimation with graph structure in the sparse environment. GSRL learns a dynamic graph with state transitions in a replay buffer, and uses the graph structure to learn exploration directions and exploitation in a sparse environment with sparse reward functions. The authors show that GSRL improves sample efficiency over state-of-the-art algorithms in terms of sample efficiency. ",This paper proposes Graph Structured Reinforcement Learning (GSRL) for value function estimation with graph structure for exploration directions and exploitation in a sparse environment. GSRL learns a dynamic graph with state transitions in a replay buffer and historical trajectories for value learning with attention strategy. The paper shows that GSRL achieves better sample efficiency than state-of-the-art algorithms with sparse reward functions.
189,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"Simulated environments EVALUATE-FOR systematic generalization of reinforcement learning agents. procedurally generated content FEATURE-OF Simulated environments. positions of entities CONJUNCTION asset appearances. asset appearances CONJUNCTION positions of entities. layout CONJUNCTION positions of entities. positions of entities CONJUNCTION layout. asset appearances CONJUNCTION rules. rules CONJUNCTION asset appearances. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR test levels. robustness EVALUATE-FOR test levels. levels USED-FOR learning progress. framework USED-FOR future learning potential. Prioritized Level Replay HYPONYM-OF framework. Prioritized Level Replay USED-FOR future learning potential. sample - efficiency CONJUNCTION generalization. generalization CONJUNCTION sample - efficiency. Procgen Benchmark environments CONJUNCTION MiniGrid environments. MiniGrid environments CONJUNCTION Procgen Benchmark environments. Prioritized Level Replay USED-FOR implicit curriculum. Generic is environment. OtherScientificTerm are environment transitions, training levels, agent, and temporal - difference ( TD ) errors. ","This paper studies systematic generalization of reinforcement learning agents in Simulated environments with procedurally generated content. The authors propose a framework called Prioritized Level Replay, which is based on prior work on the implicit curriculum. The key idea is to learn the environment transitions between training levels, and then use the learned levels to guide the learning progress. Theoretical results show that the proposed framework improves sample-efficiency, generalization, and robustness across different test levels. ","This paper presents a systematic generalization of reinforcement learning agents in Simulated environments with procedurally generated content. The authors propose a framework called Prioritized Level Replay, which is based on prior work on learning from environment transitions. The key idea of the framework is to learn the future learning potential of an agent at different training levels. The learning progress at different levels of the learning progress depends on the agent’s current state, the current state of the environment, the positions of entities, the layout, the rules, and the asset appearances. The paper also proposes an implicit curriculum based on the prior work. Experiments on Procgen Benchmark environments and MiniGrid environments show that the proposed framework improves sample-efficiency, generalization, and robustness at different test levels. "
198,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"pre - training CONJUNCTION multitask learning. multitask learning CONJUNCTION pre - training. deep learning USED-FOR data - rich settings. pre - training USED-FOR tasks. multitask learning USED-FOR tasks. automatic differentiation procedures CONJUNCTION randomized singular value decomposition. randomized singular value decomposition CONJUNCTION automatic differentiation procedures. randomized singular value decomposition USED-FOR scalability. scalability EVALUATE-FOR method. automatic differentiation procedures USED-FOR method. randomized singular value decomposition USED-FOR method. approach COMPARE baselines. baselines COMPARE approach. out - of - distribution data USED-FOR Text and Image classification tasks. Text and Image classification tasks EVALUATE-FOR approach. out - of - distribution data EVALUATE-FOR baselines. out - of - distribution data EVALUATE-FOR approach. OtherScientificTerm are model parameterizations, auxiliary tasks, auxiliary task gradients, auxiliary updates, and primary task loss. Method is modelagnostic framework. Generic are algorithm, and framework. ",This paper proposes a modelagnostic framework for deep learning in data-rich settings where pre-training and multitask learning are used to train models for different tasks. The proposed method uses automatic differentiation procedures and randomized singular value decomposition to improve the scalability of the learned model parameterizations. The authors show that the proposed algorithm achieves better performance than existing baselines on out-of-distribution data for Text and Image classification tasks. ,"This paper proposes a modelagnostic framework for deep learning in data-rich settings. The main idea is to combine pre-training and multitask learning for tasks with different model parameterizations. The proposed algorithm is based on the idea of auxiliary tasks, where auxiliary task gradients are computed from the primary task loss. The authors show that the proposed method can improve scalability by using automatic differentiation procedures and randomized singular value decomposition. The approach is evaluated on out-of-distribution data for Text and Image classification tasks and outperforms baselines."
207,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"large text - based neural language models USED-FOR one - shot learning. RL algorithms USED-FOR one - shot word learning. short - term, within - episode knowledge CONJUNCTION long - term lexical and motor knowledge. long - term lexical and motor knowledge CONJUNCTION short - term, within - episode knowledge. memory writing mechanism USED-FOR one - shot word - object binding. dual - coding memory USED-FOR intrinsic motivation. deep neural networks USED-FOR fast - mapping. episodic memory CONJUNCTION multi - modal environment. multi - modal environment CONJUNCTION episodic memory. meta - learning CONJUNCTION episodic memory. episodic memory CONJUNCTION meta - learning. multi - modal environment USED-FOR fast - mapping. transformative capacity FEATURE-OF artificial agents. human cognitive development CONJUNCTION transformative capacity. transformative capacity CONJUNCTION human cognitive development. fast - mapping HYPONYM-OF human cognitive development. meta - learning USED-FOR deep neural networks. multi - modal environment USED-FOR deep neural networks. episodic memory USED-FOR deep neural networks. Material is simulated 3D world. OtherScientificTerm are dual - coding external memory, visual perception and language, and ShapeNet category. ","This paper studies the problem of one-shot word learning in large text-based neural language models. The authors propose a novel memory writing mechanism for the task, called ShapeNet, which is based on the dual-coding external memory. The main idea is to learn a set of short-term, within-episode knowledge and long-term lexical and motor knowledge, which can then be used to guide the learning of a single word in a simulated 3D world. ShapeNet is then used to map the learned representations to the target language. The paper shows that ShapeNet achieves state-of-the-art performance on a variety of tasks. ","This paper proposes a novel approach to one-shot learning of large text-based neural language models. The authors propose a memory writing mechanism for one-shooting word-object binding, which is an extension of existing RL algorithms that use dual-coding external memory. The key idea is to use intrinsic motivation in the form of dual-code external memory to capture the relationship between visual perception and language. The paper also proposes a ShapeNet category, which combines short-term, within-episode knowledge with long-term lexical and motor knowledge. Experiments on a simulated 3D world show that the proposed fast-mapping with episodic memory, meta-learning, and multi-modal environment can improve the performance of deep neural networks in terms of human cognitive development and transformative capacity."
216,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"Few - shot learning USED-FOR models. support set USED-FOR setup. class - imbalance HYPONYM-OF dynamic nature of the real world. backbones USED-FOR few - shot learning methods. strategies USED-FOR imbalance. feature - transfer CONJUNCTION metric - based methods. metric - based methods CONJUNCTION feature - transfer. strategies USED-FOR few - shot case. balanced task COMPARE class - imbalance counterparts. class - imbalance counterparts COMPARE balanced task. imbalance FEATURE-OF supervised learning. strategies USED-FOR supervised learning. imbalance COMPARE support set level. support set level COMPARE imbalance. imbalance FEATURE-OF dataset level. dataset level COMPARE support set level. support set level COMPARE dataset level. class - imbalance counterparts COMPARE optimization - based methods. optimization - based methods COMPARE class - imbalance counterparts. OtherScientificTerm are few - shot class - imbalance, dataset vs. support set imbalance, and imbalance distributions. Method is rebalancing techniques. ","This paper studies the problem of few-shot learning for models with support set imbalance. The authors consider the dynamic nature of the real world, i.e., class-imbalance, and propose a new setup, called ""support set imbalance"", where the support set is a set of backbones that can be used to train the models. They show that the proposed strategies are able to reduce the imbalance in the ""few-shot"" case. They also show that these strategies can improve the performance of supervised learning with the imbalance compared to the class-balanced counterparts. ","This paper studies the problem of few-shot learning for models. The authors consider the dynamic nature of the real world, i.e., class-imbalance, and propose a new setup where the support set of the model is not balanced. They propose two strategies to deal with the imbalance: feature-transfer and metric-based methods. They show that the proposed strategies outperform the class-inbalance counterparts in the few-shooting case. They also show that in the balanced task, the imbalance of the dataset vs. support set imbalance can be reduced to zero, and that the imbalance distributions can be recovered by rebalancing techniques. "
225,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"convolution operators USED-FOR representations of graphs. Graph Convolutional Neural Networks ( GCNs ) USED-FOR representations of graphs. convolution operators USED-FOR Graph Convolutional Neural Networks ( GCNs ). neighborhood aggregating scheme USED-FOR convolution operators. local topological information USED-FOR convolution operators. decoupled representations USED-FOR them. graph convolution layer USED-FOR neighbouring nodes. topological distances FEATURE-OF neighbouring nodes. readout layers USED-FOR representations. convolution operators CONJUNCTION linear stacking. linear stacking CONJUNCTION convolution operators. Polynomial Graph Convolution ( PGC ) layer COMPARE convolution operators. convolution operators COMPARE Polynomial Graph Convolution ( PGC ) layer. Polynomial Graph Convolution ( PGC ) layer USED-FOR strategy. receptive field FEATURE-OF convolution operator. single PGC layer USED-FOR Graph Neural Network architecture. graph classification benchmarks EVALUATE-FOR Graph Neural Network architecture. OtherScientificTerm are wider topological receptive fields, and GC parameters. Method is non - linear graph convolutions. Metric is neural network expressiveness. ","This paper proposes a neighborhood aggregating scheme for graph convolutional neural networks (GCNs) to learn representations of graphs. Graph Convolutional Neural Networks are convolution operators that are trained with local topological information. The authors propose a neighborhood aggregation scheme that aggregates the topological representations of each node in the graph and uses them as decoupled representations of the neighbouring nodes. The topological distances between neighbouring nodes are computed using readout layers, and the convolution operator is trained with a receptive field that is defined as the distance between the two neighbouring nodes in the receptive field.  The authors show that the proposed strategy outperforms the standard polynomial graph Convolution (PGC) layer in terms of neural network expressiveness. They also show that their method is able to learn non-linear graph convolutions. ",This paper proposes a neighborhood aggregating scheme for learning convolution operators for representations of graphs. Graph Convolutional Neural Networks (GCNs) can be seen as a variant of graph convolutional neural Networks (GNNs) that can be used to learn representations from readout layers. The key idea is to use local topological information in the convolution operator to learn the representations of neighbouring nodes with topological distances between them. The authors propose a novel strategy that uses a Polynomial graph Convolution (PGC) layer instead of a single PGC layer in the Graph Neural Network architecture for graph classification benchmarks. They show that the proposed method is able to learn non-linear graph convolutions with wider topological receptive fields. They also show that their method can learn GC parameters that are more expressive of the receptive field. 
234,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,labeled datasets USED-FOR SG generation techniques. neural network models COMPARE real data. real data COMPARE neural network models. synthetic data USED-FOR neural network models. scalable technique USED-FOR sim - to - real transfer. sim - to - real transfer USED-FOR scene graph generation. Sim2SG HYPONYM-OF scalable technique. scalable technique USED-FOR scene graph generation. Sim2SG USED-FOR domain gap. supervision PART-OF real - world dataset. supervision USED-FOR Sim2SG. baselines USED-FOR domain gap. toy simulators CONJUNCTION realistic simulators. realistic simulators CONJUNCTION toy simulators. real - world data USED-FOR realistic simulators. realistic simulators EVALUATE-FOR approach. toy simulators EVALUATE-FOR approach. Task is Scene graph ( SG ) generation. Material is Synthetic data. OtherScientificTerm is appearance. Generic is discrepancies. ,"This paper proposes a scalable technique for sim-to-real transfer for scene graph generation, called Sim2SG. The proposed approach is based on a combination of toy simulators and real-world data. The authors show that the proposed approach can achieve state-of-the-art performance on both toy and real datasets. The paper also shows that the learned representations can be used to improve the performance of existing SG generation techniques.","This paper proposes a new method for scene graph (SG) generation, called Sim2SG, which is based on a scalable technique for sim-to-real transfer. The key idea is to use synthetic data to train neural network models on top of real data. Synthetic data is used to train SG generation techniques on labeled datasets. The authors show that the proposed approach outperforms toy simulators and realistic simulators on real-world data with supervision. The paper also shows that the domain gap between simulated and real data can be reduced by using SimSG on the supervision part of the scene graph generation, and on the other part on the real -world dataset. "
243,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,model - based methods COMPARE model - free methods. model - free methods COMPARE model - based methods. sample efficiency EVALUATE-FOR model - free methods. model - free methods USED-FOR continuous - action DRL benchmarks. continuous - action DRL benchmarks EVALUATE-FOR model - based methods. sample efficiency EVALUATE-FOR model - based methods. modelbased algorithm USED-FOR MuJoCo benchmark. REDQ COMPARE model - based method. model - based method COMPARE REDQ. parameters USED-FOR model - based method. wall - clock run time EVALUATE-FOR REDQ. parameters USED-FOR REDQ. random subset of Q functions PART-OF ensemble. REDQ USED-FOR it. ensemble of Q functions CONJUNCTION in - target minimization. in - target minimization CONJUNCTION ensemble of Q functions. random subset of Q functions USED-FOR in - target minimization. REDQ CONJUNCTION model - free algorithms. model - free algorithms CONJUNCTION REDQ. model - free DRL algorithm USED-FOR continuous - action spaces. REDQ HYPONYM-OF model - free DRL algorithm. REDQ USED-FOR continuous - action spaces. UTD ratio 1 USED-FOR model - free DRL algorithm. UTD ratio 1 USED-FOR REDQ. Method is modelfree algorithm. OtherScientificTerm is UTD ratio. ,"This paper proposes REDQ, a model-free DRL algorithm based on the UTD ratio 1. The authors show that REDQ achieves better sample efficiency than model-based methods on a variety of continuous-action DRL benchmarks. They also show that it is able to achieve wall-clock run time with fewer parameters compared to a model based method. The paper also provides a modelfree algorithm for the MuJoCo benchmark.","This paper presents a model-free DRL algorithm, REDQ, which is an extension of REDQ to continuous-action DRL benchmarks. The authors show that REDQ outperforms model-based methods on the MuJoCo benchmark in terms of sample efficiency, and it outperforms the model free methods on a range of continuous-actions. The main contribution of the paper is the introduction of the UTD ratio 1, which allows the authors to compare REDQ with other models on wall-clock run time. The model free algorithm is based on a modelfree algorithm, where the parameters of the ensemble of Q functions and the in-target minimization are learned from a random subset of the Q functions in the ensemble. "
252,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"deep learning USED-FOR classification or regression. probability distributions USED-FOR deep learning. distribution samples USED-FOR classification or regression. Lipschitz - bounded transformations of the input distribution FEATURE-OF robustness. tasks EVALUATE-FOR approach. DIDA USED-FOR meta - features. DIDA USED-FOR labelled ) dataset. meta - features USED-FOR labelled ) dataset. DIDA USED-FOR tasks. SVM CONJUNCTION logistic regression. logistic regression CONJUNCTION SVM. logistic regression CONJUNCTION linear SGD. linear SGD CONJUNCTION logistic regression. k - NN CONJUNCTION SVM. SVM CONJUNCTION k - NN. hyper - parameter configuration COMPARE configuration. configuration COMPARE hyper - parameter configuration. fixed algorithm USED-FOR hyper - parameter configuration. hyper - parameter configuration USED-FOR learning. dataset EVALUATE-FOR configuration. OpenML benchmarking suite USED-FOR dataset. SVM HYPONYM-OF fixed algorithm. DSS CONJUNCTION DATASET2VEC architectures. DATASET2VEC architectures CONJUNCTION DSS. tasks EVALUATE-FOR DIDA. tasks EVALUATE-FOR models. DIDA COMPARE models. models COMPARE DIDA. DIDA COMPARE DATASET2VEC architectures. DATASET2VEC architectures COMPARE DIDA. tasks EVALUATE-FOR DATASET2VEC architectures. DIDA COMPARE DSS. DSS COMPARE DIDA. DATASET2VEC architectures CONJUNCTION models. models CONJUNCTION DATASET2VEC architectures. hand - crafted meta - features USED-FOR models. OtherScientificTerm are permutation of the samples, and permutation of the features. Method are neural architectures, and universal approximation. Generic are architecture, and task. ","This paper studies the problem of classification or regression with probability distributions in deep learning with distribution samples. The authors propose a new approach, DIDA, to learn the meta-features of the (labeled) dataset from the (meta-features from the labelled) dataset. The proposed architecture is based on the Lipschitz-bounded transformations of the input distribution, which is a well-known property of neural architectures. The paper shows that the proposed approach is able to perform well on a variety of tasks, including SVM, logistic regression, k-NN, and linear SGD. The main contribution of the paper is a theoretical analysis of the performance of the proposed architecture, which shows that it performs better than DSS, DATASET2VEC architectures, and models trained on the same dataset. ","The paper proposes a new approach to learning the robustness of deep learning to distribution samples from distribution samples. The approach is based on the Lipschitz-bounded transformations of the input distribution, which is an important property of robustness to perturbations in the probability distributions in deep learning. The authors propose a new architecture, named DIDA, which allows for permutation of the samples, which can be used for classification or regression. The proposed approach is evaluated on a (labeled) dataset of (meta-features) from the (labelled) dataset, where the meta-features are generated by hand-crafting the features. The paper also presents an OpenML benchmarking suite to evaluate the proposed dataset. The results show that the proposed architecture outperforms DSS, DATASET2VEC architectures, and other models on different tasks. The hyper-parameter configuration of the learning is compared to the fixed algorithm of SVM, k-NN, and logistic regression, and the proposed fixed algorithm outperforms a fixed algorithm in terms of universal approximation. "
261,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,data clustering CONJUNCTION visualization. visualization CONJUNCTION data clustering. dimensionality reduction CONJUNCTION data clustering. data clustering CONJUNCTION dimensionality reduction. data representation and analysis CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION data representation and analysis. manifold learning CONJUNCTION data representation and analysis. data representation and analysis CONJUNCTION manifold learning. Graph learning USED-FOR data mining and machine learning tasks. visualization HYPONYM-OF data mining and machine learning tasks. manifold learning HYPONYM-OF data mining and machine learning tasks. data clustering HYPONYM-OF data mining and machine learning tasks. data representation and analysis HYPONYM-OF data mining and machine learning tasks. dimensionality reduction HYPONYM-OF data mining and machine learning tasks. approach USED-FOR ultra - sparse undirected graphs. graphLaplacian - like matrix PART-OF graphical Lasso. graphLaplacian - like matrix FEATURE-OF precision matrix. high - dimensional input data USED-FOR ultra - sparse undirected graphs. GRASPEL USED-FOR graphs. spectrally - critical edges PART-OF graph. nearly - linear time spectral methods USED-FOR ultrasparse yet spectrally - robust graphs. spectral clustering ( SC ) CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION spectral clustering ( SC ). graph learning approaches COMPARE GRASPEL. GRASPEL COMPARE graph learning approaches. manifold learning CONJUNCTION spectral clustering ( SC ). spectral clustering ( SC ) CONJUNCTION manifold learning. computing efficiency CONJUNCTION solution quality. solution quality CONJUNCTION computing efficiency. GRASPEL USED-FOR data mining and machine learning applications. solution quality EVALUATE-FOR data mining and machine learning applications. computing efficiency EVALUATE-FOR data mining and machine learning applications. solution quality EVALUATE-FOR GRASPEL. computing efficiency EVALUATE-FOR GRASPEL. dimensionality reduction HYPONYM-OF data mining and machine learning applications. manifold learning HYPONYM-OF data mining and machine learning applications. spectral clustering ( SC ) HYPONYM-OF data mining and machine learning applications. Method is spectral graph densification approach,"This paper proposes a spectral graph densification approach for ultra-sparse undirected graphs with high-dimensional input data. The proposed approach is based on the graph Laplacian-like matrix (GRASPEL), which is a graphical Lasso. Graph learning is an important problem in data mining and machine learning tasks such as data clustering, dimensionality reduction, data representation and analysis, and manifold learning. The authors propose a new approach to reduce the number of spectrally-critical edges in a graph. The precision matrix of the graph is computed by computing the graphLaplacians of the vertices of a graph, which is then used to compute the precision matrix for each edge in the graph.  The authors show that the proposed method is able to achieve better computing efficiency and solution quality than existing graph learning approaches. ","This paper proposes a spectral graph densification approach for ultra-sparse undirected graphs with high-dimensional input data. Graph learning is an important problem in data mining and machine learning tasks such as data clustering, dimensionality reduction, data representation and analysis, manifold learning, and manifold learning with a graphLaplacian-like matrix, which is a graphical Lasso. The authors propose an approach to reduce the number of vertices in a graph to a graph with spectrally-critical edges. The proposed approach is based on nearly-linear time spectral methods, which can be applied to ultrasparse yet spectratically-robust graphs. The precision matrix of the precision matrix is a GAN, which consists of the graph Laplacians of the vertices of the graphs.  The authors show that GRASPEL outperforms other graph learning approaches in terms of computing efficiency, solution quality, and computing efficiency. "
270,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"images CONJUNCTION text descriptions. text descriptions CONJUNCTION images. deep reinforcement learning USED-FOR goal - conditioned policy. explicit embedding space USED-FOR nonparametric distance. abstract - level policy CONJUNCTION goal - conditioned policy. goal - conditioned policy CONJUNCTION abstract - level policy. unsupervised learning approach USED-FOR goal - conditioned policy. unsupervised learning approach USED-FOR abstract - level policy. intrinsic motivation ( GPIM ) FEATURE-OF goal - conditioned policy. goal - conditioned policy HYPONYM-OF unsupervised learning approach. discriminator USED-FOR abstract - level policy. latent variable USED-FOR abstract - level policy. discriminator USED-FOR intrinsic reward function. intrinsic reward function USED-FOR goal - conditioned policy. intrinsic reward function USED-FOR trajectory. discriminator USED-FOR goal - conditioned policy. discriminator USED-FOR trajectory. abstract - level policy USED-FOR trajectory. robotic tasks EVALUATE-FOR GPIM method. GPIM method COMPARE prior techniques. prior techniques COMPARE GPIM method. robotic tasks EVALUATE-FOR prior techniques. OtherScientificTerm are perceptually - specific goals, and hand - crafted rewards. Generic is policy. ","This paper proposes an unsupervised learning approach called goal-conditioned policy (GPIM) for deep reinforcement learning. The authors propose to use explicit embedding space to represent the nonparametric distance between the abstract-level policy and the goal-conditional policy, and to use a discriminator to learn the intrinsic reward function for the goal - conditioned policy. The discriminator is trained to predict the trajectory of the abstract - level policy using a latent variable. The paper shows that the proposed GPIM method outperforms prior techniques on several robotic tasks.","This paper proposes an unsupervised learning approach to learn a goal-conditioned policy using deep reinforcement learning. The key idea is to use intrinsic motivation (GPIM) to learn an abstract-level policy with a discriminator that predicts the intrinsic reward function of the goal-conditional policy. The discriminator is trained in an explicit embedding space, where the nonparametric distance between the goal and the latent variable of the policy is defined as the distance between a set of perceptually-specific goals and the set of hand-crafted rewards. The proposed GPIM method is shown to outperform prior techniques on a variety of robotic tasks."
279,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"policy switches HYPONYM-OF low switching cost. low switching cost USED-FOR deep reinforcement learning problems. robotics CONJUNCTION dialogue agents. dialogue agents CONJUNCTION robotics. recommendation systems CONJUNCTION education. education CONJUNCTION recommendation systems. education CONJUNCTION robotics. robotics CONJUNCTION education. medical domains CONJUNCTION recommendation systems. recommendation systems CONJUNCTION medical domains. recommendation systems CONJUNCTION robotics. robotics CONJUNCTION recommendation systems. dialogue agents HYPONYM-OF applications. medical domains HYPONYM-OF applications. robotics HYPONYM-OF applications. education HYPONYM-OF applications. recommendation systems HYPONYM-OF applications. Q - network CONJUNCTION learning Q - network. learning Q - network CONJUNCTION Q - network. deep Q - networks USED-FOR policy switching criteria. feature distance USED-FOR adaptive approach. medical treatment environment CONJUNCTION Atari games. Atari games CONJUNCTION medical treatment environment. switching cost EVALUATE-FOR feature - switching criterion. sample efficiency EVALUATE-FOR feature - switching criterion. OtherScientificTerm are low - switching - cost constraint, and representation learning perspective. ","This paper studies the problem of low-switching-cost constraint in deep reinforcement learning problems. The authors propose a new low switching cost constraint, called policy switches, which is based on the representation learning perspective. The key idea is to use deep Q-net and a learning Q-network to learn policy switching criteria. The adaptive approach uses feature distance to reduce the switching cost. Experimental results show that the feature-switch criterion improves the sample efficiency. ","This paper proposes a low-switching-cost constraint for deep reinforcement learning problems. The key idea is to use deep Q-net and learning Q-network as the policy switching criteria. The adaptive approach is based on feature distance. The authors show that the low switching cost improves sample efficiency. The experiments are conducted on a variety of applications including robotics, dialogue agents, recommendation systems, and medical domains. "
288,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,First - order stochastic methods USED-FOR large - scale non - convex optimization problems. First - order stochastic methods USED-FOR big - data applications. deep neural networks HYPONYM-OF big - data applications. homotopy methods CONJUNCTION SGD. SGD CONJUNCTION homotopy methods. diffusion CONJUNCTION mollifying networks. mollifying networks CONJUNCTION diffusion. Gaussian continuation USED-FOR optimization. diffusion HYPONYM-OF heuristics. mollifying networks HYPONYM-OF heuristics. optimization HYPONYM-OF heuristics. homotopy methods USED-FOR first - order stochastic algorithm. SGD USED-FOR first - order stochastic algorithm. scheme USED-FOR H - SGD. scheme USED-FOR homotopy parameter. fast and inexpensive iterations FEATURE-OF H - SGD. global linear rate of convergence EVALUATE-FOR H - SGD. H - SGD COMPARE SGD. SGD COMPARE H - SGD. Metric is slow global convergence rate. OtherScientificTerm is neighborhood of a minimizer. Generic is algorithm. ,This paper studies the problem of large-scale non-convex optimization problems with first-order stochastic methods in deep neural networks. The authors consider two heuristics: diffusion and mollifying networks. They show that the slow global convergence rate of H-SGD is due to the neighborhood of a minimizer. They then propose a scheme to reduce the homotopy parameter of SGD and propose a first - order stochastically algorithm based on SGD. The algorithm is shown to have a global linear rate of convergence. ,"This paper proposes a new algorithm for solving large-scale non-convex optimization problems. First-order stochastic methods for solving big-data applications (e.g., deep neural networks) such as optimization, homotopy methods, SGD, and mollifying networks are used. The main idea is to use Gaussian continuation for optimization, where the neighborhood of a minimizer is a function of the number of iterations of the algorithm. The authors show that H-SGD is faster than SGD in terms of global linear rate of convergence. They also show that the proposed scheme can be used to improve the H- SGD with fast and inexpensive iterations. "
297,SP:195d090d9df0bda33103edcbbaf300e43f4562be,Bayesian meta - learning problem USED-FOR shape completion. encoder USED-FOR posterior distribution. encoder USED-FOR latent representation. posterior distribution FEATURE-OF latent representation. sparse cloud USED-FOR latent representation. encoder USED-FOR learning of object shapes. sparse point clouds USED-FOR learning of object shapes. meta - learning algorithm USED-FOR shape completion of newly - encountered objects. object - specific properties CONJUNCTION object - agnostic properties. object - agnostic properties CONJUNCTION object - specific properties. sparse observations USED-FOR shape completion of newly - encountered objects. ICL - NUIM benchmarks EVALUATE-FOR method. Method is shape representations. ,"This paper studies the Bayesian meta-learning problem of shape completion in the presence of sparse point clouds. The authors propose a new encoder for learning of object shapes from sparse cloud, which is based on the posterior distribution of the latent representation of the encoder. The shape representations are learned from sparse observations, and the object-specific properties and object-agnostic properties of the object are learned by the shape completion of newly-accomplished objects. The proposed method is evaluated on ICL-NUIM benchmarks.","The paper proposes a Bayesian meta-learning problem for shape completion, where the latent representation of an object is learned from sparse point clouds, and the encoder is trained to predict the posterior distribution of that latent representation. The paper proposes to use a sparse cloud for the learning of object shapes, and then use a meta learning algorithm for the shape completion of newly-accomplished objects from sparse observations. The proposed method is evaluated on ICL-NUIM benchmarks, showing that the proposed method can learn shape representations with object-specific properties and object-agnostic properties."
306,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. activation magnitudes FEATURE-OF adversarial examples. channels COMPARE natural examples. natural examples COMPARE channels. channel - wise activation perspective FEATURE-OF adversarial examples. defense adversarial training USED-FOR activation magnitudes. CAS USED-FOR model. model USED-FOR adversarial activation. robustness EVALUATE-FOR defense methods. OtherScientificTerm are uniform activation, redundant activation, and adversarial perturbations. Method is intermediate layer activation of DNNs. ",This paper studies the problem of intermediate layer activation of DNNs. The authors show that the activation magnitudes of adversarial examples with channel-wise activation perspective are much larger than those of natural examples with channels. They then propose a defense adversarial training method to reduce the size of the activation magnitude of these examples. The proposed model is based on CAS and is able to achieve better robustness against adversarial activation than existing defense methods. ,"This paper studies the problem of defense against adversarial examples. The authors show that the activation magnitudes of adversarial example are larger than those of natural examples. They show that this is due to the intermediate layer activation of DNNs. They also show that uniform activation is more robust to adversarial perturbations. They propose a defense adversarial training method to reduce the adversarial magnitudes in the channel-wise activation perspective. The model is based on CAS, and the model is evaluated on adversarial activation. The paper also shows that the robustness of the defense methods is lower than that of other defense methods. "
315,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"random initialization USED-FOR gradient descent. generalization EVALUATE-FOR Neural networks. gradient descent USED-FOR Neural networks. random initialization USED-FOR Neural networks. Neural Tangent Kernel ( NTK ) USED-FOR implicit regularization effect. gradient flow / descent USED-FOR infinitely wide neural networks. implicit regularization effect FEATURE-OF gradient flow / descent. random initialization USED-FOR gradient flow / descent. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance CONJUNCTION initialization. initialization CONJUNCTION generalization performance. optimization USED-FOR finite width networks. initialization USED-FOR finite width networks. optimization CONJUNCTION overparametrization. overparametrization CONJUNCTION optimization. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance EVALUATE-FOR overparametrization. hidden layer width CONJUNCTION scaled ) random initialization. scaled ) random initialization CONJUNCTION hidden layer width. low - dimensional manifold FEATURE-OF network parameters. min - norm solution USED-FOR linear case. O(h−1/2 ) upper - bound FEATURE-OF operator norm distance. network CONJUNCTION min - norm solution. min - norm solution CONJUNCTION network. operator norm distance FEATURE-OF network. operator norm distance EVALUATE-FOR min - norm solution. OtherScientificTerm are regularization, and imbalance of the network weights. Method are non - asymptotic analysis, overparametrized single - hidden layer linear networks, and gradient flow. Metric are squared loss, and convergence rate. Generic is manifold. ","This paper studies the implicit regularization effect of gradient flow/descent for infinitely wide neural networks with random initialization in gradient descent. The authors propose Neural Tangent Kernel (NTK), which is a non-asymptotic analysis of the gradient flow of overparametrized single-hidden layer linear networks. They show that the squared loss of the NTK is a function of the number of parameters of the network parameters on the low-dimensional manifold. The convergence rate of the manifold is shown to be O(h−1/2) upper-bound. They also provide a min-norm solution for the linear case where the network is infinitely wide and the operator norm distance between the network and the hidden layer width is O(1/\sqrt{T}). The authors show that initialization and optimization for finite width networks with (hidden layer width and scaled) random initialization can improve the generalization performance of Neural networks. ","This paper studies the implicit regularization effect of gradient flow/descent for infinitely wide neural networks with random initialization in the context of gradient descent for Neural networks. The authors propose Neural Tangent Kernel (NTK) which is a non-asymptotic analysis of the gradient flow in the case of overparametrized single-hidden layer linear networks, where the regularization is based on the imbalance of the network weights. The main contribution of the paper is to study the generalization performance of Neural networks in terms of initialization, optimization, and optimization for finite width networks with (hidden layer width, scaled) random initialization. The network parameters are defined on a low-dimensional manifold with O(h−1/2) upper-bound on the operator norm distance between the network and the min-norm solution of the linear case. The convergence rate is defined as the squared loss, and the number of layers is defined in the manifold. "
324,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,Deep networks COMPARE shallow ones. shallow ones COMPARE Deep networks. approximation EVALUATE-FOR shallow ones. tractable algorithms USED-FOR deep models. deep networks COMPARE shallow ones. shallow ones COMPARE deep networks. approximation USED-FOR kernels. tractable ) kernel methods USED-FOR over - parameterized regime. gradient descent USED-FOR deep networks. architecture USED-FOR kernel. eigenvalue decay FEATURE-OF integral operator. kernels COMPARE shallow ” two - layer counterpart. shallow ” two - layer counterpart COMPARE kernels. kernels USED-FOR ReLU activations. deep fully - connected networks USED-FOR kernels. approximation properties EVALUATE-FOR kernels. kernel framework USED-FOR deep architectures. differentiability properties FEATURE-OF kernel function. sphere FEATURE-OF kernels. kernel function USED-FOR eigenvalue decays. differentiability properties USED-FOR eigenvalue decays. ,This paper proposes a new kernel framework for deep neural networks. The main idea is to learn a kernel function that is invariant to the eigenvalue decay of the integral operator of the kernel. The kernel function is then used as a surrogate for the kernel function of the deep network. The authors show that the proposed kernel can be used to approximate the kernel of a two-layer neural network. They also show that their kernel can approximate the kernels of ReLU activations. ,"This paper proposes a new kernel framework for deep architectures. The authors propose to use (tractable) kernel methods for the over-parameterized regime. The main idea is to use tractable algorithms for deep models. Deep networks are compared to shallow ones, and the authors show that the approximation of the kernels is better than the shallow ones in terms of the approximation properties of the kernel function in the sphere of the integral operator.  The authors also propose a new architecture for the kernel, which is based on gradient descent. They show that kernels are more robust to ReLU activations than their “shallow” two-layer counterpart. They also show that deep fully-connected networks are better at learning kernels.  "
333,SP:3dd495394b880cf2fa055ee3fe218477625d2605,amplified value estimates CONJUNCTION suboptimal policies. suboptimal policies CONJUNCTION amplified value estimates. overestimation problem PART-OF deep value learning. underestimation bias CONJUNCTION instability. instability CONJUNCTION underestimation bias. algorithm USED-FOR overestimation. overestimation issues FEATURE-OF continuous control. algorithm USED-FOR policy improvement. deep reinforcement learning USED-FOR continuous control. deep reinforcement learning USED-FOR overestimation issues. combined value of weighted critics USED-FOR policy. weight factor USED-FOR independent critics. method USED-FOR policy improvement. algorithms USED-FOR continuous control. algorithms COMPARE algorithms. algorithms COMPARE algorithms. classical control tasks EVALUATE-FOR method. OtherScientificTerm is function approximation errors. ,"This paper studies the overestimation problem in deep value learning, which is an important problem in continuous control with amplified value estimates and suboptimal policies. The authors propose a new algorithm for overestimation, which uses the combined value of weighted critics to estimate the policy and the weight factor for independent critics. They show that the proposed algorithm is able to achieve state-of-the-art performance on a variety of continuous control tasks. The proposed method is also able to perform better than existing algorithms for continuous control. ",This paper studies the overestimation problem in deep value learning with amplified value estimates and suboptimal policies. The authors propose an algorithm for minimizing overestimation and instability in continuous control with deep reinforcement learning. The proposed algorithm is based on the combined value of weighted critics for each policy and the weight factor for independent critics. The method is evaluated on classical control tasks and outperforms other algorithms for policy improvement. The paper also provides a theoretical analysis of the function approximation errors.
342,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"inverse reinforcement learning ( IRL ) problem USED-FOR reward functions. expert demonstrations USED-FOR reward functions. policy USED-FOR reward functions. expert demonstrations USED-FOR policies. ill - posed inverse problem HYPONYM-OF IRL problem. IRL problem USED-FOR well - posed expectation optimization problem. solution USED-FOR SIRL problem. solution USED-FOR learning task. solutions USED-FOR IRL problem. solution USED-FOR solutions. formulation USED-FOR IRL problem. objectworld EVALUATE-FOR approach. OtherScientificTerm are probability distribution over reward functions, and probability distribution. ","This paper studies the inverse reinforcement learning (IRL) problem of learning reward functions from expert demonstrations. The IRL problem is a well-posed expectation optimization problem, which is a variant of ill-posed inverse problem, where the goal is to find a policy that maximizes the probability distribution over reward functions. The authors propose a solution to the SIRL problem, and show that the proposed solution can be used to solve the learning task. The proposed solution is shown to converge to solutions that are close to the optimal IRL solution. The approach is evaluated on the objectworld.","This paper studies the inverse reinforcement learning (IRL) problem for learning reward functions based on expert demonstrations. The IRL problem is an extension of the ill-posed inverse problem, which is a well-posed expectation optimization problem. The key idea is to use the probability distribution over reward functions as a policy to learn the reward functions. The authors propose a solution to the SIRL problem, and show that the proposed solution can be applied to the learning task. The proposed formulation is evaluated on the IRL problems in the objectworld."
351,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"Self - training algorithms USED-FOR model. model USED-FOR pseudolabels. model USED-FOR pseudolabels. neural networks USED-FOR unlabeled data. self - training USED-FOR linear models. unsupervised domain adaptation CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION unsupervised domain adaptation. self - training USED-FOR semi - supervised learning. deep networks USED-FOR semi - supervised learning. deep networks USED-FOR unsupervised domain adaptation. semi - supervised learning CONJUNCTION unsupervised domain adaptation. unsupervised domain adaptation CONJUNCTION semi - supervised learning. self - training USED-FOR unsupervised domain adaptation. deep networks USED-FOR self - training. self - training CONJUNCTION input - consistency regularization. input - consistency regularization CONJUNCTION self - training. accuracy EVALUATE-FOR minimizers of population objectives. self - training USED-FOR minimizers of population objectives. input - consistency regularization USED-FOR minimizers of population objectives. margin CONJUNCTION Lipschitzness. Lipschitzness CONJUNCTION margin. sample complexity guarantees FEATURE-OF neural nets. margin FEATURE-OF neural nets. Lipschitzness FEATURE-OF neural nets. input consistency regularization USED-FOR self - training algorithms. OtherScientificTerm are neighborhoods, ground - truth labels, and generalization bounds. Generic is assumptions. ","This paper studies self-training algorithms for learning a model that can generate pseudolabels from unlabeled data. The authors consider the problem of unsupervised domain adaptation, semi-supervised learning with deep networks, and unclassified domain adaptation with self-trained linear models. They show that under certain assumptions on the number of neighborhoods and ground-truth labels, the accuracy of the minimizers of population objectives can be improved by self-train and input-consistency regularization. They also show that the margin of neural nets with Lipschitzness and the sample complexity guarantees of the trained neural nets are better than those of deep networks. ","This paper proposes self-training algorithms for learning a model to predict pseudolabels from unlabeled data. The model is trained on the ground-truth labels. The authors show that under certain assumptions, the model is able to predict the pseudolabeled labels. They also show that the model can predict the neighborhoods of the pseudoloabels. The paper also provides some generalization bounds on the generalization error of the model.  The authors also provide some experiments on unsupervised domain adaptation, semi-supervised learning with deep networks, self-train for linear models, and un-supervision of the input-consistency regularization. They show that these minimizers of population objectives improve the accuracy and the margin of neural nets with sample complexity guarantees."
360,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"hierarchical models USED-FOR video prediction. stochastic recurrent estimator USED-FOR long - term prediction. car driving CONJUNCTION human dancing. human dancing CONJUNCTION car driving. it USED-FOR complicated scene structures. video prediction COMPARE approaches. approaches COMPARE video prediction. Generic is method. OtherScientificTerm are semantic structures, structures, and discrete semantic structure space. Method is videoto - video translation. ","This paper proposes a new method for video prediction based on hierarchical models. The proposed method is based on a stochastic recurrent estimator for long-term prediction. The main idea of the method is to learn the semantic structures of the video, and then translate these structures into a discrete semantic structure space. Theoretically, it is shown that it can learn complicated scene structures such as car driving and human dancing. Experiments show that the proposed video prediction outperforms existing approaches in terms of video prediction. ","This paper proposes a new method for video prediction based on hierarchical models. The proposed method is based on a stochastic recurrent estimator for long-term prediction. The main idea is to learn a set of semantic structures, which can be represented as discrete semantic structure space, and then translate these structures into a discrete semantic space space. The authors show that it can learn complicated scene structures such as car driving, human dancing, etc. They also show that the proposed method can be used for videoto-video translation. The experimental results show that video prediction is better than other approaches in terms of video prediction."
369,SP:e50b1931800daa7de577efd3edca523771227b3f,"undirected graph CONJUNCTION directed graph. directed graph CONJUNCTION undirected graph. Iterated Graph Neural Network System ( IGNNS ) HYPONYM-OF Graph Neural Networks ( GNNs ). Iterated Function System ( IFS ) HYPONYM-OF fractal geometry. Iterated Function System ( IFS ) PART-OF IGNNS. adjoint probability vector USED-FOR IFS layer. affine transformations USED-FOR IGNNS. geometric properties FEATURE-OF IGNNS. dynamical system USED-FOR IGNNS. dynamical system USED-FOR geometric properties. Frobenius norm FEATURE-OF constant matrix. IGNNS USED-FOR IFS. Hausdorff distance FEATURE-OF fractal set of IFS. Cora CONJUNCTION PubMed. PubMed CONJUNCTION Cora. citeser CONJUNCTION Cora. Cora CONJUNCTION citeser. citation network datasets USED-FOR semi - supervised node classification. PubMed HYPONYM-OF citation network datasets. citeser HYPONYM-OF citation network datasets. Cora HYPONYM-OF citation network datasets. OtherScientificTerm are graph nodes, latent space, and node features. Method are high - level representation of graph nodes, and fractal representation of graph nodes. Generic is method. ","This paper proposes a new method for graph neural networks (GNNs) based on the Iterated Graph Neural Network System (IGNNS). The main idea is to learn a high-level representation of graph nodes in the latent space, which is then used to train the IFS layer using an adjoint probability vector. The IFS is then applied to the graph nodes, which are then fed to the GNN. The proposed method is evaluated on several citation network datasets for semi-supervised node classification, including Cora, PubMed, and Cora.","This paper proposes a novel method for learning a high-level representation of graph nodes. The authors propose to use the Iterated Graph Neural Network System (IGNNS) of Graph Neural Networks (GNNs) to represent the fractal geometry of an undirected graph and a directed graph. The IFS layer is based on the adjoint probability vector of the graph nodes in the latent space. The main idea of the method is to use affine transformations to improve the geometric properties of IGNNS using a dynamical system. The proposed method is evaluated on citation network datasets for semi-supervised node classification, including Cora, PubMed, and Cora. The results show that the IFS of the IGNNs can be approximated by the Frobenius norm of the constant matrix of the node features, and that the Hausdorff distance of IFS is a function of the number of node features."
378,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"modeling complex relations CONJUNCTION modeling isomorphic graphs. modeling isomorphic graphs CONJUNCTION modeling complex relations. GG - GAN USED-FOR graphs. GG - GAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GG - GAN. GG - GAN USED-FOR distribution statistics. Task is graph generation. OtherScientificTerm are similarity function, complex relations, isomorphic graphs, latent distribution, and problem - specific knowledge. Method are geometric interpretation, and Wasserstein GAN. ","This paper studies the problem of graph generation. The authors propose a new graph generation method based on GG-GAN. The main idea is to learn a similarity function between two graphs, where the similarity function is a function of the number of nodes in the graph. The similarity function can then be used to model complex relations between the nodes in a graph, as well as modeling isomorphic graphs. The proposed method is based on Wasserstein GAN, which is an extension of the geometric interpretation of GG-GAN.  The authors show that the proposed method achieves better performance than state-of-the-art methods on distribution statistics.  ","This paper studies the problem of graph generation from complex relations and modeling isomorphic graphs. The main idea is to use GG-GAN to generate graphs with a similarity function. The similarity function is defined as the sum of the complex relations of the isomorphic graph and the latent distribution of the graph. The authors propose a geometric interpretation of the similarity function, and propose a Wasserstein GAN, which is a variant of GG-GAN for graphs with complex relations. They show that the proposed GG-GNAN outperforms state-of-the-art methods on distribution statistics. They also show that their method can be applied to problem-specific knowledge."
387,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"activation values FEATURE-OF network. Minimum Description Length principle USED-FOR problem. rules CONJUNCTION super - charge prototyping. super - charge prototyping CONJUNCTION rules. Generic are method, and they. Method are neural network, and unsupervised EXPLAINN algorithm. OtherScientificTerm are noise - robust rules, class - specific traits, and convolutional layers. Material is activation data. ","This paper studies the problem of unsupervised EXPLAINN with noise-robust rules. The authors propose a new method, which is based on the Minimum Description Length principle, to solve the problem. The main idea is to learn the activation values of a neural network with different activation values for different class-specific traits, and then use these activation values to train the network. The proposed method is evaluated on a variety of datasets, and they show that they are able to achieve state-of-the-art performance in terms of both the number of rules and super-charge prototyping. ","This paper proposes a novel method for learning noise-robust rules for class-specific traits in a neural network. The main idea is to use the Minimum Description Length principle to solve the problem. The proposed method is based on an unsupervised EXPLAINN algorithm, where the activation values of the network depend on the activation data. The authors show that they can learn rules and super-charge prototyping. "
396,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"worst - case guarantees FEATURE-OF fixed - dataset policy optimization algorithms. algorithms USED-FOR regime. principle USED-FOR algorithms. tabular gridworld CONJUNCTION deep learning. deep learning CONJUNCTION tabular gridworld. MinAtar environments EVALUATE-FOR deep learning. Method are naı̈ve approaches, pessimism principle, and pessimistic algorithms. OtherScientificTerm are erroneous value overestimation, and policy. ","This paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms in the regime where the erroneous value overestimation is large. The authors propose a pessimism principle for these algorithms, and show that the proposed algorithms can achieve better performance than naı̈ve approaches. They also show that pessimistic algorithms can be used to improve the performance of deep learning in tabular gridworld and deep learning on MinAtar environments.",This paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms with naı̈ve approaches. The authors propose a pessimism principle to motivate the algorithms. The main idea is to minimize the erroneous value overestimation of the policy. The proposed algorithms are evaluated on a tabular gridworld and deep learning in MinAtar environments.
405,SP:363661edd15a06a800b51abc1541a3191311ee0e,"Neural ordinary differential equations ( Neural ODEs ) HYPONYM-OF deeplearning models. continuous depth FEATURE-OF deeplearning models. naive method CONJUNCTION adaptive checkpoint adjoint method ( ACA ). adaptive checkpoint adjoint method ( ACA ) CONJUNCTION naive method. continuous case FEATURE-OF numerical estimation of the gradient. accuracy EVALUATE-FOR gradient estimation. accuracy EVALUATE-FOR reverse - time trajectory. constant memory cost FEATURE-OF ALF Integrator ( MALI ). heavy memory burden CONJUNCTION inaccuracy. inaccuracy CONJUNCTION heavy memory burden. MALI COMPARE ResNet. ResNet COMPARE MALI. MALI COMPARE adjoint method. adjoint method COMPARE MALI. MALI USED-FOR Neural ODE. MALI COMPARE methods. methods COMPARE MALI. image recognition tasks EVALUATE-FOR MALI. tasks EVALUATE-FOR MALI. ImageNet USED-FOR Neural ODE. image recognition tasks HYPONYM-OF tasks. tasks EVALUATE-FOR MALI. MALI USED-FOR continuous generative models. image recognition tasks EVALUATE-FOR MALI. adjoint method USED-FOR time series modeling. MALI USED-FOR time series modeling. Metric is memory cost. OtherScientificTerm are integration time, and solver steps. Method are asynchronous leapfrog ( ALF ) solver, and pypi package. ","This paper proposes a new asynchronous leapfrog (ALF) solver, called ALF Integrator (MALI), for continuous depth deep learning models with continuous depth. MALI is based on neural ordinary differential equations (Neural ODEs) and is able to approximate the reverse-time trajectory of the gradient in the continuous case. The authors show that the ALF integrator has a constant memory cost of $O(\sqrt{T})$ and that the memory cost depends on the integration time, the number of solver steps, and the size of the pypi package. The paper also shows that the heavy memory burden and inaccuracy of gradient estimation in a continuous case are the main reasons for the high accuracy of MALi. The main contribution of the paper is to propose an adaptive checkpoint adjoint method (ACA) and a naive method (ALI) that can be combined with an adaptive naive method. The proposed methods are evaluated on a variety of tasks, including image recognition tasks, and continuous generative models. The results show that, compared to ResNet, MALIs outperform the adjoint methods and Neural ODE on ImageNet. ","This paper proposes an asynchronous leapfrog (ALF) solver for deep learning models with continuous depth. The main idea is to use neural ordinary differential equations (Neural ODEs) in deeplearning models. The authors propose a naive method and an adaptive checkpoint adjoint method (ACA) to reduce the memory cost of the ALF Integrator (MALI), which has a constant memory cost. In the continuous case, the numerical estimation of the gradient is performed in a continuous case and the reverse-time trajectory is computed. The paper shows that MALI outperforms ResNet on a number of tasks, including image recognition tasks, and for continuous generative models. In addition, the authors show that the proposed methods outperform other methods in time series modeling. "
414,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,methodology EVALUATE-FOR complex scene conditional generation models. model USED-FOR seen conditionings. seen object combinations FEATURE-OF unseen conditionings. unseen object combinations USED-FOR unseen conditionings. methods USED-FOR recognizable scenes. compositionality USED-FOR unseen conditionings. compositionality USED-FOR methods. seen conditionings USED-FOR methods. seen object combinations USED-FOR unseen conditionings. unseen object combinations FEATURE-OF conditionings. image quality degradation EVALUATE-FOR methods. semantically aware losses USED-FOR generation process. robustness EVALUATE-FOR unseen conditionings. robustness FEATURE-OF unseen conditionings. instance - wise spatial conditioning normalizations USED-FOR compositionality. scene - graph perceptual similarity HYPONYM-OF semantically aware losses. Generic is models. Method is pipeline components. ,"This paper proposes a new methodology for complex scene conditional generation models. The proposed methods are based on compositionality to generate recognizable scenes from unseen conditionings, where the model is trained to generate seen conditionings from unseen object combinations rather than from seen object combinations. The compositionality is achieved by using instance-wise spatial conditioning normalizations. The authors show that the proposed methods can improve the robustness of unseen conditions in terms of image quality degradation compared to existing methods. The main contribution of the paper is the use of semantically aware losses for the generation process, such as scene-graph perceptual similarity. ","This paper proposes a new methodology for complex scene conditional generation models. The authors propose two methods to generate recognizable scenes from unseen conditionings (e.g. seen object combinations). The first method uses a model to generate seen conditionings from unseen object combinations, and the second method uses compositionality to generate unseen conditions. The two methods are evaluated in terms of image quality degradation and robustness. The paper proposes two semantically aware losses for the generation process: scene-graph perceptual similarity and instance-wise spatial conditioning normalizations. Experiments show that the proposed models outperform other pipeline components."
423,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"expressive power CONJUNCTION learning. learning CONJUNCTION expressive power. multi - hop operators FEATURE-OF graph. multi - hop operators USED-FOR node features. GA - MLPs USED-FOR non - isomorphic graphs. WeisfeilerLehman ( WL ) test CONJUNCTION GNNs. GNNs CONJUNCTION WeisfeilerLehman ( WL ) test. operators USED-FOR GA - MLPs. GA - MLPs CONJUNCTION GNNs. GNNs CONJUNCTION GA - MLPs. expressive power EVALUATE-FOR GNNs. expressive power EVALUATE-FOR GA - MLPs. node - level functions USED-FOR them. GNNs COMPARE GA - MLPs. GA - MLPs COMPARE GNNs. GA - MLPs USED-FOR attributed walks. community detection EVALUATE-FOR GA - MLPs. GNNs USED-FOR learning. operator family USED-FOR GA - MLPs. Generic is alternative. Method is learnable node - wise functions. OtherScientificTerm are node - wise functions, and rooted graphs. Task is graph isomorphism testing. ","This paper studies the problem of graph isomorphism testing in the context of non-isomorphic graphs with multi-hop operators on the node features of a graph. The authors propose to use GA-MLPs as an alternative to GNNs in the WeisfeilerLehman (WL) test, which is used to evaluate the expressive power and the learning performance of the learned node-wise functions. They show that the learned operators can be used to improve the expressive performance of GA- MLPs in non-Isomorphic graphs. They also show that using the learned operator family can improve the performance in the presence of node-level functions. Finally, the authors show that GA - MLPs are able to perform attributed walks for community detection. ",This paper proposes a new method for graph isomorphism testing. The authors propose to use multi-hop operators to learn node features in a graph. The proposed method is based on the WeisfeilerLehman (WL) test. The main idea is to learn learnable node-wise functions that are independent of the number of nodes in the graph. They show that the proposed method outperforms GNNs and GA-MLPs in terms of expressive power and learning. They also show that they can learn them with node-level functions. 
432,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"model complexity EVALUATE-FOR Reinforcement Learning ( RL ) agents. real - world applications EVALUATE-FOR Reinforcement Learning ( RL ) agents. robotics HYPONYM-OF real - world applications. acting USED-FOR distributed RL settings. unaccelerated hardware USED-FOR acting. CPUs HYPONYM-OF unaccelerated hardware. model complexity EVALUATE-FOR supervised learning. distillation USED-FOR learning progress. large capacity learner model COMPARE small capacity actor model. small capacity actor model COMPARE large capacity learner model. system USED-FOR acting. transformer models COMPARE LSTMs. LSTMs COMPARE transformer models. procedure USED-FOR partially - observable environments. computational complexity EVALUATE-FOR transformer models. transformer models CONJUNCTION LSTMs. LSTMs CONJUNCTION transformer models. fast inference CONJUNCTION total training time. total training time CONJUNCTION fast inference. Actor - Learner Distillation USED-FOR transformer learner model. total training time EVALUATE-FOR LSTM actor model. fast inference EVALUATE-FOR LSTM actor model. Actor - Learner Distillation USED-FOR memory environments. OtherScientificTerm are compute, model size, intractable experiment run times, actor - latency ” constrained settings, and model capacity. ","This paper studies the model complexity of Reinforcement Learning (RL) agents in real-world applications such as robotics. The authors propose a system for performing acting in distributed RL settings using unaccelerated hardware such as CPUs. The system is based on the Actor-Learner Distillation, which distills the learning progress from a large capacity learner model to a small capacity actor model. The main contribution of the paper is to show that the computational complexity of transformer models and LSTMs is comparable to the performance of the LSTM actor model in terms of fast inference and total training time.  The authors also provide a procedure for performing partially-observable environments where the model size is limited and the actor-latency” constrained settings. ","This paper studies the model complexity of Reinforcement Learning (RL) agents in real-world applications (e.g., robotics). The authors show that a large capacity learner model outperforms a small capacity actor model in supervised learning. The authors also show that the computational complexity of transformer models outperforms LSTMs in terms of fast inference and total training time in “actor-latency” constrained settings.  The authors propose a new system for acting in distributed RL settings with unaccelerated hardware (i.e., CPUs). The main contribution of the paper is a new procedure for partially-observable environments where the model size is small and the intractable experiment run times are high. The paper also proposes to use Actor-Learner Distillation for memory environments to improve the model capacity."
441,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"benchmarks USED-FOR problem. Meta - Dataset HYPONYM-OF benchmarks. universal features USED-FOR few - shot classification. Meta - Dataset EVALUATE-FOR URT. model USED-FOR cross - domain generalization. Task are Few - shot classification, multi - domain few - shot image classification, and multi - domain setting. Material is diverse data sources. Method are feature representations, Universal Representation Transformer ( URT ) layer, and domain - specific representations. Generic is it. OtherScientificTerm is attention score heatmaps. ","This paper studies the problem of few-shot classification with universal features. The authors propose a new benchmark, Meta-Dataset, to evaluate the performance of feature representations learned by a Universal Representation Transformer (URT) layer. They show that URT can achieve better performance than the state-of-the-art in the multi-domain setting. They also show that the model can be used for cross-domain generalization. ","This paper proposes a new model for cross-domain generalization. Few-shot classification is an important problem in few-shot image classification, where diverse data sources are available. The problem is formulated as a multi-domain setting where the feature representations are generated by a Universal Representation Transformer (URT) layer. The authors propose two benchmarks for the problem, Meta-Dataset and Few-Shot classification. The URT is evaluated on the Meta-dataset, where it is shown that the universal features can be used to improve the performance of few-shooting classification, while the domain-specific representations are not. The attention score heatmaps are also evaluated."
450,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"non - stationary stream of unlabeled data USED-FOR salient representations. representations USED-FOR classification tasks. Self - Taught Associative Memory ( STAM ) HYPONYM-OF online clustering module. architecture USED-FOR UPL problem. online clustering module PART-OF architecture. online clustering CONJUNCTION novelty detection. novelty detection CONJUNCTION online clustering. novelty detection CONJUNCTION forgetting outliers. forgetting outliers CONJUNCTION novelty detection. novelty detection USED-FOR Layered hierarchies of STAM modules. online clustering USED-FOR Layered hierarchies of STAM modules. UPL context EVALUATE-FOR latter. Task is Unsupervised Progressive Learning ( UPL ) problem. Material is limited labeled data. Method are prototypical representations, and STAM architecture. ","This paper studies the Unsupervised Progressive Learning (UPL) problem, where the goal is to learn representations from a non-stationary stream of unlabeled data. The authors propose a new architecture, Self-Taught Associative Memory (STAM), which is an online clustering module for the UPL problem. The STAM architecture is based on the idea of prototypical representations, which can be used to learn salient representations for classification tasks. Layered hierarchies of STAM modules are used for online clustining and novelty detection, and forgetting outliers are also used for the latter. Experiments on limited labeled data demonstrate the effectiveness of the proposed StAM architecture in UPL context.","This paper proposes a novel architecture for the Unsupervised Progressive Learning (UPL) problem. The proposed architecture is based on Self-Taught Associative Memory (STAM) which is an online clustering module that learns salient representations from a non-stationary stream of unlabeled data. The key idea of the proposed STAM architecture is to learn prototypical representations that can be used for classification tasks with limited labeled data. Experiments on the UPL context show that the proposed architecture outperforms the state-of-the-art in terms of novelty detection and forgetting outliers. Layered hierarchies of STAM modules are also proposed, and the novelty detection is performed by using online clusters."
459,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"communication topology CONJUNCTION data partitioning. data partitioning CONJUNCTION communication topology. network size CONJUNCTION communication topology. communication topology CONJUNCTION network size. models COMPARE models. models COMPARE models. data partitioning HYPONYM-OF parameters. communication topology HYPONYM-OF parameters. network size HYPONYM-OF parameters. network topology CONJUNCTION learning rate. learning rate CONJUNCTION network topology. generalization gap FEATURE-OF decentralized deep learning. Method are deep learning models, on - device learning over networks, decentralized training, centralized training, communication efficient training schemes, and training schemes. OtherScientificTerm are large compute clusters, and consensus distance. ","This paper studies the problem of on-device learning over networks. The authors consider the setting where the number of compute clusters is large and the communication topology, network size, and data partitioning are different. They show that the generalization gap of decentralized deep learning is larger than that of centralized deep learning. They then propose two communication efficient training schemes, and show that these models perform better than centralized models.","This paper proposes a new framework for decentralized deep learning, which is based on on-device learning over networks. The authors propose two new parameters, namely, communication topology and data partitioning, to improve the generalization of deep learning models to large compute clusters. The proposed models are shown to outperform existing models in terms of generalization gap, and the authors show that decentralized training is more efficient than centralized training. The paper also proposes two communication efficient training schemes, which are based on consensus distance and network topology."
468,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,activity recognition CONJUNCTION natural language processing. natural language processing CONJUNCTION activity recognition. sequence alignment approaches CONJUNCTION representation learning. representation learning CONJUNCTION sequence alignment approaches. Sequence metric learning USED-FOR applications. sequential multi - variate data USED-FOR applications. natural language processing HYPONYM-OF applications. activity recognition HYPONYM-OF sequential multi - variate data. natural language processing HYPONYM-OF sequential multi - variate data. sequence alignment approaches USED-FOR applications. activity recognition HYPONYM-OF applications. representation learning USED-FOR applications. synchronized trajectories CONJUNCTION distance between similar sequences. distance between similar sequences CONJUNCTION synchronized trajectories. dynamical systems USED-FOR synchronized trajectories. siamese recurrent neural network USED-FOR distance between similar sequences. sub - networks CONJUNCTION dynamical systems. dynamical systems CONJUNCTION sub - networks. dynamical systems PART-OF siamese recurrent network. sub - networks PART-OF siamese recurrent network. gate PART-OF classical Gated Recurrent Unit architecture. neural network model USED-FOR coupling. gate USED-FOR neural network model. gate USED-FOR coupling. model USED-FOR synchronization of unaligned multi - variate sequences. model USED-FOR similarity metric. similarity metric CONJUNCTION synchronization of unaligned multi - variate sequences. synchronization of unaligned multi - variate sequences CONJUNCTION similarity metric. coupling USED-FOR siamese Gated Recurrent Unit architecture. activity recognition dataset EVALUATE-FOR siamese Gated Recurrent Unit architecture. Method is dynamical system theory. ,"This paper studies the problem of sequence metric learning in sequential multi-variate data. Sequence alignment approaches and sequence alignment approaches are widely used in various applications such as activity recognition, natural language processing, and representation learning. The authors propose a classical Gated Recurrent Unit architecture with a gate in which a neural network model predicts the distance between similar sequences using a siamese recurrent neural network and a dynamical system. The coupling between the sub-networks and the dynamical systems is modeled by a differentiable model. The model is then used to learn the similarity metric between the synchronized trajectories and the distance to the ground truth.  The authors show that this coupling can be achieved by using a differential model to perform synchronization of unaligned multi-viableate sequences and a similarity metric. They also show that the proposed siameses can be combined with the classical gated recurrent unit architecture by using the gate in order to achieve better coupling.   The paper also provides a theoretical analysis of the siamesed recurrent network and its sub-network. The theoretical analysis is based on the dynamics of dynamical network theory. The experimental results on the activity recognition dataset demonstrate the effectiveness of the proposed Siamese Gated ReLU architecture. ","This paper proposes a novel Gated Recurrent Unit architecture with a gate that is based on dynamical system theory. Sequence metric learning is used for applications such as sequential multi-variate data, natural language processing, and sequence alignment approaches. The proposed siamese recurrent network consists of sub-networks and dynamical systems, which are used to learn synchronized trajectories and distance between similar sequences. The gate is used as a neural network model for coupling, and the model is used to compute the similarity metric and the synchronization of unaligned multi-viable sequences. Experiments on the activity recognition dataset show that the proposed Siamese Gated ReLU architecture is able to achieve better performance than the classical Gated recurrent unit architecture. "
477,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"codistillation PART-OF distributed training setup. models COMPARE models. models COMPARE models. codistillation USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. codistillation USED-FOR models. batch sizes CONJUNCTION learning rate schedules. learning rate schedules CONJUNCTION batch sizes. it USED-FOR distributed computing environment. Method is Codistillation. OtherScientificTerm are auxiliary loss, model replicas, large batch sizes, and moderate batch sizes. Metric is accuracy. ",This paper studies the problem of distributed training setup with codistillation in the distributed setting. The authors propose a synchronization mechanism for training models with synchronous data-parallel methods. They show that models trained with this synchronization mechanism can achieve better performance than models trained without it. They also show that it can be applied to the distributed computing environment. ,This paper proposes to use codistillation in the distributed training setup to improve the accuracy of models compared to models trained with the same auxiliary loss. The authors propose a synchronization mechanism for models trained using synchronous data-parallel methods. They show that it improves the accuracy in a distributed computing environment. They also show that the model replicas are more robust to large batch sizes and learning rate schedules. 
486,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"capacity CONJUNCTION complexity. complexity CONJUNCTION capacity. stochastic gradient descent ( SGD ) USED-FOR deep learning. SGD USED-FOR local minimum. SGD iterates USED-FOR heavy - tailed stationary distribution. algorithm parameters CONJUNCTION b. b CONJUNCTION algorithm parameters. independent and identically distributed Gaussian data USED-FOR linear regression problem. dimension CONJUNCTION curvature. curvature CONJUNCTION dimension. algorithm parameters CONJUNCTION dimension. dimension CONJUNCTION algorithm parameters. algorithm parameters USED-FOR tails. SGD USED-FOR deep learning. synthetic data CONJUNCTION fully connected neural networks. fully connected neural networks CONJUNCTION synthetic data. OtherScientificTerm are eigenvalues of the Hessian, batch size b, stochastic gradient noise, tail - index ’, network weights, and Hessian. Metric is tail - index. Task are generalization, and quadratic optimization. ","This paper studies the problem of generalization in stochastic gradient descent (SGD) for deep learning. The authors consider the linear regression problem on independent and identically distributed Gaussian data, where the eigenvalues of the Hessian are unknown and the number of iterations of SGD iterates for a heavy-tailed stationary distribution is unknown. The main contribution of the paper is to show that SGD is a local minimum for the local minimum of the tail-index of the SGD. The paper also shows that the size of the batch size b is a function of the dimension, the curvature, the algorithm parameters, and the b.    The authors also provide a theoretical analysis of the generalization properties of the weights of the network weights. They show that the weights can be computed by quadratic optimization. They also provide an algorithm that can be used to approximate the tails of the tails. Finally, the authors provide some theoretical guarantees for the performance of the proposed SGD in deep learning on synthetic data and fully connected neural networks.","The paper proposes stochastic gradient descent (SGD) for deep learning. SGD iterates over a heavy-tailed stationary distribution, where the eigenvalues of the Hessian are the batch size b, and the network weights are the tail-index’. The authors show that SGD converges to the local minimum, which is a linear regression problem on independent and identically distributed Gaussian data. The paper also shows that the generalization of SGD to synthetic data and fully connected neural networks is equivalent to quadratic optimization. The main contribution of the paper is the analysis of the dimension, curvature, and algorithm parameters for the tails."
495,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,models USED-FOR community detection. spectral manipulations USED-FOR models. bandpass filtering USED-FOR GCN. high - frequencies USED-FOR community detection. images HYPONYM-OF Euclidean graph. spectral components USED-FOR supervised community detection task. graph structure USED-FOR cascade of filtering. low - frequency domain FEATURE-OF spectral components. low frequencies USED-FOR classifiers. Task is nodes classification. Method is GCNs. ,This paper studies the problem of community detection with spectral manipulations in models trained on high-frequencies. The authors propose a new supervised community detection task with spectral components in the low-frequency domain. The proposed GCN is based on bandpass filtering. The graph structure of the cascade of filtering is used to learn the graph structure. The spectral components are then used to train the classifiers on the low frequencies. The experiments show the effectiveness of the proposed GCNs.,This paper proposes a novel framework for learning models for community detection using spectral manipulations. The authors propose to use bandpass filtering to train a GCN with high-frequency and low-frequency components. The proposed framework is based on the Euclidean graph (e.g. images) and the graph structure of the GCN. The spectral components of the supervised community detection task are learned in the low-frequencies domain and the classifiers are trained on the low frequencies. The paper also proposes to use GCNs with graph structure to learn the cascade of filtering. 
504,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"graph structure USED-FOR Graph neural networks ( GNNs ). structure COMPARE real - world applications. real - world applications COMPARE structure. structure CONJUNCTION GNN parameters. GNN parameters CONJUNCTION structure. taskspecific supervision USED-FOR structure. taskspecific supervision USED-FOR GNN parameters. method USED-FOR supervision. method USED-FOR graph structure. self - supervision USED-FOR method. models USED-FOR task - specific graph structure. SLAPS COMPARE models. models COMPARE SLAPS. OtherScientificTerm are task - specific latent structure, graph structures, and Self - supervision. Method is GNN. ","This paper proposes a new graph structure for Graph neural networks (GNNs). The proposed structure is different from real-world applications in that it is a task-specific latent structure. The proposed method uses taskspecific supervision to learn the structure and the GNN parameters. The authors show that the proposed method can learn the graph structure by self-supervision, and then use the learned graph structure to train the models for learning the task -specific graph structure. Self-supervised GNNs are shown to perform better than SLAPS.",This paper proposes a new graph structure for Graph neural networks (GNNs) that is different from real-world applications. The proposed structure is based on taskspecific supervision of GNN parameters and structure. Self-supervision is used to learn the task-specific latent structure of the graph structures. The authors show that the proposed method is able to learn a graph structure that is more similar to the real graph structure than other models that do not learn a task- specific graph structure. They also show that their proposed GNN outperforms other models in terms of SLAPS.
513,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,Continual Learning USED-FOR catastrophic forgetting. supervised training USED-FOR they. model USED-FOR label - agnostic incremental setting. network confusion USED-FOR novelty detection method. class - imbalance USED-FOR detection method. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. CIFAR-100 CONJUNCTION CRIB. CRIB CONJUNCTION CIFAR-100. image classification benchmarks EVALUATE-FOR approach. CRIB HYPONYM-OF image classification benchmarks. MNIST HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. SVHN HYPONYM-OF image classification benchmarks. ,"This paper proposes a novel novelty detection method based on network confusion. The proposed method is based on class-imbalance, which is a common problem in the label-agnostic incremental setting. The authors propose to use supervised training to train the model, and they show that they can achieve state-of-the-art performance on image classification benchmarks such as CIFAR-10, SVHN, and MNIST. ","This paper proposes a novel novelty detection method based on network confusion. The authors propose a novel model for label-agnostic incremental setting, where they use supervised training to prevent catastrophic forgetting. The proposed detection method is based on class-imbalance and class-overlapping. Experiments on image classification benchmarks show the effectiveness of the proposed approach on CIFAR-10, MNIST, SVHN, and CRIB."
522,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,robotics CONJUNCTION autonomous cars. autonomous cars CONJUNCTION robotics. domain expertise USED-FOR specifications. natural language constraints USED-FOR safe RL. HAZARDWORLD HYPONYM-OF multi - task benchmark. agent USED-FOR multi - task benchmark. free - form text USED-FOR constraints. agent USED-FOR tasks. modular architecture FEATURE-OF agent. policy network USED-FOR policy. constraint interpreter USED-FOR spatial and temporal representations of forbidden states. constraint interpreter USED-FOR textual constraints. representations USED-FOR policy. minimal constraint violations FEATURE-OF policy. representations USED-FOR policy network. policy network PART-OF model. constraint interpreter PART-OF model. method COMPARE approaches. approaches COMPARE method. rewards CONJUNCTION constraint violations. constraint violations CONJUNCTION rewards. HAZARDWORLD EVALUATE-FOR method. constraint violations EVALUATE-FOR method. rewards EVALUATE-FOR method. Task is safe reinforcement learning ( RL ). OtherScientificTerm is mathematical form. ,"This paper studies the problem of safe reinforcement learning (RL) with natural language constraints. The authors propose a new multi-task benchmark called HAZARDWORLD, where the agent is trained to solve a set of tasks using free-form text. The agent learns to solve the tasks using a modular architecture, where each task is represented in a mathematical form, and the constraints are learned using domain expertise. The policy network is trained using a policy network that learns the representations of the policy, and a constraint interpreter is used to learn the textual constraints. Experiments show that the proposed method can achieve better rewards and fewer constraint violations than existing approaches.","This paper proposes a new framework for safe reinforcement learning (RL) based on natural language constraints. The authors propose a multi-task benchmark, called HAZARDWORLD, where the agent is trained on robotic robotics and autonomous cars. The agent is a modular architecture, where each task is represented as a set of specifications, and the domain expertise is used to enforce the specifications. The proposed model consists of a policy network, a constraint interpreter, and two representations for the policy network: spatial and temporal representations of forbidden states. The constraints are learned from free-form text. The paper shows that the proposed method outperforms existing approaches in terms of rewards and constraint violations. "
531,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"few - shot semantic edge detection USED-FOR boundaries of novel categories. few - shot semantic edge detection HYPONYM-OF few - shot learning challenge. image generation CONJUNCTION medical imaging. medical imaging CONJUNCTION image generation. semantic segmentation CONJUNCTION localization. localization CONJUNCTION semantic segmentation. object reconstruction CONJUNCTION image generation. image generation CONJUNCTION object reconstruction. boundary information USED-FOR semantic segmentation. boundary information USED-FOR localization. boundary information USED-FOR object reconstruction. Few - shot semantic edge detection USED-FOR recovery of accurate boundaries. small - scale FEATURE-OF semantic segmentation module. semantic segmentation module USED-FOR CAFENet. predicted segmentation mask USED-FOR attention map. multi - split matching USED-FOR regularization method. meta - training USED-FOR metric - learning problem. highdimensional vectors USED-FOR metric - learning problem. FSE-1000 CONJUNCTION SBD-5. SBD-5 CONJUNCTION FSE-1000. them EVALUATE-FOR CAFENet. FSE-1000 HYPONYM-OF datasets. SBD-5 HYPONYM-OF datasets. CAFENet COMPARE baseline methods. baseline methods COMPARE CAFENet. fine - tuning or few - shot segmentation USED-FOR CAFENet. fine - tuning or few - shot segmentation USED-FOR baseline methods. Method are meta - learning strategy, and decoder module. OtherScientificTerm are lack of semantic information, and low - dimensional sub - vectors. ","This paper proposes a few-shot semantic edge detection for the boundaries of novel categories. The authors propose a meta-learning strategy to recover the recovery of accurate boundaries from the lack of semantic information. The semantic segmentation module of CAFENet is trained on a small-scale, and the decoder module is trained using the predicted segmentation mask. The regularization method is based on multi-split matching, where the attention map is computed by predicting the predicted segments of the input image.  The authors show that the proposed method is able to recover highdimensional vectors for the metric-learning problem with meta-training. The experiments on two datasets, FSE-1000 and SBD-5, show that CAFenet outperforms the baseline methods with fine-tuning or few-shooting segmentation.","This paper presents a few-shot semantic edge detection for the boundaries of novel categories. The authors propose a meta-learning strategy, which is a regularization method based on multi-split matching. The main contribution of the paper is to propose a semantic segmentation module on small-scale. The proposed decoder module is based on a predicted segmentation mask, which maps the attention map to low-dimensional sub-vices. The meta-training is performed on a metric-learning problem with highdimensional vectors. The results show that the proposed CAFENet outperforms baseline methods on several datasets (e.g. FSE-1000, SBD-5, and medical imaging). "
540,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"explainability EVALUATE-FOR GNN. feature attribution USED-FOR explanation generation. causal interpretability FEATURE-OF GNNs. It USED-FOR graph feature. causal attribution FEATURE-OF graph feature. edge HYPONYM-OF graph feature. Causal Screening USED-FOR GNN model. Causal Screening USED-FOR model - agnostic tool. Causal Screening COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE Causal Screening. graph classification datasets EVALUATE-FOR Causal Screening. predictive accuracy CONJUNCTION contrastivity. contrastivity CONJUNCTION predictive accuracy. predictive accuracy HYPONYM-OF quantitative metrics. contrastivity HYPONYM-OF quantitative metrics. Method is graph neural networks ( GNNs ). OtherScientificTerm are graph features, features, cause - effect, and sanity checks. Metric is statistical interpretability. Generic is method. ",This paper studies the problem of explainability of graph neural networks (GNNs) in the context of graph features. The authors propose a new model-agnostic tool called Causal Screening to improve the explainability performance of a GNN model. The key idea is to use feature attribution for explanation generation in GNNs. It uses the edge of a graph feature as the causal attribution of the graph feature. The graph features are then used to train the GNN. The proposed method is evaluated on a variety of graph classification datasets. The results show that the proposed method performs better than state-of-the-art approaches in terms of predictive accuracy and contrastivity.,"This paper studies the problem of statistical interpretability of graph neural networks (GNNs). The authors propose a novel model-agnostic tool called Causal Screening, which uses feature attribution for explanation generation. It is based on causal interpretability for GNNs. The key idea of the proposed method is to use a graph feature (e.g., edge) as the graph feature and use causal attribution for graph feature. The authors show that the graph features can be interpreted as a cause-effect, and the authors also provide some sanity checks. The proposed method outperforms state-of-the-art approaches on several graph classification datasets. The quantitative metrics are predictive accuracy and contrastivity."
549,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"parameter norms HYPONYM-OF simplicity. measure EVALUATE-FOR network ’s simplicity. measure USED-FOR model. convolutional networks USED-FOR model. CIFAR-10 USED-FOR convolutional networks. flatness of minima CONJUNCTION optimization speed. optimization speed CONJUNCTION flatness of minima. models ’ margin CONJUNCTION flatness of minima. flatness of minima CONJUNCTION models ’ margin. mutual information EVALUATE-FOR measures. mutual information EVALUATE-FOR measure. measure COMPARE measures. measures COMPARE measure. flatness of minima USED-FOR measures. optimization speed USED-FOR measures. models ’ margin USED-FOR measures. measure COMPARE flatness - based measures. flatness - based measures COMPARE measure. Method is over - parameterized neural networks. Generic is they. Task are machine learning, and pruning. OtherScientificTerm are Occam ’s razor, and network ’s parameters. Metric is training loss. ","This paper studies the problem of over-parameterized neural networks. The authors propose a new measure of the network’s simplicity, called parameter norms, which is a measure of how well a model can be pruned by convolutional networks trained on CIFAR-10. They show that the proposed measure is better than other measures such as flatness of minima, optimization speed, models’ margin, and mutual information. They also show that they are better than flatness-based measures. ","The paper proposes a new measure for measuring the network’s simplicity, called parameter norms, for over-parameterized neural networks. The measure is based on CIFAR-10, which is used to evaluate the model trained on convolutional networks.  The authors show that the proposed measure is better than other measures such as models’ margin, flatness of minima, optimization speed, and mutual information. They also show that they are more robust to pruning. "
558,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,Discrete representations USED-FOR temporally - extended tasks. expert specifications USED-FOR they. deep reinforcement learning USED-FOR long - horizon tasks. exploratory video data USED-FOR temporally - abstracted discrete representations. mutual information maximization objective USED-FOR temporally - abstracted discrete representations. abstract states USED-FOR low - level model - predictive controller. DORP USED-FOR low - level model - predictive controller. DORP USED-FOR abstract states. DORP USED-FOR long - horizon tasks. it USED-FOR binary properties. key - and - door HYPONYM-OF binary properties. ,"This paper studies the problem of learning discrete representations for temporally-extended tasks from exploratory video data. The authors propose a mutual information maximization objective to learn temporally abstracted discrete representations with expert specifications. They use deep reinforcement learning for long-horizon tasks and show that they can be learned with DORP. They also show that abstract states can be used as a low-level model-predictive controller. Finally, it is shown that it can learn binary properties such as key-and-door.",This paper proposes to use discrete representations for temporally-extended tasks with expert specifications. The authors propose to use deep reinforcement learning for long-horizon tasks with exploratory video data. They use a mutual information maximization objective to learn temporally abstracted discrete representations from exploratory videos. They show that they can learn abstract states for a low-level model-predictive controller using DORP. They also show that it can learn binary properties such as key-and-door.
567,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"performance CONJUNCTION compression rate. compression rate CONJUNCTION performance. Mixed - precision quantization USED-FOR deep neural networks. compression rate EVALUATE-FOR deep neural networks. performance EVALUATE-FOR deep neural networks. compression rate EVALUATE-FOR Mixed - precision quantization. neural architecture search USED-FOR vast search space. manuallydesigned search space USED-FOR methods. neural architecture search USED-FOR methods. bit - level sparsity quantization ( BSQ ) USED-FOR mixed - precision quantization. bit - level sparsity quantization ( BSQ ) USED-FOR inducing bit - level sparsity. BSQ USED-FOR dynamic precision reduction. BSQ USED-FOR mixed - precision quantization scheme. BSQ USED-FOR all - zero bits. mixed - precision quantization scheme USED-FOR model. hyperparameter USED-FOR gradient - based optimization process. gradient - based optimization process USED-FOR method. BSQ COMPARE methods. methods COMPARE BSQ. accuracy CONJUNCTION bit reduction. bit reduction CONJUNCTION accuracy. model architectures EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR model architectures. bit reduction EVALUATE-FOR BSQ. accuracy EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR BSQ. Generic are it, and approaches. OtherScientificTerm are quantization scheme, optimal quantization scheme, independent trainable variable, and weight elements. Method is differentiable bit - sparsity regularizer. Metric is compression. ","This paper studies the problem of bit-level sparsity quantization (BSQ) in deep neural networks. Mixed-precision quantization improves the performance and compression rate of the training data, but it is not clear how it can be used in practice. The authors propose a differentiable bit-sparsity regularizer, BSQ, which can be applied to any quantization scheme. They show that BSQ is able to achieve dynamic precision reduction in a variety of settings. They also show that the proposed method can be combined with a gradient-based optimization process with a hyperparameter to improve the performance of the model. ","This paper proposes a new quantization scheme for deep neural networks. Mixed-precision quantization (BSQ) is used for inducing bit-level sparsity in the training data. The authors show that BSQ can be used for dynamic precision reduction. They also show that it can be combined with existing methods in the manuallydesigned search space. The proposed method is based on a differentiable bit-sparsity regularizer, where each independent trainable variable is defined as a sum of the weight elements of the model. The hyperparameter is learned by a gradient-based optimization process. The paper shows that the proposed BSQ outperforms existing methods on CIFAR-10 and ImageNet datasets. "
576,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,memory consumption CONJUNCTION faster computation. faster computation CONJUNCTION memory consumption. bitwise operations FEATURE-OF quantized networks. bitwise operations USED-FOR faster computation. generalization capabilities EVALUATE-FOR they. quantized networks USED-FOR gradient based adversarial attacks. robustness FEATURE-OF quantized networks. robustness FEATURE-OF quantized models. gradient vanishing issues FEATURE-OF quantized models. temperature scaling approach USED-FOR decision boundary. forward - backward signal propagation PART-OF network. forward - backward signal propagation USED-FOR gradient vanishing. adversarially trained models CONJUNCTION floating - point networks. floating - point networks CONJUNCTION adversarially trained models. temperature scaled attacks COMPARE attacks. attacks COMPARE temperature scaled attacks. CIFAR-10/100 datasets CONJUNCTION multiple network architectures. multiple network architectures CONJUNCTION CIFAR-10/100 datasets. near - perfect success rate EVALUATE-FOR quantized networks. quantized networks EVALUATE-FOR temperature scaled attacks. adversarially trained models EVALUATE-FOR attacks. floating - point networks EVALUATE-FOR attacks. near - perfect success rate EVALUATE-FOR temperature scaled attacks. Method is Neural network quantization. ,"This paper studies the problem of gradient based adversarial attacks on quantized networks with bitwise operations. In particular, the authors consider the gradient vanishing issues of quantized models with robustness. The authors propose a temperature scaling approach to reduce the decision boundary of the network by using forward-backward signal propagation in the network. They show that the temperature scaled attacks have a near-perfect success rate on CIFAR-10/100 datasets and multiple network architectures. They also show that they have generalization capabilities. ",This paper studies the problem of gradient based adversarial attacks on quantized networks with bitwise operations. The authors propose a novel temperature scaling approach to reduce the decision boundary of the quantized models. The main idea is to use forward-backward signal propagation in the network to reduce gradient vanishing issues. They show that they can improve the generalization capabilities of quantized network by reducing memory consumption and faster computation. They also show that the proposed attacks are more robust to temperature scaled attacks compared to adversarially trained models and floating-point networks on CIFAR-10/100 datasets and multiple network architectures. 
585,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,prototype trajectories PART-OF interpretable recurrent neural network ( RNN ) model. ProtoryNet HYPONYM-OF interpretable recurrent neural network ( RNN ) model. prototype theory USED-FOR ProtoryNet. prototype USED-FOR ProtoryNet. RNN backbone USED-FOR temporal pattern. RNN backbone USED-FOR prototypes. temporal pattern FEATURE-OF prototypes. method COMPARE prototype - based method. prototype - based method COMPARE method. ProtoryNet COMPARE prototype - based methods. prototype - based methods COMPARE ProtoryNet. Material is text sequence. Generic is model. ,"This paper proposes ProtoryNet, a new interpretable recurrent neural network (RNN) model based on prototype trajectories. The prototype theory is based on the prototype theory, and the authors propose a new prototype that can be used as a prototype for ProtoryN. The prototypes are learned using an RNN backbone that predicts the temporal pattern of the prototypes. The authors show that the proposed method outperforms the prototype-based method in terms of performance. ","This paper presents ProtoryNet, an interpretable recurrent neural network (RNN) model with prototype trajectories. The model is based on the prototype theory, and the authors propose a method to learn the prototypes from the RNN backbone. The authors show that the prototypes of the model have a temporal pattern that is similar to the text sequence. The proposed method is shown to outperform the prototype-based method, and is also shown to be more interpretable than the prototype based methods."
594,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,Hidden Markov models ( HMMs ) USED-FOR disease progression modeling. patient covariates USED-FOR estimation. local optima FEATURE-OF HMMs. HMRNN COMPARE discrete - observation HMM. discrete - observation HMM COMPARE HMRNN. likelihood function FEATURE-OF HMRNN. hidden Markov recurrent neural networks ( HMRNNs ) HYPONYM-OF recurrent neural networks ( RNNs ). HMRNN CONJUNCTION predictive neural networks. predictive neural networks CONJUNCTION HMRNN. patient covariate information USED-FOR predictive neural networks. Baum - Welch algorithm USED-FOR HMRNN parameter estimates. HMRNN USED-FOR parameter estimation. HMRNN CONJUNCTION neural networks. neural networks CONJUNCTION HMRNN. neural networks USED-FOR parameter estimation. Alzheimer ’s disease dataset EVALUATE-FOR HMRNN. HMRNN ’s solution COMPARE HMM. HMM COMPARE HMRNN ’s solution. HMRNN ’s solution USED-FOR clinical interpretation. HMRNN ’s solution USED-FOR disease forecasting. HMM USED-FOR clinical interpretation. OtherScientificTerm is patient health state. ,"This paper studies the problem of disease progression modeling with hidden Markov models (HMMs). The authors propose to use patient covariates in the estimation to improve the local optima of HMMs. They show that the likelihood function of an HMRNN with the patient covariate information is similar to the discrete-observation HMM, and that HMRnn can be used to perform parameter estimation with a Baum-Welch algorithm. The authors also show that a combination of HMRN and predictive neural networks can be combined to improve parameter estimation on Alzheimer’s disease dataset. Finally, the authors show that HMM can perform better than HMM for clinical interpretation. ","This paper proposes a new model for disease progression modeling based on hidden Markov models (HMMs) that are based on the local optima of HMMs. The authors show that HMRNN outperforms discrete-observation HMM on the Alzheimer’s disease dataset. They show that the likelihood function of HMRnn is similar to that of recurrent neural networks (RNNs). They also show that for parameter estimation, the proposed Baum-Welch algorithm can be used to estimate H MRNN parameter estimates. They further show that predictive neural networks with patient covariate information can be trained on the patient health state. Finally, the authors provide a theoretical analysis of the proposed model. They also provide an empirical evaluation of the performance of HMM in clinical interpretation and on disease forecasting. "
603,SP:6355337707f1dd373813290e26e9c0a264b993f9,"phenotypes FEATURE-OF structural and functional properties of neuronal types. supervised learning approach USED-FOR gene expression data. components USED-FOR phenotypic characteristics. phenotypic feature CONJUNCTION feature combination. feature combination CONJUNCTION phenotypic feature. sparsity - based regularization algorithm USED-FOR feature combination. sparsity - based regularization algorithm USED-FOR phenotypic feature. sparsity - based regularization algorithm USED-FOR approach. dendritic and axonal phenotypes FEATURE-OF single - cell RNA - Seq dataset. Drosophila T4 / T5 neurons FEATURE-OF single - cell RNA - Seq dataset. single - cell RNA - Seq dataset EVALUATE-FOR approach. analysis EVALUATE-FOR methods. Task are neurobiology, and linear transformation of gene expressions. Material is Single - cell RNA sequencing. OtherScientificTerm are neuronal phenotypes, phenotypic factor, and genetic organization. Generic is method. Method is factorized linear discriminant analysis ( FLDA ). ","This paper proposes a novel supervised learning approach for generating gene expression data from single-cell RNA sequencing. The proposed method is based on factorized linear discriminant analysis (FLDA). The proposed approach uses a sparsity-based regularization algorithm to learn a phenotypic feature and a feature combination, which are then used to train a neural network to predict the phenotypes of the structural and functional properties of neuronal types. The authors show that the proposed method outperforms existing methods in terms of analysis on the single-coding RNA-Seq dataset with dendritic and axonal phenotypes. ","This paper proposes a supervised learning approach for gene expression data. Single-cell RNA sequencing is an important research topic in neurobiology. The proposed method is based on factorized linear discriminant analysis (FLDA). The key idea is to learn the structural and functional properties of neuronal types. The phenotypic factor is defined as the sum of the components of the gene expression, and the genetic organization is modeled as a linear transformation of gene expressions. The authors propose a sparsity-based regularization algorithm for learning the phenotypeic feature and the feature combination. The approach is evaluated on a single cell RNA-Seq dataset with dendritic and axonal phenotypes of Drosophila T4/T5 neurons. The results show that the proposed methods outperform other methods in terms of analysis."
612,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"Saliency maps USED-FOR image classifier. interpretability method USED-FOR posterior distribution. posterior distribution FEATURE-OF saliency map. random variable FEATURE-OF saliency map. image USED-FOR classifier ’s predictive probability. approximate posterior USED-FOR classifier. OtherScientificTerm are likelihood function, prior distribution, positive correlation, and auxiliary information. Method is variational approximation. Generic is It. ","This paper studies the problem of learning saliency maps for image classifier. The authors propose an interpretability method for estimating the posterior distribution of the saliency map of the image under a random variable. The posterior distribution is defined as the likelihood function of the prior distribution over the image, where the prior is the positive correlation between the image’s predictive probability and the image. The approximate posterior is then used to train the classifier by using a variational approximation. It is shown that the auxiliary information of the posterior can be used to improve the performance.","This paper proposes a variational approximation method for learning saliency maps for image classifier. The posterior distribution of the saliency map is defined as the posterior distribution over a random variable. The authors propose an interpretability method to learn the prior distribution over positive correlation, and then use this posterior distribution to train the classifier’s predictive probability over the image. It is shown that this approximate posterior can be used to train a classifier on the image, and that the likelihood function can be approximated by a prior distribution. "
621,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"Pretrained text encoders USED-FOR natural language processing ( NLP ) tasks. BERT HYPONYM-OF Pretrained text encoders. social bias FEATURE-OF pretrained NLP models. sentence - level fairness EVALUATE-FOR pretrained encoders. neural debiasing method USED-FOR pretrained sentence encoder. fair filter ( FairFil ) network USED-FOR debiased representations. filtered embeddings CONJUNCTION bias words. bias words CONJUNCTION filtered embeddings. contrastive learning framework USED-FOR FairFil. real - world datasets EVALUATE-FOR FairFil. FairFil USED-FOR bias degree. FairFil USED-FOR pretrained text encoders. bias degree FEATURE-OF pretrained text encoders. FairFil USED-FOR downstream tasks. Task is word - level debiasing. OtherScientificTerm are pretrained encoder outputs, and rich semantic information. Method are post hoc method, and text encoders. ","This paper studies the problem of word-level debiasing in pretrained text encoders for natural language processing (NLP) tasks. The authors propose a post hoc method, called FairFil, to reduce the social bias of pretrained NLP models with social bias. FairFil is a neural debiased method to train a pretrained sentence encoder with a fair filter (FairFil) network. The paper shows that FairFil can reduce the bias degree of the pre-trained text encoder outputs by reducing the number of filtered embeddings and bias words, and also by using a contrastive learning framework to train the FairFil. Experiments are conducted on real-world datasets to demonstrate the effectiveness of FairFil on several downstream tasks.","This paper studies the problem of word-level debiasing in natural language processing (NLP) tasks. The authors propose a novel post hoc method, BERT, to reduce the social bias of pretrained NLP models. The pretrained text encoders are trained with BERT and evaluated on sentence-level fairness. They show that the pretrained sentence encoder can be debiased by a neural debiase method. They also show that a fair filter (FairFil) network can be used to generate debious representations of the pretraining encoder outputs. FairFil is based on a contrastive learning framework, and is evaluated on real-world datasets. They find that FairFil can reduce the bias degree of the pre-trained text encoder to a lower bias degree for downstream tasks. "
630,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"certified models USED-FOR machine learning security. certified models USED-FOR adversarial perturbations. randomized smoothing USED-FOR models. l2 perturbations FEATURE-OF models. test accuracy CONJUNCTION average certified robust radius. average certified robust radius CONJUNCTION test accuracy. sample - wise randomized smoothing USED-FOR noise levels. sample - wise randomized smoothing USED-FOR defense. robust regions USED-FOR certification. accuracy - robustness trade - off FEATURE-OF transductive setting. transductive setting EVALUATE-FOR method. accuracy - robustness trade - off EVALUATE-FOR method. Task is certifying l2 perturbations. OtherScientificTerm are Gaussian noise, and noise level. Method is pretrain - to - finetune framework. Generic is model. Material is CIFAR-10 and MNIST datasets. ",This paper studies the problem of certifying models for machine learning security against adversarial perturbations. The authors propose a pretrain-to-finetune framework that uses randomized smoothing to improve the performance of certified models against l2 perturbation. The proposed method is evaluated on CIFAR-10 and MNIST datasets. The results show that the proposed method improves the accuracy-robustness trade-off in the transductive setting. ,"This paper proposes a method for certifying models for machine learning security against adversarial perturbations using randomized smoothing. The authors propose a pretrain-to-finetune framework, where the model is trained on CIFAR-10 and MNIST datasets. The proposed method is evaluated in a transductive setting with test accuracy and average certified robust radius. The certification is performed on robust regions, and the noise level is computed by sampling from the robust regions. "
639,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,probabilistic method USED-FOR unsupervised recovery of corrupted data. method USED-FOR posteriors of clean values. degraded samples USED-FOR method. variational methods USED-FOR collapsed densities. reduced entropy condition approximate inference method USED-FOR rich posteriors. imputation CONJUNCTION de - noising. de - noising CONJUNCTION imputation. missing values CONJUNCTION noise. noise CONJUNCTION missing values. variational methods USED-FOR de - noising. variational methods USED-FOR imputation. model COMPARE variational methods. variational methods COMPARE model. real data sets USED-FOR variational methods. real data sets USED-FOR de - noising. data recovery task EVALUATE-FOR model. propagating uncertainty USED-FOR downstream tasks. classification accuracy EVALUATE-FOR imputation. model USED-FOR downstream tasks. OtherScientificTerm is solution space. ,This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The method is based on a reduced entropy condition approximate inference method for recovering posteriors of clean values from degraded samples. The proposed method uses variational methods to recover collapsed densities from the solution space. The model is evaluated on a data recovery task and shows that the proposed model is able to achieve better imputation and de-noising performance than the state-of-the-art in terms of classification accuracy. ,This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The method is based on a reduced entropy condition approximate inference method for recovering posteriors of clean values from degraded samples. The proposed method is motivated by variational methods for collapsed densities. The model is evaluated on the data recovery task and compared to variational method for imputation and de-noising on real data sets. The authors show that the model is able to achieve better classification accuracy and propagating uncertainty for downstream tasks. 
648,SP:4b7d050f57507166992034e5e264cccab3cb874f,"Self - attention mechanism USED-FOR graph neural networks ( GNNs ). Self - attention mechanism USED-FOR graph representation learning task. multi - hop context information USED-FOR attention computation. long - range interactions PART-OF GNN. MAGNA USED-FOR attention. diffusion prior USED-FOR MAGNA. MAGNA USED-FOR large - scale structural information. MAGNA USED-FOR informative attention. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. Citeseer CONJUNCTION Pubmed. Pubmed CONJUNCTION Citeseer. node classification CONJUNCTION knowledge graph completion benchmarks. knowledge graph completion benchmarks CONJUNCTION node classification. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. relative error reduction EVALUATE-FOR state - of - the - art. node classification EVALUATE-FOR MAGNA. MAGNA COMPARE MAGNA. MAGNA COMPARE MAGNA. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. node classification EVALUATE-FOR MAGNA. Citeseer EVALUATE-FOR state - of - the - art. Pubmed EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. large - scale Open Graph Benchmark dataset EVALUATE-FOR MAGNA. WN18RR CONJUNCTION FB15k237. FB15k237 CONJUNCTION WN18RR. Method is attention mechanism. OtherScientificTerm are nodes, network context, attention scores, and receptive field. Generic is network. Task is knowledge graph completion. Metric is performance metrics. ","This paper proposes a self-attention mechanism for graph neural networks (GNNs). The attention mechanism is based on multi-hop context information, which is useful for graph representation learning task. The attention computation is done by using multi-h hop context information in the attention computation. MAGNA is a diffusion prior for the attention. The authors show that MAGNA can capture large-scale structural information from long-range interactions in a GNN. They also show that the attention scores of MAGNA are better than the state-of-the-art on node classification and knowledge graph completion benchmarks.    The authors also provide performance metrics for MAGNA on WN18RR and FB15k237. Cora and Citeseer are shown to outperform MAGNA in terms of relative error reduction and the state of the art on Pubmed. ","This paper proposes a self-attention mechanism for graph neural networks (GNNs) for the graph representation learning task. The attention mechanism is based on multi-hop context information in the attention computation, where nodes are represented as a set of long-range interactions, and the network context is represented by a receptive field. The authors propose a GNN that is able to learn to learn the attention scores of each node in the network. The diffusion prior of MAGNA is used as a diffusion prior. The proposed network is evaluated on a large-scale Open Graph Benchmark dataset. The performance metrics are the relative error reduction and the state-of-the-art on node classification and knowledge graph completion benchmarks. "
657,SP:36310d761deb19e71c8a57de19b48f857707d48b,"test EVALUATE-FOR text model. multitask accuracy EVALUATE-FOR test. multitask accuracy EVALUATE-FOR text model. computer science CONJUNCTION law. law CONJUNCTION computer science. US history CONJUNCTION computer science. computer science CONJUNCTION US history. elementary mathematics CONJUNCTION US history. US history CONJUNCTION elementary mathematics. elementary mathematics CONJUNCTION computer science. computer science CONJUNCTION elementary mathematics. tasks PART-OF test. elementary mathematics HYPONYM-OF tasks. law HYPONYM-OF tasks. US history HYPONYM-OF tasks. computer science HYPONYM-OF tasks. world knowledge CONJUNCTION problem solving ability. problem solving ability CONJUNCTION world knowledge. world knowledge FEATURE-OF models. problem solving ability FEATURE-OF models. GPT-3 model COMPARE random chance. random chance COMPARE GPT-3 model. models COMPARE GPT-3 model. GPT-3 model COMPARE models. near random - chance accuracy EVALUATE-FOR models. tasks EVALUATE-FOR models. lopsided performance FEATURE-OF Models. morality CONJUNCTION law. law CONJUNCTION morality. nearrandom accuracy EVALUATE-FOR they. academic and professional understanding FEATURE-OF model. Metric are accuracy, and expert - level accuracy. ","This paper presents a new test to evaluate the multitask accuracy of a text model. The test consists of three tasks: (1) a set of tasks consisting of elementary mathematics, (2) a law, and (3) a computer science. The authors show that the GPT-3 model achieves near random-chance accuracy compared to random chance. They also show that models with lopsided performance have better world knowledge and problem solving ability than models with high academic and professional understanding. ","This paper presents a new test on multitask accuracy of a text model. The test consists of three tasks: (1) a set of tasks consisting of elementary mathematics, (2) law, and (3) computer science. The authors show that the GPT-3 model performs better than random chance on all three tasks, and that models with lopsided performance perform better than models with near random-chance accuracy on all tasks. They also show that models trained with world knowledge and problem solving ability are more robust to changes in expert-level accuracy than models trained without academic and professional understanding."
666,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"pre - training approach USED-FOR table semantic parsing. GRAPPA HYPONYM-OF pre - training approach. synchronous context - free grammar ( SCFG ) USED-FOR synthetic question - SQL pairs. GRAPPA USED-FOR structural properties. structural properties PART-OF pre - training language model. structural properties PART-OF table semantic parsing. synthetic data USED-FOR GRAPPA. masked language modeling ( MLM ) USED-FOR pre - training process. model USED-FOR real - world data. table - and - language datasets USED-FOR masked language modeling ( MLM ). OtherScientificTerm are compositional inductive bias, and pre - trained embeddings. Method is pre - training strategy. ","This paper proposes a pre-training approach for table semantic parsing based on GRAPPA, which is based on synchronous context-free grammar (SCFG) to generate synthetic question-SQL pairs. The structural properties of the pre-trained language model are learned from synthetic data, and the compositional inductive bias is used to train the model on real-world data. The authors show that the proposed masked language modeling (MLM) can be used to speed up the training process and improve the performance of the model in the presence of table-and-language datasets. ","This paper proposes a pre-training approach for table semantic parsing, GRAPPA, which is based on synchronous context-free grammar (SCFG) for synthesizing synthetic question-SQL pairs. The authors propose to use compositional inductive bias to reduce the number of pre-trained embeddings. The proposed pre-train strategy is a variant of masked language modeling (MLM) which is used to accelerate the training process. The structural properties of the pre-learning language model are incorporated into the structural properties in the synthetic data. The model is then applied to real-world data and evaluated on table-and-language datasets."
675,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"random matrix analysis USED-FOR Gaussian mixture data model. random matrix analysis USED-FOR MTL LS - SVM. small - dimensional ) statistics FEATURE-OF deterministic limit. single - task LS - SVMs COMPARE MTL approach. MTL approach COMPARE single - task LS - SVMs. sufficient statistics USED-FOR method. MTL LS - SVM method COMPARE multi - task and transfer learning techniques. multi - task and transfer learning techniques COMPARE MTL LS - SVM method. Method are multi - task and transfer learning methods, MTL LS - SVM algorithm, and cross - validation procedure. OtherScientificTerm is hyperparameters. ","This paper proposes a new MTL LS-SVM based on random matrix analysis for the Gaussian mixture data model. The authors show that the MTL approach outperforms single-task LS-VMs and multi-task and transfer learning methods in terms of (small-dimensional) statistics in the deterministic limit. They also provide sufficient statistics for the proposed method, and provide a cross-validation procedure. ","This paper proposes a random matrix analysis for the Gaussian mixture data model. The authors show that the MTL LS-SVM algorithm can be applied to multi-task and transfer learning methods. The MTL approach is shown to outperform single-task LS-VMs on small-dimensional (small-dimensional) statistics, which is a deterministic limit on the number of hyperparameters. The proposed method is evaluated on sufficient statistics, and the authors also propose a cross-validation procedure to validate the proposed method. The experimental results show the effectiveness of the proposed MTLLS-sVM method compared to other multi-tasks and transferlearning techniques."
684,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,group equivariant conditional neural process ( EquivCNP ) HYPONYM-OF metalearning method. permutation invariance FEATURE-OF data set. data space FEATURE-OF transformation equivariance. permutation invariance FEATURE-OF metalearning method. transformation equivariance FEATURE-OF it. rotation and scaling equivariance HYPONYM-OF group equivariance. EquivCNPs USED-FOR group symmetries. decomposition theorem USED-FOR permutation - invariant and group - equivariant maps. infinite - dimensional latent space USED-FOR group symmetries. decomposition theorem USED-FOR EquivCNPs. infinite - dimensional latent space USED-FOR EquivCNPs. Lie group convolutional layers USED-FOR architecture. EquivCNP COMPARE CNPs. CNPs COMPARE EquivCNP. translation equivariance FEATURE-OF EquivCNP. 1D regression task EVALUATE-FOR EquivCNP. 1D regression task EVALUATE-FOR CNPs. EquivCNP USED-FOR zero - shot generalization. zero - shot generalization USED-FOR image - completion task. EquivCNP USED-FOR image - completion task. Lie group equivariance PART-OF EquivCNP. Lie group equivariance USED-FOR EquivCNP. Method is conditional neural processes ( CNPs ). OtherScientificTerm is symmetry of real - world data. ,"This paper studies the group equivariant conditional neural process (EquivCNP), a metalearning method based on permutation invariance in the data space of a data set. The authors derive a decomposition theorem for permutation-invariant and group-equivariant maps, and show that it is equivalent to the transformation equivariance of the original data space. EquivCNPs are able to learn group symmetries in an infinite-dimensional latent space with Lie group convolutional layers, which is a new architecture. Theoretically, the authors show that the Lie group equivance of EquivCPN is the same as that of Lie group convexity in the real-world data, and that it can be used to derive rotation and scaling equivariances of the group. Finally, the paper shows that EquivCCPN can achieve better translation equivariancy than CNPs on the 1D regression task and zero-shot generalization on the image-completion task. ",This paper proposes a metalearning method based on the group equivariant conditional neural process (EquivCNP) which is a permutation invariant and group-equivariant method. The authors show that it is equivalent to the transformation equivariance of the data set in the data space. EquivCNPs are able to learn group symmetries in the infinite-dimensional latent space. The decomposition theorem is used to decompose permutation-invariant-and-group-equivant maps. The architecture is based on Lie group convolutional layers. Experiments are conducted on the 1D regression task and on the zero-shot generalization task. The results show that the EquivCPNP outperforms the CNPs in terms of translation equivance. The paper also provides a theoretical analysis of the symmetry of real-world data.
693,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"autoregressive models CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION autoregressive models. approaches USED-FOR text generation. autoregressive models USED-FOR approaches. maximum likelihood estimation USED-FOR approaches. mismatched history distributions FEATURE-OF exposure bias. expert demonstrations USED-FOR offline reinforcement learning ( RL ) problem. offline reinforcement learning ( RL ) problem USED-FOR text generation. demonstrations USED-FOR easy - to - optimize algorithm. importance weighting USED-FOR easy - to - optimize algorithm. optimization issues FEATURE-OF prior RL approaches. online data collection USED-FOR prior RL approaches. MLE CONJUNCTION policy gradient. policy gradient CONJUNCTION MLE. summarization CONJUNCTION question generation. question generation CONJUNCTION summarization. question generation CONJUNCTION machine translation. machine translation CONJUNCTION question generation. models COMPARE those. those COMPARE models. automatic and human evaluation EVALUATE-FOR models. policy gradient USED-FOR question generation. MLE USED-FOR question generation. GOLD USED-FOR those. policy gradient USED-FOR summarization. policy gradient USED-FOR machine translation. summarization EVALUATE-FOR those. summarization EVALUATE-FOR models. MLE USED-FOR models. MLE USED-FOR those. policy gradient USED-FOR those. GOLD USED-FOR models. models USED-FOR exposure bias. decoding algorithms USED-FOR models. Generic is paradigm. OtherScientificTerm are mismatched learning objective, and model - generated histories. ","This paper studies the offline reinforcement learning (RL) problem with expert demonstrations. Previous approaches to text generation are based on autoregressive models and maximum likelihood estimation. The authors propose a new paradigm where the mismatched learning objective is used to estimate the exposure bias between the two mismatched history distributions. They then propose an easy-to-optimize algorithm based on importance weighting. They show that prior RL approaches with online data collection have optimization issues due to the mismatch between the model-generated histories and the expert demonstrations, and propose two decoding algorithms to improve the performance of the models. They also show that the models trained with MLE and policy gradient outperform those trained with GOLD in question generation, summarization, and machine translation. Finally, they show that those models perform well in both automatic and human evaluation.","This paper proposes a new paradigm for offline reinforcement learning (RL) based on expert demonstrations. The authors propose two approaches for text generation based on autoregressive models and maximum likelihood estimation. In particular, the authors propose an easy-to-optimize algorithm based on importance weighting, which is based on demonstrations of expert demonstrations, and an online data collection based on prior RL approaches with optimization issues. The main contribution of the paper is to introduce a mismatched learning objective, which aims to reduce the exposure bias in the case of mismatched history distributions. The proposed models are evaluated on machine translation, question generation, and summarization, and outperform those based on MLE, policy gradient, and GOLD on both automatic and human evaluation. The models are also evaluated on different decoding algorithms. "
702,SP:e77eca51db362909681965092186af2e502aaedc,"intermediate activations USED-FOR back - propagation. gradient - isolated modules PART-OF network. local supervision USED-FOR network. early layers FEATURE-OF task - relevant information. E2E loss FEATURE-OF local modules. information propagation ( InfoPro ) loss USED-FOR local modules. reconstruction loss CONJUNCTION normal cross - entropy / contrastive term. normal cross - entropy / contrastive term CONJUNCTION reconstruction loss. ImageNet CONJUNCTION Cityscapes. Cityscapes CONJUNCTION ImageNet. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. SVHN CONJUNCTION STL-10. STL-10 CONJUNCTION SVHN. InfoPro COMPARE E2E training. E2E training COMPARE InfoPro. CIFAR CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR. CIFAR CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR. datasets EVALUATE-FOR InfoPro. memory footprint EVALUATE-FOR E2E training. Cityscapes HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. STL-10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. method USED-FOR training acceleration. local modules USED-FOR training acceleration. method USED-FOR local modules. Method are deep networks, and locally supervised learning. Metric is GPUs memory footprint. Generic are model, and algorithm. OtherScientificTerm are useful information, task - irrelevant information, InfoPro loss, surrogate optimization objective, and GPU memory constraint. ","This paper studies the problem of backpropagation in deep neural networks. The authors propose a method to learn local modules in the training process. The proposed method is based on the information propagation (InfoPro) loss, where the local modules are learned using the E2E loss. The paper shows that the proposed method can achieve better performance than the state-of-the-art in terms of memory footprint. ","This paper proposes a new approach to local supervised learning. The authors propose to use gradient-isolated modules in the network to improve the back-propagation of the model. The idea is to use intermediate activations for back-probation, and then use local supervision to guide the network through local supervision. The local modules are learned using the information propagation (InfoPro) loss, and the E2E loss for the local modules is based on the reconstruction loss and the normal cross-entropy/contrastive term. The paper shows that InfoPro improves the performance of the network in terms of the memory footprint, compared to the state-of-the-art on several datasets (CIFAR, ImageNet, Cityscapes, STL-10, SVHN). The paper also shows that the proposed method can improve the training acceleration of local modules. "
711,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,Graph neural networks ( GNNs ) USED-FOR framework. expressive power FEATURE-OF learning representation of nodes and graphs. expressive power FEATURE-OF GNNs. multiple aggregation functions HYPONYM-OF complex neighborhood aggregation functions. injective aggregation function HYPONYM-OF complex neighborhood aggregation functions. aggregation function USED-FOR expressive power. framework USED-FOR GNNs. framework USED-FOR expressive power. expressive power EVALUATE-FOR GNNs. diverse sampling USED-FOR diverse neighborhoods. diverse sampling USED-FOR representation of target node. GNN model USED-FOR representation of diverse neighborhoods. representation of diverse neighborhoods USED-FOR representation of target node. rooted sub - graphs HYPONYM-OF diverse neighborhoods. diversity of different neighborhoods USED-FOR expressive power. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GNNs EVALUATE-FOR framework. GAT HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. multi - class node classification task CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION multi - class node classification task. benchmark datasets CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION benchmark datasets. benchmark datasets EVALUATE-FOR multi - class node classification task. dataset USED-FOR multi - label node classification task. method USED-FOR GNN models. framework USED-FOR GNN models. framework USED-FOR GNNs. Method is layer - wise neighborhood aggregation. ,"Graph neural networks (GNNs) are known to have expressive power in the learning representation of nodes and graphs. This paper proposes a framework to improve the expressive power of GNNs by combining multiple aggregation functions, such as multiple aggregations of complex neighborhood aggregation functions (e.g. injective aggregation function) with a single aggregation function. The authors show that the aggregation function can be used to improve expressive power for the target GNN model by learning a representation of diverse neighborhoods (i.e., rooted sub-graphs). The representation of the target node is learned by diverse sampling from the representation of target node, which is then used by the GNN to learn a representation for the diverse neighborhoods. The proposed framework is evaluated on several benchmark datasets for multi-class node classification task, multi-label node classifier task, and multi-labels classification task. The experimental results show the effectiveness of the proposed framework for GNN models such as GCN and GAT.","Graph neural networks (GNNs) are known to have expressive power in learning representation of nodes and graphs. This paper proposes a framework to improve the expressive power of GNNs, including GCN, GAT, and multiple aggregation functions (e.g. injective aggregation function). The proposed framework is based on layer-wise neighborhood aggregation. The representation of target node is learned by a GNN model, and the representation of diverse neighborhoods (i.e., rooted sub-graphs) is learned using diverse sampling. The expressive power is measured by the diversity of different neighborhoods. The proposed method is evaluated on the multi-class node classification task and on the benchmark datasets."
720,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,network transmission failures CONJUNCTION hardware errors. hardware errors CONJUNCTION network transmission failures. defenses USED-FOR corruptions. video machine learning models USED-FOR bit - level network and file corruptions. robustness EVALUATE-FOR video machine learning models. common action recognition CONJUNCTION multi - object tracking tasks. multi - object tracking tasks CONJUNCTION common action recognition. corruption levels FEATURE-OF network and file corruptions. defenses USED-FOR bit - level corruptions. corruption - agnostic and corruption - aware defenses HYPONYM-OF defenses. corruption - agnostic defenses COMPARE no - defense baseline. no - defense baseline COMPARE corruption - agnostic defenses. adversarial training HYPONYM-OF corruption - agnostic defenses. Bit - corruption Augmented Training ( BAT ) HYPONYM-OF corruptionaware baseline. model invariance USED-FOR corruptions. knowledge of bit - level corruptions FEATURE-OF corruptionaware baseline. BAT COMPARE corruption - agnostic defenses. corruption - agnostic defenses COMPARE BAT. BAT COMPARE no - defense baseline. no - defense baseline COMPARE BAT. highly - corrupted videos EVALUATE-FOR no - defense baseline. highly - corrupted videos EVALUATE-FOR BAT. Material is clean / near - clean data. ,"This paper studies the robustness of video machine learning models against bit-level network and file corruptions in the presence of network transmission failures and hardware errors. The authors propose two defenses against corruptions: corruption-agnostic and corruption-aware defenses. The first defense, Bit-corruption Augmented Training (BAT), is a corruptionaware baseline that uses model invariance to detect corruptions at different corruption levels. The second defense, adversarial training, is an extension of corruption-agnostic defenses. BAT outperforms BAT on highly-corrupted videos and no-defense baseline.","This paper presents a corruptionaware baseline based on Bit-corruption Augmented Training (BAT) for video machine learning models for bit-level network and file corruptions. The authors propose two defenses against corruptions: corruption-agnostic and corruption-aware defenses. The corruption levels of the network and the hardware errors are defined as the number of network transmission failures and hardware errors. The defense is based on model invariance to corruptions, and is evaluated on clean/near-clean data and on common action recognition and multi-object tracking tasks. BAT outperforms the no-defense baseline on highly-corrupted videos and the corruption-agnostic defenses on adversarial training."
729,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"Representation learning models USED-FOR graphs. machine learning algorithms USED-FOR feature spaces. feature spaces FEATURE-OF nodes. skip - gram embedding approach USED-FOR implicit tensor factorization. implicit tensor factorization USED-FOR tensor representations of time - varying graphs. skip - gram embedding approach USED-FOR tensor representations of time - varying graphs. learned representations COMPARE state - of - the - art methods. state - of - the - art methods COMPARE learned representations. learned representations USED-FOR downstream tasks. state - of - the - art methods USED-FOR downstream tasks. approach USED-FOR downstream tasks. network reconstruction HYPONYM-OF downstream tasks. method USED-FOR contagion risk. method USED-FOR early risk awareness. disease spreading HYPONYM-OF dynamical processes. contact tracing data USED-FOR early risk awareness. Material is real - world networks. Generic are techniques, and approaches. ",This paper studies the problem of representation learning models for graphs. The authors propose a new skip-gram embedding approach for implicit tensor factorization for tensor representations of time-varying graphs. They show that the learned representations are more robust than state-of-the-art methods on downstream tasks such as network reconstruction and disease spreading. The proposed method also improves early risk awareness on contact tracing data. ,This paper presents a new approach to learning representations of graphs. The authors propose a skip-gram embedding approach for implicit tensor factorization for tensor representations of time-determining graphs. They show that the learned representations can be used for downstream tasks such as network reconstruction and disease spreading. The proposed method is shown to improve early risk awareness on contact tracing data. 
738,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,self - supervised language modeling USED-FOR logical reasoning. self - supervised language modeling USED-FOR mathematical formulas. logical reasoning abilities EVALUATE-FOR language models. evaluation ( downstream ) tasks EVALUATE-FOR logical reasoning abilities. evaluation ( downstream ) tasks EVALUATE-FOR language models. language models USED-FOR formal mathematics. skip - tree task USED-FOR language models. models COMPARE models. models COMPARE models. skip - tree task USED-FOR models. mathematical reasoning abilities FEATURE-OF models. skipsequence tasks USED-FOR models. ,This paper studies the problem of self-supervised language modeling for logical reasoning. The authors propose a skip-tree task to evaluate the performance of language models on logical reasoning abilities on evaluation (downstream) tasks. They show that the proposed models perform better than existing models on mathematical reasoning abilities. They also show that their models perform well on skipsequence tasks.,"This paper proposes to use self-supervised language modeling for logical reasoning for mathematical formulas. The authors evaluate language models for formal mathematics on a number of evaluation (downstream) tasks, and show that the logical reasoning abilities of language models are better than those of other models on the skip-tree task. They also show that models trained on skipsequence tasks are better at mathematical reasoning abilities than other models."
747,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"robustness EVALUATE-FOR defense model. robustness PART-OF adversarial robustness research. defense model PART-OF adversarial robustness research. Obfuscated gradients HYPONYM-OF gradient masking. Obfuscated gradients USED-FOR defense methods. Margin Decomposition ( MD ) attack USED-FOR margin loss. attackability FEATURE-OF terms. Margin Decomposition ( MD ) attack USED-FOR imbalanced gradients. two - stage process USED-FOR terms. two - stage process USED-FOR attackability. models USED-FOR imbalanced gradients. label smoothing USED-FOR models. PGD attack EVALUATE-FOR PGD robustness. PGD robustness EVALUATE-FOR MD attacks. attack USED-FOR defenses. PGD robustness EVALUATE-FOR defenses. PGD robustness EVALUATE-FOR attack. adversarial robustness EVALUATE-FOR imbalanced gradients. OtherScientificTerm are Imbalanced Gradients, and gradient. Metric is overestimated adversarial robustness. Method is defense models. ",This paper studies the robustness of a defense model against imbalanced gradients in adversarial robustness research. The authors propose two defense methods: Obfuscated gradients and gradient masking. The first is a Margin Decomposition (MD) attack to improve margin loss. The second is a two-stage process to improve the attackability of the two terms. The attack improves the PGD robustness against MD attacks. ,"This paper studies the robustness of a defense model in adversarial robustness research. The authors propose two defense methods: Obfuscated gradients and gradient masking. They show that the Margin Decomposition (MD) attack is the best margin loss for imbalanced gradients, and that the two-stage process improves the attackability and the PGD robustness against MD attacks. They also show that models trained with label smoothing are more robust to imbalanced gradientients. "
756,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,rich multi - type symbolic language USED-FOR linear algebra. proving semantic equivalence FEATURE-OF complex expressions. expressions USED-FOR system. typed trees HYPONYM-OF complex expressions. rich multi - type symbolic language USED-FOR expressions. rich multi - type symbolic language USED-FOR system. graph - to - sequence deep learning system USED-FOR axiomatic proofs of equivalence. operators PART-OF expressions. scalars PART-OF expressions. incremental graph - to - sequence networks USED-FOR complex and verifiable symbolic reasoning. robustness EVALUATE-FOR system. zero false positives EVALUATE-FOR It. average true positive coverage EVALUATE-FOR It. OtherScientificTerm is axioms of equivalence. ,"This paper proposes a graph-to-sequence deep learning system for proving semantic equivalence of complex expressions (e.g., typed trees). The system uses a rich multi-type symbolic language to represent the expressions in linear algebra. The expressions are composed of operators in the form of scalars, and the system is trained using a graph - to-sequences deep learning method. It is shown that the system achieves a state-of-the-art average true positive coverage with zero false positives. The system is also shown to be robust to axioms of equivalence. ","This paper proposes a graph-to-sequence deep learning system for proving semantic equivalence of complex expressions such as typed trees. The system is based on rich multi-type symbolic language for linear algebra, where the expressions are composed of operators and scalars. It is shown that the system is robust to zero false positives and has average true positive coverage. Experiments are conducted on complex and verifiable symbolic reasoning using incremental graph-t-sequence networks. The paper also provides axioms of equivalence for the system."
765,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"it USED-FOR multimodal setting. Transformers USED-FOR language domain. Transformers USED-FOR multimodal setting. language model USED-FOR visual model. multimodal Transformers USED-FOR audio - visual video representation learning. modality - specific and modality - shared parts PART-OF Transformer. low - rank approximation USED-FOR parameter sharing scheme. approach USED-FOR Transformers. approach USED-FOR model. model CONJUNCTION Transformers. Transformers CONJUNCTION model. CNN embedding space FEATURE-OF instance similarity. instance similarity USED-FOR negative sampling approach. it USED-FOR audio - visual classification tasks. Method is vision module. OtherScientificTerm are cross - modal information, and memory requirement. Material is Kinetics-700. ","This paper proposes a new approach to learn a language model for the multimodal setting using Transformers in the audio-visual video representation learning. The proposed approach is based on a low-rank approximation of the parameter sharing scheme in the Transformer, where the cross-modal information is represented by a vision module. The authors show that the instance similarity in the CNN embedding space can be used to improve the performance of the model and Transformers. They also show that it can be applied to audio-video classification tasks. ","This paper proposes a new approach to learn Transformers for the multimodal setting. The authors propose a low-rank approximation for the parameter sharing scheme, which is based on Kinetics-700. The key idea is to use a language model to learn a visual model, and then use Transformers to learn the language domain. The Transformer consists of modality-specific and modality -shared parts, and the vision module is used to encode cross-modal information. The proposed negative sampling approach relies on instance similarity in the CNN embedding space. Experiments are conducted on audio-visual video representation learning, and it is shown that it can be used for audio-videographic classification tasks. "
774,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"few - shot learning USED-FOR online, continual setting. large scale indoor imagery USED-FOR visual experience. large scale indoor imagery USED-FOR few - shot learning dataset. spatiotemporal contextual information USED-FOR contextual prototypical memory model. Task are human and machine - learning environments, and online few - shot learning setting. Generic are setting, and models. OtherScientificTerm are spatiotemporal context, and Object classes. Method is few - shot learning approaches. ","This paper studies the problem of few-shot learning in the online, continual setting, where the goal is to learn a contextual prototypical memory model with spatiotemporal contextual information from large scale indoor imagery for visual experience. The setting is similar to the human and machine-learning environments, but the setting is different from the human setting in the sense that it is online, and the models are trained in an online setting. Object classes are not available in the human environment, but in the machine setting they are available. The authors propose to use the few-shoot learning dataset with large scale indoors imagery to train a few-shots learning dataset, and then use this dataset to train an online few-hot learning setting. They show that this setting is a good way to train models that are able to learn contextual prototypes of objects. They also show that these few-hoot learning approaches can be used to improve the performance of existing models. ","This paper proposes a few-shot learning for the online, continual setting. The setting is similar to human and machine-learning environments, where the goal is to learn a contextual prototypical memory model with spatiotemporal contextual information. In this setting, the authors propose to use large scale indoor imagery to capture the visual experience. Object classes are represented as a set of objects in the scene, and models are trained on these objects. The authors show that the proposed method is able to learn better models than other few-shoot learning approaches. The paper also provides a few experiments on a few -shot learning dataset, which is based on large-scale indoor imagery. "
783,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"distribution shift CONJUNCTION gbv rowing. gbv rowing CONJUNCTION distribution shift. temporal graphs USED-FOR GNNs. GNN architectures CONJUNCTION scalable GNN techniques. scalable GNN techniques CONJUNCTION GNN architectures. vertices CONJUNCTION edges. edges CONJUNCTION vertices. accuracy EVALUATE-FOR GNN ’s receptive field. Method is graph neural networks ( GNNs ). OtherScientificTerm are full graph, and temporal window. ","This paper studies the problem of graph neural networks (GNNs) with temporal graphs. The authors show that the distribution shift and gbv rowing in GNNs are caused by the temporal graphs in the full graph. They then propose to use GNN architectures and scalable GNN techniques to solve this problem. The accuracy of the GNN’s receptive field is shown to be a function of the number of vertices, edges, and the size of the temporal window.","This paper studies the problem of graph neural networks (GNNs) with temporal graphs. The authors show that the distribution shift and gbv rowing in GNNs are related to temporal graphs, and that the GNN’s receptive field is a function of the full graph. They also show that GNN architectures and scalable GNN techniques can be seen as temporal graphs with vertices, edges, and edges in the temporal window. The paper also shows that the accuracy of the “GNN ’s” receptive field depends on the number of vertices and edges."
792,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"Representation learning USED-FOR deep reinforcement learning ( RL ). visualized input USED-FOR Representation learning. visualized input USED-FOR feature space. technique USED-FOR representation feature space. cross - state self - constraint(CSSC ) HYPONYM-OF technique. constraint USED-FOR general feature recognition. implicit feedback USED-FOR constraint. learning process USED-FOR general feature recognition. generalization EVALUATE-FOR constraint. generalization EVALUATE-FOR method. OpenAI ProcGen benchmark EVALUATE-FOR method. Method is RL agent. OtherScientificTerm are representation similarity, and Procgen games. ","This paper proposes a new technique called cross-state self-constraint(CSSC) for representation learning in deep reinforcement learning (RL). Representation learning uses a visualized input to represent the feature space of the RL agent, and the goal is to maximize the representation similarity between the agent and the input. The authors propose a new constraint for general feature recognition based on implicit feedback. The proposed method is evaluated on the OpenAI ProcGen benchmark and shows that the proposed constraint improves the generalization of the learning process. ",This paper proposes a novel technique for representation learning in deep reinforcement learning (RL). Representation learning uses a visualized input to represent the feature space. The authors propose a technique called cross-state self-constraint(CSSC) to learn the representation feature space using the RL agent. The constraint is used for general feature recognition based on implicit feedback. The proposed method is evaluated on the OpenAI ProcGen benchmark and shows that the proposed constraint improves the generalization of the learning process. 
801,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"GNNs USED-FOR real - world applications. robustness FEATURE-OF GNNs. adversarial attacks FEATURE-OF GNNs. model parameters CONJUNCTION model predictions. model predictions CONJUNCTION model parameters. restricted near - black - box setup FEATURE-OF GNNs. attacks CONJUNCTION influence maximization problem. influence maximization problem CONJUNCTION attacks. influence maximization problem FEATURE-OF graph. adversarial attack FEATURE-OF GNNs. strategies COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE strategies. GNN models COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE GNN models. GNN models EVALUATE-FOR strategies. Method are Graph neural networks ( GNNs ), and near - black - box attack strategies. Material is realistic setups. OtherScientificTerm is features. ",This paper studies the robustness of Graph neural networks (GNNs) in real-world applications. The authors consider the restricted near-black-box setup of GNNs in the context of adversarial attacks on the features. They show that attacks on a graph with a small number of nodes can have a significant impact on the influence maximization problem of a GNN. They then propose two strategies to defend against these attacks. The proposed strategies outperform baseline adversarial attack strategies on GNN models.,"This paper studies the robustness of Graph neural networks (GNNs) in real-world applications. The authors consider the restricted near-black-box setup of GNNs in the context of adversarial attacks on the model parameters and model predictions. They show that GNN models trained on realistic setups are more robust to such attacks than baseline adversarial attack strategies. They also show that the influence maximization problem of the graph of a GNN can be reduced to a simple one, and that the features of a graph can be modified to be more robust. "
810,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"directed acyclic graphs ( DAGs ) USED-FOR learning causal structures. ( weighted ) adjacency matrix FEATURE-OF DAG causal model. low rank assumption FEATURE-OF ( weighted ) adjacency matrix. low rank assumption USED-FOR DAG causal model. methods USED-FOR causal structure learning. interpretable graphical conditions CONJUNCTION low rank assumption. low rank assumption CONJUNCTION interpretable graphical conditions. assumption USED-FOR methods. maximum rank FEATURE-OF hubs. low rank FEATURE-OF scale - free networks. rank FEATURE-OF DAG. they COMPARE algorithms. algorithms COMPARE they. OtherScientificTerm are causal structures, graphs, and low rank condition. Task is high dimensional settings. Method is low rank adaptations. ","This paper studies the problem of learning causal structures from directed acyclic graphs (DAGs) in high dimensional settings. The authors propose a low rank assumption on the (weighted) adjacency matrix of a DAG causal model under the (low rank assumption) of the interpretable graphical conditions and the (high rank assumption). The authors show that under the low rank condition, DAGs with low rank can learn causal structures that are more interpretable than those with high rank. They also show that these methods can be applied to causal structure learning under the high rank assumption. Finally, they show that they can be used to train scale-free networks with a maximum rank that is close to the true rank of the DAG. ","This paper proposes directed acyclic graphs (DAGs) for learning causal structures in high dimensional settings. The authors show that the (weighted) adjacency matrix of a DAG causal model with a low rank assumption is equivalent to the (low rank assumption of a (heavy weighted) DAG with a high rank assumption. They show that these methods can be used for causal structure learning under interpretable graphical conditions and low rank assumptions. They also show that they can be applied to scale-free networks with low rank under the assumption of the low rank condition. Finally, they show that their algorithms outperform other algorithms in terms of rank. "
819,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"neural architecture search ( NAS ) CONJUNCTION hyper - parameter optimization ( HPO ). hyper - parameter optimization ( HPO ) CONJUNCTION neural architecture search ( NAS ). automated data augmentation ( DA ) CONJUNCTION neural architecture search ( NAS ). neural architecture search ( NAS ) CONJUNCTION automated data augmentation ( DA ). components PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) HYPONYM-OF components. hyper - parameter optimization ( HPO ) HYPONYM-OF components. neural architecture search ( NAS ) HYPONYM-OF components. components USED-FOR joint optimization. it USED-FOR NAS. end - to - end solution USED-FOR ready - to - use model. hyper - parameters CONJUNCTION data augmentation policies. data augmentation policies CONJUNCTION hyper - parameters. co - optimization USED-FOR neural architectures. co - optimization USED-FOR method. data augmentation policies USED-FOR method. DiffAutoML COMPARE end - to - end AutoML algorithms. end - to - end AutoML algorithms COMPARE DiffAutoML. DiffAutoML COMPARE multi - stage AutoML algorithms. multi - stage AutoML algorithms COMPARE DiffAutoML. ImageNet EVALUATE-FOR end - to - end AutoML algorithms. computational efficiency EVALUATE-FOR multi - stage AutoML algorithms. ImageNet EVALUATE-FOR DiffAutoML. NAS CONJUNCTION HPO. HPO CONJUNCTION NAS. automated DA CONJUNCTION NAS. NAS CONJUNCTION automated DA. en - to - end manner FEATURE-OF HPO. Generic is component. OtherScientificTerm is search dimension. Task is search and retraining stages. Method are differentiable joint optimization solution, model retraining, and retraining. ","This paper proposes a differentiable joint optimization solution for neural architectures. The proposed method is based on co-optimization of neural architectures with hyper-parameters and data augmentation policies. The main idea is to learn a set of components for joint optimization. These components are: (1) a neural architecture search (NAS), (2) automated data augmentation (DA), (3) hyper-parallel optimization (HPO), and (4) a combination of these components in an automated machine learning (AutoML) pipeline. The authors show that the proposed method achieves better computational efficiency than existing end-to-end AutoML algorithms such as DiffAutoML on ImageNet. ","This paper proposes a differentiable joint optimization solution for the search and retraining stages of a neural architecture search (NAS) and hyper-parameter optimization (HPO) pipeline. The main idea of the proposed method is to use co-optimization to optimize the neural architectures. The authors show that it can be used for NAS and HPO in an en-to-end manner, and it can also be applied to NAS. The proposed method also uses data augmentation policies to improve the computational efficiency of multi-stage AutoML algorithms such as DiffAutoML on ImageNet. "
828,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,Prior Networks HYPONYM-OF models. interpretable measures of uncertainty EVALUATE-FOR models. tasks EVALUATE-FOR ensemble approaches. calibration CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION calibration. They USED-FOR ensemble of models. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. uncertainty estimates PART-OF model. Ensemble Distribution Distillation ( EnD ) USED-FOR ensemble of models. Prior Networks USED-FOR classification tasks. Prior Networks CONJUNCTION EnD. EnD CONJUNCTION Prior Networks. EnD USED-FOR regression tasks. Prior Networks USED-FOR regression tasks. synthetic data CONJUNCTION UCI datasets. UCI datasets CONJUNCTION synthetic data. UCI datasets CONJUNCTION monocular depth estimation tasks. monocular depth estimation tasks CONJUNCTION UCI datasets. monocular depth estimation tasks EVALUATE-FOR Regression Prior Networks. UCI datasets EVALUATE-FOR Regression Prior Networks. synthetic data EVALUATE-FOR Regression Prior Networks. They COMPARE ensemble approaches. ensemble approaches COMPARE They. OtherScientificTerm is Normal - Wishart distribution. ,"This paper studies the problem of interpretable measures of uncertainty for models such as Prior Networks and Ensemble Distribution Distillation (EnD). Prior Networks are models that are trained with a Normal-Wishart distribution. They are used to train an ensemble of models, and EnD is used to distill the uncertainty estimates of each model into the accuracy and calibration of the model. Prior Networks can be used for classification tasks and regression tasks. Regression Prior Networks on synthetic data, UCI datasets, and monocular depth estimation tasks are shown to perform better than ensemble approaches on these tasks. ","This paper proposes a new model, Regression Prior Networks, which is a variant of Ensemble Distribution Distillation (EnD) for learning models with interpretable measures of uncertainty. They can be used to train an ensemble of models with different calibration and uncertainty estimates. They are shown to outperform other ensemble approaches on a variety of tasks, including classification tasks, and they can also be used for regression tasks on synthetic data, UCI datasets, and monocular depth estimation tasks. "
837,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,catastrophic forgetting FEATURE-OF Neural networks. problem USED-FOR large - scale supervised classification. catastrophic forgetting USED-FOR few - shot classification problems. few - shot tasks USED-FOR Few - shot metalearning algorithms. Bayesian online meta - learning framework USED-FOR sequential few - shot tasks problems. Bayesian online meta - learning framework USED-FOR catastrophic forgetting. catastrophic forgetting CONJUNCTION sequential few - shot tasks problems. sequential few - shot tasks problems CONJUNCTION catastrophic forgetting. Laplace approximation CONJUNCTION variational inference. variational inference CONJUNCTION Laplace approximation. MAML PART-OF Bayesian online learning algorithm. Laplace approximation USED-FOR Bayesian online learning algorithm. variational inference USED-FOR Bayesian online learning algorithm. MAML PART-OF framework. framework USED-FOR few - shot classification. sequentially arriving datasets USED-FOR few - shot classification. framework USED-FOR catastrophic forgetting. framework USED-FOR online meta - learning. online meta - learning USED-FOR few - shot classification settings. Material is sequential datasets. Generic is algorithm. Method is meta - learned model. Task is sequentially arriving few - shot tasks. ,This paper studies the problem of catastrophic forgetting in Neural networks. The authors propose a Bayesian online meta-learning framework for few-shot classification problems with catastrophic forgetting. The proposed framework uses Laplace approximation and variational inference to improve the performance of the proposed algorithm. The paper also shows that the proposed framework can be applied to few -shot classification in a variety of settings. ,This paper studies catastrophic forgetting in Neural networks. The authors propose a Bayesian online meta-learning framework for catastrophic forgetting on sequential few-shot tasks problems. The framework is based on MAML and variational inference. The proposed algorithm is evaluated on two sequential datasets. The results show that the proposed framework can improve few-shoot classification on sequentially arriving datasets. 
846,SP:89d2765946e70455105a608d998c3b900969cb8d,"expressive power EVALUATE-FOR higher - order GNNs. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. RNP - GNNs COMPARE higher - order k - GNN. higher - order k - GNN COMPARE RNP - GNNs. higher - order k - GNN CONJUNCTION Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks CONJUNCTION higher - order k - GNN. RNP - GNNs COMPARE Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks COMPARE RNP - GNNs. computational complexity EVALUATE-FOR higher - order k - GNN. computational complexity EVALUATE-FOR Local Relational Pooling ( LRP ) networks. computational complexity EVALUATE-FOR RNP - GNNs. Task is learning with graphs. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. OtherScientificTerm is local neighborhoods. ","This paper proposes a recursive pooling technique of local neighborhoods for learning with graphs. The proposed model learns subgraphs from a set of nodes in a local neighborhood, and then uses the learned model to learn subgraph representations for each node in the local neighborhood. The authors show that the proposed RNP-GNNs have better computational cost and expressive power compared to higher-order k-GANs and Local Relational Pooling (LRP) networks in terms of computational complexity. The paper also shows that low-order GNNs are more computationally efficient than high-order gNNs.","This paper proposes a recursive pooling technique of local neighborhoods to improve the computational cost and expressive power of higher-order GNNs compared to RNP-GNNs and Local Relational Pooling (LRP) networks. The model learns subgraphs of the same subgraph, and then uses the model to learn subgraph of different local neighborhoods. The authors show that the computational complexity of RNP and LRP networks is much lower than that of higher order k-GANs, and that the expressive power is much higher than the one of low-order gNNs. "
855,SP:c43f5deb340555d78599a3496318514a826b1aae,"competitive environments CONJUNCTION games. games CONJUNCTION competitive environments. GANs HYPONYM-OF games. irregular behaviors FEATURE-OF systems. Multiplicative Weights Update ( MWU ) HYPONYM-OF learning algorithms. canonical game decomposition USED-FOR zero - sum and coordination components. volume - expansion argument USED-FOR characterizations. canonical game decomposition USED-FOR volume - expansion argument. components USED-FOR volume - changing behaviors. matrix domination CONJUNCTION linear program. linear program CONJUNCTION matrix domination. general games CONJUNCTION graphical games. graphical games CONJUNCTION general games. MWU CONJUNCTION OMWU. OMWU CONJUNCTION MWU. MWU USED-FOR potential games. OMWU USED-FOR potential games. local equivalence of volume change USED-FOR multi - player games. Method is Machine Learning. Material is two - person zero - sum games. OtherScientificTerm are Lyapunov chaos, cumulative payoff space, persistent chaos, and zero - sum games. Task are normal - form game settings, and bimatrix games. ","This paper proposes Multiplicative Weights Update (MWU), a new learning algorithms based on canonical game decomposition for zero-sum and coordination components. The main idea is to use a volume-expansion argument to learn the characterizations of the components of the game, and then use these components to learn volume-changing behaviors. The authors show that MWU can be used to learn potential games in competitive environments and games with irregular behaviors, such as GANs. They also show that the local equivalence of volume change in multi-player games such as matrix domination and linear program can be learned by MWU.","This paper proposes a novel learning algorithms, Multiplicative Weights Update (MWU) for two-person zero-sum games. The main idea is to use canonical game decomposition to decompose the zero-sum and coordination components of the systems into irregular behaviors in the Lyapunov chaos. The authors also propose a volume-expansion argument for characterizations of the characterizations, which is based on the local equivalence of volume change in multi-player games. Experiments are conducted on normal-form game settings, and on bimatrix games. MWU and OMWU are shown to outperform other potential games such as matrix domination, linear program, and graphical games. "
864,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"adaptive algorithms USED-FOR deep learning. AMSGrad CONJUNCTION Radam. Radam CONJUNCTION AMSGrad. Radam HYPONYM-OF adaptive algorithms. AMSGrad HYPONYM-OF adaptive algorithms. convergence rate EVALUATE-FOR adaptive algorithms. marginal regret bound minimization HYPONYM-OF proximal function of adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. marginal optimality FEATURE-OF adaptive algorithms. deep learning EVALUATE-FOR adaptive algorithms. Generic are modifications, and algorithm. ","This paper studies the problem of adaptive algorithms for deep learning. The authors propose two adaptive algorithms, AMSGrad and Radam, which are based on the marginal regret bound minimization, which is a proximal function of the adaptive algorithms. They show that the convergence rate of these adaptive algorithms has a marginal optimality of $O(\sqrt{T})$ with respect to the number of modifications. They also show that adaptive algorithms are more efficient than adaptive algorithms without modifications. ","This paper studies the problem of learning adaptive algorithms for deep learning. The authors propose two adaptive algorithms, AMSGrad and Radam, and show that the convergence rate of these adaptive algorithms converges to a proximal function of adaptive algorithms with marginal regret bound minimization. They also provide some modifications to the algorithm to improve the marginal optimality of the adaptive algorithms. "
873,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"risk management USED-FOR real - world decision - making problems. mean - variance criterion HYPONYM-OF risk management approaches. quadratic utility function USED-FOR risk management. reward - constrained variance minimization CONJUNCTION regularization. regularization CONJUNCTION reward - constrained variance minimization. EQUM COMPARE mean - variance RL methods. mean - variance RL methods COMPARE EQUM. double sampling USED-FOR mean - variance RL methods. RL and financial data EVALUATE-FOR EQUM. Method are expected quadratic utility maximization ( EQUM ), and mean - variance control. Task is agent utility maximization. ","This paper studies the problem of risk management in real-world decision-making problems. The authors propose a new risk management approach called expected quadratic utility maximization (EQUM), which is based on the mean-variance criterion. The main contribution of the paper is to introduce a quadrastic utility function for risk management, which can be used as a reward-constrained variance minimization and regularization. Theoretically, the authors show that the proposed EQUM can achieve better performance than mean-various RL methods with double sampling. Empirical results on RL and financial data demonstrate the effectiveness of the proposed approach.","This paper proposes a new risk management approach for real-world decision-making problems. The main idea is to use the expected quadratic utility maximization (EQUM) as the risk management function. The authors propose two risk management approaches, the mean-variance criterion and the reward-constrained variance minimization. The proposed approach is evaluated on both RL and financial data. The results show that the proposed EQUM outperforms other mean-variance RL methods with double sampling. "
882,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"neural networks USED-FOR auxiliary tasks. auxiliary tasks PART-OF coherent loss. network USED-FOR coherent objective function. network USED-FOR nonlinear interactions. network USED-FOR auxiliary task. it COMPARE methods. methods COMPARE it. tasks EVALUATE-FOR AuxiLearn. image segmentation HYPONYM-OF tasks. Task are multi - task learning setting, and designing useful auxiliary tasks. Generic is framework. OtherScientificTerm are implicit differentiation, useful auxiliaries, and low data regime. ","This paper proposes AuxiLearn, a framework for designing auxiliary tasks in the multi-task learning setting. Auxilearn learns a coherent objective function by using a neural network to learn auxiliary tasks that are useful to the coherent loss. The auxiliary task is learned by a network that learns nonlinear interactions between the auxiliary task and the original objective function. The proposed framework is based on implicit differentiation between the two auxiliary tasks, and the auxiliary tasks are learned by designing useful auxiliaries. The authors show that it can achieve better performance than existing methods on a variety of tasks such as image segmentation, and that it is able to learn useful auxiliary tasks at a low data regime.","This paper proposes a framework for learning auxiliary tasks in multi-task learning setting. AuxiLearn is an extension of neural networks to learn auxiliary tasks for a coherent loss. The proposed framework is based on implicit differentiation, where the network learns a coherent objective function for each task, and the auxiliary task is learned by the network for nonlinear interactions. The auxiliary tasks are designed by designing useful auxiliaries, which are learned in a low data regime. Experiments are conducted on a variety of tasks (e.g., image segmentation) and show that it outperforms existing methods."
891,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,out - of - training - distribution sentences PART-OF Neural Machine Translation. Bayesian Deep Learning equivalent of Transformer models USED-FOR out - of - training - distribution sentences. measure USED-FOR long sequences of discrete random variables. approaches USED-FOR long sentences. measure USED-FOR Transformer model. dropout approximate inference USED-FOR Transformer model. WMT13 CONJUNCTION Europarl. Europarl CONJUNCTION WMT13. dropout uncertainty USED-FOR measure. model COMPARE German. German COMPARE model. measure USED-FOR Dutch source sentences. measure USED-FOR German - English translation. WMT13 USED-FOR German - English translation. Europarl USED-FOR German - English translation. ,"This paper proposes a Bayesian Deep Learning equivalent of Transformer models for out-of-training-distribution sentences in Neural Machine Translation. The proposed measure is based on dropout approximate inference for the Transformer model. The authors show that the proposed measure can be applied to long sequences of discrete random variables, which is an important problem in the context of long sentences. They also show that their measure improves the performance of WMT13 and Europarl for German-English translation. ","This paper proposes a Bayesian Deep Learning equivalent of Transformer models for out-of-training-distribution sentences in Neural Machine Translation. The proposed measure is based on dropout approximate inference for long sequences of discrete random variables. The authors compare the proposed measure to WMT13 and Europarl on German-English translation, and show that the proposed model outperforms German on Dutch source sentences. "
900,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"SNIP CONJUNCTION GraSP. GraSP CONJUNCTION SNIP. GraSP CONJUNCTION SynFlow. SynFlow CONJUNCTION GraSP. SynFlow CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION SynFlow. GraSP CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION GraSP. methods COMPARE random pruning. random pruning COMPARE methods. accuracy EVALUATE-FOR magnitude pruning. they COMPARE magnitude pruning. magnitude pruning COMPARE they. methods USED-FOR per - weight pruning decisions. Task is pruning neural networks. Method are neural networks, and pruning heuristics. ","This paper studies the problem of pruning neural networks. The authors propose two methods for per-weight pruning decisions: SNIP and GraSP. They show that these methods have better accuracy than random pruning, and that they can be combined with magnitude pruning and SynFlow. They also show that pruning heuristics can be used to improve the performance.","This paper proposes a new method for pruning neural networks. The main idea is to prune the weights of the neural networks by pruning heuristics. The authors compare SNIP, GraSP, and GraSP with magnitude pruning and show that they outperform random pruning in accuracy. They also show that these methods can be applied to per-weight pruning decisions."
909,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,semi - honest server CONJUNCTION Byzantine malicious clients. Byzantine malicious clients CONJUNCTION semi - honest server. FED - LEARNING HYPONYM-OF federated learning protocol. robust mean estimator USED-FOR FED - LEARNING. FED - LEARNING HYPONYM-OF FL protocol. FED - LEARNING USED-FOR dimension - free estimation error. robust mean estimator USED-FOR FL protocol. FL protocol USED-FOR dimension - free estimation error. FilterL2 HYPONYM-OF robust mean estimator. secure aggregation USED-FOR FED - LEARNING. FilterL2 CONJUNCTION secure aggregation. secure aggregation CONJUNCTION FilterL2. optimal or close - to - optimal performance EVALUATE-FOR FED - LEARNING. OtherScientificTerm is shards. Method is robust FL protocols. ,"This paper proposes a federated learning protocol called FED-LEARNING, which combines a semi-honest server with Byzantine malicious clients. The authors propose a robust mean estimator called FilterL2 to improve the dimension-free estimation error of the FL protocol by using the FED - LEARNING. They also propose a secure aggregation for FED and demonstrate its optimal or close-to-optimal performance. ","This paper proposes a federated learning protocol called FED-LEARNING, which combines a semi-honest server with Byzantine malicious clients. The main idea is to use a robust mean estimator (FilterL2) to estimate the dimension-free estimation error of the FL protocol. The authors show that the proposed FL protocol can achieve optimal or close-to-optimal performance in terms of secure aggregation and secure aggregation. The paper also shows that the robust FL protocols are robust to shards."
918,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"aircraft CONJUNCTION robot morphology. robot morphology CONJUNCTION aircraft. design input USED-FOR unknown objective function. robot morphology HYPONYM-OF domains. aircraft HYPONYM-OF domains. method USED-FOR function. offline MBO methods USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR offline MBO methods. evaluation tasks EVALUATE-FOR field. Design - Bench HYPONYM-OF offline MBO tasks. Design - Bench HYPONYM-OF benchmark suite. benchmark suite EVALUATE-FOR offline MBO tasks. unified evaluation protocol USED-FOR benchmark suite. unified evaluation protocol USED-FOR offline MBO tasks. biology CONJUNCTION material science. material science CONJUNCTION biology. material science CONJUNCTION robotics. robotics CONJUNCTION material science. benchmark EVALUATE-FOR offline MBO methods. Generic are problems, and benchmarks. OtherScientificTerm are feedback, and objective function. Task is data - driven offline MBO setting. ","This paper studies the problem of offline MBO in a data-driven offline setting, where the objective function is unknown. The authors propose a method to learn an unknown objective function from the design input, and then use the learned function to solve the problem. The proposed method uses a high-capacity deep neural network function approximator to solve highdimensional problems. The paper also proposes a unified evaluation protocol to evaluate the performance of the proposed benchmark suite, Design-Bench, on a variety of different off-line MBO tasks from biology, robotics, and material science. ","This paper proposes a data-driven offline MBO setting, where the goal is to learn an unknown objective function from design input. The authors propose a novel method to learn the function from feedback. The paper presents a new benchmark suite, Design-Bench, for high-dimensional problems with high-capacity deep neural network function approximators. The field is well-studied in the literature, and the evaluation tasks in the field are well-motivated. The proposed benchmark suite is a unified evaluation protocol for a range of different offline mBO tasks, including biology, robotics, and material science. "
927,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"self - supervised generative models USED-FOR ELBO. self - supervised generative models USED-FOR multimodal models. generalized ELBO formulation USED-FOR multimodal data. methods PART-OF objective. method COMPARE state - of - the - art models. state - of - the - art models COMPARE method. selfsupervised, generative learning tasks EVALUATE-FOR state - of - the - art models. selfsupervised, generative learning tasks EVALUATE-FOR method. OtherScientificTerm are real - world phenomena, posterior approximation functions, semantic coherence, and joint data distribution. Generic is them. Task is machine learning research. ","This paper proposes a generalized ELBO formulation for multimodal data. The authors propose to use self-supervised generative models to train ELBO. The proposed method outperforms state-of-the-art models on a variety of selfsupervised, generative learning tasks.","This paper proposes a generalized ELBO formulation for multimodal data, which is based on self-supervised generative models for ELBO. The authors propose two methods to achieve this objective, one based on real-world phenomena, the other based on posterior approximation functions. The main contribution of the paper is to introduce a new objective, which allows for semantic coherence between the joint data distribution. Experiments on selfsupervised, generative learning tasks show that the proposed method outperforms state-of-the-art models. "
936,SP:98004554447b82b3d2eb9724ec551250eec7a595,"Bayesian Optimization ( BO ) USED-FOR optimizing expensive black - box functions. priors USED-FOR PrBO. probabilistic model USED-FOR pseudo - posterior. BO USED-FOR pseudo - posterior. priors CONJUNCTION BO. BO CONJUNCTION priors. priors CONJUNCTION probabilistic model. probabilistic model CONJUNCTION priors. BO CONJUNCTION probabilistic model. probabilistic model CONJUNCTION BO. PrBO USED-FOR pseudo - posterior. probabilistic model USED-FOR PrBO. BO USED-FOR PrBO. priors USED-FOR PrBO. PrBO COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PrBO. PrBO COMPARE random search. random search COMPARE PrBO. state - of - the - art methods COMPARE random search. random search COMPARE state - of - the - art methods. real - world hardware design application EVALUATE-FOR PrBO. it USED-FOR misleading priors. OtherScientificTerm are function evaluations, machine learning hyperparameters, and user priors. Method are Prior - guided Bayesian Optimization ( PrBO ), and optimization process. ","This paper proposes Prior-guided Bayesian Optimization (PrBO) for optimizing expensive black-box functions. PrBO uses priors and a probabilistic model to construct a pseudo-posterior for the function evaluations. The authors show that PrBO outperforms state-of-the-art methods such as BO, priors, and BO with respect to the performance of PrBO. They also show that it can be used to identify misleading priors in the optimization process. Finally, PrBO is evaluated in a real-world hardware design application.",This paper proposes Prior-guided Bayesian Optimization (PrBO) for optimizing expensive black-box functions. PrBO combines priors and a probabilistic model for pseudo-posterior. The authors show that PrBO outperforms state-of-the-art methods and random search on a real-world hardware design application. They also show that it is more robust to misleading priors. 
945,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"Deep generative models USED-FOR real - world data. execution time FEATURE-OF computational cost. binary weights USED-FOR neural networks. binary neural networks USED-FOR generative models. binary neural networks USED-FOR generative models. computational cost EVALUATE-FOR models. techniques USED-FOR deep generative models. ResNet VAE and Flow++ models HYPONYM-OF deep generative models. Generic is they. Metric is complexity. Method are binary weight normalization, binarized generative models, binary models, and regular models. ",This paper studies the problem of training deep generative models on real-world data. The authors propose a new binary weight normalization method to reduce the computational cost of the binary weights used in neural networks trained with binary neural networks. They show that the proposed method is able to achieve better performance than binarized and regular models in terms of computational cost. They also show that they are able to improve the complexity of the models by a significant margin. ,"This paper proposes a new way to normalize the weights of deep generative models for real-world data. Specifically, the authors propose to use binary weight normalization to reduce the computational cost in the execution time of neural networks trained with binary weights. The authors show that binary neural networks can be used to train the models with lower computational cost, and they show that they can reduce the complexity of the models. They also show that the binary models can be trained with regular models. Finally, they evaluate the proposed techniques on ResNet VAE and Flow++ models."
954,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"frameworks USED-FOR DL. adversarial training USED-FOR DL. approach USED-FOR DL. adversarial training USED-FOR approach. robustness EVALUATE-FOR DL. adversarial training USED-FOR robustness. norm - bounded perturbations FEATURE-OF DL. robustness EVALUATE-FOR approach. unbounded shifts in the data distribution FEATURE-OF DL. natural, out - of - distribution shifts FEATURE-OF robustness. perturbation - based adversarial robustness CONJUNCTION model - based robust deep learning. model - based robust deep learning CONJUNCTION perturbation - based adversarial robustness. paradigm USED-FOR models of natural variation. model - based robust training algorithms USED-FOR DL. robustness EVALUATE-FOR DL. robustness EVALUATE-FOR model - based robust training algorithms. adversarial training CONJUNCTION domain adaptation techniques. domain adaptation techniques CONJUNCTION adversarial training. ERM CONJUNCTION adversarial training. adversarial training CONJUNCTION ERM. classifiers COMPARE classifiers. classifiers COMPARE classifiers. algorithms COMPARE classifiers. classifiers COMPARE algorithms. domain adaptation techniques USED-FOR classifiers. ERM USED-FOR classifiers. algorithms USED-FOR classifiers. ERM USED-FOR classifiers. domain adaptation techniques USED-FOR classifiers. adversarial training USED-FOR classifiers. adversarial training USED-FOR classifiers. algorithms COMPARE baseline methods. baseline methods COMPARE algorithms. top-1 accuracy EVALUATE-FOR baseline methods. top-1 accuracy EVALUATE-FOR algorithms. Method is deep learning ( DL ). OtherScientificTerm are natural variation, data distribution, and natural conditions. Material are images, ImageNet, ImageNet - c, and natural, out - ofdistribution data. Generic are models, and methods. ","This paper studies the problem of robustness in deep learning (DL) under natural variation. The authors propose a new approach to improve the robustness of DL under norm-bounded perturbations in the data distribution. The proposed approach is based on adversarial training and domain adaptation techniques. The main contribution of the paper is a new paradigm for training models of natural variation in DL.   The authors show that the proposed approach can improve the performance of DL in the presence of natural, out-of-distribution shifts. They also show that their approach can be combined with model-based robust deep learning and perturbation-based adversarial robustness to achieve better robustness. They demonstrate that their algorithms achieve better top-1 accuracy than baseline methods on ImageNet-c and ERM, and outperform classifiers trained with adversarial learning and the domain adaptation technique. ","This paper proposes a new framework for learning models of natural variation. The framework is based on deep learning (DL) and is motivated by the observation that the robustness of DL under unbounded shifts in the data distribution (i.e., natural, out-of-distribution shifts) is due to norm-bounded perturbations. The authors propose a new approach to learn models that are robust to these natural shifts. The proposed approach is evaluated on ImageNet, ImageNet-c, and ImageNet - c, and is compared to a number of existing DL frameworks. "
963,SP:011dab90d225550e77235cbec1615e583ae3297e,polynomial complexity FEATURE-OF exact convex optimization formulations. ReLU activations USED-FOR Convolutional Neural Networks ( CNNs ). convex analytic framework USED-FOR convex optimization problems. convex optimization problems USED-FOR twoand three - layer CNN architectures. semi - infinite duality USED-FOR convex analytic framework. ` 2 norm regularized convex program USED-FOR two - layer CNNs. ` 1 regularized convex program USED-FOR sparsity. spectral domain FEATURE-OF sparsity. ` 1 regularized convex program USED-FOR multi - layer circular CNN training problems. ReLU layer USED-FOR multi - layer circular CNN training problems. ReLU layers USED-FOR three - layer CNNs. approach USED-FOR pooling methods. convex regularizers USED-FOR implicit architectural bias. OtherScientificTerm is data dimension. ,"This paper studies the problem of exact convex optimization formulations with polynomial complexity. The authors propose a convex analytic framework based on semi-infinite duality, which is a well-studied problem in Convolutional Neural Networks (CNNs). The authors show that a `2 norm regularized convex program can be used to train two-layer CNNs with ReLU activations, and that the resulting ReLU layer can be applied to multi-layer circular CNN training problems. They also show that the sparsity in the spectral domain can be reduced by using a `1 regularized convolutional program. Finally, the authors propose an approach to pooling methods using convex regularizers to reduce the implicit architectural bias.",This paper proposes a convex analytic framework for solving convex optimization problems with ReLU activations in Convolutional Neural Networks (CNNs). The main idea is to use semi-infinite duality to reduce the polynomial complexity of exact convex optimal formulations. The authors show that the `2 norm regularized convex program for two-layer CNNs and the ReLU layer for multi-layer circular CNN training problems can reduce the sparsity in the spectral domain. The paper also shows that the proposed approach can be applied to pooling methods and that convex regularizers can be used to reduce implicit architectural bias. 
972,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"rich sensory modalities USED-FOR Robotic manipulation tasks. Human - robot interaction USED-FOR teaching robots. probabilistic generative model USED-FOR optimisation problem. high - capacity neural network USED-FOR model. latent variables USED-FOR model. latent variables CONJUNCTION high - level notions. high - level notions CONJUNCTION latent variables. table - top robot manipulation tasks EVALUATE-FOR approach. PR2 robot USED-FOR table - top robot manipulation tasks. visual information CONJUNCTION arm joint positions. arm joint positions CONJUNCTION visual information. arm joint positions CONJUNCTION arm joint efforts. arm joint efforts CONJUNCTION arm joint positions. robot USED-FOR visual information. robot USED-FOR arm joint positions. arm joint efforts FEATURE-OF robot. OtherScientificTerm are soft sponge, restricted vocabulary, and sponge. Material is rich data streams. Generic are alignment, and tasks. ","This paper proposes a probabilistic generative model to solve the optimisation problem in the context of Robotic manipulation tasks with rich sensory modalities. The model is trained using a high-capacity neural network, where the latent variables and high-level notions are learned by a model trained with a soft sponge. Human-robotic interaction is used to train the teaching robots. The proposed approach is evaluated on table-top robot manipulation tasks using a PR2 robot and visual information and arm joint positions. The results show that the proposed approach can achieve state-of-the-art performance on most of the tasks.","This paper proposes a probabilistic generative model to solve the optimisation problem of Robotic manipulation tasks with rich sensory modalities. The model is based on a high-capacity neural network. The authors propose a soft sponge, where the soft sponge is a restricted vocabulary, and the authors propose to use human-robotic interaction to train teaching robots. The proposed approach is evaluated on table-top robot manipulation tasks using a PR2 robot. The robot is able to capture visual information, arm joint positions, and arm joint efforts.  The authors show that the proposed model can capture both latent variables and high-level notions. They also show that it can capture the alignment between different tasks. "
981,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,low - resource scenarios FEATURE-OF overfitting. tasks EVALUATE-FOR large - scale pretrained language models. general - purpose feature extractors USED-FOR models. Variational Information Bottleneck ( VIB ) USED-FOR irrelevant features. Variational Information Bottleneck ( VIB ) USED-FOR fine - tuning. method USED-FOR overfitting. fine - tuning USED-FOR low - resource target tasks. Variational Information Bottleneck ( VIB ) USED-FOR low - resource target tasks. VIB model USED-FOR sentence representations. natural language inference datasets USED-FOR sentence representations. generalization EVALUATE-FOR VIB model. low - resource datasets EVALUATE-FOR method. method USED-FOR transfer learning. low - resource scenarios FEATURE-OF transfer learning. low - resource scenarios EVALUATE-FOR method. generalization EVALUATE-FOR it. Generic is they. OtherScientificTerm is features. Material is out - of - domain datasets. ,"This paper studies the problem of overfitting in low-resource scenarios in large-scale pretrained language models. The authors propose a new method called Variational Information Bottleneck (VIB) for fine-tuning the models by using general-purpose feature extractors. The VIB model learns sentence representations from natural language inference datasets, which are then used to train the model. The proposed method is shown to improve the transfer learning performance in both high-resource and low-reward scenarios.","This paper proposes a novel method to reduce overfitting in low-resource scenarios in the context of large-scale pretrained language models. The authors propose a Variational Information Bottleneck (VIB) for fine-tuning the model to avoid irrelevant features. The VIB model learns sentence representations from natural language inference datasets and then applies them to out-of-domain datasets. The proposed method is evaluated on a variety of transfer learning tasks, including a number of high-resource target tasks, and it is shown to improve generalization. "
990,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,Natural images HYPONYM-OF projections of 3D objects. 2D image plane FEATURE-OF projections of 3D objects. they USED-FOR 3D object structures. 2D generative models USED-FOR natural image manifold. GANs HYPONYM-OF 2D generative models. knowledge USED-FOR 3D shapes of objects. 2D GAN USED-FOR 3D geometric cues. RGB images USED-FOR 2D GAN. pre - trained GAN USED-FOR 3D shape. rich 3D knowledge PART-OF pre - trained GAN. unsupervised manner USED-FOR 3D shape. iterative strategy USED-FOR diverse viewpoint and lighting variations. diverse viewpoint and lighting variations FEATURE-OF GAN image manifold. iterative strategy USED-FOR framework. 2D keypoint CONJUNCTION 3D annotations. 3D annotations CONJUNCTION 2D keypoint. cars CONJUNCTION buildings. buildings CONJUNCTION cars. it USED-FOR 3D shapes. human faces CONJUNCTION cars. cars CONJUNCTION human faces. precision FEATURE-OF 3D shapes. precision EVALUATE-FOR it. relighting CONJUNCTION object rotation. object rotation CONJUNCTION relighting. 3D shapes USED-FOR image editing. object rotation HYPONYM-OF image editing. relighting HYPONYM-OF image editing. 3D shape reconstruction CONJUNCTION face rotation. face rotation CONJUNCTION 3D shape reconstruction. approach COMPARE methods. methods COMPARE approach. methods USED-FOR face rotation. approach USED-FOR face rotation. methods USED-FOR 3D shape reconstruction. approach USED-FOR 3D shape reconstruction. OtherScientificTerm is object shapes. ,"This paper proposes a method to learn 3D shapes of 3D objects from natural images on the 2D image plane. The 3D shape is learned in an unsupervised manner using a pre-trained GAN with rich 3D knowledge. The framework is based on an iterative strategy to learn diverse viewpoint and lighting variations in the GAN image manifold. The authors show that the proposed approach can achieve state-of-the-art performance on object shapes in terms of precision. They also show that it can be used for image editing, 3D 3Dshape reconstruction, and face rotation.","This paper proposes a generative model for learning 3D shapes from natural images of 3D objects in the 2D image plane. The authors propose to use 2D generative models for the natural image manifold, i.e., GANs, to learn 3D object structures. The framework is based on an iterative strategy to learn diverse viewpoint and lighting variations of the GAN image manifold. The 3D shape is learned in an unsupervised manner in a pre-trained GAN with rich 3D knowledge, where the 3D geometric cues are generated by a 2D GAN trained on RGB images.  The authors show that the proposed approach outperforms existing methods for 3Dshape reconstruction and face rotation. The paper also shows that it can be used to learn more complex shapes, such as human faces, cars, buildings, 3D annotations, and object rotation. "
999,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"recognition methods USED-FOR imbalanced classification. tail accuracies CONJUNCTION head accuracies. head accuracies CONJUNCTION tail accuracies. class re - balancing / re - weighting USED-FOR recognition methods. model bias CONJUNCTION variance analysis. variance analysis CONJUNCTION model bias. RoutIng Diverse Experts ( RIDE ) HYPONYM-OF long - tailed classifier. It USED-FOR model variance. It USED-FOR model bias. computational cost EVALUATE-FOR dynamic expert routing module. distribution - aware diversity loss USED-FOR model bias. CIFAR100 - LT CONJUNCTION ImageNet - LT. ImageNet - LT CONJUNCTION CIFAR100 - LT. ImageNet - LT CONJUNCTION iNaturalist 2018 benchmarks. iNaturalist 2018 benchmarks CONJUNCTION ImageNet - LT. RIDE COMPARE state - of - the - art. state - of - the - art COMPARE RIDE. ImageNet - LT EVALUATE-FOR RIDE. iNaturalist 2018 benchmarks EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR state - of - the - art. backbone networks CONJUNCTION long - tailed algorithms. long - tailed algorithms CONJUNCTION backbone networks. universal framework USED-FOR backbone networks. universal framework USED-FOR long - tailed algorithms. long - tailed algorithms CONJUNCTION training mechanisms. training mechanisms CONJUNCTION long - tailed algorithms. It HYPONYM-OF universal framework. It USED-FOR backbone networks. Material are Natural data, and tail data. OtherScientificTerm are dynamic view of the training data, hard negatives, and tail. Method is long - tail classifiers. Metric is head - tail model bias gap. ","This paper studies the problem of imbalanced classification in the context of class re-balancing/re-weighting in recognition methods. The authors propose a new long-tail classifier called RoutIng Diverse Experts (RIDE) which is based on the distribution-aware diversity loss. It is a universal framework for training backbone networks and long-tailed algorithms. It minimizes the model bias and variance analysis by using a dynamic view of the training data. The computational cost of the dynamic expert routing module is also reduced. RIDE outperforms the state-of-the-art on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks.","This paper proposes a novel long-tail classifier called RoutIng Diverse Experts (RIDE) which is a long-tailed classifier based on class re-balancing/re-weighting for imbalanced classification. The key idea is to use a dynamic view of the training data, where the head of the classifier is trained with hard negatives, and the tail data are trained with soft negatives. The authors show that the head-tail model bias gap is larger than the tail-tail bias gap, and show that RIDE is able to reduce the computational cost of the dynamic expert routing module. It is also shown that the model bias and variance analysis can be reduced by using distribution-aware diversity loss. It can also be used to reduce model variance. The proposed universal framework can be applied to backbone networks and long-tailed algorithms. The paper is well-written and easy to follow. The experiments on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks show the effectiveness of RIDE. "
1008,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"pruning criteria USED-FOR redundant filters. Channel pruning USED-FOR compressing convolutional neural networks ( CNNs ). pruning criteria USED-FOR filters ’ Importance Score. layer - wise pruning CONJUNCTION global pruning. global pruning CONJUNCTION layer - wise pruning. layer - wise pruning USED-FOR pruning criteria. global pruning USED-FOR pruning criteria. Gaussian - alike distribution FEATURE-OF convolutional filters. Material is convolutional neural networks ( CNNs ). Generic is criteria. OtherScientificTerm are pruned structures, and Convolutional Weight Distribution Assumption. Method is ` 1 and ` 2 pruning. ","This paper proposes a new pruning criteria for redundant filters in convolutional neural networks (CNNs). Channel pruning is a popular technique for compressing convolutionally neural networks. The proposed criteria are based on the filters’ Importance Score, which is a function of the number of pruned structures. The authors show that layer-wise pruning and global pruning can be used to improve the performance of the pruned criteria. Convolutional Weight Distribution Assumption (CWA) is also proposed. ","This paper proposes Channel pruning for compressing convolutional neural networks (CNNs). The pruning criteria are based on the filters’ Importance Score, which is a measure of the importance of the pruned structures. The authors propose two criteria: layer-wise pruning and global pruning. The first one is based on Convolutional Weight Distribution Assumption, and the second one relies on `1 and `2 pruning, where the pruning structures are pruned separately. "
1017,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"code completion CONJUNCTION code summarization. code summarization CONJUNCTION code completion. code search CONJUNCTION code completion. code completion CONJUNCTION code search. Pre - trained models USED-FOR programming language. Pre - trained models USED-FOR code - related tasks. code search HYPONYM-OF code - related tasks. code summarization HYPONYM-OF code - related tasks. code completion HYPONYM-OF code - related tasks. code snippet USED-FOR pre - trained models. pre - trained model USED-FOR programming language. GraphCodeBERT HYPONYM-OF pre - trained model. inherent structure of code USED-FOR pre - trained model. data flow USED-FOR pre - training stage. data flow USED-FOR semantic - level structure of code. abstract syntax tree ( AST ) HYPONYM-OF syntactic - level structure of code. Transformer USED-FOR GraphCodeBERT. graph - guided masked attention function USED-FOR code structure. graph - guided masked attention function USED-FOR model. code translation CONJUNCTION code refinement. code refinement CONJUNCTION code translation. clone detection CONJUNCTION code translation. code translation CONJUNCTION clone detection. code search CONJUNCTION clone detection. clone detection CONJUNCTION code search. tasks EVALUATE-FOR model. code refinement HYPONYM-OF tasks. code search HYPONYM-OF tasks. clone detection HYPONYM-OF tasks. code translation HYPONYM-OF tasks. pre - training tasks USED-FOR GraphCodeBERT. code structure CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION code structure. code structure USED-FOR GraphCodeBERT. structure - level attentions COMPARE token - level attentions. token - level attentions COMPARE structure - level attentions. model COMPARE token - level attentions. token - level attentions COMPARE model. structure - level attentions USED-FOR model. OtherScientificTerm are code semantics, semantic - level structure, hierarchy of AST, and code structure edges. Task are code understanding process, masked language modeling, and structure - aware pre - training tasks. Generic is downstream tasks. ","This paper proposes a pre-trained model for the programming language, GraphCodeBERT, that learns the code structure of the code by using a graph-guided masked attention function. The model is based on a Transformer, and is trained using a set of pre-training tasks, including code search, code completion, code summarization, and code search. The authors show that the model is able to perform well on these tasks.   ","This paper proposes a pre-trained model for learning a programming language from a code snippet. Pre-trained models are typically used to learn a code understanding process, where the code semantics of the code is encoded in an abstract syntax tree (AST) and the syntactic-level structure of code is represented as a hierarchy of AST, and the code structure edges are represented by a Transformer. The authors propose to use pre-training models for code-related tasks such as code completion, code summarization, code search, code refinement, code translation, and clone detection. The proposed model is based on a graph-guided masked attention function that maps code structure to code structure. The model is evaluated on a set of tasks where it is shown to outperform the model in terms of structure-level attentions compared to the model on the other tasks. "
1026,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"drug discovery CONJUNCTION material design. material design CONJUNCTION drug discovery. drug discovery HYPONYM-OF scientific fields. material design HYPONYM-OF scientific fields. distribution FEATURE-OF labeled result data. it USED-FOR regression model. skewed data USED-FOR it. approach USED-FOR regression models. accuracy EVALUATE-FOR regression models. accuracy EVALUATE-FOR approach. skewed dataset USED-FOR regression models. forcing algorithm USED-FOR regression. domain knowledge USED-FOR true distribution. neural networks USED-FOR regression model. pLogP CONJUNCTION Diamond. Diamond CONJUNCTION pLogP. pLogP HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. Diamond HYPONYM-OF real - world datasets. datasets EVALUATE-FOR approach. approach COMPARE regression models. regression models COMPARE approach. root mean squared error EVALUATE-FOR regression. root mean squared error EVALUATE-FOR regression models. datasets EVALUATE-FOR regression models. adjustment of the distribution USED-FOR regression models. regression EVALUATE-FOR approach. root mean squared error EVALUATE-FOR approach. Generic is method. OtherScientificTerm are regression outputs, and estimated ‘ true ’ distribution. Material is unlabeled data. Method is adversarial network. ","This paper proposes a new method to improve the performance of regression models on unlabeled data. The proposed method is based on the observation that the true distribution of labeled result data is not the same as the distribution of the regression outputs. The authors propose a new forcing algorithm for regression based on domain knowledge, and then use it to train a regression model on the skewed data. They then use neural networks to train the regression model, and use the estimated ‘true’ distribution to adjust the parameters of the adversarial network. They demonstrate the effectiveness of the proposed approach on two datasets, pLogP and Diamond, and show that it improves the accuracy on regression models with the root mean squared error.","This paper proposes a new adversarial network for learning the true distribution of unlabeled data. The proposed method is based on the observation that the distribution of labeled result data is not always the same as the one of the regression outputs. The authors propose to use domain knowledge to learn a true distribution, and then use it to train a regression model on the skewed data. They propose a forcing algorithm for training the regression model, and show that it can be applied to any regression model trained with neural networks. They show that the proposed approach improves the accuracy of regression models with respect to the root mean squared error of the estimated ‘true’ distribution. They evaluate their approach on two real-world datasets, pLogP and Diamond, showing that their approach outperforms regression models on both datasets."
1035,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"Compositional generalization HYPONYM-OF algebraic capacity. known components USED-FOR algebraic capacity. human intelligence USED-FOR out - of - distribution generalization. It PART-OF human intelligence. It USED-FOR out - of - distribution generalization. model USED-FOR representations. representations USED-FOR inference. regularized hidden representations USED-FOR auxiliary reconstruction network. approach COMPARE baselines. baselines COMPARE approach. accuracy EVALUATE-FOR approach. compositional representations USED-FOR it. compositional generalization CONJUNCTION artificial intelligence research. artificial intelligence research CONJUNCTION compositional generalization. Method are neural networks, and extraction network. OtherScientificTerm are extraction ability, divergence of distributions, and compositionality. Task is transferability of compositionality. ","This paper studies the transferability of compositionality in neural networks. The authors consider the problem of out-of-distribution generalization in the presence of known components of the algebraic capacity, i.e., the divergence of distributions. It is an important problem in human intelligence, and the authors propose a model that learns representations that can be used for inference. The main idea is to use regularized hidden representations for the auxiliary reconstruction network, and then use the extracted representations to perform inference on the extraction network. The proposed approach achieves better accuracy than baselines, and it can be applied to compositional generalization and artificial intelligence research.","This paper proposes a novel approach to improve the transferability of compositionality in neural networks. The authors propose to use known components of algebraic capacity, i.e., the divergence of distributions, to improve out-of-distribution generalization in human intelligence. This is achieved by learning a model that predicts the representations of these known components, which are then used as an auxiliary reconstruction network. The extraction network is trained using a regularized hidden representations of the known components. These representations are used for inference, and the authors show that the proposed approach improves the accuracy over baselines on compositional generalization and artificial intelligence research. "
1044,SP:ffab573a977c819e86601de74690c29a39c264cd,"Poisoning attacks USED-FOR Reinforcement Learning ( RL ) systems. RL algorithm USED-FOR Poisoning attacks. poisoning methods USED-FOR supervised learning. supervised learning USED-FOR RL. poisoning methods USED-FOR RL. generic poisoning framework USED-FOR online RL. heterogeneous poisoning models USED-FOR RL. heterogeneous poisoning models USED-FOR generic poisoning framework. poisoning method USED-FOR policy - based RL agents. strategic poisoning algorithm USED-FOR on - policy deep RL agents. stability radius FEATURE-OF RL. stability radius HYPONYM-OF metric. stability radius USED-FOR VA2C - P. metric USED-FOR VA2C - P. Task are learning, and poisoning RL. OtherScientificTerm are Markov Decision Process ( MDP ), MDP, and limited attacking budget. Method are RL algorithms, and poisoning algorithm. Material is deep RL agents. ","Poisoning attacks against Reinforcement Learning (RL) systems have been a popular topic in recent years. Poisoning methods have been widely used in supervised learning for RL. However, in online RL, a generic poisoning framework has been proposed for RL with heterogeneous poisoning models. The proposed poisoning method can be applied to policy-based RL agents with a strategic poisoning algorithm to attack on-policy deep RL agents. The authors also propose a new metric called stability radius for RL called VA2C-P.   The authors provide a theoretical analysis of poisoning RL. They show that poisoning algorithms can be used in RL algorithms with a limited attacking budget. They also show that the Markov Decision Process (MDP) can be a good poisoner. ",Poisoning attacks against Reinforcement Learning (RL) systems is an important problem in reinforcement learning. Poisoning attacks are used in supervised learning in order to improve the performance of RL algorithms. The authors propose a generic poisoning framework for online RL with heterogeneous poisoning models for RL. The proposed poisoning algorithm is based on the Markov Decision Process (MDP). The authors show that the MDP is robust to poisoning attacks. They also show that poisoning RL agents can be trained with a strategic poisoning algorithm for on-policy deep RL agents. They show that VA2C-P with stability radius is a good metric for RL with a limited attacking budget.
1053,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,Checkpointing USED-FOR deep learning models. restricted memory budgets FEATURE-OF deep learning models. static computation graphs USED-FOR checkpointing techniques. greedy online algorithm USED-FOR checkpointing. Dynamic Tensor Rematerialization ( DTR ) HYPONYM-OF greedy online algorithm. Dynamic Tensor Rematerialization ( DTR ) USED-FOR online algorithm. DTR USED-FOR anN -layer linear feedforward network. O(N ) tensor operations USED-FOR DTR. Ω ( √ N ) memory budget FEATURE-OF anN -layer linear feedforward network. DTR COMPARE optimal static checkpointing. optimal static checkpointing COMPARE DTR. tensor allocations CONJUNCTION operator calls. operator calls CONJUNCTION tensor allocations. DTR prototype PART-OF PyTorch. lightweight metadata PART-OF tensors. OtherScientificTerm is eviction policy. Method is dynamic models. ,"This paper studies the problem of checkpointing in deep learning models with restricted memory budgets. The authors propose a greedy online algorithm called Dynamic Tensor Rematerialization (DTR) to perform checkpointing on static computation graphs. DTR can be applied to an anN-layer linear feedforward network with a Ω (√ N) memory budget. The DTR prototype in PyTorch is based on O(N) tensor operations. Theoretically, the authors show that DTR performs better than optimal static checkpointing. ","This paper proposes a greedy online algorithm for checkpointing in deep learning models with restricted memory budgets. Dynamic Tensor Rematerialization (DTR) is an online algorithm based on O(N) tensor operations. DTR is applied to an anN-layer linear feedforward network with a Ω (√ N) memory budget. The authors show that DTR outperforms optimal static checkpointing with lightweight metadata in terms of tensor allocations, operator calls, and eviction policy. They also show that the DTR prototype in PyTorch outperforms dynamic models."
1062,SP:20efc610911443724b56f57f857060d0e0302243,"manually annotated evaluation sets USED-FOR task. method USED-FOR hallucination detection. synthetic data USED-FOR pretrained language models. pretrained language models USED-FOR method. machine translation CONJUNCTION abstract text summarization. abstract text summarization CONJUNCTION machine translation. machine translation EVALUATE-FOR approach. abstract text summarization EVALUATE-FOR approach. average F1 EVALUATE-FOR benchmark datasets. average F1 EVALUATE-FOR approach. token - level hallucination labels USED-FOR fine - grained loss. fine - grained loss PART-OF low - resource machine translation. Method is Neural sequence models. Generic are they, model, and baseline methods. OtherScientificTerm is automatically inserted hallucinations. Material is annotated data. ",This paper proposes a novel method for hallucination detection based on neural sequence models. The proposed method is based on pretrained language models trained on synthetic data. The authors propose a new task based on manually annotated evaluation sets for the task. The main idea is to use token-level hallucination labels as a fine-grained loss in low-resource machine translation and abstract text summarization. The approach is evaluated on two benchmark datasets with average F1. The results show that the proposed approach performs better than baseline methods.,"This paper proposes a novel method for hallucination detection based on manually annotated evaluation sets for the task. The method is based on pretrained language models on synthetic data. Neural sequence models are used to train the model, and they are trained on the annotated data. The proposed approach is evaluated on three benchmark datasets, including machine translation, abstract text summarization, and average F1. The authors propose a fine-grained loss for low-resource machine translation based on token-level hallucination labels. They show that the proposed approach outperforms baseline methods on automatically inserted hallucinations."
1071,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"Conditional Generative Adversarial Networks ( cGAN ) USED-FOR images. idea USED-FOR architecture. NAS USED-FOR architecture. NAS USED-FOR idea. reduction of training data USED-FOR class generator. latter USED-FOR class - specific information. regular and class - modulated convolutions PART-OF search space. weight - sharing pipeline CONJUNCTION mixed - architecture optimization. mixed - architecture optimization CONJUNCTION weight - sharing pipeline. weight - sharing pipeline USED-FOR search algorithm. Markov decision process PART-OF search algorithm. Markov decision process USED-FOR sampling policy. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. FID scores FEATURE-OF image generation quality. OtherScientificTerm are class - level distributions, and generating architecture. Metric is search cost. Method are moving average, and cGAN models. ","This paper proposes Conditional Generative Adversarial Networks (cGAN) for generating images from images. The main idea is to use NAS to learn the architecture of the cGAN by reducing the number of class-level distributions in the search space. The reduction of training data allows the class generator to be trained with a smaller number of classes than the moving average. The search space is divided into regular and class-modulated convolutions, and the latter is used to capture the class-specific information. The proposed search algorithm is based on a weight-sharing pipeline and mixed-architecture optimization. The Markov decision process is used for the sampling policy. The authors show that the proposed approach improves the FID scores of image generation quality on CIFAR10 CONJUNCTION. ","This paper proposes Conditional Generative Adversarial Networks (cGAN) to generate images with different class-level distributions. The idea is to use NAS to learn the architecture. The main idea of the architecture is to learn a class generator from the reduction of training data. The authors propose to use regular and class-modulated convolutions in the search space. The latter is used to capture class-specific information, while the former is used for the search cost. The search algorithm is based on a weight-sharing pipeline and mixed-architecture optimization. The Markov decision process is used as the sampling policy. The proposed approach is evaluated on CIFAR10, CIFar100, and CifAR100 with FID scores. The results show that the proposed generating architecture is more efficient than existing cGAN models."
1080,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,estimation of causal effects USED-FOR Decision - making. observational data USED-FOR estimation of causal effects. regularization framework USED-FOR unconfoundedness. orthogonality constraint USED-FOR unconfoundedness. asymptotically normal estimator USED-FOR average causal effect. estimators COMPARE asymptotic variance. asymptotic variance COMPARE estimators. regularization framework USED-FOR deep orthogonal networks. deep orthogonal networks USED-FOR unconfounded treatments ( DONUT ). DONUT COMPARE state - of - the - art. state - of - the - art COMPARE DONUT. benchmark datasets EVALUATE-FOR DONUT. benchmark datasets USED-FOR causal inference. benchmark datasets EVALUATE-FOR state - of - the - art. OtherScientificTerm is treatment assignment. ,"This paper studies the problem of causal inference from observational data in Decision-making. The authors propose an asymptotically normal estimator for the average causal effect, which is based on a regularization framework for unconfoundedness under the orthogonality constraint. They show that the estimators are better than existing estimators when the treatment assignment is not known. They also show that deep orthogonal networks can be used to perform unconfounding treatments (DONUT) on several benchmark datasets for causal inference. ","The paper proposes a novel regularization framework for unconfoundedness of deep orthogonal networks to unconfound treatments (DONUT) in Decision-making. The main idea is to use observational data for the estimation of causal effects, and then use an orthogonality constraint to ensure that the average causal effect is close to the asymptotically normal estimator. The authors show that their estimators are more robust to asymptonality variance than other estimators. The paper also shows that DONUT outperforms state-of-the-art on several benchmark datasets for causal inference. "
1089,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"style transfer CONJUNCTION multitask learning. multitask learning CONJUNCTION style transfer. multitask learning HYPONYM-OF deep learning techniques. style transfer HYPONYM-OF deep learning techniques. affine transformations of features USED-FOR deep learning techniques. affine parameters USED-FOR features. parameters PART-OF BatchNorm. randomly chosen parameters PART-OF network. affine parameters USED-FOR deep learning. shifting and rescaling random features USED-FOR neural networks. Method is affine transform. OtherScientificTerm are random initializations, and random features. Metric is accuracy. ","This paper studies the problem of style transfer and multitask learning in deep learning techniques such as style transfer. The authors propose a new affine transform, called BatchNorm, which is based on the affine transformations of features. The key idea is to use randomly chosen parameters in the network to learn the features, and then use these affine parameters to improve the performance of deep learning by shifting and rescaling random features in neural networks. The paper shows that the proposed method is able to achieve better performance than random initializations in terms of accuracy.","This paper proposes a novel affine transform for deep learning techniques such as style transfer and multitask learning. The affine transformations of features can be used to improve the accuracy of deep learning methods. The key idea is to add random initializations to the training set of neural networks by shifting and rescaling random features. The network is trained with randomly chosen parameters in the BatchNorm, and the affine parameters of the features are used to train the features. "
1098,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"model USED-FOR test - time adaptation. model USED-FOR confidence. method USED-FOR normalization statistics. method USED-FOR channel - wise affine transformations. Tent USED-FOR source - free domain adaptation. corrupted ImageNet CONJUNCTION CIFAR-10/100. CIFAR-10/100 CONJUNCTION corrupted ImageNet. Tent USED-FOR semantic segmentation. GTA CONJUNCTION Cityscapes. Cityscapes CONJUNCTION GTA. Tent USED-FOR image classification. Tent USED-FOR digit recognition. source - free domain adaptation USED-FOR digit recognition. generalization error EVALUATE-FOR image classification. Tent USED-FOR Tent. CIFAR-10/100 USED-FOR image classification. corrupted ImageNet USED-FOR image classification. VisDA - C benchmark EVALUATE-FOR Tent. generalization error EVALUATE-FOR Tent. Metric is entropy. Material are SVHN, and MNIST / MNIST - M / USPS. Method is test - time optimization. ","This paper proposes a new model for test-time adaptation based on SVHN. Tent is a method for normalization statistics for channel-wise affine transformations. The authors show that Tent improves the generalization error for image classification on corrupted ImageNet and CIFAR-10/100, and for source-free domain adaptation for digit recognition and semantic segmentation. Tent also improves the VisDA-C benchmark on MNIST/MNIST-M/USPS. ","This paper proposes a model for test-time adaptation to improve the confidence of the model. The method is based on channel-wise affine transformations. The authors propose a method for normalization statistics. They show that the entropy of SVHN and MNIST/MNIST-M/USPS can be reduced to zero. Tent is evaluated on corrupted ImageNet, CIFAR-10/100, and Cityscapes. Tent improves the generalization error of image classification on the VisDA-C benchmark and for digit recognition on the source-free domain adaptation. "
1107,SP:ed544ee661580592063aa17aee8924cc99919130,Uncertainty quantification USED-FOR machine learning systems. recurrent timesteps FEATURE-OF stochastic discrete state transitions. stochastic discrete state transitions USED-FOR recurrent neural networks ( RNNs ). uncertainty quantification COMPARE method. method COMPARE uncertainty quantification. method USED-FOR deterministic and probabilistic automata. well - calibrated models USED-FOR real - world classification tasks. method USED-FOR well - calibrated models. explorationexploitation trade - off FEATURE-OF reinforcement learning. method USED-FOR out - of - distribution detection. method USED-FOR explorationexploitation trade - off. Generic is model. OtherScientificTerm is recurrent state transition distribution. ,This paper studies the problem of uncertainty quantification in machine learning systems with recurrent timesteps. The authors propose a new method for estimating the uncertainty of a recurrent neural networks (RNNs) with stochastic discrete state transitions. The proposed method can be applied to deterministic and probabilistic automata and well-calibrated models for real-world classification tasks. The method can also be used for explorationexploitation trade-off in reinforcement learning and out-of-distribution detection.,"This paper proposes a new uncertainty quantification for machine learning systems. The key idea is to use stochastic discrete state transitions for recurrent neural networks (RNNs) with recurrent timesteps. The proposed method is evaluated on deterministic and probabilistic automata. The method is shown to outperform well-calibrated models on real-world classification tasks, and the method is also applied to explorationexploitation trade-off in reinforcement learning. "
1116,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"Protecting data privacy PART-OF deep learning ( DL ). stochastic differential equation principled residual perturbation USED-FOR privacy - preserving DL. Gaussian noise USED-FOR residual mapping of ResNets. residual perturbation USED-FOR differential privacy ( DP ). generalization gap FEATURE-OF DL. residual perturbation USED-FOR generalization gap. residual perturbation COMPARE DP stochastic gradient descent ( DPSGD ). DP stochastic gradient descent ( DPSGD ) COMPARE residual perturbation. DP stochastic gradient descent ( DPSGD ) USED-FOR membership privacy protection. residual perturbation USED-FOR DL models ’ utility. residual perturbation USED-FOR membership privacy protection. ResNet8 USED-FOR IDC dataset classification. residual perturbation USED-FOR perfect membership privacy. residual perturbation COMPARE DPSGD. DPSGD COMPARE residual perturbation. accuracy EVALUATE-FOR DPSGD. accuracy EVALUATE-FOR residual perturbation. Task is data privacy. OtherScientificTerm are utility degradation, and ResNets. ","This paper studies the problem of protecting data privacy in deep learning (DL). The authors propose a stochastic differential equation principled residual perturbation for privacy-preserving DL, which is based on Gaussian noise in the residual mapping of ResNets. The authors show that the generalization gap of DL with residual perturbed data is larger than that of DL without residual perturbing data. They also show that residual perturtation improves the accuracy of DL models’ utility by reducing the utility degradation. Finally, the authors provide a theoretical analysis of the relationship between the performance of residual perturgation and DPSGD. ",This paper proposes a new method for protecting data privacy in deep learning (DL). The authors propose a stochastic differential equation principled residual perturbation for privacy-preserving DL. The authors show that the generalization gap of a DL with the proposed method is the same as the one of a DP with the same method. The paper also shows that the residual mapping of ResNets with Gaussian noise is equivalent to the utility degradation of the original DL.  The authors also provide a theoretical analysis of the proposed methods.  
1125,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"limited computational resources USED-FOR inference. natural language processing EVALUATE-FOR transformers. computational efficiency USED-FOR inference. model USED-FOR inference scenario. inefficiency CONJUNCTION redundancy. redundancy CONJUNCTION inefficiency. PoWER - BERT USED-FOR inefficiency. PoWER - BERT USED-FOR redundancy. it USED-FOR inference scenarios. extension USED-FOR large - scale transformer. Length - Adaptive Transformer HYPONYM-OF large - scale transformer. LengthDrop HYPONYM-OF structural variant of dropout. LengthDrop USED-FOR transformer. multi - objective evolutionary search USED-FOR length configuration. accuracy EVALUATE-FOR length configuration. PoWER - BERT USED-FOR token - level classification. sequence - level classification USED-FOR token - level classification. PoWER - BERT USED-FOR sequence - level classification. span - based question - answering HYPONYM-OF token - level classification. SQuAD 1.1 CONJUNCTION MNLI - m. MNLI - m CONJUNCTION SQuAD 1.1. MNLI - m CONJUNCTION SST-2. SST-2 CONJUNCTION MNLI - m. accuracyefficiency trade - off EVALUATE-FOR setups. accuracyefficiency trade - off EVALUATE-FOR approach. SST-2 HYPONYM-OF setups. SQuAD 1.1 HYPONYM-OF setups. MNLI - m HYPONYM-OF setups. Generic are they, and approaches. Metric is computational complexity. OtherScientificTerm are computational budget, Drop - and - Restore, and word - vectors. ","This paper studies the computational efficiency of transformers in natural language processing. The authors consider the problem of using limited computational resources for inference with limited computational budget. In this model, they consider the inference scenario where the number of parameters is limited and the computational complexity of the model is high. They propose a structural variant of dropout, called LengthDrop, which is an extension to the standard Drop-and-Restore. The length configuration of the transformer is learned by multi-objective evolutionary search. They show that it can be used in two different inference scenarios: sequence-level classification and span-based question-answer. They also show that PoWER-BERT is able to reduce the inefficiency and the redundancy inefficiency. The proposed approach is evaluated on three different setups: SQuAD 1.1, SST-2, and MNLI-m.","This paper studies the computational efficiency of transformers in natural language processing. The authors consider the problem of limited computational resources for inference, where the computational budget is limited. They propose two approaches to reduce the computational complexity. The first approach, Drop-and-Restore, is a structural variant of dropout. The second approach, Length-Adaptive Transformer, is an extension to the standard large-scale transformer. The main idea is to use multi-objective evolutionary search to find the length configuration that maximizes the accuracy of the final length configuration. The proposed approach is evaluated on three different setups: SQuAD 1.1, SST-2, and MNLI-m. PoWER-BERT is used for sequence-level classification, and span-based question-answer, and inefficiency and redundancy are used for the inference scenario. "
1134,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"1 - WL test USED-FOR distinguishing graph structures. expressiveness EVALUATE-FOR graph neural networks ( GNNs ). neighborhood aggregation GNNs COMPARE 1 - WL test. 1 - WL test COMPARE neighborhood aggregation GNNs. neighborhood aggregation GNNs USED-FOR distinguishing graph structures. aggregators PART-OF GNNs. aggregators USED-FOR expressiveness. aggregation coefficient matrix USED-FOR aggregators. aggregation coefficient matrix USED-FOR injective aggregators. aggregators CONJUNCTION injective aggregators. injective aggregators CONJUNCTION aggregators. aggregation coefficient matrix USED-FOR aggregation. It USED-FOR rank of hidden features. nonlinear units USED-FOR aggregation - based GNNs. ExpandingConv CONJUNCTION CombConv. CombConv CONJUNCTION ExpandingConv. CombConv HYPONYM-OF GNN layers. ExpandingConv HYPONYM-OF GNN layers. models USED-FOR large and densely connected graphs. OtherScientificTerm are graph structures, weak distinguishing strength, and low - rank transformations. Method is WL test. Generic is it. ","This paper studies the expressiveness of graph neural networks (GNNs) under the 1-WL test for distinguishing graph structures. The paper shows that neighborhood aggregation GNNs are more expressive than the standard 1-WAIL test, and that the aggregation coefficient matrix of aggregators and injective aggregators can be used to improve expressiveness. It also shows that the rank of hidden features can be computed using nonlinear units. The authors also show that the WL test can be applied to GNN layers such as ExpandingConv and CombConv, and show that it can improve the performance of GNN models on large and densely connected graphs.","The paper proposes a new 1-WL test to measure the expressiveness of graph neural networks (GNNs). The paper shows that neighborhood aggregation GNNs outperform the standard 1- WL test for distinguishing graph structures. The paper also shows that the aggregation coefficient matrix for aggregators and injective aggregators can be used to measure expressiveness. It is also shown that the rank of hidden features can be computed using nonlinear units. The authors also show that models trained on large and densely connected graphs can be trained on models with weak distinguishing strength. Finally, it is shown that low-rank transformations can be applied to GNN layers such as ExpandingConv and CombConv."
1143,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"robustness EVALUATE-FOR generative models. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR generative models. method USED-FOR quantifying disentanglement. conditional submanifolds PART-OF representation. topological similarity FEATURE-OF conditional submanifolds. generative model USED-FOR method. unsupervised and supervised variants PART-OF method. method COMPARE models. models COMPARE method. Task are Learning disentangled representations, and measuring disentanglement. ",This paper studies the problem of learning disentangled representations. The authors propose a new method for quantifying disentanglement. The proposed method is based on a generative model. The unsupervised and supervised variants of the proposed method are evaluated on a variety of datasets. The method is shown to outperform existing models in terms of generalization and robustness. ,"This paper proposes a method for quantifying disentanglement in generative models. Learning disentangled representations is an important problem in the context of generalization and robustness. The proposed method is based on unsupervised and supervised variants of the generative model, where conditional submanifolds of the representation are computed based on topological similarity. The authors show that the proposed method outperforms existing models in terms of quantification of the loss. "
1152,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"unauthorized exploitation of personal data USED-FOR commercial models. sample - wise and class - wise forms FEATURE-OF error - minimizing noise. personal data USED-FOR deep learning models. Method is deep learning. Task are unauthorized data exploitation, and face recognition. OtherScientificTerm are Error - minimizing noise, and noise. Generic is model. Metric is normal data utility. ","This paper studies the problem of unauthorized data exploitation in commercial models trained on personal data. In particular, the authors focus on the case of face recognition, where the model is trained on data from multiple sources. Error-minimizing noise is defined in both sample-wise and class-wise forms, and the authors show that the noise can be used to improve the performance of deep learning models. The authors also provide a normal data utility for the noise.","This paper studies the problem of unauthorized data exploitation of personal data for commercial models. In particular, the authors focus on the case of face recognition, where the model is trained on a large amount of data. The authors propose to use error-minimizing noise in both sample-wise and class-wise forms. Error-Minimising noise is defined as the difference between the normal data utility and the noise generated by the model. The paper also proposes a new way to measure the noise. "
1161,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,chess CONJUNCTION Go. Go CONJUNCTION chess. Go CONJUNCTION shogi. shogi CONJUNCTION Go. MuZero USED-FOR game - playing agents. MuZero COMPARE AlphaZero. AlphaZero COMPARE MuZero. game - playing agents COMPARE AlphaZero. AlphaZero COMPARE game - playing agents. model of environmental dynamics USED-FOR MuZero. deterministic environments USED-FOR MuZero. MuZero USED-FOR Nondeterministic MuZero ( NDMZ ). Nondeterministic Monte Carlo Tree Search CONJUNCTION extensive - form games. extensive - form games CONJUNCTION Nondeterministic Monte Carlo Tree Search. MuZero network architecture CONJUNCTION tree search. tree search CONJUNCTION MuZero network architecture. chance player PART-OF MuZero network architecture. chance player PART-OF tree search. Nondeterministic Monte Carlo Tree Search USED-FOR NDMZ. extensive - form games USED-FOR NDMZ. NDMZ USED-FOR chance. chance player PART-OF NDMZ. NDMZ USED-FOR model. model USED-FOR game. Method is MuZero algorithm. Material is Atari suite. OtherScientificTerm is environmental dynamics. ,"This paper proposes a MuZero algorithm for Nondeterministic MuZero (NDMZ) in deterministic environments. MuZero uses a model of environmental dynamics to learn game-playing agents that are more efficient than AlphaZero. NDMZ is trained on extensive-form games such as chess, Go, and shogi. The model is then used to train a model to play the game. The MuZero network architecture is combined with tree search and a chance player in order to learn the chance player. The authors show that MuZero outperforms AlphaZero in the Atari suite.","This paper presents MuZero, an extension of Nondeterministic MuZero (NDMZ) to deterministic environments. MuZero is a model of environmental dynamics that can be used to train game-playing agents. The authors show that MuZero outperforms AlphaZero in a number of games, including chess, Go, shogi, and extensive-form games. They also show that the model can be applied to any game, including tree search, MuZero network architecture, tree search with a chance player, and NDMZ with chance player. The proposed MuZero algorithm is evaluated on the Atari suite."
1170,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"Hierarchical approaches USED-FOR reinforcement learning. Hierarchical approaches USED-FOR learning. Hierarchical approaches USED-FOR data efficiency. Hindsight Off - policy Options ( HO2 ) HYPONYM-OF off - policy option learning algorithm. temporal and action abstraction USED-FOR option framework. flat policies COMPARE mixture policies. mixture policies COMPARE flat policies. mixture policies COMPARE option policies. option policies COMPARE mixture policies. flat policies COMPARE on - policy option methods. on - policy option methods COMPARE flat policies. off - policy training CONJUNCTION backpropagation. backpropagation CONJUNCTION off - policy training. policy components USED-FOR backpropagation. dynamic programming inference procedure USED-FOR off - policy training. dynamic programming inference procedure USED-FOR backpropagation. HO2 COMPARE option learning methods. option learning methods COMPARE HO2. raw pixel inputs USED-FOR simulated robot manipulation tasks. intuitive extension USED-FOR temporal abstraction. OtherScientificTerm are abstractions, data - generating behavior policy, trust - region constraints, and pre - trained options. Method are policy optimization, off - policy optimization, and action and temporal abstraction. Task is off - policy option learning. ","This paper proposes Hindsight Off-policy Options (HO2), an off-policy option learning algorithm based on Hierarchical approaches to improve the data efficiency in reinforcement learning. The authors propose a new option framework based on temporal and action abstraction, where the abstractions are used to learn a data-generating behavior policy. The paper shows that HO2 outperforms flat policies and mixture policies when compared to on-policy options methods. HO2 also outperforms option learning methods when combined with a dynamic programming inference procedure to perform backpropagation using the policy components. ","This paper proposes Hindsight Off-policy Options (HO2), an off-policy option learning algorithm based on Hierarchical approaches to improve the data efficiency in reinforcement learning. The idea is to learn abstractions of the data-generating behavior policy and the option framework based on temporal and action abstraction. The paper proposes to use a trust-region constraints to enforce the pre-trained options to be close to the action and temporal abstraction, and then use an intuitive extension to the temporal abstraction. HO2 is evaluated on simulated robot manipulation tasks with raw pixel inputs. Compared to flat policies and mixture policies, HO2 outperforms on-policy options methods, and outperforms other option learning methods. The authors also show that HO2 can be combined with a dynamic programming inference procedure in order to achieve backpropagation with different policy components. "
1179,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,Reinforcement learning ( RL ) algorithms USED-FOR maximizing the expected cumulative return. drug discovery HYPONYM-OF applications. objective function USED-FOR expected maximum reward. functional form USED-FOR Bellman equation. Bellman operators USED-FOR functional form. formulation USED-FOR synthesizable molecule generation. real - world drug discovery pipeline FEATURE-OF synthesizable molecule generation. Generic is framework. Method is RL agent. OtherScientificTerm is expected cumulative return. ,"This paper proposes a framework for maximizing the expected cumulative return of Reinforcement learning (RL) algorithms. The authors propose a functional form of the Bellman equation, where the objective function is a function of the expected maximum reward. The functional form is learned by Bellman operators, and the authors show that this formulation can be used for synthesizable molecule generation in a real-world drug discovery pipeline.  The authors also show that their framework can be applied to any RL agent. ","This paper proposes a novel framework for maximizing the expected cumulative return in reinforcement learning (RL) algorithms for applications such as drug discovery. The authors propose a functional form of the Bellman equation with Bellman operators, where the objective function is the expected maximum reward, and the RL agent is a Bellman operator. This formulation is applied to synthesizable molecule generation in a real-world drug discovery pipeline. The paper also proposes a framework for estimating the expected cumulant return."
1188,SP:bd4b1781448def4327214c78f07538d285119ef9,"neural networks USED-FOR fixed output dimension. neural network architectures USED-FOR output features. neural networks USED-FOR features. Contextual HyperNetwork ( CHN ) HYPONYM-OF auxiliary model. base model USED-FOR feature. CHN COMPARE re - training and fine - tuning approaches. re - training and fine - tuning approaches COMPARE CHN. neural network USED-FOR CHN. CHN USED-FOR partial variational autoencoder ( P - VAE ). partial variational autoencoder ( P - VAE ) HYPONYM-OF deep generative model. deep generative model USED-FOR missing features. missing features PART-OF sparsely - observed data. CHN USED-FOR CHNs. e - learning CONJUNCTION healthcare tasks. healthcare tasks CONJUNCTION e - learning. system COMPARE imputation and meta - learning baselines. imputation and meta - learning baselines COMPARE system. recommender systems CONJUNCTION e - learning. e - learning CONJUNCTION recommender systems. imputation and meta - learning baselines USED-FOR recommender systems. few - shot learning USED-FOR features. system USED-FOR features. few - shot learning EVALUATE-FOR imputation and meta - learning baselines. healthcare tasks EVALUATE-FOR system. recommender systems EVALUATE-FOR system. few - shot learning EVALUATE-FOR system. Method is deep learning. Task are online learning settings, and recommender system. ","This paper proposes a new auxiliary model called Contextual HyperNetwork (CHN) which is a deep generative model for online learning settings. The authors propose to use neural network architectures to learn the fixed output dimension of the output features of neural networks. The base model is trained to predict the feature of each input feature, and the auxiliary model is a neural network that predicts the feature from the base model. The proposed CHN can be used as a partial variational autoencoder (P-VAE). The authors show that the proposed system outperforms existing imputation and meta-learning baselines in few-shot learning, e-learning, and recommender systems. ","This paper proposes Contextual HyperNetwork (CHN), a new auxiliary model for deep learning. The main idea is to use neural network architectures to learn the output features of a fixed output dimension. The base model is trained to predict the feature of the input, and the auxiliary model is a deep generative model that predicts the missing features of sparsely-observed data. The authors show that CHN outperforms re-training and fine-tuning approaches. The system is evaluated on e-learning, recommender systems, and few-shot learning. "
1197,SP:8e4677cc6071a33397347679308165c10dca2aae,data inefficiency CONJUNCTION catastrophic forgetting. catastrophic forgetting CONJUNCTION data inefficiency. Bayesian paradigm USED-FOR deep learning. poor calibration CONJUNCTION data inefficiency. data inefficiency CONJUNCTION poor calibration. Bayesian inference USED-FOR high - dimensional parameter spaces. high - dimensional parameter spaces FEATURE-OF deep neural networks. deep neural networks USED-FOR Bayesian inference. restrictive approximations USED-FOR Bayesian inference. model parameters USED-FOR inference. expressive posterior approximations USED-FOR full model. Bayesian deep learning method USED-FOR full covariance Gaussian posterior approximation. Bayesian deep learning method USED-FOR point estimate. subnetwork USED-FOR full covariance Gaussian posterior approximation. subnetwork selection procedure USED-FOR posterior uncertainty. approach COMPARE point - estimated networks. point - estimated networks COMPARE approach. approach COMPARE methods. methods COMPARE approach. full network USED-FOR expressive posterior approximations. expressive posterior approximations USED-FOR methods. OtherScientificTerm is point estimates. ,This paper proposes a Bayesian paradigm for deep learning based on Bayesian inference in high-dimensional parameter spaces. The main idea is to use restrictive approximations in Bayesian deep neural networks to improve the generalization performance of the full model. The authors propose a Bayes deep learning method for the full covariance Gaussian posterior approximation with a subnetwork. The proposed approach is shown to perform better than existing point-estimated networks in terms of posterior uncertainty. ,"This paper proposes a Bayesian paradigm for deep learning, where the Bayesian inference is applied to high-dimensional parameter spaces. The authors propose to use restrictive approximations to improve the robustness of Bayes inference to data inefficiency and catastrophic forgetting. In particular, the authors propose a new Bayesian deep learning method for the full covariance Gaussian posterior approximation with a subnetwork selection procedure to reduce the posterior uncertainty. The proposed approach is shown to outperform point-estimated networks in terms of robustness to point estimates. The main contribution of the paper is that the proposed method is able to learn a full network that can be used to train a full model without the need for any additional model parameters. "
1206,SP:be361952fe9de545f68b8a060f790d54c6755998,generalization CONJUNCTION applicability. applicability CONJUNCTION generalization. generalization EVALUATE-FOR embedding techniques. state representations USED-FOR Model - free reinforcement learning approaches. approach USED-FOR jointly learning embeddings. model USED-FOR embeddings. generic architecture USED-FOR policy. these USED-FOR policy. these USED-FOR generic architecture. embedded representations USED-FOR generalization. approach USED-FOR embedded representations. it COMPARE models. models COMPARE it. approach COMPARE it. it COMPARE approach. gaming EVALUATE-FOR it. recommender systems EVALUATE-FOR it. approach COMPARE models. models COMPARE approach. state / action spaces FEATURE-OF discrete / continuous domains. discrete / continuous domains EVALUATE-FOR models. discrete / continuous domains EVALUATE-FOR it. recommender systems EVALUATE-FOR approach. gaming EVALUATE-FOR approach. Method is reinforcement learning. Generic is approaches. Material is discrete and continuous domains. OtherScientificTerm is embedding spaces. ,This paper proposes a new approach to jointly learning embeddings from state representations for Model-free reinforcement learning approaches. The authors propose a generic architecture to learn a policy from these representations and then use these to train a model to learn the embedded representations. The proposed approach is evaluated on both discrete and continuous domains and shows that it outperforms existing models in both discrete/continuous domains and on recommender systems. ,"This paper presents a new approach to jointly learning embeddings for reinforcement learning. The authors propose a generic architecture for learning the policy and the embedding spaces. The approach is evaluated on both discrete and continuous domains, and it outperforms existing models in both discrete/continuous domains and recommender systems. The generalization and applicability of the proposed embedding techniques is shown to be better than existing approaches. The paper also shows that the proposed approach is able to learn the embedded representations for generalization. "
1215,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"viewmaker networks HYPONYM-OF generative models. Viewmakers HYPONYM-OF stochastic bounded adversaries. they USED-FOR views. cropping CONJUNCTION color jitter. color jitter CONJUNCTION cropping. transfer accuracy EVALUATE-FOR welltuned SimCLR augmentations. color jitter HYPONYM-OF transformations. cropping HYPONYM-OF transformations. speech recordings CONJUNCTION wearable sensor data. wearable sensor data CONJUNCTION speech recordings. wearable sensor data EVALUATE-FOR baseline augmentations. speech recordings EVALUATE-FOR baseline augmentations. Viewmaker views CONJUNCTION handcrafted views. handcrafted views CONJUNCTION Viewmaker views. transfer performance EVALUATE-FOR they. robustness EVALUATE-FOR they. viewmakers USED-FOR representation learning algorithms. Viewmaker networks USED-FOR unsupervised learning. Viewmaker networks USED-FOR complex and diverse input - dependent views. complex and diverse input - dependent views USED-FOR unsupervised learning. Task is unsupervised representation learning. Generic is models. Method is unsupervised representation learning methods. OtherScientificTerm are ` p - bounded perturbation, common image corruptions, and domain expertise. Material is CIFAR-10. ","This paper studies the problem of unsupervised representation learning with viewmaker networks. Viewmaker networks are generative models that are trained with stochastic bounded adversaries (e.g. Viewmakers). The authors show that they can be used to learn views that are more robust to common image corruptions. They also show that the transfer accuracy of welltuned SimCLR augmentations can be improved by such transformations as cropping, color jitter, and cropping on speech recordings and wearable sensor data. Finally, they show that viewmakers can improve the transfer performance of representation learning algorithms with viewmakers. ","This paper presents a new perspective on unsupervised representation learning methods. The authors propose to use viewmaker networks, which are generative models, to learn views from stochastic bounded adversaries (e.g., Viewmakers). They show that they can learn views with different transformations such as cropping, color jitter, etc. and improve the transfer accuracy of welltuned SimCLR augmentations on speech recordings and wearable sensor data. They also show that Viewmaker networks are able to learn complex and diverse input-dependent views that are more robust to common image corruptions. Finally, they show that viewmakers can improve the robustness of representation learning algorithms. "
1224,SP:ef7735be9423ad53059505c170e75201ca134573,"autonomous driving CONJUNCTION air traffic management. air traffic management CONJUNCTION autonomous driving. deep learning models USED-FOR high - assurance systems. air traffic management CONJUNCTION medical diagnosis. medical diagnosis CONJUNCTION air traffic management. medical diagnosis HYPONYM-OF high - assurance systems. autonomous driving HYPONYM-OF high - assurance systems. air traffic management HYPONYM-OF high - assurance systems. statistical, geometric, or topological signatures USED-FOR techniques. detection approaches USED-FOR outliers. KMNIST CONJUNCTION F - MNIST. F - MNIST CONJUNCTION KMNIST. CIFAR10 ( for SVHN ) CONJUNCTION KMNIST. KMNIST CONJUNCTION CIFAR10 ( for SVHN ). SVHN CONJUNCTION MNIST. MNIST CONJUNCTION SVHN. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. Imagenet CONJUNCTION LSUN. LSUN CONJUNCTION Imagenet. WideResNet CONJUNCTION DenseNet. DenseNet CONJUNCTION WideResNet. DenseNet CONJUNCTION LeNet5. LeNet5 CONJUNCTION DenseNet. in - distribution data CONJUNCTION Imagenet. Imagenet CONJUNCTION in - distribution data. ResNet34 CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNet34. F - MNIST USED-FOR OOD data. F - MNIST HYPONYM-OF DNN architectures. SVHN USED-FOR in - distribution data. MNIST CONJUNCTION Imagenet. Imagenet CONJUNCTION MNIST. MNIST USED-FOR in - distribution data. SVHN CONJUNCTION Imagenet. Imagenet CONJUNCTION SVHN. ResNet34 HYPONYM-OF DNN architectures. LeNet5 HYPONYM-OF DNN architectures. DenseNet HYPONYM-OF DNN architectures. WideResNet HYPONYM-OF DNN architectures. Method are Deep neural networks ( DNNs ), and integrated","This paper studies the problem of identifying outliers in deep learning models for high-assurance systems. The authors propose to use statistical, geometric, or topological signatures to identify outliers, which is an important problem in high-performing systems such as autonomous driving, air traffic management, and medical diagnosis. Deep neural networks (DNNs) are used to detect outliers. The proposed techniques are based on the observation that outliers are more likely to have statistical or geometric patterns than topological patterns. The main contribution of the paper is to show that these detection approaches can be used to identify outlier patterns in the OOD data.  The authors show that the detection approaches are able to find outliers even when the training data is in-distribution. They also show that such detection approaches have the ability to identify the outliers when the data is out of distribution.   The main contributions of this paper are as follows:  1) The authors introduce a new class of DNN architectures such as WideResNet, DenseNet, LeNet5, F-MNIST, SVHN, MNIST, Imagenet, and LSUN.  2) They show that their detection approach is able to distinguish outliers from other detection approaches. ","This paper proposes a new method for identifying outliers in deep learning models for high-assurance systems such as autonomous driving, medical diagnosis, and air traffic management. Deep neural networks (DNNs) are used to detect outliers. The techniques are based on statistical, geometric, or topological signatures. The authors propose two detection approaches to identify outliers, one based on detection approaches that rely on the topological signature of the outliers and the other based on a combination of detection approaches based on the statistical and geometric signatures. Experiments are performed on a variety of DNN architectures such as DenseNet, LeNet5, WideResNet, ResNet34, F-MNIST, KMNIST and CIFAR10 (for SVHN) as well as on OOD data. The results show that the proposed method is able to identify outlier detection approaches. "
1233,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"hierarchical VAE COMPARE PixelCNN. PixelCNN COMPARE hierarchical VAE. log - likelihood EVALUATE-FOR PixelCNN. natural image benchmarks EVALUATE-FOR PixelCNN. VAEs USED-FOR autoregressive models. VAEs USED-FOR models. autoregressive models COMPARE VAEs. VAEs COMPARE autoregressive models. loglikelihood EVALUATE-FOR autoregressive models. loglikelihood EVALUATE-FOR VAEs. ImageNet CONJUNCTION FFHQ. FFHQ CONJUNCTION ImageNet. stochastic depth FEATURE-OF VAE. PixelCNN COMPARE VAEs. VAEs COMPARE PixelCNN. likelihoods EVALUATE-FOR VAEs. VAE USED-FOR hierarchical visual representations. FFHQ-256 USED-FOR VAE. VAEs USED-FOR global features. VAEs USED-FOR local details. multiscale generative procedure COMPARE PixelCNN. PixelCNN COMPARE multiscale generative procedure. log - likelihood EVALUATE-FOR PixelCNN. log - likelihood EVALUATE-FOR multiscale generative procedure. Generic is they. OtherScientificTerm are insufficient depth, and Low resolution High resolution. Material is high - resolution images. Method is generative process. ","This paper proposes a new hierarchical VAE, PixelCNN, which is able to generate high-resolution images in a multiscale generative process. PixelCNN outperforms existing VAEs on natural image benchmarks. The authors show that VAEs are more efficient than autoregressive models in terms of loglikelihood compared to VAEs, and that they do not suffer from insufficient depth. They also show that the VAE can be used to generate hierarchical visual representations. ","This paper proposes a hierarchical VAE that is more efficient than PixelCNN on natural image benchmarks. The authors show that VAEs are better than autoregressive models in terms of loglikelihood, and that they are more robust to insufficient depth. They also show that the VAE is better than FFHQ-256 for hierarchical visual representations.  The authors also provide a multiscale generative procedure where VAEs can capture local details while PixelCNN can only capture global details.  "
1242,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"methods USED-FOR unsupervised visual representations. uninformative examples PART-OF this. randomly sampled negative examples USED-FOR NCE. semi - hard negatives USED-FOR contrastive representations. bias CONJUNCTION variance. variance CONJUNCTION bias. estimators COMPARE NCE. NCE COMPARE estimators. variance EVALUATE-FOR NCE. CMC CONJUNCTION MoCo. MoCo CONJUNCTION CMC. IR CONJUNCTION CMC. CMC CONJUNCTION IR. image benchmarks EVALUATE-FOR linear evaluation. models USED-FOR approach. IR HYPONYM-OF models. linear evaluation EVALUATE-FOR approach. MoCo HYPONYM-OF models. accuracy EVALUATE-FOR approach. image benchmarks EVALUATE-FOR approach. CMC HYPONYM-OF models. instance segmentation CONJUNCTION key - point detection. key - point detection CONJUNCTION instance segmentation. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. features USED-FOR image distributions. object detection CONJUNCTION key - point detection. key - point detection CONJUNCTION object detection. features USED-FOR downstream tasks. Meta - Dataset collection FEATURE-OF image distributions. key - point detection HYPONYM-OF downstream tasks. object detection HYPONYM-OF downstream tasks. instance segmentation HYPONYM-OF downstream tasks. Method are contrastive learning, metric learning, and mutual information estimators. OtherScientificTerm are noise - contrastive estimation ( NCE ) bound, mutual information, and lower - bounds of mutual information. ","This paper studies the problem of contrastive learning for unsupervised visual representations. The authors propose a new noise-contrastive estimation (NCE) bound based on the mutual information estimators. The NCE is based on randomly sampled negative examples from the Meta-Dataset collection, where the uninformative examples are used to train the NCE. The main idea is to use semi-hard negatives to learn contrastive representations that are more robust to bias and variance. The proposed estimators are shown to perform better than NCE in terms of accuracy and variance compared to other estimators such as IR, CMC, MoCo, and CMC. The approach is evaluated on several image benchmarks and shows that the proposed approach performs better than existing models on linear evaluation. ","This paper proposes two methods for learning unsupervised visual representations from uninformative examples. The first method is based on contrastive learning. The authors propose a noise-contrastive estimation (NCE) bound, which is a generalization of the mutual information estimators. NCE uses randomly sampled negative examples to estimate the NCE. The second method uses semi-hard negatives to learn contrastive representations. The main difference between the two methods is that the authors use metric learning, and the authors show that their estimators outperform NCE in terms of bias and variance. The proposed approach is evaluated on three different models: CMC, MoCo, and IR. The results show that the proposed approach outperforms linear evaluation on image benchmarks and features for image distributions from the Meta-Dataset collection. "
1251,SP:613a0e2d8cbe703f37c182553801be7537333f64,"gradient sharing mechanism USED-FOR machine learning systems. gradient sharing mechanism USED-FOR Private training data. federated learning ( FL ) HYPONYM-OF machine learning systems. data leakage attack USED-FOR batch data. shared aggregated gradients USED-FOR batch data. catastrophic data leakage PART-OF federated learning ( CAFE ). data leakage attacks COMPARE CAFE. CAFE COMPARE data leakage attacks. CAFE USED-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR CAFE. CAFE USED-FOR private data. vertical and horizontal FL settings EVALUATE-FOR CAFE. shared aggregated gradients USED-FOR private data. vertical case HYPONYM-OF FL. data leakage risks FEATURE-OF learning settings. OtherScientificTerm are batch size, data leakage, and training gradients. Generic is method. ","This paper proposes a new gradient sharing mechanism for machine learning systems called federated learning (FL). Private training data can be aggregated using a shared aggregated gradients. The authors show that CAFE can be used as a data leakage attack against batch data, which can lead to catastrophic data leakage in the case of large-batch data leakage attacks such as CAFE. The proposed method is evaluated on both vertical and horizontal FL settings, and the proposed method outperforms CAFE in terms of data recovery quality. ","This paper proposes a new gradient sharing mechanism for machine learning systems, called federated learning (FL) for private training data. The authors propose catastrophic data leakage (CAFE), which is a data leakage attack on batch data. CAFE is based on shared aggregated gradients for batch data, where the batch size is small and the training gradients are large. The proposed method is evaluated on both vertical and horizontal FL settings, and CAFE outperforms CAFE in terms of data recovery quality. "
1260,SP:ce229295081ff04b26f33829f2c3396b90897b5d,physics CONJUNCTION vision. vision CONJUNCTION physics. vision CONJUNCTION robotics. robotics CONJUNCTION vision. Unsupervised learning of interactions USED-FOR physics. physics CONJUNCTION robotics. robotics CONJUNCTION physics. Unsupervised learning of interactions USED-FOR vision. Unsupervised learning of interactions USED-FOR robotics. multi - agent trajectories USED-FOR Unsupervised learning of interactions. neural relational inference USED-FOR static relations. deep generative model USED-FOR dynamic relations. simulated physics system USED-FOR dynamic relation scenarios. periodic and additive dynamics HYPONYM-OF dynamic relation scenarios. training scheme CONJUNCTION model architecture. model architecture CONJUNCTION training scheme. dynamic relational inference accuracy EVALUATE-FOR model architecture. model USED-FOR coordination and competition patterns. real - world multi - agent basketball trajectories USED-FOR model. real - world multi - agent basketball trajectories USED-FOR coordination and competition patterns. Task is dynamic relational inference. OtherScientificTerm is interactions. ,"This paper studies the problem of dynamic relational inference in the context of physics, physics, vision, robotics, and unsupervised learning of interactions between multi-agent trajectories. The authors propose a deep generative model to model dynamic relations between agents in a simulated physics system. The model is trained with a training scheme and a model architecture, and is evaluated on a variety of dynamic relation scenarios (periodic and additive dynamics). The model achieves state-of-the-art performance on both coordination and competition patterns.","This paper proposes a novel approach to dynamic relational inference. The authors propose a deep generative model to model dynamic relations in a simulated physics system. The model is trained on real-world multi-agent basketball trajectories, and is able to learn coordination and competition patterns. The training scheme and the model architecture are evaluated on a variety of dynamic relation scenarios, including periodic and additive dynamics. "
1269,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"Collaborative filtering USED-FOR predicting potential user - item ratings. latent factors PART-OF user - item rating matrix. transductive setting USED-FOR user - specific latent factors. inductive collaborative filtering framework USED-FOR hidden relational graph. rating matrix USED-FOR hidden relational graph. model USED-FOR inductively computing user - specific representations. expressiveness EVALUATE-FOR feature - driven inductive models. model COMPARE feature - driven inductive models. feature - driven inductive models COMPARE model. feature USED-FOR inductively computing user - specific representations. model COMPARE transductive models. transductive models COMPARE model. model USED-FOR inductive learning. cold - start users EVALUATE-FOR them. matrix completion benchmarks EVALUATE-FOR inductive learning. matrix completion benchmarks EVALUATE-FOR model. OtherScientificTerm are user - item ratings, dense weighted graphs, historical rating patterns, relational graphs, and latent space. Method are base matrix factorization model, and relation inference model. ","This paper proposes an inductive collaborative filtering framework for predicting potential user-item ratings. The proposed model is based on a base matrix factorization model, and is able to learn user-specific latent factors in a transductive setting. The latent factors are represented as a hidden relational graph with a rating matrix, and the latent factors can be used to learn the user's rating matrix. The authors show that the proposed model achieves better expressiveness than feature-driven inductive models on matrix completion benchmarks and inductive learning with cold-start users. ","This paper proposes an inductive collaborative filtering framework for predicting potential user-item ratings. The key idea is to use a base matrix factorization model to model the latent factors in the user's rating matrix. The latent factors are learned in a transductive setting, and the user-specific latent factors can be represented as a hidden relational graph. The model is evaluated on a number of inductive learning tasks, and is shown to outperform feature-driven inductive models in terms of expressiveness. The authors also show that the model is able to learn inductively computing users-specific representations for cold-start users, and outperforms them on matrix completion benchmarks. "
1278,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"autoencoder - based disentangled representation learning methods USED-FOR disentanglement. disentangled representation learning CONJUNCTION reconstruction quality. reconstruction quality CONJUNCTION disentangled representation learning. detail information FEATURE-OF image data. correlated latent variables USED-FOR detail information. deep generative model USED-FOR missing correlated latent variables. deep generative model USED-FOR low - quality reconstruction. β - TCVAE HYPONYM-OF disentangled representation learning method. disentangled representation learning method USED-FOR disentangled factors. normalizing flows CONJUNCTION mixtures of Gaussians. mixtures of Gaussians CONJUNCTION normalizing flows. likelihood - based models CONJUNCTION implicit models. implicit models CONJUNCTION likelihood - based models. implicit models CONJUNCTION tractable models. tractable models CONJUNCTION implicit models. variational autoencoders CONJUNCTION implicit models. implicit models CONJUNCTION variational autoencoders. generative adversarial networks HYPONYM-OF implicit models. variational autoencoders HYPONYM-OF likelihood - based models. tractable models HYPONYM-OF model classes. likelihood - based models HYPONYM-OF model classes. mixtures of Gaussians HYPONYM-OF tractable models. implicit models HYPONYM-OF model classes. normalizing flows HYPONYM-OF tractable models. multi - stage model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE multi - stage model. reconstruction quality EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR multi - stage model. reconstruction quality EVALUATE-FOR multi - stage model. OtherScientificTerm are statistical independence of the latent factors, and D - separation. Generic are approach, and model. Method is multi - stage modelling approach. ","This paper proposes a disentangled representation learning method based on β-TCVAE. The key idea is to use a deep generative model to learn missing correlated latent variables for low-quality reconstruction. The proposed approach is based on the idea of statistical independence of the latent factors, which allows for D-separation. The authors show that the proposed multi-stage modelling approach achieves better disentanglement than state-of-the-art methods in terms of reconstruction quality. The model consists of two model classes: tractable models with normalizing flows and mixtures of Gaussians, and implicit models with variational autoencoders and generative adversarial networks.","This paper proposes a disentangled representation learning method for disentangling factors in image data. The key idea is to use a deep generative model to capture missing correlated latent variables for low-quality reconstruction. The proposed approach, called β-TCVAE, is based on the idea of statistical independence of the latent factors, which is achieved by minimizing the D-separation between the model and the latent variables. The authors also propose a multi-stage modelling approach to improve the performance of the multi-staging model. Experiments show improved disentanglement and reconstruction quality compared to state-of-the-art methods in terms of disentanglelement and reconstruct quality. "
1287,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ","This paper studies mutual information (MI) maximization for learning representations of data in reinforcement learning (RL). The authors consider the problem of learning representations for learning in RL using MI objectives. The MI objectives are based on samples of high-dimensional observations from a simulated game environment, where irrelevant and redundant information is available. The authors propose to use MI based objectives to learn representations of the MDP. The state representation of the optimal policy is then used as a state representation for learning the optimal representations. The proposed methods are evaluated on a variety of simulated game environments with visual observations.",This paper proposes mutual information (MI) maximization for learning representations of data. The authors show that MI objectives can be used to learn representations for reinforcement learning (RL) that are more robust to irrelevant and redundant information. The MI objectives are learned from samples of high-dimensional observations. The paper also shows that MI based objectives can learn representations that are robust to insufficient representations.  The authors propose two methods for learning such representations. The first one is based on the structure of the MDP. The second one uses the state representation of the optimal policy to learn an optimal policy. Experiments are conducted on a simulated game environment with visual observations.
1296,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"finite dimensional representation FEATURE-OF semi - infinite dual. finite - dimensional convex copositive program USED-FOR non - convex neural network training problem. global optima CONJUNCTION copositive programs. copositive programs CONJUNCTION global optima. neural networks CONJUNCTION copositive programs. copositive programs CONJUNCTION neural networks. neural networks USED-FOR global optima. neural networks USED-FOR copositive programs. semi - nonnegative matrix factorization USED-FOR neural networks. semi - nonnegative matrix factorization USED-FOR copositive programs. algorithms USED-FOR global minimum. algorithms USED-FOR vector output neural network training problem. global minimum FEATURE-OF vector output neural network training problem. computational complexity EVALUATE-FOR filter size. computational complexity EVALUATE-FOR convolutional architectures. global optimum FEATURE-OF neural network training problem. soft - thresholded SVD USED-FOR neural network training problem. OtherScientificTerm is convex semi - infinite dual. Method are copositive relaxation, and Stochastic Gradient Descent. ","This paper studies the non-convex neural network training problem with a finite-dimensional convex copositive program. The authors consider the convex semi-infinite dual with finite dimensional representation, where the objective is to find a global optima and a set of copoitive programs. The paper proposes two algorithms for finding the global minimum of the vector output neural network learning problem with the global optimum. The first algorithm, Stochastic Gradient Descent (SVD), is based on semi-nonnegative matrix factorization to learn the coposive programs and the neural networks. The second algorithm, Soft-Thresholded SVD, uses soft-thresholded gradient descent to train the neural network. The theoretical results show that SVD is able to achieve a global optimum with a small computational complexity. ","This paper studies the non-convex neural network training problem with a finite-dimensional convex copositive program. The authors show that the convex semi-infinite dual can be represented as a finite dimensional representation, and that the global optima of the semi-invariant dual is a function of the number of parameters of the coposive relaxation. The paper also shows that the proposed algorithms are able to achieve the global minimum of the vector output neural network learning problem with the global optimum, which is a result of semi-nonnegative matrix factorization of the neural networks and the copoitive programs. They also show the computational complexity of the filter size of the convolutional architectures, and show that Stochastic Gradient Descent is a soft-thresholded SVD."
1305,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"vision USED-FOR learning disentangled, object - centric scene representations. unsupervised object segmentation USED-FOR LORL. MONet and Slot Attention HYPONYM-OF unsupervised object segmentation. algorithms USED-FOR object - centric representation. properties CONJUNCTION spatial relationships. spatial relationships CONJUNCTION properties. object categories CONJUNCTION properties. properties CONJUNCTION object categories. representations USED-FOR concepts. object categories HYPONYM-OF concepts. spatial relationships HYPONYM-OF concepts. properties HYPONYM-OF concepts. object - centric concepts USED-FOR object - centric representations. language USED-FOR object - centric concepts. LORL CONJUNCTION unsupervised segmentation algorithms. unsupervised segmentation algorithms CONJUNCTION LORL. LORL USED-FOR object segmentation. LORL USED-FOR MONet and Slot Attention. MONet and Slot Attention USED-FOR object segmentation. language USED-FOR LORL. concepts USED-FOR tasks. LORL CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION LORL. concepts CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION concepts. segmentation algorithms USED-FOR tasks. LORL USED-FOR concepts. MONet HYPONYM-OF segmentation algorithms. referring expression comprehension HYPONYM-OF tasks. OtherScientificTerm is language input. ","This paper proposes a new vision for learning disentangled, object-centric scene representations. The proposed LORL is a combination of unsupervised object segmentation and MONet and Slot Attention, which are algorithms for learning an object-centric representation from the language input. The key idea is to learn concepts such as object categories, objects, spatial relationships, properties, etc. using the language. The concepts are then used to train the segmentation algorithms and the concepts are used to learn the objects. Experiments are conducted on three tasks: referring expression comprehension, segmentation, and LORDL.","This paper proposes a new vision for learning disentangled, object-centric scene representations. The key idea is to use unsupervised object segmentation for LORL, MONet and Slot Attention, as well as various unsupersupervised segmentation algorithms for learning an object-centric representation. The authors propose two algorithms to learn the object- centric representation from the language input. The proposed concepts are a combination of object categories, properties, spatial relationships, and object categories. The language is used to represent the concepts, and the concepts are then used to learn object-categories and spatial relationships. Experiments are conducted on two tasks: referring expression comprehension and segmentation. The results show that the proposed concepts can be combined with other concepts and other segmentation methods, and that the learned concepts are useful for tasks such as MONet, Slot Attention. "
1314,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"fact triplets PART-OF KG. fact triplets USED-FOR embedding methods. logic rules USED-FOR rich background information. rules USED-FOR reasoning. EM - RBR USED-FOR multi - relation reasoning link prediction. relational background knowledge USED-FOR multi - relation reasoning link prediction. rules FEATURE-OF relational background knowledge. relational background knowledge USED-FOR EM - RBR. FB15k CONJUNCTION WN18. WN18 CONJUNCTION FB15k. EM - RBR COMPARE models. models COMPARE EM - RBR. dataset EVALUATE-FOR model. WN18 EVALUATE-FOR models. FB15k EVALUATE-FOR EM - RBR. FB15k EVALUATE-FOR models. Task are Knowledge graph completion, and representation of the knowledge graph. OtherScientificTerm are knowledge graph, algebraic space, and relational patterns. Method are embedding models, and embedding. Generic is framework. Metric is prediction accuracy. Material is FB15k - R. ","This paper studies the problem of Knowledge Graph completion. The authors propose a new embedding method based on fact triplets in the KG. The key idea is to embed the knowledge graph into a knowledge graph and then use logic rules to extract the rich background information from the knowledge graphs. The embedding is then used to train the embedding models. Empirically, the authors show that the proposed EM-RBR is able to perform multi-relation reasoning link prediction with relational background knowledge. The proposed framework is evaluated on FB15k-R and WN18 and shows that the model performs better than other models on the dataset. ","This paper proposes a new framework for embedding models. The key idea is to use fact triplets in KG, which is a special case of embedding methods based on the knowledge graph completion. The authors propose to use logic rules to encode rich background information in the algebraic space, and then use these rules for reasoning. The proposed framework is evaluated on a dataset of FB15k-R and WN18, and shows that EM-RBR is able to learn relational background knowledge for multi-relation reasoning link prediction. The paper also shows that the proposed models outperform other models in terms of prediction accuracy. "
1323,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"video game HYPONYM-OF structured, dynamic environment. procedural knowledge USED-FOR Black - box models. monolithic hidden state USED-FOR Black - box models. modularity FEATURE-OF knowledge. architecture USED-FOR declarative and procedural knowledge. schemata USED-FOR state updates. active modules CONJUNCTION passive external knowledge sources. passive external knowledge sources CONJUNCTION active modules. passive external knowledge sources USED-FOR state updates. object files HYPONYM-OF active modules. schemata HYPONYM-OF passive external knowledge sources. active modules PART-OF architecture. attention USED-FOR object files. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. input - output interface FEATURE-OF drop - in replacement. drop - in replacement PART-OF architecture. LSTM HYPONYM-OF normal recurrent networks. GRU HYPONYM-OF normal recurrent networks. OtherScientificTerm are declarative knowledge, systematicity, and object tokens. Generic is they. Metric is generalization. Material is intuitive physics benchmark. ","This paper proposes a new architecture for learning declarative and procedural knowledge for Black-box models with a monolithic hidden state in a structured, dynamic environment. The key idea is to use the modularity of knowledge in the form of schemata, active modules and passive external knowledge sources to generate state updates from the object files. The architecture is based on drop-in replacement in the input-output interface, which is a common technique in normal recurrent networks such as LSTM and GRU. The authors show that they can achieve better generalization than the intuitive physics benchmark. ","This paper proposes a novel architecture for learning declarative and procedural knowledge in a structured, dynamic environment. The key idea is to learn a monolithic hidden state for Black-box models with procedural knowledge. The modularity of knowledge is used to encourage the declaratives to be more generalizable. The architecture consists of two active modules and two passive external knowledge sources: object files and schemata. The active modules are used to generate object files, while the passive external know the object tokens, and the attention is used for generating the object files. The drop-in replacement is done via an input-output interface. The authors evaluate the proposed architecture on LSTM, GRU, and normal recurrent networks. The results show that the architecture achieves better generalization compared to the intuitive physics benchmark. "
1332,SP:42a3c0453ab136537b5944a577d63412f3c22560,"Neural module networks ( NMN ) USED-FOR image - grounded tasks. synthetic images USED-FOR Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) HYPONYM-OF image - grounded tasks. NMN USED-FOR video - grounded language tasks. complexity EVALUATE-FOR visual tasks. tasks COMPARE visual tasks. visual tasks COMPARE tasks. visual temporal variance FEATURE-OF visual tasks. complexity EVALUATE-FOR tasks. NMN approaches USED-FOR image - grounded tasks. information retrieval process USED-FOR video - grounded language tasks. VilNMN USED-FOR action - based inputs. VilNMN USED-FOR language components. video QA CONJUNCTION video - grounded dialogues. video - grounded dialogues CONJUNCTION video QA. video - grounded language tasks EVALUATE-FOR VilNMN. video - grounded dialogues HYPONYM-OF video - grounded language tasks. video QA HYPONYM-OF video - grounded language tasks. Method are neural modules, and neural module networks. OtherScientificTerm is visual cues. ","This paper studies the problem of visual question answering in video-grounded video-based language tasks. The authors propose to use neural module networks (NMN) to learn the language components of a video QA and video dialogues. In particular, the authors propose a new information retrieval process for video-bounded language tasks that uses VilNMN to extract action-based inputs from the video. They show that the proposed method can achieve state-of-the-art performance on a variety of tasks.","This paper proposes a new approach to learn neural modules for image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. The authors propose to use neural module networks (NMN) to learn the language components of the image. They show that NMN approaches can be used to learn language components for video-based language tasks using an information retrieval process. They also show that VilNMN can learn the action-based inputs of the visual cues. They compare the complexity of these tasks to other visual tasks in terms of visual temporal variance, and show that the proposed approach is able to achieve better performance on video QA and video-grounding dialogues."
1341,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"empirical game analysis CONJUNCTION deep reinforcement learning ( Deep RL ). deep reinforcement learning ( Deep RL ) CONJUNCTION empirical game analysis. Deep RL USED-FOR best response. mixture of opponent policies USED-FOR best response. PSRO USED-FOR Deep RL training. algorithms USED-FOR PSRO. PSRO USED-FOR policies. policies PART-OF empirical game. Mixed - Opponents USED-FOR pure - strategy opponent. strategy ’s action - value estimates COMPARE policies. policies COMPARE strategy ’s action - value estimates. strategy ’s action - value estimates USED-FOR pure - strategy opponent. algorithms USED-FOR PSRO. algorithms USED-FOR game. Method are Policy - Space Response Oracles ( PSRO ), and Mixed - Oracles. Task is learning policies in multiagent systems. Generic are algorithm, first, second, and policy. OtherScientificTerm is unobserved distribution of opponents. ","This paper studies the problem of learning policies in multiagent systems. The authors propose a new algorithm called Policy-Space Response Oracles (PSRO), which learns the best response from a mixture of opponent policies. PSRO can be applied to Deep RL training. The main idea is to learn the best policies in an empirical game, where the first, second, and third opponents are unknown and the unobserved distribution of opponents is unknown. Then, the strategy’s action-value estimates are used to train the pure-strategy opponent against the mixed-oracles. The algorithms are then applied to the game.","The paper proposes a new algorithm for learning policies in multiagent systems. The algorithm is called Policy-Space Response Oracles (PSRO) and is based on empirical game analysis and deep reinforcement learning (Deep RL) where the goal is to learn the best response from the mixture of opponent policies. PSRO is used in Deep RL training. The authors propose two algorithms for learning the policies in the empirical game: first, a pure-strategy opponent is learned using “mixed-opponents”, and second, a strategy’s action-value estimates are used to estimate the best-response from the mixed-opponent. The algorithms are applied to the game, and the authors show that the proposed algorithms outperform other algorithms in the game. "
1350,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,attention mechanism CONJUNCTION duration predictor. duration predictor CONJUNCTION attention mechanism. Tacotron 2 text - to - speech model USED-FOR Non - Attentive Tacotron. unaligned duration ratio CONJUNCTION word deletion rate. word deletion rate CONJUNCTION unaligned duration ratio. metrics USED-FOR large - scale robustness evaluation. pre - trained speech recognition model USED-FOR metrics. pre - trained speech recognition model USED-FOR large - scale robustness evaluation. Gaussian upsampling USED-FOR Non - Attentive Tacotron. Non - Attentive Tacotron COMPARE Tacotron 2. Tacotron 2 COMPARE Non - Attentive Tacotron. 5 - scale mean opinion score USED-FOR naturalness. 5 - scale mean opinion score EVALUATE-FOR Non - Attentive Tacotron. fine - grained variational auto - encoder USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR method. fine - grained variational auto - encoder USED-FOR method. Metric is robustness. Method is supervised training. ,This paper proposes a novel non-attentive Tacotron 2 text-to-speech model based on a pre-trained speech recognition model for large-scale robustness evaluation. The proposed method uses a fine-grained variational auto-encoder for the duration predictor and a semi-supervised or unsupervised manner for the attention mechanism and duration predictor. The authors show that the proposed method achieves a better 5-scale mean opinion score for naturalness than the non-AttentiveTacotron with Gaussian upsampling. ,This paper proposes a novel non-attentive Tacotron 2 text-to-speech model for large-scale robustness evaluation using a pre-trained speech recognition model. The proposed method is based on a semi-supervised or unsupervised manner where the attention mechanism and the duration predictor are learned in a supervised training. The authors propose two metrics to measure the robustness: unaligned duration ratio and word deletion rate. They also propose Gaussian upsampling to improve the performance of Non-AttentiveTacotron. They show that the proposed method uses a fine-grained variational auto-encoder to learn a duration predictor and a 5-scale mean opinion score to measure naturalness. 
1359,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"layout estimation USED-FOR planning and navigation. planning and navigation USED-FOR robotics applications. layout estimation USED-FOR robotics applications. self - driving HYPONYM-OF robotics applications. supervised end - to - end framework USED-FOR estimation of bird ’s eye view layout. deep learning networks USED-FOR disparity estimation. deep learning networks USED-FOR network. internal bird ’s eye view feature representation USED-FOR layout estimation. stereo images USED-FOR features. features USED-FOR disparity feature volume. stereo images USED-FOR disparity feature volume. scene structure FEATURE-OF coarse - grained information. rich bird ’s eye view representation USED-FOR spatial reasoning. IPM features USED-FOR rich bird ’s eye view representation. IPM features CONJUNCTION projected feature volume. projected feature volume CONJUNCTION IPM features. projected feature volume USED-FOR rich bird ’s eye view representation. representation USED-FOR BEV semantic map. IPM features USED-FOR supervisory signal. supervisory signal USED-FOR stereo features. IPM features USED-FOR stereo features. datasets EVALUATE-FOR approach. synthetically generated dataset EVALUATE-FOR approach. synthetically generated dataset HYPONYM-OF datasets. datasets EVALUATE-FOR baseline techniques. Method are explicit depth estimation, and inverse perspective mapping ( IPM ). Generic is it. OtherScientificTerm are bird ’s eye view coordinates, bird ’s eye view, and fine - grained texture information. ","This paper proposes a supervised end-to-end framework for the estimation of bird’s eye view layout. The proposed method is based on inverse perspective mapping (IPM), which is a popular technique for explicit depth estimation. The key idea is to use the internal bird's eye view feature representation to perform layout estimation for planning and navigation in robotics applications such as self-driving. The features are learned using stereo images and projected feature volume from deep learning networks for disparity estimation. IPM features are used for stereo features and a supervisory signal is used for the stereo features. The representation of the BEV semantic map is then used for spatial reasoning. The approach is evaluated on two datasets: a synthetically generated dataset and a real-world dataset.","This paper proposes a supervised end-to-end framework for the estimation of bird’s eye view layout. The main idea is to use explicit depth estimation, which is based on inverse perspective mapping (IPM), to estimate the distance between a set of birds’ eye view coordinates and their corresponding bird ’s view coordinates. The proposed network is trained using deep learning networks on stereo images, and the features are used for disparity feature volume and projected feature volume. The representation is then used as a BEV semantic map. Experiments on a synthetic dataset and a synthetically generated dataset show that the proposed approach outperforms baseline techniques. "
1368,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,over - parameterization CONJUNCTION over - smoothing. over - smoothing CONJUNCTION over - parameterization. vanishing gradients CONJUNCTION over - parameterization. over - parameterization CONJUNCTION vanishing gradients. Relational Graph Neural Networks ( GNN ) COMPARE GNNs. GNNs COMPARE Relational Graph Neural Networks ( GNN ). methods USED-FOR GNNs. normalization techniques CONJUNCTION skip connection. skip connection CONJUNCTION normalization techniques. normalization techniques PART-OF methods. learning long - range patterns FEATURE-OF multi - relational graphs. GNNs USED-FOR learning long - range patterns. relation - aware GNN architecture USED-FOR long - range modeling between nodes. vector - based approach USED-FOR relation - aware GNN architecture. gated skip connections USED-FOR relation - aware GNN architecture. Graph Attention Network USED-FOR relation - aware GNN architecture. method COMPARE architectures. architectures COMPARE method. method COMPARE GNN variants. GNN variants COMPARE method. GNN variants COMPARE architectures. architectures COMPARE GNN variants. GNN variants USED-FOR deeper configurations. Method is deeper networks. Material is synthetic and real data. ,This paper proposes a new relation-aware GNN architecture for long-range modeling between nodes. Relational Graph Neural Networks (GNN) have been shown to perform better than GNNs on synthetic and real data. The main contribution of the paper is a vector-based approach to train a relation-specific GNN based on a Graph Attention Network. The authors show that the proposed method is more efficient than existing GNN variants and can learn deeper configurations than existing architectures. ,"This paper proposes a novel relation-aware GNN architecture for long-range modeling between nodes in multi-relational graphs. Relational Graph Neural Networks (GNN) have been shown to outperform GNNs in terms of over-parameterization, over-smoothing, vanishing gradients, and skip connection. The authors propose two methods to improve GNN's generalization ability to multi-relationship graphs by normalization techniques. The main contribution of the paper is a vector-based approach to train a relation-specific GNN with gated skip connections. The proposed method is evaluated on synthetic and real data, and compared to other architectures and GNN variants for deeper configurations."
1377,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"Symbolic techniques USED-FOR neural network properties. Satisfiability Modulo Theory ( SMT ) solvers USED-FOR Symbolic techniques. gradient - based methods CONJUNCTION symbolic techniques. symbolic techniques CONJUNCTION gradient - based methods. technique USED-FOR gradient - based methods. technique USED-FOR minimal regions. technique USED-FOR large networks. Integrated Gradients USED-FOR gradient information. gradient information USED-FOR approach. SMT constraints USED-FOR minimal input mask discovery problem. approach USED-FOR mask regions. approach USED-FOR minimal masks. ImageNet CONJUNCTION Beer Reviews. Beer Reviews CONJUNCTION ImageNet. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. saliency scores EVALUATE-FOR gradient - based methods. approach COMPARE gradient - based methods. gradient - based methods COMPARE approach. saliency scores EVALUATE-FOR approach. MNIST EVALUATE-FOR technique. Task are model explanation, and neural network ’s prediction. OtherScientificTerm are threshold, mask, and saliency map. ","This paper proposes a new method for learning representations of mask regions in neural networks. The proposed method is based on the Satisfiability Modulo Theory (SMT) solvers. The main idea is to use the SMT constraints to solve the minimal input mask discovery problem. The authors show that the proposed method outperforms the state-of-the-art gradient-based methods on MNIST, ImageNet, and Beer Reviews.","This paper presents a new approach to learning the neural network properties. Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers are used to learn the model explanation. The approach is based on Integrated Gradients, where the gradient information of the mask is used as the threshold. The authors show that the proposed approach is able to learn minimal masks for large networks. They also show that their approach can learn the mask regions with SMT constraints for the minimal input mask discovery problem. The proposed technique is evaluated on ImageNet, MNIST, Beer Reviews, and MNIST with saliency scores better than gradient-based methods. "
1386,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,3D pose estimation HYPONYM-OF computer vision. deep learning approaches USED-FOR 3D pose estimation. deep neural networks CONJUNCTION 3D generative representations of objects. 3D generative representations of objects CONJUNCTION deep neural networks. deep neural networks PART-OF unified neural architecture. 3D generative representations of objects PART-OF unified neural architecture. generative vision models USED-FOR partial occlusion. NeMo HYPONYM-OF unified neural architecture. generative model of neural feature activations PART-OF dense 3D mesh. generative model of neural feature activations USED-FOR NeMo. differentiable rendering USED-FOR 3D object pose. NeMo CONJUNCTION feature representation of the target image. feature representation of the target image CONJUNCTION NeMo. reconstruction error EVALUATE-FOR NeMo. feature representations PART-OF mesh. local optima FEATURE-OF reconstruction loss. feature extractor USED-FOR feature representations. contrastive learning USED-FOR feature representations. contrastive learning USED-FOR feature extractor. occluded - PASCAL3D+ CONJUNCTION ObjectNet3D. ObjectNet3D CONJUNCTION occluded - PASCAL3D+. PASCAL3D+ CONJUNCTION occluded - PASCAL3D+. occluded - PASCAL3D+ CONJUNCTION PASCAL3D+. NeMo COMPARE deep networks. deep networks COMPARE NeMo. NeMo USED-FOR partial occlusion. regular data EVALUATE-FOR NeMo. mesh representation USED-FOR object geometry. 3D geometry USED-FOR 3D pose estimation. cuboid USED-FOR mesh representation. mesh representation USED-FOR NeMo. cuboid USED-FOR object geometry. Generic is code. ,"This paper studies the problem of 3D pose estimation in computer vision with deep learning approaches. The authors propose NeMo, a unified neural architecture that combines deep neural networks with 3D generative representations of objects and a feature representation of the target image. NeMo uses a generative model of neural feature activations in a dense 3D mesh. The feature extractor is trained using contrastive learning to extract the feature representations from the mesh, and the reconstruction loss is based on local optima. The paper shows that NeMo achieves better reconstruction error on regular data compared to deep networks.   The paper also proposes a differentiable rendering for 3D object pose based on a cuboid, which can be used to represent the object geometry. The proposed code is well written and easy to follow. ","This paper presents a unified neural architecture, NeMo, for 3D pose estimation in computer vision. NeMo combines deep neural networks with 3D generative representations of objects in a dense 3D mesh. The feature extractor uses contrastive learning to extract feature representations of the target image and NeMo uses a generative model of neural feature activations to generate a denser 3D meshes. The reconstruction error of NeMo is based on local optima of the reconstruction loss. The mesh representation is constructed from cuboid and a mesh representation of the object geometry. Differentiable rendering is used to predict the 3D object pose using differentiable rendering. Experimental results show that NeMo outperforms deep networks on regular data. "
1395,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"large scale retrieval - based applications USED-FOR Feature Compatible Learning ( FCL ). approaches USED-FOR feature compatible learning. old training data CONJUNCTION classifiers. classifiers CONJUNCTION old training data. old training data USED-FOR approaches. classifiers USED-FOR approaches. approach USED-FOR feature compatible learning. features USED-FOR approach. unified framework USED-FOR FCL. model USED-FOR pseudo classifier. random walk algorithm USED-FOR it. model USED-FOR embedding features. ImageNet ILSVRC 2012 and Places365 data EVALUATE-FOR approach. Method are embedding model, and Non - Inherent Feature Compatible Learning. Generic is old model. ","This paper proposes a novel approach for feature compatible learning in large scale retrieval-based applications. The authors propose a new embedding model, called Non-Inherent Feature Compatible Learning (NIPCL), which is based on a unified framework for FCL. NIPCL combines two existing approaches based on old training data and classifiers. The model is trained using a pseudo classifier, and the embedding features are learned using a random walk algorithm. The proposed approach is evaluated on ImageNet ILSVRC 2012 and Places365 data.","This paper proposes a novel approach for feature compatible learning (FCL) for large scale retrieval-based applications. The approach is based on a unified framework for FCL. The authors propose two approaches: (1) Non-Inherent Feature Compatible Learning (NICL), which uses old training data and classifiers to train an embedding model, and (2) a pseudo classifier, which uses a random walk algorithm to train it. The proposed approach is evaluated on ImageNet ILSVRC 2012 and Places365 data."
1404,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"generalization error EVALUATE-FOR deep neural networks ( DNNs ). validation accuracy EVALUATE-FOR model selection. gradient norm USED-FOR model selection criterion. generalization error CONJUNCTION gradient norm measures. gradient norm measures CONJUNCTION generalization error. efficiency CONJUNCTION effectiveness. effectiveness CONJUNCTION efficiency. approximated gradient norm USED-FOR models. approximated gradient norm USED-FOR hyper - parameter search objectives. generalization error EVALUATE-FOR models. gradient norm CONJUNCTION generalization error. generalization error CONJUNCTION gradient norm. BOHB HYPONYM-OF bandit - based or population - based algorithms. gradient norm USED-FOR models. gradient norm USED-FOR generalization. generalization EVALUATE-FOR models. gradient norm COMPARE algorithms. algorithms COMPARE gradient norm. models COMPARE algorithms. algorithms COMPARE models. architectures USED-FOR models. Method are neural network architectures, hyper - parameter optimization, and DNNs. Metric are gradient complexity, computation cost, and computation overhead. OtherScientificTerm are loss gradient, and gradient norm objectives. ","This paper studies the generalization error of deep neural networks (DNNs) in the context of validation accuracy for model selection. The authors consider the problem of hyper-parameter optimization in the setting of neural network architectures, where the gradient complexity of the loss gradient is large and the computation cost is high.  The authors propose a new model selection criterion based on the gradient norm, which is an approximated gradient norm that can be used to select the best models to be used in the hyper -parameter search objectives.  They show that the proposed gradient norm can improve the performance of models in terms of generalization, efficiency, and gradient norm measures. They also show that their algorithms are more efficient than existing algorithms such as BOHB. ","This paper studies the generalization error of deep neural networks (DNNs) in the context of validation accuracy for model selection for hyper-parameter optimization. The authors propose a new model selection criterion based on the gradient norm of the loss gradient. They show that the proposed algorithm, BOHB, outperforms other bandit-based or population-based algorithms in terms of generalization, efficiency, and gradient norm measures. They also show that models trained with the proposed gradient norm can achieve better generalization than other algorithms. "
1413,SP:13359456defb953dd2d19e1f879100ce392d6be6,"Wikipedia HYPONYM-OF Encyclopedias. entity linking CONJUNCTION open - domain question answering. open - domain question answering CONJUNCTION entity linking. open - domain question answering HYPONYM-OF knowledge - intensive tasks. entity linking HYPONYM-OF knowledge - intensive tasks. weight vectors USED-FOR entity representations. entity meta information USED-FOR entity representations. memory footprint USED-FOR dense representations. vector dot product USED-FOR entity affinity. GENRE HYPONYM-OF system. context CONJUNCTION entity name. entity name CONJUNCTION context. vocabulary size COMPARE entity count. entity count COMPARE vocabulary size. datasets USED-FOR entity disambiguation. datasets EVALUATE-FOR approach. Generic is approaches. Method are classifiers, autoregressive formulation, and encoder - decoder architecture. OtherScientificTerm are missing fine - grained interactions, softmax loss, and entities. Material is negative data. ","This paper studies the problem of entity disambiguation in Encyclopedias (e.g., Wikipedia). The authors propose a new approach to disambigenize the entity representations from weight vectors. The key idea is to use the entity meta information from the weight vectors to learn entity representations that are close to the true entity representations. The authors also propose a novel autoregressive formulation of the encoder-decoder architecture, GENRE. The proposed system is evaluated on a variety of datasets and shows that the proposed approach performs better than existing approaches.","This paper proposes a new approach to disambiguate the representation of Encyclopedias (e.g., Wikipedia) from the missing fine-grained interactions. The authors propose an autoregressive formulation of the encoder-decoder architecture, GENRE, which uses weight vectors to encode entity representations from the entity meta information. The key idea is to use the memory footprint to encode dense representations with a softmax loss. The proposed system is evaluated on two datasets for entity disambigraphing, and the authors show that the proposed approach outperforms existing approaches on both negative data and positive data. The paper also shows that the vocabulary size is smaller than the entity count, and that the entity affinity is better than the vector dot product."
1422,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"infinite time horizon FEATURE-OF unknown congestion functions. fe CONJUNCTION congestion function. congestion function CONJUNCTION fe. observation USED-FOR routing decisions. algorithm USED-FOR ce. algorithm USED-FOR routing decisions. observation USED-FOR algorithm. total cost CONJUNCTION minimum cost. minimum cost CONJUNCTION total cost. cumulative regret FEATURE-OF algorithm. space complexity CONJUNCTION time complexity. time complexity CONJUNCTION space complexity. time complexity EVALUATE-FOR algorithm. space complexity EVALUATE-FOR algorithm. Task is routing users. OtherScientificTerm are unknown distribution, routing requests, and regret. Material is New York City road networks. ","This paper studies the problem of routing users in the infinite time horizon of unknown congestion functions with finite time horizon. The authors propose a new algorithm for routing users with unknown distribution. The algorithm is based on the observation that the total cost and the minimum cost of the routing decisions can be computed by the distance between the unknown distribution and the true unknown distribution (i.e., the congestion function). The authors show that the proposed algorithm has a cumulative regret of $O(\sqrt{T})$ with respect to the number of routing requests and the distance to the true distribution. They also show that their algorithm has better space complexity and time complexity than existing algorithms. ","This paper proposes a new algorithm for routing users in the infinite time horizon of unknown congestion functions. The authors show that the unknown distribution can be represented as a finite time horizon, and the congestion function can be decomposed into a fe and a congestion function. The algorithm is based on observation of the ce and the routing decisions made by the routing users. The total cost and the minimum cost of the algorithm are computed as a function of the number of routing requests and the distance to the unknown distributions. The proposed algorithm is evaluated on New York City road networks and shows better time complexity and space complexity compared to the original algorithm. It also shows better cumulative regret compared to previous work."
1431,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"BERT HYPONYM-OF Masked Language Models ( MLMs ). uniform masking USED-FOR MLM. PMI - Masking HYPONYM-OF masking strategy. Pointwise Mutual Information ( PMI ) USED-FOR masking strategy. entity / phrase masking CONJUNCTION random - span masking. random - span masking CONJUNCTION entity / phrase masking. whole - word masking CONJUNCTION entity / phrase masking. entity / phrase masking CONJUNCTION whole - word masking. PMIMasking COMPARE prior more heuristic approaches. prior more heuristic approaches COMPARE PMIMasking. entity / phrase masking HYPONYM-OF prior more heuristic approaches. whole - word masking HYPONYM-OF prior more heuristic approaches. random - span masking HYPONYM-OF random uniform token masking. entity / phrase masking HYPONYM-OF random uniform token masking. whole - word masking HYPONYM-OF random uniform token masking. PMI - Masking COMPARE prior masking approaches. prior masking approaches COMPARE PMI - Masking. OtherScientificTerm are shallow local signals, token n - gram, and collocation. Task is pretraining inefficiency. ","This paper proposes a new masking strategy based on Pointwise Mutual Information (PMI) called PMI-Masking, which is a uniform masking for Masked Language Models (MLMs) such as BERT. PMIMasking achieves better pretraining inefficiency than prior more heuristic approaches such as entity/phrase masking and random-span masking, as well as random uniform token masking (e.g. entity/phrase masking or random-span masking). The main idea is to use shallow local signals to mask out the token n-gram, and then use the masking technique to train the MLM. The authors show that PMI -Masking achieves better performance than prior masking approaches like entity/pseudo-masking and whole-word masking.","This paper proposes a new masking strategy based on Pointwise Mutual Information (PMI) for Masked Language Models (MLMs) called BERT. PMI-Masking is an extension of uniform masking to the MLM. PMIMasking is shown to outperform prior more heuristic approaches such as entity/phrase masking and random-span masking (e.g., whole-word masking, random uniform token masking). The authors also show that PMI -Masking outperforms prior masking approaches in terms of pretraining inefficiency. "
1440,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"Amortised inference USED-FOR sequential latent - variable models ( LVMs ). Bayesian filter HYPONYM-OF mixture of smoothing posteriors. ELBO objective USED-FOR partially - conditioned amortised posteriors. partially - conditioned amortised posteriors USED-FOR products of smoothing posteriors. traffic flow CONJUNCTION handwritten digits. handwritten digits CONJUNCTION traffic flow. handwritten digits CONJUNCTION aerial vehicle dynamics. aerial vehicle dynamics CONJUNCTION handwritten digits. aerial vehicle dynamics HYPONYM-OF scenarios. traffic flow HYPONYM-OF scenarios. handwritten digits HYPONYM-OF scenarios. generative modelling CONJUNCTION multi - step prediction. multi - step prediction CONJUNCTION generative modelling. fully - conditioned approximate posteriors USED-FOR generative modelling. OtherScientificTerm are evidence lower bound ( ELBO ), variational posteriors, posteriors, and approximate posteriors. Generic is setting. Method is generative model. ","This paper studies the problem of amortised inference for sequential latent-variable models (LVMs). The setting is a mixture of smoothing posteriors (e.g., Bayesian filter). The authors propose an evidence lower bound (ELBO) objective for partially-conditioned amortized posteriors, which are products of the products of two partially-computed amortising posteriors: (1) a variational posteriors and (2) an approximate posteriors. The authors show that the ELBO objective can be applied to partially-conditional amortise posteriors in a generative model and in multi-step prediction. The paper also shows that the generative modelling can be trained with fully-conditionive approximate posterior, and that the results can be used in a variety of scenarios such as traffic flow, handwritten digits, and aerial vehicle dynamics.","This paper proposes a new framework for learning sequential latent-variable models (LVMs) with amortised inference. The authors propose an evidence lower bound (ELBO) for variational posteriors, which is a mixture of smoothing posteriors (e.g., a Bayesian filter) and the product of the two. The ELBO objective is applied to partially-conditioned amortized posteriors. The proposed setting is similar to the generative model, where the posteriors are generated from a set of approximate posteriors and the authors show that the partially-conditional amortising posteriors can be used to learn the products of smoothening posteriors with respect to the input. Experiments are conducted on three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. The generative modelling and multi-step prediction are shown to be able to learn fully-conditionated approximate posterior. "
1449,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"statistical properties FEATURE-OF distributed kernel ridge regression. distributed kernel ridge regression CONJUNCTION random features ( DKRR - RF ). random features ( DKRR - RF ) CONJUNCTION distributed kernel ridge regression. statistical properties FEATURE-OF random features ( DKRR - RF ). divide - and - conquer technique CONJUNCTION random features. random features CONJUNCTION divide - and - conquer technique. O(|D| ) memory CONJUNCTION O(|D| ) time. O(|D| ) time CONJUNCTION O(|D| ) memory. statistical accuracy EVALUATE-FOR KRR. random features USED-FOR KRR. divide - and - conquer technique USED-FOR KRR. O(|D| ) time USED-FOR KRR. O(|D| ) memory USED-FOR KRR. communication strategy USED-FOR DKRR - RF. OtherScientificTerm are optimal generalization bounds, generalization bounds, and average information. Generic is theoretical bounds. ","This paper studies the statistical properties of distributed kernel ridge regression and random features (DKRR-RF) under the assumption of optimal generalization bounds. The authors show that KRR with O(|D|) memory, random features, divide-and-conquer technique, and a communication strategy can achieve better statistical accuracy than KRR in terms of O(O|D) time. They also provide theoretical bounds for these theoretical bounds. ","This paper studies the statistical properties of distributed kernel ridge regression and random features (DKRR-RF) under the assumption of optimal generalization bounds. The theoretical bounds are based on the assumption that the average information of the two distributions is close to each other. The authors propose a divide-and-conquer technique for KRR with random features and O(|D|) memory, and a communication strategy to improve the performance of KRR. The empirical results show that KRR improves the statistical accuracy of the proposed KRR, and that the communication strategy improves the generalization bound."
1458,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"search step USED-FOR architectures. validation performance EVALUATE-FOR architectures. accuracy EVALUATE-FOR weight - sharing architectures. RandomNAS USED-FOR architectures. global search space(GS ) FEATURE-OF architectures. top - performing architectures PART-OF GS. proxy search space ( PS ) USED-FOR RandomNAS. EPS HYPONYM-OF Proxy Search Space. EPS HYPONYM-OF RandomNAS - based approach. EPS COMPARE state - of - the - art. state - of - the - art COMPARE EPS. NASBench-201 EVALUATE-FOR EPS. image classification CONJUNCTION natural language processing. natural language processing CONJUNCTION image classification. DARTS - like search spaces USED-FOR tasks. EPS USED-FOR tasks. search time EVALUATE-FOR EPS. natural language processing HYPONYM-OF tasks. image classification HYPONYM-OF tasks. Method is NAS approach. Metric are achievable accuracy, and RandomNAS ’s search efficiency. OtherScientificTerm are NAS search space, and ground - truth ranking. ","This paper proposes a new NAS approach to improve the performance of weight-sharing architectures. The main idea is to use a proxy search space (PS) instead of the global search space(GS) in the original NAS search space, where the goal is to maximize the achievable accuracy. The authors propose a RandomNAS-based approach, called EPS, which is based on the Proxy Search Space (PS). The authors show that the proposed algorithms can achieve better performance than the state-of-the-art on NASBench-201. They also show that EPS is able to achieve better search time than EPS on two tasks: image classification and natural language processing.","This paper proposes a new NAS approach to improve the accuracy of weight-sharing architectures. The authors propose a new search step to evaluate architectures in the global search space(GS) of the NAS search space. The architectures are based on the top-performing architectures in GS. The proposed RandomNAS-based approach, called EPS, uses a proxy search space (PS) to evaluate the performance of the architectures in terms of validation performance. They show that EPS outperforms state-of-the-art on NASBench-201 and outperforms other tasks in DARTS-like search spaces. They also show that RandomNAS’s search efficiency is better than the state of the-art. "
1467,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"it CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION it. imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation learning. imitation learning CONJUNCTION meta reinforcement learning. meta reinforcement learning CONJUNCTION imitation learning. imitation learning USED-FOR method. Probabilistic Embeddings USED-FOR method. PERIL USED-FOR exploration policies. Dual inference strategies USED-FOR PERIL. imitation learning COMPARE approach. approach COMPARE imitation learning. uncertainties FEATURE-OF it. it EVALUATE-FOR approach. meta - RL USED-FOR PERIL. sparse rewards FEATURE-OF meta - RL benchmarks. Method are Imitation learning, and meta reinforcement learning ( meta - RL ). Task is exploration. Material is interaction data. Metric is adaptation rates. ","This paper proposes a new method for meta-RL based on Probabilistic Embeddings (PERIL) that combines imitation learning with reinforcement learning. The main idea is to use PERIL to learn exploration policies from interaction data. PerIL is based on Dual inference strategies, and it is shown to outperform imitation learning and reinforcement learning in terms of uncertainty. The authors also show that PERIL is more robust to adaptation rates than imitation learning. ","This paper proposes a meta-RL approach to improve the performance of Imitation learning (IML) and reinforcement learning (REINFORCE) in the context of exploration. The proposed method is based on Probabilistic Embeddings (PERIL) and combines it with reinforcement learning and meta reinforcement learning. PERIL is able to learn exploration policies from interaction data. Dual inference strategies are used to optimize PERIL. The authors show that PERIL outperforms imitation learning in terms of the uncertainties of it, and it outperforms the approach of imitation learning. The paper also shows that the proposed approach is more robust to adaptation rates, and that it performs better on meta- RL benchmarks with sparse rewards."
1476,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"learning method USED-FOR image classification. overparameterized convolutional neural networks USED-FOR image classification. overparameterized convolutional neural networks HYPONYM-OF learning method. overparameterized convolutional neural networks CONJUNCTION gradient based optimization. gradient based optimization CONJUNCTION overparameterized convolutional neural networks. 3 - layer overparameterized convolutional network CONJUNCTION stochastic gradient descent ( SGD ). stochastic gradient descent ( SGD ) CONJUNCTION 3 - layer overparameterized convolutional network. orthogonal patches PART-OF images. 3 - layer overparameterized convolutional network USED-FOR images. pattern detectors CONJUNCTION detected patterns. detected patterns CONJUNCTION pattern detectors. SGD USED-FOR setting. pattern statistics USED-FOR dot - product. learning algorithm USED-FOR PSI. learning algorithm USED-FOR setting. sample complexity EVALUATE-FOR learning algorithm. overparameterized CNNs USED-FOR MNIST. non - orthogonal patches FEATURE-OF MNIST. non - orthogonal patches USED-FOR overparameterized CNNs. Task is image classification task. OtherScientificTerm are Pattern Statistics Inductive Bias ( PSI ), filter dimension, and VC dimension lower bound. Generic is it. ","This paper proposes a new learning method for image classification based on overparameterized convolutional neural networks, called Pattern Statistics Inductive Bias (PSI). The main idea is to learn a filter dimension for each image, and then use this filter dimension to learn the dot-product of each image. The authors show that this learning method can improve the sample complexity of the image classification task. The main contribution of the paper is to show that the learning algorithm for PSI can be applied to the setting of MNIST with non-orthogonal patches in MNIST. ",This paper proposes a new learning method for image classification using overparameterized convolutional neural networks and stochastic gradient descent (SGD). The key idea of the paper is to use Pattern Statistics Inductive Bias (PSI) to estimate the dot-product of images with orthogonal patches in the filter dimension. The authors show that the learning algorithm for PSI can reduce the sample complexity of this setting by a factor of 1.5. The paper also provides a VC dimension lower bound for the learning of overparametersized CNNs for MNIST with non-orthogonal patch. 
1485,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,approach USED-FOR representation learning. Contrastive learning USED-FOR representation learning. contrastive learning USED-FOR representation of documents. topic modeling assumptions FEATURE-OF document classification. topic posterior information FEATURE-OF representation of documents. procedure USED-FOR semi - supervised setup. linear classifiers USED-FOR document classification tasks. representations USED-FOR linear classifiers. OtherScientificTerm is embeddings of data. Method is linear models. ,"This paper proposes a new approach for representation learning based on contrastive learning. The authors propose a semi-supervised setup in which the embeddings of data are represented by linear classifiers, and the topic posterior information of the representation of documents is extracted from the topic modeling assumptions in document classification. The proposed procedure is based on the semi-Supervised setup. The paper shows that the proposed representations can be used to improve the performance of linear classifier on document classification tasks. ","This paper proposes a novel approach for representation learning based on contrastive learning. The authors propose a semi-supervised setup where the embeddings of data are represented by linear models, and the representation of documents is based on topic modeling assumptions for document classification under topic posterior information. The proposed procedure is evaluated on a number of document classification tasks, where the authors show that the proposed procedure can be applied to any semi-Supervised setup. The main contribution of the paper is the use of linear classifiers to learn representations for linear classifier tasks."
1494,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"vision CONJUNCTION language. language CONJUNCTION vision. Multimodal learning USED-FOR generative models. it USED-FOR generalisable representations. related ” multimodal data USED-FOR models. contrastive framework USED-FOR generative model learning. contrastive framework USED-FOR model. method USED-FOR multimodal learning. framework USED-FOR generative model. Task is learning generalisable representations. Method is multimodal variational autoencoder ( VAE ) models. Material is unlabeled, unpaired multimodal data. ","This paper proposes a contrastive framework for generative model learning from “related” multimodal data. Multimodal learning is an important problem in the field of generative models, as it can be used to learn generalisable representations from both vision and language. The authors propose a novel method to combine the learned representations from unlabeled, unpaired, and unlabeled data. The proposed framework can be applied to any generative framework for multi-modal learning, and is shown to improve the performance of VAE models.","This paper proposes a novel framework for learning generalisable representations from unlabeled, unpaired multimodal data. Multimodal learning is an important problem in generative models, and it is important to learn generative representations from “related” multimodals data. The authors propose a contrastive framework for generative model learning using the proposed model. The proposed framework is based on multimodality variational autoencoder (VAE) models. "
1503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"Variational autoencoders ( VAEs ) HYPONYM-OF likelihood - based generative models. base prior distribution CONJUNCTION reweighting factor. reweighting factor CONJUNCTION base prior distribution. reweighting factor USED-FOR energy - based prior. base prior distribution USED-FOR energy - based prior. it USED-FOR hierarchical VAEs. latent variable groups FEATURE-OF hierarchical VAEs. noise contrastive estimation USED-FOR reweighting factor. CIFAR-10 CONJUNCTION CelebA 64. CelebA 64 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CelebA 64 CONJUNCTION CelebA HQ 256 datasets. CelebA HQ 256 datasets CONJUNCTION CelebA 64. generative EVALUATE-FOR VAEs. noise contrastive priors USED-FOR VAEs. noise contrastive priors USED-FOR generative. MNIST EVALUATE-FOR VAEs. Generic is they. OtherScientificTerm are prior, tempering, prior hole problem, prior distribution, aggregate approximate posterior, latent space, and aggregate posterior. ","This paper proposes Variational autoencoders (VAEs), a family of likelihood-based generative models based on Variational Autoencoder (VAE). The authors show that VAEs can be used as a base prior distribution and a reweighting factor for an energy-based prior, and that it can be applied to hierarchical VAEs with latent variable groups. The authors also show that they can be combined with noise contrastive priors to improve the generative performance of VAEs on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.","This paper proposes Variational autoencoders (VAEs), a family of likelihood-based generative models. The authors show that VAEs can be seen as hierarchical VAEs with latent variable groups, and that they can be decomposed into a base prior distribution, a reweighting factor, and an energy-based prior with noise contrastive estimation. They also show that the prior distribution is a prior hole problem, where the prior is the aggregate approximate posterior over the latent space. The paper also shows that the generative of VAEs is a generative problem, and they show that it can be solved by using the reweighted factor. They evaluate VAEs on MNIST, CIFAR-10, CelebA 64 and CelebA HQ 256 datasets."
1512,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"convex regularizers USED-FOR learner ’s policy. degenerate solutions HYPONYM-OF constant rewards. convex regularizers USED-FOR Regularized IRL. constant rewards USED-FOR expert ’s behavior. practical methods USED-FOR them. them USED-FOR regularized IRL. practical methods USED-FOR regularized IRL. tractable solutions CONJUNCTION practical methods. practical methods CONJUNCTION tractable solutions. maximum - entropy IRL framework USED-FOR methods. Shannon - entropy regularizers USED-FOR them. theoretical backing USED-FOR IRL method. IRL method USED-FOR discrete and continuous controls. Method is Inverse Reinforcement Learning ( IRL ). OtherScientificTerm are expert behavior, and reward functions. Generic are solutions, and tasks. ","This paper studies the problem of Inverse Reinforcement Learning (IRL), where the goal is to learn an expert behavior that is consistent with the learner’s policy. Regularized IRL uses convex regularizers to learn a learner ‘s policy, and the constant rewards, such as degenerate solutions, are used to train them. The proposed methods are based on the maximum-entropy IRL framework. The theoretical backing for the proposed IRL method is based on theoretical backing. Theoretical results show that the proposed method can learn tractable solutions and practical methods to regularize IRL. Experiments are conducted on both discrete and continuous controls, and show the effectiveness of the proposed methods. ","This paper proposes Inverse Reinforcement Learning (IRL), a method for learning a learner’s policy with convex regularizers. Regularized IRL can be seen as a regularized version of convolutional neural networks with degenerate solutions, where the expert behavior is modeled as a function of the reward functions of the learner. The authors propose to use them to regularize IRL using a maximum-entropy IRL framework, and show that they can be combined with tractable solutions and practical methods. The theoretical backing is provided for the proposed IRL method, and experiments are conducted on discrete and continuous controls. "
1521,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"hard binary gates USED-FOR LS or shared paths. CLSR USED-FOR LS or shared paths. hard binary gates USED-FOR token representations. hard binary gates USED-FOR CLSR. translation signals CONJUNCTION budget constraints. budget constraints CONJUNCTION translation signals. LS capacity PART-OF MNMT. translation signals USED-FOR MNMT. CLSR COMPARE many - to - one translation. many - to - one translation COMPARE CLSR. one - to - many translation COMPARE many - to - one translation. many - to - one translation COMPARE one - to - many translation. LS computation USED-FOR top and/or bottom encoder / decoder layers. unbalanced training data USED-FOR many - to - one translation. LS modeling USED-FOR MNMT. OPUS-100 and WMT datasets EVALUATE-FOR Transformer. CLSR USED-FOR one - to - many translation. shared capacity CONJUNCTION LS capacity. LS capacity CONJUNCTION shared capacity. LS capacity USED-FOR multilingual translation. Task is multilingual neural machine translation ( MNMT ). Method are conditional language - specific routing ( CLSR ), and multilingual Transformers. Generic is gates. ",This paper studies the problem of multilingual neural machine translation (MNMT) in the context of conditional language-specific routing (CLSR). CLSR is an extension of CLSR that uses hard binary gates to learn LS or shared paths between token representations. The main contribution of the paper is to combine the translation signals and budget constraints in MNMT with the LS capacity in CLSR. The authors show that the LS computation can be used to learn top and/or bottom encoder/decoder layers of the Transformer. The LS modeling is also used to improve the performance of MNMT on the OPUS-100 and WMT datasets. The paper also shows that CLSR can achieve better performance than one-to-many translation with shared capacity and LS capacity for multilingual translation. ,"This paper proposes conditional language-specific routing (CLS) for multilingual neural machine translation (MNMT). CLSR uses hard binary gates to encode token representations. The authors show that CLSR is more efficient than many-to-one translation, and that the LS capacity of MNMT is larger than the shared capacity and the budget constraints of CLSR. The main contribution of the paper is the use of LS computation for the top and/or bottom encoder/decoder layers of multilingual Transformers. Experiments on the OPUS-100 and WMT datasets show that the Transformer outperforms CLSR in terms of shared capacity, LS capacity, and budget constraints. "
1530,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"Wasserstein distributional normalization ( WDN ) algorithm USED-FOR noisy labels. Wasserstein distributional normalization ( WDN ) algorithm USED-FOR accurate classification. noisy labels USED-FOR accurate classification. geometric constraints FEATURE-OF uncertain samples. Wasserstein ball USED-FOR them. WDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE WDN. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR state - of - the - art methods. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR WDN. WDN COMPARE classification methods. classification methods COMPARE WDN. accuracy EVALUATE-FOR it. accuracy EVALUATE-FOR WDN. OtherScientificTerm are small loss criteria, geometric relationship, and diverse noisy labels. Generic is relation. ",This paper proposes a new Wasserstein distributional normalization (WDN) algorithm for accurate classification with noisy labels. WDN is based on geometric constraints on uncertain samples with geometric constraints. The authors show that the small loss criteria can be used to define the geometric relationship between the noisy labels and the true label. They also provide a relation between the relation and the Wassersteins ball. The paper shows that WDN performs better than state-of-the-art methods on Clothing1 M and CIFAR-10/100 datasets. ,"This paper proposes a Wasserstein distributional normalization (WDN) algorithm for noisy labels for accurate classification. WDN is based on the small loss criteria, which are defined as the geometric relationship between a pair of uncertain samples with different geometric constraints. The relation between the two sets of unknown samples is defined as a relation between them. The authors show that WDN outperforms state-of-the-art methods on Clothing1 M and CIFAR-10/100 datasets. They also show that it improves the accuracy of classification methods. "
1539,SP:e0029422e28c250dfb8c62c29a15b375030069e8,predictive accuracy EVALUATE-FOR Convolutional image classifiers. uncertainty quantification techniques USED-FOR probability estimates. uncertainty quantification techniques USED-FOR network. network USED-FOR probability estimates. Platt scaling HYPONYM-OF uncertainty quantification techniques. algorithm USED-FOR predictive set. algorithm USED-FOR classifier. user - specified probability FEATURE-OF predictive set. algorithm COMPARE Platt scaling. Platt scaling COMPARE algorithm. formal finite - sample coverage guarantee FEATURE-OF model. formal finite - sample coverage guarantee FEATURE-OF algorithm. conformal prediction algorithm USED-FOR method. scheme COMPARE approaches. approaches COMPARE scheme. scheme COMPARE stand - alone Platt scaling baseline. stand - alone Platt scaling baseline COMPARE scheme. Imagenet - V2 EVALUATE-FOR scheme. Imagenet CONJUNCTION Imagenet - V2. Imagenet - V2 CONJUNCTION Imagenet. Imagenet EVALUATE-FOR scheme. ResNet-152 CONJUNCTION classifiers. classifiers CONJUNCTION ResNet-152. classifiers EVALUATE-FOR scheme. Imagenet - V2 EVALUATE-FOR approaches. Imagenet - V2 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION Imagenet - V2. ResNet-152 EVALUATE-FOR scheme. Imagenet - V2 CONJUNCTION classifiers. classifiers CONJUNCTION Imagenet - V2. coverage EVALUATE-FOR approaches. coverage EVALUATE-FOR scheme. OtherScientificTerm is formal guarantees. ,"This paper studies the problem of predicting the accuracy of Convolutional image classifiers. The authors propose a new algorithm, Platt scaling, which uses uncertainty quantification techniques to train a network to estimate the probability estimates of a given image. The proposed algorithm is based on a conformal prediction algorithm, and is able to learn a predictive set with a user-specified probability of the predicted image, which is then used to train the classifier. The paper provides a formal finite-sample coverage guarantee for the model, and shows that the proposed scheme achieves better coverage than existing approaches such as Imagenet-V2, Imagenets-V1, and ResNet-152. ","This paper studies the problem of predicting the accuracy of Convolutional image classifiers. The authors propose to use uncertainty quantification techniques to improve the probability estimates of the network. Platt scaling is one of the most popular methods for estimating the probability of a classifier. The paper proposes a new algorithm to estimate the predictive set with user-specified probability. The proposed algorithm is based on the formal finite-sample coverage guarantee of the model. The method uses a conformal prediction algorithm. The scheme is evaluated on Imagenet, V2, and ResNet-152. The results show that the proposed scheme outperforms other approaches in terms of coverage. "
1548,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"algorithm USED-FOR Wasserstein-2 barycenters. input convex neural networks CONJUNCTION cycle - consistency regularization. cycle - consistency regularization CONJUNCTION input convex neural networks. entropic or quadratic regularization USED-FOR approaches. minimax optimization USED-FOR approach. low - dimensional qualitative scenarios CONJUNCTION high - dimensional quantitative experiments. high - dimensional quantitative experiments CONJUNCTION low - dimensional qualitative scenarios. high - dimensional quantitative experiments EVALUATE-FOR approach. low - dimensional qualitative scenarios EVALUATE-FOR approach. OtherScientificTerm are Wasserstein barycenters, and error bounds. Method is optimal transport. ",This paper proposes a new algorithm for optimizing Wasserstein-2 barycenters. The main idea is to use entropic or quadratic regularization instead of input convex neural networks and cycle-consistency regularization. The proposed approach is based on minimax optimization and is shown to achieve optimal transport. The experimental results on low-dimensional qualitative scenarios and high-dimensional quantitative experiments demonstrate the effectiveness of the proposed approach.,"This paper proposes a new algorithm for learning Wasserstein-2 barycenters. The main idea is to use the optimal transport between input convex neural networks and cycle-consistency regularization. Previous approaches are based on entropic or quadratic regularization, and the proposed approach is based on minimax optimization. Experiments on low-dimensional qualitative scenarios and high-dimensional quantitative experiments show the effectiveness of the approach. "
1557,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"multiple manifold problem HYPONYM-OF binary classification task. deep fully - connected neural network USED-FOR multiple manifold problem. randomly - initialized gradient descent USED-FOR manifolds. randomlyinitialized network CONJUNCTION gradients. gradients CONJUNCTION randomlyinitialized network. nonasymptotic framework USED-FOR generalization of networks. neural tangent kernel PART-OF deep fullyconnected ReLU networks. NTK regime FEATURE-OF generalization of networks. structured data USED-FOR generalization of networks. martingale concentration USED-FOR statistical dependencies. approach USED-FOR network architectures. Task are machine vision, and practically - motivated model problem. OtherScientificTerm are manifold configuration, network depth L, geometric and statistical properties of the data, network width n, i.i.d. samples, depth, fitting resource, class manifolds, width, and statistical resource. Method are nonasymptotic analysis of training overparameterized neural networks, and random network. ","This paper proposes a nonasymptotic analysis of training overparameterized neural networks. The main idea is to use a deep fully-connected neural network to solve a binary classification task called the multiple manifold problem. The manifold configuration is defined as a set of manifolds with a randomly-initialized gradient descent on the manifolds and a randomly initialized network and gradients. The network depth L is the width of the manifold, and the network width n is the distance between the two manifolds. The goal of the paper is to show that the generalization of networks in the NTK regime under structured data can be improved by using a neural tangent kernel in the deep fullyconnected ReLU networks.  The main contribution of this paper is a theoretical analysis of the statistical properties of the data. The paper shows that the statistical dependencies of the network are defined by the martingale concentration of the parameters of the random network. The authors then propose a new approach to train the network architectures. The proposed approach is based on a practically-motivated model problem. ","This paper proposes a novel nonasymptotic analysis of training overparameterized neural networks. The authors propose a novel approach to study the generalization of networks on structured data, where the manifold configuration of the data is a function of the network depth L, and the network width n is the number of samples. The main idea is to use a randomly-initialized gradient descent to learn manifolds of different widths, and then use the randomly initialized network to compute the gradients of the manifolds. This is a practically-motivated model problem. The generalization is performed in the NTK regime, where a neural tangent kernel is used to train deep fullyconnected ReLU networks. "
1566,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"supervised learning methods USED-FOR subroutines. subroutines USED-FOR reinforcement learning algorithm. supervised learning methods USED-FOR reinforcement learning algorithm. weighted target actions USED-FOR policy. supervised learning steps PART-OF approach. one HYPONYM-OF supervised learning steps. supervised learning methods USED-FOR method. experience replay USED-FOR off - policy data. it COMPARE RL algorithms. RL algorithms COMPARE it. AWR COMPARE RL algorithms. RL algorithms COMPARE AWR. OpenAI Gym benchmark tasks EVALUATE-FOR AWR. AWR USED-FOR policies. AWR COMPARE off - policy algorithms. off - policy algorithms COMPARE AWR. environmental interactions FEATURE-OF static datasets. off - policy algorithms USED-FOR policies. complex simulated characters FEATURE-OF continuous control tasks. continuous control tasks EVALUATE-FOR algorithm. Method is advantage - weighted regression ( AWR ). OtherScientificTerm are value function, and continuous and discrete actions. ","This paper proposes a new reinforcement learning algorithm based on subroutines. The proposed method is based on supervised learning methods for subroutine learning. The key idea is to use weighted target actions to learn a policy from the experience replay of off-policy data, where the value function is a weighted sum of a set of actions. The approach is a combination of two supervised learning steps, one for continuous and discrete actions, and one for discrete actions. Experimental results on OpenAI Gym benchmark tasks show that the proposed AWR can learn policies with better performance than existing off -policy algorithms on a variety of continuous control tasks with complex simulated characters. ",This paper proposes a new reinforcement learning algorithm based on supervised learning methods for subroutines. The proposed method is based on advantage-weighted regression (AWR). The approach consists of two supervised learning steps: one for learning the value function and one for training the policy. The value function is learned using weighted target actions. Experiments on OpenAI Gym benchmark tasks show that it outperforms other RL algorithms on the off-policy data and on the experience replay. The algorithm is also evaluated on continuous control tasks with complex simulated characters and on static datasets with environmental interactions. The results show that AWR can learn policies with better performance than other off -policy algorithms on both continuous and discrete actions.
1575,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,Heterogeneous assignment of bitwidths FEATURE-OF layers. parametrized sinusoidal regularizer USED-FOR WaveQ. WaveQ USED-FOR quantized weights. training USED-FOR stochastic gradient descent. sinusoidal regularizer USED-FOR stochastic gradient descent. quantized weights CONJUNCTION heterogeneous bitwidths. heterogeneous bitwidths CONJUNCTION quantized weights. WaveQ HYPONYM-OF gradient - based mechanism. gradient - based mechanism USED-FOR quantized weights. gradient - based mechanism USED-FOR heterogeneous bitwidths. ResNet-20 CONJUNCTION SVHN. SVHN CONJUNCTION ResNet-20. ResNet-18 CONJUNCTION ResNet-20. ResNet-20 CONJUNCTION ResNet-18. MobileNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION MobileNet. heterogeneous bitwidth assignment USED-FOR quantization. CIFAR10 CONJUNCTION MobileNet. MobileNet CONJUNCTION CIFAR10. AlexNet CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION AlexNet. SVHN CONJUNCTION VGG-11. VGG-11 CONJUNCTION SVHN. compute efficiency CONJUNCTION accuracy. accuracy CONJUNCTION compute efficiency. WaveQ USED-FOR heterogeneous bitwidth assignment. heterogeneous bitwidth assignment USED-FOR deep networks. WaveQ USED-FOR deep networks. accuracy EVALUATE-FOR WaveQ. compute efficiency EVALUATE-FOR WaveQ. VGG-11 HYPONYM-OF deep networks. AlexNet HYPONYM-OF deep networks. SVHN HYPONYM-OF deep networks. MobileNet HYPONYM-OF deep networks. ResNet-20 HYPONYM-OF deep networks. CIFAR10 HYPONYM-OF deep networks. ResNet-18 HYPONYM-OF deep networks. predetermined bitwidths USED-FOR WaveQ. DoReFa CONJUNCTION WRPN. WRPN CONJUNCTION DoReFa. accuracy EVALUATE-FOR quantized training algorithms. quantized training algorithms EVALUATE-FOR,"This paper studies the problem of quantization in deep networks with heterogeneous assignment of bitwidths in layers. The authors propose WaveQ, a parametrized sinusoidal regularizer for stochastic gradient descent. WaveQ is a gradient-based mechanism for quantized weights and heterogeneous bitwidth. The paper shows that WaveQ improves the accuracy and compute efficiency of existing quantized training algorithms in terms of the number of quantized parameters. ","This paper proposes WaveQ, a parametrized sinusoidal regularizer for stochastic gradient descent. WaveQ is a gradient-based mechanism for learning quantized weights with heterogeneous assignment of bitwidths in layers. The authors show that WaveQ can be used in training to improve the accuracy and compute efficiency of quantized training algorithms. The paper also provides a theoretical analysis of heterogeneous bitwidth assignment for quantization. "
1584,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"data augmentation methods USED-FOR translation. monolingual data USED-FOR data augmentation methods. data augmentation methods USED-FOR neural machine translation ( NMT ). in - domain monolingual data USED-FOR it. backtranslation HYPONYM-OF data augmentation methods. data augmentation method USED-FOR neural machine translation. small and large scale datasets EVALUATE-FOR method. method COMPARE baseline models. baseline models COMPARE method. small and large scale datasets EVALUATE-FOR baseline models. Method is neural machine translation models. OtherScientificTerm are aligned word pairs, and bilingual embeddings. ","This paper proposes a new data augmentation method for neural machine translation (NMT) based on monolingual data. The proposed method, called backtranslation, is based on the idea that aligned word pairs can be used to improve the performance of NMT. The authors show that it can be applied to both in-domain and out-of-domain data. They also show that the proposed method performs better than baseline models on both small and large scale datasets.",This paper proposes a data augmentation method for neural machine translation (NMT) based on in-domain monolingual data. The idea is to augment the translation with aligned word pairs. The authors show that it is equivalent to backtranslation. They also show that the proposed method outperforms baseline models on both small and large scale datasets. 
1593,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"Federated learning USED-FOR distributed data privacy. data quantity CONJUNCTION data quality. data quality CONJUNCTION data quantity. Shapley Value PART-OF game theory. Shapley Value COMPARE method. method COMPARE Shapley Value. maintaining real - time EVALUATE-FOR method. data quality EVALUATE-FOR method. data quantity EVALUATE-FOR method. Method are contribution measurement mechanism, real - time contribution measurement method, and pseudo - distributed training. Generic is mechanism. OtherScientificTerm is contribution rate. Material is Penn Treebank dataset. ",This paper proposes a new contribution measurement mechanism for distributed data privacy. The proposed mechanism is based on the Shapley Value in game theory. The authors propose a real-time contribution measurement method that can be applied to pseudo-distributed training. They show that the proposed method achieves better data quantity and data quality than Shapley value while maintaining real-times. ,"This paper proposes a new contribution measurement mechanism for distributed data privacy. The authors propose a real-time contribution measurement method, which is based on the Shapley Value in game theory. They show that the proposed method is more robust to changes in data quantity and data quality than the existing method. They also show that their contribution rate is more sensitive to changes of the contribution rate than Shapley value. The paper also shows that their method is robust to change in the data quantity, and that their proposed method can be used for maintaining real-times. "
1602,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"graph structure FEATURE-OF fully - observable case. nearlylinear time algorithm USED-FOR problem. dimension - independent error guarantee FEATURE-OF nearlylinear time algorithm. error guarantees EVALUATE-FOR robust algorithms. robust learning of Bayesian networks CONJUNCTION robust mean estimation. robust mean estimation CONJUNCTION robust learning of Bayesian networks. Task is learning Bayesian networks. Method are Bayesian networks, Bayesian network, and robust mean estimation algorithm. Generic is algorithm. ","This paper studies the problem of learning Bayesian networks with graph structure in a fully-observable case. The authors propose a nearlylinear time algorithm for this problem with dimension-independent error guarantee. They show that the proposed algorithm is robust to perturbations in the Bayesian network. They also provide a robust mean estimation algorithm for the same problem. Finally, they provide some theoretical guarantees on the error guarantees of the proposed robust algorithms. ",This paper studies the problem of learning Bayesian networks in a fully-observable case with graph structure. The authors propose a nearlylinear time algorithm to solve the problem. The proposed algorithm is based on the dimension-independent error guarantee of the Bayesian network. The robust mean estimation algorithm is also proposed. Experiments show that the proposed algorithm outperforms other robust algorithms in terms of error guarantees. 
1611,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"raw high - dimensional observations USED-FOR autonomous agents. images HYPONYM-OF raw high - dimensional observations. shaped reward functions USED-FOR model - based reinforcement learning ( RL ). short - horizon reasoning USED-FOR shaped reward functions. trajectory optimization USED-FOR long - horizon reasoning. it USED-FOR image - based setting. probabilistic latent variable models USED-FOR algorithm. probabilistic latent variable models USED-FOR it. approach USED-FOR longer - horizon visual planning. latent collocation method ( LatCo ) USED-FOR approach. latent collocation method ( LatCo ) USED-FOR longer - horizon visual planning. approach COMPARE prior model - based approaches. prior model - based approaches COMPARE approach. sparse rewards CONJUNCTION long - term goals. long - term goals CONJUNCTION sparse rewards. sparse rewards FEATURE-OF visual control tasks. long - term goals FEATURE-OF visual control tasks. visual control tasks EVALUATE-FOR prior model - based approaches. visual control tasks EVALUATE-FOR approach. Method are temporally extended reasoning, myopic, short - sighted planning, and collocation - based planning. OtherScientificTerm is latent variables. ",This paper proposes a new approach for longer-horizon visual planning based on a latent collocation method (LatCo) for the task of temporally extended reasoning. The proposed approach is based on the idea of using a set of sparse rewards and long-term goals to guide the learning of the agent’s actions. The authors show that the proposed approach outperforms prior model-based approaches on a variety of visual control tasks. ,"This paper proposes a new approach for longer-horizon visual planning based on a latent collocation method (LatCo) that learns a set of latent variables that can be used for temporally extended reasoning. The key idea of the approach is to learn the shape of the latent variables in the image-based setting, and then use the learned shape of these latent variables to guide the agent in the long horizon planning. The authors show that the proposed approach outperforms prior model-based approaches on several visual control tasks with sparse rewards and long-term goals. "
1620,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"Bayesian neural networks COMPARE neural networks. neural networks COMPARE Bayesian neural networks. tempered ” or “ cold ” posterior USED-FOR uncertainty. BNNs USED-FOR image classification. CIFAR-10 HYPONYM-OF image benchmark datasets. generative model USED-FOR curation. generative model USED-FOR Bayesian account of cold posteriors. likelihood COMPARE tempered likelihoods. tempered likelihoods COMPARE likelihood. generative model USED-FOR likelihood. Method is Bayesian inference / decision theory. OtherScientificTerm are posterior, and prior. ","This paper studies Bayesian inference/decision theory in the context of Bayesian neural networks. The authors propose a generative model for curation of a Bayesian account of cold posteriors. The posterior is a “tempered” or “cold” posterior, which is a measure of uncertainty. The likelihood of the posterior is computed by a generatively trained model.  The authors show that the likelihood is better than the tempered likelihoods. They also show that BNNs can be used for image classification on CIFAR-10.","This paper proposes a Bayesian inference/decision theory for the problem of uncertainty in Bayesian neural networks. The authors propose a “tempered” or “cold” posterior to reduce uncertainty in BNNs for image classification. They show that the likelihood of a generative model trained on CIFAR-10 is similar to that of the prior. They also show that this likelihood is equivalent to the tempered likelihoods. Finally, they show that curation can be performed using the generative models. "
1629,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,non - autoregressive neural machine translation COMPARE autoregressive machine translation. autoregressive machine translation COMPARE non - autoregressive neural machine translation. GPUs USED-FOR autoregressive machine translation. latter COMPARE former. former COMPARE latter. non - autoregressive models COMPARE autoregressive baselines. autoregressive baselines COMPARE non - autoregressive models. translation quality - speed tradeoffs EVALUATE-FOR autoregressive baselines. translation quality - speed tradeoffs EVALUATE-FOR non - autoregressive models. accuracy EVALUATE-FOR autoregressive baselines. encoders USED-FOR autoregressive models. single - layer autoregressive decoder COMPARE non - autoregressive models. non - autoregressive models COMPARE single - layer autoregressive decoder. inference speed EVALUATE-FOR single - layer autoregressive decoder. inference speed EVALUATE-FOR non - autoregressive models. suboptimal layer allocation CONJUNCTION insufficient speed measurement. insufficient speed measurement CONJUNCTION suboptimal layer allocation. autoregressive baselines COMPARE non - autoregressive methods. non - autoregressive methods COMPARE autoregressive baselines. speed disadvantage EVALUATE-FOR autoregressive baselines. speed disadvantage EVALUATE-FOR non - autoregressive methods. OtherScientificTerm is knowledge distillation. Task is machine translation. ,This paper studies the problem of machine translation in the context of knowledge distillation. The authors show that autoregressive neural machine translation outperforms non-autoregressive machine translation when the number of encoders is small. They also show that the translation quality-speed tradeoffs of autoreprogressive baselines are better than those of the non-auto-regressive models. ,"This paper studies the problem of knowledge distillation in machine translation. The authors show that autoregressive machine translation outperforms non-autoregressive neural machine translation in terms of accuracy and translation quality-speed tradeoffs. In particular, they show that the former outperforms the latter when the number of encoders is small, while the latter outperforms autoreprogressive models when the size of the encoder is large. They also show that a single-layer autororegressive decoder is more efficient than a non-auto-regressive decoder, and that the inference speed of the non-adversarial models is better than the speed disadvantage of the autorative baselines."
1638,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"test error EVALUATE-FOR deep neural network ( DNN ). bell - shaped variance USED-FOR model - wise double descent. test error EVALUATE-FOR DNN. test error HYPONYM-OF epoch - wise double descent. bias - variance analysis USED-FOR epoch - wise double descent. variance USED-FOR zero - one loss. metric USED-FOR diversity of model updates. stochastic gradients of random training batches USED-FOR diversity of model updates. It USED-FOR generalization ability. It USED-FOR DNN. generalization ability EVALUATE-FOR DNN. It USED-FOR early stopping. zero - one loss USED-FOR DNN. validation set USED-FOR early stopping. Method are statistical learning theory, and bias - variance decomposition. OtherScientificTerm are double descent, model complexity, U - shaped curve, OV, and unknown ) test error. Generic is descent. Metric is optimization variance ( OV ). ","This paper studies the problem of estimating the test error of a deep neural network (DNN) trained with bell-shaped variance. The authors propose a bias-variance analysis for epoch-wise double descent based on bias-varied analysis. They show that under certain assumptions on the double descent, the error is bounded by a U-shaped curve. They then propose a new metric for measuring the diversity of model updates based on stochastic gradients of random training batches. It is shown that the new metric improves the generalization ability of a DNN trained with a zero-one loss on the validation set. ","The paper proposes a new metric for measuring the diversity of model updates in a deep neural network (DNN). The metric is based on the bell-shaped variance of the epoch-wise double descent, which is a well-studied metric in statistical learning theory. The authors show that under certain assumptions on the model complexity and the O-shaped curve, the OV of the DNN converges to the (unknown) test error. It is also shown that the zero-one loss of a DNN can be used as a validation set for early stopping. "
1647,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"meta - learning techniques PART-OF few - shot learning. It USED-FOR meta - initialization of model parameters. labeled training data USED-FOR It. labeled training data USED-FOR meta - initialization of model parameters. few - shot learning USED-FOR MAML. MAML USED-FOR adversarial robustness. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. robustness USED-FOR meta - model. adversarial robustness FEATURE-OF MAML. robustness USED-FOR task - specific fine - tuning stage. training protocol USED-FOR latter. robust regularization USED-FOR MAML. fast adversarial attack generation CONJUNCTION computationally - light fine - tuning. computationally - light fine - tuning CONJUNCTION fast adversarial attack generation. unlabeled data augmentation CONJUNCTION fast adversarial attack generation. fast adversarial attack generation CONJUNCTION unlabeled data augmentation. auxiliary contrastive learning task USED-FOR MAML. auxiliary contrastive learning task USED-FOR adversarial robustness. adversarial robustness FEATURE-OF MAML. methods USED-FOR robust few - shot learning. OtherScientificTerm are robustness - promoting regularization, and meta - update stage. Metric is robustness adaptation. ","This paper studies the problem of few-shot learning with meta-learning techniques in the context of adversarial robustness. It focuses on the meta-initialization of model parameters on labeled training data. The authors propose a new MAML based on the idea of robustness-promoting regularization, which aims to improve the robustness of a meta-model in the presence of the adversarial perturbations. It is shown that the proposed robustness adapts the task-specific fine-tuning stage of the latter by using a training protocol that is based on robust regularization. The paper also provides a theoretical analysis of the performance of the proposed methods in the setting of robust few-Shot learning. ","This paper proposes a meta-learning techniques for few-shot learning. It uses labeled training data for meta-initialization of model parameters and adversarial robustness for training the meta-model. The authors show that MAML is robust to adversarial perturbations of the training data and robustness to the task-specific fine-tuning stage. They also show that robustness-promoting regularization can be used to improve the robustness adaptation of the final meta-update stage. Finally, the authors propose a new training protocol for the latter, which is based on robust regularization. Experiments show that the proposed methods are effective for robust few-Shot learning. "
1656,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"optimization algorithms USED-FOR optimizer. Learning - to - learn USED-FOR optimizers. optimization algorithms USED-FOR Learning - to - learn. metagradient descent USED-FOR meta - objective. trajectory USED-FOR metagradient descent. metagradient descent USED-FOR approach. step size USED-FOR quadratic loss. backpropagation USED-FOR meta - gradient. neural networks USED-FOR learned optimizers. OtherScientificTerm are metagradient explosion / vanishing problems, metagradient, and numerical issues. Method is learning - to - learn approach. Task is gradient explosion / vanishing problems. ","This paper proposes a new learning-to-learn approach for the problem of gradient explosion/vanishing problems. The authors propose a new optimization algorithms for the optimizer based on the optimization algorithms used in Learning-To-learn for optimizers trained on neural networks. The approach is based on metagradient descent, where the meta-objective is learned by using a trajectory that follows the trajectory of the metragradient. The meta-gradient is computed by backpropagation, and the step size of the quadratic loss depends on the number of steps in the trajectory. The proposed approach is evaluated on a number of benchmark problems and shows that the proposed approach outperforms the state-of-the-art.",This paper proposes a learning-to-learn approach to solve the gradient explosion/vanishing problems. The authors propose two optimization algorithms for learning optimizers. The first one is a meta-gradient based on backpropagation. The second one is an approach based on metagradient descent to achieve the meta-objective. The main idea is to use a trajectory to learn the meta objective. The step size of the quadratic loss is also used. The proposed learned optimizers are trained on neural networks.
1665,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"methods USED-FOR large and complex graph problems. neural networks USED-FOR methods. dual views USED-FOR representations. neighborhood aggregation capability FEATURE-OF GVCLN. loss functions CONJUNCTION supervised loss. supervised loss CONJUNCTION loss functions. loss functions USED-FOR view - consistent representations. view - consistent loss USED-FOR consistent representation. view - consistent loss USED-FOR views. supervised loss CONJUNCTION view - consistent loss. view - consistent loss CONJUNCTION supervised loss. view - consistent loss CONJUNCTION pseudo - label loss. pseudo - label loss CONJUNCTION view - consistent loss. known labeled set USED-FOR supervised loss. common high - confidence predictions USED-FOR pseudo - label loss. GVCLN USED-FOR view - consistent representations. loss functions USED-FOR view - consistent representations. loss functions USED-FOR GVCLN. Citeseer CONJUNCTION PubMed. PubMed CONJUNCTION Citeseer. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. node classification tasks EVALUATE-FOR GVCLN. Task are acquisition of ground - truth labels, semisupervised learning, and classification tasks. OtherScientificTerm are viewing angles, observation objects, observation representations, and node features. ","This paper studies the problem of acquisition of ground-truth labels for large and complex graph problems. The authors propose two methods to solve this problem using neural networks. The first method, GVCLN, uses dual views to learn representations from dual views. The second method, pseudo-label loss, uses common high-confidence predictions to learn a consistent representation from a view-consistent loss and a supervised loss from a known labeled set. The proposed method is evaluated on node classification tasks on Citeseer and PubMed.","This paper presents a new method for learning large and complex graph problems. The proposed methods are based on neural networks. The main idea is to use dual views to learn representations that are consistent across different viewing angles and observation objects. The authors propose a GVCLN with neighborhood aggregation capability, which is an extension of the neighborhood-agnostic neighborhood loss. The paper also proposes a view-consistent loss to learn a consistent representation of views. The supervised loss is based on a known labeled set, and the pseudo-label loss uses common high-confidence predictions. Experiments are conducted on Citeseer, PubMed, and Cora, showing that the proposed method outperforms other methods in terms of acquisition of ground-truth labels, semisupervised learning, and classification tasks."
1674,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"physics bias USED-FOR neural networks. neural networks USED-FOR dynamics of systems. coordinates USED-FOR conserved quantities. cyclic coordinates HYPONYM-OF coordinates. Hamiltonian dynamics USED-FOR classical systems. canonical transformations USED-FOR coordinates. Hamiltonian dynamics USED-FOR loss functions. loss functions USED-FOR coordinates. network USED-FOR conserved quantities. network COMPARE networks. networks COMPARE network. network USED-FOR dynamics of the system. Hamiltonian USED-FOR networks. classical physics systems EVALUATE-FOR method. synthetic and experimental data EVALUATE-FOR method. symmetry orbits PART-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF symmetry orbits. analytic formulae USED-FOR networks. conserved quantities USED-FOR networks. ( angular ) momentum HYPONYM-OF conserved quantities. OtherScientificTerm are dynamics, and symmetries. Task is description of physical systems. ","This paper studies the physics bias of neural networks in the dynamics of systems. The authors propose a new Hamiltonian dynamics for classical systems based on canonical transformations. The proposed network is able to learn conserved quantities such as (angular) momentum and (cyclic) coordinates. Theoretically, the authors show that the proposed network can learn the conserved quantity better than existing networks on analytic formulae. Experimental results on classical physics systems demonstrate the effectiveness of the proposed method on both synthetic and experimental data. ",This paper studies the physics bias in neural networks for modeling the dynamics of systems. The authors propose to use the Hamiltonian dynamics of classical systems to learn the coordinates for conserved quantities such as cyclic coordinates and canonical transformations. The proposed method is evaluated on classical physics systems and on synthetic and experimental data. They show that the proposed network can learn the conserved quantity of (angular) momentum and (symmetric) symmetry orbits in the phase space and lower dimensional sub-spaces of phase space. They also show that their network is more robust to physics bias than other networks. 
1683,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"models USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) HYPONYM-OF models. gradient boosted decision trees ( GBDT ) COMPARE machine learning methods. machine learning methods COMPARE gradient boosted decision trees ( GBDT ). heterogeneous tabular data EVALUATE-FOR machine learning methods. approach USED-FOR graphs with tabular node features. GNN models USED-FOR networks. homogeneous sparse features FEATURE-OF networks. GBDT CONJUNCTION GNN. GNN CONJUNCTION GBDT. architecture USED-FOR GBDT. architecture USED-FOR GNN. GBDT model USED-FOR heterogeneous features. GNN USED-FOR graph structure. endto - end optimization USED-FOR model. Material is heterogeneous setting. OtherScientificTerm are trees, and graphs with tabular features. Method is GBDT and GNN models. ","This paper proposes a novel approach to learn graphs with tabular node features from heterogeneous tabular data. Graph neural networks (GNNs) are well-known models for graph representation learning tasks, but they are not well-suited for the heterogeneous setting. The authors propose to use gradient boosted decision trees (GBDT) and GNN models to learn these networks with homogeneous sparse features. The architecture of GBDT and the GNN is based on the GBDT model to learn heterogeneous features and the graph structure. The proposed model is trained by endto-end optimization. The experimental results demonstrate the effectiveness of the proposed method.","This paper presents a novel approach to learning graphs with tabular node features. Graph neural networks (GNNs) are popular models for graph representation learning tasks. The authors show that gradient boosted decision trees (GBDT) outperform other machine learning methods on heterogeneous tabular data. However, GBDT and GNN models have homogeneous sparse features, which is not the case in the heterogeneous setting. To address this issue, the authors propose a novel architecture for GNN to learn the graph structure. The proposed model is based on endto-end optimization. The experiments show that the proposed GBDT model is able to learn heterogeneous features."
1692,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"Meta - learning USED-FOR fast adaptation. train - validation split USED-FOR meta - learning. train - validation split COMPARE non - splitting method. non - splitting method COMPARE train - validation split. training EVALUATE-FOR non - splitting method. per - task data USED-FOR non - splitting method. train - validation split USED-FOR linear centroid meta - learning problem. splitting method COMPARE non - splitting method. non - splitting method COMPARE splitting method. regularization parameter CONJUNCTION split ratio. split ratio CONJUNCTION regularization parameter. split ratio USED-FOR methods. regularization parameter USED-FOR methods. asymptotic excess risk EVALUATE-FOR non - splitting method. non - splitting method COMPARE splitting method. splitting method COMPARE non - splitting method. Generic are predictor, and model. Method are linear models, splitting and non - splitting methods, and data splitting. OtherScientificTerm is data distribution. ","This paper studies the problem of fast adaptation in meta-learning with train-validation split for linear models. The authors propose a new non-splitting method based on per-task data, and show that the training of the non-sparing method outperforms the splitting method in terms of asymptotic excess risk. The main contribution of the paper is a theoretical analysis that shows that the split ratio and the regularization parameter of the two methods can be used to improve the performance of both methods. ","This paper studies the problem of meta-learning for fast adaptation in linear models. The authors propose a new training method that uses a train-validation split to solve the linear centroid meta-Learning problem. The main idea is to split the training data into two parts: 1) the predictor and the model, and 2) the data distribution. The splitting and non-splitting methods are based on the same regularization parameter and the same split ratio. The non-spinning method is based on per-task data, and the splitting method uses the same training as the non-split method. The results show that the non splitting method is more robust to asymptotic excess risk than the split method."
1701,SP:bb566eda95867f83a80664b2f685ad373147c87b,"noisy training data USED-FOR hard confident examples. physics USED-FOR momentum. non - simple patterns PART-OF hard confident examples. Me - Momentum USED-FOR hard confident examples. classification EVALUATE-FOR Me - Momentum. OtherScientificTerm are decision boundary, hard examples, and simple patterns. Method are classifiers, deep learning paradigm, deep neural networks, and classifier. Task are Extracting confident examples, and extracting hard confident examples. Material are noisy labels, and inaccurately labeled examples. Generic is approach. ","This paper studies the problem of extracting confident examples from noisy training data from hard confident examples with non-simple patterns. The authors propose a new deep learning paradigm, called Me-Momentum, where the decision boundary between the hard examples and the simple patterns is defined by the physics of momentum. The key idea is to use Me-momentum to extract hard examples from the noisy labels, and then use a classifier trained on these hard examples to classify them. Extracting confident examples is a challenging problem for deep neural networks, and the authors propose an approach to solve this problem. The proposed approach is evaluated on a variety of classification tasks, and it is shown that the proposed method outperforms existing methods.","This paper proposes a method for extracting hard confident examples from noisy training data. The idea is to use Me-Momentum to learn hard confident example from non-simple patterns in the decision boundary. The authors propose a deep learning paradigm, where the classifiers are trained on a set of noisy labels, and the goal is to learn a classifier that is robust to noisy labels. The main contribution of the paper is to introduce Me-momentum, which is an extension of Me-Gradientum to the case of hard examples. The key idea is that the momentum of the classifier depends on the physics of the momentum, and that the hard examples can be extracted from the set of simple patterns. Extracting confident examples is a challenging problem, and this paper proposes an approach to solve this problem. The proposed method is evaluated on classification, and it is shown that the proposed method outperforms Me-Mentum. "
1710,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,certified defenses USED-FOR data poisoning attacks. certified defenses USED-FOR majority vote mechanism. k nearest neighbors ( kNN ) CONJUNCTION radius nearest neighbors ( rNN ). radius nearest neighbors ( rNN ) CONJUNCTION k nearest neighbors ( kNN ). intrinsic majority vote mechanisms FEATURE-OF Nearest neighbor algorithms. radius nearest neighbors ( rNN ) HYPONYM-OF Nearest neighbor algorithms. k nearest neighbors ( kNN ) HYPONYM-OF Nearest neighbor algorithms. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic majority vote mechanisms USED-FOR certified robustness guarantees. intrinsic majority vote mechanisms USED-FOR rNN. intrinsic majority vote mechanisms USED-FOR kNN. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic certified robustness guarantees EVALUATE-FOR rNN. intrinsic certified robustness guarantees EVALUATE-FOR kNN. intrinsic certified robustness guarantees COMPARE certified defenses. certified defenses COMPARE intrinsic certified robustness guarantees. Task is Data poisoning attacks. Method is machine learning model. OtherScientificTerm is voter. ,This paper studies the problem of data poisoning attacks against a machine learning model. The authors propose a certified defenses for the majority vote mechanism of a majority vote model. They show that the intrinsic majority vote mechanisms of Nearest neighbor algorithms such as k nearest neighbors (kNN) and radius nearest neighbor (rNN) are robust to data poisoning attack. They also show that intrinsic certified robustness guarantees for kNN and rNN are better than the intrinsic certifications of certified defenses.  ,"This paper proposes a new defense against data poisoning attacks. The authors propose to use certified defenses for the majority vote mechanism of a machine learning model. The proposed Nearest neighbor algorithms are based on intrinsic majority vote mechanisms, k nearest neighbors (kNN) and radius nearest neighbors(rNN). The authors show that kNN and rNN can achieve intrinsic certified robustness guarantees, while rNN and kNN cannot. They also show that MNIST and CIFAR10 can also achieve certified defenses. "
1719,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"training time CONJUNCTION model. model CONJUNCTION training time. model EVALUATE-FOR it. training time EVALUATE-FOR it. stochastic gradient decent ( SGD ) method USED-FOR deep learning models. batch size selection problem USED-FOR graph neural network ( GNN ). SGD method USED-FOR graph neural network ( GNN ). variance of gradients CONJUNCTION compute time. compute time CONJUNCTION variance of gradients. compute time FEATURE-OF mini - batch. compute time FEATURE-OF metric. variance of gradients PART-OF metric. formula USED-FOR optimal batch size. estimator USED-FOR gradients. randomness USED-FOR estimator. Ogbnarxiv CONJUNCTION Reddit. Reddit CONJUNCTION Ogbnarxiv. Ogbn - products CONJUNCTION Ogbnarxiv. Ogbnarxiv CONJUNCTION Ogbn - products. FastGCN CONJUNCTION GraphSAINT. GraphSAINT CONJUNCTION FastGCN. Reddit CONJUNCTION Pubmed. Pubmed CONJUNCTION Reddit. ClusterGCN CONJUNCTION FastGCN. FastGCN CONJUNCTION ClusterGCN. GraphSAINT HYPONYM-OF datasets. FastGCN HYPONYM-OF datasets. Pubmed HYPONYM-OF datasets. Ogbn - products HYPONYM-OF datasets. Reddit HYPONYM-OF datasets. Ogbnarxiv HYPONYM-OF datasets. deep learning models COMPARE GNNs. GNNs COMPARE deep learning models. large batch sizes USED-FOR GNNs. OtherScientificTerm are Batch size, batch - size, and batch size. Method are decent model, and GNN. ","This paper studies the batch size selection problem in graph neural network (GNN). The authors propose a stochastic gradient decent (SGD) method for training deep learning models with large batch sizes. The authors show that the variance of gradients and the compute time of a mini-batch can be used to improve the performance of a decent model and reduce it's training time. They also provide a formula for estimating the optimal batch size based on the randomness of the gradients. Finally, the authors demonstrate that GNNs trained with large batches can achieve better performance than GNN trained with small batch sizes, and show that their estimator is more robust to gradients than the standard GNN.  The authors conduct extensive experiments on three datasets: GraphSAINT, FastGCN, and Ogbnarxiv. ","This paper proposes a stochastic gradient decent (SGD) method for training deep learning models. The SGD method is based on the batch size selection problem in graph neural network (GNN). Batch size, batch-size, and batch size. The authors propose a decent model for each batch size, and show that it improves the training time and the compute time of a mini-batch.  The authors also propose a metric based on variance of gradients and compute time. The estimator for gradients depends on randomness and the optimal batch size is defined by a formula. They show that GNNs with large batch sizes outperform GNN with small batch sizes. "
1728,SP:30d97322709cd292a49f936c767099f11b0e2913,"neural network classifiers USED-FOR real - world applications. confidence scores USED-FOR detecting misclassification errors. framework USED-FOR detecting misclassification errors. framework USED-FOR confidence scores. Gaussian Processes USED-FOR calibrated confidence scores. confidence estimation methods COMPARE approach. approach COMPARE confidence estimation methods. UCI datasets EVALUATE-FOR confidence estimation methods. method USED-FOR neural network classifiers. deep learning architecture USED-FOR vision task. OtherScientificTerm are lowconfidence predictions, and classifier ’s inherent confidence indicators. Metric is confidence metrics. Method is RED. Material is out - of - distribution and adversarial samples. ","This paper proposes a new framework for estimating confidence scores for detecting misclassification errors in neural network classifiers for real-world applications. The framework uses Gaussian Processes to compute calibrated confidence scores based on lowconfidence predictions. The authors show that the proposed approach outperforms the state-of-the-art confidence estimation methods on UCI datasets. The proposed method is based on a deep learning architecture for the vision task, and is able to capture the classifier’s inherent confidence indicators. ",This paper proposes a new framework for measuring confidence scores for detecting misclassification errors in neural network classifiers for real-world applications. The framework uses Gaussian Processes to measure confidence scores of the classifier’s inherent confidence indicators. The proposed approach is evaluated on UCI datasets and compared to other confidence estimation methods. The authors show that the proposed method is able to detect misclassifications in the training data and improve the performance of neural networkclassifiers on a vision task using a deep learning architecture. The confidence metrics are based on out-of-distribution and adversarial samples. 
1737,SP:131b3da98f56d3af273171f496b217b90754a0a7,information retrieval PART-OF natural language processing systems. open domain question answering HYPONYM-OF natural language processing systems. methods COMPARE continuous representations. continuous representations COMPARE methods. neural networks USED-FOR continuous representations. hand - crafted features USED-FOR methods. supervised data USED-FOR retriever model. supervised data USED-FOR methods. retriever models USED-FOR downstream tasks. technique USED-FOR retriever models. technique USED-FOR downstream tasks. approach USED-FOR synthetic labels. attention scores USED-FOR task. synthetic labels USED-FOR retriever. reader model USED-FOR task. attention scores USED-FOR reader model. approach USED-FOR task. reader model USED-FOR approach. attention scores USED-FOR approach. retrieved documents USED-FOR approach. retrieved documents USED-FOR task. question answering EVALUATE-FOR method. Task is knowledge distillation. ,This paper studies the problem of information retrieval in natural language processing systems such as open domain question answering. The authors propose a technique to train retriever models to perform downstream tasks using supervised data. The approach uses attention scores to generate synthetic labels for the retriever and a reader model to perform the task using the retrieved documents. The proposed method is evaluated on question answering and knowledge distillation tasks.,"This paper proposes an open domain question answering method for natural language processing systems. The method is based on knowledge distillation, which is an important problem in the field of information retrieval. The main idea is to use hand-crafted features to guide the retriever model from supervised data to the retrieved data. The proposed method is evaluated on the question answering task, where it is shown to outperform existing methods that use neural networks to generate continuous representations. The technique is also applied to other downstream tasks where retriever models are trained on synthetic labels and attention scores are used for the task. The approach uses a reader model trained on the retrieved documents to perform the task and the attention scores for the reader model."
1746,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"exploration USED-FOR reinforcement learned ( RL ) controllers. software engineering CONJUNCTION controller synthesis. controller synthesis CONJUNCTION software engineering. constraints FEATURE-OF constrained Markov decision process. controller synthesis USED-FOR safety methods. software engineering USED-FOR safety methods. formal languages USED-FOR them. finite automata USED-FOR constraint violations. finite automata USED-FOR constraints. Constraint states USED-FOR dense cost function. Constraint states USED-FOR MDP state. methods USED-FOR RL algorithms. constraints USED-FOR RL algorithms. Safety Gym HYPONYM-OF constraints. Atari environments HYPONYM-OF constraints. OtherScientificTerm are safety conditions, safety critical situations, and joint MDP / constraint dynamics. Method are safe controller, and learning. ","This paper studies the problem of exploration for reinforcement learned (RL) controllers in the presence of safety conditions. The authors consider constraints on the constrained Markov decision process (MDP) in the context of constraints on safety methods such as software engineering and controller synthesis. The constraints are based on finite automata that can be used to prevent constraint violations in the case of safety critical situations. Constraint states for the MDP state are defined in the form of a dense cost function, and formal languages are used to describe them. The proposed methods are evaluated on a variety of RL algorithms, including Safety Gym and Atari environments. The results show that the proposed safe controller can achieve better performance than existing methods when the joint MDP/constraint dynamics are known.","This paper proposes a new exploration for reinforcement learned (RL) controllers. The authors propose a constrained Markov decision process with constraints on the constraints of the safety conditions. The constraints are based on finite automata, and the authors propose two safety methods: software engineering and controller synthesis for safety methods. The safety conditions are defined in formal languages, and they can be expressed as joint MDP/constraint dynamics. Constraint states are defined as the dense cost function of the MDP state, and learning is performed in a safe controller. Experiments on Safety Gym and Atari environments show that the proposed methods outperform existing RL algorithms in terms of safety."
1755,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"binary classification algorithm USED-FOR models. decision tree learning HYPONYM-OF binary classification algorithm. first - class transparency FEATURE-OF models. decision tree model USED-FOR comprehensibility of classifications. Cascading Decision Trees HYPONYM-OF decision tree model. decision path CONJUNCTION explanation path. explanation path CONJUNCTION decision path. monolithic decision tree COMPARE decision subtrees. decision subtrees COMPARE monolithic decision tree. subtree USED-FOR features. subtrees USED-FOR positive classification. model COMPARE decision tree model. decision tree model COMPARE model. datasets CONJUNCTION real - world applications. real - world applications CONJUNCTION datasets. datasets EVALUATE-FOR model. real - world applications EVALUATE-FOR model. positive classifications EVALUATE-FOR model. explanation depth EVALUATE-FOR model. real - world applications EVALUATE-FOR algorithm. datasets EVALUATE-FOR algorithm. Method are decision trees, cascading decision subtrees, and cascading decision trees. OtherScientificTerm is decision paths. ","This paper proposes a binary classification algorithm for models with first-class transparency. The authors propose a decision tree learning algorithm, called Cascading Decision Trees. The decision tree model is designed to improve the comprehensibility of classifications and the explanation depth of the models. The main idea is to use cascading decision subtrees instead of a monolithic decision tree to learn the features of a subtree. The proposed algorithm is evaluated on two datasets and two real-world applications. The model outperforms the decision tree by a large margin.",This paper proposes a binary classification algorithm for models with first-class transparency. The authors propose a decision tree model to improve the comprehensibility of classifications. Cascading Decision Trees are a monolithic decision tree with a decision path and an explanation path. The main difference between decision trees and cascading decision subtrees is that the features of each subtree can be used for positive classification. The proposed algorithm is evaluated on two datasets and two real-world applications. The model achieves better explanation depth than the decision tree.
1764,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"random, static sparsity pattern FEATURE-OF weight tensors. random, static sparsity pattern USED-FOR models. training accuarcy EVALUATE-FOR model. Gaussian Process kernels USED-FOR models. sparse finite - width model kernel CONJUNCTION infinite - width kernel. infinite - width kernel CONJUNCTION sparse finite - width model kernel. Method is neural networks. OtherScientificTerm are network width, and model width. ","This paper studies the problem of weight tensors with a random, static sparsity pattern in neural networks. The authors propose Gaussian Process kernels to train models with a sparse finite-width model kernel and an infinite-width kernel. They show that the training accuarcy of the model improves with the network width. ","This paper proposes a new way to train neural networks. The authors propose a random, static sparsity pattern for weight tensors. The models are based on Gaussian Process kernels. The network width is defined as the number of layers, and the network width of the weight tensor is a function of the model width. The model is evaluated on the training accuarcy of the proposed model. The sparse finite-width model kernel and the infinite-width kernel are compared."
1773,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,world knowledge CONJUNCTION entities. entities CONJUNCTION world knowledge. Knowledge graphs ( KGs ) USED-FOR world knowledge. entities PART-OF Knowledge graphs ( KGs ). they COMPARE pre - trained language models. pre - trained language models COMPARE they. KG USED-FOR language modeling. joint pre - training framework USED-FOR knowledge graph. knowledge graph CONJUNCTION language. language CONJUNCTION knowledge graph. joint pre - training framework USED-FOR language. JAKET USED-FOR knowledge graph. JAKET USED-FOR language. JAKET HYPONYM-OF joint pre - training framework. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module USED-FOR embeddings. language module USED-FOR context - aware initial embeddings. knowledge - aware NLP tasks EVALUATE-FOR framework. knowledge in language understanding USED-FOR framework. OtherScientificTerm is graph. Method is pre - trained model. ,"This paper proposes Knowledge graphs (KGs) to represent world knowledge and entities in the form of entities. Knowledge graphs can be used as a pre-trained language models, and they are shown to perform better than pre-trainable language models. The authors propose a joint pre-training framework called JAKET, which learns a knowledge graph and a language from the knowledge graph. The knowledge module and the language module are used to learn embeddings for the embedding of the knowledge graphs. The framework is evaluated on a variety of knowledge-aware NLP tasks. ","This paper proposes Knowledge graphs (KGs) for learning world knowledge and entities. Knowledge graphs are a type of pre-trained language models, and they can be seen as an extension of the KG for language modeling. The authors propose a joint pre-training framework, called JAKET, to learn a knowledge graph and a language. The knowledge graph is composed of a knowledge module, a language module, and a context-aware initial embeddings. The framework is evaluated on knowledge-aware NLP tasks, and the results show that the proposed framework is able to learn knowledge in language understanding."
1782,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"hand - designed loss functions USED-FOR specific orders. domain - specific insight USED-FOR approaches. unsupervised learner USED-FOR autoregressive orders. neural network USED-FOR variational inference. learner HYPONYM-OF neural network. autoregressive ordering USED-FOR variational inference. algorithm USED-FOR end - to - end optimization. policy gradients USED-FOR algorithm. algorithm USED-FOR autoregressive orders. algorithm COMPARE fixed orders. fixed orders COMPARE algorithm. sequence modeling tasks EVALUATE-FOR algorithm. autoregressive orders COMPARE fixed orders. fixed orders COMPARE autoregressive orders. sequence modeling tasks EVALUATE-FOR solution. Task is language modeling. OtherScientificTerm are predefined ordering, insertion operations, domain - specific prior, latent variable, and variational lower bound. Metric is time complexity. ","This paper studies the problem of language modeling with hand-designed loss functions for specific orders. The authors propose an unsupervised learner to learn autoregressive orders from a neural network trained with the learner. The autoregression ordering is used for variational inference in the neural network. The proposed algorithm is based on policy gradients, which are used for end-to-end optimization. The paper shows that the proposed algorithm outperforms the fixed orders on a variety of sequence modeling tasks. ","This paper proposes an unsupervised learner for learning autoregressive orders for variational inference. The authors propose to use hand-designed loss functions to learn specific orders from the predefined ordering. The key idea is to use domain-specific insight to guide the approaches to learn the specific orders. The proposed algorithm is based on policy gradients, and the authors propose an algorithm for end-to-end optimization. The algorithm is shown to outperform fixed orders on sequence modeling tasks. The paper also provides a variational lower bound on the time complexity of the proposed algorithm. "
1791,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - related applications. large graphs USED-FOR GCNs. evolving parameters USED-FOR optimization. doubly variance reduction schema USED-FOR sampling method. O(1 / T ) convergence rate EVALUATE-FOR it. schema USED-FOR sampling methods. them USED-FOR large real - world graphs. OtherScientificTerm are computational and memory issues, nodes, memory budget, and induced variance. Method are sampling - based methods, variance of sampling methods, forward propagation, and backward propagation. Generic is works. Metric is theoretical convergence guarantees. ","Graph Convolutional Networks (GCNs) are widely used in graph-related applications. However, the computational and memory issues of GCNs are not well-studied. This paper proposes a new sampling method based on doubly variance reduction schema. The authors show that the proposed sampling methods have O(1/T) convergence rate, and that it can be used to optimize large graphs with evolving parameters. They also provide theoretical convergence guarantees for their sampling methods. ","Graph Convolutional Networks (GCNs) are widely used in graph-related applications, where the computational and memory issues are high. However, GCNs are typically large graphs with evolving parameters, which can be challenging for optimization. This paper proposes a sampling method based on a doubly variance reduction schema. The authors show that the variance of sampling methods depends on the number of nodes, the memory budget, and the induced variance. They also show theoretical convergence guarantees on the O(1/T) convergence rate of the proposed sampling methods, and show that they can be used for large real-world graphs with forward propagation and backward propagation."
1800,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"deep neural network methods USED-FOR image manipulation tasks. conditional adversarial generator USED-FOR complex image manipulations. edges CONJUNCTION segmentation. segmentation CONJUNCTION edges. primitive input representation USED-FOR generator. Task are Image manipulation, and single image training. Generic are task, network, method, and it. Method are deep methods, and augmentation method. ","This paper proposes a conditional adversarial generator for complex image manipulations using deep neural network methods for image manipulation tasks. The generator is based on a primitive input representation of the input image, and is trained with edges and segmentation. Image manipulation is an important problem in the context of single image training. The authors propose a new task, where the network is trained to predict the output of the generator, and the goal is to learn a network that can perform well on the new task. The proposed method, called augmentation method, can be applied to any image manipulation task, and it is shown that it can perform better than existing deep methods.",This paper proposes a conditional adversarial generator for complex image manipulations. The generator is based on a primitive input representation. The proposed method is evaluated on a single image training task. 
1809,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"malware detection CONJUNCTION cloud computing. cloud computing CONJUNCTION malware detection. biomedical analysis CONJUNCTION malware detection. malware detection CONJUNCTION biomedical analysis. Detecting the Maximum Common Subgraph ( MCS ) USED-FOR biomedical analysis. heuristics in search USED-FOR MCS solvers. Graph Neural Network based model USED-FOR MCS detection. GLSEARCH HYPONYM-OF Graph Neural Network based model. branch and bound algorithm USED-FOR backbone search algorithm. model USED-FOR subgraphs. branch and bound algorithm USED-FOR subgraphs. branch and bound algorithm USED-FOR model. search process USED-FOR supervision. imitation learning stage USED-FOR agent. search process USED-FOR DQN. search CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION search. framework USED-FOR reinforcement learning. framework USED-FOR search. synthetic and real - world large graph pairs EVALUATE-FOR model. MCS solvers CONJUNCTION neural graph matching network models. neural graph matching network models CONJUNCTION MCS solvers. model COMPARE neural graph matching network models. neural graph matching network models COMPARE model. model COMPARE MCS solvers. MCS solvers COMPARE model. synthetic and real - world large graph pairs EVALUATE-FOR neural graph matching network models. synthetic and real - world large graph pairs EVALUATE-FOR MCS solvers. OtherScientificTerm are Maximum Common Subgraph ( MCS ), large graph pairs, limited search budget, and node selection decision. Task are drug design, extraction of common substructures, and MCS computation. Method is node selection heuristics. ","This paper proposes a new method for finding the Maximum Common Subgraph (MCS) of a drug design. The main idea is to use a Graph Neural Network based model, GLSEARCH, for MCS detection and biomedical analysis. The authors propose a new backbone search algorithm based on a branch and bound algorithm to find subgraphs of the MCS. The search process for DQN is based on an imitation learning stage, where the agent is trained in a search process to find the optimal node selection heuristics in search. The agent is then trained to find a subgraph with the most common substructures.  The authors show that the proposed method outperforms existing MCS solvers on synthetic and real-world large graph pairs and neural graph matching network models.",This paper proposes a Graph Neural Network based model for MCS detection and cloud computing. The model is based on the Maximum Common Subgraph (MCS) for biomedical analysis and for malware detection. The main idea is to use heuristics in search to improve the performance of MCS solvers on synthetic and real-world large graph pairs. The authors propose a branch and bound algorithm for subgraphs and a backbone search algorithm based on a DQN with a search process for supervision and imitation learning stage for the agent. The paper also proposes a novel framework for search and reinforcement learning. The proposed model is shown to outperform other neural graph matching network models on both synthetic-and-real-world small graph pairs and larger graph pairs on MCS computation. 
1818,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"PC2WF USED-FOR wireframe model. network USED-FOR wireframe. vertices CONJUNCTION edges. edges CONJUNCTION vertices. model USED-FOR architecture. it USED-FOR candidate vertices. features USED-FOR it. candidate edges USED-FOR wireframe. ground truth wireframes FEATURE-OF synthetic dataset. real - world dataset EVALUATE-FOR model. synthetic dataset EVALUATE-FOR model. model COMPARE baselines. baselines COMPARE model. model USED-FOR wireframe abstractions. OtherScientificTerm are 3D point cloud, line segments, feature vectors, and corner vertices. Task is Recovering the wireframe. Generic is It. ","This paper proposes a new wireframe model based on PC2WF. The proposed architecture is based on the 3D point cloud, where vertices and edges are represented by feature vectors. The network is trained to reconstruct a wireframe from a set of vertices, edges, and edges. Then, it is used to learn candidate vertices from these features. The paper shows that the proposed model is able to recover wireframe abstractions from the ground truth wireframes on a synthetic dataset and a real-world dataset. It also shows that it can recover the wireframe.","This paper proposes a new wireframe model based on PC2WF. The architecture is based on the 3D point cloud. The network is used to reconstruct a wireframe from the 3d point cloud, where the vertices and edges of the wireframe are represented as feature vectors. It is shown that it is able to reconstruct candidate vertices, edges, and features. The model is evaluated on a synthetic dataset of ground truth wireframes, and on a real-world dataset. It shows that the model outperforms baselines on wireframe abstractions. "
1827,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"assumptions FEATURE-OF distribution of noise. assumptions USED-FOR stochastic optimization. uniform bound USED-FOR moments of the gradient noise. optimal convergence rates FEATURE-OF stochastic optimization. optimization algorithms USED-FOR neural network training. noise level FEATURE-OF stochastic gradients. convergence rates FEATURE-OF stochastic gradient methods. adaptive step size methods COMPARE SGD. SGD COMPARE adaptive step size methods. Method are neural networks, online estimator of the noise level, and RMSProp. OtherScientificTerm are noise, nonstationary behavior of noise, stochastic oracle, noise variation, step - size, noise variability, noise statistics, and theoretical guarantees. ","This paper studies the problem of stochastic optimization under assumptions on the distribution of noise in neural networks. In particular, the authors consider the case where the noise is nonstationary behavior of noise, where the gradient noise has a uniform bound on the moments of the noise, and the noise variation is non-stationary. The authors propose two optimization algorithms for neural network training: RMSProp and SGD. The main contribution of the paper is to provide an online estimator of the Noise level, which is the first of its kind. The paper also provides theoretical guarantees on the convergence rates of the stochastically gradient methods with respect to the noise level, and shows that adaptive step size methods can converge faster than SGD with noise variability.","This paper proposes a new algorithm for stochastic optimization of neural networks. The main idea is to use the assumption that the distribution of noise is nonstationary, which is a result of stochastically changing the stochasticity of the gradient noise. This assumption is based on the assumption of the uniform bound on the moments of the gradients. The authors show that this assumption holds for all neural networks with the same number of parameters. They also provide an online estimator of the noise level, RMSProp, and provide theoretical guarantees on the noise variation. They show that the optimal convergence rates of the proposed algorithm are better than those of adaptive step size methods and SGD. The paper also provides theoretical guarantees for the noise variance of the step-size and the noise statistics."
1836,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,Prior word alignment USED-FOR translation. statistical machine translation ( SMT ) models USED-FOR word alignment. method USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR method. dictionaries USED-FOR approaches. decoding speed EVALUATE-FOR methods. model PART-OF neural MT model. learning model USED-FOR target information. target information USED-FOR MT input. method COMPARE baseline model. baseline model COMPARE method. English - Korean EVALUATE-FOR baseline model. English - Korean EVALUATE-FOR method. Generic is prior. Task is decoding process. Method is enhancement learning model. OtherScientificTerm is prior alignment information. ,"This paper proposes a new method for neural machine translation (NMT) based on prior word alignment to improve the decoding speed. Prior word alignment is an important problem in the translation literature, but it is not well-studied in the literature. The authors propose a new approach that uses a learning model to learn the target information of the MT input, which is then used as a prior for the decoding process. The proposed method is evaluated on English-Korean and Korean-Chinese datasets. The results show that the proposed method performs better than the baseline model.","This paper proposes a novel method for learning neural machine translation (NMT) models with prior word alignment for translation. The proposed method is based on statistical machine translation(SMT) models, where the prior is learned in the decoding process. Previous approaches are based on dictionaries. The authors propose an enhancement learning model that uses the prior alignment information to improve the decoding speed. The model consists of a neural MT model and a learning model to predict the target information for MT input. Experiments on English-Korean show that the proposed method outperforms the baseline model."
1845,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,MDP Playground HYPONYM-OF Reinforcement Learning ( RL ) algorithms. MDP Playground HYPONYM-OF benchmark. benchmark EVALUATE-FOR Reinforcement Learning ( RL ) algorithms. dimensions of hardness FEATURE-OF MDP Playground. stochasticity CONJUNCTION image representations. image representations CONJUNCTION stochasticity. image representations CONJUNCTION irrelevant features. irrelevant features CONJUNCTION image representations. sparsity of rewards CONJUNCTION stochasticity. stochasticity CONJUNCTION sparsity of rewards. time unit CONJUNCTION action range. action range CONJUNCTION time unit. irrelevant features CONJUNCTION time unit. time unit CONJUNCTION irrelevant features. delayed rewards CONJUNCTION rewardable sequences. rewardable sequences CONJUNCTION delayed rewards. rewardable sequences CONJUNCTION sparsity of rewards. sparsity of rewards CONJUNCTION rewardable sequences. action range HYPONYM-OF hardness dimensions. time unit HYPONYM-OF hardness dimensions. sparsity of rewards HYPONYM-OF hardness dimensions. stochasticity HYPONYM-OF hardness dimensions. irrelevant features HYPONYM-OF hardness dimensions. delayed rewards HYPONYM-OF hardness dimensions. image representations HYPONYM-OF hardness dimensions. rewardable sequences HYPONYM-OF hardness dimensions. benchmarks EVALUATE-FOR RL algorithms. benchmarks EVALUATE-FOR RL algorithms. fine - grained control FEATURE-OF environments ’ hardness. MDP Playground USED-FOR adaptive and intelligent RL algorithms. MDP Playground EVALUATE-FOR algorithms. OtherScientificTerm is hardness. Material is OpenAI Gym. Generic is dimensions. ,"This paper presents a new benchmark for Reinforcement Learning (RL) algorithms called MDP Playground, which is a benchmark for measuring the dimensions of hardness of MDP playground. The hardness is defined as the difference between the sparsity of rewards and the rewardable sequences, the time unit, the action range, the irrelevant features, and the image representations.  The hardness dimensions are defined as a combination of stochasticity, image representations, and time unit. The authors show that the hardness of the MDP playsground can be used to evaluate the performance of adaptive and intelligent RL algorithms on several benchmarks.   The authors also show that environments with fine-grained control have ‘harder’ hardness. ","This paper presents a new benchmark for Reinforcement Learning (RL) algorithms, called MDP Playground, which is based on the dimensions of hardness. The hardness is defined as the difference between the sparsity of rewards and the stochasticity of image representations, the time unit, and the action range. The authors show that the hardness of environments’ hardness is correlated with the fine-grained control of the environment. The paper also shows that the proposed benchmarks can be used to evaluate RL algorithms, including adaptive and intelligent RL algorithms. "
1854,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"they USED-FOR overconfident predictions. approaches USED-FOR classification models. Isotonic Regression USED-FOR regression calibration. Isotonic Regression USED-FOR regression calibration. formulation USED-FOR quantile regularizer. quantile regularizer USED-FOR probabilistic regression model. approaches USED-FOR regression models. Dropout VI CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION Dropout VI. approach USED-FOR regression models. calibration USED-FOR regression models. architectures USED-FOR uncertainty estimates. approach USED-FOR calibration. architectures USED-FOR regression models. Deep Ensembles HYPONYM-OF uncertainty estimates. Dropout VI HYPONYM-OF uncertainty estimates. Method are Deep learning models, quantile calibration, and entropy estimation. Generic are it, model, and method. ",This paper studies the problem of regression calibration using Isotonic Regression for regression calibration. The authors propose a new formulation for quantile regularizer for a probabilistic regression model. They show that it can be used to improve the performance of existing approaches for classification models when they are trained with overconfident predictions. They also show that the proposed approach can improve the calibration performance of regression models trained with different architectures such as Dropout VI and Deep Ensembles. ,"This paper proposes a new quantile regularizer for a probabilistic regression model. The authors show that it can be used for overconfident predictions in classification models. The formulation is based on Isotonic Regression, which is used for regression calibration. They show that the quantile calibration can be applied to any model, and that it is equivalent to entropy estimation. The proposed approach is evaluated on three different architectures for calibration of regression models (Dropout VI, Deep Ensembles, and Dropout VI)."
1863,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"6 - DoF localisation CONJUNCTION 3D dense reconstruction in spatial environments. 3D dense reconstruction in spatial environments CONJUNCTION 6 - DoF localisation. deep state - space model USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR 3D dense reconstruction in spatial environments. approximate Bayesian inference USED-FOR 6 - DoF localisation. multiple - view geometry CONJUNCTION rigid - body dynamics. rigid - body dynamics CONJUNCTION multiple - view geometry. rigid - body dynamics USED-FOR learning and domain knowledge. multiple - view geometry USED-FOR learning and domain knowledge. learning and domain knowledge USED-FOR approach. neural networks CONJUNCTION differentiable raycaster. differentiable raycaster CONJUNCTION neural networks. variational inference CONJUNCTION neural networks. neural networks CONJUNCTION variational inference. realistic unmanned aerial vehicle flight data EVALUATE-FOR approach. model USED-FOR generative prediction and planning. OtherScientificTerm is spatial environments. Method are visual SLAM solutions, and visual - inertial odometry systems. ","This paper proposes a deep state-space model for approximate Bayesian inference for 6-DoF localisation and 3D dense reconstruction in spatial environments. The proposed approach is based on learning and domain knowledge from multiple-view geometry and rigid-body dynamics, as well as neural networks and a differentiable raycaster. The model is then used for generative prediction and planning. Experimental results on realistic unmanned aerial vehicle flight data demonstrate the effectiveness of the proposed approach.","This paper proposes a deep state-space model for approximate Bayesian inference for 6-DoF localisation and 3D dense reconstruction in spatial environments. The approach is based on learning and domain knowledge based on multiple-view geometry and rigid-body dynamics. The proposed model is evaluated on realistic unmanned aerial vehicle flight data, and is used for generative prediction and planning. Experiments are conducted on visual SLAM solutions, and on visual-inertial odometry systems. Results show that the proposed approach can be combined with neural networks and a differentiable raycaster."
1872,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"textual descriptions USED-FOR generalization of control policies. symbol grounding CONJUNCTION control policy. control policy CONJUNCTION symbol grounding. environment rewards USED-FOR supervision. environment rewards USED-FOR EMMA. free - form natural language FEATURE-OF text manuals. framework EVALUATE-FOR model. crowd - sourcing USED-FOR free - form natural language. crowd - sourcing USED-FOR text manuals. zeroshot generalization EVALUATE-FOR EMMA. noisy descriptions FEATURE-OF grounding. EMMA USED-FOR grounding. OtherScientificTerm are prior knowledge, concrete supervision, and dynamics. Generic is policies. Method is multi - modal entity - conditioned attention module. ","This paper proposes a novel framework for learning a multi-modal entity-conditioned attention module in the context of text-to-text learning. The proposed framework is based on EMMA, which uses environment rewards to encourage the agent to learn the prior knowledge of the agent’s environment. The authors show that the proposed framework can be applied to a wide range of tasks, and is able to achieve state-of-the-art performance. ","This paper proposes a novel framework for generalization of control policies based on textual descriptions. The key idea is to use a multi-modal entity-conditioned attention module, where prior knowledge is used to guide the learning of the prior knowledge, and the control policy is conditioned on the learned prior knowledge. The proposed model is evaluated on a set of text manuals with crowd-sourcing and free-form natural language. The grounding is based on EMMA, which uses environment rewards for supervision and a control policy based on noisy descriptions. Experiments on zeroshot generalization show that EMMA is able to generalize well to grounding with noisy descriptions, and can also generalize to grounding without noisy descriptions with concrete supervision."
1881,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"Policy gradient algorithms USED-FOR decision making and control tasks. high sample complexity CONJUNCTION instability issues. instability issues CONJUNCTION high sample complexity. instability issues EVALUATE-FOR methods. high sample complexity EVALUATE-FOR methods. approach USED-FOR critic. actor - critic framework FEATURE-OF critic. mean value COMPARE absolute value. absolute value COMPARE mean value. continuous control tasks CONJUNCTION algorithms. algorithms CONJUNCTION continuous control tasks. sparse rewards USED-FOR method. Method are actor - critic algorithms, actor - critic, and gradient estimator. OtherScientificTerm is value function. ","This paper studies the problem of policy gradient algorithms for decision making and control tasks with high sample complexity and instability issues. The authors propose two actor-critic algorithms, where the critic is trained using the actor -critic framework with sparse rewards. The main idea of the proposed method is to use sparse rewards to encourage the critic to maximize the mean value of the value function, rather than the absolute value. The proposed method uses a gradient estimator to estimate the mean of the critic, and then uses this approach to train the critic. Experiments on continuous control tasks and algorithms show that the proposed methods achieve better performance than existing methods.","This paper proposes a new method for learning policy gradient algorithms for decision making and control tasks with high sample complexity and instability issues. The proposed method is based on actor-critic algorithms, where the critic is trained with sparse rewards, and the value function is computed by a gradient estimator. The authors show that the proposed approach is able to improve the quality of the critic in terms of mean value compared to the absolute value. The method is evaluated on a variety of continuous control tasks and algorithms."
1890,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"network USED-FOR overparameterization. overparameterized regime FEATURE-OF depth. locality of the relevant feature FEATURE-OF classification rule. initialization CONJUNCTION infinitesimal learning rate. infinitesimal learning rate CONJUNCTION initialization. finite networks COMPARE neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) COMPARE finite networks. infinitely wide network USED-FOR neural tangent kernel ( NTK ). initialization USED-FOR infinitely wide network. infinitesimal learning rate FEATURE-OF infinitely wide network. depth dependence FEATURE-OF generalization performance. feature learning COMPARE lazy learning. lazy learning COMPARE feature learning. NTK USED-FOR depth dependence. generalization performance EVALUATE-FOR NTK. Task are generalization, and machinelearning tasks. OtherScientificTerm are local and global labels, classification rules, local labels, and global labels. ",This paper studies the problem of generalization in the overparameterized regime of deep neural networks. The authors propose a new network that is able to learn overparameters of the depth of the neural tangent kernel (NTK) in an infinitely wide network with initialization and infinitesimal learning rate. They show that the NTK can achieve better generalization performance than finite networks. They also show that NTK achieves better depth dependence on the number of local and global labels than feature learning. ,This paper proposes a novel network for overparameterization of the depth of the neural tangent kernel (NTK) for generalization. The main idea is to use an infinitely wide network with initialization and infinitesimal learning rate to learn the local and global labels of the classification rules. The depth is defined as the locality of the relevant feature in the classification rule. The authors show that NTK is able to achieve better generalization performance in terms of depth dependence compared to finite networks. They also show that feature learning outperforms lazy learning on both machinelearning tasks. 
1899,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"sample complexity EVALUATE-FOR representation. representation learning USED-FOR few - shot learning. i.i.d. task assumption USED-FOR Ω ( 1 T ) barrier. high - dimensional linear regression CONJUNCTION neural networks. neural networks CONJUNCTION high - dimensional linear regression. representation learning USED-FOR high - dimensional linear regression. representation learning USED-FOR neural networks. representation learning USED-FOR representation learning. Material is n2 ( n1 ) data. Generic is common representation. Task is sample size reduction. Metric is risk bound. OtherScientificTerm are linear representation class, and representation function class. ","This paper studies the problem of sample size reduction in few-shot learning with representation learning in the context of representation learning. The authors propose a new task assumption for the Ω (1 T) barrier, i.e., the common representation of a linear representation class. They show that the sample complexity of the representation can be reduced by reducing the number of samples in the representation by a factor of i.i.d. in the case of n2 (n1) data. They also provide a risk bound for this risk bound. Finally, they show that representation learning can be used in high-dimensional linear regression and neural networks.","This paper proposes a new representation learning method for few-shot learning. The main idea is to reduce the sample complexity of the representation by minimizing the risk bound of the linear representation class. The authors propose a new task assumption for the Ω (1 T) barrier, i.i.d. of the common representation. The risk bound is defined as the difference between the sample size reduction with respect to the number of n2 (n1) data, and the one where the sample function class is the same. Experiments on high-dimensional linear regression and neural networks show that the proposed representation learning improves the performance of the neural networks."
1908,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,lowestlevel features FEATURE-OF network robustness. robustness EVALUATE-FOR networks. semantic features USED-FOR networks. black - box approach USED-FOR features. network USED-FOR features. features USED-FOR provably robust neighborhoods. provably robust neighborhoods CONJUNCTION adversarial examples. adversarial examples CONJUNCTION provably robust neighborhoods. robust features CONJUNCTION adversarial examples. adversarial examples CONJUNCTION robust features. weak features USED-FOR adversarial examples. robust features USED-FOR provably robust neighborhoods. PCA features EVALUATE-FOR approach. provably robust neighborhoods COMPARE neighborhoods. neighborhoods COMPARE provably robust neighborhoods. adversarial examples COMPARE state - of - the - art. state - of - the - art COMPARE adversarial examples. L2 distortion EVALUATE-FOR state - of - the - art. L2 distortion EVALUATE-FOR adversarial examples. attack USED-FOR ensemble adversarial training. Method is neural networks. Task is neural networks ’ robustness. OtherScientificTerm is perturbations. ,"This paper studies the problem of neural networks’ robustness in the presence of lowestlevel features. The authors propose a black-box approach to learn the features of a network using the semantic features of the input data. The proposed approach is based on PCA features, which are learned by a network that is trained with perturbations. The paper shows that the proposed approach improves the robustness of the networks in terms of robustness against adversarial examples with weak features and provably robust neighborhoods with robust features. In addition, the paper also shows that these neighborhoods are more robust against L2 distortion than the state-of-the-art. ",This paper studies the problem of neural networks’ robustness. The authors propose a black-box approach to measure the network robustness under lowestlevel features. The key idea is to use semantic features in neural networks to evaluate the robustness of the networks. The proposed approach is evaluated on PCA features and on provably robust neighborhoods and adversarial examples. The results show that the proposed approach outperforms the state-of-the-art in terms of L2 distortion and robust features for the two types of perturbations. The paper also provides an attack against ensemble adversarial training.
1917,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"transfer learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION transfer learning. Meta - learning CONJUNCTION transfer learning. transfer learning CONJUNCTION Meta - learning. uniform similarity USED-FOR approaches. method USED-FOR clusters of related tasks. sample complexity EVALUATE-FOR these. expectation - maximization algorithm USED-FOR method. policies USED-FOR agent. expectation step EVALUATE-FOR policies. method COMPARE multi - task learning algorithms. multi - task learning algorithms COMPARE method. complex bipedal walker tasks CONJUNCTION Atari games. Atari games CONJUNCTION complex bipedal walker tasks. discrete and continuous control tasks CONJUNCTION complex bipedal walker tasks. complex bipedal walker tasks CONJUNCTION discrete and continuous control tasks. complex bipedal walker tasks EVALUATE-FOR approach. discrete and continuous control tasks EVALUATE-FOR approach. sample complexity EVALUATE-FOR approaches. Method are reinforcement learning agents, and maximization step. Task is training. OtherScientificTerm is policy. ","This paper studies the problem of training reinforcement learning agents with uniform similarity between tasks. The authors propose a method to learn clusters of related tasks by using the expectation-maximization algorithm. The agent is trained with a set of policies and the goal is to maximize the performance of each policy. The agents are trained using the maximization step, and the agent is evaluated on both discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games. The proposed method outperforms existing multi-task learning algorithms in terms of sample complexity. ","The paper proposes a method for learning clusters of related tasks with uniform similarity. The method is based on reinforcement learning agents, where the goal is to learn a policy that maximizes the sample complexity of the agent. The authors propose an expectation-maximization algorithm to learn the policies. The proposed method is evaluated on complex bipedal walker tasks, discrete and continuous control tasks, and Atari games. The results show that the proposed method outperforms other multi-task learning algorithms in terms of sample complexity. "
1926,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"self - supervised framework USED-FOR generalizable representations. generalizable representations USED-FOR non - stationary time series. local smoothness USED-FOR neighborhoods in time with stationary properties. local smoothness USED-FOR approach. debiased contrastive objective USED-FOR framework. framework USED-FOR time series representations. method COMPARE unsupervised representation learning approaches. unsupervised representation learning approaches COMPARE method. clustering and classification tasks EVALUATE-FOR multiple datasets. clustering and classification tasks EVALUATE-FOR method. Material are Time series, time series data, and labeling data. Method is Temporal Neighborhood Coding ( TNC ). OtherScientificTerm are encoding space, neighborhood, and distribution of non - neighboring signals. Task is medical field. ","This paper proposes a self-supervised framework for learning generalizable representations for non-stationary time series. The approach is based on local smoothness in neighborhoods in time with stationary properties. The authors propose Temporal Neighborhood Coding (TNC), a framework based on a debiased contrastive objective. The proposed framework is able to learn time series representations that are more generalizable than unsupervised representation learning approaches. Experiments on clustering and classification tasks demonstrate the effectiveness of the proposed method on multiple datasets.","This paper proposes a self-supervised framework for learning generalizable representations for non-stationary time series. The framework is based on Temporal Neighborhood Coding (TNC). The approach is motivated by local smoothness for neighborhoods in time with stationary properties. The authors propose a debiased contrastive objective for the framework, where the encoding space is represented as a medical field, and the neighborhood is represented by the distribution of non-neighboring signals. The proposed method is evaluated on clustering and classification tasks on multiple datasets. Results show that the proposed method outperforms unsupervised representation learning approaches."
1935,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"Conditional computation CONJUNCTION modular networks. modular networks CONJUNCTION Conditional computation. Conditional computation USED-FOR multitask learning and other problems. modular networks USED-FOR multitask learning and other problems. fully - differentiable approach USED-FOR modular networks. modules USED-FOR knowledge transfer. knowledge transfer USED-FOR tasks. soft weight sharing USED-FOR tasks. transfer learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION transfer learning. method USED-FOR self - organization of modules. transfer learning HYPONYM-OF tasks. multi - task learning CONJUNCTION transfer learning. transfer learning CONJUNCTION multi - task learning. domain adaptation HYPONYM-OF tasks. self - organization of modules USED-FOR multi - task learning. tasks EVALUATE-FOR method. it USED-FOR unsupervised multi - source domain adaptation. architectures USED-FOR image classification tasks. accuracy EVALUATE-FOR architectures. approach USED-FOR architectures. it USED-FOR adaptation. computation order USED-FOR modules. accuracy EVALUATE-FOR approach. order of pretrained modules USED-FOR adaptation. IMAGENET HYPONYM-OF image classification tasks. Task is problem solving. OtherScientificTerm are order of computation, and parameter increase. ","This paper proposes a fully-differentiable approach to learn modular networks for multitask learning and other problems. Conditional computation and modular networks are used to learn the modules for different tasks such as knowledge transfer, transfer learning, and domain adaptation. The authors propose a method to learn self-organization of modules for multi-task learning and for domain adaptation, where soft weight sharing is used for both tasks. The proposed approach improves the accuracy of the learned architectures on image classification tasks like IMAGENET. In addition, it can be used for unsupervised multi-source domain adaptation by learning the order of pretrained modules for adaptation. ","This paper proposes a fully-differentiable approach to modular networks for multitask learning and other problems. Conditional computation and modular networks are used for different tasks such as knowledge transfer, transfer learning, and domain adaptation with soft weight sharing. The proposed approach is evaluated on three different architectures for image classification tasks (IMAGENET, CIFAR10, and ImageNet) and it is shown to improve the performance of multi-task learning and adaptation with order of pretrained modules. The authors also show that the order of computation improves the parameter increase."
1944,SP:cae669c631e11fe703bf6cb511404866b19f474a,"local optima FEATURE-OF objective function. hyperparameter USED-FOR data variance. hyperparameter USED-FOR local optima. variance parameter USED-FOR VAE. variance parameter USED-FOR smoothness. gradient FEATURE-OF smoothness. It USED-FOR regularization. variance parameter USED-FOR It. variance parameter USED-FOR regularization. Fréchet inception distance ( FID ) EVALUATE-FOR Generation models. MNIST and CelebA datasets USED-FOR Fréchet inception distance ( FID ) of images. objectives USED-FOR Generation models. Method are Variational autoencoders ( VAEs ), and AR - ELBO. OtherScientificTerm are posterior collapse, latent space, oversmoothness, and linear approximated objective function. Generic are parameter, and model. ","This paper studies Variational autoencoders (VAEs). It proposes a new objective function that is based on the hyperparameter of the local optima of the objective function. It is shown that the variance parameter of VAE can be used to improve the smoothness of the posterior collapse of the latent space. It can also be used for regularization of the VAE. The authors show that the Fréchet inception distance (FID) of Generation models trained on MNIST and CelebA datasets can improve the performance of their objectives.    The paper also shows that the gradient of smoothness depends on the number of iterations of the model, and that the parameters of the parameter can be approximated by a linear approximated objective function (AR-ELBO). ","The paper proposes Variational autoencoders (VAEs), a new family of VAEs that can be seen as a variant of Variational Auto-encoder (VAE). It is based on the idea that the local optima of the objective function can be approximated by a hyperparameter that minimizes the data variance. It is shown that the variance parameter can be used to improve the smoothness of the VAE by minimizing the gradient of the gradient in the latent space. The paper also proposes a new objective function called Fréchet inception distance (FID) for Generation models on MNIST and CelebA datasets. Experiments are conducted on AR-ELBO and show that the proposed objectives can improve the performance of Generation models. "
1953,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,non i.i.d. variational autoencoders USED-FOR global dependencies. non i.i.d. variational autoencoders USED-FOR deep generative model. semi - supervised alternatives USED-FOR global modeling. mixture model CONJUNCTION global Gaussian latent variable. global Gaussian latent variable CONJUNCTION mixture model. global modeling USED-FOR deep generative models. semi - supervised alternatives USED-FOR deep generative models. semi - supervised alternatives COMPARE approach. approach COMPARE semi - supervised alternatives. local or data - dependent space FEATURE-OF mixture model. mixture model PART-OF approach. global Gaussian latent variable PART-OF approach. induced latent global space USED-FOR interpretable disentangled representations. user - defined regularization FEATURE-OF evidence lower bound. domain alignment USED-FOR model. shared attributes CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION shared attributes. face images CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION face images. shared attributes FEATURE-OF face images. face images HYPONYM-OF non - trivial underlying structures. Method is beta - VAE. OtherScientificTerm is global space. ,"This paper proposes a new approach to learn a mixture model with a global Gaussian latent variable in the induced latent global space. The approach is based on the idea of non-i.i.d. variational autoencoders, which can be used to model global dependencies between the input and the output of a deep generative model using semi-supervised alternatives for global modeling. The key idea of the approach is to learn the mixture model in a local or data-dependent space, where the global space is represented as a beta-VAE. The authors show that the evidence lower bound of the proposed approach is obtained by using user-defined regularization. The model is trained using domain alignment and shared attributes and defined sequences of digits images and face images. ",This paper proposes a novel approach to learning a generative model from a mixture of non-i.i.d. variational autoencoders. The approach is based on a mixture model with a global Gaussian latent variable in the local or data-dependent space. The authors show that the proposed approach outperforms semi-supervised alternatives for global modeling in terms of evidence lower bound and user-defined regularization. They also show that domain alignment can be used to improve the performance of the model. 
1962,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"images CONJUNCTION videos. videos CONJUNCTION images. images HYPONYM-OF visual data. videos HYPONYM-OF visual data. visual data USED-FOR representation learning approaches. representations COMPARE visualonly representations. visualonly representations COMPARE representations. human interaction and attention cues USED-FOR approach. body part movements CONJUNCTION gaze. gaze CONJUNCTION body part movements. gaze FEATURE-OF human interactions. human interactions FEATURE-OF dataset. body part movements FEATURE-OF human interactions. gaze FEATURE-OF dataset. body part movements FEATURE-OF dataset. dynamics prediction ( physics ) CONJUNCTION walkable surface estimation ( affordance ). walkable surface estimation ( affordance ) CONJUNCTION dynamics prediction ( physics ). scene classification ( semantic ) CONJUNCTION action recognition ( temporal ). action recognition ( temporal ) CONJUNCTION scene classification ( semantic ). action recognition ( temporal ) CONJUNCTION depth estimation ( geometric ). depth estimation ( geometric ) CONJUNCTION action recognition ( temporal ). depth estimation ( geometric ) CONJUNCTION dynamics prediction ( physics ). dynamics prediction ( physics ) CONJUNCTION depth estimation ( geometric ). muscly - supervised ” representation USED-FOR interaction and attention cues. muscly - supervised ” representation USED-FOR target tasks. muscly - supervised ” representation COMPARE MoCo. MoCo COMPARE muscly - supervised ” representation. walkable surface estimation ( affordance ) HYPONYM-OF target tasks. scene classification ( semantic ) HYPONYM-OF target tasks. dynamics prediction ( physics ) HYPONYM-OF target tasks. depth estimation ( geometric ) HYPONYM-OF target tasks. action recognition ( temporal ) HYPONYM-OF target tasks. human ’s interactions USED-FOR representation learning. cues USED-FOR visual embedding. representation COMPARE self - supervised vision - only techniques. self - supervised vision - only techniques COMPARE representation. Task are representations of visual data, and computer vision. OtherScientificTerm is first person observations. ","This paper studies the problem of learning representations of visual data from first person observations. The authors propose a new approach based on human interaction and attention cues to learn representations from visual data. The proposed approach is based on the “muscly-supervised” representation, which is a combination of two existing representation learning approaches: MoCo and MoCo-based. The key idea is to use human’s interactions in the representation learning to guide the visual embedding of the images, videos, and body part movements in a dataset of human interactions, such as gaze, gaze, etc.  The authors show that the proposed representation performs better than self-supervision vision-only techniques on target tasks such as scene classification (semantic), action recognition (temporal), depth estimation (geometric), and walkable surface estimation (affordance). ","This paper presents a new approach to learning representations of visual data from first person observations. The authors propose to use human interaction and attention cues to guide representation learning approaches on visual data. The proposed approach is based on the observation that the representations learned from visual data are more expressive than visualonly representations, which is not always the case in computer vision. The key idea is to use the human’s interactions in representation learning to guide the visual embedding. Experiments show that the proposed “muscly-supervised” representation outperforms MoCo on a range of target tasks, including dynamics prediction (physics), walkable surface estimation (affordance), and depth estimation (geometric). "
1971,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"pretrained model COMPARE model. model COMPARE pretrained model. generalization EVALUATE-FOR model. generalization EVALUATE-FOR pretrained model. learning rate USED-FOR pretraining. neural network training CONJUNCTION pretraining. pretraining CONJUNCTION neural network training. OtherScientificTerm are Negative pretraining, negative pretraining effect, learning process, learning task - level, discretization of data distribution, model - level, negative pretraining effects, and negative pretraining. Method is neural networks. Generic is interventions. ","This paper studies the problem of negative pretraining in neural networks. Negative pretraining is defined as a learning process where the goal is to improve the generalization performance of a pretrained model over a model trained on the same data. The authors propose a new learning rate for pretraining, which is based on the discretization of data distribution. They show that the learning rate can be used to reduce the number of interventions in the learning process. They also provide a theoretical analysis of the effect of the learning task-level on the performance of the model at the model-level. Finally, the authors provide some empirical evidence that negative pre-training can lead to better generalization than the original model. ","This paper studies the problem of negative pretraining in neural networks. Negative pretraining is defined as a learning process where the learning task-level is a discretization of data distribution, and the model is trained on a subset of the data distribution. The authors show that the generalization of a pretrained model is better than that of a model trained on the same data distribution with the same learning rate. They also show that, when the learning rate is low enough, the pretraining can be done with interventions. "
1980,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"adversarially perturbed inputs FEATURE-OF classifier ’s robustness. bi - level optimization algorithm USED-FOR adversarially trained classifiers. bi - level optimization algorithm USED-FOR safe spots. ImageNet datasets EVALUATE-FOR adversarially trained classifiers. they USED-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR they. safe spot inducing model training scheme CONJUNCTION safe spot generation method. safe spot generation method CONJUNCTION safe spot inducing model training scheme. out - of - distribution detection algorithm USED-FOR near - distribution outliers. safe spot generation method USED-FOR out - of - distribution detection algorithm. Task is adversarial defense. Method are classifier, and classifiers. Material is natural images. OtherScientificTerm is adversarial attacks. ","This paper studies the problem of adversarial defense against classifier’s robustness under adversarially perturbed inputs. The authors propose a bi-level optimization algorithm to find safe spots in the input space, and show that they can be used to train smoothed classifiers with empirical and certified robustness on ImageNet datasets. They also propose an out-of-distribution detection algorithm to detect near-distinct outliers, and a safe spot inducing model training scheme to train the classifiers. ",This paper studies adversarial defense against adversarially perturbed inputs. The authors propose a bi-level optimization algorithm to find safe spots in natural images. They show that they can find smoothed classifiers with empirical and certified robustness on ImageNet datasets. They also propose a safe spot inducing model training scheme and an out-of-distribution detection algorithm to detect near-distinct outliers.
1989,SP:1350ab543b6a5cf579827835fb27011751cc047f,"regularities CONJUNCTION order. order CONJUNCTION regularities. regularities FEATURE-OF temporal dimension. order FEATURE-OF temporal dimension. grid based convolutions USED-FOR video processing. point spatio - temporal ( PST ) convolution USED-FOR informative representations of point cloud sequences. PST convolution USED-FOR point cloud sequences. temporal convolution USED-FOR dynamics of the spatial regions. spatial convolution USED-FOR local structure. time dimension FEATURE-OF dynamics of the spatial regions. deep network USED-FOR features of point cloud sequences. PST convolution PART-OF deep network. PST convolution USED-FOR features of point cloud sequences. hierarchical manner USED-FOR point cloud sequences. PSTNet HYPONYM-OF deep network. PSTNet USED-FOR point cloud sequences. 3D action recognition CONJUNCTION 4D semantic segmentation datasets. 4D semantic segmentation datasets CONJUNCTION 3D action recognition. 4D semantic segmentation datasets EVALUATE-FOR PSTNet. Material is Point cloud sequences. OtherScientificTerm are spatial dimension, and 3D space. ","This paper proposes a novel point spatio-temporal (PST) convolution for learning informative representations of point cloud sequences. Point cloud sequences are generated by grid based convolutions for video processing. The temporal dimension of the temporal dimension is defined by regularities and order. PST convolution is used to learn the dynamics of the spatial regions with respect to the time dimension. The spatial convolution can be used to represent the local structure of a point cloud sequence. The authors propose a deep network called PSTNet, which is a combination of PST convolved in a hierarchical manner to capture the features of points in a 3D space. Experiments on 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet.","This paper proposes point spatio-temporal (PSTP) convolution for learning informative representations of point cloud sequences. Point cloud sequences are generated by grid based convolutions for video processing. The authors show that the temporal dimension of the spatial dimension can be decomposed into regularities and order. The temporal convolution is used to capture the dynamics of the temporal regions in the 3D space, and spatial convolution can capture the local structure in the time dimension. PSTNet is a deep network with PST convolution in a hierarchical manner, which is used for learning the features of points in the spatial regions, and 3D action recognition and 4D semantic segmentation datasets."
1998,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"source TTS model USED-FOR personal voice. Custom voice HYPONYM-OF text to speech ( TTS ) service. text to speech ( TTS ) service PART-OF commercial speech platforms. Custom voice USED-FOR TTS adaptation. adaptive TTS system USED-FOR customization of new voices. AdaSpeech HYPONYM-OF adaptive TTS system. utterance and phoneme level FEATURE-OF acoustic information. acoustic encoder USED-FOR utterance - level vector. acoustic predictor USED-FOR phonemelevel vectors. one USED-FOR phoneme - level vectors. acoustic encoder CONJUNCTION one. one CONJUNCTION acoustic encoder. acoustic encoder USED-FOR phoneme - level vectors. utterance - level vector USED-FOR inference. adaptation parameters CONJUNCTION voice quality. voice quality CONJUNCTION adaptation parameters. speaker embedding USED-FOR adaptation. mel - spectrogram decoder PART-OF AdaSpeech. part CONJUNCTION speaker embedding. speaker embedding CONJUNCTION part. conditional layer normalization USED-FOR mel - spectrogram decoder. conditional layer normalization PART-OF AdaSpeech. acoustic conditions PART-OF LibriTTS. acoustic conditions FEATURE-OF VCTK and LJSpeech datasets. LibriTTS datasets USED-FOR source TTS model. VCTK and LJSpeech datasets USED-FOR it. adaptation data USED-FOR it. AdaSpeech COMPARE baseline methods. baseline methods COMPARE AdaSpeech. adaptation quality EVALUATE-FOR baseline methods. AdaSpeech USED-FOR custom voice. adaptation quality EVALUATE-FOR AdaSpeech. Method is adaptation model. Material are source speech data, and audio samples. OtherScientificTerm is memory usage. ","This paper proposes AdaSpeech, a text to speech (TTS) service that adapts a source TTS model to a personal voice. The adaptive TTS system is designed for the customization of new voices. The authors show that the acoustic information at utterance and phoneme level can be used to improve the performance of the TTS adaptation model. The acoustic encoder is used to predict the utterance-level vector for inference, and the acoustic predictor is used for predicting the phonemelevel vectors. The author also propose a mel-spectrogram decoder that is used as a conditional layer normalization for the mel-speech decoder. The proposed model is evaluated on the VCTK and LJSpeech datasets under acoustic conditions. The adaptation parameters and the voice quality are compared with the baseline methods. The performance is also compared with AdaSpespeech on the LibriTTS datasets.","This paper proposes an adaptive TTS system for the customization of new voices, called AdaSpeech, which is an extension of the text to speech (TTS) service in commercial speech platforms. The source TTS model is used to learn a personal voice, and the custom voice is used for TTS adaptation. The acoustic information at utterance and phoneme level is extracted from the source speech data, and an acoustic encoder and one that predicts the utterance-level vector are used for inference. An acoustic predictor predicts the phonemelevel vectors, and a mel-spectrogram decoder is used as a conditional layer normalization. The adaptation model is based on the speaker embedding and the adaptation parameters are based on voice quality and memory usage. The authors show that AdaSpech outperforms baseline methods on VCTK and LJSpeech datasets with different acoustic conditions, and it outperforms the adaptation data on LibriTTS."
2007,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,sparse networks COMPARE dense neural architectures. dense neural architectures COMPARE sparse networks. regularizers USED-FOR dense networks. activation functions CONJUNCTION regularizers. regularizers CONJUNCTION activation functions. optimizers CONJUNCTION activation functions. activation functions CONJUNCTION optimizers. activation functions USED-FOR dense networks. regularizers USED-FOR sparse networks. gradient flow USED-FOR sparse networks. training regime USED-FOR gradient flow. tailoring optimization USED-FOR sparse networks. OtherScientificTerm is initialization. Task is training sparse networks. ,"This paper studies the problem of training sparse networks with regularizers. The authors show that sparse networks are more efficient than dense neural architectures. They show that the gradient flow of sparse networks can be optimized using a training regime that is similar to the training regime of dense networks. They also show that tailoring optimization can be used to train sparse networks. Finally, they show that regularizers can be combined with optimizers and activation functions to improve the performance of the sparse networks in practice. ","This paper studies the problem of training sparse networks. The authors show that sparse networks outperform dense neural architectures with regularizers and optimizers. They also show that the gradient flow of sparse networks in the training regime is similar to that of dense networks in terms of activation functions and regularizers for dense networks. They further show that for sparse networks with tailoring optimization, gradient flow is equivalent to gradient flow in the initialization."
2016,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Graph Convolutional Neural Networks ( GCN ) HYPONYM-OF message passing algorithms. Label Propagation ( LPA ) CONJUNCTION Graph Convolutional Neural Networks ( GCN ). Graph Convolutional Neural Networks ( GCN ) CONJUNCTION Label Propagation ( LPA ). Label Propagation ( LPA ) HYPONYM-OF message passing algorithms. graphs USED-FOR message passing algorithms. GCN USED-FOR node feature information. LPA USED-FOR node label information. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA USED-FOR node classification. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. end - to - end model USED-FOR node classification. GCN USED-FOR node classification. GCN PART-OF end - to - end model. LPA PART-OF end - to - end model. LPA USED-FOR GCN. LPA USED-FOR regularization. model COMPARE feature - based attention models. feature - based attention models COMPARE model. attention weights USED-FOR model. node labels USED-FOR attention weights. real - world graphs EVALUATE-FOR model. model COMPARE GCN - based methods. GCN - based methods COMPARE model. real - world graphs EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR model. OtherScientificTerm are edges of the graph, feature / label, feature / label influence, and edge weights. Task are feature / label smoothing, and classification. Method is unified model. ",This paper proposes a new message passing algorithm based on Graph Convolutional Neural Networks (GCN) and Label Propagation (LPA) to improve the performance of message passing algorithms on graphs. The authors propose a unified model for node classification and label smoothing. The proposed model uses attention weights based on node labels instead of feature-based attention models. The model achieves better node classification accuracy on real-world graphs compared to GCN-based methods. ,"This paper introduces Graph Convolutional Neural Networks (GCN) and Label Propagation (LPA) for message passing algorithms on graphs. The authors propose a unified model for feature/label smoothing, where the edges of the graph are represented by a feature/labels, and the nodes are represented as edges of a graph. The main idea of the paper is to use graph convolutional neural networks to model the node feature information and the node label information, which is then used to train an end-to-end model for node classification using GCN, LPA, GCN and LPA. The model is evaluated on real-world graphs, and outperforms GCN-based methods in terms of node classification accuracy and regularization. The proposed model also outperforms feature-based attention models on node labels and attention weights. "
2025,SP:c5883e3a59e6575eff044251b38175a6ed024034,"VC - dimension CONJUNCTION Rademacher complexity ( R - Complexity ). Rademacher complexity ( R - Complexity ) CONJUNCTION VC - dimension. complexity EVALUATE-FOR classifier ’s function space. complexity FEATURE-OF generalization gap. generalization gap EVALUATE-FOR classifier ’s function space. classifier CONJUNCTION generator function spaces. generator function spaces CONJUNCTION classifier. R - Complexity EVALUATE-FOR generator function spaces. R - Complexity EVALUATE-FOR classifier. generalization performance EVALUATE-FOR generator space. invariances CONJUNCTION local smoothness. local smoothness CONJUNCTION invariances. generator space USED-FOR constraints. invariances HYPONYM-OF constraints. local smoothness HYPONYM-OF constraints. classifier CONJUNCTION generator. generator CONJUNCTION classifier. invariance co - complexity term CONJUNCTION dissociation co - complexity term. dissociation co - complexity term CONJUNCTION invariance co - complexity term. classifier USED-FOR generator. invariant transformations FEATURE-OF generator. invariance co - complexity term PART-OF It. invariant transformations FEATURE-OF classifier. dissociation co - complexity term PART-OF It. invariance co - complexity FEATURE-OF classifier. CNN architecture CONJUNCTION transformation - equivariant extensions. transformation - equivariant extensions CONJUNCTION CNN architecture. Co - complexity USED-FOR classifiers. Metric are generalization error bounds, generalization error, co - complexity, dissociation co - complexity, and training error. OtherScientificTerm are ground truth label generating function ( LGF ), LGF, ground truth labels, and function space. Generic is it. ","This paper studies the generalization error bounds of the ground truth label generating function (LGF). The authors show that the VC-dimension and Rademacher complexity (R-Complexity) of the classifier’s function space are the main components of the complexity of generalization gap between the classified and unclassified versions of the same classifier. They show that under certain constraints (invariances, local smoothness, invariances) the generator space is more general than the generator function spaces, and that the R-complexity of a classifier can be used as a proxy for R- Complexity of the generator. The authors also show that it is possible to use the CNN architecture and transformation-equivariant extensions to train the generator and the invariance co-computation term in the invariant transformations in the generator as well as the dissociation co-compassity term in It. ","This paper studies the generalization error bounds of a ground truth label generating function (LGF). The authors show that the VC-dimension and Rademacher complexity (R-Complexity) of the classifier’s function space are correlated with the complexity of a generalization gap between the VC and R-complexity of a classifier and the generator function spaces. The authors also show that under certain constraints (e.g., invariances, local smoothness, etc.), the generator space is more robust to these constraints than the generator. The generalization performance of a generator space can be measured by the R-complexity of the generated ground truth labels, and it is shown that it is a function space that is robust to the dissociation co-computation term, the invariance co-compassity term, and the invariant transformations of the generator, and that the training error can be defined as the sum of the co-consistency of the gradients of the two classifiers. The paper also shows that the CNN architecture and the transformation-equivariant extensions are more robust."
2034,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"neural network quantization USED-FOR task. end - toend retraining USED-FOR neural network quantization. Post - training Quantization ( PTQ ) HYPONYM-OF neural network quantization. PTQ USED-FOR quantized models. PTQ COMPARE Quantization - Aware Training ( QAT ). Quantization - Aware Training ( QAT ) COMPARE PTQ. Quantization - Aware Training ( QAT ) USED-FOR quantized models. bitwidth FEATURE-OF PTQ. BRECQ HYPONYM-OF PTQ framework. neural networks USED-FOR BRECQ. crosslayer dependency CONJUNCTION generalization error. generalization error CONJUNCTION crosslayer dependency. crosslayer dependency EVALUATE-FOR BRECQ. generalization error EVALUATE-FOR BRECQ. mixed precision technique PART-OF framework. handcrafted and searched neural architectures USED-FOR image classification and object detection tasks. 4 - bit ResNet CONJUNCTION MobileNetV2. MobileNetV2 CONJUNCTION 4 - bit ResNet. PTQ COMPARE QAT. QAT COMPARE PTQ. MobileNetV2 COMPARE QAT. QAT COMPARE MobileNetV2. MobileNetV2 EVALUATE-FOR PTQ. 4 - bit ResNet EVALUATE-FOR PTQ. OtherScientificTerm are INT2, quantization, and inter - layer and intra - layer sensitivity. Metric is second - order error. ","This paper studies the problem of end-to-end retraining of neural network quantization for a task where the number of layers is large. The authors propose Post-training Quantization (PTQ), a new method for quantization that is based on a mixed precision technique. They show that PTQ can achieve better performance than Quantization-Aware Training (QAT) for quantized models in terms of bitwidth, crosslayer dependency, and generalization error. They also show that the second-order error of PTQ is lower than QAT. ","This paper proposes Post-training Quantization (PTQ), an extension of neural network quantization to the task of end-toend retraining. The authors propose a new PTQ framework, BRECQ, which is based on the mixed precision technique in INT2. The main idea of PTQ is to reduce the bitwidth of the quantization, which allows for better inter-layer and intra-layer sensitivity. Experiments are conducted on image classification and object detection tasks with handcrafted and searched neural architectures. The results show that the proposed PTQ outperforms QAT and 4-bit ResNet in terms of crosslayer dependency, generalization error, and second-order error."
2043,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"large labeled datasets USED-FOR deep learning deployment. distributional shift CONJUNCTION labeling cost. labeling cost CONJUNCTION distributional shift. medicine HYPONYM-OF real - world applications. dataset properties USED-FOR calibration. dataset properties COMPARE architecture. architecture COMPARE dataset properties. common strategies USED-FOR class imbalance. dataset properties USED-FOR calibration. dataset curation USED-FOR calibration. Method are Neural networks, and neural networks. Metric are downstream prediction accuracy, accuracy, and calibration error. OtherScientificTerm are model uncertainty, label quality, label noise, small dataset sizes, and network expressivity. Generic is complementary approach. Task is dataset imbalance. ","This paper studies the problem of large labeled datasets for deep learning deployment. Neural networks have been shown to suffer from downstream prediction accuracy, and the authors propose a complementary approach to address this issue. The proposed approach is based on the observation that the distributional shift and the labeling cost are important factors in the calibration error. The authors then propose a dataset curation to improve the calibration by using the dataset properties rather than the architecture. They also show that the dataset imbalance can be alleviated by small dataset sizes, which improves the network expressivity. ","This paper proposes a new approach for training large labeled datasets for deep learning deployment. The authors propose a complementary approach to the existing work, which is based on the notion of model uncertainty. The main idea of the proposed approach is to use the distributional shift, the labeling cost, and the label quality to improve the downstream prediction accuracy. They show that the proposed architecture is more robust to class imbalance than the existing dataset properties for calibration. They also show that using dataset curation can improve the calibration error. "
2052,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"information exchange CONJUNCTION cooperation. cooperation CONJUNCTION information exchange. Effective communication USED-FOR information exchange. Effective communication USED-FOR cooperation. symbolic channels USED-FOR emergent communication. 3D environment FEATURE-OF joints. non - uniform distribution of intents CONJUNCTION commonknowledge energy cost. commonknowledge energy cost CONJUNCTION non - uniform distribution of intents. agents USED-FOR protocols. OtherScientificTerm are discrete cheap - talk channels, and latent feature. Method is emergent protocols. Generic is modality. Material is training curricula. ","This paper studies the problem of emergent communication with symbolic channels in a 3D environment. The authors propose a new modality, called emergent protocols, which are based on effective communication between agents. The key idea is to use discrete cheap-talk channels to communicate information exchange and cooperation between agents, and then use the information exchange between agents to learn a latent feature. Theoretical results show that the proposed protocols have a non-uniform distribution of intents and a commonknowledge energy cost. Empirical results demonstrate the effectiveness of the proposed training curricula.","This paper proposes emergent protocols that use Effective communication for information exchange and cooperation. The key idea is to use discrete cheap-talk channels, where the input is a latent feature, and the output is a set of symbolic channels. The modality is based on the training curricula. The proposed protocols are evaluated on three joints in a 3D environment. The authors show that the non-uniform distribution of intents and the commonknowledge energy cost are the key components of the proposed protocols."
2061,SP:5ba686e2eef369fa49b10ba3f41f102740836859,Generating high quality uncertainty estimates USED-FOR sequential regression. deep recurrent networks HYPONYM-OF sequential regression. real world non - stationary signals CONJUNCTION drift. drift CONJUNCTION real world non - stationary signals. method USED-FOR symmetric and asymmetric uncertainty estimates. method COMPARE baselines. baselines COMPARE method. drift and non drift scenarios EVALUATE-FOR baselines. sequential regression USED-FOR real - world applications. modeling toolbox USED-FOR sequential uncertainty quantification. Generic is approaches. OtherScientificTerm is stationarity. ,"This paper proposes a new method for generating high quality uncertainty estimates for sequential regression in deep recurrent networks. The proposed method combines symmetric and asymmetric uncertainty estimates, and is able to achieve state-of-the-art performance in both real world non-stationary signals and drift. The authors show that the proposed method outperforms existing baselines in both drift and non drift scenarios. ",This paper proposes a new method for generating high quality uncertainty estimates for sequential regression in deep recurrent networks. The authors propose a new modeling toolbox for sequential uncertainty quantification. The proposed method can be applied to symmetric and asymmetric uncertainty estimates. The method is evaluated on both real world non-stationary signals and drift. The results show that the proposed method outperforms existing baselines in both drift and non drift scenarios. 
2070,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,metric measure spaces USED-FOR machine learning problems. metric space HYPONYM-OF Comparing metric measure spaces. probability distribution FEATURE-OF metric space. Gromov - Wasserstein ( GW ) distance HYPONYM-OF metric measure spaces. probability distribution FEATURE-OF metric measure spaces. distance CONJUNCTION upper - bounding relaxation. upper - bounding relaxation CONJUNCTION distance. upper - bounding relaxation HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. distance HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. They USED-FOR metric spaces. isometries FEATURE-OF positive measures. positive measures FEATURE-OF metric spaces. formulation USED-FOR positive and definite divergence. relaxation of the mass conservation constraint USED-FOR positive and definite divergence. quadratically - homogeneous divergence USED-FOR relaxation of the mass conservation constraint. quadratically - homogeneous divergence USED-FOR positive and definite divergence. entropic regularization approach USED-FOR large scale optimal transport problems. entropic regularization approach USED-FOR divergence. parallelizable and GPU - friendly iterative scheme USED-FOR non - convex optimization problem. distance FEATURE-OF mm - spaces. distance USED-FOR formulation. isometries FEATURE-OF distance. isometries FEATURE-OF mm - spaces. conic lifting USED-FOR distance. synthetic examples CONJUNCTION domain adaptation data. domain adaptation data CONJUNCTION synthetic examples. unbalanced divergence USED-FOR ML. domain adaptation data CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION domain adaptation data. synthetic examples CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION synthetic examples. Task is quadratic assignment problem. OtherScientificTerm is GW distance. ,"This paper studies the problem of computing the Gromov-Wasserstein (GW) distance between two metric measure spaces for machine learning problems, namely the metric space and the Comparing metric measure space. The authors propose a parallelizable and GPU-friendly iterative scheme to solve the non-convex optimization problem. The proposed formulation is based on the relaxation of the mass conservation constraint and the quadratically-homogeneous divergence between the positive and definite divergence. The distance between the two metric spaces is computed by using isometries of the positive measures of the metric spaces. They show that the distance is a function of the probability distribution of the two metrics measure spaces.  The authors also show that this distance can be used as an upper-bounding relaxation for the Unbalanced Gervov-wasserstein formulations and distance for the distance between a pair of metric measure functions.  Finally, the authors propose an entropic regularization approach for large scale optimal transport problems, which is shown to improve the unbalanced divergence for ML. ","This paper proposes a new metric measure spaces for machine learning problems, called Gromov-Wasserstein (GW) distance. The metric space is an extension of the metric space, which is used for Comparing metrics measure spaces. The proposed metric spaces are based on the probability distribution of a metric space. They can be used to define metric spaces with positive measures, e.g. isometries. The authors propose a new formulation for the positive and definite divergence, based on quadratically-homogeneous divergence and relaxation of the mass conservation constraint. They also propose a parallelizable and GPU-friendly iterative scheme for solving the non-convex optimization problem, where the quadratic assignment problem is solved by a non-vanishingly large number of samples. They show that the proposed GW distance can be approximated by conic lifting, and the proposed distance is equivalent to the sum of the upper-bounding relaxation and the distance of the distance between the positive measures and the negative measures. Experiments are performed on a number of domains, including a Positive-Unlabeled learning task, a domain adaptation data, and a synthetic examples. The results show the convergence of the proposed divergence under the entropic regularization approach for large scale optimal transport problems, and that the unbalanced divergence can be solved by ML. "
2079,SP:47dcefd5515e772f29e03219c01713e2403643ce,"computational cost CONJUNCTION memory consumption. memory consumption CONJUNCTION computational cost. Network pruning USED-FOR memory consumption. compression ratios EVALUATE-FOR saliencybased pruning. sparse parameters FEATURE-OF well - trainable networks. pruning method USED-FOR pruned networks. all - alive pruning ( AAP ) HYPONYM-OF pruning method. trainable weights USED-FOR pruned networks. AAP USED-FOR saliency - based pruning methods. AAP USED-FOR model architectures. saliency - based pruning methods CONJUNCTION model architectures. model architectures CONJUNCTION saliency - based pruning methods. one - shot pruning CONJUNCTION dynamic pruning. dynamic pruning CONJUNCTION one - shot pruning. iterative pruning CONJUNCTION one - shot pruning. one - shot pruning CONJUNCTION iterative pruning. pruning methods USED-FOR AAP. dynamic pruning USED-FOR AAP. accuracy EVALUATE-FOR AAP. benchmark datasets EVALUATE-FOR AAP. dynamic pruning HYPONYM-OF pruning methods. iterative pruning HYPONYM-OF pruning methods. one - shot pruning HYPONYM-OF pruning methods. Material is low - resource devices. Metric is accuracy loss. Method is network pruning. OtherScientificTerm are model capacity, and dead connections. ","This paper proposes a new pruning method for pruned networks, called all-alive pruning (AAP). The main idea is to reduce the computational cost and memory consumption of network pruning by reducing the number of parameters in the network. The authors show that the compression ratios of saliency-based pruning with different compression ratios can lead to different performance gains in terms of accuracy loss and model capacity. The paper also shows that AAP can be applied to different model architectures and different pruning methods (e.g. one-shot pruning, dynamic pruning and iterative pruning).","This paper proposes a pruning method for pruned networks. The main idea is to reduce the computational cost and memory consumption of network pruning. The authors propose all-live pruning (AAP) which is an extension of all-alive pruning where the model capacity is limited by the number of dead connections in the network. They show that saliency-based pruning can reduce the compression ratios of well-trainable networks with sparse parameters. They also show that AAP can be applied to different model architectures and different pruning methods such as dynamic pruning, one-shot pruning and iterative pruning to improve the accuracy loss. They evaluate AAP on several benchmark datasets."
2088,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"Generative Adversarial Net ( GAN ) USED-FOR latent space. approaches USED-FOR task. Generative Adversarial Net ( GAN ) USED-FOR latent - space transformations. latent space CONJUNCTION latent - space transformations. latent - space transformations CONJUNCTION latent space. Generative Adversarial Net ( GAN ) USED-FOR approaches. Generative Adversarial Net ( GAN ) USED-FOR task. global image identity CONJUNCTION diminished photo - realism. diminished photo - realism CONJUNCTION global image identity. attribute edits CONJUNCTION global image identity. global image identity CONJUNCTION attribute edits. content loss CONJUNCTION adversarial loss. adversarial loss CONJUNCTION content loss. maintenance of image identity CONJUNCTION photo - realism. photo - realism CONJUNCTION maintenance of image identity. attribute regression USED-FOR transformation functions. adversarial loss USED-FOR maintenance of image identity. content loss USED-FOR maintenance of image identity. quantitative evaluation strategies EVALUATE-FOR controllable editing. image identity CONJUNCTION realism. realism CONJUNCTION image identity. model USED-FOR singleand multipleattribute editing. model USED-FOR targeted image manipulation. natural and synthetic images EVALUATE-FOR model. Task are Controllable semantic image editing, and qualitative evaluation. OtherScientificTerm are image attributes, and multiple attribute transformations. ","This paper studies Controllable semantic image editing, where the goal is to learn a set of image attributes that are controllable to multiple attribute transformations. The authors propose two approaches to this task using Generative Adversarial Net (GAN) to learn the latent space and the latent-space transformations. They use attribute regression to train the transformation functions and use content loss and adversarial loss for the maintenance of image identity, global image identity and diminished photo-realism. The model is evaluated on both natural and synthetic images and is shown to perform well on both singleand multipleattribute editing. ","This paper proposes a novel approach for Controllable semantic image editing. The authors propose two approaches to tackle the task: Generative Adversarial Net (GAN) for learning latent space and latent-space transformations, and adversarial loss for learning global image identity and diminished photo-realism. The proposed model is evaluated on both natural and synthetic images, and is shown to perform well on both controllable editing and on qualitative evaluation. The model can be used for singleand multipleattribute editing, and the model can also be applied to targeted image manipulation. The key idea is to use attribute regression to learn transformation functions that are more robust to multiple attribute transformations. "
2097,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"mode dropping CONJUNCTION unstable training. unstable training CONJUNCTION mode dropping. Generative Adversarial Networks ( GAN ) USED-FOR synthesizing sequences of discrete elements. unstable training HYPONYM-OF synthesizing sequences of discrete elements. mode dropping HYPONYM-OF synthesizing sequences of discrete elements. binary classifier PART-OF discriminator. Feature Statistics Alignment ( FSA ) paradigm USED-FOR fine - grained signals. latent high - dimensional representation space FEATURE-OF fine - grained signals. FSA USED-FOR mean statistics. finite - dimensional feature space FEATURE-OF real data. approach USED-FOR discrete sequence generation. synthetic and real benchmark datasets EVALUATE-FOR approach. quantitative evaluation EVALUATE-FOR approach. Gumbel - Softmax based GAN framework USED-FOR sequence generation. feature alignment regularization USED-FOR Gumbel - Softmax based GAN framework. OtherScientificTerm are learning signals, and binary classification feedback. Task is adversarial training. ",This paper proposes a novel GAN framework for sequence generation based on the Feature Statistics Alignment (FSA) paradigm for fine-grained signals in a latent high-dimensional representation space. The authors propose a Gumbel-Softmax based GAN with feature alignment regularization. The proposed approach is evaluated on synthetic and real benchmark datasets and shows that the proposed approach improves the performance of discrete sequence generation.,This paper proposes Generative Adversarial Networks (GAN) for synthesizing sequences of discrete elements. The authors propose a Feature Statistics Alignment (FSA) paradigm for fine-grained signals in the latent high-dimensional representation space. The main contribution of the paper is to introduce a binary classifier to train the discriminator. The discriminator is based on a Gumbel-Softmax based GAN framework based on feature alignment regularization. The proposed approach is evaluated on both synthetic and real benchmark datasets for discrete sequence generation. The mean statistics of the generated mean statistics are computed using FSA in the finite-dimensional feature space. 
2106,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"progressive rewards FEATURE-OF reinforcement learning tasks. tasks HYPONYM-OF reinforcement learning tasks. Spectral DQN USED-FOR reward. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE it. it COMPARE it. Spectral DQN USED-FOR reward progressivity. OtherScientificTerm are rewards, training loss, and extreme reward progressivity. Method are value - based deep reinforcement learning agents, and value - based methods. Generic are agent, and approach. Material is Atari games. ","This paper studies the problem of value-based deep reinforcement learning agents, where the goal is to maximize the reward of the agent. The authors propose a new approach, called Spectral DQN, to improve the reward progressivity in reinforcement learning tasks such as tasks with progressive rewards. They show that it can achieve better performance than other benchmarks, and that it is more efficient than other value-base methods. They also show that the training loss does not affect the performance of their approach. ","This paper proposes a new method for learning value-based deep reinforcement learning agents. The approach is based on Spectral DQN, where the agent is trained on a set of tasks with progressive rewards, and the rewards are learned by minimizing the training loss. The authors show that the proposed approach outperforms other value-base methods on Atari games, and it outperforms it on other benchmarks. The paper also shows that the extreme reward progressivity of the proposed method can be reduced."
2115,SP:bff215c695b302ce31311f2dd105dace06307cfc,representations USED-FOR task. usable information FEATURE-OF representation. deep network USED-FOR representation. minimal sufficient representations USED-FOR task. learning - rate CONJUNCTION small batch size. small batch size CONJUNCTION learning - rate. learning - rate FEATURE-OF Stochastic Gradient Descent. Stochastic Gradient Descent USED-FOR implicit regularization. neuroscience literature USED-FOR perceptual decision - making tasks. Generic is it. Method is minimal sufficient representation. OtherScientificTerm is learning dynamics. Task is image classification tasks. ,This paper studies the problem of learning a minimal sufficient representation for a task with minimal sufficient representations for the task. The goal is to learn a representation with usable information. The representation is learned from a deep network. The learning dynamics of the learning dynamics is modeled by Stochastic Gradient Descent. The authors show that the learning-rate and the small batch size of the learned representation can be used to improve the performance of implicit regularization. They also show that it is possible to use the learned representations in neuroscience literature for perceptual decision-making tasks. ,"This paper proposes a novel method for learning a minimal sufficient representation for a given task. The main idea is to use a deep network to generate a representation with usable information, and then use it for the task. This is done by using Stochastic Gradient Descent, which is an implicit regularization of the learning-rate and small batch size. The authors show that the learning dynamics of this method can be used to improve the performance of image classification tasks. "
2124,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"robust adversarial learning CONJUNCTION inverse reinforcement learning. inverse reinforcement learning CONJUNCTION robust adversarial learning. Min - max optimization USED-FOR machine learning problems. nonconvexstrongly - concave min - max optimization HYPONYM-OF machine learning problems. robust adversarial learning HYPONYM-OF machine learning problems. inverse reinforcement learning HYPONYM-OF machine learning problems. variance reduction algorithm SREDA USED-FOR problem. accuracy level FEATURE-OF optimal complexity dependence. initialization accuracy CONJUNCTION -dependent stepsize. -dependent stepsize CONJUNCTION initialization accuracy. -dependent stepsize USED-FOR per - iteration progress. convergence guarantee EVALUATE-FOR SREDA. initialization accuracy USED-FOR convergence guarantee. analytical framework USED-FOR SREDA. SREDA - Boost COMPARE SREDA. SREDA COMPARE SREDA - Boost. SREDA - Boost USED-FOR zeroth - order variance reduction algorithm. ZO - SREDA - Boost COMPARE complexity dependence on. complexity dependence on COMPARE ZO - SREDA - Boost. ZO - SREDA - Boost HYPONYM-OF zeroth - order variance reduction algorithm. variance reduction technique USED-FOR zeroth - order algorithm. zeroth - order algorithm USED-FOR min - max optimization problems. variance reduction technique USED-FOR min - max optimization problems. OtherScientificTerm are restrictive initialization requirement, and gradients. ","This paper studies the problem of Min-max optimization in machine learning problems such as robust adversarial learning and inverse reinforcement learning. The authors propose a variance reduction algorithm SREDA for this problem, which is a variant of the popular zeroth-order variance reduction technique ZO-SREDA-Boost. The main contribution of the paper is a theoretical analysis of the convergence guarantee of SRED as a function of initialization accuracy, initialization accuracy and-dependent stepsize for per-iteration progress. The theoretical analysis shows that the optimal complexity dependence on the initialization accuracy level is a measure of the number of iterations needed to reach a restrictive initialization requirement. The paper also provides an analytical framework to analyze the convergence guarantees of the proposed SRED a.i.d. algorithm. ","This paper studies the problem of Min-max optimization for machine learning problems such as robust adversarial learning and inverse reinforcement learning. The authors propose a variance reduction algorithm SREDA to solve the problem, which is a nonconvexstrongly-concave version of the problem. The main contribution of the paper is to propose an analytical framework to evaluate the convergence guarantee of the proposed algorithm. The paper shows that the optimal complexity dependence on the initialization accuracy and the-dependent stepsize of the per-iteration progress depends on the restrictive initialization requirement and the accuracy level of the gradients. The convergence guarantee is obtained by minimizing the variance reduction technique of the zeroth-order variance reduction method of the authors, ZO-SREDA-Boost. "
2133,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"COCO EVALUATE-FOR state - of - the - art. it USED-FOR models. Example based object detection USED-FOR Detection of Novel Objects. Task are one - shot object detection, few - shot learning, and data annotation. OtherScientificTerm are generalization gap, Object categories, and object categories. Metric is generalization. Method are few - shot detection models, and metric learning approaches. ","This paper studies the problem of one-shot object detection in the context of few-shot learning. In particular, it focuses on the generalization gap between the two types of models. The authors propose a new metric called COCO, which is based on the fact that the number of classes of objects in the dataset is larger than that of the classes in the training data. The paper then shows that the generalizability of the two models can be improved by using the metric. ","This paper studies the problem of one-shot object detection in few-shot learning, where the goal is to reduce the generalization gap between two classes of objects. The authors propose a new metric, COCO, to measure the state-of-the-art of the models, and show that it can be used to improve the performance of existing models. Object categories are defined in terms of the number of classes, and object categories can be represented as a set of classes. The paper also shows that the metric learning approaches can improve generalization. The main contribution of the paper is the introduction of Example based object detection for Detection of Novel Objects, where it is shown that it is able to achieve better generalization compared to existing methods. "
2149,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"3D representations USED-FOR modeling clean mesh surfaces. occupancy fields CONJUNCTION signed distance functions ( SDF ). signed distance functions ( SDF ) CONJUNCTION occupancy fields. Implicit neural shape functions USED-FOR 3D representations. signed distance functions ( SDF ) HYPONYM-OF Implicit neural shape functions. occupancy fields HYPONYM-OF Implicit neural shape functions. representations USED-FOR single - view object reconstruction. Existing approaches USED-FOR single - view object reconstruction. representations USED-FOR Existing approaches. supervision signals USED-FOR Existing approaches. spatial gradient FEATURE-OF implicit field. supervision USED-FOR single - view reconstruction. feature map USED-FOR spatial gradient. real - world scenes USED-FOR single view implicit surface reconstructions. scanned dataset USED-FOR single view implicit surface reconstructions. ShapeNet CONJUNCTION ScannetV2. ScannetV2 CONJUNCTION ShapeNet. ShapeNet HYPONYM-OF datasets. ScannetV2 HYPONYM-OF datasets. model USED-FOR 3D implicit surface reconstruction. RGB image USED-FOR model. Task are real - world scenarios, and training on large - scale scenes. Material are ideal watertight geometric training data, large - scale scenes, internet, Internet, and pix3d dataset. OtherScientificTerm are training signal, spatial gradients, feature maps, and dense 3D supervision. Generic is this. Method are Pix3D, and DGS module. ","This paper studies the problem of learning 3D representations for modeling clean mesh surfaces. Implicit neural shape functions such as occupancy fields and signed distance functions (SDF) are commonly used in 3D representation learning. Existing approaches for single-view object reconstruction are based on supervision signals, and the authors propose to use these representations to improve the performance of existing approaches. The authors show that the spatial gradient of the implicit field can be computed using a feature map, and that this can be used as a training signal to guide the training of the 3D model. The paper also shows that the proposed model is able to achieve state-of-the-art performance on a scanned dataset for single view implicit surface reconstructions on real-world scenes.","This paper proposes to use 3D representations for modeling clean mesh surfaces. Implicit neural shape functions (SDF) and occupancy fields are used to train 3D models. Existing approaches for single-view object reconstruction are based on supervision signals. The authors propose to use these representations to improve the performance of existing approaches. This is done by training on ideal watertight geometric training data and on large-scale scenes (e.g., Pix3D). The authors show that this can be done in real-world scenarios, and that the proposed model can be used for 3D implicit surface reconstruction on a scanned dataset. The model is trained on an RGB image and a pix3d dataset, and is evaluated on ShapeNet and ScannetV2. Experiments are performed on the Pix3d, and on the DGS module on the Internet. The paper shows that the spatial gradients of the implicit field can be approximated by the spatial gradient of the feature map of the training signal. The spatial gradient can be computed by the feature maps, and dense 3D supervision is used to guide the single view reconstruction. "
2165,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"deep learning algorithms CONJUNCTION distributed training. distributed training CONJUNCTION deep learning algorithms. distributed training CONJUNCTION hardware design. hardware design CONJUNCTION distributed training. hardware design USED-FOR large models. GPT-3 CONJUNCTION Switch Transformer. Switch Transformer CONJUNCTION GPT-3. distributed training USED-FOR large models. Switch Transformer HYPONYM-OF extreme - scale models. GPT-3 HYPONYM-OF extreme - scale models. limited resources USED-FOR extreme - scale model training. memory footprint USED-FOR extreme - scale model training. Pseudo - to - Real USED-FOR high - memoryfootprint - required large models. training strategy USED-FOR high - memoryfootprint - required large models. Pseudo - to - Real HYPONYM-OF training strategy. Pseudo - to - Real CONJUNCTION large models. large models CONJUNCTION Pseudo - to - Real. architecture of sequential layers USED-FOR large models. GPUs USED-FOR state - of - the - art. Granular CPU offloading USED-FOR CPU memory. technique USED-FOR CPU memory. Granular CPU offloading HYPONYM-OF technique. Metric are model convergence, and carbon footprint. Method is large model. OtherScientificTerm is GPU utilities. Task is greener AI. ","This paper proposes a new training strategy for high-memoryfootprint-required large models such as GPT-3, Switch Transformer, and Pseudo-to-Real. The main idea is to use limited resources for extreme-scale model training, and to use distributed training and hardware design to train large models. The authors also propose a new technique called Granular CPU offloading to improve the CPU memory. Finally, the authors show that the proposed architecture of sequential layers can be used to learn large models with GPUs. ","This paper proposes a novel training strategy for high-memoryfootprint-required large models such as Pseudo-to-Real, GPT-3, and Switch Transformer. The authors propose to use limited resources for extreme-scale model training to reduce the memory footprint for large models. The paper also proposes a new architecture of sequential layers to improve the model convergence and reduce the carbon footprint of the large model. Finally, the authors propose a new technique for CPU memory, called Granular CPU offloading, which is based on GPU utilities. Experiments are conducted on state-of-the-art GPUs."
2181,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"Energy - based models ( EBMs ) HYPONYM-OF generative models. maximum likelihood estimation USED-FOR generative models. Gibbs distribution USED-FOR energy. Fenchel duality USED-FOR variational principles. variational principles USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR variational principles. dual formulation USED-FOR training algorithm. particles CONJUNCTION neurons. neurons CONJUNCTION particles. particles PART-OF sample space. parameter space FEATURE-OF neurons. dual formulation USED-FOR active regime. maximum likelihood CONJUNCTION score matching training. score matching training CONJUNCTION maximum likelihood. intermediate parameter setups USED-FOR dual algorithm. Generic are approach, and algorithm. ","This paper studies the problem of maximum likelihood estimation for generative models with energy-based models (EBMs). The authors propose a dual formulation for the training algorithm, which is based on Fenchel duality. The authors show that the maximum likelihood EBMs with shallow overparametrized neural network energies can be approximated by variational principles based on the Fenchel Duality. They also show that this approach can be applied to the active regime. The main contribution of the paper is a dual algorithm with intermediate parameter setups, which can be used for both maximum likelihood and score matching training. ","This paper proposes a new approach to learning energy-based models (EBMs) for generative models. The authors propose a dual formulation of the training algorithm, which is based on Fenchel duality. The main idea is to use the Gibbs distribution to estimate the energy of the model, and then use variational principles to learn maximum likelihood EBMs based on shallow overparametrized neural network energies. The proposed dual algorithm is evaluated on a number of intermediate parameter setups, and is shown to outperform the active regime in terms of maximum likelihood and score matching training. "
2197,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"differentially private ERM USED-FOR convex functions. lower bounds FEATURE-OF convex functions. lower bounds FEATURE-OF differentially private ERM. logarithmic terms USED-FOR lower bounds. biased mean property USED-FOR fingerprinting codes. ` 2 loss function COMPARE linear functions. linear functions COMPARE ` 2 loss function. ` 2 loss function USED-FOR pure - DP. Method are approximate - DP, and DP - ERM. Material is constrained case. OtherScientificTerm are unconstrained case, auxiliary dimension, ` 2 loss, and one - way marginals. Generic is it. ","This paper studies the problem of differentially private ERM for convex functions with lower bounds on the logarithmic terms. The authors consider the unconstrained case, where the auxiliary dimension of the function is unknown. They show that the `2 loss function of the pure-DP can be used to approximate the approximate-DP, and that DP-ERM can approximate the true-DP. They also provide a biased mean property for fingerprinting codes. Finally, they show that it is possible to approximate one-way marginals.","The paper proposes a differentially private ERM for convex functions with lower bounds in logarithmic terms. In the constrained case, the proposed DP-ERM is equivalent to approximate-DP, and in the unconstrained case, it is similar to DP-DP. The main difference is that the `2 loss function for pure-DP is a linear function, and the auxiliary dimension of the ` 2 loss is the one-way marginals. The paper also proposes to use biased mean property for fingerprinting codes."
2213,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"gradient flow USED-FOR machine learning applications. Wasserstein metric FEATURE-OF space of probability densities. approach USED-FOR Wasserstein gradient flow. finite difference USED-FOR approach. finite difference USED-FOR Wasserstein gradient flow. scalable proximal gradient type algorithm USED-FOR Wasserstein gradient flow. variational formulation of the objective function USED-FOR JKO proximal map. variational formulation of the objective function USED-FOR method. primal - dual optimization USED-FOR JKO proximal map. heat equation CONJUNCTION porous medium equation. porous medium equation CONJUNCTION heat equation. framework USED-FOR Wasserstein gradient flows. porous medium equation HYPONYM-OF Wasserstein gradient flows. heat equation HYPONYM-OF Wasserstein gradient flows. OtherScientificTerm are grid, and inner and outer loops. Task is primal - dual problem. Generic is algorithm. ",This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow for machine learning applications. The proposed approach is based on a finite difference between the space of probability densities of the Wassersteins metric and the grid. The method uses a variational formulation of the objective function for the JKO proximal map and a primal-dual optimization to find the optimal solution. The authors show that the proposed algorithm is computationally efficient and can be applied to a wide range of problems. ,"This paper proposes a novel approach to learn Wasserstein gradient flow for machine learning applications. The approach is based on a scalable proximal gradient type algorithm. The proposed method uses a variational formulation of the objective function for the JKO proximal map, and a primal-dual optimization for the inner and outer loops. The authors show that the proposed approach can achieve a finite difference in the space of probability densities, which is a special case of the primal -dual problem. The paper also proposes a new framework for learning WASSERstein gradient flows such as the heat equation and the porous medium equation. "
2229,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,ML algorithm CONJUNCTION hyper - parameter configuration. hyper - parameter configuration CONJUNCTION ML algorithm. ML algorithm USED-FOR AutoML problem. approach USED-FOR meta - features. Optimal Transport procedure USED-FOR meta - features. MetaBu meta - features USED-FOR topology. hyper - parameter configurations USED-FOR AutoML. MetaBu meta - features USED-FOR AutoML systems. AutoSkLearn CONJUNCTION Probabilistic Matrix Factorization. Probabilistic Matrix Factorization CONJUNCTION AutoSkLearn. OpenML CC-18 benchmark EVALUATE-FOR AutoML systems. OpenML CC-18 benchmark EVALUATE-FOR MetaBu meta - features. AutoSkLearn HYPONYM-OF AutoML systems. Probabilistic Matrix Factorization HYPONYM-OF AutoML systems. topology USED-FOR intrinsic dimensionality. intrinsic dimensionality FEATURE-OF OpenML benchmark. MetaBu meta - features USED-FOR intrinsic dimensionality. MetaBu meta - features USED-FOR topology. Method is MetaBu. OtherScientificTerm is manually designed meta - features. ,"This paper studies the AutoML problem with an ML algorithm and a hyper-parameter configuration. The authors propose MetaBu, a new approach to learn meta-features from Optimal Transport procedure. MetaBu is an extension of MetaBu to AutoML systems such as AutoSkLearn and Probabilistic Matrix Factorization. The main idea is to learn a topology of the topology by using MetaBu meta-feature to learn the intrinsic dimensionality of AutoML. The experiments on the OpenML CC-18 benchmark show that MetaBu improves the performance of Meta-features on AutoMLs. ","This paper proposes a new approach to learn meta-features for the AutoML problem. The approach is based on the Optimal Transport procedure. The authors propose a new ML algorithm and a hyper-parameter configuration for AutoML. MetaBu meta-feature is used to learn the topology of AutoML systems such as AutoSkLearn, Probabilistic Matrix Factorization, and OpenML CC-18 benchmark. The topology is used for intrinsic dimensionality, which is a measure of how well a system is able to learn a set of manually designed meta- features. "
2245,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"Federated learning ( FL ) USED-FOR distributed learning framework. robustness FEATURE-OF models. Split - Mix FL strategy USED-FOR heterogeneous participants. base sub - networks USED-FOR customization. communication CONJUNCTION storage. storage CONJUNCTION communication. storage CONJUNCTION inference. inference CONJUNCTION storage. split - mix strategy USED-FOR customization. method COMPARE heterogeneous - architecture FL methods. heterogeneous - architecture FL methods COMPARE method. in - situ customization EVALUATE-FOR heterogeneous - architecture FL methods. in - situ customization EVALUATE-FOR method. Task is FL scenarios. OtherScientificTerm are hardware and inference dynamics, and inference requirements. Method are FL approaches, and FL. ","This paper studies federated learning (FL) for distributed learning framework. The authors propose a Split-Mix FL strategy to learn heterogeneous participants from a set of base sub-networks, which can then be used for customization. They show that the proposed method achieves better performance than heterogeneous-architecture FL methods in terms of in-situ customization and robustness against hardware and inference dynamics. They also provide a theoretical analysis of FL scenarios. ","This paper proposes federated learning (FL) for distributed learning framework. The authors propose a Split-Mix FL strategy for heterogeneous participants that allows for customization of base sub-networks with different hardware and inference dynamics. They show that the proposed method outperforms heterogeneous-architecture FL methods in terms of in-situ customization and robustness to FL scenarios. They also show that FL approaches are more robust to FL approaches, and that the inference requirements are lower than those of FL. "
2261,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"extragradient - type algorithm USED-FOR nonconvex - nonconcave minimax problems. local solution USED-FOR general minimax problems. first order methods USED-FOR variational inequalities. problem class USED-FOR non - trivial structures. algorithms USED-FOR limit cycles. algorithm USED-FOR constrained and regularized problems. adaptive stepsize USED-FOR stepsizes. adaptive stepsize PART-OF algorithm. limit cycles FEATURE-OF operator. it USED-FOR training of generative adversarial networks. variant USED-FOR training of generative adversarial networks. stochastic oracles USED-FOR variant. monotone setting FEATURE-OF it. OtherScientificTerm are weak Minty variational inequality ( MVI ), and weak MVI. Generic is scheme. Method are generative adversarial networks, and stochastic algorithm. ","This paper proposes an extension of the Extragradient-type algorithm for nonconvex-nonconcave minimax problems with a local solution. The authors consider the weak Minty variational inequality (MVI) problem class, where the objective is to find a solution to a set of non-trivial structures in the problem class. The main idea is to use first order methods to solve the variational inequalities. The proposed scheme is based on a stochastic algorithm, and it is applied to the training of generative adversarial networks. The algorithm is able to solve both constrained and regularized problems in a monotone setting, and the authors show that the adaptive stepsize of the stepsizes of the operator can be used to reduce the number of limit cycles in the operator. ","This paper proposes an extension of the extension-type algorithm for nonconvex-nonconcave minimax problems to non-trivial structures with weak Minty variational inequality (MVI). The main idea of the proposed scheme is to learn a local solution to general minimax problem with variational inequalities. The authors show that the proposed algorithm can be used to solve constrained and regularized problems with limit cycles. The proposed algorithm uses adaptive stepsize to learn the stepsizes of the operator in the limit cycles, and it can be applied to training of generative adversarial networks with stochastic oracles. "
2277,SP:af22742091277b726f67e7155b412dd35f29e804,"neural contextual bandits HYPONYM-OF contextual bandits. learning algorithm USED-FOR raw feature vector. upper confidence bound ( UCB ) approach USED-FOR last linear layer ( shallow exploration ). upper confidence bound ( UCB ) approach USED-FOR learning algorithm. finitetime regret EVALUATE-FOR algorithm. neural contextual bandit algorithms COMPARE approach. approach COMPARE neural contextual bandit algorithms. deep neural network USED-FOR it. OtherScientificTerm are reward generating function, and learning time horizon. ","This paper studies the problem of contextual bandits, i.e., contextual bandits where the goal is to generate a reward generating function from a set of observations. The authors propose a learning algorithm to learn the raw feature vector from the observations. They use the upper confidence bound (UCB) approach for the last linear layer (shallow exploration) of the learning algorithm. They show that the proposed algorithm has a finitetime regret of $O(\sqrt{T})$ with respect to the learning time horizon. They also show that it can be trained with a deep neural network. Finally, they show that their approach outperforms other neural contextual bandit algorithms.",This paper proposes a novel learning algorithm for learning a raw feature vector. The learning algorithm is based on the upper confidence bound (UCB) approach for the last linear layer (shallow exploration) of the reward generating function. The authors show that the proposed algorithm achieves finitetime regret with respect to the learning time horizon. The proposed approach is shown to outperform existing neural contextual bandit algorithms. 
2293,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"Training action space selection USED-FOR reinforcement learning ( RL ). Shapley - inspired methodology USED-FOR action space categorization. Monte Carlo simulation USED-FOR unnecessary explorations. Monte Carlo simulation PART-OF methodology. cloud infrastructure resource tuning case study EVALUATE-FOR methodology. It USED-FOR search space. it USED-FOR RL model design. data - driven methodology USED-FOR reinforcement learning algorithms. OtherScientificTerm are complex state - action relationships, and exponential - time shapley computations. ",This paper proposes a Shapley-inspired methodology for action space selection in reinforcement learning (RL). The methodology is based on Monte Carlo simulation in order to avoid unnecessary explorations in the search space. It is a data-driven methodology that can be applied to a variety of reinforcement learning algorithms with complex state-action relationships. Experiments on cloud infrastructure resource tuning case study demonstrate the effectiveness of the proposed methodology. It also shows that it can be used in RL model design.,"This paper proposes a Shapley-inspired methodology for reinforcement learning (RL) based on training action space selection for complex state-action relationships. The methodology is based on Monte Carlo simulation to avoid unnecessary explorations. It is shown that it can be used in the search space, and it can also be used for RL model design. Experiments on cloud infrastructure resource tuning case study show that the proposed methodology can be applied to reinforcement learning algorithms based on a data-driven methodology. "
2309,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"approach USED-FOR probably approximately correct ( PAC ) prediction sets. importance weights USED-FOR algorithm. confidence intervals FEATURE-OF importance weights. DomainNet CONJUNCTION ImageNet. ImageNet CONJUNCTION DomainNet. approach USED-FOR covariate shifts. DomainNet USED-FOR approach. ImageNet USED-FOR approach. PAC constraint FEATURE-OF approaches. PAC constraint FEATURE-OF algorithm. Method are machine learning, predictive model, and uncertainty quantification algorithms. OtherScientificTerm are data distribution, and covariate shift. Generic is shifts. Metric is average normalized size. ","This paper proposes a new approach for probably approximately correct (PAC) prediction sets. The proposed algorithm is based on importance weights with confidence intervals that are based on the data distribution. The key idea is to use a predictive model to predict the covariate shift in the data, and then use uncertainty quantification algorithms to estimate the shifts. The approach is evaluated on DomainNet and ImageNet. The PAC constraint of the proposed algorithm outperforms existing approaches in terms of PAC constraint.","This paper proposes a new approach to learn probably approximately correct (PAC) prediction sets. The proposed algorithm is based on importance weights with confidence intervals. The key idea is to learn a predictive model that predicts the covariate shift in the data distribution, and then use uncertainty quantification algorithms to quantify the shifts. The approach is evaluated on DomainNet and ImageNet. The authors show that the proposed algorithm outperforms other approaches under the PAC constraint. "
2325,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"generalization error EVALUATE-FOR iterative SSL algorithms. information - theoretic principles USED-FOR generalization error. binary Gaussian mixture model HYPONYM-OF model. benchmark datasets EVALUATE-FOR model. MNIST and CIFAR datasets HYPONYM-OF benchmark datasets. OtherScientificTerm are model parameters, class conditional variances, and pseudo - labelling iterations. ","This paper studies the generalization error of iterative SSL algorithms under information-theoretic principles. The model is a binary Gaussian mixture model, where the model parameters are defined by class conditional variances. The proposed model is evaluated on several benchmark datasets, including MNIST and CIFAR datasets. The results show that pseudo-labelling iterations can improve the performance.","This paper studies the generalization error of iterative SSL algorithms based on information-theoretic principles. The authors propose a model based on a binary Gaussian mixture model, where the model parameters are defined as class conditional variances. The model is evaluated on two benchmark datasets, MNIST and CIFAR datasets, and is shown to be robust to pseudo-labelling iterations."
2341,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"model - free reinforcement learning algorithms USED-FOR policy. intentional action sequences USED-FOR high value regions. intentional action sequences USED-FOR plans. it USED-FOR plans. multi - step plans USED-FOR temporally coordinated exploration. GPM USED-FOR temporally coordinated exploration. multi - step plans USED-FOR GPM. GPM USED-FOR it. crude initial plan generator USED-FOR GPM. benchmark environments EVALUATE-FOR baseline methods. OtherScientificTerm are inefficient exploration, single step nature, single step level, consistent movement, and multi - step plan. Method are Generative Planning method ( GPM ), generative planning, and actionrepeat strategy. ","This paper proposes a new generative planning method (GPM) for the problem of learning a policy from model-free reinforcement learning algorithms. The GPM is based on the idea that inefficient exploration can lead to a single step nature, where the goal is to learn a policy that maximizes the performance of the current policy. The authors propose to use intentional action sequences to learn high value regions, and then use the learned plans as the initial plan generator to generate new plans. The multi-step plans are then used to train the GPM for temporally coordinated exploration, and it is shown that it is able to learn better plans than existing baseline methods in a variety of benchmark environments. ","The paper proposes a generative planning method (GPM) that uses model-free reinforcement learning algorithms to learn a policy. GPM uses a crude initial plan generator to generate a set of high value regions and then uses intentional action sequences to generate plans for those plans. The single step nature of GPM is that it can be used to generate multiple plans at a single step level, and it can also be used for temporally coordinated exploration with multi-step plans. This is done by minimizing the number of steps in the single step, and the number in the multi step level is proportional to the consistent movement. The paper also proposes an actionrepeat strategy to avoid inefficient exploration. Experiments on several benchmark environments show that GPM outperforms baseline methods."
2357,SP:ce6a93847209a0926ed0be5190378a3f61db1935,"deep linear and nonlinear matrix factorizations USED-FOR machine learning. deep learning CONJUNCTION tensor decomposition. tensor decomposition CONJUNCTION deep learning. matrices CONJUNCTION tensors. tensors CONJUNCTION matrices. factorization methods USED-FOR matrix and tensor completion problems. methods COMPARE matrix and tensor factorization methods. matrix and tensor factorization methods COMPARE methods. generalization error bounds EVALUATE-FOR matrix and tensor factorization methods. generalization error bounds EVALUATE-FOR methods. synthetic data and real datasets EVALUATE-FOR methods. recovery accuracy EVALUATE-FOR baselines. methods COMPARE baselines. baselines COMPARE methods. synthetic data and real datasets EVALUATE-FOR baselines. recovery accuracy EVALUATE-FOR methods. Method are deep nonlinear matrix factorization methods, and multi - mode deep matrix and tensor factorizations. ",This paper studies the problem of matrix factorization in deep linear and nonlinear matrix factorizations for machine learning. The authors consider the matrix and tensor completion problems where the number of matrices and tensors is large. They show that the generalization error bounds for matrix and the tensor factorization methods are lower than those of existing methods. They also show that their methods have better recovery accuracy than existing baselines on synthetic data and real datasets. ,This paper proposes a new method for matrix factorization for machine learning. The main idea is to use deep linear and nonlinear matrix factorizations to improve the generalization error bounds of deep learning and tensor decomposition. The authors show that the proposed method outperforms the existing methods on both synthetic data and real datasets. They also show that their method can achieve better recovery accuracy than the baselines. 
2373,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"interpretation technique USED-FOR structured output models. features USED-FOR structured model. function USED-FOR interpreter. energy - based training process USED-FOR interpreter function. structural information PART-OF model. simulated and real data sets EVALUATE-FOR method. OtherScientificTerm are output variables, computational path of output variables, feature, output variable, and input space. Method are structured models, and structured output model. ","This paper proposes a new interpretation technique for structured output models. The proposed method is based on the idea that the output variables of a structured model can be represented as a set of features, and the input variables can be interpreted as a computational path of output variables. The key idea is to learn a function that can be used as an interpreter for each input variable. The interpreter function is trained using an energy-based training process. The authors show that the proposed method performs well on simulated and real data sets. ","This paper proposes an interpretation technique for structured output models, where the output variables are represented as a set of features, and the input variables are a computational path of output variables. The proposed method is evaluated on simulated and real data sets. The authors propose a structured output model with structural information in the input space, and an interpreter function based on an energy-based training process. "
2389,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"approaches USED-FOR distributional DRL. reward function USED-FOR agent behavior. variance reduction measures USED-FOR on - policy learning. asymptotically consistent estimate of the policy gradient USED-FOR CDF - based objectives. sampling USED-FOR asymptotically consistent estimate of the policy gradient. sampling USED-FOR CDF - based objectives. algorithm USED-FOR agents. risk profiles FEATURE-OF penalty - based formulations. accumulation of positive rewards CONJUNCTION frequency of incurred penalties. frequency of incurred penalties CONJUNCTION accumulation of positive rewards. OpenAI Safety Gym environments FEATURE-OF penalty - based formulations. penalty - based formulations USED-FOR agents. risk profiles FEATURE-OF agents. risk profile COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) COMPARE risk profile. risk profile COMPARE PPO. PPO COMPARE risk profile. Proximal Policy Optimization ( PPO ) COMPARE PPO. PPO COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) CONJUNCTION positive reward. positive reward CONJUNCTION Proximal Policy Optimization ( PPO ). positive reward COMPARE PPO. PPO COMPARE positive reward. Lagrangians USED-FOR cost levels. positive reward EVALUATE-FOR risk profile. Lagrangians USED-FOR PPO. Method is deep reinforcement learning ( DRL ) agents. Generic are policy, approach, and technique. Task is human decision - making. OtherScientificTerm are distributional context, projected distribution of returns, distribution of full - episode outcomes, cumulative distribution function ( CDF ), relative quality, continuous and discrete action spaces, and policy gradient. ","This paper studies the problem of deep reinforcement learning (DRL) agents in the distributional context, where the agent behavior depends on the reward function. The authors propose two approaches to learn a distributional DRL. The first approach, Proximal Policy Optimization (PPO), is based on variance reduction measures for on-policy learning. The second approach, PPO, uses sampling to estimate the asymptotically consistent estimate of the policy gradient for CDF-based objectives.  The authors show that the risk profiles of the agents trained with these two penalty-based formulations in OpenAI Safety Gym environments are different risk profiles for different agents, and that PPO has a better risk profile compared to Pro-PPO and PPO with a positive reward than PPO without the Lagrangians.   ","This paper presents a new approach to learning a distributional DRL agent in the context of human decision-making. The approach is based on variance reduction measures for on-policy learning, where the goal is to learn an agent behavior that maximizes the reward function in the distributional context. The authors propose a new algorithm for learning agents with different risk profiles for different penalty-based formulations in the OpenAI Safety Gym environments. The key idea of the proposed approach is to use sampling for the asymptotically consistent estimate of the policy gradient for CDF-based objectives. The proposed algorithm is evaluated on both continuous and discrete action spaces, and the authors show that the proposed risk profile is better than PPO and Proximal Policy Optimization (PPO) in terms of positive reward and Lagrangians for cost levels."
2405,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"Interactive Neural Process ( INP ) HYPONYM-OF Bayesian active learning framework. Bayesian active learning framework USED-FOR deep learning surrogate model. Interactive Neural Process ( INP ) USED-FOR simulation. deep sequence model CONJUNCTION active learning. active learning CONJUNCTION deep sequence model. neural process CONJUNCTION deep sequence model. deep sequence model CONJUNCTION neural process. deep sequence model USED-FOR framework. neural process USED-FOR framework. active learning USED-FOR framework. spatiotemporal neural process model USED-FOR simulator dynamics. model USED-FOR latent process. latent process USED-FOR intrinsic uncertainty. latent information gain USED-FOR acquisition function. Bayesian active learning algorithms USED-FOR simulator. approach COMPARE random sampling. random sampling COMPARE approach. sample complexity EVALUATE-FOR random sampling. high dimension FEATURE-OF random sampling. sample complexity EVALUATE-FOR approach. framework USED-FOR rapid simulation and scenario exploration. framework USED-FOR complex infectious disease simulator. Task is Stochastic simulations. OtherScientificTerm are fine - grained resolution, and theoretical analysis. ","This paper proposes a Bayesian active learning framework for deep learning surrogate model. The framework is based on the Interactive Neural Process (INP) HYPONYM, which is an extension of the recent work on simulation. The authors propose a framework that combines a deep sequence model, active learning, and a neural process. The model learns a latent process to capture the intrinsic uncertainty in the simulator dynamics, and uses a spatiotemporal neural process model to model simulator dynamics. The proposed approach is shown to have a lower sample complexity than random sampling with high dimension. The theoretical analysis shows that the proposed approach outperforms random sampling in terms of sample complexity. The experimental results demonstrate the effectiveness of the proposed framework for rapid simulation and scenario exploration.","This paper proposes a Bayesian active learning framework for deep learning surrogate model for simulation. The framework is based on the Interactive Neural Process (INP) HYPONYM, which combines a deep sequence model, active learning, and a neural process. The proposed framework is used for rapid simulation and scenario exploration in Stochastic simulations. The simulator dynamics are modeled by a spatiotemporal neural process model. The model learns a latent process to capture intrinsic uncertainty, and the latent information gain is used to optimize the acquisition function. The approach is compared to random sampling with high dimension in terms of sample complexity. Experiments on a complex infectious disease simulator show that the proposed approach achieves better sample complexity than random sampling. "
2421,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"Differentially Private ( DP ) learning USED-FOR large deep learning models of text. hyperparameters USED-FOR DP optimization. hyperparameters CONJUNCTION fine - tuning objectives. fine - tuning objectives CONJUNCTION hyperparameters. pretraining procedure USED-FOR fine - tuning objectives. private NLP models COMPARE private training approaches. private training approaches COMPARE private NLP models. private training approaches CONJUNCTION nonprivate baselines. nonprivate baselines CONJUNCTION private training approaches. private NLP models COMPARE nonprivate baselines. nonprivate baselines COMPARE private NLP models. moderately - sized corpora USED-FOR DP optimization. DP optimization USED-FOR pretrained models. moderately - sized corpora USED-FOR pretrained models. linear layer PART-OF model. per - example gradients USED-FOR linear layer. memory saving technique USED-FOR clipping. clipping PART-OF DP - SGD. large Transformers USED-FOR DP - SGD. memory saving technique USED-FOR DP - SGD. privately training Transformers COMPARE non - private training. non - private training COMPARE privately training Transformers. technique USED-FOR privately training Transformers. memory cost EVALUATE-FOR non - private training. memory cost EVALUATE-FOR privately training Transformers. modest run - time overhead EVALUATE-FOR non - private training. DP optimization USED-FOR high - dimensional models. pretrained models USED-FOR private learning. Task is NLP tasks. Metric is computational overhead. Method is large pretrained models. OtherScientificTerm are noise, and dimension - dependent performance degradation. ",This paper studies the problem of Differentially Private (DP) learning for large deep learning models of text. The authors propose DP optimization for pretrained models with hyperparameters and fine-tuning objectives using a pretraining procedure. They show that DP optimization with a moderately-sized corpora improves the performance of the pretrained model with a linear layer with per-example gradients. The paper also shows that DP-SGD with large Transformers outperforms private NLP models with private training approaches and nonprivate baselines on NLP tasks.  ,"This paper proposes Differentially Private (DP) learning for large deep learning models of text. The authors propose to use hyperparameters, fine-tuning objectives, and a pretraining procedure to improve the computational overhead of large pretrained models. DP optimization for high-dimensional models using DP optimization on moderately-sized corpora and on the linear layer of a model with per-example gradients is proposed. The paper shows that DP-SGD with clipping and a memory saving technique can reduce the memory cost of privately training Transformers compared to non-private training with modest run-time overhead. Experiments on NLP tasks are conducted to show that DP optimization improves the performance of high-deterministic models with noise and dimension-dependent performance degradation compared to private training approaches and nonprivate baselines."
2437,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. length CONJUNCTION size. size CONJUNCTION length. size CONJUNCTION strength. strength CONJUNCTION size. strength HYPONYM-OF joint attributes. length HYPONYM-OF joint attributes. size HYPONYM-OF joint attributes. design procedure PART-OF decision - making process. agent PART-OF decision - making process. design procedure USED-FOR agent. skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. transform actions USED-FOR joint attributes. transform actions USED-FOR skeletal structure. control actions USED-FOR design. message passing USED-FOR joint - specific actions. policy gradient methods USED-FOR approach. approach USED-FOR joint optimization of agent design and control. joint optimization of agent design and control CONJUNCTION experience sharing. experience sharing CONJUNCTION joint optimization of agent design and control. experience sharing USED-FOR approach. approach COMPARE prior methods. prior methods COMPARE approach. Transform2Act COMPARE prior methods. prior methods COMPARE Transform2Act. Transform2Act HYPONYM-OF approach. convergence speed EVALUATE-FOR approach. convergence speed EVALUATE-FOR prior methods. giraffes CONJUNCTION squids. squids CONJUNCTION giraffes. squids CONJUNCTION spiders. spiders CONJUNCTION squids. OtherScientificTerm are agent ’s functionality, and design space. Generic is function. Method are optimal controller, conditional policy, and graph - based policy. Metric is sample efficiency. ","This paper proposes a new approach for joint optimization of agent design and control. The key idea is to use a design procedure to learn an agent’s functionality and joint attributes (e.g., skeletal structure, joint attributes, strength, length, size, etc.) in a decision-making process. The design procedure is then used to train the agent to select the optimal controller, and the conditional policy is learned by message passing between the design and the control actions. The authors show that the proposed approach achieves better sample efficiency than prior methods by using policy gradient methods. They also show that their approach performs better than the state-of-the-art in terms of convergence speed compared to prior methods. ","This paper proposes a new approach for joint optimization of agent design and control. The approach is based on policy gradient methods. The key idea is to learn an agent’s functionality in the design space, and then use a design procedure to guide the agent in the decision-making process. The optimal controller is a conditional policy, and the control actions are joint-specific actions. The paper also proposes a graph-based policy. Experiments show that the proposed approach outperforms prior methods in terms of convergence speed and experience sharing."
2453,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"view synthesis CONJUNCTION 3D object representation and rendering. 3D object representation and rendering CONJUNCTION view synthesis. Implicit neural representations USED-FOR tasks. multi - layer perceptrons ( MLPs ) USED-FOR Implicit neural representations. 3D object representation and rendering HYPONYM-OF tasks. view synthesis HYPONYM-OF tasks. MLP USED-FOR image, video, or 3D object. MLP USED-FOR training. coordinate - based MLPs USED-FOR implicit neural representations. inference CONJUNCTION training. training CONJUNCTION inference. CoordX USED-FOR initial layers. coordinate - based MLPs USED-FOR inference. coordinate - based MLPs USED-FOR training. layers USED-FOR intermediate features. accuracy EVALUATE-FOR baseline MLP. training CONJUNCTION inference. inference CONJUNCTION training. architecture USED-FOR implicit neural representation tasks. speedup EVALUATE-FOR baseline model. Generic are representations, approach, and them. Method is split MLP architecture. OtherScientificTerm is memory overheads. ","This paper proposes a new split MLP architecture for the task of image, video, and 3D object recognition. Implicit neural representations are used for these tasks using multi-layer perceptrons (MLPs) and are trained using MLP. The proposed approach is based on coordinate-based MLPs to learn implicit neural representations. The initial layers of the initial layers are trained with CoordX, and the intermediate layers are learned with a different memory overheads. The authors show that the proposed architecture is able to achieve state-of-the-art performance on implicit neural representation tasks such as view synthesis and rendering. The accuracy of the baseline MLP is also improved by a significant speedup compared to the baseline model. ","This paper proposes a new split MLP architecture for the task of multi-layer perceptrons (MLPs) for tasks such as view synthesis, 3D object representation and rendering. Implicit neural representations for these tasks are generated by MLP on an image, video, or 3D scene. The proposed approach is based on the idea that the initial layers of the MLP are shared across multiple layers, and the intermediate layers are shared among all of them. CoordX is used for initial layers, while coordinate-based MLPs are used for the implicit neural representations. Experiments show that the proposed architecture is able to perform well on implicit neural representation tasks with speedup compared to the baseline MLP with respect to accuracy, training, and inference. "
2469,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"method USED-FOR object - centric representations of visual scenes. appearance CONJUNCTION 3D pose. 3D pose CONJUNCTION appearance. structured representation USED-FOR shape. shape CONJUNCTION appearance. appearance CONJUNCTION shape. structured representation USED-FOR appearance. localized neural radiance field USED-FOR 2D views of the scene. object representation USED-FOR localized neural radiance field. differentiable rendering process USED-FOR localized neural radiance field. differentiable rendering process USED-FOR 2D views of the scene. reconstruction loss USED-FOR model. inferred scenes USED-FOR representations. 3D object representations USED-FOR visual reasoning task. CATER dataset USED-FOR 3D object representations. Method are INFERNO, and neural 3D rendering. OtherScientificTerm are annotations, rendered scenes, and supervision. ","This paper proposes a method for learning object-centric representations of visual scenes. The proposed method, INFERNO, is based on a differentiable rendering process that uses a localized neural radiance field to generate 2D views of the scene and a structured representation of the shape and the 3D pose. The model is trained using a reconstruction loss to learn the representations from inferred scenes. In the experiments, the authors show that the proposed method can achieve state-of-the-art performance on the CATER dataset for 3D object representations for a visual reasoning task. ","This paper proposes a method for learning object-centric representations of visual scenes. The key idea is to use a differentiable rendering process to generate 2D views of the scene with a localized neural radiance field based on the object representation and a structured representation of the shape and the 3D pose. The model is trained with a reconstruction loss, where annotations are extracted from the rendered scenes and the representations are learned from inferred scenes. Experiments on the CATER dataset show that the proposed method is able to learn 3D object representations for a visual reasoning task. "
2485,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"feature attribution framework USED-FOR GNN ’s prediction. features USED-FOR GNN ’s prediction. edges HYPONYM-OF features. subgraph USED-FOR model prediction. distribution shift USED-FOR out - ofdistribution problem. surrogate variable USED-FOR subgraphs. generative model USED-FOR unbiased estimation of subgraph importance. explanation fidelity EVALUATE-FOR DSE. Method are graph neural networks ( GNNs ), GNN, in - depth causal analysis, Deconfounded Subgraph Evaluation ( DSE ), and front - door adjustment. OtherScientificTerm are influential subgraph, subgraph importance, OOD effect, explanatory subgraph, and data distribution. Task is evaluation. ","This paper proposes a novel feature attribution framework for GNN’s prediction based on graph neural networks (GNNs). The key idea is to use features such as edges and edges of a subgraph as a surrogate variable for the subgraph for model prediction. The subgraph importance is defined as the OOD effect between the two subgraphs, and the influence of the influential subgraph on the other subgraph. The authors then propose Deconfounded Subgraph Evaluation (DSE), which uses a generative model to perform unbiased estimation of subgraphness using the surrogate variable. They show that the explanation fidelity of DSE is better than the original GNN in terms of in-depth causal analysis. They also provide a front-door adjustment to improve the evaluation. ","This paper proposes a feature attribution framework for GNN’s prediction. The key idea is to use graph neural networks (GNNs) as a surrogate variable for the subgraphs. The subgraph is defined as a set of features (e.g., edges) and the influential subgraph as an OOD effect. The authors propose Deconfounded Subgraph Evaluation (DSE), which uses an in-depth causal analysis to estimate the model prediction of a subgraph based on the distribution shift in the out-ofdistribution problem. The generative model is used for unbiased estimation of subgraph importance. The evaluation is performed on a dataset with different data distribution, and the authors show that DSE improves explanation fidelity. "
2501,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"pretrained models COMPARE active learners. active learners COMPARE pretrained models. spurious correlations CONJUNCTION latent minority groups. latent minority groups CONJUNCTION spurious correlations. latent minority groups CONJUNCTION domain shifts. domain shifts CONJUNCTION latent minority groups. pretrained models COMPARE random sampling. random sampling COMPARE pretrained models. spurious correlations FEATURE-OF image and text datasets. data USED-FOR pretrained models. uncertainty sampling USED-FOR data. accuracy EVALUATE-FOR pretrained models. uncertainty sampling USED-FOR pretrained models. minority classes CONJUNCTION informative examples. informative examples CONJUNCTION minority classes. spurious feature CONJUNCTION class label. class label CONJUNCTION spurious feature. active learning COMPARE unpretrained models. unpretrained models COMPARE active learning. Active learning USED-FOR task ambiguity. Pretraining USED-FOR models. Pretraining USED-FOR task ambiguity. Pretraining USED-FOR active learners. active learners USED-FOR task ambiguity. disambiguating examples USED-FOR active learners. Method are machine learning systems, and pretraining process. Material is few - shot settings. OtherScientificTerm is shape. ","This paper studies the problem of few-shot learning, where the goal is to learn a model that is robust to spurious correlations, latent minority groups, and domain shifts. In this setting, the authors propose to use uncertainty sampling on the data to improve the performance of the pretrained models. The authors show that the uncertainty sampling improves the accuracy of the trained models by a significant margin compared to random sampling. They also show that active learners are more robust than unpretrained models in this setting. ","This paper proposes a new way of training machine learning systems. The authors propose a new pretraining process, called active learning, which is based on disambiguating examples. Active learning is used to reduce task ambiguity in the training of models. They show that active learning outperforms unpretrained models in few-shot settings. They also show that pretrained models outperform random sampling on image and text datasets with spurious correlations, latent minority groups, and domain shifts. "
2517,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,GRAPHIX HYPONYM-OF pre - trained graph edit model. automatically detecting and fixing bugs CONJUNCTION code quality issues. code quality issues CONJUNCTION automatically detecting and fixing bugs. pre - trained graph edit model USED-FOR automatically detecting and fixing bugs. code quality issues FEATURE-OF Java programs. pre - trained graph edit model USED-FOR code quality issues. sequence - tosequence models COMPARE GRAPHIX. GRAPHIX COMPARE sequence - tosequence models. abstract syntax structure of code USED-FOR GRAPHIX. multi - head graph encoder USED-FOR GRAPHIX. model USED-FOR graph edit actions. graph edit actions USED-FOR automated program repair. model USED-FOR automated program repair. autoregressive tree decoder PART-OF model. pre - training strategy USED-FOR GRAPHIX. pre - training strategy USED-FOR model. implicit knowledge of program structures USED-FOR model. deleted sub - tree reconstruction HYPONYM-OF pre - training strategy. unlabeled source code USED-FOR implicit knowledge of program structures. bug fixing task USED-FOR downstream learning. pre - training objective CONJUNCTION bug fixing task. bug fixing task CONJUNCTION pre - training objective. pre - training objective USED-FOR downstream learning. abstract and concrete code USED-FOR GRAPHIX. Wild Java benchmark EVALUATE-FOR GRAPHIX. CodeBERT CONJUNCTION BART. BART CONJUNCTION CodeBERT. GRAPHIX COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE GRAPHIX. GRAPHIX COMPARE baselines. baselines COMPARE GRAPHIX. baselines COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE baselines. GRAPHIX COMPARE BART. BART COMPARE GRAPHIX. GRAPHIX COMPARE CodeBERT. CodeBERT COMPARE GRAPHIX. BART HYPONYM-OF baselines. CodeBERT HYPONYM-OF baselines. GRAPHIX USED-FOR structural and semantic code patterns. Material is abstract and concrete source code. ,"This paper proposes GRAPHIX, a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The model is based on a multi-head graph encoder with an autoregressive tree decoder. The pre-training strategy is to learn a model with implicit knowledge of program structures from the abstract syntax structure of code, and then use the model to generate graph edit actions for automated program repair. The authors show that GRAPHix is able to identify structural and semantic code patterns in both abstract and concrete source code. They also show that the pre-train objective for downstream learning with a bug fixing task is equivalent to a pre -training objective for the pre -trained model with deleted sub-tree reconstruction. Finally, the authors demonstrate the effectiveness of the proposed model on the Wild Java benchmark. ","This paper proposes a pre-trained graph edit model, GRAPHIX, for automatically detecting and fixing bugs in Java programs. The pre-training strategy is based on deleted sub-tree reconstruction, and the model is a multi-head graph encoder with an autoregressive tree decoder. The model is trained on both abstract and concrete source code, and is evaluated on the Wild Java benchmark. The authors show that the model can automatically detect and fix bugs in code quality issues of Java programs, and that it can also perform graph edit actions for automated program repair. The implicit knowledge of program structures in the model comes from unlabeled source code. The paper also shows that GRAPHix is able to learn structural and semantic code patterns in both abstract-and-concrete source code (e.g., the abstract syntax structure of code) and sequence-tosequence models. Experiments are conducted on the wild Java benchmark, showing that GRAPIX outperforms the baselines (CodeBERT, BART, etc.) and outperforms other pre-train Transformer models. "
2533,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,Federated Adversarial Training ( FAT ) USED-FOR data privacy and governance issues. adversarial attack FEATURE-OF model robustness. inner - maximization optimization of Adversarial Training USED-FOR data heterogeneity. lower bound USED-FOR Federated Learning. convergence FEATURE-OF FAT. convergence EVALUATE-FOR α - weighted mechanism. adversarial learning methods CONJUNCTION federated optimization methods. federated optimization methods CONJUNCTION adversarial learning methods. α - WFAT COMPARE FAT. FAT COMPARE α - WFAT. benchmark datasets EVALUATE-FOR α - WFAT. benchmark datasets EVALUATE-FOR FAT. adversarial learning methods USED-FOR FAT. adversarial learning methods USED-FOR α - WFAT. Method is inner - maximization of Adversarial Training. ,"This paper studies Federated Adversarial Training (FAT) in the context of data privacy and governance issues. The authors consider the problem of model robustness against adversarial attack in the presence of data heterogeneity, and propose a lower bound on the convergence of Federated Learning under the inner-maximization optimization of Adversary Training. They show that the α-weighted mechanism of FAT achieves the same convergence as other adversarial learning methods and federated optimization methods. They also show that FAT achieves better performance on several benchmark datasets than the standard α-WFAT.","This paper proposes Federated Adversarial Training (FAT) to address data privacy and governance issues. The authors propose a lower bound for Federated Learning, which is based on the inner-maximization optimization of Adversary Training to reduce the data heterogeneity and improve model robustness against adversarial attack. The convergence of FAT is shown to be a convergence of the α-weight mechanism. The paper also shows that FAT outperforms adversarial learning methods and federated optimization methods on several benchmark datasets."
2549,SP:ff3c787512035e2af20778d53586752852196be9,"LML USED-FOR supervised learning. models USED-FOR semi - supervised continual learning exceptions. Mako HYPONYM-OF wrapper tool. wrapper tool PART-OF supervised LML frameworks. data programming USED-FOR wrapper tool. data programming USED-FOR Mako. Mako USED-FOR continual semi - supervised learning. labeled data USED-FOR continual semi - supervised learning. tool COMPARE fully labeled data. fully labeled data COMPARE tool. per - task accuracy CONJUNCTION resistance. resistance CONJUNCTION per - task accuracy. resistance EVALUATE-FOR catastrophic forgetting. resistance EVALUATE-FOR tool. per - task accuracy EVALUATE-FOR tool. CIFAR-10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. accuracy CONJUNCTION catastrophic forgetting prevention. catastrophic forgetting prevention CONJUNCTION accuracy. Mako USED-FOR unlabeled data. Mako USED-FOR LML tools. LML tools USED-FOR supervised learning. fully labeled data USED-FOR supervised learning. accuracy EVALUATE-FOR supervised learning. image classification data sets USED-FOR LML task sequences. CIFAR100 HYPONYM-OF image classification data sets. CIFAR-10 HYPONYM-OF image classification data sets. MNIST HYPONYM-OF image classification data sets. ORDisCo CONJUNCTION DistillMatch. DistillMatch CONJUNCTION ORDisCo. CNNL CONJUNCTION ORDisCo. ORDisCo CONJUNCTION CNNL. Mako COMPARE them. them COMPARE Mako. baseline semi - supervised LML tools COMPARE Mako. Mako COMPARE baseline semi - supervised LML tools. DistillMatch HYPONYM-OF baseline semi - supervised LML tools. CNNL HYPONYM-OF baseline semi - supervised LML tools. accuracy EVALUATE-FOR Mako. ORDisCo HYPONYM-OF baseline semi - supervised LML tools. Task are Lifelong machine learning ( LML ), and human learning process. Method is LML methods. OtherScientificTerm is knowledge base overhead. ",This paper proposes a new wrapper tool for supervised learning in Lifelong machine learning (LML). The wrapper tool is an extension of supervised LML frameworks such as Mako. The main idea of the wrapper is to use data programming for continual semi-supervised learning with labeled data. The authors show that the proposed tool is able to achieve better per-task accuracy and resistance than fully labeled data in supervised learning. The paper also shows that the performance of the proposed LML tools is comparable to the state-of-the-art in terms of accuracy and catastrophic forgetting prevention.,"This paper proposes a new wrapper tool for continual semi-supervised learning with labeled data. The wrapper tool is an extension of supervised LML frameworks, and the authors show that the proposed wrapper tool improves the per-task accuracy and the resistance to catastrophic forgetting. The authors also show that their wrapper tool outperforms fully labeled data in supervised learning. "
2565,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"adversarial examples USED-FOR Evading adversarial example detection defenses. attack techniques USED-FOR adversarial examples. Selective Projected Gradient Descent CONJUNCTION Orthogonal Projected Gradient Descent. Orthogonal Projected Gradient Descent CONJUNCTION Selective Projected Gradient Descent. Selective Projected Gradient Descent CONJUNCTION attack techniques. attack techniques CONJUNCTION Selective Projected Gradient Descent. technique USED-FOR detection defenses. accuracy EVALUATE-FOR technique. Generic are model, and attacks. OtherScientificTerm is gradients. Method is gradient - based attacks. Metric is detection rate. ","This paper studies the problem of learning adversarial examples for Evading adversarial example detection defenses against gradient-based attacks. The authors propose a new attack technique, Selective Projected Gradient Descent, which is a combination of two existing attack techniques. The proposed technique is shown to improve the detection accuracy of detection defenses by a significant margin. ",This paper proposes a new defense against adversarial examples for Evading adversarial example detection defenses. The proposed defense is based on gradient-based attacks. The authors propose two attack techniques: Selective Projected Gradient Descent (which is a variant of Orthogonal Projected Descent) and Orthogonality-based Attacks (which are based on Gradient-based Descent). The authors show that the proposed model is more robust to adversarial attacks than the baseline model. They also show that their technique improves detection defenses in terms of accuracy. 
2581,SP:5eef907024017849303477eed92f317438c87a69,data valuation CONJUNCTION model valuation. model valuation CONJUNCTION data valuation. feature interpretation CONJUNCTION data valuation. data valuation CONJUNCTION feature interpretation. model valuation USED-FOR ensembles. Valuation problems PART-OF machine learning applications. model valuation HYPONYM-OF Valuation problems. data valuation HYPONYM-OF Valuation problems. feature interpretation HYPONYM-OF Valuation problems. Shapley value CONJUNCTION Banzhaf value. Banzhaf value CONJUNCTION Shapley value. game - theoretic criteria USED-FOR problems. Banzhaf value HYPONYM-OF game - theoretic criteria. Shapley value HYPONYM-OF game - theoretic criteria. energy - based treatment USED-FOR cooperative games. maximum entropy principle USED-FOR energy - based treatment. one - step fixed point iteration USED-FOR ELBO objective. mean - field variational inference USED-FOR classical game - theoretic valuation criteria. mean - field variational inference USED-FOR energy - based model. one - step fixed point iteration USED-FOR classical game - theoretic valuation criteria. uniform initializations USED-FOR variational valuations. game - theoretic axioms FEATURE-OF variational valuations. decoupling error CONJUNCTION valuation. valuation CONJUNCTION decoupling error. valuation FEATURE-OF synthetic and real - world valuation problems. valuation EVALUATE-FOR Variational Index. decoupling error EVALUATE-FOR Variational Index. synthetic and real - world valuation problems EVALUATE-FOR Variational Index. Generic is criteria. Method is fixed point iteration. ,"This paper studies the problem of model valuation for ensembles. Valuation problems in machine learning applications such as data valuation, model valuation, and feature interpretation have been studied in the literature for many years. The authors propose a new set of game-theoretic criteria for these problems, Shapley value and Banzhaf value, which is a combination of the classical game-thoretic valuation criteria such as one-step fixed point iteration for the ELBO objective and mean-field variational inference for the energy-based model for cooperative games. These criteria are based on the maximum entropy principle, which allows for uniform initializations for variational valuations, which are well-studied in previous works. The paper shows that the Variational Index is a good measure of the decoupling error and the valuation in both synthetic and real-world valuation problems. ","This paper studies the problem of learning ensembles with variational valuations. Valuation problems in machine learning applications are important for many applications, including data valuation, model valuation, and feature interpretation. The authors propose a set of game-theoretic criteria for these problems, Shapley value, Banzhaf value, and other game-thruthic criteria. The main contribution of the paper is a one-step fixed point iteration for the ELBO objective, which is based on the maximum entropy principle of the energy-based treatment for cooperative games. The paper also proposes two new criteria, which are based on mean-field variational inference for the energy -based model, and uniform initializations for variational values. The Variational Index is a measure of the decoupling error and the valuation of a variational Valuation with respect to the game- theoretic axioms. "
2597,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"out - of - sample prediction error FEATURE-OF Epistemic uncertainty. approach USED-FOR epistemic uncertainty. intrinsic unpredictability HYPONYM-OF estimate of aleatoric uncertainty. active learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION active learning. estimator USED-FOR interactive learning environments. estimator USED-FOR epistemic uncertainty. active learning USED-FOR interactive learning environments. reinforcement learning USED-FOR interactive learning environments. sequential model optimization CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION sequential model optimization. methods USED-FOR uncertainty estimation. methods USED-FOR tasks. uncertainty estimation USED-FOR tasks. sequential model optimization HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. uncertainty estimates USED-FOR estimating uncertainty. DEUP USED-FOR probabilistic classification of images. uncertainty estimates USED-FOR probabilistic classification of images. synergistic drug combinations FEATURE-OF estimating uncertainty. DEUP USED-FOR uncertainty estimates. OtherScientificTerm are model variance, and generalization error. Method is Direct Epistemic Uncertainty Prediction ( DEUP ). ","This paper studies the problem of estimating the out-of-sample prediction error of Epistemic Uncertainty Prediction (DEUP). The authors propose a new approach to estimate epistemic uncertainty, i.e., the estimate of aleatoric uncertainty (i.e. intrinsic unpredictability). The estimator is based on active learning and reinforcement learning in interactive learning environments, where the model variance is large and the generalization error is large. The authors show that the estimator can be used for estimating epistemic uncertainties in a variety of tasks, including sequential model optimization, reinforcement learning, and active learning. The uncertainty estimates for estimating uncertainty for probabilistic classification of images with synergistic drug combinations are also provided. The experimental results show the effectiveness of the proposed methods for uncertainty estimation in various tasks.","This paper proposes a new approach to estimate epistemic uncertainty, i.e., the out-of-sample prediction error of Epistemic uncertainty. The key idea is to estimate the estimate of aleatoric uncertainty, which is the intrinsic unpredictability of the model variance. This estimator can be applied to interactive learning environments where active learning and reinforcement learning are used. The authors show that the proposed method, Direct Eqnepistemic Uncertainty Prediction (DEUP), can be used for estimating uncertainty estimates for probabilistic classification of images with synergistic drug combinations. They also show that DEUP can be combined with existing methods for uncertainty estimation for different tasks such as sequential model optimization and continuous reinforcement learning."
2613,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,Product quantization ( PQ ) USED-FOR approximate nearest neighbor ( ANN ) search systems. Product quantization ( PQ ) CONJUNCTION space rotation. space rotation CONJUNCTION Product quantization ( PQ ). disk storage USED-FOR embeddings. rotation learning methods USED-FOR quantization distortion. quantization distortion FEATURE-OF fixed embeddings. block Givens coordinate descent algorithms USED-FOR rotation matrix. geometric intuitions USED-FOR block Givens coordinate descent algorithms. Lie group theory USED-FOR geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF Lie group theory. SVD method COMPARE Givens algorithms. Givens algorithms COMPARE SVD method. runtime EVALUATE-FOR GPUs. GPUs USED-FOR Givens algorithms. runtime EVALUATE-FOR Givens algorithms. vanilla product quantization USED-FOR end - to - end training scenario. vanilla product quantization USED-FOR They. end - to - end training scenario EVALUATE-FOR They. Task is inner product computation. OtherScientificTerm is convex objectives. ,"This paper studies the problem of approximate nearest neighbor (ANN) search systems with Product quantization (PQ) and space rotation. Product quantized embeddings are stored in disk storage, and rotation learning methods have been shown to suffer from quantization distortion in fixed embedding. The authors propose block Givens coordinate descent algorithms to approximate the rotation matrix using geometric intuitions derived from Lie group theory (e.g., special orthogonal group SO(n) and geometric intuition of Lie groups theory). They show that the SVD method performs better than Given algorithms on GPUs in the end-to-end training scenario with vanilla product quantization. ","This paper proposes a novel quantization method for approximate nearest neighbor (ANN) search systems. Product quantization (PQ) and space rotation can be used to reduce the quantization distortion of fixed embeddings in disk storage. The authors propose to use block Givens coordinate descent algorithms to compute the rotation matrix of the embedding, which is a special orthogonal group of Lie group theory. They show that the proposed SVD method outperforms other Given algorithms in terms of runtime on GPUs and in an end-to-end training scenario with vanilla product quantization. "
2629,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"sensory information USED-FOR conceptual abstractions. Structure Mapping Theory USED-FOR human analogical reasoning. ( higher - order ) relations USED-FOR analogical mappings. two - stage neural framework USED-FOR visual analogies. Raven ’s Progressive Matrices HYPONYM-OF abstract visual reasoning test of fluid intelligence. Neural Structure Mapping ( NSM ) HYPONYM-OF two - stage neural framework. Raven ’s Progressive Matrices USED-FOR visual analogies. multi - task visual relationship encoder CONJUNCTION neural module net - based analogy inference engine. neural module net - based analogy inference engine CONJUNCTION multi - task visual relationship encoder. neural module net - based analogy inference engine USED-FOR framework. raw visual input USED-FOR multi - task visual relationship encoder. multi - task visual relationship encoder USED-FOR framework. structure USED-FOR analogical reasoning. NSM approach USED-FOR relational structure. Generic is them. Task are human intelligence, and Abstract reasoning. OtherScientificTerm are analogies, and novel domains. Material is known domains. Method is machine learning ( ML ) models. ","This paper proposes a two-stage neural framework, Neural Structure Mapping (NSM), for learning conceptual abstractions from sensory information. The framework is based on Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The authors propose a multi-task visual relationship encoder, a neural module net-based analogy inference engine, and an NSM approach to learn the relational structure of the data. The proposed framework is evaluated on a variety of known domains and novel domains.","This paper proposes a two-stage neural framework for learning visual analogies based on Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The proposed framework is based on Neural Structure Mapping (NSM) and a multi-task visual relationship encoder based on raw visual input. The authors propose to use (higher-order) relations to learn the analogical mappings between two images, and then use them for human analogical reasoning. They show that the proposed NSM approach is able to learn relational structure of the image, which is useful for learning the structure of an image. They also show that their approach can be applied to other machine learning (ML) models. "
2645,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,contrastive self - supervised method USED-FOR nucleotide genome representation learning. Self - GenomeNet HYPONYM-OF contrastive self - supervised method. Self - GenomeNet HYPONYM-OF self - supervised framework. method USED-FOR latent space. reverse - complement of genomic sequences USED-FOR method. reverse - complement of genomic sequences USED-FOR latent space. framework USED-FOR semantic representations. features USED-FOR context network. context network USED-FOR framework. encoder network USED-FOR features. context network USED-FOR semantic representations. unsupervised contrastive loss USED-FOR network. method COMPARE deep learning methods. deep learning methods COMPARE method. self - supervised and semi - supervised settings USED-FOR deep learning methods. self - supervised and semi - supervised settings USED-FOR method. representations USED-FOR datasets. Material is nucleotide genomic data. OtherScientificTerm is domain - specific characteristics. ,This paper proposes a contrastive self-supervised method for nucleotide genome representation learning. Self-GenomeNet uses reverse-combination of genomic sequences to represent the latent space. The proposed framework uses a context network to learn semantic representations from the encoder network. The network is trained with an unsupervised contrastive loss. Experiments show that the proposed method outperforms other deep learning methods in both self- supervised and semi-supervision settings. ,"This paper proposes a contrastive self-supervised method for nucleotide genome representation learning. Self-GenomeNet is an extension of Self-genomeNet, which is a self supervised framework for learning semantic representations from nucleotide genomic data. The proposed method learns a latent space using reverse-complement of genomic sequences, and then uses features from the context network to train an encoder network to generate semantic representations with domain-specific characteristics. The network is trained with an unsupervised contrastive loss, and the proposed method outperforms other deep learning methods on datasets with self- supervised and semi-supervision settings."
2661,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"low - level vision theory USED-FOR steerable filters. steerable feed - forward learning - based approach USED-FOR point clouds. spherical decision surfaces PART-OF steerable feed - forward learning - based approach. 3D steerability constraint USED-FOR hypersphere neurons. conformal embedding of Euclidean space USED-FOR 3D steerability constraint. conformal embedding of Euclidean space USED-FOR hypersphere neurons. synthetic point set CONJUNCTION real - world 3D skeleton data. real - world 3D skeleton data CONJUNCTION synthetic point set. spherical filter banks USED-FOR invariant class predictions. online optimization USED-FOR invariant class predictions. invariant class predictions USED-FOR known point sets. online optimization USED-FOR spherical filter banks. unknown orientations FEATURE-OF invariant class predictions. unknown orientations FEATURE-OF known point sets. Method is steerable convolutional neural networks. OtherScientificTerm are 3D geometry, rotational equivariance, and model parameters. Task is learning representations of point sets. ","This paper studies the problem of steerable filters in low-level vision theory. The authors propose a steerable feed-forward learning-based approach for point clouds based on spherical decision surfaces. The 3D steerability constraint for hypersphere neurons is derived from a conformal embedding of Euclidean space. Theoretically, the authors show that the 3D geometry of a point set is invariant to rotational equivariance, and that the model parameters are also invariant.  The authors then propose spherical filter banks for invariant class predictions on known point sets and unknown orientations. The theoretical results are validated on a synthetic point set and real-world 3D skeleton data.","This paper proposes a new low-level vision theory for steerable filters. The authors propose a steerable feed-forward learning-based approach for point clouds with spherical decision surfaces. The 3D steerability constraint for hypersphere neurons is based on the conformal embedding of Euclidean space. The main contribution of the paper is to show that the 3D geometry of the point clouds is invariant to rotational equivariance, which is an important property of steerable convolutional neural networks. Experiments are performed on a synthetic point set and real-world 3D skeleton data. The invariant class predictions of known point sets are obtained by online optimization of spherical filter banks. The paper also provides a theoretical analysis of the learning representations of point sets."
2677,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,Pre - trained language models ( PLMs ) USED-FOR continual learning. continual learning USED-FOR natural language problems. continual learning methods CONJUNCTION PLMs. PLMs CONJUNCTION continual learning methods. PLMs CONJUNCTION CL approaches. CL approaches CONJUNCTION PLMs. benchmarks EVALUATE-FOR continual learning. benchmarks EVALUATE-FOR CL approaches. PLMs CONJUNCTION CL methods. CL methods CONJUNCTION PLMs. representativeness probing analyses USED-FOR PLMs ’ performance characteristics. representativeness probing analyses USED-FOR layer - wise and task - wise manner. layer - wise and task - wise manner FEATURE-OF PLMs ’ performance characteristics. Task is Continual learning ( CL ). Generic is model. OtherScientificTerm is forgetting. Method is continual learning techniques. ,"This paper studies the problem of continual learning with pre-trained language models (PLMs) for continual learning in natural language problems. The authors propose Continual learning (CL), a model that is able to learn a sequence of PLMs in a continuous manner. The paper shows that CL approaches and PLMs are able to achieve state-of-the-art performance on a variety of benchmarks on continual learning tasks. They also show that PLMs’ performance characteristics can be improved by representativeness probing analyses in a layer-wise and task-wise manner. ","This paper proposes Continual learning (CL), a method for continual learning of pre-trained language models (PLMs) for natural language problems. The main idea is to learn a model that is able to learn to perform continual learning in the presence of forgetting. Experiments show that CL approaches outperform PLMs and other CL approaches on a variety of benchmarks. The authors also show that PLMs’ performance characteristics can be learned in a layer-wise and task-wise manner using representativeness probing analyses. "
2693,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"model poisoning attacks COMPARE centralized learning. centralized learning COMPARE model poisoning attacks. Federated learning COMPARE centralized learning. centralized learning COMPARE Federated learning. Federated learning USED-FOR model poisoning attacks. multi - party, distributed learning HYPONYM-OF Federated learning. model updates USED-FOR global model. Bulyan CONJUNCTION FABA. FABA CONJUNCTION Bulyan. FABA CONJUNCTION FoolsGold. FoolsGold CONJUNCTION FABA. Krum CONJUNCTION Bulyan. Bulyan CONJUNCTION Krum. FoolsGold HYPONYM-OF Byzantine - resilient federated learning algorithms. Krum HYPONYM-OF Byzantine - resilient federated learning algorithms. FABA HYPONYM-OF Byzantine - resilient federated learning algorithms. Bulyan HYPONYM-OF Byzantine - resilient federated learning algorithms. defense USED-FOR directed deviation attack. TESSERACT HYPONYM-OF defense. TESSERACT USED-FOR directed deviation attack. learning algorithms CONJUNCTION models. models CONJUNCTION learning algorithms. reputation scores USED-FOR TESSERACT. TESSERACT USED-FOR attack. robustness EVALUATE-FOR TESSERACT. Method are untargeted model poisoning attack, and federated learning. OtherScientificTerm are gradient updates, and gradient flips. Metric is test error rate. Task is model poisoning attack. ","This paper studies the problem of untargeted model poisoning attack in federated learning. Federated learning is a variant of multi-party, distributed learning where the global model is updated with model updates. The authors show that centralized learning is more robust to model poisoning attacks than Federated training. They also show that TESSERACT is a good defense against directed deviation attack. ","This paper studies the untargeted model poisoning attack against federated learning. Federated learning is a variant of multi-party, distributed learning. The authors show that centralized learning is more robust to model poisoning attacks compared to Federated Learning. They also show that TESSERACT is a defense against the directed deviation attack. The defense is based on the idea that the test error rate of the global model depends on the number of model updates in the training set. The paper also shows that the defense is robust to gradient flips. Finally, the authors provide some theoretical analysis on the robustness of the attack against different learning algorithms and models. "
2709,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"regularization CONJUNCTION model selection. model selection CONJUNCTION regularization. debiasing USED-FOR regularization. debiasing USED-FOR model selection. debiasing USED-FOR √ n - consistent and asymptotically normal estimation. double robustness CONJUNCTION Neyman orthogonality. Neyman orthogonality CONJUNCTION double robustness. functional - specific theoretical derivation USED-FOR influence function. correction term PART-OF plug - in estimator of the functional. Neural Nets CONJUNCTION Random Forests. Random Forests CONJUNCTION Neural Nets. Neural Nets USED-FOR Riesz representation of the linear functional. Random Forests USED-FOR Riesz representation of the linear functional. Neural Nets USED-FOR automatic debiasing procedure. Random Forests USED-FOR automatic debiasing procedure. value query oracle access USED-FOR linear functional. value query oracle access USED-FOR method. representation layers USED-FOR functions. stochastic gradient descent minimization USED-FOR Riesz representer and regression loss. stochastic gradient descent minimization USED-FOR multi - tasking Neural Net debiasing method. Random Forest method USED-FOR locally linear representation of the Riesz function. methodology USED-FOR arbitrary functionals. neural net based estimator USED-FOR average treatment effect functional. it COMPARE neural net based estimator. neural net based estimator COMPARE it. method USED-FOR estimating average marginal effects. gasoline demand FEATURE-OF semi - synthetic data of gasoline price changes. semi - synthetic data of gasoline price changes EVALUATE-FOR method. continuous treatments USED-FOR estimating average marginal effects. Task are causal and policy effects of interest, and Debiasing. ",This paper proposes a multi-tasking Neural Net debiasing method based on stochastic gradient descent minimization for the Riesz representer and regression loss. The proposed method is based on Neural Nets and Random Forests. The authors show that the proposed method outperforms the state-of-the-art in terms of double robustness and Neyman orthogonality. They also show that their method can be applied to arbitrary functionals.,This paper proposes a new method for estimating average marginal effects of the causal and policy effects of interest of interest. The authors propose to use a functional-specific theoretical derivation of the influence function as a plug-in estimator of the functional with a correction term. The proposed method is based on stochastic gradient descent minimization of the Riesz representer and regression loss. The method is evaluated on the multi-tasking Neural Net debiasing method with value query oracle access for the linear functional and Neural Nets and Random Forests for the RIESz representation of the linear function. The paper shows that the proposed methodology is able to estimate arbitrary functionals with respect to the representation layers of the functions. Debiasing is used to improve the double robustness and Neyman orthogonality of the regularization and model selection. Experiments are conducted on the gasoline demand of semi-synthetic data of gasoline price changes. 
2725,SP:96e1da163020441f9724985ae15674233e0cfe0d,"finite - time convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION finite - time convergence. sample complexity EVALUATE-FOR algorithm. single - agent actorcritic algorithms USED-FOR reinforcement learning. sample complexity bound EVALUATE-FOR single - agent actorcritic algorithms. Method is actor - critic algorithm. OtherScientificTerm are average reward, global average reward, and communication network. Generic is problem. Task is MARL setting. ","This paper proposes a new actor-critic algorithm for reinforcement learning with finite-time convergence and sample complexity. The algorithm is based on single-agent actorcritic algorithms. The main idea is to learn an average reward for each agent, and then use the global average reward to train a communication network. The authors show that the proposed algorithm achieves a sample complexity bound of $O(\sqrt{T})$ for the same number of iterations. ","This paper proposes an actor-critic algorithm for reinforcement learning with finite-time convergence and sample complexity. The algorithm is based on single-agent actorcritic algorithms. The main idea of the problem is to learn an average reward, which is the global average reward between the agent and the communication network. Experiments on MARL setting show that the sample complexity bound of the proposed algorithm is lower than that of the single-agents."
2741,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"contrastive learning USED-FOR large - scale self - supervised learning. approach USED-FOR large - scale self - supervised learning. contrastive learning HYPONYM-OF approach. conditional independence assumption USED-FOR guarantee. theory COMPARE contrastive methods. contrastive methods COMPARE theory. synthetic and real - world datasets EVALUATE-FOR contrastive methods. synthetic and real - world datasets EVALUATE-FOR theory. contrastive learning USED-FOR class - separated representations. Generic is it. OtherScientificTerm are positive samples, and intra - class samples. Task is surrogate task. ","This paper proposes a new approach for large-scale self-supervised learning, called contrastive learning. The main idea is to learn positive samples from intra-class samples, and then use the positive samples to train the surrogate task. The guarantee is based on conditional independence assumption. The theory is tested on synthetic and real-world datasets, and shows that contrastive methods can learn class-separated representations.","This paper proposes a new approach for large-scale self-supervised learning, called contrastive learning. The main idea is to learn class-separated representations from positive samples and negative samples. The guarantee is based on conditional independence assumption. Experiments on synthetic and real-world datasets show that the theory outperforms contrastive methods. "
2757,SP:b491314336c503b276e34e410cf461cb81294890,"Speech restoration USED-FOR distortions in speech signals. speech denoising CONJUNCTION speech declipping. speech declipping CONJUNCTION speech denoising. Prior methods USED-FOR single - task speech restoration ( SSR ). speech declipping HYPONYM-OF single - task speech restoration ( SSR ). speech denoising HYPONYM-OF single - task speech restoration ( SSR ). SSR systems USED-FOR speech restoration tasks. SSR systems USED-FOR speech restoration problem. speech super - resolution HYPONYM-OF speech restoration tasks. generative framework USED-FOR GSR task. VoiceFixer1 HYPONYM-OF generative framework. VoiceFixer1 USED-FOR GSR task. analysis stage CONJUNCTION synthesis stage. synthesis stage CONJUNCTION analysis stage. VoiceFixer USED-FOR speech analysis. synthesis stage USED-FOR speech analysis. synthesis stage PART-OF VoiceFixer. analysis stage PART-OF VoiceFixer. ResUNet USED-FOR analysis stage. neural vocoder USED-FOR synthesis stage. ResUNet CONJUNCTION neural vocoder. neural vocoder CONJUNCTION ResUNet. neural vocoder USED-FOR analysis stage. additive noise CONJUNCTION room reverberation. room reverberation CONJUNCTION additive noise. room reverberation CONJUNCTION low - resolution, and clipping distortions. low - resolution, and clipping distortions CONJUNCTION room reverberation. additive noise USED-FOR VoiceFixer. room reverberation FEATURE-OF VoiceFixer. low - resolution, and clipping distortions EVALUATE-FOR VoiceFixer. baseline GSR model COMPARE speech denoising SSR model. speech denoising SSR model COMPARE baseline GSR model. mean opinion score ( MOS ) EVALUATE-FOR speech denoising SSR model. mean opinion score ( MOS ) EVALUATE-FOR baseline GSR model. VoiceFixer COMPARE GSR baseline model. GSR baseline model COMPARE VoiceFixer. MOS score EVALUATE-FOR GSR baseline model. MOS score EVALUATE-FOR VoiceFixer. old movies CONJUNCTION historical speeches. historical speeches CONJUNCTION old movies. VoiceFixer USED-FOR historical speeches.","Speech restoration is an important problem in the field of speech restoration. Prior methods have focused on single-task speech restoration (SSR), where the goal is to remove distortions in speech signals. In this paper, the authors propose a generative framework, VoiceFixer1, to tackle the GSR task, which is a combination of two existing SSR systems: speech denoising and speech declipping. The authors show that the proposed model achieves state-of-the-art performance in terms of mean opinion score (MOS) compared to the baseline GSR baseline model.  ","Speech restoration is an important problem in the field of speech restoration, which aims to remove distortions in speech signals. Prior methods have been applied to single-task speech restoration (SSR), speech declipping, speech denoising, and speech super-resolution. However, the speech restoration problem is not well-studied in the literature. This paper proposes a generative framework, VoiceFixer1, to tackle the GSR task. The authors propose a synthesis stage, an analysis stage, and a neural vocoder for the synthesis stage and synthesis stage. The analysis stage consists of two stages: ResUNet and Resundet. The synthesis stage is used for speech analysis, while the analysis stage uses the neural vocoders for speech synthesis. Experiments are conducted on three speech restoration tasks: low-resolution, and clipping distortions, room reverberation, and additive noise. The results show that the proposed speech Denoising SSR model outperforms the baseline GSR model in terms of mean opinion score (MOS) and GSR baseline model on the low-resonance, and the additive noise is better on the room reverberations. "
2773,SP:c80a7392ec6147395a664734601fb389a1eb4470,"Residual Tensor Networks ( MVSRTN ) USED-FOR multivariate time series. Residual Tensor Networks ( MVSRTN ) USED-FOR Variable Space. tensor network USED-FOR variable space. low - rank approximation USED-FOR variable space. low - rank approximation USED-FOR tensor network. translation invariance FEATURE-OF network. tensor components USED-FOR translation invariance. tensor components USED-FOR network. it USED-FOR space - approximated tensor network. seriesvariable encoder USED-FOR variable space. skip - connection layer USED-FOR dissemination of information. scale HYPONYM-OF dissemination of information. multivariate time series forecasting benchmark datasets EVALUATE-FOR method. Material is Multivariate time series. OtherScientificTerm are latent space, time window, and long - term sequences. Generic is framework. Method is N - order residual connection approach. ","This paper proposes a new framework for multivariate time series forecasting based on Residual Tensor Networks (MVSRTN). The framework is based on the N-order residual connection approach, where the latent space is represented as a series variable encoder, and the tensor network is used to represent the variable space as a low-rank approximation of the latent variable space. The tensor components of the network are then used to improve the translation invariance of the trained network, and it is used as a space-approximated tensor for the space-invariant tensor. The authors also propose a skip-connection layer for the dissemination of information in the form of scale. The proposed method is evaluated on a variety of multi-scale forecasting benchmark datasets.",This paper proposes Residual Tensor Networks (MVSRTN) for multivariate time series. The framework is based on the N-order residual connection approach. The key idea is to use a tensor network to represent the variable space using a low-rank approximation to the latent space. The tensor components of the network have translation invariance to the time window. The authors also propose a skip-connection layer for the dissemination of information in terms of scale. The proposed method is evaluated on multivariate Time series forecasting benchmark datasets. 
2789,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"Neighbor sampling USED-FOR Graph Neural Networks ( GNNs ). large graphs USED-FOR Graph Neural Networks ( GNNs ). Stochastic Compositional Optimization ( SCO ) problems USED-FOR samplingbased GNN training. SCO algorithms USED-FOR samplingbased GNN training. SCO algorithms USED-FOR GNNs. moving averages USED-FOR aggregated features. moving averages USED-FOR they. large graphs USED-FOR GNNs. GPU memory limit CONJUNCTION CPU memory limit. CPU memory limit CONJUNCTION GPU memory limit. GPU memory limit FEATURE-OF moving averages. SCO algorithms USED-FOR GNN training. sparse moving averages USED-FOR SCO algorithms. moving averages USED-FOR algorithm. fixed size buffer USED-FOR algorithm. convergence rate EVALUATE-FOR SCO algorithm. algorithm USED-FOR SCO algorithm. convergence rate EVALUATE-FOR algorithm. Adam SGD USED-FOR GNN training. algorithm COMPARE Adam SGD. Adam SGD COMPARE algorithm. algorithm USED-FOR GNN training. memory overhead EVALUATE-FOR algorithm. OtherScientificTerm are graph, graph size, and buffer size. ","This paper studies the problem of neighborhood sampling in Graph Neural Networks (GNNs). The authors consider the Stochastic Compositional Optimization (SCO) problems for samplingbased GNN training. The authors show that SCO algorithms for GNNs can be used as sparse moving averages for aggregated features, and that they can be applied to large graphs. They also show that the moving averages have a GPU memory limit and a CPU memory limit. Finally, the authors propose an algorithm based on the fixed size buffer. The algorithm is shown to have a better convergence rate than the Adam SGD in the setting where the graph size is larger than the buffer size. ","This paper studies the problem of neighborhood sampling for Graph Neural Networks (GNNs) with large graphs. The authors propose Stochastic Compositional Optimization (SCO) problems for samplingbased GNN training. SCO algorithms are used for sampling based on sparse moving averages for GNNs. The main idea is to use moving averages to capture aggregated features in the graph, and they have a GPU memory limit and a CPU memory limit. The proposed algorithm is based on a fixed size buffer, and the convergence rate of the proposed SCO algorithm is better than Adam SGD in terms of memory overhead. "
2805,SP:72e0cac289dce803582053614ec9ee93e783c838,"random hashes USED-FOR Jaccard ( resemblance ) similarity. Minwise hashing ( MinHash ) USED-FOR random hashes. massive binary ( 0/1 ) data USED-FOR Jaccard ( resemblance ) similarity. large - scale learning models CONJUNCTION approximate near neighbor search. approximate near neighbor search CONJUNCTION large - scale learning models. massive data USED-FOR approximate near neighbor search. independent random permutations USED-FOR MinHash. that COMPARE classical MinHash. classical MinHash COMPARE that. Jaccard estimation variance EVALUATE-FOR classical MinHash. circulant manner FEATURE-OF independent random permutations. permutations USED-FOR it. estimation accuracy EVALUATE-FOR it. Method are Circulant MinHash ( C - MinHash ), and C - MinHash variant. Generic is method. ","This paper proposes a new method to approximate the Jaccard (similarity) similarity between random hashes with Minwise hashing (MinHash) using massive binary (0/1) data. The proposed method is based on Circulant MinHash (C-MinHash), which is a variant of the classical MinHash with independent random permutations in a circulant manner. The authors show that it achieves better estimation accuracy than the classical minHash and approximate near neighbor search in large-scale learning models. ","This paper proposes a novel method for learning random hashes for Jaccard (similarity) similarity on massive binary (0/1) data. Minwise hashing (MinHash) is a popular method for random hashes. The authors propose a Circulant MinHash (C-MinHash), which is a variant of the classical MinHash. The main idea is to learn independent random permutations for MinHash in a circulant manner. The proposed method is evaluated on large-scale learning models and approximate near neighbor search on massive data. The results show that the proposed method achieves better estimation accuracy compared to classical minHash, and the authors also show that it is more robust to permutations that differ from the original one."
2821,SP:d254b38331b6b6f30de398bae09380cd5c951698,Adversarial training ( AT ) USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR single lpthreat models. lp - threat models USED-FOR adversarial robustness. training scheme USED-FOR adversarial robustness. training scheme USED-FOR union of lp - threat models. adversarial training USED-FOR lp - threat model. E - AT scheme USED-FOR lp - robust model. multiple - norm robustness EVALUATE-FOR state - of - the - art. multiple norm robustness FEATURE-OF ImageNet models. CIFAR-10 EVALUATE-FOR multiple - norm robustness. CIFAR-10 EVALUATE-FOR state - of - the - art. adversarial robustness FEATURE-OF threat models. CIFAR-10 EVALUATE-FOR SOTA l1 - robustness. Task is safety - critical systems. OtherScientificTerm is lp - balls. Metric is multiple norm adversarial robustness. ,This paper studies adversarial training (AT) for adversarial robustness in single lpthreat models. The authors propose a training scheme for the union of lp-threat models trained with adversarial trained with Lp-robust models. They show that the E-AT scheme improves the robustness of the lp - robust model by a factor of at least 1. The paper also shows that the multiple norm robustness achieved by ImageNet models with multiple-norm robustness improves the state-of-the-art on CIFAR-10 for SOTA l1-robastness. ,"This paper proposes adversarial training (AT) to improve adversarial robustness of single lpthreat models. The proposed training scheme is based on the union of lp-threat models, and the proposed E-AT scheme is used to train the lp - robust model. Experiments on CIFAR-10 and ImageNet models show that multiple-norm robustness improves the state-of-the-art of threat models, especially in safety-critical systems. "
2837,SP:4c2928f6772664d63c02c29f913b476e1c932983,MTL models COMPARE single - task counterpart. single - task counterpart COMPARE MTL models. private encoders CONJUNCTION gates. gates CONJUNCTION private encoders. gates CONJUNCTION private decoders. private decoders CONJUNCTION gates. public encoder CONJUNCTION private encoders. private encoders CONJUNCTION public encoder. private encoder CONJUNCTION gate. gate CONJUNCTION private encoder. gate CONJUNCTION private decoder. private decoder CONJUNCTION gate. public encoder CONJUNCTION private encoder. private encoder CONJUNCTION public encoder. storage cost FEATURE-OF inference stage. SMTL USED-FOR gate. SMTL USED-FOR gates. benchmark datasets EVALUATE-FOR methods. Method is Multi - Task Learning ( MTL ). Generic is problem. OtherScientificTerm is negative sharing. Task is safe multi - task learning. ,"This paper studies the problem of safe multi-task learning. The authors propose a new problem called Multi-Task Learning (MTL) where the goal is to learn a single-task counterpart that can be shared across multiple tasks without negative sharing. The main idea is to use MTL models to train multiple gates, private encoders, and private decoders. The inference stage has a storage cost of $O(\sqrt{T})$ and the gate is trained using SMTL. The proposed methods are evaluated on three benchmark datasets.","This paper proposes a new problem called Multi-Task Learning (MTL) where the goal is to improve the performance of multi-task learning (MTL models compared to their single-task counterpart. The problem is formulated as negative sharing between private encoders, gates, private decoders and the public encoder. The authors propose two methods to solve the problem, one based on SMTL and the other based on gates. The main contribution of the paper is to propose a new inference stage where the storage cost of the gate and the private decoder is reduced. The paper also proposes two new benchmark datasets to evaluate the proposed methods. "
2853,SP:c4cee0d44198559c417750ec4729d26b41061929,"energy - based sequence models USED-FOR partition functions. expressive parametric families USED-FOR energy - based sequence models. model selection CONJUNCTION learning model parameters. learning model parameters CONJUNCTION model selection. partition functions FEATURE-OF sequence model families. asymptotic guarantees FEATURE-OF statistical procedures. OtherScientificTerm are model parameters, partition function, rational number, reduced expressiveness, and computability concerns. Generic is they. Task is sequence modeling. Method is model parametrizations. ","This paper studies the problem of partition functions for energy-based sequence models with expressive parametric families. The authors show that the partition functions of sequence model families with partition functions have asymptotic guarantees for statistical procedures. They also show that they are computationally efficient, and that they can be used in sequence modeling.  The authors also provide a theoretical analysis of the model parametrizations of these partition functions. They show that when the partition function is a rational number, the model parameters of the sequence model have reduced expressiveness. They then provide theoretical guarantees for model selection and learning model parameters. ","This paper proposes a new family of expressive parametric families for energy-based sequence models for partition functions. The authors show that the partition functions of sequence model families can be expressed as partition functions with respect to model parameters and learning model parameters. They also show that they have asymptotic guarantees for statistical procedures with respect the partition function, and that they are computability concerns.  The authors also provide a theoretical analysis of the model parametrizations. "
2869,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"Wasserstein distance USED-FOR large - scale machine learning problems. random projection USED-FOR sliced Wasserstein distance. computational efficiency EVALUATE-FOR sliced Wasserstein distance. augmented sliced Wasserstein distances ( ASWDs ) HYPONYM-OF distance metrics. neural networks USED-FOR higher - dimensional hypersurfaces. they USED-FOR complex structures of the data distribution. gradient ascent USED-FOR hypersurfaces. ASWD COMPARE Wasserstein variants. Wasserstein variants COMPARE ASWD. Wasserstein variants USED-FOR synthetic and real - world problems. synthetic and real - world problems EVALUATE-FOR ASWD. Metric is computational cost. OtherScientificTerm are projections, ( random ) linear projections, and nonlinear projections. Method is injective neural network architecture. ","This paper studies the Wasserstein distance for large-scale machine learning problems. The main idea is to use a random projection to approximate the sliced Wassersteins distance between two points in a hypersurface. The proposed distance metrics are augmented sliced WASSWasserstein distances (ASWDs), which are based on (random) linear projections. The authors show that ASWD can achieve better computational efficiency than WASSSTEIN variants in both synthetic and real-world problems. They also show that they can capture complex structures of the data distribution.   The authors also provide a theoretical analysis of the computational cost of ASWD. ","This paper proposes a new Wasserstein distance for large-scale machine learning problems. The proposed distance metrics, augmented sliced Wassersteins (ASWDs), are based on the random projection of the sliced WASSERstein distance. The authors show that ASWD is more computationally efficient than the WASSSTEIN variants, and can be applied to both synthetic and real-world problems. They also show that the computational cost of ASWD does not depend on the number of projections, but on the (random) linear projections. "
2885,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,coordinated exploration and behaviour PART-OF multi - agent systems. framework USED-FOR multi - agent reinforcement learners ( MARL ). reinforcement learning ( RL ) CONJUNCTION switching controls. switching controls CONJUNCTION reinforcement learning ( RL ). switching controls USED-FOR LIGS. LIGS USED-FOR learning process. intrinsic rewards USED-FOR learning process. LIGS USED-FOR intrinsic rewards. reinforcement learning ( RL ) USED-FOR LIGS. LIGS USED-FOR systems of RL agents. sparse rewards USED-FOR systems of RL agents. it USED-FOR joint policies. multi - agent RL algorithms USED-FOR LIGS. Foraging CONJUNCTION StarCraft. StarCraft CONJUNCTION Foraging. LIGS framework USED-FOR Foraging. LIGS framework USED-FOR StarCraft. Method is reinforcement learners ( RL ). ,"This paper proposes a framework for multi-agent reinforcement learners (MARL) that learns to coordinate exploration and behaviour in multi-Agent systems. The LIGS framework is based on reinforcement learning (RL) and switching controls. The learning process uses intrinsic rewards to learn the intrinsic rewards for each agent in the learning process. The authors show that it can be used to learn joint policies between agents, and it can also be used for learning the joint policies of multiple agents. They also show that the LigS framework can be applied to Foraging and StarCraft. ","This paper proposes a framework for multi-agent reinforcement learners (MARL). MARL is an extension of reinforcement learning (RL) with switching controls. In MARL, the agent is encouraged to learn a coordinated exploration and behaviour. The learning process is based on LIGS with intrinsic rewards and sparse rewards. The authors show that LIGs can be applied to systems of RL agents with sparse rewards, and that it can be used to learn joint policies. In addition, the authors show how to use multi- agent RL algorithms to improve the generalization performance of MARL. Foraging and StarCraft are two applications of the proposed LigS framework."
2901,SP:9eadc19f7f712c488cf50d091f372092f6352930,multi - hop QA systems COMPARE DOCHOPPER. DOCHOPPER COMPARE multi - hop QA systems. document information CONJUNCTION q. q CONJUNCTION document information. QA tasks EVALUATE-FOR DOCHOPPER. datasets EVALUATE-FOR DOCHOPPER. inference time EVALUATE-FOR DOCHOPPER. Generic is model. Method is compact neural representation of q. ,"This paper proposes a new model called DOCHOPPER, which is a compact neural representation of q. The authors show that DOCHopPER outperforms existing multi-hop QA systems on a variety of QA tasks. They also show that the model is able to perform well in terms of inference time.  The authors conduct extensive experiments on several datasets to demonstrate the effectiveness of the proposed model.","This paper proposes a new model for multi-hop QA systems, called DOCHOPPER. The model is based on compact neural representation of q and document information. The proposed model is evaluated on a variety of QA tasks and datasets. The results show that the proposed model achieves better inference time and is more robust to adversarial attacks."
2917,SP:4e79b326bbda5d1509e88869dde9886764366d41,"modalities USED-FOR voice search request. voice casting USED-FOR audiovisual productions. it USED-FOR modalities. it USED-FOR voice recommendation system. it USED-FOR voice search request. characteristic extraction USED-FOR voice casting. taxonomy USED-FOR comedian voices. taxonomy USED-FOR annotation protocol. Label Refining HYPONYM-OF semi - supervised learning method. vocal characteristics HYPONYM-OF refined labels. clustering algorithm USED-FOR refined representation extractor. refined labels USED-FOR refined representation extractor. representation extractor USED-FOR method. clustering algorithm USED-FOR refined labels. Label Refining USED-FOR method. subsidiary corpus USED-FOR voice characteristics. OtherScientificTerm are characteristic, and priori knowledge. Material is MassEffect 3 video game. ","This paper proposes a semi-supervised learning method called Label Refining, which uses a taxonomy of modalities for voice search request in audiovisual productions. The taxonomy is used to define the taxonomy for comedian voices, and it is then used to train a voice recommendation system. The proposed method uses a representation extractor based on refined labels such as vocal characteristics and a clustering algorithm to extract the refined labels from the subsidiary corpus. The method is evaluated on the MassEffect 3 video game.","This paper proposes a semi-supervised learning method called Label Refining, which is based on the idea of defining modalities for voice search request. The key idea is to use the taxonomy of comedian voices as an annotation protocol. The authors show that it can be used for voice recommendation system, and it can also be used to define modalities in audiovisual productions. The proposed method uses a clustering algorithm to extract refined labels of vocal characteristics, which are then used as a representation extractor for the refined labels. The method is evaluated on the MassEffect 3 video game, where it is shown that the proposed method is able to extract the voice characteristics from a subsidiary corpus. "
2933,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"federated and split learning HYPONYM-OF Distributed collaborative learning approaches. Vision Transformer ( ViT ) USED-FOR common representation. Vision Transformer ( ViT ) USED-FOR computer vision applications. global attention USED-FOR common representation. distributed learning framework USED-FOR image processing tasks. ViT USED-FOR distributed learning framework. task - agnostic Vision Transformer CONJUNCTION task - specific head / tail. task - specific head / tail CONJUNCTION task - agnostic Vision Transformer. task - specific heads and tails CONJUNCTION task - agnostic Transformer body. task - agnostic Transformer body CONJUNCTION task - specific heads and tails. features PART-OF representation. global attention USED-FOR Transformer body. task - agnostic learning USED-FOR Transformer. task - specific learning USED-FOR heads. alternating training strategy USED-FOR task - specific learning. method USED-FOR task - specific network. multi - task learning EVALUATE-FOR method. Method is neural networks. Generic are they, applications, and translation. OtherScientificTerm is customer - specific head and tail. Material is medical image data. ","This paper studies federated and split learning in Distributed collaborative learning approaches. The authors propose Vision Transformer (ViT), a distributed learning framework for image processing tasks. ViT uses global attention to learn a common representation from a set of features in the representation. The Transformer body is trained with task-agnostic learning and task-specific learning for the heads and tails. The proposed method is applied to multi-task learning and is shown to improve the performance of neural networks. ","This paper presents Federated and split learning, a family of Distributed collaborative learning approaches. The authors propose Vision Transformer (ViT), a distributed learning framework for image processing tasks. ViT is based on the idea that the common representation is learned with global attention, while the task-specific heads and tails are learned with task-agnostic learning. The proposed method is evaluated on multi-task learning on medical image data. The paper shows that the proposed method can be used to train a task -specific network with an alternating training strategy, and that the Transformer body is learned using global attention on the features in the representation, and on the customer-specific head and tail. "
2949,SP:249a72ef4e9cf02221243428174bb749068af6b2,"misspecified reward functions USED-FOR RL agents. misspecified rewards FEATURE-OF RL environments. action space resolution CONJUNCTION observation space noise. observation space noise CONJUNCTION action space resolution. model capacity CONJUNCTION action space resolution. action space resolution CONJUNCTION model capacity. observation space noise CONJUNCTION training time. training time CONJUNCTION observation space noise. agent capabilities USED-FOR reward hacking. training time HYPONYM-OF agent capabilities. model capacity HYPONYM-OF agent capabilities. observation space noise HYPONYM-OF agent capabilities. action space resolution HYPONYM-OF agent capabilities. proxy reward CONJUNCTION true reward. true reward CONJUNCTION proxy reward. capability thresholds HYPONYM-OF phase transitions. anomaly detection task USED-FOR aberrant policies. Task are Reward hacking, and ML systems. OtherScientificTerm is reward misspecifications. Method is baseline detectors. ","This paper studies the problem of reward hacking in RL environments with misspecified reward functions. Reward hacking is a common problem in ML systems. The authors propose a new anomaly detection task to detect aberrant policies. They show that the agent capabilities such as action space resolution, observation space noise, model capacity, and training time are important factors in reward hacking. They also show that proxy reward and true reward are also important factors. ","This paper studies the problem of reward hacking in RL environments with misspecified reward functions for RL agents. Reward hacking is an important problem in ML systems. The authors propose a new anomaly detection task to detect aberrant policies in the context of reward misspecifications. The agent capabilities for reward hacking include: (1) action space resolution, (2) observation space noise, (3) model capacity, (4) training time, (5) true reward, (6) proxy reward, and (7) capability thresholds for phase transitions. Experiments are conducted on a number of baseline detectors."
2965,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,Kullback – Leibler ( KL ) divergence CONJUNCTION arbitary differeitiable f divergence. arbitary differeitiable f divergence CONJUNCTION Kullback – Leibler ( KL ) divergence. f -TVO USED-FOR Thermodynamic Variational Objective ( TVO ). f -TVO USED-FOR dual function of model evidence f∗(p(x ) ). log model evidence PART-OF TVO. dual function of model evidence f∗(p(x ) ) COMPARE log model evidence. log model evidence COMPARE dual function of model evidence f∗(p(x ) ). deformed χ - geometry perspective USED-FOR f -TVO. variational posterior distribution CONJUNCTION true posterior distribution. true posterior distribution CONJUNCTION variational posterior distribution. χ - exponential family exponential USED-FOR f -TVO. χ - path USED-FOR f -TVO. reparameterization trick CONJUNCTION Monte Carlo approximation. Monte Carlo approximation CONJUNCTION reparameterization trick. reparameterization trick PART-OF f -TVO. Monte Carlo approximation PART-OF f -TVO. VAE CONJUNCTION Bayesian neural network. Bayesian neural network CONJUNCTION VAE. f -TVO COMPARE cooresponding baseline f -divergence variational inference. cooresponding baseline f -divergence variational inference COMPARE f -TVO. Bayesian neural network EVALUATE-FOR f -TVO. VAE EVALUATE-FOR f -TVO. Bayesian neural network EVALUATE-FOR cooresponding baseline f -divergence variational inference. OtherScientificTerm is deformed geodesic. ,"This paper proposes a new f-TVO for the Thermodynamic Variational Objective (TVO).    The main contribution of the paper is to propose a dual function of model evidence f∗(p(x)) that is a combination of the Kullback-Leibler (KL) divergence and the arbitary differeitiable f divergence.  The dual function in TVO is a function of the variational posterior distribution and the true posterior distribution. The deformed χ-geometry perspective is used to define f-television from a deformed geodesic.   For the dual function, the authors use a χ exponential family exponential to define the f -TVO. The authors also propose a reparameterization trick and a Monte Carlo approximation for f-TeVo.  Experiments on VAE and a Bayesian neural network show that f-TEVO outperforms the cooresponding baseline f-divergence variational inference in terms of performance. ","The paper proposes a new method for learning the Thermodynamic Variational Objective (TVO) from data. The main idea is to use f-TVO to learn the dual function of model evidence f∗(p(x) and the true posterior distribution f(x), which is a Kullback-Leibler (KL) divergence and an arbitary differeitiable f divergence. The paper shows that TVO can be learned from log model evidence (e.g., VAE) and a Bayesian neural network. The deformed χ-geometry perspective is used to learn f-tevo from a deformed geodesic. The authors also show that f-Tevo can be approximated by a Monte Carlo approximation and a reparameterization trick. "
2981,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"critics ’ initialization USED-FOR ensemble - based actor - critic exploration. approximated UCB CONJUNCTION weighted Bellman backup. weighted Bellman backup CONJUNCTION approximated UCB. strategy COMPARE approximated UCB. approximated UCB COMPARE strategy. weighted Bellman backup COMPARE clipped double Q - Learning. clipped double Q - Learning COMPARE weighted Bellman backup. additive action noise USED-FOR exploration. weighted Bellman backup USED-FOR strategy. actors ’ initialization USED-FOR training. posterior sampling USED-FOR strategy. methods USED-FOR policies. Method are deep reinforcement learning ( RL ), RL toolbox, and ED2. Generic are task, and tools. Material is continuous control setting. OtherScientificTerm is evaluation runs. Task is continuous control tasks. ","This paper proposes a new method for ensemble-based actor-critic exploration based on “critics’ initialization” in deep reinforcement learning (RL), where the goal is to learn policies in a continuous control setting. The proposed strategy is based on a combination of approximated UCB and weighted Bellman backup with additive action noise for exploration. The authors propose a new RL toolbox, called ED2, which can be applied to any task where evaluation runs are available. They show that the proposed method can achieve better performance than clipped double Q-Learning, and also outperform other methods for learning policies.","This paper proposes a method for ensemble-based actor-critic exploration with “critics’ initialization”. The proposed method is based on deep reinforcement learning (RL), where the goal is to learn policies in a continuous control setting. The key idea is to use additive action noise to guide exploration. The authors show that the proposed strategy outperforms approximated UCB and weighted Bellman backup in clipped double Q-Learning, and outperforms ED2 in the RL toolbox. They also show how the proposed methods can be used to train policies. "
2997,SP:21819b54433fa274657d9fe418f66407eee83eeb,"natural language processing CONJUNCTION face recognition. face recognition CONJUNCTION natural language processing. lending CONJUNCTION college admission. college admission CONJUNCTION lending. Supervised learning models USED-FOR domains. college admission CONJUNCTION natural language processing. natural language processing CONJUNCTION college admission. lending HYPONYM-OF domains. face recognition HYPONYM-OF domains. college admission HYPONYM-OF domains. natural language processing HYPONYM-OF domains. fairness notions USED-FOR fairness issues. fair predictor USED-FOR constrained optimization problem. Equalized Loss ( EL ) HYPONYM-OF fairness notion. prediction error / loss USED-FOR fairness notion. algorithms USED-FOR global optimum. algorithms USED-FOR non - convex problem. global optimum USED-FOR non - convex problem. convex programming tools USED-FOR algorithms. ELminimizer algorithm USED-FOR EL fair predictor. non - convex optimization problem USED-FOR EL fair predictor. convex constrained optimizations USED-FOR non - convex optimization problem. algorithm USED-FOR sub - optimal EL fair predictor. algorithm COMPARE ELminimizer. ELminimizer COMPARE algorithm. unconstrained convex programming tools USED-FOR algorithm. unconstrained convex programming tools USED-FOR sub - optimal EL fair predictor. real - world data EVALUATE-FOR algorithms. Generic are models, it, and constraint. OtherScientificTerm are protected social groups, and loss function. Method is learning process. ","This paper studies the problem of non-convex optimization in the presence of protected social groups. Supervised learning models are used in domains such as natural language processing, face recognition, college admission, and lending. The authors consider fairness notions for fairness issues in these domains. The fairness notion is Equalized Loss (EL) which is a fair predictor for a constrained optimization problem with a prediction error/loss. The ELminimizer algorithm is used to train an EL fair predictor in the non-Convex optimizing problem. The algorithms are trained using convex programming tools to find a global optimum for the global optimum. The algorithm is evaluated on real-world data and shows that the proposed algorithm performs better than the ELmaximizer. ",This paper proposes a new algorithm for the non-convex optimization problem of the ELminimizer algorithm. The main idea is to learn a fair predictor for the constrained optimization problem with fairness notions for fairness issues. The fairness notion is Equalized Loss (EL) which is based on the prediction error/loss of the protected social groups. The authors show that the proposed algorithms converge to the global optimum in the case of a given constraint. The proposed algorithms are evaluated on real-world data with convex programming tools. 
3013,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"neural networks USED-FOR cognitive capacity. meaningful learning USED-FOR systematic generalization. compositional skills FEATURE-OF models. semantic connections USED-FOR models. semantic connections USED-FOR compositional skills. semantic links USED-FOR models. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. CNNs CONJUNCTION Transformers. Transformers CONJUNCTION CNNs. SCAN CONJUNCTION real - world datasets. real - world datasets CONJUNCTION SCAN. real - world datasets USED-FOR semantic parsing. semantic linking USED-FOR sequenceto - sequence models. RNNs HYPONYM-OF sequenceto - sequence models. CNNs HYPONYM-OF sequenceto - sequence models. Transformers PART-OF sequenceto - sequence models. prior knowledge CONJUNCTION semantic linking. semantic linking CONJUNCTION prior knowledge. prior knowledge USED-FOR systematic generalization. semantic linking USED-FOR systematic generalization. inductive learning COMPARE deductive learning. deductive learning COMPARE inductive learning. neural networks USED-FOR systematic generalization. learning schemes USED-FOR neural networks. learning schemes USED-FOR systematic generalization. Material is SCAN dataset. Method are meaningful learning principle, and data augmentation techniques. OtherScientificTerm is inductive or deductive manner. Generic is them. ","This paper studies the problem of systematic generalization of neural networks in the context of cognitive capacity. The authors propose a meaningful learning principle, which is an extension of the inductive or deductive manner. They show that the compositional skills of models trained with semantic connections between different semantic connections can be used to improve the performance of models with different semantic links, such as RNNs, CNNs, Transformers, and sequenceto-sequence models such as SCAN and CNNs on real-world datasets for semantic parsing. They also show that these learning schemes can improve the systematic generalisation of the neural networks with prior knowledge and semantic linking. ","This paper proposes a new way to improve the cognitive capacity of neural networks for systematic generalization. The authors propose a meaningful learning principle, which is an extension of the inductive or deductive manner. The models are trained with semantic connections between the compositional skills of the models, and the semantic links are used to train the models. The proposed method is evaluated on the SCAN dataset and on real-world datasets for semantic parsing. The results show that the proposed learning schemes can improve the performance of the neural networks in terms of prior knowledge and semantic linking in the context of sequenceto-sequence models such as RNNs, CNNs, Transformers, etc. "
3029,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"method USED-FOR 3D shape representation. multi - scale wavelet decomposition USED-FOR method. up / down - sampling USED-FOR hierarchies. sub - bands components PART-OF 3D shapes. lifting scheme USED-FOR high or low sub - bands components. Transformers USED-FOR AWT - Net. shape features USED-FOR them. 3D shape classification and segmentation benchmarks EVALUATE-FOR AWT - Net. OtherScientificTerm are decomposition tree, approximation or detail wavelet coefficients, features, and wavelet coefficients. Method are multi - resolution wavelet analysis, and holistic representations. ","This paper proposes a method for 3D shape representation based on multi-scale wavelet decomposition. The main idea is to decompose the 3D shapes into sub-bands components, and then use up/down-sampling to learn hierarchies. The authors propose a lifting scheme to learn the high or low sub-band components, which is then used in AWT-Net. The proposed method is based on a decomposition tree, where each subband is represented by an approximation or detail wavelet coefficients, and the shape features are used to learn them. The paper shows that the proposed multi-resolution wavelet analysis is able to achieve better results than previous work on holistic representations. The experimental results show that AWT - Net can achieve state-of-the-art performance on 3Dshape classification and segmentation benchmarks.",This paper proposes a method for 3D shape representation based on multi-scale wavelet decomposition. The decomposition tree consists of two parts: 1) the original 3D shapes and 2) the sub-bands components of the original shapes. The authors use up/down-sampling to learn hierarchies of sub-band components in the decomposable tree. They also propose a lifting scheme to encourage high or low sub-bands components to be close to each other. They show that the proposed AWT-Net outperforms the state-of-the-art Transformers on both shape classification and segmentation benchmarks. 
3045,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"pretrained language model USED-FOR natural language generation tasks. prefixtuning CONJUNCTION adapters. adapters CONJUNCTION prefixtuning. Lightweight finetuning COMPARE full finetuning. full finetuning COMPARE Lightweight finetuning. adapters HYPONYM-OF Lightweight finetuning. prefixtuning HYPONYM-OF Lightweight finetuning. lightweight finetuning COMPARE full finetuning in - distribution ( ID ). full finetuning in - distribution ( ID ) COMPARE lightweight finetuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR methods. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR ID. full and lightweight finetuning USED-FOR OOD. cocktail finetuning USED-FOR full finetuning. model CONJUNCTION cocktail finetuning. cocktail finetuning CONJUNCTION model. distillation USED-FOR lightweight model. lightweight model USED-FOR full finetuning. distillation USED-FOR full finetuning. distillation USED-FOR OOD behavior. distillation USED-FOR model. distillation USED-FOR ID data. OOD behavior FEATURE-OF model. Method are pretrained model, and lightweight and full finetuning models. Task is multiclass logistic regression setting. ","This paper proposes a new pretrained language model for natural language generation tasks. Lightweight finetuning, such as adapters and prefixtuning, is used to improve the performance of the pretrained model in the multiclass logistic regression setting. The authors show that the proposed methods outperform the full finetuned in-distribution (ID) and full-finetuning (OOD) methods in terms of OOD performance. The main contribution of the paper is the distillation of the lightweight model into a lightweight model that can be combined with the full model to improve OOD behavior. ",This paper proposes a novel pretrained language model for natural language generation tasks. Lightweight finetuning is an extension of the work on adapters and prefixtuning in-distribution (ID) which is used in the multiclass logistic regression setting. The authors show that the proposed methods outperform both full and lightweight Finetuning in terms of both ID and OOD. They also show that distillation improves the OOD behavior of the model and the lightweight model in the context of distillation for ID data. 
3061,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"health CONJUNCTION governance. governance CONJUNCTION health. pointillistically labeled data USED-FOR data - hungry ML algorithms. Data programming USED-FOR probabilistic training labels. domain experts USED-FOR labelling functions. approach USED-FOR iterative and interactive improvement of weakly supervised models. WARM HYPONYM-OF Active Refinement of Weakly Supervised Models. active learning USED-FOR approach. active learning USED-FOR weakly supervised models. probabilistic accuracy EVALUATE-FOR label model. expert labelling functions PART-OF weak supervision model. probabilistic labels USED-FOR downstream classifiers. real - world medical classification datasets EVALUATE-FOR WARM. WARM USED-FOR probabilistic labels. accuracy EVALUATE-FOR probabilistic labels. accuracy EVALUATE-FOR WARM. domain shift CONJUNCTION artificial noise. artificial noise CONJUNCTION domain shift. population characteristics CONJUNCTION noisy initial labelling functions. noisy initial labelling functions CONJUNCTION population characteristics. noisy initial labelling functions FEATURE-OF WARM. population characteristics FEATURE-OF WARM. WARM USED-FOR weakly supervised systems. Method are Supervised machine learning ( ML ), and ML methods. Task are clinical research, and data collection. Generic is framework. OtherScientificTerm are weak supervision, and Gradient updates. ","This paper proposes a new approach for iterative and interactive improvement of weakly supervised models, called WARM, which is a variant of Active Refinement of Weakly Supervised Models. The framework is based on the observation that the performance of weak supervision can be affected by the domain shift, domain shift and artificial noise. The authors propose to use data programming to learn probabilistic training labels from the domain experts, which are then used to train a weak supervision model with expert labelling functions. The proposed approach is evaluated on real-world medical classification datasets and shows that the proposed WARM improves the accuracy of the label model with respect to the accuracy in terms of the number of samples. ","This paper proposes a new approach for iterative and interactive improvement of weakly supervised models. The approach is called WARM, Active Refinement of Weakly Supervised Models. The framework is based on Supervised machine learning (ML), where the goal is to improve the robustness of the weak supervision. The authors propose to use pointillistically labeled data for training data-hungry ML algorithms. The key idea is to use domain experts as the labelling functions for the probabilistic training labels in the data programming. The proposed approach is evaluated on real-world medical classification datasets, where the authors show that the proposed WARM improves the accuracy of the label model with respect to the expected Probabilistic accuracy on the downstream classifiers.   "
3077,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"group annotated training data USED-FOR classification model. empirical risk minimization ( ERM ) objective USED-FOR models. it COMPARE ERM. ERM COMPARE it. Group - DRO COMPARE ERM. ERM COMPARE Group - DRO. ERM USED-FOR minority groups. Group - DRO USED-FOR minority groups. algorithm USED-FOR learning of features. algorithm COMPARE baselines. baselines COMPARE algorithm. ERM CONJUNCTION Group - DRO. Group - DRO CONJUNCTION ERM. minority groups FEATURE-OF benchmarks. Group - DRO HYPONYM-OF baselines. benchmarks EVALUATE-FOR Group - DRO. ERM HYPONYM-OF baselines. benchmarks EVALUATE-FOR baselines. benchmarks EVALUATE-FOR algorithm. algorithm USED-FOR smooth nonconvex functions. descent method USED-FOR algorithm. OtherScientificTerm are distribution shift, and learning of shared / common features. Task is domain generalization. Metric is regularized loss. ","This paper studies the problem of domain generalization with group annotated training data. The authors propose a new empirical risk minimization (ERM) objective for models trained with the same distribution shift. They show that ERM can be applied to minority groups, and that it outperforms ERM and Group-DRO in terms of generalization. They also show that the algorithm can be used for the learning of features for smooth nonconvex functions. ","This paper proposes a new empirical risk minimization (ERM) objective for training models on group annotated training data. The authors show that it is more robust to distribution shift than ERM and that it outperforms ERM for minority groups. They also show that the algorithm can be used for learning of features that are more likely to be shared across groups. The algorithm is evaluated on three benchmarks for smooth nonconvex functions and outperforms other baselines on minority groups, including ERM, Group-DRO, and a regularized loss. "
3093,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"influential features USED-FOR prediction. bivariate methods USED-FOR feature interactions. bivariate methods USED-FOR black - box models. univariate explanation USED-FOR higher - order. feature interactions PART-OF black - box models. univariate explanation USED-FOR explainability. directionality USED-FOR influential features. directional explanations USED-FOR feature interactions. Shapley value explanations USED-FOR bivariate method. IMDB CONJUNCTION Census. Census CONJUNCTION IMDB. CIFAR10 CONJUNCTION IMDB. IMDB CONJUNCTION CIFAR10. method COMPARE state - of - the - art. state - of - the - art COMPARE method. Drug, and gene data EVALUATE-FOR method. Drug, and gene data EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR method. Census EVALUATE-FOR state - of - the - art. Census EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. CIFAR10 EVALUATE-FOR state - of - the - art. Method are machine learning algorithms, and explanation methods. Generic are they, and graph. OtherScientificTerm are directed graph, and features. ","This paper proposes a new bivariate explanation method for black-box models with feature interactions. The proposed method is based on Shapley value explanations. The authors show that the univariate explanation can be used to explain higher-order features in a directed graph. They also show that by using directional explanations for the feature interactions, the proposed method can improve the explainability of the model. The method is evaluated on Drug, and gene data and compared to state-of-the-art on IMDB and Census.","This paper proposes a new way to model the influence of features in machine learning algorithms. The authors propose to use bivariate methods to model feature interactions in black-box models with feature interactions as a directed graph. The key idea is to use Shapley value explanations to explain the feature interactions. The proposed method is evaluated on CIFAR10, IMDB, and Census. The method outperforms state-of-the-art on Drug, and gene data. "
3109,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"observed behaviour USED-FOR models of human decision - making. clinical care HYPONYM-OF real - world policies. framework USED-FOR interpretable policy learning. probabilistic tree policies USED-FOR physician actions. Policy Extraction USED-FOR interpretable policy learning. decision Trees ( POETREE ) USED-FOR Policy Extraction. medical history USED-FOR probabilistic tree policies. representation of patient history USED-FOR decision tree policies. complexity USED-FOR modelling task. Fullydifferentiable tree architectures USED-FOR modelling task. recurrence USED-FOR representation of patient history. patient information USED-FOR decision tree policies. policy learning method COMPARE stateof - the - art. stateof - the - art COMPARE policy learning method. policy learning method USED-FOR decision support systems. it USED-FOR decision support systems. real and synthetic medical datasets EVALUATE-FOR stateof - the - art. real and synthetic medical datasets EVALUATE-FOR policy learning method. Method is policy learning approaches. Generic is they. Task are decision - making process, and optimization. ","This paper proposes a new framework for interpretable policy learning based on probabilistic tree policies for real-world policies (e.g., clinical care). Policy Extraction is based on decision Trees (POETREE) which can be used to learn models of human decision-making based on observed behaviour. The authors show that the complexity of the modelling task can be reduced by using Fullydifferentiable tree architectures. The paper also shows that the proposed policy learning method can achieve better performance than stateof-the-art on both real and synthetic medical datasets.  ","This paper proposes a framework for interpretable policy learning based on observed behaviour in models of human decision-making. The framework is based on decision Trees (POETREE) which is a probabilistic tree policies for real-world policies (e.g., clinical care). The authors show that the complexity of the modelling task can be reduced by using Fullydifferentiable tree architectures. The paper also shows that the representation of patient history can be learned from the recurrence of the decision tree policies. The authors also show that their policy learning method outperforms stateof-the-art on both real and synthetic medical datasets, and it can be applied to decision support systems. "
3125,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"Data augmentation ( DA ) USED-FOR deep learning models. search USED-FOR automated DA methods. image level FEATURE-OF search. joint optimal augmentation policies USED-FOR patches. Patch AutoAugment HYPONYM-OF fine - grained automated DA approach. Stanford Cars CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION Stanford Cars. FGVC - Aircraft CONJUNCTION Pascal VOC 2007. Pascal VOC 2007 CONJUNCTION FGVC - Aircraft. CUB-200 - 2011 CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION CUB-200 - 2011. image classification CONJUNCTION fine - grained image recognition. fine - grained image recognition CONJUNCTION image classification. ImageNet CONJUNCTION CUB-200 - 2011. CUB-200 - 2011 CONJUNCTION ImageNet. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. fine - grained image recognition CONJUNCTION object detection. object detection CONJUNCTION fine - grained image recognition. CUB-200 - 2011 CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION CUB-200 - 2011. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-100 HYPONYM-OF object detection. CIFAR-10 HYPONYM-OF object detection. object detection EVALUATE-FOR method. fine - grained image recognition EVALUATE-FOR method. image classification EVALUATE-FOR method. method COMPARE DA methods. DA methods COMPARE method. computational resources EVALUATE-FOR method. computational resources EVALUATE-FOR DA methods. OtherScientificTerm are DA policies, grid of patches, augmentation policy, semantics, and team reward. Task is exploration of diversity in local regions. Generic are it, and agents. ","This paper proposes Patch AutoAugment, a fine-grained automated DA approach to data augmentation (DA) for deep learning models. The proposed method is based on joint optimal augmentation policies for patches. The authors propose a grid of patches, where each patch is represented by a set of DA policies, and the goal is to find the best one for each of the patches in the grid. The search is done at the image level, where the search is similar to existing automated DA methods.  The authors show that the proposed method performs better than existing DA methods in terms of both image classification and fine-grain image recognition on CIFAR-100, ImageNet, CUB-200-2011, FGVC-Aircraft, Pascal VOC 2007, and Stanford Cars. They also show that their method is able to achieve better computational resources than other DA methods, and can be used for exploration of diversity in local regions.  ","This paper proposes a fine-grained automated DA approach called Patch AutoAugment, which is a variant of Data augmentation (DA) for deep learning models. The main idea is to learn joint optimal augmentation policies for all patches of the network, and then apply it to each of the patches in the network. The authors show that the proposed method outperforms existing automated DA methods in terms of search at the image level. They also show that their method is more computationally efficient than existing DA methods. "
3141,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"adversarial vulnerability FEATURE-OF deep neural networks. deep neural networks PART-OF machine learning. machine learning USED-FOR adversarial vulnerability. causality USED-FOR distribution change. causal reasoning USED-FOR distribution change. adversarial attacks FEATURE-OF distribution change. causal formulations USED-FOR intuition of adversarial attacks. causal formulations USED-FOR robust DNNs. causal graph USED-FOR generation process of adversarial examples. adversarial distribution USED-FOR intuition of adversarial attacks. models USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR adversarial distribution alignment method. causality USED-FOR adversarial vulnerability. OtherScientificTerm are causal perspective, natural and adversarial distribution, and natural and adversarial distributions. Method is causal understanding. Generic is method. ","This paper studies the problem of adversarial vulnerability in deep neural networks in the context of machine learning. The authors consider the problem from a causal perspective, where the distribution change in the training data is determined by causal reasoning. They show that adversarial attacks can cause distribution change due to spurious correlations in the data, and propose a new adversarial distribution alignment method based on spurious correlations. They also show that such spurious correlations can be used to improve the intuition of the adversarial threat in the presence of natural and adversarial distributions. Finally, they show that the proposed method can be applied to robust DNNs.","This paper studies the adversarial vulnerability of deep neural networks in the context of machine learning. The authors propose a causal understanding of the distribution change caused by adversarial attacks, which is based on causal reasoning. They show that adversarial changes are caused by a natural and adversarial distribution, and propose a method to learn a causal graph for the generation process of adversarial examples. They also propose two causal formulations for robust DNNs, one based on the natural and the other based on adversarial distributions. They evaluate the proposed models on a variety of datasets, and show that the authors' method is more robust to adversarial perturbations. They further propose a spurious correlations method for adversarial adversarial vulnerabilities, which can be used in conjunction with other models to identify the origin of the original distribution change."
3157,SP:9f09449a47464efb5458d0732df7664865558e6f,"Continual learning USED-FOR catastrophic forgetting of deep neural networks. network layer FEATURE-OF convolutional filters. filter atoms USED-FOR convolutional filters. filter atom swapping USED-FOR continual learning. filter subspace FEATURE-OF convolutional layer. models USED-FOR forgetting. scheme USED-FOR continual learning. atom swapping framework USED-FOR model ensemble. optimization schemes CONJUNCTION convolutional network structures. convolutional network structures CONJUNCTION optimization schemes. method USED-FOR optimization schemes. method USED-FOR convolutional network structures. benchmark datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. accuracy CONJUNCTION scalability. scalability CONJUNCTION accuracy. benchmark datasets EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR method. accuracy EVALUATE-FOR method. Method is deep neural networks. OtherScientificTerm are low - rank filter subspace, subspace coefficients, and continual learning settings. ","This paper studies the catastrophic forgetting of deep neural networks in continual learning. The authors propose a new scheme for continual learning based on filter atom swapping, which replaces the filter atoms in convolutional filters in the network layer with a low-rank filter subspace. The proposed method is evaluated on several benchmark datasets and shows that the proposed method outperforms state-of-the-art methods in terms of accuracy and scalability. ","This paper studies the catastrophic forgetting of deep neural networks in the context of continual learning. The authors propose a novel scheme for continual learning based on filter atom swapping, where the network layer of convolutional filters consists of filter atoms. The filter subspace of the convolutionally layer is the same as the filter atoms of the original network layer, but the subspace coefficients of the low-rank filters are different. The proposed method is evaluated on two benchmark datasets and compared to state-of-the-art methods in terms of accuracy and scalability. It is shown that the proposed method outperforms other optimization schemes and convolutionsal network structures in the continual learning settings. "
3173,SP:b806dd540708b39c10d3c165ea7d394a02376805,"Stein variational gradient descent ( SVGD ) HYPONYM-OF deterministic inference algorithm. variance collapse FEATURE-OF SVGD. SVGD update COMPARE gradient descent. gradient descent COMPARE SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR gradient descent. proportional asymptotic limit FEATURE-OF variance collapse. SVGD USED-FOR variance collapse. SVGD CONJUNCTION MMD - descent. MMD - descent CONJUNCTION SVGD. equilibrium variance USED-FOR SVGD. equilibrium variance USED-FOR MMD - descent. equilibrium variance USED-FOR learning high - dimensional isotropic Gaussians. Task are variance collapse phenomenon, and variance estimation. Method are deterministic updates, and high - dimensional isotropic Gaussians. OtherScientificTerm is near - orthogonality condition. ","This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD), a deterministic inference algorithm. SVGD has been shown to suffer from variance collapse under a proportional asymptotic limit. The authors show that SVGD update suffers from the same variance collapse as gradient descent under the maximum mean discrepancy (MMD) objective. They also show that the equilibrium variance of SVGD and MMD-descent can be used to improve the performance of learning high-dimensional isotropic Gaussians. ","This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD), a deterministic inference algorithm. The authors show that SVGD can suffer from variance collapse due to the proportional asymptotic limit of variance collapse. They show that the SVGD update is more robust to variance collapse than gradient descent under the maximum mean discrepancy (MMD) objective. They also show that under the near-orthogonality condition, SVGD and MMD-descent converges to equilibrium variance for learning high-dimensional isotropic Gaussians. "
3189,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"Noisy labels ( NL ) CONJUNCTION adversarial examples. adversarial examples CONJUNCTION Noisy labels ( NL ). measure USED-FOR intrinsic geometric property. AT COMPARE NL. NL COMPARE AT. sample selection USED-FOR NL. PGD steps USED-FOR sample selection. AT COMPARE training. training COMPARE AT. NL COMPARE training. training COMPARE NL. AT HYPONYM-OF NL correction. AT USED-FOR NL. smoothing effects FEATURE-OF AT. AT USED-FOR general - purpose robust learning criterion. NL USED-FOR AT. natural accuracy EVALUATE-FOR AT. Generic are models, and they. OtherScientificTerm are projected gradient descent ( PGD ) steps, adversarial example, class boundary, noisy - class boundary, and NL corrections. Metric is robustness. Material is natural data. ","This paper studies the problem of robustness to noisy labels (NL) and adversarial examples. The authors propose a new measure for measuring the intrinsic geometric property of a model, called the projected gradient descent (PGD) steps. The proposed measure is based on the observation that models trained on natural data are more robust to noisy examples than they are to adversarial ones. The paper then proposes a new NL correction called AT, which uses PGD steps to improve sample selection for NL. The main contribution of the paper is to show that AT is a general-purpose robust learning criterion that can be used to evaluate the performance of NL compared to training. Theoretically, the paper shows that AT has better smoothing effects than NL in terms of natural accuracy. Empirically, it is shown that AT performs better than NL when the class boundary is larger than the noisy-class boundary.","The paper proposes a new measure to measure the intrinsic geometric property of Noisy labels (NL) and adversarial examples. This measure is based on projected gradient descent (PGD) steps, where the adversarial example is a class boundary, and the class boundary is a noisy-class boundary. The paper shows that AT is more robust than NL, and that AT outperforms NL in terms of natural accuracy. The authors also show that AT can be used as a general-purpose robust learning criterion. "
3205,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"language processing CONJUNCTION protein folding. protein folding CONJUNCTION language processing. classification CONJUNCTION language processing. language processing CONJUNCTION classification. Neural network models USED-FOR tasks. classification HYPONYM-OF tasks. language processing HYPONYM-OF tasks. protein folding HYPONYM-OF tasks. adversarial inputs FEATURE-OF reliability. small input perturbations HYPONYM-OF adversarial inputs. neural networks USED-FOR critical systems. expected robustness EVALUATE-FOR neural network model. statistical method EVALUATE-FOR neural network model. statistical method USED-FOR expected robustness. random input perturbation USED-FOR misclassification. robustness EVALUATE-FOR models. neural network certification USED-FOR safety - critical applications. risk and robustness assessments USED-FOR risk mitigation. risk mitigation USED-FOR neural network certification. categorial basis USED-FOR risk mitigation. categorial basis USED-FOR risk and robustness assessments. Generic are model, method, and approach. OtherScientificTerm is Adversarial inputs. Method are Robustness Measurement and Assessment ( RoMA ), RoMA, verification methods, and classification network. Metric are model ’s robustness, robustness levels, and categorial robustness. ","This paper proposes Robustness Measurement and Assessment (RoMA), a new method to measure the robustness of a model’s robustness against adversarial inputs. The proposed method is based on a statistical method to estimate the expected robustness for a neural network model trained with neural networks for critical systems such as classification, language processing, and protein folding. Adversarial inputs such as small input perturbations are used to improve the reliability of the model. The method is evaluated on a variety of tasks including classification and language processing. The results show that the proposed method achieves better robustness than existing verification methods. The authors also show that their method can be used for risk mitigation for neural network certification for safety-critical applications. ","This paper proposes Robustness Measurement and Assessment (RoMA), a method to measure the robustness of a model’s robustness to adversarial inputs. The method is based on the idea that adversarial input can be represented as a set of small input perturbations, and the reliability of the model depends on the number of such perturbs. The authors propose two verification methods, one based on categorial robustness and the other based on risk and robustness assessments. The proposed approach is evaluated on three tasks: classification, language processing, and protein folding. The results show that the proposed statistical method improves the expected robustness for a neural network model, and that the risk mitigation for neural network certification for safety-critical applications is also improved. "
3221,SP:6ba17dd4b31a39478abd995df894447675f2f974,"chunking USED-FOR cognitive science. HCM USED-FOR representations. non - i.i.d sequential data USED-FOR HCM. non - i.i.d sequential data USED-FOR representations. learning guarantees USED-FOR HCM. approaches USED-FOR representation learning. cognitive science CONJUNCTION theories of chunking. theories of chunking CONJUNCTION cognitive science. theories of chunking USED-FOR approaches. OtherScientificTerm are proximity, minimal atomic sequential units, sequential dependence, and partial representational structure. Method are hierarchical chunking model ( HCM ), and hierarchy of chunk representation. ","This paper proposes a hierarchical chunking model (HCM) for learning representations from non-i.i.d sequential data. The HCM is based on a hierarchical graph, where each node in the graph is represented as a set of minimal atomic sequential units. Each node is represented by a hierarchy of chunks, and each chunk represents a subset of the nodes in the hierarchy. The goal is to learn representations that are close enough to the cluster of nodes to be useful for downstream tasks. The authors provide learning guarantees for the HCM and show that HCM can learn representations with sequential dependence. They also provide theoretical guarantees for HCM. Finally, they show that the proposed approaches can be used for representation learning in both cognitive science and theories of chunking.",This paper proposes a hierarchical chunking model (HCM) for cognitive science. HCM learns representations from non-i.i.d sequential data. The key idea of HCM is to learn a hierarchy of chunk representation. The hierarchy of chunks is based on the proximity between minimal atomic sequential units. The authors provide learning guarantees for HCM. They show that the partial representational structure of a chunk representation is similar to that of the full representation. They also show that HCM can learn representations with sequential dependence. They provide several approaches for representation learning based on cognitive science and theories of chunking.
3237,SP:625e3908502fd5be949bb915116ed7569ba84298,"gradient flow FEATURE-OF neural reparametrization. graph convolutional network ( GCN ) USED-FOR neural network architecture. GCN USED-FOR aggregation function. gradients of the loss function USED-FOR aggregation function. network synchronization CONJUNCTION persistent homology optimization. persistent homology optimization CONJUNCTION network synchronization. method USED-FOR optimization problems. persistent homology optimization HYPONYM-OF optimization problems. network synchronization HYPONYM-OF optimization problems. OtherScientificTerm are optimization variables, maximum speed up, and Hessian. Method is neural network. Task is optimization. ","This paper proposes a graph convolutional network (GCN) as a neural network architecture for neural reparametrization with gradient flow. The main idea is to use GCN to learn an aggregation function based on the gradients of the loss function of the neural network. The optimization variables are defined as the maximum speed up, maximum speed down, and the Hessian. The proposed method is applied to several optimization problems such as network synchronization and persistent homology optimization.",This paper proposes a graph convolutional network (GCN) for neural network architecture. The GCN is used to compute the aggregation function of the gradients of the loss function. The authors show that the gradient flow of neural reparametrization can be approximated by the gradient of the optimization variables. The maximum speed up is computed by the Hessian. The proposed method is applied to several optimization problems such as network synchronization and persistent homology optimization.
3253,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,Deep convolutional neural networks ( DCNNs ) USED-FOR image data. DCNNs USED-FOR supervised learning of image data. pre - labeled images USED-FOR real - world problems. SVMnet USED-FOR non - parametric image classification. SVMnet HYPONYM-OF method. method COMPARE DCNNs. DCNNs COMPARE method. accuracy EVALUATE-FOR DCNNs. SVMs COMPARE neural networks. neural networks COMPARE SVMs. accuracy EVALUATE-FOR method. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. ResNet-50 COMPARE SVMnet. SVMnet COMPARE ResNet-50. accuracy EVALUATE-FOR SVMnet. ResNet-50 HYPONYM-OF DCNN architectures. Material is labeled “ ground truth ” images. OtherScientificTerm is real - world cases. ,This paper proposes a method called SVMnet for non-parametric image classification based on deep convolutional neural networks (DCNNs) for image data. The proposed method improves the accuracy of DCNNs in supervised learning of image data by using pre-labeled images to solve real-world problems. The authors show that SVMs perform better than neural networks in terms of accuracy than DCNN architectures such as ResNet-50. The paper also shows that the proposed method is able to perform well in real -world cases.,"This paper proposes a new method for learning deep convolutional neural networks (DCNNs) for image data. The proposed method is based on SVMnet, which is used for non-parametric image classification with pre-labeled images for real-world problems. The authors show that the proposed method improves the accuracy of DCNNs compared to SVMs and other neural networks. The method is compared to other DCNN architectures (ResNet-50, ResNet-100, etc) and shows that it outperforms DCNN. "
3269,SP:a18f4697f350a864866dac871f581b8fc67e8088,"large graphs USED-FOR GNNs. distributed algorithm USED-FOR GNN training. centralized storage CONJUNCTION model learning. model learning CONJUNCTION centralized storage. excessive communication costs CONJUNCTION large memory overheads. large memory overheads CONJUNCTION excessive communication costs. excessive communication costs FEATURE-OF distributed GNN training methods. Learn Locally, Correct Globally ( LLCG ) HYPONYM-OF distributed GNN training technique. local machine USED-FOR GNN. local machine PART-OF LLCG. local data USED-FOR GNN. Global Server Corrections USED-FOR locally learned models. Global Server Corrections USED-FOR server. distributed methods USED-FOR GNNs. periodic model averaging USED-FOR distributed methods. global corrections USED-FOR fast convergence rate. global corrections USED-FOR residual error. real - world datasets EVALUATE-FOR LLCG. efficiency EVALUATE-FOR LLCG. Method are Graph Neural Networks ( GNNs ), and locally trained model. OtherScientificTerm are graph, privacy concern, dependency between nodes, node dependency, and irreducible residual error. Metric are scalability, and communication and memory overhead. ","This paper studies the problem of graph neural networks (GNNs) with large graphs. The authors propose a distributed algorithm for GNN training, called Learn Locally, Correct Globally (LLG), which is based on the distributed algorithm in GNNs. The main idea of LLCG is to use a local machine to train a GNN on the local data, and then use Global Server Corrections to train locally learned models on the global data. Theoretically, the authors show that the global corrections improve the fast convergence rate of the residual error of the locally trained model. Empirically, they show that LLCG improves efficiency on real-world datasets. ","This paper proposes a distributed algorithm for GNN training with large graphs. Graph Neural Networks (GNNs) are typically trained with centralized storage and model learning. However, the privacy concern is high due to the dependency between nodes. The authors propose to learn Locally, Correct Globally (LLG) which is a distributed GNN learning technique with a local machine and a global machine. The local data is used to train the GNN. The server uses Global Server Corrections to train locally learned models, and the global corrections are used to improve the fast convergence rate and the irreducible residual error. Experiments on real-world datasets show that LLCG improves the efficiency of GNNs with large communication costs and large memory overheads. In addition, the authors also show that distributed methods with periodic model averaging can improve the scalability and reduce the communication and memory overhead."
3285,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"model USED-FOR Anytime inference. image classification PART-OF anytime visual recognition. unified and end - toend model approach USED-FOR anytime pixel - level recognition. depth and spatial resolution FEATURE-OF features. redesigned exit architecture CONJUNCTION spatial adaptivity. spatial adaptivity CONJUNCTION redesigned exit architecture. full model USED-FOR anytime inference. spatial adaptivity USED-FOR anytime inference. spatial adaptivity USED-FOR full model. redesigned exit architecture USED-FOR full model. accuracy EVALUATE-FOR full model. semantic segmentation EVALUATE-FOR approach. approach USED-FOR anytime inference. Cityscapes semantic segmentation CONJUNCTION MPII human pose estimation. MPII human pose estimation CONJUNCTION Cityscapes semantic segmentation. Cityscapes semantic segmentation EVALUATE-FOR approach. MPII human pose estimation EVALUATE-FOR approach. total FLOPs EVALUATE-FOR models. total FLOPs EVALUATE-FOR approach. accuracy - computation curve EVALUATE-FOR method. deep equilibrium networks CONJUNCTION feature - based stochastic sampling approach. feature - based stochastic sampling approach CONJUNCTION deep equilibrium networks. method COMPARE them. them COMPARE method. accuracy - computation curve EVALUATE-FOR them. method COMPARE deep equilibrium networks. deep equilibrium networks COMPARE method. method COMPARE feature - based stochastic sampling approach. feature - based stochastic sampling approach COMPARE method. OtherScientificTerm are exits, and prior predictions. Metric is total computation. Method is spatially adaptive approach. Task is human pose estimation. ","This paper proposes a new model for Anytime inference for image classification in the context of anytime visual recognition. The authors propose a unified and end-toend model approach for anytime pixel-level recognition, which can be applied to a wide range of tasks. The main idea is to use a full model with a redesigned exit architecture and spatial adaptivity to improve the accuracy of the full model for anytime inference. The proposed approach is evaluated on Cityscapes semantic segmentation and MPII human pose estimation and shows that the proposed method outperforms deep equilibrium networks and feature-based stochastic sampling approach in terms of total FLOPs. ",This paper proposes a novel model for Anytime inference for image classification in the context of anytime visual recognition. The authors propose a unified and end-toend model approach for anytime pixel-level recognition. They propose a spatially adaptive approach to learn the features in depth and spatial resolution. They also propose a redesigned exit architecture and spatial adaptivity to improve the accuracy of the full model for anytime inference. They evaluate their approach on Cityscapes semantic segmentation and MPII human pose estimation. They show that their method outperforms deep equilibrium networks and feature-based stochastic sampling approach in terms of total FLOPs and accuracy-computation curve. 
3301,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,Neural Processes USED-FOR stochastic processes. neural networks USED-FOR stochastic processes. Modeling functional uncertainty PART-OF learning stochastic processes. bootstrap method USED-FOR functional uncertainty. Gaussian assumption FEATURE-OF latent variable. bootstrap method USED-FOR Bootstrapping Neural Processes ( B(A)NP ). B(A)NP USED-FOR bootstrapping. ANP CONJUNCTION BANP. BANP CONJUNCTION ANP. NeuBANP USED-FOR bootstrap distribution of random functions. encoder CONJUNCTION loss function. loss function CONJUNCTION encoder. Bayesian optimization CONJUNCTION contextual multi - armed bandit. contextual multi - armed bandit CONJUNCTION Bayesian optimization. Bayesian optimization EVALUATE-FOR models. sequential decision - making tasks EVALUATE-FOR NP methods. NeuBANP COMPARE NP methods. NP methods COMPARE NeuBANP. functional uncertainty modeling EVALUATE-FOR NeuBANP. functional uncertainty modeling EVALUATE-FOR method. sequential decision - making tasks EVALUATE-FOR NeuBANP. Generic is approach. Method is Neural Bootstrapping Attentive Neural Processes ( NeuBANP ). ,This paper proposes a new bootstrap method for Bootstrapping Neural Processes (B(A)NP). The bootstrap is based on the idea that neural networks can be used to bootstrap stochastic processes by learning functional uncertainty in the latent variable under a Gaussian assumption. NeuBANP uses the bootstrap distribution of random functions and the encoder and loss function of an ANP and BANP to perform bootstrapping. The proposed approach is evaluated on sequential decision-making tasks with Bayesian optimization and contextual multi-armed bandit. The experimental results show that the proposed method performs better than existing NP methods on functional uncertainty modeling.,"This paper proposes a new approach to learning stochastic processes from neural networks. The authors propose a bootstrap method for bootstrapping Neural Processes (B(A)NP), which is a variant of B(A). The authors show that the bootstrap distribution of random functions can be modeled as a Gaussian assumption on the latent variable, and the authors propose an encoder and a loss function that can be used to model the functional uncertainty of the latent variables. The proposed approach is evaluated on Bayesian optimization and contextual multi-armed bandit, and compared to other NP methods on sequential decision-making tasks. "
3317,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory genome modeling USED-FOR regulatory downstream tasks. regulatory genome modeling PART-OF genome biology research. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. deep learning methods USED-FOR genome sequences. approach USED-FOR pre - training genome data. genome data USED-FOR multi - modal and self - supervised manner. multi - modal and self - supervised manner USED-FOR pre - training genome data. robustness EVALUATE-FOR model. pre - training tasks USED-FOR model. pre - training tasks USED-FOR robustness. genome sequences USED-FOR ATAC - seq dataset. ATAC - seq dataset EVALUATE-FOR model. transaction factor binding sites prediction CONJUNCTION disease risk estimation. disease risk estimation CONJUNCTION transaction factor binding sites prediction. disease risk estimation CONJUNCTION splicing sites prediction. splicing sites prediction CONJUNCTION disease risk estimation. promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory downstream tasks EVALUATE-FOR GeneBERT. splicing sites prediction HYPONYM-OF regulatory downstream tasks. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. disease risk estimation HYPONYM-OF regulatory downstream tasks. OtherScientificTerm is regulatory elements. Generic is them. Task is biological applications. Material are 1d sequence of genome data, and large - scale regulatory genomics data. ","This paper proposes a novel approach to pre-training genome data using pre-trained deep learning methods on a 1d sequence of genome data. The authors propose a multi-modal and self-supervised manner to train a model on the pre-train genome data, which is then used to train regulatory genome modeling in regulatory downstream tasks such as promoter classification, transaction factor binding sites prediction, and disease risk estimation. The proposed model is evaluated on the ATAC-seq dataset, where it achieves state-of-the-art robustness on a variety of biological applications. ","This paper proposes a novel approach to pre-training genome data using deep learning methods for genome sequences. The authors propose a multi-modal and self-supervised manner to train a model on genome data. The model is evaluated on the ATAC-seq dataset, which is a 1d sequence of genome data, and on a large-scale regulatory genomics data, where it is shown that the model can achieve better robustness compared to other models on a range of regulatory downstream tasks (promoter classification, transaction factor binding sites prediction, and disease risk estimation). "
3333,SP:841b12443d0274e34b78940f220b17d36798899b,"method USED-FOR detecting OOD samples. IGEOOD USED-FOR detecting OOD samples. IGEOOD HYPONYM-OF method. IGEOOD USED-FOR pre - trained neural network. geodesic ( FisherRao ) distance USED-FOR discriminator. confidence scores CONJUNCTION features. features CONJUNCTION confidence scores. features PART-OF deep neural network. confidence scores USED-FOR discriminator. logits outputs USED-FOR confidence scores. features USED-FOR discriminator. deep neural network USED-FOR discriminator. IGEOOD COMPARE state - of - the - art methods. state - of - the - art methods COMPARE IGEOOD. Method are machine learning ( ML ) systems, and ML model. OtherScientificTerm are OOD samples, and data distributions. Material is OOD data. ","This paper proposes a method for detecting OOD samples from machine learning (ML) systems. The method, IGEOOD, is based on the pre-trained neural network. The discriminator is trained with a geodesic (FisherRao) distance between the data distributions and the ML model. The confidence scores and features of the discriminator are derived from logits outputs of the deep neural network, and the features are used to train a discriminator. The authors show that IGOEOD outperforms state-of-the-art methods on OOD data.","This paper proposes a method for detecting OOD samples from machine learning (ML) systems. The method is called IGEOOD, which is a pre-trained neural network that is trained on OOD data. The discriminator is based on the geodesic (FisherRao) distance between the data distributions and the confidence scores of a deep neural network. The confidence scores and features of the discriminator are computed from the logits outputs of the ML model. The authors show that the proposed method outperforms state-of-the-art methods."
3349,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"translations CONJUNCTION rotations. rotations CONJUNCTION translations. group FEATURE-OF identity - preserving transformations. identity - preserving transformations USED-FOR representations of objects. translations HYPONYM-OF group. translations HYPONYM-OF identity - preserving transformations. group equivariance USED-FOR representation. Cover ’s Function Counting Theorem USED-FOR linearly separable and group - invariant binary dichotomies. linearly separable and group - invariant binary dichotomies USED-FOR equivariant representations of objects. element - wise nonlinearities CONJUNCTION global and local pooling. global and local pooling CONJUNCTION element - wise nonlinearities. convolutions CONJUNCTION element - wise nonlinearities. element - wise nonlinearities CONJUNCTION convolutions. relation USED-FOR operations. global and local pooling HYPONYM-OF operations. convolutions HYPONYM-OF operations. element - wise nonlinearities HYPONYM-OF operations. OtherScientificTerm are Equivariance, separable dichotomies, group action, and local pooling. Generic is theory. ","This paper studies the group equivariance of identity-preserving transformations (i.e., translations, rotations, etc.) for representations of objects in a group such as translations, translations, and rotations. The authors derive Cover’s Function Counting Theorem for linearly separable and group-invariant binary dichotomies for equivariant representations of the objects in the group. Equivariance is defined as the difference between the group action and group action in the representation of the group, and the representation is defined by the groupequivariance. The theory is well-motivated and well-written. The relation between these operations (e.g., convolutions, element-wise nonlinearities, global and local pooling) are well-understood. ","This paper proposes a new theory of group equivariance for the representation of objects in a group. Equivariance is defined as the sum of the group of translations, rotations, and other identity-preserving transformations. The authors prove the Cover’s Function Counting Theorem for linearly separable and group-invariant binary dichotomies for equivariant representations of objects. The paper also provides a relation between these operations and other operations such as global and local pooling, element-wise nonlinearities, convolutions, etc. "
3365,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"models USED-FOR machine learning objectives. concept drift CONJUNCTION mitigating biases. mitigating biases CONJUNCTION concept drift. robustness HYPONYM-OF machine learning objectives. mitigating biases HYPONYM-OF machine learning objectives. concept drift HYPONYM-OF machine learning objectives. counterfactual explanations CONJUNCTION concept activation vectors. concept activation vectors CONJUNCTION counterfactual explanations. it USED-FOR models. pretrained models USED-FOR approach. prior ideas USED-FOR CCE. counterfactual explanations HYPONYM-OF prior ideas. concept activation vectors HYPONYM-OF prior ideas. CCE USED-FOR spurious correlation. spurious correlations USED-FOR models. CCE USED-FOR models. data USED-FOR models. medical applications EVALUATE-FOR CCE. Generic are model, and systematic approach. Method are conceptual counterfactual explanations ( CCE ), and classifier. OtherScientificTerm are human - understandable concepts, faint stripes, and model mistakes. ","This paper proposes a systematic approach to learn conceptual counterfactual explanations (CCEs) for machine learning objectives such as robustness, concept drift, and mitigating biases. The approach is based on pretrained models trained with prior ideas such as counterfactually explanations and concept activation vectors. The authors show that CCE is able to reduce spurious correlation between the models and spurious correlations between the data and the model, and that it can improve the performance of models trained on the data. The paper also shows that the proposed CCE can be applied to medical applications.","This paper proposes a systematic approach to learn conceptual counterfactual explanations (CCE) for machine learning objectives such as robustness and mitigating biases. The approach is based on pretrained models trained on a set of human-interpretable concepts. The authors show that CCE is more robust to spurious correlation than prior ideas (counterfactual explanation and concept activation vectors) and that it can be used to train models with spurious correlations. The proposed approach is evaluated on two medical applications, where the model is trained on data with faint stripes, and the classifier is trained using CCE. "
3381,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"3D point cloud applications EVALUATE-FOR Kernel Point Convolution ( KPConv ). KPConv network USED-FOR mobile scenarios. KPConv USED-FOR neighbor - kernel correlation. Euclidean distance USED-FOR neighbor - kernel correlation. module USED-FOR KPConv. efficiency EVALUATE-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) USED-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) HYPONYM-OF module. efficiency EVALUATE-FOR module. depthwise kernel USED-FOR resource consumption. depthwise kernel USED-FOR MAKPConv. Inverted Residual Bottleneck ( IRB ) USED-FOR design space. MAKPConv USED-FOR 3D networks. Wide & Deep Predictor USED-FOR dense and sparse neural architecture representations. carrying feature engineering USED-FOR neural architecture representations. Wide & Deep Predictor USED-FOR error in performance prediction. searchable features USED-FOR carrying feature engineering. predictor USED-FOR design space. 3D point cloud classification and segmentation benchmarks EVALUATE-FOR NAS - crafted MAKPConv network. NAScrafted model SPVNAS COMPARE NAS - crafted MAKPConv network. NAS - crafted MAKPConv network COMPARE NAScrafted model SPVNAS. Multiply - Accumulates EVALUATE-FOR NAS - crafted MAKPConv network. mIOU EVALUATE-FOR NAS - crafted MAKPConv network. OtherScientificTerm are kernel relationship, and weak representation power. Method is Neighbor - Kernel attention. Metric is representation power. ","This paper proposes a new module for Kernel Point Convolution (KPConv) for 3D point cloud applications. The proposed module, Mobile Attention Kernel Point Convergence (MAKPconv), is based on Neighbor-Kernel Attention (NKA) and is able to learn the neighbor-kernel correlation using Euclidean distance. The authors show that the proposed module improves the efficiency of KPConv in terms of resource consumption by using a depthwise kernel to reduce the resource consumption. The paper also shows that the prediction of the predictor can be used as a design space in the Inverted Residual Bottleneck (IRB) of 3D networks.  The authors also show that using the Wide & Deep Predictor for dense and sparse neural architecture representations can improve the error in performance prediction by using the searchable features in the network. Finally, the authors demonstrate that the NAS-crafted model SPVNAS can outperform the standard NAS-Crafted MAKConv network on Multiply-Accumulates and mIOU. ","This paper proposes a new module for Kernel Point Convolution (KPConv) for 3D point cloud applications. The proposed module is called Mobile Attention Kernel Point Conv (MAKPconv) and is based on Neighbor-Kernel attention. The main idea of the module is to learn a neighbor-kernel correlation based on the Euclidean distance between two points in the kernel relationship. The paper shows that the proposed module improves the efficiency of KPConv in terms of resource consumption and efficiency of the depthwise kernel for resource consumption. In addition, the paper also proposes an Inverted Residual Bottleneck (IRB) for the design space of the predictor, which is used to reduce the error in performance prediction for dense and sparse neural architecture representations, and also for carrying feature engineering for neural architectures representations with searchable features. Experiments are conducted on NAS-crafted model SPVNAS and a NAS-crafted MAKConv network on mIOU and Multiply-Accumulates. The results show that the NAS-welded version outperforms the original NAS-designed version of the network on both point cloud classification and segmentation benchmarks. "
3397,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. robustness overestimation CONJUNCTION robustness - accuracy trade - off. robustness - accuracy trade - off CONJUNCTION robustness overestimation. problems PART-OF adversarial training. robustness - accuracy trade - off HYPONYM-OF problems. robust overfitting HYPONYM-OF problems. robustness overestimation HYPONYM-OF problems. lowquality samples PART-OF dataset. strategy USED-FOR data quality. learning behaviors USED-FOR strategy. learning behaviors USED-FOR data quality. data quality CONJUNCTION problems. problems CONJUNCTION data quality. problems FEATURE-OF adversarial training. data quality CONJUNCTION adversarial training. adversarial training CONJUNCTION data quality. robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. Material is low - quality data. Metric is adversarial robustness. ,"This paper studies the problem of robust overfitting and robustness overestimation in the presence of low-quality data. The authors propose a new strategy to improve the data quality by using learning behaviors. The proposed problems are a combination of three existing problems: robustness-accuracy trade-off, robustness overfitting, and adversarial training. The experimental results show the effectiveness of the proposed strategy.","This paper studies the problem of robust overfitting and robustness overestimation in adversarial training. The authors propose a new strategy to improve the data quality and the robustness of the adversarial robustness on low-quality data. The proposed strategy is based on learning behaviors to improve data quality. The paper presents a dataset with lowquality samples and a set of problems, including robust-accuracy trade-off, robustness-overfitting, and robusts overestimation. The problems in the paper are well-motivated and well-studied. "
3413,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"neural network USED-FOR multivariate functions of bounded second mixed derivatives. Korobov functions HYPONYM-OF multivariate functions of bounded second mixed derivatives. upper bounds USED-FOR shallow and deep neural networks. quantities USED-FOR shallow and deep neural networks. upper bounds USED-FOR quantities. bounds FEATURE-OF activation functions. ReLU HYPONYM-OF activation functions. continuous function approximator USED-FOR Korobov functions. neural networks HYPONYM-OF near - optimal function approximators. OtherScientificTerm are training parameters, and curse of dimensionality. ","This paper proposes a new neural network for learning multivariate functions of bounded second mixed derivatives, called Korobov functions. Theoretically, the authors show that such functions can be approximated by a continuous function approximator. The authors also provide upper bounds for these quantities for both shallow and deep neural networks. ","This paper proposes a new neural network for learning multivariate functions of bounded second mixed derivatives, called Korobov functions. The main idea is to use a continuous function approximator to approximate the Korobogov functions, which are defined as a function of the training parameters. The authors provide upper bounds for these quantities for both shallow and deep neural networks. The bounds for the activation functions of ReLU and ReLU-based neural networks are also provided.  The authors also provide a theoretical analysis of the curse of dimensionality. "
3429,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"Populations USED-FOR language. agent population size FEATURE-OF speaker - listener Lewis Game. agent population size USED-FOR emergent language properties. training speed CONJUNCTION network capacity. network capacity CONJUNCTION training speed. speaker - listener asymmetry USED-FOR language structure. network capacity HYPONYM-OF diversity factors. training speed HYPONYM-OF diversity factors. relative difference of factors USED-FOR emergent language properties. Material are sociolinguistic literature, and structured languages. Method is neural agents. OtherScientificTerm are agent community, population heterogeneity, confounding factors, training speed heterogeneities, and simulated communities. ","This paper studies the speaker-listener Lewis Game with agent population size in the context of the emergent language properties of the agent community. The authors show that the diversity factors such as training speed, network capacity, and the relative difference of factors in the agent population are important for the language structure. They also provide a theoretical analysis of the effect of these diversity factors on the performance of neural agents. ","This paper studies the problem of learning a language with multiple languages. The authors propose a speaker-listener Lewis Game, where the agent community is composed of a speaker and a listener. The agent population size of the speaker is proportional to the agent population of the listener, and the listener's population size corresponds to the number of speakers in the agent's community. The author(s) propose to use the relative difference of factors for learning emergent language properties (e.g., training speed, network capacity, etc.) to measure the diversity factors in the population community. They show that the diversity of the population heterogeneity is a measure of the confounding factors. They also show that there is a correlation between training speed heterogeneities and the diversity in population heterogeneity. "
3445,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"Graph Neural Networks ( GNNs ) USED-FOR node classification task. node features CONJUNCTION graph topology. graph topology CONJUNCTION node features. node features USED-FOR Graph Neural Networks ( GNNs ). heterophilic graphs EVALUATE-FOR models. homophily FEATURE-OF GNNs. polynomial graph filters USED-FOR models. models USED-FOR polynomials. spectrum FEATURE-OF adaptive polynomial filters. model USED-FOR filter. classification accuracy EVALUATE-FOR model. node classification task EVALUATE-FOR polynomials. model COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE model. model COMPARE models. models COMPARE model. models COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE models. OtherScientificTerm are connected nodes, overdetermined system of equations, and eigencomponents. Method are polynomial graph filter models, eigendecomposition of the graph, and latent polynomial filters. Material is anonymized code. ","This paper studies the problem of node classification on heterophilic graphs. The authors propose a new class of polynomial graph filter models, where the eigendecomposition of the graph is a function of the node features and the graph topology, and the connected nodes are represented as an overdetermined system of equations. The proposed models are based on the idea of homophily in GNNs, and are able to learn polynomials that are homophilic in the spectrum of the input data. The model is able to generate a filter that can be used as a proxy for the eigencomponents of a given node, and is also able to be used to train the filter. The paper shows that the proposed model achieves better classification accuracy on the node classification task than existing polynomic filter-based approaches. ","This paper proposes a novel approach to learning graph neural networks (GNNs) for node classification tasks. The authors propose to use polynomial graph filter models to learn the eigendecomposition of the graph. The proposed approach is based on the idea of homophily in GNNs, which is a special case of heterophilic graphs, where the connected nodes have an overdetermined system of equations, and the eigencomponents of each node are encoded in an anonymized code.  The authors show that the proposed models are able to learn polynomials in the spectrum, and that the model is able to find the filter that is most suitable for a given node. The model is evaluated on the node classification task, and shows better classification accuracy compared to other models. "
3461,SP:903545b1b340ec5c13070e0f25f550c444de4124,biomedical structure prediction CONJUNCTION social relationship analysis. social relationship analysis CONJUNCTION biomedical structure prediction. graphs FEATURE-OF Shortest Distance Queries ( SDQs ). Shortest Distance Queries ( SDQs ) HYPONYM-OF network analysis. social relationship analysis HYPONYM-OF network analysis. biomedical structure prediction HYPONYM-OF network analysis. Approximate algorithms of SDQs USED-FOR complex graph applications. reduced complexity FEATURE-OF Approximate algorithms of SDQs. approaches USED-FOR embedding - based distance prediction. accuracy EVALUATE-FOR embedding - based distance prediction. efficiency EVALUATE-FOR embedding - based distance prediction. truncated random walk CONJUNCTION Pointwise Mutual Information ( PMI)-based optimization. Pointwise Mutual Information ( PMI)-based optimization CONJUNCTION truncated random walk. predictor USED-FOR global extraction of nodes ’ mutual shortest distance. Pointwise Mutual Information ( PMI)-based optimization USED-FOR Embedding - based distance prediction. truncated random walk USED-FOR Embedding - based distance prediction. Random walk HYPONYM-OF unstrained node sequence. limited distance exploration FEATURE-OF Random walk. graph domain FEATURE-OF intrinsic metric. distance range FEATURE-OF Betweenness Centrality(BC)-based random walk. intrinsic metric USED-FOR distance range. intrinsic metric EVALUATE-FOR Betweenness Centrality(BC)-based random walk. steering optimization objective USED-FOR global distance matrix. strategy USED-FOR global distance matrix. maximum likelihood optimization COMPARE PMI - based optimization. PMI - based optimization COMPARE maximum likelihood optimization. strategy USED-FOR distance relation. Distance Resampling ( DR ) COMPARE PMI - based optimization. PMI - based optimization COMPARE Distance Resampling ( DR ). maximum likelihood optimization USED-FOR Distance Resampling ( DR ). walk paths USED-FOR Distance Resampling ( DR ). steering optimization objective USED-FOR strategy. steering optimization objective USED-FOR distance relation. method COMPARE methods. methods COMPARE method. realworld graph datasets USED-FOR SDQ problems. SDQ problems EVALUATE-FOR method. SDQ problems EVALUATE-FOR methods. realworld graph datasets EVALUATE-FOR method. realworld graph datasets,"This paper studies the problem of shortest distance Queries (SDQs) on graphs. The authors propose a novel approach for embedding-based distance prediction based on Pointwise Mutual Information (PMI)-based optimization and truncated random walk. The main idea is to learn a global predictor for each node’s mutual shortest distance, and then use the global extraction of nodes’ mutual shortest distances to estimate the distance between the two nodes. The proposed method is evaluated on a variety of graph datasets, and shows that the proposed method outperforms existing approaches in terms of accuracy.","This paper studies the problem of Shortest Distance Queries (SDQs) in graphs. The authors propose two approaches for embedding-based distance prediction, namely truncated random walk and Pointwise Mutual Information (PMI)-based optimization. The proposed approach is based on the notion of mutual shortest distance, which is defined as the distance between two nodes’ mutual shortest distances. The distance range is defined by the intrinsic metric of the distance range in the graph domain. The paper shows that the proposed approach can achieve better performance than PMI-based optimization, Distance Resampling (DR) and maximum likelihood optimization on walk paths. "
3477,SP:13db440061fed785f05bb41d0767225403ecf7a1,"fact - checking CONJUNCTION open dialogue. open dialogue CONJUNCTION fact - checking. question answering CONJUNCTION fact - checking. fact - checking CONJUNCTION question answering. web corpus USED-FOR knowledge - dependent downstream tasks. Large Language Models ( LMs ) USED-FOR world knowledge. open dialogue HYPONYM-OF knowledge - dependent downstream tasks. question answering HYPONYM-OF knowledge - dependent downstream tasks. fact - checking HYPONYM-OF knowledge - dependent downstream tasks. world knowledge PART-OF LMs. Continual Knowledge Learning ( CKL ) HYPONYM-OF continual learning ( CL ) problem. retention of time - invariant world knowledge CONJUNCTION update of outdated knowledge. update of outdated knowledge CONJUNCTION retention of time - invariant world knowledge. update of outdated knowledge CONJUNCTION acquisition of new knowledge. acquisition of new knowledge CONJUNCTION update of outdated knowledge. metric USED-FOR retention of time - invariant world knowledge. metric USED-FOR acquisition of new knowledge. metric USED-FOR update of outdated knowledge. benchmark USED-FOR retention of time - invariant world knowledge. benchmark CONJUNCTION metric. metric CONJUNCTION benchmark. CKL USED-FOR ever - changing LMs1. OtherScientificTerm are real - world scenarios, catastrophic forgetting, invariant knowledge, and knowledge forgetting. Task is maintenance of ever - changing LMs. Generic are baselines, and CL setups. Method is parameter expansion. ","This paper studies the continual learning (CL) problem of Continual Knowledge Learning (CKL), where the goal is to learn a set of knowledge-dependent downstream tasks from a web corpus, such as fact-checking, open dialogue, and question answering. Large Language Models (LMs) are used to model world knowledge in LMs, and the goal of this paper is to maintain the invariant knowledge in these LMs. The authors propose a new benchmark for the retention of time-invariant world knowledge, the update of outdated knowledge, and acquisition of new knowledge. They also propose a metric to measure the performance of CKL in the maintenance of ever-changing LMs1. They show that CKL can achieve better performance than existing baselines in both real-world scenarios and in CL setups. ","This paper proposes a continual learning (CL) problem called Continual Knowledge Learning (CKL) where the goal is to maintain invariant knowledge in the presence of catastrophic forgetting in real-world scenarios. Large Language Models (LMs) are used to store world knowledge in a web corpus for knowledge-dependent downstream tasks such as fact-checking, open dialogue, and question answering. The authors propose two baselines for the maintenance of ever-changing LMs. The first one is based on parameter expansion, and the second one focuses on the CL setups. The main contribution of the paper is a new metric for the retention of time-invariant world knowledge, the update of outdated knowledge, and acquisition of new knowledge. Experiments show that CKL outperforms the baseline on the ever-varying LMs1."
3493,SP:639fd88482330389019fb5be7446a909b99a8609,"approach USED-FOR supervised learning task. Decision trees USED-FOR supervised learning task. Decision trees USED-FOR applications. medical imaging CONJUNCTION computer vision. computer vision CONJUNCTION medical imaging. computer vision HYPONYM-OF applications. medical imaging HYPONYM-OF applications. feature CONJUNCTION threshold. threshold CONJUNCTION feature. exhaustive search algorithm USED-FOR criterion minimization problem. stochastic approach USED-FOR criterion minimization. algorithm COMPARE exhaustive search. exhaustive search COMPARE algorithm. algorithm COMPARE decision tree learning methods. decision tree learning methods COMPARE algorithm. algorithm COMPARE baseline non - stochastic approach. baseline non - stochastic approach COMPARE algorithm. baseline non - stochastic approach HYPONYM-OF decision tree learning methods. algorithm COMPARE baseline algorithm. baseline algorithm COMPARE algorithm. accuracy CONJUNCTION computational cost. computational cost CONJUNCTION accuracy. computational cost EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. computational cost EVALUATE-FOR baseline algorithm. accuracy EVALUATE-FOR baseline algorithm. algorithm USED-FOR Haar tree. MNIST dataset FEATURE-OF Haar tree. tree COMPARE axis - aligned tree. axis - aligned tree COMPARE tree. MNIST COMPARE axis - aligned tree. axis - aligned tree COMPARE MNIST. MNIST EVALUATE-FOR tree. test accuracy EVALUATE-FOR tree. OtherScientificTerm are leaf nodes, stopping criterion, node, features, and criterion. Method is oblique trees. Metric is training times. ","This paper proposes a new approach to learn decision trees for supervised learning task. Decision trees are used in many applications such as medical imaging, computer vision, and medical imaging. The authors propose an exhaustive search algorithm for the criterion minimization problem, which is a stochastic approach to the problem. The main idea is to learn a set of oblique trees, where the leaf nodes are connected by a stopping criterion. The stopping criterion is a function of the number of nodes in the tree, the distance between the node and the stopping criterion, and the feature and the threshold of the feature. The paper shows that the proposed algorithm outperforms the baseline non-stochastic approach and the baseline algorithm in terms of accuracy, computational cost, and test accuracy. The algorithm also outperforms other decision tree learning methods such as the MNIST dataset. ","This paper presents a novel approach to learning decision trees for a supervised learning task. Decision trees are used for applications such as medical imaging, computer vision, and medical imaging. The authors propose an exhaustive search algorithm for the criterion minimization problem, which is a stochastic approach to the problem. The main idea of the algorithm is to learn a set of oblique trees, where each node is represented by a feature and a threshold, and each of the leaf nodes is represented as a stopping criterion. The feature and threshold are computed as a function of the number of nodes in the node, and the stopping criterion is computed as the sum of the features of the node and the threshold. The algorithm is compared to a baseline non-stochastic approach and a baseline algorithm, and compared to other decision tree learning methods. The proposed algorithm outperforms the baseline algorithm in terms of accuracy, computational cost, and test accuracy of the Haar tree on the MNIST dataset. "
3509,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,Learning rate schedulers USED-FOR deep neural networks. SGD USED-FOR problems. optimizing quadratic objectives HYPONYM-OF problems. eigenvalue distribution FEATURE-OF Hessian matrix. Eigencurve HYPONYM-OF learning rate schedules. SGD USED-FOR quadratic objectives. Eigencurve USED-FOR SGD. learning rate schedules USED-FOR SGD. minimax optimal convergence rates EVALUATE-FOR learning rate schedules. Eigencurve COMPARE step decay. step decay COMPARE Eigencurve. step decay USED-FOR image classification tasks. image classification tasks EVALUATE-FOR Eigencurve. CIFAR-10 USED-FOR image classification tasks. learning rate schedulers USED-FOR eigencurve. theory USED-FOR learning rate schedulers. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. schedulers USED-FOR optimal shape. cosine decay USED-FOR optimal shape. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. ,"This paper studies the problem of learning rate schedulers for deep neural networks. The authors consider SGD for solving problems such as optimizing quadratic objectives, where the eigenvalue distribution of the Hessian matrix is a function of the number of iterations of the SGD. They show that learning rate schedules for SGD with Eigencurve can achieve minimax optimal convergence rates. They also show that EigenCurve can be used as a proxy for step decay for image classification tasks such as CIFAR-10. Finally, the authors propose a theory that shows that the optimal shape can be obtained by using the learned scheduler with cosine decay.","This paper proposes a new method for learning rate schedulers for deep neural networks. The main idea is to use SGD for solving problems such as optimizing quadratic objectives, where the eigenvalue distribution of the Hessian matrix is a function of the number of parameters of the SGD. The authors show that the learning rate schedules for SGD are equivalent to the minimax optimal convergence rates of the Eigencurve, which is a generalization of the theory. The paper also shows that the proposed method is equivalent to cosine decay on image classification tasks such as CIFAR-10. "
3525,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"Offline reinforcement learning USED-FOR control policies. total variation distance FEATURE-OF model. imaginary rollout horizon HYPONYM-OF hyperparameters. Bayesian Optimization USED-FOR hyperparameters. Material is online data collection. Method are offline model - based reinforcement learning, dynamics model, probabilistic model, and uncertainty heuristics. OtherScientificTerm are model uncertainty, pessimistic MDP, MDP, pessimistic return, and estimated model uncertainty. Generic are Existing methods, heuristics, and protocols. ","This paper studies the problem of offline model-based reinforcement learning in the context of online data collection. In this setting, the goal is to learn control policies that are robust to model uncertainty. The authors propose a new dynamics model, which is a probabilistic model with a total variation distance between the model and the environment. The hyperparameters, such as the imaginary rollout horizon, are learned by Bayesian Optimization. Existing methods are based on pessimistic MDPs, where the model uncertainty is estimated using the pessimistic return.  The authors show that the uncertainty heuristics are not optimal for offline reinforcement learning, and propose two new protocols. ","This paper proposes an offline model-based reinforcement learning for control policies. The authors propose a dynamics model, which is a probabilistic model with total variation distance. The model uncertainty is modeled as a pessimistic MDP, where the pessimistic return is the difference between the estimated model uncertainty and the true model uncertainty. The hyperparameters of the model are the imaginary rollout horizon and Bayesian Optimization. Existing methods are based on heuristics, and the authors propose two protocols for online data collection. "
3541,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"methods USED-FOR sampling. critic networks USED-FOR policy learning. TD - error HYPONYM-OF features. TD - error USED-FOR sampled experiences. sampled experiences USED-FOR Q - values. auxiliary features USED-FOR TD - error. auxiliary features USED-FOR sampling. learnable features USED-FOR experience replay method. curriculum learning USED-FOR predicting Q - values. MaPER USED-FOR curriculum learning. MaPER COMPARE vanilla PER. vanilla PER COMPARE MaPER. computational overhead COMPARE vanilla PER. vanilla PER COMPARE computational overhead. critic network USED-FOR curriculum learning. critic network USED-FOR predicting Q - values. tasks EVALUATE-FOR MaPER. offpolicy MfRL CONJUNCTION MbRL. MbRL CONJUNCTION offpolicy MfRL. MaPER USED-FOR MbRL. off - policy MfRL algorithms USED-FOR policy optimization procedure. MaPER USED-FOR offpolicy MfRL. off - policy MfRL algorithms PART-OF MbRL. Method are Experience replay, and model - based RL ( MbRL ). ","This paper proposes a new method for sampling from critic networks for policy learning. Experience replay is an important problem in the context of model-based RL (MbRL). The authors propose to use TD-error as auxiliary features to improve the sampling of sampled experiences. The main idea is to use the learnable features in the experience replay method, which is a combination of TD-erroneous features (e.g. TD- error) and auxiliary features that can be used for sampling. The proposed method, MaPER, is evaluated on two tasks: offpolicy MfRL and MbRL. MaPER outperforms vanilla PER in terms of computational overhead and curriculum learning. The authors also show that MaPER can be applied to offpolicy and off-policy mfRL algorithms in the policy optimization procedure. ","The paper proposes two methods for sampling from experience replay. The first method, Experience replay, is based on TD-error, which is a combination of two features: TD- error and auxiliary features. The second method, MaPER, uses critic networks for policy learning and curriculum learning. Experiments show that MaPER outperforms vanilla PER in terms of computational overhead on a number of tasks, and outperforms off-policy MfRL and MbRL algorithms in the context of model-based RL (MbRL). "
3557,SP:0db83e057c21ac10fe91624876498d8456797492,"fast learning CONJUNCTION training safety. training safety CONJUNCTION fast learning. human knowledge PART-OF reinforcement learning. Human intervention USED-FOR human knowledge. trial - and - error exploration CONJUNCTION human ’s partial demonstration. human ’s partial demonstration CONJUNCTION trial - and - error exploration. human ’s partial demonstration USED-FOR HACO. agent USED-FOR proxy values. HACO USED-FOR agent. HACO USED-FOR proxy state - action values. environmental reward USED-FOR HACO. safe driving benchmark EVALUATE-FOR sample efficiency. sample efficiency EVALUATE-FOR HACO. safe driving benchmark EVALUATE-FOR HACO. reinforcement learning CONJUNCTION imitation learning baselines. imitation learning baselines CONJUNCTION reinforcement learning. Method are learning agent, and Human - AI Copilot Optimization ( HACO ). OtherScientificTerm are human expert, trivial behaviors, human interventions, and human intervention budget. Generic is It. ","This paper proposes a new learning agent, called Human-AI Copilot Optimization (HACO), which combines human knowledge in reinforcement learning with training safety. Human intervention is used to learn human knowledge about the human expert. The agent learns proxy values from the human’s partial demonstration and trial-and-error exploration, and uses HACO to learn proxy state-action values for the agent. It also learns the environmental reward to encourage the agent to avoid trivial behaviors. Experiments on a safe driving benchmark demonstrate the sample efficiency and sample efficiency of HACOO compared to other reinforcement learning and imitation learning baselines. ","This paper proposes a new learning agent, called Human-AI Copilot Optimization (HACO), which combines human knowledge with reinforcement learning and training safety to improve the sample efficiency. Human intervention is used in reinforcement learning to encourage the human expert to avoid trivial behaviors. The agent learns proxy values from the human’s partial demonstration and trial-and-error exploration. HACO uses the agent to learn proxy state-action values from an environmental reward. It is evaluated on a safe driving benchmark and on a sample efficiency benchmark. Experiments are conducted on reinforcement learning, imitation learning, and other imitation learning baselines. "
3573,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"long - horizon unsegmented demonstrations USED-FOR subskills. hierarchical structure USED-FOR Transferring and reorganizing modular sub - skills. Dual Meta Imitation Learning ( DMIL ) HYPONYM-OF hierarchical meta imitation learning method. highlevel network USED-FOR sub - skill adaptation. likelihood of state - action pairs USED-FOR supervision. likelihood of state - action pairs USED-FOR sub - skill. high - level network adaptation USED-FOR DMIL. highlevel network USED-FOR DMIL. likelihood of state - action pairs USED-FOR DMIL. DMIL CONJUNCTION Expectation - Maximization algorithm. Expectation - Maximization algorithm CONJUNCTION DMIL. iterative training process USED-FOR DMIL. Kitchen environment EVALUATE-FOR few - shot imitation learning. Method are Hierarchical Imitation learning ( HIL ), and model - agnostic meta - learning. OtherScientificTerm is high - level network. ","This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method based on a hierarchical structure for Transferring and reorganizing modular sub-skills from long-horizon unsegmented demonstrations to subskills. DMIL uses a high-level network for sub-skill adaptation and a likelihood of state-action pairs for supervision. The authors show that DMIL is able to achieve state-of-the-art performance on the Kitchen environment in few-shot imitation learning with an iterative training process. ","This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method based on hierarchical structure for Transferring and reorganizing modular sub-skills from long-horizon unsegmented demonstrations. DMIL uses a high-level network for sub-skill adaptation and a likelihood of state-action pairs for supervision. The authors also propose Hierarchical Imitation learning (HIL), which is a model-agnostic meta-learning. The experiments show that DMIL outperforms DMIL and the Expectation-Maximization algorithm in the Kitchen environment. The iterative training process for DMIL is similar to few-shot imitation learning."
3589,SP:fb0efa670729796471a7a562b231172103bb8749,Graph neural networks ( GNNs ) HYPONYM-OF deep learning models. deep learning models USED-FOR graph data. node features USED-FOR they. graph without node feature USED-FOR networks. number of degrees HYPONYM-OF graph - based node features. embeddings HYPONYM-OF input node representation. approach USED-FOR node embeddings. node embeddings CONJUNCTION GNNs. GNNs CONJUNCTION node embeddings. graphics processing unit ( GPU ) memory FEATURE-OF GNNs. embedding compression methods USED-FOR natural language processing ( NLP ) models. bit vector COMPARE float - point vector. float - point vector COMPARE bit vector. bit vector USED-FOR node. parameters USED-FOR compression method. GNNs USED-FOR parameters. node embedding compression method COMPARE alternatives. alternatives COMPARE node embedding compression method. Generic is network. Material is industrial scale graph data. ,"Graph neural networks (GNNs) are well-known deep learning models for graph data. However, they are not well-suited for graph without node feature. In this paper, the authors propose a new approach to compress node embeddings in the input node representation. The proposed method is based on graph-based node features such as number of degrees. The authors show that GNNs with a graphics processing unit (GPU) memory are able to compress the parameters of a node using a bit vector instead of a float-point vector. The paper also shows that the proposed node embedding compression method is more efficient than other alternatives for natural language processing (NLP) models. ","Graph neural networks (GNNs) are one of the most popular deep learning models for graph data. However, they are typically trained with node features that are not graph without node feature. This paper proposes a new approach to learn node embeddings that are the number of degrees of the input node representation. GNNs are trained with a graphics processing unit (GPU) memory. The proposed compression method is based on two parameters: the bit vector of the node and the float-point vector. The authors show that the proposed node embedding compression method outperforms other alternatives for natural language processing (NLP) models. "
3605,SP:15c243829ed3b2505ed1e122bd499089f8a862da,domain adaptation USED-FOR learning invariant representations. domain - adversarial training USED-FOR learning invariant representations. asymptotic convergence guarantees FEATURE-OF optimizer. gradient descent USED-FOR domain - adversarial training. optimal solutions PART-OF domain - adversarial training. local Nash equilibria USED-FOR optimal solutions. gradient descent CONJUNCTION high - order ODE solvers. high - order ODE solvers CONJUNCTION gradient descent. Runge – Kutta HYPONYM-OF high - order ODE solvers. optimizers COMPARE optimizers. optimizers COMPARE optimizers. drop - in replacement COMPARE optimizers. optimizers COMPARE drop - in replacement. optimizers USED-FOR drop - in replacement. learning rates FEATURE-OF optimizers. optimizers PART-OF domain - adversarial framework. Generic is approach. Method is domain - adversarial methods. ,"This paper proposes a domain adaptation method for learning invariant representations. The proposed approach is based on gradient descent, where the optimizer is trained with asymptotic convergence guarantees. The authors show that the optimal solutions in domain-adversarial training are obtained by local Nash equilibria. They also show that gradient descent and high-order ODE solvers, such as Runge-Kutta, outperform drop-in replacement in terms of learning rates. ",This paper proposes a domain adaptation method for learning invariant representations. The proposed approach is based on gradient descent and high-order ODE solvers such as Runge-Kutta. The authors show that the optimizer has asymptotic convergence guarantees. They also show that optimal solutions in domain-adversarial training can be obtained from local Nash equilibria. The paper also shows that the proposed optimizers are more robust to drop-in replacement than other optimizers in the domain-defensive framework. 
3621,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"regularization methods USED-FOR machine learning models. loss function USED-FOR Flooding. individual Flood HYPONYM-OF regularizer. iFlood USED-FOR models. instance - level constraints FEATURE-OF training loss. instance - level constraints USED-FOR iFlood. it USED-FOR applications. it USED-FOR models. iFlood COMPARE regularizers. regularizers COMPARE iFlood. image classification and language understanding tasks EVALUATE-FOR models. iFlood USED-FOR models. Generic is one. OtherScientificTerm are under - fitted instances, inductive biases, and instance - level. Metric is generalization ability. ","This paper studies the problem of regularization methods for machine learning models with under-fitted instances. The authors propose a new regularizer called individual Flood, which is a loss function for Flooding. They show that iFlood can be used to train models with instance-level constraints on the training loss. They also show that it can be applied to other applications such as image classification and language understanding tasks. ","This paper proposes a new regularization methods for machine learning models, iFlood, which is an extension of individual Flood, an existing regularizer. The main idea is to use the loss function for Flooding as an additional loss function to improve the generalization ability of the model. The training loss is based on instance-level constraints, where the under-fitted instances are considered as inductive biases. The authors show that it improves the performance of the models on image classification and language understanding tasks, and it can be applied to other applications as well.  "
3637,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"Reinforcement learning USED-FOR complex tasks. Reinforcement learning USED-FOR policies. policies USED-FOR complex tasks. methods USED-FOR long - horizon tasks. Hierarchical reinforcement learning USED-FOR lowlevel skills. Hierarchical reinforcement learning USED-FOR action abstractions. action abstractions USED-FOR lowlevel skills. lower - level policies USED-FOR state abstraction. approach USED-FOR representation. value functions USED-FOR lower - level skill. value functions USED-FOR approach. value functions USED-FOR representation. value functions USED-FOR representation. approach COMPARE model - free and model - based methods. model - free and model - based methods COMPARE approach. maze - solving and robotic manipulation tasks EVALUATE-FOR approach. zero - shot generalization EVALUATE-FOR model - free and model - based methods. zero - shot generalization EVALUATE-FOR approach. OtherScientificTerm are horizon, lower - level skills, Hierarchies, and space states. Method is Value Function Spaces. ","This paper proposes a new method for learning low-level skills using Hierarchical Reinforcement Learning (HRL). The key idea is to learn a set of lower-level policies for each state abstraction, and then use these policies to learn policies to solve complex tasks. The proposed approach is based on value functions, which can be used to learn the representation of the lower- level skill. The paper shows that the proposed approach achieves better zero-shot generalization than model-free and model-based methods on maze-solving and robotic manipulation tasks. ","This paper proposes a new approach to learn lowlevel skills from Hierarchical reinforcement learning for complex tasks. Reinforcement learning is used to learn policies that can be applied to complex tasks, and methods are used for long-horizon tasks. The key idea of the paper is to learn a representation of the lower-level skill using value functions, and then use these value functions to learn the representation for the lower level skill. The paper is well-written and well-motivated. The approach is evaluated on maze-solving and robotic manipulation tasks and shows better zero-shot generalization compared to model-free and model-based methods. "
3653,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"one - shot probabilistic decoders USED-FOR vector - shaped prior. generative adversarial networks ( GAN ) CONJUNCTION normalizing flows. normalizing flows CONJUNCTION generative adversarial networks ( GAN ). variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). functions PART-OF variational autoencoders ( VAE ). functions USED-FOR drug discovery. Transformer layers CONJUNCTION graph neural networks. graph neural networks CONJUNCTION Transformer layers. them CONJUNCTION prior vector. prior vector CONJUNCTION them. Transformer layers USED-FOR prior vector. Transformer layers USED-FOR them. graph neural networks USED-FOR them. architecture USED-FOR exchangeable distributions. VAEs CONJUNCTION GANs. GANs CONJUNCTION VAEs. exchangeability USED-FOR VAEs. exchangeability USED-FOR GANs. Top - n HYPONYM-OF deterministic, non - exchangeable set creation mechanism. VAE CONJUNCTION GAN. GAN CONJUNCTION VAE. Top - n USED-FOR VAE. i.i.d. generation USED-FOR VAE. i.i.d. generation USED-FOR GAN. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. Top - n USED-FOR complex dependencies. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. SetMNIST reconstruction EVALUATE-FOR Top - n. SetMNIST reconstruction EVALUATE-FOR i.i.d. generation. algorithm USED-FOR molecule generation methods. algorithm USED-FOR one - shot generation. Task is Set and graph generation. OtherScientificTerm are normal distribution, and equivariance. Material are synthetic molecule - like dataset, and QM9 dataset. ","This paper proposes a deterministic, non-exchangeable set creation mechanism called Top-n, which uses one-shot probabilistic decoders to generate a vector-shaped prior. Top-N is a combination of variational autoencoders (VAE) and generative adversarial networks (GAN) with normalizing flows. Transformer layers and graph neural networks are used to encode the prior vector into them, which is then used to train them. The authors show that the proposed architecture is able to generate exchangeable distributions with high equivariance. The proposed algorithm is shown to outperform the state-of-the-art molecule generation methods on Set and graph generation. ","This paper proposes a deterministic, non-exchangeable set creation mechanism, Top-n, for one-shot probabilistic decoders for a vector-shaped prior. Top-N is based on variational autoencoders (VAE), generative adversarial networks (GAN) and normalizing flows. Transformer layers and graph neural networks are used to encode them and the prior vector. The authors propose a new architecture for exchangeable distributions, which allows for the normal distribution, and equivariance. The proposed algorithm is evaluated on a synthetic molecule-like dataset, and on a QM9 dataset. Results show that the proposed algorithm outperforms existing molecule generation methods, and outperforms one-shoot generation on Set and graph generation. "
3669,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,Deep Ritz Method ( DRM ) CONJUNCTION Physics - Informed Neural Networks ( PINNs ). Physics - Informed Neural Networks ( PINNs ) CONJUNCTION Deep Ritz Method ( DRM ). deep learning techniques USED-FOR elliptic partial differential equations ( PDEs ). Deep Ritz Method ( DRM ) USED-FOR deep learning techniques. random samples USED-FOR deep learning techniques. Physics - Informed Neural Networks ( PINNs ) USED-FOR deep learning techniques. hypercube FEATURE-OF Schrödinger equation. Schrödinger equation HYPONYM-OF prototype elliptic PDE. upper bounds USED-FOR problem. upper and lower bounds USED-FOR methods. upper bounds USED-FOR upper and lower bounds. rate generalization bound USED-FOR upper bounds. rate generalization bound USED-FOR problem. PINN CONJUNCTION DRM. DRM CONJUNCTION PINN. DRM USED-FOR minimax optimal bounds. PINN USED-FOR minimax optimal bounds. Sobolev spaces FEATURE-OF minimax optimal bounds. dimension dependent power law USED-FOR deep PDE solvers. power law USED-FOR deep model accuracy. OtherScientificTerm is zero Dirichlet boundary condition. Task is quantummechanical systems. Method is Deep Ritz Method. Generic is it. ,"This paper studies the problem of learning elliptic partial differential equations (PDEs) in quantummechanical systems. The problem is formulated as a Schrödinger equation on a hypercube with a zero Dirichlet boundary condition. The authors propose two deep learning techniques, Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs), to solve this problem using random samples. The proposed methods have upper and lower bounds on the problem. The upper bounds are based on the rate generalization bound for the problem, and the upper bounds for the lower bounds depend on the dimension dependent power law of the deep PDE solvers. The theoretical results show that PINN and DRM can achieve minimax optimal bounds in Sobolev spaces. ",This paper proposes a new method for learning quantummechanical systems. The authors propose a novel method called Deep Ritz Method (DRM) to solve elliptic partial differential equations (PDEs) using deep learning techniques such as Physics-Informed Neural Networks (PINNs) with random samples. The proposed methods are based on upper and lower bounds on the problem of solving the Schrödinger equation of the hypercube of a prototype elliptic PDE. The main contribution of the paper is to propose a rate generalization bound for the upper bounds of the problem. The paper also proposes a dimension dependent power law for deep PDE solvers to improve the deep model accuracy. Experiments show that the proposed PINN and DRM outperform the minimax optimal bounds in Sobolev spaces.
3685,SP:80614db60d27a48c3c1b1882844e298666b798d4,Machine learning ( ML ) robustness CONJUNCTION generalization. generalization CONJUNCTION Machine learning ( ML ) robustness. data distribution shift FEATURE-OF they. adversarial and natural settings FEATURE-OF data distribution shift. other USED-FOR one. norm of the last layer CONJUNCTION Jacobian norm. Jacobian norm CONJUNCTION norm of the last layer. Jacobian norm CONJUNCTION data augmentations ( DA ). data augmentations ( DA ) CONJUNCTION Jacobian norm. function class regularization process USED-FOR domain generalization. adversarial training USED-FOR robustness. function class regularization USED-FOR robustness. DA USED-FOR generalization. DA USED-FOR regularization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. Generic is theoretical framework. OtherScientificTerm is sufficient conditions. ,"This paper studies the problem of robustness and generalization in machine learning (ML) robustness in the presence of data distribution shift in adversarial and natural settings. The authors propose a theoretical framework for this problem, which is based on the Jacobian norm of the last layer and data augmentations (DA). The main idea is to use a function class regularization process to improve domain generalization and robustness through adversarial training. The main contribution of the paper is to provide sufficient conditions under which DA can be used for regularization, and to show that DA can improve robustness as well as generalization.","This paper proposes a theoretical framework to study the relationship between Machine learning (ML) robustness and generalization under data distribution shift in adversarial and natural settings. The main idea is to use the Jacobian norm of the last layer and data augmentations (DA) as the two main components of the one. The authors show that adversarial training improves robustness, while DA improves generalization. They also show that the function class regularization process can improve domain generalization in the presence of sufficient conditions."
3701,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"Meta - learning USED-FOR quick learning of few - shot tasks. meta - training tasks USED-FOR meta - knowledge. Wellgeneralized meta - knowledge USED-FOR fast adaptation. methods PART-OF framework. deconfounder approaches USED-FOR methods. Dropout USED-FOR meta - knowledge. deconfounder algorithms USED-FOR memorization. causal perspective USED-FOR memorization. causal perspective USED-FOR deconfounder algorithms. benchmark datasets USED-FOR memorization. benchmark datasets EVALUATE-FOR deconfounder algorithms. Task is task - specific adaptation. Method are regularizer - based and augmentation - based methods, meta - learning, and front - door adjustment. OtherScientificTerm are causality, and universal label space. ","This paper proposes a new framework for meta-learning of few-shot tasks. The framework is based on well-generalized meta-knowledge, which can be used for fast adaptation to meta-training tasks. In particular, the authors propose two methods: (1) regularizer-based and augmentation-based methods, and (2) deconfounder approaches that use deconfounder algorithms to perform memorization from a causal perspective. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed techniques.","This paper proposes a new framework for meta-learning for the quick learning of few-shot tasks. The framework is based on the idea of Wellgeneralized meta-knowledge for fast adaptation to meta-training tasks. In particular, the authors propose two methods for adapting existing regularizer-based and augmentation-based methods to the task-specific adaptation. The main idea is to use deconfounder approaches to improve the performance of these methods. The authors propose to use a causal perspective for the memorization, which is a generalization of the existing deconfounder algorithms. They also propose a novel way to use Dropout for the meta-knowing of the task. They show that the proposed methods outperform the existing methods on several benchmark datasets for memorization. "
3717,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"ad hoc teamwork HYPONYM-OF problem. full observability CONJUNCTION fixed and predefined teammates ’ types. fixed and predefined teammates ’ types CONJUNCTION full observability. fixed and predefined teammates ’ types HYPONYM-OF assumptions. full observability HYPONYM-OF assumptions. reinforcement learning framework USED-FOR autonomous agent. ODITS HYPONYM-OF reinforcement learning framework. information - based regularizer USED-FOR proxy representations of the learned variables. local observations USED-FOR information - based regularizer. local observations USED-FOR proxy representations of the learned variables. ODITS COMPARE baselines. baselines COMPARE ODITS. ad hoc teamwork tasks EVALUATE-FOR ODITS. ad hoc teamwork tasks EVALUATE-FOR baselines. Task is Autonomous agents. OtherScientificTerm are teammates, and partial observability. ","This paper studies the problem of ad hoc teamwork in the context of Autonomous agents. The authors propose a reinforcement learning framework called ODITS, which learns an autonomous agent from a set of observations. The assumptions include full observability, fixed and predefined teammates’ types, and partial observability. The information-based regularizer is used to learn proxy representations of the learned variables from local observations. ODITS is shown to perform better than other baselines on a number of standard AD hoc teamwork tasks.",This paper studies the problem of ad hoc teamwork in the context of Autonomous agents. The authors propose a reinforcement learning framework for training an autonomous agent that combines two assumptions: full observability and fixed and predefined teammates’ types. The main idea is to use an information-based regularizer to learn proxy representations of the learned variables based on local observations. The paper shows that ODITS outperforms other baselines on a number of standard AD hoc teamwork tasks. 
3733,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"missing values PART-OF high - dimensional data. approach USED-FOR down - stream analysis. imputation CONJUNCTION model estimation. model estimation CONJUNCTION imputation. model estimation USED-FOR down - stream analysis. imputation PART-OF approach. model estimation PART-OF approach. algorithm USED-FOR imputation. normalizing flow ( NF ) model USED-FOR data space. latent space FEATURE-OF imputation. normalizing flow ( NF ) model USED-FOR algorithm. Expectation - Maximization ( EM ) algorithm USED-FOR imputation. EMFlow COMPARE methods. methods COMPARE EMFlow. predictive accuracy CONJUNCTION speed of algorithmic convergence. speed of algorithmic convergence CONJUNCTION predictive accuracy. high - dimensional multivariate and image datasets EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR EMFlow. predictive accuracy EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR methods. predictive accuracy EVALUATE-FOR methods. Method are data mining and machine learning methods, EMFlow algorithm, and NF alternatively. ",This paper proposes a new approach for down-stream analysis with missing values in high-dimensional data. The proposed approach combines imputation and model estimation in the imputation part of the approach. Expectation-Maximization (EM) algorithm is used to perform imputation in the latent space and normalizing flow (NF) model is used in the data space for imputation. EmFlow achieves better predictive accuracy and speed of algorithmic convergence than existing methods on both high- dimensional multivariate and image datasets. The authors also show that the EMFlow algorithm can be combined with NF alternatively.,"This paper proposes a new approach for down-stream analysis with missing values in high-dimensional data. The approach is based on imputation and model estimation. The authors propose an Expectation-Maximization (EM) algorithm for imputation in the latent space, and a normalizing flow (NF) model for the data space. Experiments show that EMFlow outperforms other methods in terms of predictive accuracy and speed of algorithmic convergence on high-dimensionality multivariate and image datasets. "
3749,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,rectified linear units ( ReLUs ) USED-FOR DNNs. gates USED-FOR neural path kernel ( NPK ). rotational invariance CONJUNCTION ensemble structure. ensemble structure CONJUNCTION rotational invariance. global pooling CONJUNCTION skip connection. skip connection CONJUNCTION global pooling. convolution USED-FOR NPK. ensemble structure USED-FOR NPK. convolution USED-FOR ensemble structure. convolution USED-FOR rotational invariance. skip connection FEATURE-OF convolution. global pooling FEATURE-OF convolution. gates USED-FOR weights. external masks USED-FOR weights. weights PART-OF network. gates USED-FOR external masks. ReLUs FEATURE-OF DNNs. deep linear network USED-FOR pre - activations. DNNs USED-FOR ‘ black box’-ness. disentanglement CONJUNCTION interpretable re - arrangement of the computations. interpretable re - arrangement of the computations CONJUNCTION disentanglement. interpretable re - arrangement of the computations PART-OF DNN. ReLUs USED-FOR interpretable re - arrangement of the computations. ReLUs USED-FOR DNN. path space FEATURE-OF weights network. DLGN USED-FOR computations. primal ’ linearity CONJUNCTION dual ’ linearity. dual ’ linearity CONJUNCTION primal ’ linearity. path space FEATURE-OF dual ’ linearity. ‘ mathematically ’ interpretable linearities PART-OF DLGN. dual ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. primal ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DGN CONJUNCTION DLGN. DLGN CONJUNCTION DGN. DNN CONJUNCTION DGN. DGN CONJUNCTION DNN. DLGN COMPARE DNNs. DNNs COMPARE DLGN. ‘ disentangled and interpretable ’ computations PART-OF DLGN. entang,"This paper studies the problem of ‘black box’-ness of DNNs with ReLUs. The authors propose to use rectified linear units (ReLUs) as DNN’s in order to improve the interpretability of the computations in a DNN. The main contribution of the paper is the use of gates to learn the neural path kernel (NPK) of a deep linear network for pre-activations. The ensemble structure of the NPK is learned by convolution with skip connection, global pooling, and skip connection. The weights of the network are learned by external masks that are connected to the weights in the network. These gates are then used to train the weights and the weights are used as external masks in the weights network.  The authors show that DLGN is able to learn ‘disentangled and interpretable’ computations by DLGN and DGN. They also show that the dual’ linearity and primal’linearity of DLGN are better than the ‘mathematically’ interpretable linearities in the path space of a weights network and the interpretable re-arrangement of the computation in DLGN. ","This paper proposes to use rectified linear units (ReLUs) in DNNs to improve the ‘black box’-ness. ReLUs can be seen as gates for the neural path kernel (NPK) of a DNN. The weights of the network are generated by a deep linear network, and the weights in the network can be represented by external masks. The authors show that the ensemble structure of the NPK is invariant to rotational invariance, convolution, skip connection, global pooling, and skip connection. In addition, the authors show the dual’ linearity of the weights network in the path space and the dual ’linearity of a weights network with gates. They also show that DLGN outperforms DNN and DGN on a set of ‘disentangled and interpretable’ computations, including disentanglement, interpretable re-ordering of the computations in the DNN, as well as the interpretability re-approximating the weights. The paper also shows that the weights of a DLGN can be decomposed into two parts: (1) the weights with gates, and (2) weights with external masks, where the weights are computed in the paths of the path kernel. "
3765,SP:5676944f4983676b5ad843fdb190bf029ad647bb,Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. Vision Transformer ( ViT ) USED-FOR computer vision tasks. PVT HYPONYM-OF Vision Transformer ( ViT ). Swin HYPONYM-OF Vision Transformer ( ViT ). Layer Normalization ( LN ) USED-FOR models. Transformers USED-FOR inductive bias. LN USED-FOR positional context. positional context HYPONYM-OF inductive bias. Dynamic Token Normalization ( DTN ) HYPONYM-OF normalizer. it USED-FOR normalization methods. unified formulation USED-FOR it. global contextual information CONJUNCTION local positional context. local positional context CONJUNCTION global contextual information. Transformers USED-FOR local positional context. Transformers USED-FOR global contextual information. DTN USED-FOR Transformers. DTN USED-FOR intra - token and inter - token manners. PVT CONJUNCTION LeViT. LeViT CONJUNCTION PVT. BigBird CONJUNCTION Reformer. Reformer CONJUNCTION BigBird. Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. LeViT CONJUNCTION T2T - ViT. T2T - ViT CONJUNCTION LeViT. T2T - ViT CONJUNCTION BigBird. BigBird CONJUNCTION T2T - ViT. ViT CONJUNCTION Swin. Swin CONJUNCTION ViT. DTN USED-FOR vision transformers. ViT HYPONYM-OF vision transformers. LeViT HYPONYM-OF vision transformers. Swin HYPONYM-OF vision transformers. T2T - ViT HYPONYM-OF vision transformers. PVT HYPONYM-OF vision transformers. Reformer HYPONYM-OF vision transformers. BigBird HYPONYM-OF vision transformers. transformer COMPARE baseline model. baseline model COMPARE transformer. DTN USED-FOR transformer. computational overhead EVALUATE-FOR baseline model. DTN COMPARE LN. LN COMPARE DTN. accuracy EVALUATE-FOR Long ListOps. Long - Range Arena FEATURE-OF,"This paper proposes a new normalizer called Dynamic Token Normalization (DTN) to normalize the tokens of a Transformer (ViT) for computer vision tasks. The proposed normalizer is based on a unified formulation, and it can be applied to a variety of normalization methods. The authors show that DTN is able to reduce the computational overhead of the transformer compared to a baseline model, and that it can also be used to improve the inductive bias induced by Transformers. The paper also shows that the LN can be used as a proxy for the positional context in the transformer. ","This paper proposes a new normalization method, Dynamic Token Normalization (DTN), for computer vision tasks. The authors propose a unified formulation of the normalization methods, which is based on the idea of transformer-based normalization. The proposed method is evaluated on the Long-Range Arena, where it is shown to outperform the baseline model in terms of computational overhead and accuracy. "
3781,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,deep learning models USED-FOR expressive functions. SGD USED-FOR deep learning models. theoretical models USED-FOR spectral bias. methodologies USED-FOR spectral bias. spectral bias FEATURE-OF image classification networks. interventions USED-FOR generalization. spectral bias FEATURE-OF networks. regularization USED-FOR learning of high frequencies. models COMPARE ones. ones COMPARE models. models USED-FOR high frequencies. function frequency CONJUNCTION image frequency. image frequency CONJUNCTION function frequency. low frequencies PART-OF natural images. low frequencies USED-FOR spectral bias. natural images USED-FOR spectral bias. neural networks USED-FOR image classification. OtherScientificTerm is Spectral bias. Method is deep models. ,This paper studies the spectral bias of image classification networks with spectral bias in the presence of SGD. The authors propose two methodologies to reduce spectral bias. The first is to use regularization to encourage the learning of high frequencies. The second one is to learn the function frequency and the image frequency. Theoretical results show that these two methods can be combined to improve the performance of deep models.,This paper studies the spectral bias of image classification networks with deep learning models for expressive functions. Spectral bias is a well-studied problem in deep models. The authors propose two methodologies to reduce spectral bias in image classification. The first is SGD. The second is regularization to encourage the learning of high frequencies. The experiments show that the proposed models are able to learn high frequencies better than the ones that do not. The paper also shows that low frequencies in natural images are more susceptible to spectral bias than low frequencies of natural images. The main contribution of the paper is to propose two interventions to improve generalization. 
3797,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"Exploration PART-OF reinforcement learning ( RL ). monolithic behaviour policy USED-FOR methods. nonmonolithic exploration USED-FOR RL. algorithmic components USED-FOR switching mechanism. two - mode exploration CONJUNCTION switching. switching CONJUNCTION two - mode exploration. sub - episodic time - scales FEATURE-OF switching. sub - episodic time - scales FEATURE-OF two - mode exploration. switching USED-FOR Atari. two - mode exploration USED-FOR Atari. OtherScientificTerm are exploratory behaviours, switching triggers, and hyper - parameter - tuning burden. ","This paper studies the problem of exploration in reinforcement learning (RL). In RL, the goal is to learn a monolithic behaviour policy that maximizes the performance of the agent. The authors propose to use nonmonolithic exploration in RL as an alternative to monolithic exploration. The main idea is to use algorithmic components in the switching mechanism to learn the optimal switching mechanism. They show that two-mode exploration with sub-episodic time-scales and switching with hyper-parameter-tuning burden can achieve better performance than monolithic behavior policy. They also show that switching can be used to improve the performance in Atari. ","This paper proposes a new exploration method for reinforcement learning (RL). The main idea is to use nonmonolithic exploration in RL, where the goal is to learn a monolithic behaviour policy. The authors propose two methods, one based on monolithic exploration and the other based on multi-modal exploration. The main contribution of the paper is to introduce algorithmic components for the switching mechanism. The switching mechanism is based on two-mode exploration and switching on sub-episodic time-scales. The proposed method is evaluated on Atari, where it is shown to outperform other methods. "
3813,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,initialization scheme USED-FOR k - median problem. metric space FEATURE-OF k - median problem. metric embedding tree structure USED-FOR initialization scheme. discrete space HYPONYM-OF metric space. search algorithm USED-FOR initial centers. search algorithm USED-FOR local search algorithm. differential privacy ( DP ) USED-FOR private initial centers. HST initialization HYPONYM-OF method. k - median++ USED-FOR non - DP setting. initialization method USED-FOR non - DP setting. initialization method USED-FOR initial centers. HST initialization USED-FOR initial centers. k - median++ HYPONYM-OF initialization method. DP local search CONJUNCTION private HST initialization. private HST initialization CONJUNCTION DP local search. Method is clustering algorithms. Task is construction of metric embedding tree structure. OtherScientificTerm is privacy constraint. Generic is methods. ,"This paper proposes a new initialization scheme for the k-median problem in the metric space of k-mean problem in discrete space. The authors consider the construction of metric embedding tree structure for the initialization scheme, where the initial centers are defined in the discrete space, and the privacy constraint is enforced by differential privacy (DP). The authors propose a search algorithm for initial centers, which is based on a local search algorithm with DP local search and private HST initialization. The initialization method, k-midian++, is applied to the non-DP setting, and is shown to outperform existing clustering algorithms. ","This paper proposes a new initialization scheme for the k-median problem in the metric space, which is defined as the metric embedding tree structure in the discrete space. The main idea is to use differential privacy (DP) for private initial centers and a search algorithm for initial centers in the local search algorithm. The paper also proposes a non-DP setting using k-midi++ and HST initialization for the initial centers. Experiments on clustering algorithms show that the proposed methods outperform other methods. "
3829,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"planning USED-FOR agent. representation USED-FOR visual perception tasks. agent USED-FOR complex dynamics of the real - world. agent USED-FOR representation. complicated dynamics CONJUNCTION broader domain. broader domain CONJUNCTION complicated dynamics. broader domain FEATURE-OF real - life datasets. complicated dynamics FEATURE-OF real - life datasets. narrow benchmarks EVALUATE-FOR video prediction models. underfitting USED-FOR low quality predictions. FitVid HYPONYM-OF architecture. image augmentation techniques USED-FOR it. FitVid COMPARE models. models COMPARE FitVid. video prediction benchmarks EVALUATE-FOR models. video prediction benchmarks EVALUATE-FOR FitVid. metrics EVALUATE-FOR models. metrics EVALUATE-FOR FitVid. Generic are task, they, and state - of - theart models. Method is video models. OtherScientificTerm is overfitting. ","This paper proposes a new architecture called FitVid for video models. The proposed architecture is based on the idea of planning an agent to learn a representation of the complex dynamics of the real-world, which is then used to train an agent for visual perception tasks. The agent is trained on a variety of real-life datasets, including complicated dynamics and a broader domain. The authors show that the proposed model performs better than existing models on several video prediction benchmarks on narrow benchmarks. They also show that underfitting can lead to low quality predictions and overfitting can result in overfitting. Finally, they show that it is possible to use image augmentation techniques to improve the performance of the proposed models.","This paper proposes a new architecture, FitVid, for visual perception tasks. The main idea is to learn a representation of the complex dynamics of the real-world using planning. The agent is trained in a single task, and the goal is to generate a representation that can be used to perform the task. The authors show that the proposed architecture outperforms state-of-theart models on several video prediction benchmarks on narrow benchmarks and on a broader domain of real-life datasets with complicated dynamics. They also show that underfitting can lead to low quality predictions due to overfitting, and they show that it can be reduced by using image augmentation techniques. Finally, the authors provide a set of metrics to evaluate the performance of the proposed models."
3845,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,generalization EVALUATE-FOR machine learning algorithm. neural network HYPONYM-OF machine learning algorithm. model USED-FOR test loss. model USED-FOR stochastic gradient descent ( SGD ). data structure USED-FOR test loss dynamics. arbitrary covariance structure FEATURE-OF features. Gaussian features CONJUNCTION arbitrary features. arbitrary features CONJUNCTION Gaussian features. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. Gaussian model USED-FOR test loss. Gaussian model USED-FOR nonlinear random - feature models. nonlinear random - feature models CONJUNCTION deep neural networks. deep neural networks CONJUNCTION nonlinear random - feature models. theory USED-FOR Gaussian features. Gaussian model USED-FOR deep neural networks. real datasets EVALUATE-FOR deep neural networks. SGD USED-FOR deep neural networks. MNIST HYPONYM-OF real datasets. CIFAR-10 HYPONYM-OF real datasets. fixed compute budget FEATURE-OF optimal batch size. feature correlation structure USED-FOR optimal batch size. small batch sizes USED-FOR SGD. theory USED-FOR stochastic gradient descent. framework USED-FOR training and test error. real data EVALUATE-FOR framework. fixed subsampled training set USED-FOR stochastic gradient descent. OtherScientificTerm is structure of the data distribution. ,"This paper studies the problem of generalization of a machine learning algorithm called neural network. The authors propose a model for stochastic gradient descent (SGD) based on a Gaussian model trained on a fixed subsampled training set. The main idea is to use the data structure of the data distribution to model the test loss dynamics of the model, and then use this model to train a test loss for the target task. The paper shows that the model can be used to improve the performance of deep neural networks and nonlinear random-feature models trained with SGD.  The paper also shows that SGD can be trained with small batch sizes, and that the optimal batch size has a fixed compute budget.  Finally, the paper provides a theoretical analysis of the feature correlation structure of SGD, and shows that this structure can be applied to the training and test error. ","This paper proposes a new model for stochastic gradient descent (SGD) based on a neural network. The main idea is to use the structure of the data distribution to model the test loss dynamics. The paper shows that the features of the features have an arbitrary covariance structure, which can be used to improve the generalization of the machine learning algorithm. The authors also propose a Gaussian model for test loss, which is used to train deep neural networks and nonlinear random-feature models. The proposed framework is evaluated on real datasets such as MNIST and CIFAR-10. The theoretical results show that SGD with small batch sizes can achieve better training and test error than SGD on a fixed subsampled training set with fixed compute budget. "
3861,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"Stochastic gradient descent ( SGD ) USED-FOR nonlinear, nonconvex problem. learning rate CONJUNCTION model. model CONJUNCTION learning rate. AMSGrad USED-FOR local maxima. sharp minima USED-FOR SGD. Method are deep neural networks, and minimal neural network - like construction. Task is optimization problems. ","This paper studies the nonlinear, nonconvex problem of Stochastic gradient descent (SGD) in deep neural networks. The authors consider the case where the learning rate and the model are different, and the local maxima of the AMSGrad are not known. They show that SGD with sharp minima can be approximated by a minimal neural network-like construction. They also show that the optimization problems can be solved with SGD.","This paper studies the nonlinear, nonconvex problem of Stochastic gradient descent (SGD) in deep neural networks. The main idea is to use AMSGrad to compute local maxima for SGD with sharp minima. The learning rate of the model and the model of the SGD are the same. The authors propose a minimal neural network-like construction. The optimization problems can be solved by solving optimization problems."
3877,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"gradientbased hyperparameter optimization ( HO ) methods USED-FOR hyperparameters. Implicit Function Theorem ( IFT ) based methods USED-FOR online optimization. high - dimensional hyperparameters CONJUNCTION horizon length. horizon length CONJUNCTION high - dimensional hyperparameters. short horizon bias FEATURE-OF short horizon approximations. knowledge distillation USED-FOR second - order term. Jacobian - vector product ( JVP ) USED-FOR HO step. hyperparameter dimension CONJUNCTION horizon length. horizon length CONJUNCTION hyperparameter dimension. method USED-FOR online optimization. hyperparameter dimension USED-FOR method. meta - learning methods CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION meta - learning methods. meta - learning methods EVALUATE-FOR method. benchmark datasets EVALUATE-FOR method. Method are gradient - based meta - learning methods, inner - optimization, Unrolled differentiation methods, and HO method. ",This paper studies the problem of online optimization with Implicit Function Theorem (IFT) based methods for hyperparameters. The authors propose a gradient-based meta-learning methods for online optimization. The main idea is to use the Jacobian-vector product (JVP) of the HO step as the second-order term in the knowledge distillation. The proposed method is evaluated on a variety of benchmark datasets and shows that the proposed method achieves better performance than existing methods with hyperparameter dimension and horizon length. ,"This paper proposes a new method for online optimization based on Implicit Function Theorem (IFT) based methods for hyperparameter optimization (HO) methods. The main idea is to use the Jacobian-vector product (JVP) as the second-order term in the HO step, and then use knowledge distillation to reduce the short horizon bias of short horizon approximations. The proposed method is evaluated on several meta-learning methods and benchmark datasets, and compared to other gradient-based meta-Learning methods and inner-optimization methods. Unrolled differentiation methods are also used in the proposed HO method."
3893,SP:a64b26faef315c3ece590322291bab198932c604,"tasks USED-FOR meta - learning. meta - learning USED-FOR learning of new tasks. globally shared metalearner USED-FOR tasks. globally shared metalearner USED-FOR meta - learning. customization CONJUNCTION generalization. generalization CONJUNCTION customization. task clustering USED-FOR task - aware modulation. methods USED-FOR task representation. baselearner model USED-FOR task - specific optimization process. features USED-FOR task representation. features CONJUNCTION learning path. learning path CONJUNCTION features. features USED-FOR task representation. learning path USED-FOR task representation. geometric quantities USED-FOR learning path. path representation USED-FOR downstream clustering and modulation. meta path learner USED-FOR path representation. shortcut tunnel USED-FOR feature cluster assignments. shortcut tunnel USED-FOR path. path CONJUNCTION feature cluster assignments. feature cluster assignments CONJUNCTION path. few - shot image classification CONJUNCTION cold - start recommendation. cold - start recommendation CONJUNCTION few - shot image classification. CTML COMPARE baselines. baselines COMPARE CTML. real - world application domains EVALUATE-FOR CTML. real - world application domains EVALUATE-FOR baselines. cold - start recommendation EVALUATE-FOR CTML. cold - start recommendation HYPONYM-OF real - world application domains. few - shot image classification HYPONYM-OF real - world application domains. OtherScientificTerm is task heterogeneity. Method are global meta - learner, rehearsed task learning, and rehearsed learning. ","This paper studies the problem of meta-learning for learning of new tasks with task heterogeneity. The authors propose a globally shared metalearner for tasks where the global meta-learner is trained to learn a task representation from a set of features and a learning path. The learning path represents the task representation using geometric quantities. The path representation is then used for downstream clustering and modulation. The task-aware modulation is based on task clustering. The baselearner model is used for the task-specific optimization process.  The authors show that CTML outperforms baselines in several real-world application domains, including few-shot image classification, cold-start recommendation, and CTML with rehearsed task learning. ","This paper proposes a new meta-learning framework for learning new tasks. The authors propose to use a globally shared metalearner for learning of new tasks, and a global meta-learner for the task-specific optimization process. The proposed methods are based on methods for learning a task representation from a set of features and a learning path. The learning path is based on geometric quantities. The path representation is then used for downstream clustering and modulation. The paper also proposes a shortcut tunnel to learn feature cluster assignments and a meta path learner to learn the path representation. Experiments on CTML and few-shot image classification show that the proposed baselines outperform CTML in both real-world application domains and cold-start recommendation. "
3909,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"in - distribution ( ID ) data EVALUATE-FOR deep neural networks. methods USED-FOR near OOD samples. labeled data USED-FOR near OOD samples. ensemble - based procedure USED-FOR semi - supervised novelty detection ( SSND ). ensemble - based procedure USED-FOR detection. unlabeled ID and OOD samples USED-FOR ensemble - based procedure. regularization USED-FOR OOD data. regularization USED-FOR It. approach COMPARE SSND methods. SSND methods COMPARE approach. image data sets CONJUNCTION medical image data sets. medical image data sets CONJUNCTION image data sets. medical image data sets EVALUATE-FOR SSND methods. image data sets EVALUATE-FOR SSND methods. medical image data sets EVALUATE-FOR approach. image data sets EVALUATE-FOR approach. Task is expert evaluation. Method is OOD detection algorithms. Material are near OOD data, and ID data. Metric is computational cost. ",This paper studies the problem of semi-supervised novelty detection (SSND) in deep neural networks with in-distribution (ID) data. The authors propose two methods to detect near OOD samples from labeled data. It uses an ensemble-based procedure to perform detection using unlabeled ID and ood samples. It also uses regularization on OOD data to improve the performance of the OOD detection algorithms. Experimental results show that the proposed approach performs better than the existing SSND methods on image data sets and medical images. ,"This paper proposes a novel ensemble-based procedure for semi-supervised novelty detection (SSND) on in-distribution (ID) data for deep neural networks. It is based on the idea of regularization of OOD data with unlabeled ID and OOD samples. The proposed approach is evaluated on several image data sets and two medical images, and compared to other OOD detection algorithms. The results show that the proposed approach performs better than other SSND methods on both the ID data and on the medical images. "
3925,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"multi - agent trajectory prediction USED-FOR safe control of robotic systems. representation USED-FOR planning. encoder - decoder architectures USED-FOR scene - consistent multi - agent trajectories. Latent Variable Sequential Set Transformers HYPONYM-OF encoder - decoder architectures. Latent Variable Sequential Set Transformers USED-FOR scene - consistent multi - agent trajectories. AutoBots ” HYPONYM-OF architectures. temporal and social dimensions FEATURE-OF equivariant processing. model USED-FOR single - agent prediction case. Argoverse vehicle prediction challenge EVALUATE-FOR model. global nuScenes vehicle motion prediction leaderboard EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR model. model USED-FOR socially - consistent predictions. multi - agent setting EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR socially - consistent predictions. synthetic partition of TrajNet++ dataset EVALUATE-FOR multi - agent setting. desktop GPU ( 1080 Ti ) USED-FOR models. Method are encoder, and decoder. OtherScientificTerm is sequential structure. Material is Omniglot data. Generic is method. ","This paper studies the problem of multi-agent trajectory prediction for safe control of robotic systems. The authors propose two encoder-decoder architectures, “Latent Variable Sequential Set Transformers” and “AutoBots”, to learn scene-consistent multi-Agent trajectories. The encoder is trained on Omniglot data, and the decoder learns a sequential structure. The representation is then used for planning. The model is evaluated on the Argoverse vehicle prediction challenge, and on the global nuScenes vehicle motion prediction leaderboard. The proposed model is shown to perform well in the single-agent prediction case, and is also able to make socially-consistency predictions in a synthetic partition of TrajNet++ dataset. The paper also shows that the proposed models can be trained on a desktop GPU (1080 Ti) and can be fine-tuned on a variety of tasks.","This paper proposes a method for multi-agent trajectory prediction for safe control of robotic systems. The authors propose two encoder-decoder architectures, “Latent Variable Sequential Set Transformers” and “AutoBots”, which are based on the sequential structure of Omniglot data. The representation is used for planning, and the decoder is trained on the representation. The model is evaluated on the Argoverse vehicle prediction challenge and on the global nuScenes vehicle motion prediction leaderboard. Experiments on the synthetic partition of TrajNet++ dataset show that the model is able to make socially-consistent predictions in the single-agent prediction case and in the multi-Agent setting on the desktop GPU (1080 Ti) with equivariant processing with temporal and social dimensions."
3941,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"methods USED-FOR image classification models. baseline explanation technique COMPARE concept - based and counterfactual explanations. concept - based and counterfactual explanations COMPARE baseline explanation technique. baseline COMPARE concept - based explanations. concept - based explanations COMPARE baseline. Counterfactual explanations COMPARE baseline. baseline COMPARE Counterfactual explanations. invertible neural network USED-FOR Counterfactual explanations. technical evaluations CONJUNCTION proxy tasks. proxy tasks CONJUNCTION technical evaluations. Generic are they, and model. Method is synthetic dataset generator. ","This paper proposes a new baseline explanation technique for image classification models. Counterfactual explanations are based on an invertible neural network, and they can be combined with concept-based and counterfactual explanation. The authors show that the proposed baseline is more interpretable than the baseline, and can be used to train a model on a synthetic dataset generator. The proposed method is evaluated on technical evaluations and proxy tasks.",This paper proposes a new baseline explanation technique that combines concept-based and counterfactual explanations to improve the performance of image classification models. The proposed baseline is based on an invertible neural network. The authors show that they outperform the baseline on technical evaluations and proxy tasks. The model is trained on a synthetic dataset generator. 
3957,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,deep networks USED-FOR backdoor data poisoning attacks. model USED-FOR inference. iterative training procedure USED-FOR poisoned data. boosting framework USED-FOR clean data. boosting framework USED-FOR poisoned data. bootstrapped measure of generalization USED-FOR algorithm. method USED-FOR dirty label backdoor attack. approach COMPARE defenses. defenses COMPARE approach. OtherScientificTerm is malicious data. Method is ensemble of weak learners. ,This paper studies backdoor data poisoning attacks on deep networks. The authors propose an iterative training procedure to recover poisoned data from an ensemble of weak learners. The proposed algorithm is based on bootstrapped measure of generalization and uses a boosting framework to recover clean data from poisoned data. They show that the proposed approach outperforms existing defenses. ,"This paper proposes a method for backdoor data poisoning attacks on deep networks. The authors propose an iterative training procedure for poisoned data, where the poisoned data is generated by an ensemble of weak learners, and the model is used for inference. The algorithm is based on a bootstrapped measure of generalization. The proposed boosting framework is applied to clean data and to poisoned data. The method is tested on a dirty label backdoor attack, and shows that the proposed approach outperforms defenses."
3973,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"MLTC USED-FOR modeling label correlations. single - label text classification methods COMPARE MLTC. MLTC COMPARE single - label text classification methods. document representation learning USED-FOR single - label text classification methods. label - correlation simplification CONJUNCTION sequencing label sets. sequencing label sets CONJUNCTION label - correlation simplification. sequencing label sets CONJUNCTION label - correlation overload. label - correlation overload CONJUNCTION sequencing label sets. It USED-FOR inductive bias. sequencing label sets HYPONYM-OF inductive bias. label - correlation simplification HYPONYM-OF inductive bias. latent label representations USED-FOR label correlations. latent labels USED-FOR contextual encodings. benchmark datasets EVALUATE-FOR It. label - correlation utilization CONJUNCTION document representation. document representation CONJUNCTION label - correlation utilization. token embeddings COMPARE latent labels. latent labels COMPARE token embeddings. embeddings FEATURE-OF latent labels. task information FEATURE-OF they. Task is Multi - label text classification ( MLTC ). OtherScientificTerm are complex label dependencies, and text tokens. Generic are method, and BERT. Method are latent - label encodings, latent and distributed correlation modeling, and latent label embeddings. ","This paper studies the problem of Multi-label text classification (MLTC) in the context of document representation learning. The authors propose a new method, MLTC, for modeling label correlations. The proposed method is based on the idea of latent-label encodings, which can be used for both latent and distributed correlation modeling. It is shown to reduce inductive bias in the form of label-correlation simplification and sequencing label sets. It also shows that the embeddings of the latent labels are more interpretable and can capture more task information. ","This paper proposes Multi-label text classification (MLTC), a new method for modeling label correlations. The authors propose to use document representation learning to improve the performance of MLTC over single-label language classification methods. The proposed method is based on BERT. It is evaluated on two benchmark datasets: label-correlation simplification and sequencing label sets, and it is shown to reduce inductive bias in the case of label-relation overload in the context of sequencing labels. The paper also shows that the latent-label encodings of the latent and distributed correlation modeling can be better than the token embeddings in the latent labels, and that they can capture more task information. The latent label representations can be used to capture contextual information about the label correlations, which can be useful for inferring more complex label dependencies. "
3989,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"images CONJUNCTION audio. audio CONJUNCTION images. deep convolutional networks USED-FOR tasks. deep convolutional networks USED-FOR highdimensional data. highdimensional data USED-FOR tasks. images HYPONYM-OF highdimensional data. audio HYPONYM-OF highdimensional data. convolution and pooling layers FEATURE-OF hierarchical kernels. convolutional kernel networks USED-FOR hierarchical kernels. norm USED-FOR spatial similarities. pooling layers USED-FOR norm. additive models of interaction terms PART-OF RKHS. pooling layers USED-FOR spatial similarities. pooling CONJUNCTION patches. patches CONJUNCTION pooling. sample complexity guarantees EVALUATE-FOR patches. Generic are they, and terms. Method is kernel methods. Material is vision datasets. OtherScientificTerm are functional space, inductive bias, generalization bounds, and regularities. ","This paper studies the problem of learning deep convolutional networks for high-dimensional data (e.g., images and audio). The authors propose a new kernel method, called RKHS, which is based on the additive models of interaction terms in the input space. The authors show that the proposed method can achieve better sample complexity guarantees than existing kernel methods. The main contribution of the paper is a new norm for the spatial similarities between the convolution and pooling layers of hierarchical kernels. Theoretically, the authors prove that the norm is a function of the functional space, and that the inductive bias of the norm can be explained by the generalization bounds of the regularities. ","This paper proposes a new framework for learning deep convolutional networks for high-dimensional data (e.g., images, audio) from highdimensional data. The key idea is to use additive models of interaction terms in RKHS. The authors show that they can be used to learn hierarchical kernels with convolution and pooling layers. They also show that the norm for spatial similarities between the norm and the norm of the norm can be learned from the norm. They show that these regularities are robust to inductive bias, and they show that their generalization bounds are also robust to noise in the functional space. Finally, the authors provide some experiments on vision datasets, showing that their proposed kernel methods can learn patches with high sample complexity guarantees."
4005,SP:7bee8d65c68765cbfe38767743fec27981879d34,"Neural Tangent Kernel ( NTK ) PART-OF deep learning. NTK USED-FOR training and generalization of NN architectures. infinite width limit FEATURE-OF NTK. NTK USED-FOR NNs. architecture search CONJUNCTION meta - learning. meta - learning CONJUNCTION architecture search. NTK USED-FOR finite widths. compute and memory requirements FEATURE-OF NTK computation. NTK computation PART-OF finite width networks. compute and memory requirements FEATURE-OF finite width NTK. neural networks USED-FOR algorithms. algorithms USED-FOR finite width NTK. compute and memory requirements EVALUATE-FOR algorithms. attention CONJUNCTION recurrence. recurrence CONJUNCTION attention. convolutions CONJUNCTION attention. attention CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR differentiable computation. algorithms USED-FOR differentiable computation. convolutions CONJUNCTION recurrence. recurrence CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR algorithms. recurrence HYPONYM-OF algorithms. convolutions HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF algorithms. recurrence HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF general - purpose JAX function transformations. recurrence HYPONYM-OF differentiable computation. convolutions HYPONYM-OF differentiable computation. attention HYPONYM-OF differentiable computation. OtherScientificTerm are neural network ( NN ) Jacobians, and hyper - parameters. Method is NN architectures. ","This paper proposes a new neural Tangent Kernel (NTK) for deep learning. NTK is an extension of NTK for training and generalization of NN architectures with infinite width limit. The authors show that NTK can be applied to NNs with finite widths. The NTK computation in finite width networks has compute and memory requirements that are much lower than NTK in the case of neural network (NN) Jacobians with hyper-parameters.  The authors propose two algorithms for finite width NTK: convolutions and recurrence, which are general-purpose JAX function transformations for differentiable computation such as attention, convolutions, and attention. The proposed algorithms are evaluated on a variety of datasets and show that the proposed algorithms achieve better compute-and-memory requirements than the state-of-the-art. ","Neural Tangent Kernel (NTK) is an important part of deep learning. NTK is used for training and generalization of NN architectures with infinite width limit. The authors show that NTK can be applied to NNs with finite width limit, and also to finite widths. The NTK computation in finite width networks is computationally efficient, and the compute and memory requirements of NTK computations are much lower than those of the standard neural network (NN) Jacobians. The algorithms for finite width NTK are based on neural networks with hyper-parameters. The proposed algorithms include convolutions, attention, recurrence, and general-purpose JAX function transformations for differentiable computation.   The authors also show that the proposed algorithms are computationally faster than existing algorithms in terms of compute and the memory requirements. "
4021,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"policy USED-FOR expected return. problem setting USED-FOR real - world scenarios. safety constraints FEATURE-OF policy. policy USED-FOR offline RL setting. estimation error FEATURE-OF offpolicy evaluation. offline constrained RL algorithm USED-FOR policy. stationary distribution FEATURE-OF policy. stationary distribution corrections FEATURE-OF optimal policy. algorithm USED-FOR stationary distribution corrections. algorithm USED-FOR cost - conservative policy. returns FEATURE-OF optimal policy. constraint satisfaction CONJUNCTION return - maximization. return - maximization CONJUNCTION constraint satisfaction. COptiDICE COMPARE baseline algorithms. baseline algorithms COMPARE COptiDICE. COptiDICE USED-FOR policies. return - maximization FEATURE-OF policies. constraint satisfaction FEATURE-OF policies. constraint satisfaction EVALUATE-FOR COptiDICE. return - maximization EVALUATE-FOR COptiDICE. Task is offline constrained reinforcement learning ( RL ) problem. OtherScientificTerm are cost constraints, and cost upper bound. Material is pre - collected dataset. ","This paper studies the offline constrained reinforcement learning (RL) problem, where the goal is to learn a policy that maximizes the expected return under safety constraints. The paper proposes an offline constrained RL algorithm, COptiDICE, that learns a policy in an offline RL setting where the cost constraints on the policy are enforced by stationary distribution corrections. The algorithm is based on the cost-conservative policy, which is an offpolicy evaluation with an estimation error that depends on the stationary distribution of the policy. The authors provide a cost upper bound for the algorithm, and show that the algorithm is able to learn the optimal policy in the stationary setting. The proposed algorithm is shown to achieve better return-maximization and constraint satisfaction than the baseline algorithms. ","This paper studies the offline constrained reinforcement learning (RL) problem. The authors propose an offline constrained RL algorithm for the offline RL setting, where the goal is to learn a policy that maximizes the expected return under safety constraints. This problem setting is applicable to real-world scenarios, where there are no cost constraints. The paper proposes an algorithm for learning a cost-conservative policy with stationary distribution corrections for the optimal policy. The cost upper bound is based on the estimation error of the offpolicy evaluation. The proposed COptiDICE is evaluated on a pre-collected dataset, and compared to several baseline algorithms. The results show that the proposed policies achieve better constraint satisfaction and return-maximization than other policies."
4037,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"dataparallel CONJUNCTION model - parallel training algorithms. model - parallel training algorithms CONJUNCTION dataparallel. parallelization strategies USED-FOR GRU. model - parallel training algorithms HYPONYM-OF parallelization strategies. dataparallel HYPONYM-OF parallelization strategies. training time EVALUATE-FOR approaches. parallel training scheme USED-FOR GRU. parallel - in - time HYPONYM-OF parallel training scheme. multigrid reduction in time ( MGRIT ) solver USED-FOR parallel training scheme. hierarchical correction of the hidden state USED-FOR end - to - end communication. parallel training scheme COMPARE serial approach. serial approach COMPARE parallel training scheme. HMDB51 dataset EVALUATE-FOR parallel training scheme. speedup EVALUATE-FOR serial approach. speedup EVALUATE-FOR parallel training scheme. parallelization strategy COMPARE parallel GRU algorithm. parallel GRU algorithm COMPARE parallelization strategy. sequence length FEATURE-OF parallelization strategy. Task is Parallelizing Gated Recurrent Unit ( GRU ) networks. Method are MGRIT, and gradient descent. OtherScientificTerm is processors. Material is image sequence. ","This paper studies the problem of parallelizing Gated Recurrent Unit (GRU) networks. The authors propose two parallelization strategies for GRU: dataparallel and model-parallel training algorithms. The parallel training scheme is based on the multigrid reduction in time (MGRIT) solver, and the authors show that the training time of these approaches can be reduced by a factor of at least 1.5 in the case of MGRIT. The main contribution of the paper is the hierarchical correction of the hidden state in the end-to-end communication between the two processors. The paper also shows that the parallelization strategy can achieve better speedup than the standard parallel GRU algorithm in terms of sequence length. ",This paper proposes a new parallelization strategy for parallelizing Gated Recurrent Unit (GRU) networks. The proposed parallelization strategies are based on dataparallel and model-parallel training algorithms. The authors propose a parallel training scheme for GRU based on multigrid reduction in time (MGRIT) solver. The main idea of MGRIT is to use a hierarchical correction of the hidden state of the end-to-end communication between two processors. This is done by gradient descent. Experiments are conducted on the HMDB51 dataset and show that the proposed parallelized GRU algorithm achieves better speedup compared to the standard parallelization scheme with sequence length.
4053,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"Functional magnetic resonance imaging ( fMRI ) HYPONYM-OF noisy measurement of brain activity. measurement resolution USED-FOR spatiotemporal averaging. PCA CONJUNCTION shared response modeling ( SRM ). shared response modeling ( SRM ) CONJUNCTION PCA. linear methods USED-FOR they. shared response modeling ( SRM ) HYPONYM-OF linear methods. PCA HYPONYM-OF linear methods. neural network USED-FOR common embedding. common space USED-FOR extensible manifold. classification accuracy EVALUATE-FOR stimulus features. framework USED-FOR applications. OtherScientificTerm are environmental differences, intrinsic dimension, brain activity, intrinsic structure, and noise. Generic is approaches. Material is raw fMRI signals. Task is cross - subject translation of fMRI signals. ","This paper studies the problem of cross-subject translation of fMRI signals. The authors propose a new noisy measurement of brain activity, Functional Magnetic Resonance Imaging (fMRI) HYPONYM, based on the measurement resolution of spatiotemporal averaging. They show that existing approaches such as PCA and shared response modeling (SRM) do not capture the intrinsic dimension of the input signal, and that they cannot capture the environmental differences between the input and the target signal. To address this issue, the authors propose to use a common embedding in a neural network to capture the differences between input and target signals. They also propose an extensible manifold with a common space that can be used to represent the intrinsic structure of the inputs. The proposed framework can be applied to a variety of applications, and is shown to improve the classification accuracy of stimulus features.","This paper proposes a new noisy measurement of brain activity, Functional magnetic resonance imaging (fMRI) HYPONYM, which is based on the idea of cross-subject translation of fMRI signals. The authors propose a new measurement resolution for spatiotemporal averaging, which can be applied to environmental differences between the intrinsic dimension of the brain activity and the intrinsic structure of the environment. They show that they outperform linear methods such as PCA and shared response modeling (SRM) in terms of classification accuracy and classification accuracy of stimulus features. They also show that common embedding can be achieved using a neural network. They provide a framework for applications where they can use the extensible manifold in the common space. "
4069,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"biological phenomena CONJUNCTION self - driving cars. self - driving cars CONJUNCTION biological phenomena. Detecting out - of - distribution examples USED-FOR safety - critical machine learning applications. detecting novel biological phenomena HYPONYM-OF safety - critical machine learning applications. self - driving cars HYPONYM-OF safety - critical machine learning applications. benchmarks USED-FOR large - scale settings. ImageNet-21 K USED-FOR PASCAL VOC and COCO multilabel anomaly detectors. benchmark USED-FOR anomaly segmentation. road anomalies FEATURE-OF segmentation benchmark. segmentation benchmark USED-FOR benchmark. detector COMPARE prior methods. prior methods COMPARE detector. maximum logit USED-FOR detector. OtherScientificTerm are small - scale settings, and real - world settings. Task is out - of - distribution detection. Material is high - resolution images. Method is ImageNet multiclass anomaly detectors. ",This paper proposes a new benchmark for out-of-distribution anomaly detection. The proposed benchmark is based on the PASCAL VOC and COCO multilabel anomaly detectors from ImageNet-21K. The authors show that the proposed detector is more robust than prior methods and achieves the maximum logit of the detector. ,"This paper proposes ImageNet multiclass anomaly detectors for out-of-distribution detection. The proposed method is based on ImageNet-21K, which is an extension of PASCAL VOC and COCO multilabel anomaly detectors. The main idea is to use high-resolution images as a benchmark for anomaly segmentation on road anomalies. The detector is trained with maximum logit, and compared to prior methods. The results show that the proposed detector outperforms prior methods in both small-scale settings and in real-world settings. "
4085,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"parametric models USED-FOR intransitive tournaments. d dimensional node representations USED-FOR parametric models. d dimensional representations USED-FOR class of tournaments. theory USED-FOR parametric tournament representations. d dimensional representations USED-FOR class of tournaments. forbidden configurations FEATURE-OF tournament classes. tournaments PART-OF forbidden flip class. rank 2 tournaments CONJUNCTION locally - transitive tournaments. locally - transitive tournaments CONJUNCTION rank 2 tournaments. tournament class USED-FOR minimum feedback arc set problem. Quicksort procedure USED-FOR minimum feedback arc set problem. coned - doubly regular tournament FEATURE-OF flip class. forbidden configuration FEATURE-OF flip class. minimum dimension USED-FOR tournaments. upper bound USED-FOR smallest representation dimension. flip class FEATURE-OF tournament. flip class FEATURE-OF feedback arc set. OtherScientificTerm are Real world tournaments, union of flip classes, rank d tournament class, and sign - rank of matrices. ","This paper studies the problem of learning parametric models for intransitive tournaments with d dimensional node representations. The authors propose a new theory for parametric tournament representations based on d dimensional representations for the class of tournaments with forbidden configurations. They show that the tournament class with a given tournament class can be represented as a minimum feedback arc set problem with a Quicksort procedure. They also provide an upper bound for the smallest representation dimension for the tournament with a specific flip class. The paper also shows that the tournaments with a particular type of forbidden configuration can be parametrized as a rank d tournament class. Finally, they show that rank 2 tournaments and locally-transitive tournaments can be modeled as a union of flip classes. ","This paper proposes a new theory for parametric tournament representations for intransitive tournaments. The authors propose to use d dimensional node representations to train parametric models for the class of tournaments with d dimensional representations. They show that in the case of Real world tournaments, the union of flip classes can be represented as a rank d tournament class. They also show that the forbidden configurations of the tournament classes with different tournament classes have different forbidden configurations. They then propose a novel tournament class for the minimum feedback arc set problem with a Quicksort procedure. The flip class of a tournament with a coned-doubly regular tournament has a different forbidden configuration than the flip class with a conformal regular tournament. Finally, the authors provide an upper bound for the smallest representation dimension of the smallest tournament class, which is a sign-rank of matrices. "
4101,SP:d39765dcc8950d4fc1d43e4c167208736578882e,context dataset USED-FOR Neural processes ( NPs ). identifier USED-FOR task. context representation USED-FOR identifier. dataset USED-FOR context representation. NPs USED-FOR identifier. context representation USED-FOR NPs. dataset USED-FOR NPs. network architectures CONJUNCTION aggregation functions. aggregation functions CONJUNCTION network architectures. NPs USED-FOR context embedding approaches. prediction accuracy EVALUATE-FOR NPs. permutation invariant FEATURE-OF aggregation functions. stochastic attention mechanism USED-FOR context information. stochastic attention mechanism USED-FOR NPs. NPs USED-FOR context information. method USED-FOR context embedding. method USED-FOR NPs. NPs USED-FOR context embedding. information theory USED-FOR method. features USED-FOR NPs. method USED-FOR context embedding. noisy data sets CONJUNCTION restricted task distributions. restricted task distributions CONJUNCTION noisy data sets. noisy data sets USED-FOR method. context embeddings USED-FOR NPs. predator - prey model CONJUNCTION image completion. image completion CONJUNCTION predator - prey model. 1D regression CONJUNCTION predator - prey model. predator - prey model CONJUNCTION 1D regression. approach COMPARE NPs. NPs COMPARE approach. 1D regression USED-FOR NPs. predator - prey model USED-FOR NPs. predator - prey model USED-FOR approach. image completion USED-FOR approach. 1D regression EVALUATE-FOR approach. MovieLens-10k dataset HYPONYM-OF real - world problem. real - world problem EVALUATE-FOR method. MovieLens-10k dataset EVALUATE-FOR method. ,"This paper proposes a new context dataset for Neural processes (NNs). The proposed dataset contains a set of NPs that are trained on a dataset that contains the identifier for each task and the context representation for each NPs. The NPs are trained using NPs as the identifier, and the dataset is used to train the NPs to learn the context embedding. The proposed method uses a stochastic attention mechanism to extract the context information from NPs, and then uses NPs for context embeddings for NPs in order to improve the prediction accuracy of these NPs when compared to other context-embedding approaches. The method is evaluated on MovieLens-10k dataset with noisy data sets and restricted task distributions, and on 1D regression with a predator-prey model and an image completion. The results show that the proposed method outperforms NPs on both 1D and 2D tasks. ","This paper presents a context dataset for Neural processes (NNs). The dataset contains a set of NPs for a given task, and a context representation for each NPs. The dataset is used to train NPs on the context representation, and the identifier for each task is used as the context embedding approaches. The proposed method is based on the stochastic attention mechanism to extract context information from NPs, which is a permutation invariant of aggregation functions and network architectures. The method is evaluated on the MovieLens-10k dataset with noisy data sets and restricted task distributions, and on 1D regression with a predator-prey model and an image completion model. The authors show that the proposed method outperforms other context embeddings for NPs in terms of prediction accuracy. They also show that their approach is more robust to noise than other NPs and that their method can be applied to context encoding of the features of the NPs to improve the performance of the proposed approach. "
4117,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"Transformer language models USED-FOR NLP tasks. prototype networks PART-OF model architecture. architecture COMPARE language models. language models COMPARE architecture. user interactions USED-FOR it. Metric is interpretability. Generic are black - box models, and network. OtherScientificTerm is human capabilities. Method is data - driven approaches. ","This paper proposes a transformer language models for NLP tasks. The proposed model architecture is based on prototype networks, which are black-box models that are trained with user interactions. The authors show that the proposed architecture is more interpretable than existing language models, and that it is able to handle user interactions better than existing models. The paper also shows that the network can be trained with data-driven approaches. ","This paper proposes a new model architecture that combines prototype networks with black-box models for NLP tasks. The authors show that it is more interpretable due to user interactions, and it is able to capture human capabilities. The paper also shows that the proposed architecture outperforms other language models in terms of interpretability. The proposed network is evaluated on a variety of tasks, and compared to other data-driven approaches."
4133,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"Catastrophic forgetting PART-OF continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR forward knowledge transfer. continual learning USED-FOR forward knowledge transfer. scaled weight projection USED-FOR frozen weights. frozen weights PART-OF trust region. layer - wise scaling matrix USED-FOR scaled weight projection. layer - wise scaling matrix USED-FOR frozen weights. TRGP USED-FOR knowledge transfer. scaling matrices CONJUNCTION model. model CONJUNCTION scaling matrices. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Generic are methods, and task. OtherScientificTerm are optimization space, task correlation, layer - wise and single - shot manner, and subspaces of old tasks. Method is norm of gradient projection. ",This paper studies the problem of continual learning with catastrophic forgetting in continual learning. The authors propose Trust Region Gradient Projection (TRGP) for continual learning for forward knowledge transfer. The key idea is to use frozen weights in the trust region as a layer-wise scaling matrix for the scaled weight projection for the frozen weights and then use the norm of gradient projection to estimate the parameters of the optimization space. Theoretical results show that the proposed approach can achieve better performance than state-of-the-art methods on the same task. ,"This paper proposes Trust Region Gradient Projection (TRGP), a method for continual learning with catastrophic forgetting in continual learning. The key idea of TRGP is to use the norm of gradient projection in the optimization space as a surrogate for the task correlation. The authors show that the proposed approach is more robust to catastrophic forgetting than state-of-the-art methods. The proposed method is based on scaling matrices and a model. The paper also proposes a scaled weight projection for frozen weights in the trust region, which is a combination of layer-wise and single-shot manner. Experiments are conducted on two subspaces of old tasks."
4149,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,Optimization CONJUNCTION generalization. generalization CONJUNCTION Optimization. generalization PART-OF machine learning. Optimization PART-OF machine learning. framework USED-FOR generalization. framework USED-FOR optimization. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. gradient flow algorithm USED-FOR length of optimization trajectory. length of optimization trajectory USED-FOR generalization error. initialization USED-FOR gradient flow. estimate USED-FOR length - based generalization bound. short optimization paths USED-FOR generalization. it USED-FOR generalization estimates. kernel regression CONJUNCTION overparameterized two - layer ReLU neural networks. overparameterized two - layer ReLU neural networks CONJUNCTION kernel regression. underdetermined lp linear regression CONJUNCTION kernel regression. kernel regression CONJUNCTION underdetermined lp linear regression. it USED-FOR machine learning models. generalization estimates USED-FOR machine learning models. overparameterized two - layer ReLU neural networks HYPONYM-OF machine learning models. underdetermined lp linear regression HYPONYM-OF machine learning models. kernel regression HYPONYM-OF machine learning models. Generic is approach. OtherScientificTerm is explicit length estimate. ,"This paper proposes a new framework for optimizing optimization and generalization in machine learning. The authors propose a gradient flow algorithm to estimate the length of optimization trajectory for the generalization error. The main idea of the approach is to use initialization for the gradient flow and then use the explicit length estimate for the length-based generalization bound. The paper shows that it improves generalization estimates for machine learning models such as kernel regression, overparameterized two-layer ReLU neural networks, and underdetermined lp linear regression. ","This paper proposes a new framework for generalization in machine learning, which combines optimization and generalization. The authors propose a gradient flow algorithm to estimate the length of optimization trajectory for the generalization error. The main idea of the approach is to use an explicit length estimate for the length-based generalization bound, and then use initialization for the gradient flow. The paper shows that it improves generalization estimates for machine learning models such as kernel regression, overparameterized two-layer ReLU neural networks, and underdetermined lp linear regression. It also shows that short optimization paths can be used to improve generalization, and that the proposed framework can be applied to other optimization problems."
4165,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"Adversarial examples USED-FOR deep learning systems. high - frequency noise FEATURE-OF adversarial examples. CIFAR-10 CONJUNCTION ImageNet - derived datasets. ImageNet - derived datasets CONJUNCTION CIFAR-10. ImageNet - derived datasets USED-FOR models. CIFAR-10 USED-FOR models. frequency constraints USED-FOR robust models. Task is attacks. Generic are examples, and framework. Method is frequency - based understanding of adversarial examples. OtherScientificTerm is frequency - based explanation. ","This paper studies the problem of learning robust models with high-frequency noise in the presence of adversarial examples in deep learning systems. The authors propose a new framework for learning robust examples with high frequency noise. The framework is based on frequency-based understanding of the frequency of the examples, and the authors show that these examples can be used to train robust models using CIFAR-10 and ImageNet-derived datasets. They also show that the frequency constraints on the examples can improve the robustness of the models.","This paper proposes a novel framework for learning adversarial examples for deep learning systems with high-frequency noise. Specifically, the authors propose a frequency-based understanding of adversarial example, where the examples are generated by the high-frequencies of the input data. The authors show that the proposed framework is able to learn robust models with CIFAR-10 and ImageNet-derived datasets with frequency constraints. They also provide a frequency based explanation of the attacks."
4181,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"relational inductive bias ( homophily assumption ) USED-FOR graph structures. graph structures USED-FOR Graph Neural Networks ( GNNs ). GNNs COMPARE NNs. NNs COMPARE GNNs. GNNs COMPARE graph - agnostic NNs. graph - agnostic NNs COMPARE GNNs. NNs USED-FOR real - world tasks. real - world tasks EVALUATE-FOR GNNs. aggregation operation USED-FOR GNNs. similarity matrix USED-FOR GNNs. graph structure CONJUNCTION features. features CONJUNCTION graph structure. features USED-FOR GNNs. similarity matrix USED-FOR metrics. metrics COMPARE homophily metrics. homophily metrics COMPARE metrics. synthetic graphs EVALUATE-FOR homophily metrics. diversification operation USED-FOR harmful heterophily. diversification CONJUNCTION identity channels. identity channels CONJUNCTION diversification. aggregation CONJUNCTION diversification. diversification CONJUNCTION aggregation. Adaptive Channel Mixing ( ACM ) framework USED-FOR aggregation. identity channels USED-FOR harmful heterophily. Adaptive Channel Mixing ( ACM ) framework USED-FOR diversification. Adaptive Channel Mixing ( ACM ) framework USED-FOR harmful heterophily. identity channels USED-FOR GNN layer. Adaptive Channel Mixing ( ACM ) framework USED-FOR identity channels. diversification USED-FOR GNN layer. GNN layer USED-FOR harmful heterophily. realworld node classification tasks EVALUATE-FOR ACM - augmented baselines. They COMPARE GNNs. GNNs COMPARE They. tasks EVALUATE-FOR They. tasks EVALUATE-FOR GNNs. OtherScientificTerm are Heterophily, and heterophily. Method is filterbanks. ","This paper studies the relational inductive bias (homophily assumption) of graph structures in Graph Neural Networks (GNNs). Heterophily is a well-studied problem in the literature, and GNNs have been shown to be more efficient than NNs on a variety of real-world tasks. The authors propose a new aggregation operation for GNN with a similarity matrix that can be used to learn graph structure and features. They show that the proposed Adaptive Channel Mixing (ACM) framework can improve the performance of the GNN layer by diversification and identity channels. They also show that ACM-augmented baselines outperform the state-of-the-art on several realworld node classification tasks. ","The paper proposes a novel homophily assumption for graph neural networks (GNNs) based on relational inductive bias (Heterophily). The assumption is that the graph structures of GNNs are homogeneous. The authors show that graph structures are not homogeneous, and that GNN can be seen as graph-agnostic NNs. They show that the aggregation operation is not heterophily, and they show that aggregation operation can be used to improve the generalization performance of the GNN. They also show that diversification and identity channels can improve the performance of a GNN layer in the presence of harmful heterphily. They propose Adaptive Channel Mixing (ACM) framework to improve diversification. They evaluate their proposed metrics on synthetic graphs and show that their metrics are more robust to heterophity than other homophity metrics. They demonstrate that They outperform GNN on a variety of real-world tasks, including node classification tasks. "
4197,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,generalizability FEATURE-OF deep RL approach. RL training techniques USED-FOR deep learning architecture. deep learning architecture PART-OF proposition. equivariance USED-FOR training. local search heuristics USED-FOR value landscape. local search heuristics CONJUNCTION RL training. RL training CONJUNCTION local search heuristics. RL training USED-FOR value landscape. proposition COMPARE deep RL methods. deep RL methods COMPARE proposition. random and realistic TSP problems EVALUATE-FOR deep RL methods. random and realistic TSP problems EVALUATE-FOR proposition. Method is Deep reinforcement learning ( RL ). Material is larger - sized instances. Generic is approach. Task is ablation study. ,This paper studies the generalizability of a deep RL approach with RL training techniques. The authors propose a new proposition that a deep learning architecture can be used as a deep reinforcement learning (RL) in the presence of larger-sized instances. The main idea is to use equivariance in the training to improve the performance of the training. The proposed approach is based on local search heuristics and RL training to learn the value landscape. The experimental results on random and realistic TSP problems demonstrate the effectiveness of the proposed proposition compared to other deep RL methods. ,This paper proposes a new approach to improve generalizability of deep RL approach to larger-sized instances. Deep reinforcement learning (RL) is a well-studied and well-motivated problem. The authors propose a new deep learning architecture based on RL training techniques. The main idea is to use equivariance in training to improve the generalization of the training. The proposed proposition is evaluated on random and realistic TSP problems and compared to other deep RL methods. The value landscape is learned using local search heuristics and RL training. Experiments on ablation study show the effectiveness of the proposed approach.
4213,SP:8aa471b92e2671d471107c087164378f45fb204f,"Federated learning ( FL ) HYPONYM-OF privacy - preserving collaborative learning paradigm. framework USED-FOR non - IID issue. local generative adversarial network ( GAN ) USED-FOR synthetic data. parameter server ( PS ) USED-FOR global shared synthetic dataset. confident threshold USED-FOR pseudo labeling. pseudo labeling USED-FOR PS. local private dataset CONJUNCTION labeled synthetic dataset. labeled synthetic dataset CONJUNCTION local private dataset. artificial noise USED-FOR local model gradients. local GANs USED-FOR privacy. differential privacy USED-FOR local GANs. artificial noise USED-FOR local GANs. framework COMPARE baseline methods. baseline methods COMPARE framework. supervised and semi - supervised settings FEATURE-OF benchmark datasets. supervised and semi - supervised settings EVALUATE-FOR framework. supervised and semi - supervised settings EVALUATE-FOR baseline methods. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR baseline methods. Generic is it. OtherScientificTerm are IID ( independent and identically distributed ) data, and data distributions. Material are differentially private synthetic data, and global dataset. Task is global aggregation. Method is local models. ","This paper proposes Federated learning (FL), a privacy-preserving collaborative learning paradigm. The authors propose a framework to address the non-IID issue in federated learning, where IID (independent and identically distributed) data are not available. The paper proposes a local generative adversarial network (GAN) to aggregate synthetic data from a parameter server (PS) to a global shared synthetic dataset. The local GANs are trained with differential privacy to ensure that the data distributions are differentially private. The PS is trained with a confident threshold on the pseudo labeling of the PS. The global dataset is then aggregated using the local models. The proposed framework is evaluated on several benchmark datasets in both supervised and semi-supervised settings.","This paper proposes Federated learning (FL) which is a privacy-preserving collaborative learning paradigm. The framework addresses the non-IID issue in the context of IID (independent and identically distributed) data. The paper proposes a local generative adversarial network (GAN) for generating synthetic data from a global shared synthetic dataset, where the differentially private synthetic data is generated by a parameter server (PS) and the labeled synthetic dataset is generated from a local private dataset and the global dataset is used for global aggregation. The local GANs are trained with differential privacy to preserve privacy and local model gradients are learned with artificial noise to improve the privacy. The pseudo labeling is done by using a confident threshold to prevent pseudo labeling of the local models. The proposed framework is evaluated on several benchmark datasets in both supervised and semi-supervised settings and outperforms baseline methods."
4229,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"classifier USED-FOR classifier. Gaussian noise USED-FOR classifier. robustness EVALUATE-FOR classifier. accuracy CONJUNCTION ( adversarial ) robustness. ( adversarial ) robustness CONJUNCTION accuracy. training method USED-FOR smoothed classifiers. sample - wise control of robustness USED-FOR training method. robustness CONJUNCTION prediction confidence. prediction confidence CONJUNCTION robustness. robustness EVALUATE-FOR smoothed classifiers. prediction confidence FEATURE-OF smoothed classifiers. certified robustness EVALUATE-FOR training methods. method COMPARE training methods. training methods COMPARE method. certified robustness EVALUATE-FOR method. OtherScientificTerm are ` 2 - adversarial perturbations, noise, adversarial robustness, training objective, and worst - case ( adversarial ) objective. Method is randomized smoothing. Generic is control. ","This paper studies the robustness of a classifier trained with Gaussian noise. The authors propose a new training method for smoothed classifiers with sample-wise control of robustness. The training objective is randomized smoothing, where the noise is added to the training objective and the worst-case (adversarial) objective is used to improve the performance of the classifier. They show that the proposed method achieves better robustness and prediction confidence than existing training methods. ","This paper proposes a novel training method for smoothed classifiers with Gaussian noise. The training method is based on the sample-wise control of robustness. The authors show that the robustness of the classifier is correlated with the accuracy and (adversarial) robustness, and that the adversarial robustness is independent of the noise. They also show that randomized smoothing improves robustness and prediction confidence. The paper also shows that the proposed method is more robust to adversarial perturbations than other training methods. "
4245,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"Wikipedia dataset USED-FOR pretraining BERT. histogram of sequence lengths USED-FOR packing. linear complexity EVALUATE-FOR algorithms. packing order FEATURE-OF Wikipedia dataset. model USED-FOR dataset. OtherScientificTerm are padding tokens, padding, near optimal packing, and 2x speed - up. Method is packing algorithms. Material is packed dataset. Metric is convergence. ","This paper studies the problem of pretraining BERT on the Wikipedia dataset. The authors propose to use the histogram of sequence lengths as padding tokens for the packing, and propose two packing algorithms with linear complexity. The first is near optimal packing, which is based on the 2x speed-up. The second is a variant of the second packing, where the padding tokens are added to the top of the top layers of the model. The proposed algorithms are evaluated on the packed dataset, and are shown to converge to the optimal packing order.","This paper presents a new dataset for pretraining BERT on the Wikipedia dataset. The authors propose to use the histogram of sequence lengths as padding tokens for the packing. They show that the proposed algorithms have linear complexity with respect to the number of padding tokens. They also show convergence of the algorithms in terms of the linear complexity of the packing order. The paper also shows that the packing algorithms converge to near optimal packing, which is a result of 2x speed-up. The proposed model is evaluated on the proposed dataset, and the proposed algorithm is shown to outperform other packing algorithms on the packed dataset."
4261,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm USED-FOR high - scoring outputs. adaptive tree search algorithm HYPONYM-OF Monte Carlo tree search. translation models USED-FOR high - scoring outputs. algorithm USED-FOR models. autoregressivity CONJUNCTION conditional independence assumptions. conditional independence assumptions CONJUNCTION autoregressivity. algorithm COMPARE beam search. beam search COMPARE algorithm. decoding bias USED-FOR autoregressive models. algorithm USED-FOR autoregressive models. adaptive tree search algorithm COMPARE beam search. beam search COMPARE adaptive tree search algorithm. reranking techniques USED-FOR models. beam search USED-FOR autoregressive models. adaptive tree search algorithm COMPARE reranking techniques. reranking techniques COMPARE adaptive tree search algorithm. model scores EVALUATE-FOR adaptive tree search algorithm. BLEU EVALUATE-FOR translation model objectives. noisy channel model CONJUNCTION objective. objective CONJUNCTION noisy channel model. autoregressive models CONJUNCTION noisy channel model. noisy channel model CONJUNCTION autoregressive models. expected automatic metric scores CONJUNCTION noisy channel model. noisy channel model CONJUNCTION expected automatic metric scores. autoregressive models USED-FOR expected automatic metric scores. decoder USED-FOR search. objective HYPONYM-OF models. autoregressive models HYPONYM-OF models. beam search bias USED-FOR models. noisy channel model HYPONYM-OF models. search USED-FOR models. beam search CONJUNCTION reranking based methods. reranking based methods CONJUNCTION beam search. OtherScientificTerm is search objective. Task is decoding. Generic is objectives. ,"This paper proposes an adaptive tree search algorithm for high-scoring outputs from translation models. The algorithm is based on Monte Carlo tree search, where the goal is to find models with the best autoregressivity and conditional independence assumptions. The authors show that the proposed algorithm outperforms beam search and other reranking techniques for autoregressive models with beam search. The main contribution of the paper is to provide a search objective that can be applied to different models (e.g. noisy channel model, autoregression models, and noisy channel models). The authors also provide a decoding bias that is used to evaluate the performance of the proposed algorithms. Finally, the authors provide a BLEU for the translation model objectives and show that their algorithm can achieve better model scores than beam search, reranking based methods, and other models. ","This paper proposes an adaptive tree search algorithm for high-scoring outputs of translation models. The algorithm is based on Monte Carlo tree search, where the search objective is a linear combination of autoregressivity and conditional independence assumptions. The authors show that the proposed algorithm outperforms beam search and reranking techniques for models trained with beam search with a beam search bias on autoregressive models with a decoding bias. The paper also shows that the search can be done with a decoder, and that the objective and the noisy channel model can be combined to obtain the expected automatic metric scores of the models, which are then used to improve the model scores.  The paper is well-written and the objectives are well-motivated, and the paper is easy to follow. The translation model objectives are easy to understand, and BLEU can be used to evaluate the effectiveness of the search. The search is done with the decoder and the models are trained using beam search. "
4277,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"normal distribution FEATURE-OF task. Energy Based Model ( EBM ) USED-FOR intractability of abnormal distribution. iterative optimization procedure USED-FOR Langevin Dynamics ( LD ). Langevin Dynamics ( LD ) USED-FOR EBM. iterative optimization procedure USED-FOR EBM. anomaly detector USED-FOR task. adaptive sparse coding layer USED-FOR anomaly detector. OtherScientificTerm are anomaly, normal population, plug and play feature, and sparse coding layer. Method are AI solutions, EBMs, and meta learning scheme. ","This paper studies the intractability of abnormal distribution in the Energy Based Model (EBM) under the normal distribution of a task with normal distribution. The authors propose an iterative optimization procedure for Langevin Dynamics (LD) in the EBM, where the anomaly is defined as the difference between the normal population and the plug and play feature. The anomaly detector is trained using an adaptive sparse coding layer, and the authors show that the anomaly detector performs well on the task. The paper also shows that the proposed AI solutions are more efficient than existing EBMs, and that the meta learning scheme is more efficient.","This paper proposes an Energy Based Model (EBM) for the intractability of abnormal distribution in a task with normal distribution. The authors propose an iterative optimization procedure for Langevin Dynamics (LD) for EBM, where the anomaly is defined as the normal population of the plug and play feature. The paper also proposes a meta learning scheme where an adaptive sparse coding layer is added to the anomaly detector for the task. The experiments show that the proposed AI solutions outperform existing EBMs."
4293,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,propaganda CONJUNCTION news. news CONJUNCTION propaganda. news CONJUNCTION social media. social media CONJUNCTION news. QA systems USED-FOR misinformation. misinformation FEATURE-OF QA models. large - scale dataset USED-FOR problem. CONTRAQA HYPONYM-OF large - scale dataset. contradicting contexts USED-FOR QA models. question answering CONJUNCTION misinformation detection. misinformation detection CONJUNCTION question answering. counter - measure USED-FOR misinformation - aware QA system. misinformation detection PART-OF counter - measure. question answering PART-OF counter - measure. misinformation detection PART-OF misinformation - aware QA system. Method is QA model. OtherScientificTerm is real and fake information. ,"This paper studies the problem of learning a QA model that can distinguish between real and fake information. The authors propose a large-scale dataset, CONTRAQA, which is a combination of QA systems that can be used to detect misinformation in QA models in contradicting contexts. The paper also proposes a counter-measure for the misinformation-aware QA system, which combines question answering and misinformation detection. ","This paper proposes a new QA model that is able to distinguish between real and fake information. The authors propose to use QA systems to detect misinformation in the context of news and social media. The problem is formulated as a large-scale dataset, CONTRAQA, where the authors show that the QA models can detect the misinformation in contradicting contexts. The proposed counter-measure is a combination of question answering and misinformation detection."
4309,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"embodiment CONJUNCTION morphology. morphology CONJUNCTION embodiment. expert demonstrations USED-FOR imitation agent. embodiment USED-FOR imitation agent. morphology USED-FOR imitation agent. method USED-FOR cross - domain imitation. GromovWasserstein distance USED-FOR method. GWIL USED-FOR optimality. rigid transformation of the expert domain CONJUNCTION arbitrary transformation of the state - action space. arbitrary transformation of the state - action space CONJUNCTION rigid transformation of the expert domain. GWIL USED-FOR continuous control domains. Task is Cross - domain imitation learning. OtherScientificTerm is stationary distributions. Generic are they, and theory. Method is Gromov - Wasserstein Imitation Learning ( GWIL ). ","This paper studies the problem of cross-domain imitation learning, where the goal is to learn an imitation agent from expert demonstrations. The authors propose a new method called Gromov-Wasserstein Imitation Learning (GWIL), which is based on the idea that the imitation agent can be represented as a combination of an Embodiment and a morphology. The key idea is to combine the two, so that they can be used to train a cross-domains imitation agent. The proposed method is based upon the idea of Gromovsky Wasserstein distance between the expert domain and the state-action space. Theoretically, the authors show that GWIL is able to achieve better optimality in continuous control domains with stationary distributions. Empirical results demonstrate the effectiveness of the proposed theory.",This paper proposes a method for cross-domain imitation learning. The method is based on Gromov-Wasserstein Imitation Learning (GWIL). The authors show that GWIL can achieve high optimality in continuous control domains with stationary distributions. They also show that the imitation agent can be learned from expert demonstrations. The authors also provide a theoretical analysis of the method.
4325,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"self - supervised learning ( SSL ) USED-FOR computer vision. labeling cost FEATURE-OF computer vision. labeling cost EVALUATE-FOR self - supervised learning ( SSL ). SSL USED-FOR invariant visual representations. contrastive loss EVALUATE-FOR representation invariant. hidden layer PART-OF projection head. hierarchical projection head USED-FOR raw representations of the backbone. hierarchical projection head USED-FOR HCCL. cross - level contrastive learning USED-FOR HCCL. generalization ability EVALUATE-FOR visual representations. generalization ability EVALUATE-FOR HCCL. HCCL USED-FOR SSL frameworks. detection CONJUNCTION segmentation. segmentation CONJUNCTION detection. classification CONJUNCTION detection. detection CONJUNCTION classification. segmentation CONJUNCTION few - shot learning tasks. few - shot learning tasks CONJUNCTION segmentation. HCCL USED-FOR detection. HCCL USED-FOR segmentation. few - shot learning tasks EVALUATE-FOR HCCL. classification EVALUATE-FOR HCCL. HCCL COMPARE methods. methods COMPARE HCCL. benchmark datasets EVALUATE-FOR HCCL. benchmark datasets EVALUATE-FOR methods. Method are SSL methods, and Hierarchical Cross Contrastive Learning(HCCL ). Generic is approach. OtherScientificTerm are latent spaces, and latent features. ","This paper studies the labeling cost of self-supervised learning (SSL) in computer vision. The authors propose a new approach called Hierarchical Cross Contrastive Learning(HCCL), which is based on cross-level contrastive learning. HCCL learns invariant visual representations from a hidden layer in the projection head. The representation invariant is obtained by contrastive loss. The raw representations of the backbone are learned by a hierarchical projection head, and the latent spaces are mapped to the latent features.  The authors demonstrate the effectiveness of the proposed method on a variety of benchmark datasets, and demonstrate its generalization ability to different visual representations. The proposed HccL can also be applied to other SSL frameworks, and is shown to perform well on detection, segmentation, and classification.","This paper studies the labeling cost of self-supervised learning (SSL) for computer vision. The authors propose a new approach called Hierarchical Cross Contrastive Learning(HCCL) to reduce the label cost of SSL methods. HCCL uses cross-level contrastive learning to learn invariant visual representations of the latent spaces. The representation invariant is learned by contrastive loss, where the hidden layer of the projection head is replaced by a hierarchical projection head that maps the raw representations of a backbone to the latent features of the backbone. The proposed approach is evaluated on several benchmark datasets and shows improved generalization ability for visual representations compared to other SSL frameworks. The results are also shown for detection, segmentation, and few-shot learning tasks. "
4341,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"Real economies HYPONYM-OF sequential imperfect - information game. heterogeneous, interacting strategic agents PART-OF sequential imperfect - information game. Dynamic general equilibrium models USED-FOR economic activity. Dynamic general equilibrium models USED-FOR interactions. economic activity CONJUNCTION interactions. interactions CONJUNCTION economic activity. Dynamic general equilibrium models USED-FOR systems. analytical and computational methods USED-FOR explicit equilibria. structured learning curricula CONJUNCTION GPU - only simulation and training. GPU - only simulation and training CONJUNCTION structured learning curricula. market clearing HYPONYM-OF unrealistic assumptions. GPU implementation USED-FOR training and analyzing economies. real - business - cycle models HYPONYM-OF DGE models. approach USED-FOR real - business - cycle models. RL policies CONJUNCTION economic intuitions. economic intuitions CONJUNCTION RL policies. meta - game -Nash equilibria PART-OF open RBC models. approximate best - response analyses USED-FOR meta - game -Nash equilibria. Method are joint learning, and meta - game. OtherScientificTerm are reward function, consumer ’s expendable income, -Nash equilibria, analytical tractability, and worker - consumers. Task is economic simulations. ","This paper studies the problem of joint learning in a sequential imperfect-information game, Real economies, where heterogeneous, interacting strategic agents are interacting in a meta-game. Dynamic general equilibrium models are used to model the interactions between agents and the economic activity, and the reward function. The authors show that the explicit equilibria obtained by analytical and computational methods can be used to train systems that are more interpretable than existing DGE models, such as real-business-cycle models. The paper also shows that the unrealistic assumptions of market clearing and GPU implementation in training and analyzing economies can be alleviated by a simple GPU implementation.  The authors also show that this approach can improve the performance of real-bicycle models and RL policies and economic intuitions. ","This paper proposes a sequential imperfect-information game, Real economies, which is a sequential imitation learning game with heterogeneous, interacting strategic agents. The authors propose to use Dynamic general equilibrium models to model the interactions between two systems, and use analytical and computational methods to learn explicit equilibria between the two systems. The meta-game is an extension of joint learning, where the goal is to learn a reward function that maximizes the consumer’s expendable income. The paper shows that the proposed approach outperforms state-of-the-art DGE models and real-business-cycle models in both structured learning curricula and GPU-only simulation and training.  The authors also show that the unrealistic assumptions in the paper (e.g., market clearing and market clearing) can be alleviated by using GPU implementation for training and analyzing economies.   The paper also shows that meta-Game-Nash Equilibria can be learned in open RBC models with approximate best-response analyses. "
4357,SP:f885c992df9c685f806a653398736432ba38bd80,public API USED-FOR machine learning model. robustness CONJUNCTION model utility. model utility CONJUNCTION robustness. defenses USED-FOR model stealing. query access USED-FOR model extraction. differential privacy USED-FOR calibration. victim model USED-FOR method. Task is model extraction attacks. Generic is model. Metric is computational effort. OtherScientificTerm is proof - of - work. Method is machine learning practitioners. ,"This paper studies the problem of model extraction attacks against a public API for a machine learning model. The authors propose a method based on differential privacy to protect the model against model stealing attacks. The proposed method is based on a victim model and uses query access to perform model extraction. The paper shows that the proposed method can achieve state-of-the-art performance in terms of robustness, model utility, and computational effort. ",This paper proposes a public API for training a machine learning model. The idea is to train a model that is robust to model extraction attacks. The method is based on the proof-of-work. The authors propose two defenses to prevent model stealing. The first is to use query access for model extraction. The second is to apply differential privacy for calibration. The proposed method is evaluated on a victim model. 
4373,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"Neural Ordinary Differential Equations ( ODEs ) USED-FOR generative models of images. Continuous Normalizing Flows ( CNFs ) USED-FOR generative models of images. models USED-FOR exact likelihood calculation. exact likelihood calculation CONJUNCTION invertible generation / density estimation. invertible generation / density estimation CONJUNCTION exact likelihood calculation. models USED-FOR invertible generation / density estimation. approach COMPARE prior methods. prior methods COMPARE approach. likelihood values FEATURE-OF image datasets. likelihood values EVALUATE-FOR approach. GPU USED-FOR prior methods. image datasets EVALUATE-FOR approach. training time EVALUATE-FOR prior methods. Method is MRCNF ). OtherScientificTerm are conditional distribution, fine image, coarse image, and log likelihood. ",This paper proposes a new method for learning neural ODEs from continuous normalizing flows (CNFs). The proposed method is based on the MRCNF. The main idea is to learn a conditional distribution over the CNFs and then use this conditional distribution to estimate the log likelihood of a fine image. The authors show that the proposed method outperforms the state-of-the-art in terms of training time.,This paper introduces Neural Ordinary Differential Equations (ODEs) for generating generative models of images. Continuous Normalizing Flows (CNFs) are used to generate images with conditional distribution. The authors propose to use MRCNF (MRCNF) to generate a fine image and a coarse image with log likelihood. The proposed approach outperforms prior methods in terms of training time and likelihood values on several image datasets. They also show that the proposed models can be used for exact likelihood calculation and invertible generation/density estimation. 
4389,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"Label noise PART-OF real - world datasets. robust training techniques USED-FOR DNNs. DNNs USED-FOR corrupted patterns. overfitting USED-FOR corrupted patterns. noisy supervisions USED-FOR model. and training - free solution USED-FOR detect noisy labels. neighborhood information USED-FOR methods. nearby representations USED-FOR local voting. noisy label consensuses USED-FOR local voting. one HYPONYM-OF methods. local voting USED-FOR one. ranking - based approach USED-FOR one. representations USED-FOR ranking - based approach. worst - case error bound EVALUATE-FOR ranking - based method. training - free solutions COMPARE training - based baselines. training - based baselines COMPARE training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - based baselines. Method is generalization of deep neural networks ( DNNs ). Generic is approach. OtherScientificTerm are noisy labels, and clean label. ","This paper proposes a new approach for generalization of deep neural networks (DNNs) by using noisy labels in the training data. The authors propose to use robust training techniques to train DNNs to detect corrupted patterns by overfitting the corrupted patterns. The model is trained with noisy supervisions from noisy labels, and a training-free solution is proposed to detect noisy labels. The proposed methods are based on neighborhood information, where the noisy label consensuses are used for local voting based on nearby representations. The results show that the proposed ranking-based approach achieves the best worst-case error bound on both synthetic and real-world label noise compared to training-based baselines.","This paper proposes a new approach to improve generalization of deep neural networks (DNNs) in the presence of label noise in real-world datasets. The main idea is to use robust training techniques to detect corrupted patterns in DNNs due to overfitting. The model is trained with noisy supervisions, and the training-free solution to detect noisy labels is based on neighborhood information. Two methods are proposed, one based on local voting based on nearby representations and the other based on noisy label consensuses. The ranking-based approach is shown to achieve the best-case error bound with respect to the worst-case prediction of the model. The paper also shows that training-based baselines with synthetic and real-World label noise are more robust to noisy labels."
4405,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"robustness EVALUATE-FOR RL agents. state observations USED-FOR strongest / optimal adversarial perturbations. strongest / optimal adversarial perturbations USED-FOR reinforcement learning ( RL ) agent. Existing works USED-FOR adversarial RL. heuristics - based methods USED-FOR Existing works. attacking method USED-FOR optimal attacks. designed function CONJUNCTION RL - based learner. RL - based learner CONJUNCTION designed function. algorithm COMPARE RL - based works. RL - based works COMPARE algorithm. PA - AD COMPARE RL - based works. RL - based works COMPARE PA - AD. PA - AD HYPONYM-OF algorithm. PA - AD COMPARE attacking methods. attacking methods COMPARE PA - AD. attacking methods USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR adversarial training. empirical robustness EVALUATE-FOR PA - AD. OtherScientificTerm are optimal adversary, optimal attack, agent, large state space, policy perturbation direction, policy perturbation directions, large state spaces, and strong adversaries. Method is RL - based adversary. ","This paper studies the robustness of reinforcement learning (RL) agent against the strongest/optimal adversarial perturbations based on state observations. Existing works for adversarial RL are based on heuristics-based methods. The authors propose a new algorithm PA-AD, which combines a designed function and an RL-based learner to learn the optimal attack on the agent. The proposed algorithm is evaluated on Atari and MuJoCo environments, and shows superior empirical robustness compared to existing attacking methods. ","This paper studies the robustness of reinforcement learning (RL) agent against the strongest/optimal adversarial perturbations based on state observations. Existing works for adversarial RL are based on heuristics-based methods. The authors propose a new algorithm PA-AD, which combines a designed function and an RL-based learner to learn optimal attacks on the optimal adversary. The algorithm is evaluated on Atari and MuJoCo environments and shows better empirical robustness compared to RL -based works. "
4421,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"cumulative reward FEATURE-OF monotonous policies. diversity CONJUNCTION novelty. novelty CONJUNCTION diversity. novelty FEATURE-OF policies. diversity FEATURE-OF policies. policy generation workflow USED-FOR diverse and well - performing policies. novelty metric USED-FOR novelty - seeking problem. behavioral novelty FEATURE-OF multi - objective optimization approaches. constrained optimization literature FEATURE-OF interior point method. interior point method USED-FOR Interior Policy Differentiation ( IPD ). Interior Policy Differentiation ( IPD ) HYPONYM-OF policy seeking algorithm. constrained optimization USED-FOR novelty - seeking problem. IPD COMPARE novelty - seeking methods. novelty - seeking methods COMPARE IPD. benchmark environments EVALUATE-FOR IPD. benchmark environments EVALUATE-FOR novelty - seeking methods. Task is problem - solving. Generic are problem, and metric. Method are reinforcement learning algorithms, and learning algorithms. OtherScientificTerm is novelty of generated policies. ","This paper studies the problem of problem-solving with monotonous policies with cumulative reward. The authors propose a novel novelty metric for the novelty-seeking problem. The novelty of generated policies is defined as the difference between the diversity and novelty of policies generated by a policy generation workflow. The problem is formulated as a multi-objective optimization approaches with behavioral novelty, where the goal is to find a policy that maximizes the cumulative reward of a set of policies with high diversity and low novelty. The paper proposes an interior point method called Interior Policy Differentiation (IPD) based on the constrained optimization literature. IPD is evaluated on a variety of benchmark environments and shows that IPD performs better than other novelty - seeking methods. ",This paper proposes a novel metric for problem-solving. The novelty of generated policies is defined as the cumulative reward of monotonous policies. The authors propose a policy generation workflow to generate diverse and well-performing policies. They also propose an interior point method based on the constrained optimization literature to solve the novelty-seeking problem. Experiments are conducted on several benchmark environments to show that the proposed IPD outperforms other novel-seeking methods in terms of behavioral novelty. 
4437,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"accuracy EVALUATE-FOR automatic speech recognition. quality of speech USED-FOR human perception. Reverberation FEATURE-OF audio reflecting off surfaces. audio modality USED-FOR reverberation. reverberation effects FEATURE-OF audio stream. real - world 3D scans of homes FEATURE-OF realistic acoustic renderings of speech. large - scale dataset EVALUATE-FOR task. realistic acoustic renderings of speech USED-FOR large - scale dataset. speech enhancement CONJUNCTION speech recognition. speech recognition CONJUNCTION speech enhancement. speech recognition CONJUNCTION speaker identification. speaker identification CONJUNCTION speech recognition. it COMPARE audio - only methods. audio - only methods COMPARE it. simulated and real imagery USED-FOR speech enhancement. approach USED-FOR speech enhancement. approach USED-FOR speech recognition. simulated and real imagery USED-FOR approach. OtherScientificTerm are audio - visual observations, visual environment, room geometry, speaker location, visual scene, and room acoustics. Method is end - to - end approach. ","This paper studies the accuracy of automatic speech recognition with respect to the quality of speech in human perception. The authors propose an end-to-end approach where the audio modality of the reverberation of the audio reflecting off surfaces is added to the audio stream, and the audio-visual observations are used to improve the performance. The task is a large-scale dataset with realistic acoustic renderings of speech from real-world 3D scans of homes. The audio-vacuous observations are generated from a visual environment, where the speaker location is represented as a visual scene and the room acoustics are modeled as a room geometry. The proposed approach is evaluated on both simulated and real imagery for speech enhancement, speech recognition, and speaker identification, and it outperforms audio-only methods.","This paper studies the accuracy of automatic speech recognition with respect to the quality of speech for human perception. The authors propose an end-to-end approach, where the audio modality is modeled as a reverberation of the audio reflecting off surfaces, and the audio-visual observations are sampled from the visual environment. The task is performed on a large-scale dataset of realistic acoustic renderings of speech from real-world 3D scans of homes, which are generated from a visual environment with room geometry, speaker location, and room acoustics. The proposed approach is evaluated on speech enhancement, speech recognition, and speaker identification, and it outperforms audio-only methods on both simulated and real imagery."
4453,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"model USED-FOR extrapolation. methods USED-FOR extrapolation. position representation method USED-FOR extrapolation. Attention with Linear Biases ( ALiBi ) HYPONYM-OF position method. it USED-FOR query - key attention scores. positional embeddings USED-FOR word embeddings. perplexity EVALUATE-FOR sinusoidal position embedding model. method COMPARE sinusoidal position embedding model. sinusoidal position embedding model COMPARE method. perplexity EVALUATE-FOR method. it COMPARE position methods. position methods COMPARE it. WikiText-103 benchmark EVALUATE-FOR it. WikiText-103 benchmark EVALUATE-FOR position methods. Method are transformer model, and ALiBi. OtherScientificTerm are memory, and recency. ","This paper proposes a new position representation method for extrapolation. Attention with Linear Biases (ALiBi) is a position method that uses positional embeddings instead of word embeds in the transformer model. In particular, it uses query-key attention scores instead of recency. The authors show that ALiBi achieves better perplexity than a sinusoidal position embedding model and outperforms other position methods on the WikiText-103 benchmark.",This paper proposes a position representation method for extrapolation. Attention with Linear Biases (ALiBi) is a position method based on the transformer model. The authors show that ALiBi improves the performance of query-key attention scores. They also show that the proposed method improves the perplexity of the sinusoidal position embedding model compared to other position methods on the WikiText-103 benchmark. 
4469,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,unconstrained max - min form FEATURE-OF multi - objective dynamic regret. multi - objective dynamic regret PART-OF Multi - Objective Online Convex Optimization. regret USED-FOR zero - order multi - objective bandit setting. it COMPARE regret. regret COMPARE it. vanilla min - norm solver CONJUNCTION L1 - regularized min - norm solver. L1 - regularized min - norm solver CONJUNCTION vanilla min - norm solver. variants USED-FOR composite gradient. Online Mirror Multiple Descent algorithm USED-FOR composite gradient. variants USED-FOR Online Mirror Multiple Descent algorithm. L1 - regularized min - norm solver USED-FOR composite gradient. vanilla min - norm solver USED-FOR composite gradient. regret bounds FEATURE-OF variants. lower bound FEATURE-OF L1 - regularized variant. Task is multi - objective online learning. Method is first - order gradient - based methods. Generic is algorithm. ,"This paper studies the multi-objective online learning problem. The authors propose a new algorithm, Multi-Objective Online Convex Optimization (MOCO), which is based on the Online Mirror Multiple Descent algorithm. The main idea is to learn a composite gradient using a vanilla min-norm solver and an L1-regularized min-normalized version of the L1 regularized variant. The proposed algorithm is shown to have a better regret than existing first-order gradient-based methods. ","This paper studies multi-objective online learning, where the goal is to maximize the regret of an agent in a zero-order multi-observation bandit setting. The authors propose Multi-Objective Online Convex Optimization (MOOCO), which is a variant of first-order gradient-based methods. The main idea is to use the unconstrained max-min form of the multi- objective dynamic regret in Multi-OOCO. The proposed algorithm is evaluated on the Online Mirror Multiple Descent algorithm for the composite gradient of a vanilla min-norm solver and an L1-regularized min-Norm solver. The regret bounds of the variants are shown to be better than the lower bound of the L1 -regularized variant. "
4485,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"intelligence USED-FOR real - world problems. Learning continually PART-OF intelligence. simple scenarios CONJUNCTION low - dimensional benchmarks. low - dimensional benchmarks CONJUNCTION simple scenarios. generative models USED-FOR replay patterns. simplified assumptions USED-FOR it. generative models USED-FOR Generative replay. simple scenarios HYPONYM-OF simplified assumptions. low - dimensional benchmarks HYPONYM-OF simplified assumptions. OtherScientificTerm are catastrophic forgetting, and learning experiences. Method are continual learning, replay approach, and generative replay approaches. Metric is classification accuracy. Generic are they, and approach. Material are ImageNet-1000, and high - dimensional data. ","This paper studies the problem of continual learning in the context of real-world problems. The authors propose a replay approach that uses generative models to learn replay patterns, and it is based on simplified assumptions such as simple scenarios and low-dimensional benchmarks. They show that they can achieve better classification accuracy than existing generative replay approaches. They also show that their approach can be applied to ImageNet-1000, which is a large dataset of high-dimensional data. ",This paper proposes a continual learning approach to improve the classification accuracy of deep learning models. The main idea is to learn a model that can be used to predict the future state of the world. This is done by learning to predict future states from the current state. The authors show that the proposed approach is able to improve classification accuracy on ImageNet-1000. 
4501,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"modularity maximization CONJUNCTION NCut minimization. NCut minimization CONJUNCTION modularity maximization. graph partitioning ( GP ) FEATURE-OF NP - hard combinatorial optimization problems. NP - hard combinatorial optimization problems USED-FOR network systems. NCut minimization HYPONYM-OF NP - hard combinatorial optimization problems. NCut minimization HYPONYM-OF graph partitioning ( GP ). modularity maximization HYPONYM-OF NP - hard combinatorial optimization problems. modularity maximization HYPONYM-OF graph partitioning ( GP ). machine learning techniques USED-FOR Existing methods. heuristic strategies USED-FOR GP methods. inductive graph partitioning ( IGP ) framework USED-FOR NP - hard challenge. transductive GP methods COMPARE inductive graph partitioning ( IGP ) framework. inductive graph partitioning ( IGP ) framework COMPARE transductive GP methods. inductive graph partitioning ( IGP ) framework USED-FOR graphs. dual graph neural network USED-FOR IGP. historical graph snapshots USED-FOR dual graph neural network. model USED-FOR graphs. model USED-FOR online GP. quality CONJUNCTION efficiency. efficiency CONJUNCTION quality. graphs USED-FOR online GP. IGP USED-FOR online GP. IGP HYPONYM-OF framework. graphs USED-FOR online GP. benchmarks EVALUATE-FOR IGP. efficiency EVALUATE-FOR state - of - the - art baselines. IGP COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE IGP. benchmarks EVALUATE-FOR state - of - the - art baselines. efficiency EVALUATE-FOR IGP. OtherScientificTerm are NP - hardness, quality degradation, and optimization. Method is GP. Metric is complexity. Generic is system. ","This paper studies the problem of graph partitioning (GP) in NP-hard combinatorial optimization problems such as NCut minimization and modularity maximization. Existing methods are based on machine learning techniques. The authors propose an inductive graph partitioned (IGP) framework to solve the NP - hard challenge. IGP uses a dual graph neural network to learn graphs from historical graph snapshots. The model is then used to partition the graphs into a set of smaller graphs, which are then partitioned into a larger set of larger graphs.  The authors show that IGP outperforms the state-of-the-art baselines in terms of both quality and efficiency. ","This paper studies the problem of graph partitioning (GP) in NP-hard combinatorial optimization problems, where the goal is to reduce the complexity of the network systems. The authors propose a novel framework, called IGP, to solve the NP-Hard challenge. The main idea is to use a dual graph neural network with historical graph snapshots as input and a model to partition the graph into graphs. The model is trained on a set of graphs, which are then used as input to the GP. The proposed method is evaluated on a number of benchmarks, and compared to existing methods. "
4517,SP:ad28c185efd966eea1f44a6ff474900812b4705a,Multiresolution Equivariant Graph Variational Autoencoders ( MGVAE ) HYPONYM-OF hierarchical generative model. hierarchical generative model USED-FOR graphs. multiresolution and equivariant manner USED-FOR graphs. higher order message passing USED-FOR graph. MGVAE USED-FOR graph. MGVAE USED-FOR resolution level. higher order message passing USED-FOR resolution level. higher order message passing USED-FOR MGVAE. MGVAE USED-FOR hierarchical generative model. hierarchical generative model USED-FOR hierarchy of coarsened graphs. node ordering FEATURE-OF framework. general graph generation CONJUNCTION molecular generation. molecular generation CONJUNCTION general graph generation. unsupervised molecular representation learning USED-FOR molecular properties. molecular generation CONJUNCTION unsupervised molecular representation learning. unsupervised molecular representation learning CONJUNCTION molecular generation. link prediction CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION link prediction. MGVAE COMPARE generative tasks. generative tasks COMPARE MGVAE. unsupervised molecular representation learning CONJUNCTION link prediction. link prediction CONJUNCTION unsupervised molecular representation learning. citation graphs USED-FOR link prediction. unsupervised molecular representation learning HYPONYM-OF generative tasks. general graph generation HYPONYM-OF generative tasks. graph - based image generation HYPONYM-OF generative tasks. link prediction HYPONYM-OF generative tasks. molecular generation HYPONYM-OF generative tasks. Generic is it. OtherScientificTerm is hierarchy of latent distributions. ,"This paper proposes a hierarchical generative model for learning graphs in a multiresolution and equivariant manner. The framework is based on the Multiresolution Equivariant Graph Variational Autoencoders (MGVAE) which is an extension of MGVAE to graph. The authors show that the proposed framework is able to learn a hierarchy of coarsened graphs with node ordering, and that it can be used to learn the hierarchy of latent distributions. The proposed framework can be combined with higher order message passing to improve the resolution level of the graph, and can also be used for unsupervised molecular representation learning to learn molecular properties. The experimental results show that MGVAEs perform better than other generative tasks, such as molecular generation, link prediction, and graph-based image generation. ","This paper proposes Multiresolution Equivariant Graph Variational Autoencoders (MGVAE) which is a hierarchical generative model for graphs in a multiresolution and equivariant manner. MGVAE learns a graph by higher order message passing between nodes in a hierarchy of latent distributions. The framework is based on node ordering, and it can be applied to any graph. The proposed framework is evaluated on several generative tasks, including general graph generation, molecular generation, unsupervised molecular representation learning, link prediction, and graph-based image generation. "
4533,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"Nonlinear ICA HYPONYM-OF machine learning. framework USED-FOR nonlinear ICA. volume - preserving transformation FEATURE-OF mixing function. artificial data CONJUNCTION synthesized images. synthesized images CONJUNCTION artificial data. synthesized images EVALUATE-FOR theory. volume - preserving flow - based models USED-FOR framework. artificial data EVALUATE-FOR theory. framework USED-FOR interpretable features. real - world images EVALUATE-FOR framework. OtherScientificTerm are independent components ( sources ), and temporal structure. Generic are sources, and methods. ","This paper proposes a new framework for nonlinear ICA, i.e., a nonlinear model with independent components (source and target). The authors propose a mixing function based on volume-preserving transformation of the mixing function with respect to the temporal structure of the source and target. The authors show that the proposed framework is able to learn interpretable features from both artificial data and synthesized images. They also show that their framework can be applied to real-world images.","This paper proposes a new framework for nonlinear ICA, which is an important problem in machine learning. The framework is based on the volume-preserving transformation of the mixing function of the independent components (source and target) of a mixing function, where the source and target are independent of each other. The authors show that the proposed framework is able to capture interpretable features in both artificial data and synthesized images. The paper also shows that the framework can be applied to real-world images. "
4549,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"Graph convolutional networks USED-FOR representation learning of networked data. single message passing strategy USED-FOR they. sharing scheme USED-FOR filters. sharing scheme USED-FOR spectral graph convolutional operators. sharing scheme USED-FOR filters. BankGCN HYPONYM-OF graph convolution operator. sharing scheme USED-FOR spectral methods. BankGCN USED-FOR multi - channel signals. adaptive filters USED-FOR BankGCN. graphs USED-FOR multi - channel signals. filters PART-OF filter bank. filters PART-OF subspaces. frequency response FEATURE-OF filters. filter bank CONJUNCTION signal decomposition. signal decomposition CONJUNCTION filter bank. filter bank USED-FOR spectral characteristics. signal decomposition USED-FOR spectral characteristics. spectral characteristics FEATURE-OF graph data. benchmark graph datasets EVALUATE-FOR BankGCN. Method are message passing graph convolutional networks ( MPGCNs ), and MPGCNs. OtherScientificTerm are low - frequency information, graph features, and single ‘ low - pass ’ features. Task is overfitting problems. Generic is compact architecture. ","This paper studies the problem of representation learning of graph convolutional networks in the context of overfitting problems. The authors propose a single message passing strategy to learn the low-frequency information of the graph features, and propose a sharing scheme to share the filters between the filters in the filter bank and the subspaces. They show that the sharing scheme can improve the performance of spectral methods such as BankGCN and MPGCNs. They also show that adaptive filters can be used in the Bank GCN to learn multi-channel signals from the graphs. Finally, the authors demonstrate that the proposed compact architecture can achieve state-of-the-art performance on benchmark graph datasets.","This paper proposes a new graph convolutional networks for representation learning of networked data. The authors propose to use a single message passing strategy to learn the low-frequency information in the graph features, and then they use a sharing scheme to train the filters of the spectral graphs. The proposed BankGCN is an extension of MPGCNs, and the authors show that they can learn multi-channel signals from graphs with adaptive filters. They also show that the filters in the filter bank and the subspaces of the filters have a similar frequency response. The paper also provides a compact architecture that can be used for overfitting problems. Finally, the authors evaluate the proposed bankGCN on several benchmark graph datasets."
4565,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"pretrain - finetune paradigm USED-FOR deep learning. model USED-FOR downstream tasks. self - supervised pre - training COMPARE supervised pre - training. supervised pre - training COMPARE self - supervised pre - training. transferability EVALUATE-FOR self - supervised pre - training. supervised methods USED-FOR pre - training stage. supervised pretraining model USED-FOR downstream tasks. transferability EVALUATE-FOR supervised pre - training methods. It USED-FOR overfitting upstream tasks. method USED-FOR large datasets. state - of - the - art methods USED-FOR supervised and self - supervised pre - training. LOOK COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LOOK. LOOK USED-FOR supervised and self - supervised pre - training. multiple downstream tasks EVALUATE-FOR LOOK. Material is ImageNet. OtherScientificTerm are negligence of valuable intra - class semantic difference, visual contents, multi - mode distribution, and intra - class difference. Generic is methods. Task is overfit of upstream tasks. Method is supervised pre - training method. ",This paper proposes a pretrain-finetune paradigm for deep learning. The pretraining stage is a pre-training stage where a model is trained on a set of downstream tasks and a supervised pretraining model is used to predict the performance of the downstream tasks. The goal is to reduce the overfit of upstream tasks by minimizing the negligence of valuable intra-class semantic difference between the visual contents and the input images. The authors show that the proposed method performs better than state-of-the-art methods on large datasets. The transferability of the proposed supervised pre-train methods is also shown to be better than self-supervised pret-training. ,"This paper proposes a pretrain-finetune paradigm for deep learning. The pretraining model is used to train a model for downstream tasks, where the goal is to minimize the overfit of upstream tasks. The authors propose a supervised pre-training method, called Look, which is based on ImageNet. The proposed method can be applied to large datasets with multiple visual contents. It is shown to reduce overfitting upstream tasks by minimizing the intra-class semantic difference between the visual contents and the multi-mode distribution. Compared to other methods, the authors show that Look improves transferability over self-supervised pret-training, and outperforms state-of-the-art methods on both supervised and self -supervised pre-train. "
4581,SP:2b3916ba24094c286117126e11032820f8c7c50a,"wrinkling of cheeks CONJUNCTION formation of dimples. formation of dimples CONJUNCTION wrinkling of cheeks. Morphable Models ( 3DMMs ) USED-FOR fine details. PCA - based representations USED-FOR fine details. FaceDet3D HYPONYM-OF method. single image geometric facial details USED-FOR method. vertex displacement map USED-FOR facial details. method USED-FOR facial geometric details. FDS USED-FOR detailed geometry. hallucinated details PART-OF smooth proxy geometry. facial details FEATURE-OF detailed geometry. Neural Rendering USED-FOR detailed geometry. zoom - in FEATURE-OF predicted facial details. Predicted Facial Detail CONJUNCTION Render of Predicted Facial Detail. Render of Predicted Facial Detail CONJUNCTION Predicted Facial Detail. OtherScientificTerm are Facial Expressions, 3D face geometry, smile, edit expressions, and Proxy Shading. Method is Neural Renderer. Task is Facial detail hallucination and rendering. ","The paper proposes a method called FaceDet3D, a method to learn fine details from single image geometric facial details. Morphable Models (3DMMs) can be used to encode fine details in PCA-based representations. The proposed method uses a vertex displacement map to map facial details to 3D face geometry. The method uses Neural Renderer to learn the facial geometric details from FDS. The paper shows that the predicted facial details with zoom-in are better than the Predicted Facial Detail and the Render of Predicted Faceial Detail. ","The paper proposes a method for learning facial geometric details from single image geometric facial details. The method is based on Morphable Models (3DMMs) for fine details, which can be represented as PCA-based representations. The authors propose a method called FaceDet3D, which uses Facial Expressions to learn 3D face geometry. The proposed method uses a vertex displacement map to map facial details to a smooth proxy geometry with hallucinated details in the shape of a smile. The paper also proposes a Neural Renderer to learn the detailed geometry using FDS. The key idea is to use Neural Rendering to learn detailed geometry with zoom-in, and then use Predicted Facial Detail and Render of Predicted Faceial Detail for rendering the predicted facial details with edit expressions. The results show that the proposed Facial detail hallucination and rendering is more effective than Proxy Shading."
4597,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"neural representations USED-FOR NLP models. neural representations CONJUNCTION linguistic factors. linguistic factors CONJUNCTION neural representations. syntactic roles PART-OF factors. latent variables CONJUNCTION realizations of syntactic roles. realizations of syntactic roles CONJUNCTION latent variables. attention USED-FOR deep probabilistic generative model. Attention - Driven Variational Autoencoder ( ADVAE ) HYPONYM-OF probabilistic model. attention USED-FOR ADVAEs. evaluation protocol EVALUATE-FOR disentanglement. disentanglement FEATURE-OF realizations of syntactic roles. evaluation protocol USED-FOR realizations of syntactic roles. attention maxima CONJUNCTION latent variable perturbations. latent variable perturbations CONJUNCTION attention maxima. latent variable perturbations USED-FOR decoder. attention maxima USED-FOR encoder. latent variable perturbations USED-FOR protocol. attention maxima USED-FOR protocol. ADVAE USED-FOR syntactic roles. sequence VAEs CONJUNCTION Transformer VAEs. Transformer VAEs CONJUNCTION sequence VAEs. ADVAE COMPARE Transformer VAEs. Transformer VAEs COMPARE ADVAE. ADVAE COMPARE sequence VAEs. sequence VAEs COMPARE ADVAE. raw English text PART-OF SNLI dataset. latent variables USED-FOR realizations of syntactic roles. Generic is they. OtherScientificTerm are decomposition of predicative structures, and supervision. Method is Transformer - based machine translation models. Task are disentanglement of syntactic roles, and unsupervised controllable content generation. ","This paper studies the problem of disentanglement of syntactic roles in NLP models with neural representations and linguistic factors. The authors propose Attention-Driven Variational Autoencoder (ADVAE), a probabilistic model based on attention, which is trained with attention to learn the decomposition of predicative structures. ADVAE is evaluated on the SNLI dataset with raw English text. The evaluation protocol is based on the observation that ADVAEs perform better than sequence VAEs and Transformer VAEs, and that they can disentangle the syntactic role representations from the latent variables. The paper also shows that the proposed protocol can be used to learn a decoder with attention maxima and latent variable perturbations. ","This paper proposes a deep probabilistic generative model, Attention-Driven Variational Autoencoder (ADVAE) that combines neural representations and linguistic factors. The authors show that ADVAE can disentanglement of syntactic roles in the context of the decomposition of predicative structures. They also show that the proposed evaluation protocol is able to disentangle the realizations of these roles from the latent variables in the encoder and the latent variable perturbations in the decoder. Finally, they show that they can achieve unsupervised controllable content generation without supervision. The paper is well-written and well-motivated. The experimental results are promising. The proposed protocol is evaluated on the raw English text in the SNLI dataset, and the results show better performance than Transformer-based machine translation models. The results are also shown on sequence VAEs and Transformer VAEs."
4613,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"Discovering successful coordinated behaviors PART-OF Multi - Agent Reinforcement Learning ( MARL ). joint action space USED-FOR it. mechanism USED-FOR sufficient exploration and coordination. framework USED-FOR coordination protocols. sparse rewards CONJUNCTION partial observability. partial observability CONJUNCTION sparse rewards. StarCraft Multi - Agent Challenge HYPONYM-OF exploration scheme. exploration scheme USED-FOR complex cooperative strategies. methods COMPARE baselines. baselines COMPARE methods. Method is intrinsic motivation functions. OtherScientificTerm are agents ’ interactions, and counterfactual rollouts. Generic is approach. ","This paper studies the problem of discovering successful coordinated behaviors in Multi-Agent Reinforcement Learning (MARL). In MARL, the agents’ interactions are represented as intrinsic motivation functions, and it is assumed that the agent has access to a joint action space. The authors propose a mechanism to learn sufficient exploration and coordination protocols. The framework is based on the framework of coordination protocols, and the authors propose an exploration scheme that combines sparse rewards with partial observability. The exploration scheme is shown to be able to learn complex cooperative strategies in StarCraft Multi-agent Challenge. The proposed approach is evaluated on a variety of benchmarks and shows that the proposed methods perform better than existing baselines.","This paper proposes a method for discovering successful coordinated behaviors in Multi-Agent Reinforcement Learning (MARL). The key idea is to use intrinsic motivation functions to encourage agents’ interactions in the joint action space. The authors propose a mechanism to encourage sufficient exploration and coordination between agents. The framework is evaluated on StarCraft multi-agent challenge, where it is shown that the proposed framework is able to learn coordination protocols with sparse rewards and partial observability. The proposed approach is compared to other methods and baselines. The exploration scheme is tested on complex cooperative strategies (e.g., StarCraft Multi-agent Challenge) and counterfactual rollouts."
4629,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"fine - tuning approaches COMPARE editing algorithms. editing algorithms COMPARE fine - tuning approaches. Gradient Decomposition ( MEND ) USED-FOR Model Editor Networks. Model Editor Networks USED-FOR post - hoc editing. MEND USED-FOR gradient. low - rank decomposition of the gradient USED-FOR transformation. low - rank decomposition of the gradient USED-FOR gradient. fine - tuning USED-FOR gradient. low - rank decomposition of the gradient USED-FOR MEND. MEND USED-FOR edits. edits USED-FOR pre - trained model. GPU USED-FOR MEND. MEND USED-FOR model editing. approach USED-FOR model editing. Method are large pre - trained models, large neural networks, and small auxiliary editing networks. Generic are models, and model. OtherScientificTerm are targeted edits, and pre - trained model ’s behavior. ","This paper proposes Gradient Decomposition (MEND) for Model Editor Networks for post-hoc editing on large pre-trained models. MEND uses low-rank decomposition of the gradient for the transformation and fine-tuning for the gradient. The authors show that MEND improves the performance of the editing algorithms compared to other editing algorithms in terms of the number of targeted edits. They also show that the MEND can be used to perform edits on a pre-train model without affecting the model’s behavior. Finally, they show that using MEND on the GPU can improve model editing on small auxiliary editing networks. ","This paper proposes Gradient Decomposition (MEND) for Model Editor Networks for post-hoc editing. MEND is an extension of fine-tuning approaches to editing algorithms. The main idea is to use MEND to decompose the gradient of the transformation into a low-rank decomposition of the gradient and then fine-tune the gradient. The authors show that MEND can be used for edits to a pre-trained model. They also show that it can be applied to small auxiliary editing networks. Finally, they show that using MEND on the GPU can improve model editing. "
4645,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,learning abilities CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION learning abilities. artificial neural networks CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION artificial neural networks. learning abilities FEATURE-OF artificial neural networks. FINN USED-FOR constituents of partial differential equations ( PDEs ). numerical simulation USED-FOR physical and structural knowledge. artificial neural networks USED-FOR FINN. learning abilities PART-OF FINN. oneand two - dimensional PDEs EVALUATE-FOR FINN. modeling accuracy CONJUNCTION outof - distribution generalization ability. outof - distribution generalization ability CONJUNCTION modeling accuracy. initial and boundary conditions FEATURE-OF outof - distribution generalization ability. modeling accuracy EVALUATE-FOR FINN. outof - distribution generalization ability EVALUATE-FOR FINN. diffusion - sorption HYPONYM-OF oneand two - dimensional PDEs. FINN COMPARE physics - aware models. physics - aware models COMPARE FINN. FINN COMPARE machine learning. machine learning COMPARE FINN. machine learning CONJUNCTION physics - aware models. physics - aware models CONJUNCTION machine learning. FINN COMPARE calibrated physical model. calibrated physical model COMPARE FINN. calibrated physical model USED-FOR sparse real - world data. sparse real - world data USED-FOR FINN. generalization abilities EVALUATE-FOR FINN. diffusionsorption scenario FEATURE-OF sparse real - world data. Task is spatiotemporal advection - diffusion processes. OtherScientificTerm is unknown retardation factor. ,"This paper studies the problem of spatiotemporal advection-diffusion processes. The authors propose a new method called FinN to learn constituents of partial differential equations (PDEs) from numerical simulation. The main idea is to combine the learning abilities of artificial neural networks and physical and structural knowledge from the two learning abilities in FINN. They show that FINN improves the modeling accuracy and the outof-distribution generalization ability under initial and boundary conditions. They also show that the diffusion-sorption scenario of oneand two-dimensional PDEs can be used to improve the generalization abilities of FINN in the sparse real-world data. Finally, they demonstrate that the calibrated physical model trained with FINN can outperform the calibration physical model in sparse real -world data, and the physics-aware models trained with machine learning.","This paper proposes a new method for learning partial differential equations (PDEs) in the context of diffusion-sporption. The authors propose to use a neural network to learn the constituents of partial differential equation (PDE) in terms of the unknown retardation factor. They show that the proposed method outperforms state-of-the-art methods in the diffusionsorption scenario, and in the sparse real-world data. They also show that their method is able to generalize better than existing methods."
4661,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"unsupervised machine learning ( ML ) models CONJUNCTION representations of computer programs. representations of computer programs CONJUNCTION unsupervised machine learning ( ML ) models. representations of computer programs USED-FOR representations of computer programs. unsupervised machine learning ( ML ) models USED-FOR representations of computer programs. abstract syntax tree ( AST)-related information CONJUNCTION runtime information. runtime information CONJUNCTION abstract syntax tree ( AST)-related information. brain representations USED-FOR static and dynamic properties of code. runtime information HYPONYM-OF static and dynamic properties of code. abstract syntax tree ( AST)-related information HYPONYM-OF static and dynamic properties of code. brain representations USED-FOR representations. representations USED-FOR ML models. Material is Python code. Metric is complexity. Method are Multiple Demand system, and machine learned representations of code. OtherScientificTerm is code. ","This paper studies the problem of unsupervised machine learning (ML) models learning representations of computer programs. The authors propose a Multiple Demand system, where the goal is to learn a set of code that is computationally efficient. The complexity of the code is measured by the number of iterations needed to learn the code. The paper shows that the complexity of learning the code can be reduced by using a combination of abstract syntax tree (AST)-related information and runtime information. The proposed representations are then used to train ML models using brain representations.","This paper proposes a method for learning representations of computer programs. The authors propose to use unsupervised machine learning (ML) models to learn representations of the computer programs, which are then used to improve the performance of representations of machine learned representations of code. The proposed method is based on the Multiple Demand system, where the complexity of the code is measured by the number of times the code has been updated. The main idea is to use abstract syntax tree (AST)-related information and runtime information to learn the static and dynamic properties of code, and to use brain representations to learn these representations for ML models. The paper is well-written and easy to follow. "
4677,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,phoneme decoder CONJUNCTION mel - spectrogram synthesizer. mel - spectrogram synthesizer CONJUNCTION phoneme decoder. speech encoder CONJUNCTION phoneme decoder. phoneme decoder CONJUNCTION speech encoder. mel - spectrogram synthesizer CONJUNCTION attention module. attention module CONJUNCTION mel - spectrogram synthesizer. attention module PART-OF Translatotron 2. mel - spectrogram synthesizer PART-OF Translatotron 2. phoneme decoder PART-OF Translatotron 2. speech encoder PART-OF Translatotron 2. Translatotron 2 COMPARE Translatotron. Translatotron COMPARE Translatotron 2. babbling CONJUNCTION long pause. long pause CONJUNCTION babbling. translation quality CONJUNCTION predicted speech naturalness. predicted speech naturalness CONJUNCTION translation quality. robustness EVALUATE-FOR predicted speech. long pause HYPONYM-OF over - generation. babbling HYPONYM-OF over - generation. predicted speech naturalness EVALUATE-FOR Translatotron 2. robustness EVALUATE-FOR Translatotron 2. translation quality EVALUATE-FOR Translatotron. translation quality EVALUATE-FOR Translatotron 2. model USED-FOR production deployment. Translatotron COMPARE it. it COMPARE Translatotron. method CONJUNCTION concatenation - based data augmentation. concatenation - based data augmentation CONJUNCTION method. Material is translated speech. Method is Translatotron 2 model. ,"This paper proposes a Translatotron 2 model that combines a phoneme decoder, a mel-spectrogram synthesizer, a speech encoder, and an attention module. The proposed model is shown to improve the translation quality, the predicted speech naturalness, and the robustness against babbling, long pause, and over-generation. The authors also show that the proposed method can be combined with concatenation-based data augmentation, and that it can achieve better performance than Translatron 2. ","This paper proposes a Translatotron 2 model that combines a phoneme decoder, a mel-spectrogram synthesizer, and a speech encoder. The proposed model is evaluated on production deployment and robustness to over-generation, babbling, long pause, and predicted speech naturalness. The method is based on concatenation-based data augmentation, and it is shown to improve the translation quality and the predicted speech. "
4693,SP:296102e60b842923c94f579f524fa1147328ee4b,"attribute - based representations USED-FOR concept learning. zeroshot learning HYPONYM-OF attribute - based learning paradigms. supervised learning COMPARE selfsupervised pre - training. selfsupervised pre - training COMPARE supervised learning. predictability of test attributes USED-FOR model. predictability of test attributes USED-FOR generalization ability. generalization ability EVALUATE-FOR model. OtherScientificTerm are Semantic concepts, mappings, and random splits of the attribute space. Task is rapid learning of attributes. Method is few - shot learning of semantic classes. ","This paper studies the problem of attribute-based representations for concept learning in the context of few-shot learning of semantic classes. The authors propose zeroshot learning, one of the most important attributes-based learning paradigms in recent years. The key idea is to use the predictability of test attributes in the model to improve the generalization ability of the model. Semantic concepts are represented as mappings between attributes, and the mappings are learned using random splits of the attribute space. This allows for rapid learning of attributes, which is an important problem in the field of rapid learning.  The authors show that the proposed model can achieve better performance than supervised learning and selfsupervised pre-training.","This paper proposes a new attribute-based representations for concept learning, called zeroshot learning. The key idea is to learn a set of attributes for each concept. Semantic concepts are represented as mappings, and the mappings are learned using random splits of the attribute space. The model is based on a few-shot learning of semantic classes. The generalization ability of the model is measured by the predictability of test attributes. The authors show that the model outperforms supervised learning and selfsupervised pre-training. "
4709,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,relative entropy gradient sampler ( REGS ) USED-FOR sampling from unnormalized distributions. particle method USED-FOR nonlinear transforms. REGS HYPONYM-OF particle method. gradient flow USED-FOR path of probability distributions. path of probability distributions USED-FOR reference distribution. density of evolving particles CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density of evolving particles. density ratios FEATURE-OF density of evolving particles. density ratios CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density ratios. density ratios FEATURE-OF velocity fields. velocity fields FEATURE-OF ODE system. ODE system USED-FOR It. particle evolution USED-FOR ODE system. nonparametric approach USED-FOR logarithmic density ratio. neural networks USED-FOR nonparametric approach. REGS COMPARE sampling methods. sampling methods COMPARE REGS. multimodal 1D and 2D mixture distributions CONJUNCTION Bayesian logistic regression. Bayesian logistic regression CONJUNCTION multimodal 1D and 2D mixture distributions. real datasets EVALUATE-FOR Bayesian logistic regression. Method is Wasserstein gradient flow of relative entropy. ,"This paper proposes a relative entropy gradient sampler (REGS) for sampling from unnormalized distributions. REGS is a particle method based on the Wasserstein gradient flow of relative entropy. It uses the gradient flow to learn a path of probability distributions over the reference distribution, and then uses the particle method to learn nonlinear transforms. It is based on an ODE system with velocity fields with density ratios and density ratios for the density of evolving particles and the unnormalised target density. It also uses a nonparametric approach to learn the logarithmic density ratio using neural networks. Experiments on real datasets show that REGS outperforms other sampling methods on multimodal 1D and 2D mixture distributions and Bayesian logistic regression.",This paper proposes a relative entropy gradient sampler (REGS) for sampling from unnormalized distributions. REGS is a particle method for nonlinear transforms. It is based on the Wasserstein gradient flow of relative entropy. The authors show that the path of probability distributions can be approximated by gradient flow in the direction of the reference distribution. It uses an ODE system based on particle evolution. The density of evolving particles and density ratios of the density of the evolving particles are modeled as velocity fields. A nonparametric approach is used to compute the logarithmic density ratio of the two density ratios. This is done using neural networks. Experiments are conducted on multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets.
4725,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,quantum machine learning techniques USED-FOR classical image classification. quantum neural network USED-FOR inference. qubits USED-FOR encoding schemes. quantum systems USED-FOR framework. encoding mechanism USED-FOR approach. accuracy EVALUATE-FOR classical neural networks. framework COMPARE classical neural networks. classical neural networks COMPARE framework. personal laptop FEATURE-OF MNIST dataset. accuracy EVALUATE-FOR framework. quantum computers CONJUNCTION classical simulation. classical simulation CONJUNCTION quantum computers. work USED-FOR quantum machine learning and classification. classical datasets USED-FOR quantum machine learning and classification. OtherScientificTerm is quantum states. Generic is technique. ,This paper proposes a new framework for quantum machine learning techniques for classical image classification. The proposed framework is based on quantum systems and uses qubits as encoding schemes. The authors propose a new encoding mechanism for the approach and show that the proposed technique can achieve better accuracy than classical neural networks. The experimental results on the MNIST dataset on a personal laptop demonstrate the effectiveness of the proposed framework. ,"This paper proposes a new framework for learning quantum machine learning techniques for classical image classification. The framework is based on quantum systems, where the quantum states are represented as qubits. The authors propose two encoding schemes to encode the qubits into a quantum neural network for inference. The encoding mechanism is a simple yet effective approach. The experimental results show that the proposed framework achieves better accuracy than classical neural networks on the MNIST dataset on a personal laptop. The proposed technique is evaluated on two classical datasets and compared to quantum computers and classical simulation. The work is also applied to quantum machines and classical machine learning and classification."
4741,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"data privacy FEATURE-OF face recognition. framework USED-FOR federated learning face recognition. communicating auxiliary and privacy - agnostic information USED-FOR framework. communicating auxiliary and privacy - agnostic information USED-FOR federated learning face recognition. Differentially Private Local Clustering ( DPLC ) mechanism USED-FOR sanitized clusters. local class centers USED-FOR sanitized clusters. consensus - aware recognition loss USED-FOR global consensuses. IJB - B CONJUNCTION IJB - C. IJB - C CONJUNCTION IJB - B. large - scale dataset EVALUATE-FOR method. Method are federated learning ( FL ) paradigm, FL methods, and PrivacyFace. Generic is task. Task is recognition. OtherScientificTerm are privacy leakage, privacy - utility paradox, discriminative features, and lightweight overhead. ",This paper proposes a framework for federated learning face recognition with communicating auxiliary and privacy-agnostic information. The authors propose a Differentially Private Local Clustering (DPLC) mechanism to learn sanitized clusters from local class centers. The consensus-aware recognition loss is used to train global consensuses. The proposed method is evaluated on a large-scale dataset and compared with IJB-B and IBD-C.,The paper proposes a framework for federated learning face recognition based on communicating auxiliary and privacy-agnostic information. The framework is based on the Federated learning (FL) paradigm. The main idea is to use a Differentially Private Local Clustering (DPLC) mechanism to learn sanitized clusters from local class centers. The paper also proposes a consensus-aware recognition loss to learn global consensuses. The proposed method is evaluated on a large-scale dataset. 
4757,SP:408d9e1299ee05b89855df9742b608626692b40d,Transfer - learning methods USED-FOR data - scarce target domain. data - rich source domain USED-FOR model. model USED-FOR Transfer - learning methods. strategy COMPARE method. method COMPARE strategy. fine - tuning USED-FOR model. linear probing USED-FOR intermediate layers. classification head USED-FOR target - domain. features USED-FOR classification head. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE fine - tuning. fine - tuning COMPARE Head2Toe. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE Head2Toe. Head2Toe COMPARE Head2Toe. fine - tuning USED-FOR Head2Toe. Head2Toe USED-FOR out - of - distribution transfer. Generic is source model. OtherScientificTerm is pretrained layers. ,"This paper proposes a new model for transfer-learning methods for the data-scarce target domain. The model is based on a data-rich source domain and is trained on the classification head of the target-domain. The source model is trained using a pre-trained pretrained layers. The intermediate layers are trained using linear probing and fine-tuning. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and compared to Head2Toe, which is a similar method but with a much smaller number of features. The results show that the proposed Head2toe is able to perform well in out-of-distribution transfer, and is more robust to fine-tuning.","This paper proposes a new model for the data-rich source domain, which is a data-rare target domain. The model is a variant of Transfer-learning methods for data-scrapped target domain, where the source model is trained on a set of pretrained layers, and the target-domain is a subset of the training set. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and compared to the fine-tuning of the model on intermediate layers by linear probing. Head2Toe is shown to outperform Head2toe on out-of-distribution transfer, while fine-tuning is not."
4773,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,dynamics models USED-FOR model - based planning. Dynamics models USED-FOR image - based games. fully observable states FEATURE-OF image - based games. models USED-FOR Text - Based Games ( TBGs ). noisy text observations FEATURE-OF partially observable states. planning algorithms USED-FOR decision - making problems. planning algorithms USED-FOR text domains. text domains USED-FOR decision - making problems. OOTD USED-FOR memory graph. OOTD model USED-FOR beliefs of object states. OOTD model USED-FOR dynamics. independently parameterized transition layers USED-FOR beliefs of object states. variational objectives USED-FOR stochasticity of predicted dynamics. object - supervised and self - supervised settings USED-FOR variational objectives. OOTD - based planner COMPARE model - free baselines. model - free baselines COMPARE OOTD - based planner. sample efficiency CONJUNCTION running scores. running scores CONJUNCTION sample efficiency. running scores EVALUATE-FOR OOTD - based planner. sample efficiency EVALUATE-FOR OOTD - based planner. running scores EVALUATE-FOR model - free baselines. sample efficiency EVALUATE-FOR model - free baselines. OtherScientificTerm is object - irrelevant information. ,"This paper studies the problem of model-based planning with dynamics models for image-based games. Dynamics models are commonly used for image -based games with fully observable states and partially observable states in the context of noisy text observations. The authors propose to use these models for Text-Based Games (TBGs), where the goal is to learn the dynamics of the objects in a memory graph. The OOTD model learns the beliefs of object states from independently parameterized transition layers. The planning algorithms are applied to text domains to solve decision-making problems in text domains. The variational objectives are used to improve the stochasticity of predicted dynamics in both object-supervised and self-supervision settings. The experimental results show that the OotD-based planner outperforms model-free baselines in terms of sample efficiency and running scores.",This paper proposes a new model-based planning framework for learning dynamics models for image-based games with fully observable states and partially observable states. Dynamics models are commonly used in text-based Games (TBGs) for learning models for decision-making problems in the context of planning algorithms for text domains. The authors propose an OOTD model for learning the dynamics of object states in a memory graph with independently parameterized transition layers. The variational objectives are based on the stochasticity of predicted dynamics in both object-supervised and self-supervision settings. Experiments show that the proposed model-free baselines achieve better sample efficiency and running scores compared to model-trained baselines. 
4789,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"cost - sensitive loss FEATURE-OF label taxonomy. tractable method USED-FOR hierarchical learning problem. hierarchical cost - sensitive loss CONJUNCTION layer - wise abstaining losses. layer - wise abstaining losses CONJUNCTION hierarchical cost - sensitive loss. bijective mapping USED-FOR hierarchical cost - sensitive loss. symmetry assumptions USED-FOR bijective mapping. distributionally robust learning framework USED-FOR learningto - abstain problems. large - scale bird dataset CONJUNCTION cell classification problems. cell classification problems CONJUNCTION large - scale bird dataset. LAM COMPARE methods. methods COMPARE LAM. high accuracy regions FEATURE-OF hierarchical cost - sensitive loss. hierarchical cost - sensitive loss EVALUATE-FOR LAM. high accuracy regions EVALUATE-FOR LAM. perclass loss - adjustment heuristic USED-FOR performance profile. perclass loss - adjustment heuristic USED-FOR LAM. cost design USED-FOR user requirements. cost design USED-FOR optimizable cost functions. user requirements CONJUNCTION optimizable cost functions. optimizable cost functions CONJUNCTION user requirements. Task is cost - sensitive hierarchical classification. OtherScientificTerm are hierarchy, cost - sensitive hierarchical loss, non - convexity, and taxonomy. Metric is accuracy. ","This paper proposes a tractable method to solve the hierarchical learning problem with a cost-sensitive loss for label taxonomy. The main idea is to use a bijective mapping between the hierarchical and the layer-wise abstaining losses, which is based on symmetry assumptions. The proposed method, LAM, is a distributionally robust learning framework for learningto-abstain problems with a large-scale bird dataset and cell classification problems. LAM achieves state-of-the-art performance with high accuracy regions on the hierarchical cost-sensitive loss and LAM with a perclass loss-adjustment heuristic for the performance profile. The cost design is also used for optimizable cost functions and user requirements.",This paper proposes a tractable method for solving the hierarchical learning problem of the cost-sensitive hierarchical classification problem. The main idea is to use bijective mapping to learn the label taxonomy of the hierarchy. The authors use symmetry assumptions on the bijectives of the layers and use a distributionally robust learning framework to solve learningto-abstain problems. The proposed method is evaluated on a large-scale bird dataset and cell classification problems and compared to other methods. The results show that LAM outperforms the state-of-the-art in terms of high accuracy regions of the hierarchical cost sensitive loss and layer-wise abstaining losses. The performance profile of LAM is also improved by using a perclass loss-adjustment heuristic to improve the user requirements and optimizable cost functions. 
4805,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"spatial location CONJUNCTION semantic identity label. semantic identity label CONJUNCTION spatial location. estimating sound sources ’ temporal location CONJUNCTION spatial location. spatial location CONJUNCTION estimating sound sources ’ temporal location. multi - channel sound raw waveforms USED-FOR semantic identity label. multi - channel sound raw waveforms USED-FOR estimating sound sources ’ temporal location. single - scale filter bank USED-FOR sound waveforms. STFT CONJUNCTION LogMel. LogMel CONJUNCTION STFT. they USED-FOR hand - engineered features. limited time - frequency resolution capability FEATURE-OF sound waveforms. parameter tuning USED-FOR they. STFT HYPONYM-OF hand - engineered features. LogMel HYPONYM-OF hand - engineered features. parameter tuning USED-FOR hand - engineered features. frequency resolution FEATURE-OF synperiodic filter. time - frequency resolution trade - off EVALUATE-FOR synperiodic filter. synperiodic filter bank USED-FOR multi - scale perception. synperiodic filter bank USED-FOR downsampled waveform. synperiodic filter bank group USED-FOR dynamic multi - scale time - frequency representation. multi - scale perception USED-FOR synperiodic filter bank group. time and frequency domain advantage FEATURE-OF multi - scale perception. semantic identity label CONJUNCTION spatial location representation. spatial location representation CONJUNCTION semantic identity label. Transformer - like backbone USED-FOR semantic identity label. Transformer - like backbone USED-FOR spatial location representation. parallel soft - stitched branches PART-OF Transformer - like backbone. Transformer - like backbone PART-OF synperiodic filter bank group front - end. direction of arrival estimation task CONJUNCTION physical location estimation task. physical location estimation task CONJUNCTION direction of arrival estimation task. direction of arrival estimation task EVALUATE-FOR framework. physical location estimation task EVALUATE-FOR framework. OtherScientificTerm are complex waveform mixture, periodicity term, and temporal length. Generic are representation, and Existing methods. Method are parameterized synperiodic filter banks, and synperiodic filter banks. Material is raw waveform. ","This paper proposes a method to learn a dynamic multi-scale time-frequency representation from multi-channel sound raw waveforms. The proposed method is based on parameterized synperiodic filter banks, where each waveform is composed of a single-scale filter bank and a complex waveform mixture. The authors show that they are able to learn hand-engineered features such as STFT and LogMel with parameter tuning. They also show that the performance of the synperiodi filter bank improves with frequency resolution. ","This paper proposes a new representation for multi-scale time-frequency representation. The proposed representation is based on parameterized synperiodic filter banks. Existing methods are based on a single-scale filter bank for sound waveforms. However, this paper introduces a downsampled waveform with a complex waveform mixture. The downsampling waveform can be represented as a sequence of parallel soft-stitched branches. The authors show that they can learn hand-engineered features such as STFT and LogMel with parameter tuning. They also show that a synperiodIC filter bank can be learned with a high frequency resolution trade-off between the time and frequency domain advantage. The paper is well written and the proposed framework is well-motivated. "
4821,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"self - training USED-FOR model. iterative self - training USED-FOR model. implicit curriculum USED-FOR iterative self - training. method USED-FOR virtual samples. intermediate distributions USED-FOR virtual samples. iterative self - training CONJUNCTION GIFT. GIFT CONJUNCTION iterative self - training. self - training CONJUNCTION iterative self - training. iterative self - training CONJUNCTION self - training. GIFT USED-FOR model. domain adaptation methods USED-FOR GIFT. natural distribution shifts FEATURE-OF benchmarks. benchmarks EVALUATE-FOR iterative self - training. benchmarks EVALUATE-FOR self - training. benchmarks EVALUATE-FOR GIFT. Task is domain adaptation. Method are learning domain invariant representations, and iterative selftraining. Generic is assumptions. OtherScientificTerm is synthetic distribution shifts. ","This paper studies the problem of domain adaptation in the context of learning domain invariant representations. The authors propose an implicit curriculum for iterative self-training, where the model is trained using self-trained on the data, and the goal is to learn a model that is invariant to synthetic distribution shifts. They propose a method to learn virtual samples from intermediate distributions, and then use the intermediate distributions to train the virtual samples. They show that the proposed method is able to achieve state-of-the-art performance on a variety of benchmarks. They also show that their method can be combined with existing domain adaptation methods to improve the performance of the model. ","This paper proposes a method for learning domain invariant representations. The model is trained using iterative self-training with an implicit curriculum. The authors propose a method to generate virtual samples from intermediate distributions, and then apply the learned assumptions to generate synthetic distribution shifts. Experiments are conducted on three benchmarks for self-train and GIFT, showing the effectiveness of the proposed domain adaptation methods. "
4837,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"representations USED-FOR multi - modal retrieval. recommendation CONJUNCTION search. search CONJUNCTION recommendation. representations USED-FOR applications. search HYPONYM-OF applications. recommendation HYPONYM-OF applications. video titles or audio transcripts USED-FOR video - text retrieval literature. method USED-FOR representations. videos USED-FOR representations. attention - based mechanism USED-FOR model. comments USED-FOR method. video - text retrieval benchmarks EVALUATE-FOR method. Generic is benchmarks. OtherScientificTerm are modalities, and user comments. ","This paper proposes a method to learn representations for multi-modal retrieval using video titles or audio transcripts from video-text retrieval literature. The proposed method is based on the attention-based mechanism, where the model is trained using comments from the user comments. The authors show that the proposed method can learn representations from videos and then use these representations for multiple applications such as recommendation, search, etc. The method is evaluated on a variety of video-tea retrieval benchmarks and achieves state-of-the-art performance.","This paper proposes a method to learn representations for multi-modal retrieval from video titles or audio transcripts. The proposed method is based on an attention-based mechanism, where the modalities are represented by user comments. The method is evaluated on a number of video-text retrieval benchmarks, where it is shown that the proposed method can learn representations from videos and videos from user comments, as well as other applications such as recommendation and search."
4853,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"policy USED-FOR latent - conditioned trajectories. discriminator USED-FOR distinguishability. trajectories USED-FOR latents. penalization USED-FOR exploration. objective USED-FOR epistemic uncertainty. objective USED-FOR intrinsic reward. intrinsic reward COMPARE pseudocount - based methods. pseudocount - based methods COMPARE intrinsic reward. objective COMPARE pseudocount - based methods. pseudocount - based methods COMPARE objective. DISDAIN USED-FOR skill learning. tabular grid world EVALUATE-FOR DISDAIN. DISDAIN USED-FOR pessimism. OtherScientificTerm are Unsupervised skill learning objectives, extrinsic rewards, inherent pessimism, information gain auxiliary objective, and discriminators. ","This paper proposes a policy for learning latent-conditioned trajectories in the presence of extrinsic rewards. Unsupervised skill learning objectives have been well-studied in the literature, and the intrinsic reward has been shown to be an intrinsic reward. The authors propose a discriminator to improve the distinguishability of the learned trajectories by penalizing the latents of the discriminator. The objective is based on the notion of epistemic uncertainty, and penalization is used to encourage exploration. The paper shows that the proposed objective is better than pseudocount-based methods in terms of intrinsic reward, and is also better than DISDAIN for skill learning in the tabular grid world. ","This paper proposes a policy for learning latent-conditioned trajectories for learning unsupervised skill learning objectives with extrinsic rewards. The key idea is to use a discriminator to encourage distinguishability between latents and trajectories. The discriminators are based on the information gain auxiliary objective. The authors propose a new objective for epistemic uncertainty that encourages exploration without penalization. The intrinsic reward is compared to pseudocount-based methods, and the authors show that DISDAIN is able to achieve better pessimism in the tabular grid world."
4869,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,deep neural networks USED-FOR chemistry. deep neural networks USED-FOR generating molecules. spanning tree CONJUNCTION residual edges. residual edges CONJUNCTION spanning tree. spanning tree USED-FOR molecular graph generation. tree - constructive operations USED-FOR molecular graph connectivity. formulation USED-FOR sparsity of molecular graphs. intermediate graph structure USED-FOR framework. intermediate graph structure USED-FOR construction process. chemical valence rules FEATURE-OF molecular graphs. Transformer architecture USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR Transformer architecture. Fréchet ChemNet distance CONJUNCTION fragment similarity. fragment similarity CONJUNCTION Fréchet ChemNet distance. validity CONJUNCTION Fréchet ChemNet distance. Fréchet ChemNet distance CONJUNCTION validity. QM9 CONJUNCTION ZINC250k. ZINC250k CONJUNCTION QM9. ZINC250k CONJUNCTION MOSES benchmarks. MOSES benchmarks CONJUNCTION ZINC250k. QM9 EVALUATE-FOR framework. MOSES benchmarks EVALUATE-FOR framework. metrics EVALUATE-FOR framework. fragment similarity EVALUATE-FOR framework. validity EVALUATE-FOR framework. validity HYPONYM-OF metrics. fragment similarity HYPONYM-OF metrics. Fréchet ChemNet distance HYPONYM-OF metrics. STGG USED-FOR penalized LogP value of molecules. Task is maximizing penalized LogP value of molecules. ,"This paper proposes a new framework for generating molecules from deep neural networks for chemistry. The framework is based on tree-constructive operations for molecular graph connectivity, where a spanning tree and residual edges are used to construct molecular graph generation. The authors propose a new formulation to reduce the sparsity of molecular graphs by maximizing penalized LogP value of molecules. The proposed Transformer architecture uses tree-based relative positional encodings to guide the tree construction procedure. The intermediate graph structure in the construction process allows the authors to learn the chemical valence rules of the molecular graphs.  The proposed framework is evaluated on a variety of MOSES benchmarks, including QM9, ZINC250k, and Fréchet ChemNet distance, as well as fragment similarity. ","This paper proposes a novel framework for generating molecules from deep neural networks for chemistry. The framework is based on the notion of spanning tree and residual edges in molecular graph generation. The authors propose tree-constructive operations for molecular graph connectivity. The formulation is motivated by the sparsity of molecular graphs in terms of chemical valence rules. The construction process is performed using the intermediate graph structure of the molecule. The proposed Transformer architecture is a tree construction procedure based on tree-based relative positional encodings. The main contribution of the paper is the use of STGG for maximizing penalized LogP value of molecules. The paper is evaluated on several MOSES benchmarks, including QM9, ZINC250k, Fréchet ChemNet distance, and fragment similarity. "
4885,SP:3a19340d6af65e3f949dda839a6d233369891c46,"image generation CONJUNCTION face recognition. face recognition CONJUNCTION image generation. Polynomial neural networks ( PNNs ) USED-FOR image generation. Polynomial neural networks ( PNNs ) USED-FOR face recognition. spectral bias FEATURE-OF low - frequency functions. spectral bias FEATURE-OF neural networks. spectral analysis USED-FOR Neural Tangent Kernel ( NTK ). Neural Tangent Kernel ( NTK ) USED-FOR PNNs. spectral analysis USED-FOR PNNs. parametrization of PNNs USED-FOR learning. Π - Net family USED-FOR learning. parametrization of PNNs HYPONYM-OF Π - Net family. polynomials USED-FOR multiplicative interactions. OtherScientificTerm are high - frequency information, and theoretical bias. Task is training. ","This paper studies the spectral bias of neural networks with Polynomial neural networks (PNNs) for image generation and face recognition. The authors propose a spectral analysis for PNNs based on Neural Tangent Kernel (NTK). The theoretical bias is based on high-frequency information, and the authors show that the theoretical bias does not hold for low-frequency functions.  The authors then propose a parametrization of the Π-Net family for learning and show that polynomials can be used for multiplicative interactions. ",This paper studies the spectral bias of low-frequency functions in neural networks. Polynomial neural networks (PNNs) are used for image generation and face recognition. The authors propose a spectral analysis for PNNs based on the Neural Tangent Kernel (NTK). The authors show that the high-frequency information of the PNN leads to a theoretical bias. They also propose a new family of learning called the Π-Net family. They show that polynomials are important for multiplicative interactions. 
4901,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"memory CONJUNCTION computational costs. computational costs CONJUNCTION memory. hidden subnetworks PART-OF randomly initialized NNs. edge - popup algorithm USED-FOR hidden subnetworks. subnetworks PART-OF randomly initialized NNs. disguised subnetworks HYPONYM-OF subnetworks. disguised subnetworks COMPARE hidden counterparts. hidden counterparts COMPARE disguised subnetworks. unmasking process USED-FOR subnetworks. unmasking process USED-FOR sparse subnetwork mask. two - stage algorithm USED-FOR disguised subnetworks. operations USED-FOR two - stage algorithm. random initialization USED-FOR subnetwork. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION WideResNet-28. WideResNet-28 CONJUNCTION ResNet-50. PaB COMPARE counterparts. counterparts COMPARE PaB. PaB COMPARE edge - popup. edge - popup COMPARE PaB. CIFAR-10/100 datasets EVALUATE-FOR PaB. edge - popup COMPARE counterparts. counterparts COMPARE edge - popup. large - scale models EVALUATE-FOR PaB. CIFAR-10/100 datasets EVALUATE-FOR large - scale models. WideResNet-28 HYPONYM-OF large - scale models. ResNet-18 HYPONYM-OF large - scale models. ResNet-50 HYPONYM-OF large - scale models. Method are Sparse neural networks ( NNs ), pruningand - finetuning pipeline, random networks, and training algorithm. OtherScientificTerm are disguise, latent weights, and approximated gradients. ","This paper studies the problem of Sparse neural networks (NNs) with hidden subnetworks. The authors propose a new edge-popularity algorithm, called PaB, which uses a two-stage algorithm to learn the sparse subnetwork mask from the unmasking process. The main idea of PaB is to use random initialization to initialize the subnetwork with random initialization, and then use a pruningand-finetuning pipeline to fine-tune the hidden subnetwork. The paper shows that PaB outperforms the state-of-the-art on CIFAR-10/100 datasets and large-scale models such as ResNet-18, Res-50, and WideResNet-28. ","The paper proposes a new approach to training Sparse neural networks (NNs) with a pruningand-finetuning pipeline. Specifically, the authors propose a sparse subnetwork mask, which uses an unmasking process to mask the subnetworks. The authors also propose a two-stage algorithm to train the hidden subnetwork. The main idea is to use random initialization to initialize the subnetwork with random initialization, and then apply the two-stages of operations to generate the masked subnetwork.  The authors show that the proposed approach outperforms PaB on CIFAR-10/100 datasets and large-scale models (ResNet-18, ResNet-50, WideNet-28). "
4917,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"it USED-FOR risk - sensitive domains. robust GNN model USED-FOR adversarial attacks. Unified Graph Neural Network ( GUGNN ) framework USED-FOR graph. graph CONJUNCTION features. features CONJUNCTION graph. robust GNN model(R - GUGNN ) USED-FOR adversarial attacks. operations USED-FOR robust GNN model(R - GUGNN ). operations USED-FOR it. similarity of two adjacent nodes ’ features CONJUNCTION sparsity of real - world graphs. sparsity of real - world graphs CONJUNCTION similarity of two adjacent nodes ’ features. small eigenvalues FEATURE-OF perturbed graphs. operation USED-FOR graph. convolution operation USED-FOR features. Laplacian smoothness CONJUNCTION prior knowledge. prior knowledge CONJUNCTION Laplacian smoothness. Laplacian smoothness USED-FOR convolution operation. real - world datasets EVALUATE-FOR R - GUGNN. real - world datasets EVALUATE-FOR baselines. R - GUGNN COMPARE baselines. baselines COMPARE R - GUGNN. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are graphs, and denoising features. Generic is they. Task is cleaning perturbed graph structure. ","This paper studies the problem of cleaning perturbed graph structure in graph neural networks (GNNs). The authors propose a Unified Graph Neural Network (GUGNN) framework to learn the graph and the features of a graph. The authors show that it can be used as a robust GNN model to defend against adversarial attacks in the risk-sensitive domains. In particular, they show that perturbed graphs with small eigenvalues can be denoised by a simple convolution operation, and that it is possible to use these operations to improve the robustness of a robust gNN model(R-GAN).  The authors also show that R-GAN outperforms other baselines on several real-world datasets. ","This paper proposes a novel framework for learning a graph that is robust to adversarial attacks. It is based on the Unified Graph Neural Network (GUGNN) framework, which can be applied to risk-sensitive domains. The key idea is to train a robust GNN model(R-GAN) that is able to learn a graph and its features. The authors show that it can be trained using two operations: (1) Laplacian smoothness and (2) prior knowledge. They show that R-GAN outperforms baselines on several real-world datasets, and that they can learn a perturbed graph structure with small eigenvalues. "
4933,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,approach USED-FOR texture mapping. it USED-FOR document image unwarping. texture mapping USED-FOR 3D surface. method USED-FOR surface parameterization. 3D surface positions CONJUNCTION 2D texture - space coordinates. 2D texture - space coordinates CONJUNCTION 3D surface positions. continuous bijective mapping USED-FOR 3D surface positions. 2D texture - space coordinates FEATURE-OF continuous bijective mapping. continuous bijective mapping USED-FOR method. multi - view images CONJUNCTION rendering loss. rendering loss CONJUNCTION multi - view images. surface parameterization network PART-OF differentiable rendering pipeline. rendering loss USED-FOR surface parameterization network. multi - view images USED-FOR surface parameterization network. differentiable rendering techniques USED-FOR implicit surfaces. 3D scene reconstruction CONJUNCTION view synthesis. view synthesis CONJUNCTION 3D scene reconstruction. differentiable rendering techniques USED-FOR 3D scene reconstruction. methods USED-FOR appearance color. texture map extraction CONJUNCTION texture editing. texture editing CONJUNCTION texture map extraction. differentiable renderer USED-FOR implicit surfaces. texture extraction USED-FOR document - unwarping. approach USED-FOR high - frequency textures. high - frequency textures FEATURE-OF arbitrary document shapes. synthetic and real scenarios FEATURE-OF arbitrary document shapes. it USED-FOR document texture editing. system USED-FOR document texture editing. it USED-FOR system. Method is explicit surface parameterization. Generic is they. ,"This paper proposes a new approach for texture mapping for 3D surface. The method is based on continuous bijective mapping with 3D texture-space coordinates and 2D texture -space coordinates. The paper also proposes a differentiable rendering pipeline with a surface parameterization network based on multi-view images and a rendering loss. The system is evaluated on 3D scene reconstruction, view synthesis, and view synthesis.  The paper shows that the proposed approach is able to generate high-frequency textures for arbitrary document shapes in both synthetic and real scenarios, and that it can be used for document texture editing and texture map extraction for document-unwrapping. ","This paper proposes a new approach for texture mapping for 3D surface. The proposed method is based on continuous bijective mapping, which is a method for learning surface parameterization. The main idea is to use multi-view images and a rendering loss to learn a differentiable rendering pipeline. The implicit surfaces are learned using differentiable renderer. The authors show that they can learn high-frequency textures for arbitrary document shapes in both synthetic and real scenarios. They also show that the system can be used for document texture editing and texture map extraction for document-unwarping. "
4949,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"subtle regions USED-FOR fine - grained features. strided operations USED-FOR representation. Convolutional Neural Network USED-FOR representation. strided operations USED-FOR Convolutional Neural Network. downsampling algorithm USED-FOR network. scale of receptive field CONJUNCTION granularity of feature. granularity of feature CONJUNCTION scale of receptive field. trade - off mechanism USED-FOR ARP. ARP USED-FOR network. image classification CONJUNCTION image retrieval. image retrieval CONJUNCTION image classification. pooling operation COMPARE state - of - the - arts. state - of - the - arts COMPARE pooling operation. image classification EVALUATE-FOR state - of - the - arts. Task is Fine - grained recognition. OtherScientificTerm are feature resolution, fine - grained information, resolution of sub - sampled feature, and learning - based parameters. ","This paper studies the problem of fine-grained recognition with subtle regions. The authors propose to use a Convolutional Neural Network with strided operations to generate a representation of the feature resolution. The network is trained using a downsampling algorithm, and the network is then trained using an ARP with a trade-off mechanism. The proposed network is evaluated on image classification, image retrieval, and state-of-the-arts.","This paper proposes a novel approach to fine-grained recognition. The authors propose to use a Convolutional Neural Network with strided operations to generate a representation of subtle regions that are used to generate fine-rigid features. The network is trained using a downsampling algorithm, where each sub-region is sampled from a set of sub-sampled sub-regions, and the feature resolution is computed using a trade-off mechanism between the scale of receptive field and the granularity of feature. The proposed network is evaluated on image classification and image retrieval. The results show that the proposed pooling operation outperforms state-of-the-arts in image classification, while the performance of the network is comparable to that of the original network with learning-based parameters."
4965,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"distribution shifts FEATURE-OF neural networks. structural information USED-FOR prediction. structural information FEATURE-OF graph. OOD problem USED-FOR node - level prediction. domain - invariant learning approach USED-FOR GNNs. invariant graph features USED-FOR prediction. invariant graph features USED-FOR GNNs. graphs USED-FOR OOD problem. graphs USED-FOR node - level prediction. Explore - to - Extrapolate Risk Minimization HYPONYM-OF domain - invariant learning approach. graph editers HYPONYM-OF multiple context explorers. cross - domain transfers CONJUNCTION dynamic graph evolution. dynamic graph evolution CONJUNCTION cross - domain transfers. artificial spurious features CONJUNCTION cross - domain transfers. cross - domain transfers CONJUNCTION artificial spurious features. real - world datasets USED-FOR distribution shifts. method USED-FOR distribution shifts. real - world datasets EVALUATE-FOR method. Material are Euclidean data, and graph - structured data. Method are invariant models, and OOD solution. OtherScientificTerm is virtual environments. Generic is model. ","This paper proposes a domain-invariant learning approach called Explore-to-Extrapolate Risk Minimization (Explore-TO-EX) for node-level prediction in the OOD problem in neural networks. The authors propose to use invariant graph features in GNNs to improve the performance of the prediction by using structural information in the graph to guide the prediction. In particular, the authors use graph editers, which are multiple context explorers, to learn the graph-structured data from Euclidean data. They show that the proposed method is able to avoid distribution shifts in real-world datasets and cross-domain transfers and dynamic graph evolution. ","This paper proposes a domain-invariant learning approach for GNNs that uses structural information for prediction. The authors propose Explore-to-Extrapolate Risk Minimization, an OOD problem for node-level prediction with invariant graph features. The proposed method is evaluated on two real-world datasets, where distribution shifts are observed in virtual environments, and on graph-structured data with graph editers. The OOD solution is shown to be robust to cross-domain transfers, dynamic graph evolution and artificial spurious features."
4981,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"contrastive learning USED-FOR time series data. encoder USED-FOR robust and discriminative representations. augmentations USED-FOR contrastive learning. ad - hoc manual selection EVALUATE-FOR time series augmentations. prefabricated human priors USED-FOR rule of thumb. rule of thumb USED-FOR augmented samples. augmentations of time series data USED-FOR contrastive learning tasks. meta - learning mechanism USED-FOR information - aware approach. InfoTS HYPONYM-OF information - aware approach. meta - learner CONJUNCTION encoder. encoder CONJUNCTION meta - learner. classification task EVALUATE-FOR leading baselines. accuracy EVALUATE-FOR classification task. MSE EVALUATE-FOR forecasting task. accuracy EVALUATE-FOR leading baselines. datasets EVALUATE-FOR forecasting task. forecasting task EVALUATE-FOR leading baselines. datasets EVALUATE-FOR leading baselines. classification task EVALUATE-FOR forecasting task. datasets EVALUATE-FOR classification task. Method are contrastive learning approaches, information theory, and contrastive representation learning. Material is image and language domains. OtherScientificTerm is sub - optimal solutions. ","This paper studies the problem of contrastive learning for time series data. The authors propose an information-aware approach called InfoTS, which uses a meta-learning mechanism to learn robust and discriminative representations by augmentations to time series augmentations from ad-hoc manual selection. The augmentations are based on prefabricated human priors, and the rule of thumb is used to select augmented samples from the pre-trained meta-learner and encoder. The proposed approach is evaluated on a classification task and a forecasting task on MSE. The results show that InfoTS achieves sub-optimal solutions.","This paper presents a meta-learning approach to learn robust and discriminative representations from time series data. The authors propose an information-aware approach, InfoTS, which is based on the meta-learner and the encoder. The meta-learner learns a set of sub-optimal solutions for each time series, which are then used to train a meta learner. The encoder is trained to predict the robustness of the image and language domains. The proposed approach is evaluated on the classification task and the forecasting task on MSE. "
4997,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"winning tickets PART-OF task. theoretical physics FEATURE-OF renormalization group theory. iterative magnitude pruning USED-FOR winning tickets. method USED-FOR winning tickets. iterative magnitude pruning HYPONYM-OF method. iterative magnitude pruning HYPONYM-OF renormalization group scheme. numerical and theoretical tools USED-FOR winning ticket universality. winning ticket universality FEATURE-OF large scale lottery ticket experiments. Task are Lottery Ticket Hypothesis, and machine learning. Generic is tasks. ","This paper studies the Lottery Ticket Hypothesis. The authors propose a new method for winning tickets based on iterative magnitude pruning, a renormalization group scheme based on theoretical physics. They show that this method can achieve winning ticket universality in large scale lottery ticket experiments with numerical and theoretical tools. They also show that the proposed method can be applied to other tasks such as machine learning.","This paper proposes a novel method for generating winning tickets for a task, which is based on the Lottery Ticket Hypothesis. The authors propose a novel renormalization group theory based on theoretical physics. The proposed method, iterative magnitude pruning, is an extension of the original renormalized group scheme. The paper also proposes numerical and theoretical tools to improve the winning ticket universality of large scale lottery ticket experiments. Experiments are conducted on a variety of tasks, including machine learning."
5013,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"Transformer - based models USED-FOR information retrieval problem. BERT USED-FOR information retrieval problem. BERT HYPONYM-OF Transformer - based models. joint embedding USED-FOR cross - attention ( CA ) models. separate embeddings USED-FOR dual - encoder ( DE ) models. cross - attention ( CA ) models HYPONYM-OF models. DE models USED-FOR scores. real - world problems EVALUATE-FOR DE models. CA COMPARE DE models. DE models COMPARE CA. real - world problems EVALUATE-FOR CA. benchmark neural re - ranking datasets EVALUATE-FOR distillation strategy. Method are CA models, and cross - attention. ",This paper studies the information retrieval problem in BERT with Transformer-based models. The authors propose a joint embedding for cross-attention (CA) models and separate embeddings for dual-encoder (DE) models. They show that CA models achieve better scores than DE models on a variety of real-world problems. They also propose a distillation strategy on benchmark neural re-ranking datasets.,"This paper proposes a new model for the information retrieval problem, BERT, based on Transformer-based models. The authors propose a joint embedding for cross-attention (CA) models and dual-encoder (DE) models with separate embeddings. They show that CA models outperform DE models in terms of scores on real-world problems. They also propose a distillation strategy on benchmark neural re-ranking datasets. "
5029,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"Off - policy methods USED-FOR Policy Optimization ( PO ) algorithms. IS PART-OF Monte Carlo simulation. variance minimization approach USED-FOR IS. variance minimization approach PART-OF Monte Carlo simulation. behavioral distribution USED-FOR sampling. variance minimization USED-FOR performance improvement tool. variance minimization COMPARE off - policy learning. off - policy learning COMPARE variance minimization. PO algorithm USED-FOR Policy Optimization. variance minimization USED-FOR Policy Optimization. Optimal Policy Evaluation ( POPE ) USED-FOR Policy Optimization. small batch sizes FEATURE-OF robustness. Method is Importance Sampling ( IS ). OtherScientificTerm are behavioral policy, and trust region. Material is continuous RL benchmarks. ","This paper studies the problem of Importance Sampling (IS) in the context of Monte Carlo simulation, where the goal is to learn a behavioral policy that maximizes the performance of the agent. The authors propose a new algorithm called Policy Optimization (PO) based on the Optimal Policy Evaluation (POPE) algorithm. The main idea is to use the variance minimization approach in IS to improve the robustness of the policy.  The authors show that the performance improvement tool can be used as a variance minimisation for off-policy learning. ","This paper proposes a new method for improving the robustness of off-policy methods for Policy Optimization (PO) algorithms. The authors propose Importance Sampling (IS), which is a variance minimization approach to the IS in Monte Carlo simulation. The key idea of IS is to use the behavioral distribution for sampling from the behavioral policy to the trust region, and then use a PO algorithm to improve the robust performance of Policy Optimized by Optimal Policy Evaluation (POPE) on small batch sizes.  The authors also propose a performance improvement tool based on variance minimisation, which is an extension of the idea of the variance minimizing approach in the context of the IS. Experiments on continuous RL benchmarks show that the proposed method improves robustness in terms of the number of samples and the size of the sample set."
5045,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"Graph Neural Networks ( GNNs ) USED-FOR atomic simulations. Graph Neural Networks ( GNNs ) USED-FOR catalyst discovery. GNNs USED-FOR task. triplets CONJUNCTION quadruplets of atoms. quadruplets of atoms CONJUNCTION triplets. they USED-FOR higher - order interactions. higher - order interactions PART-OF graphs. method USED-FOR GNNs. method USED-FOR graphs. GPUs USED-FOR graphs. force MAE metric EVALUATE-FOR S2EF task. AFbT metric EVALUATE-FOR IS2RS task. force MAE metric EVALUATE-FOR graph - parallelized models. OtherScientificTerm is climate change. Generic is models. Method are Graph Parallelism, and DimeNet++ and GemNet models. ","Graph Neural Networks (GNNs) are used for atomic simulations. Graph Parallelism is a popular method for graph discovery. The authors propose to use GNNs to solve this task using graph-parallelized models, where the graph is composed of triplets and quadruplets of atoms, and they can be used to learn higher-order interactions in the graphs. They show that the proposed method is able to learn graphs with GPUs that are more efficient than DimeNet++ and GemNet models. They also provide a new force MAE metric for the S2EF task based on the AFbT metric. ","Graph Neural Networks (GNNs) are used for atomic simulations. The authors propose Graph Parallelism, a method to learn graphs with higher-order interactions in the triplets and quadruplets of atoms. GNNs are applied to this task, and the authors show that they can achieve better performance than DimeNet++ and GemNet models on the IS2RS task with the force MAE metric on the S2EF task. They also show that graph-parallelized models can achieve good performance on the AFbT metric."
5061,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"language models USED-FOR text generation tasks. meandering CONJUNCTION incoherent content. incoherent content CONJUNCTION meandering. incoherent content HYPONYM-OF structural flaws. meandering HYPONYM-OF structural flaws. Time Control ( TC ) HYPONYM-OF language model. latent stochastic process USED-FOR language model. representation USED-FOR TC. representation USED-FOR language model. stochastic process USED-FOR document plan. text infilling CONJUNCTION discourse coherence. discourse coherence CONJUNCTION text infilling. domain - specific methods COMPARE TC. TC COMPARE domain - specific methods. domain - specific methods CONJUNCTION fine - tuning GPT2. fine - tuning GPT2 CONJUNCTION domain - specific methods. fine - tuning GPT2 COMPARE TC. TC COMPARE fine - tuning GPT2. text domains EVALUATE-FOR fine - tuning GPT2. discourse coherence EVALUATE-FOR TC. text infilling EVALUATE-FOR TC. ordering CONJUNCTION text length consistency. text length consistency CONJUNCTION ordering. long text generation settings EVALUATE-FOR TC. TC USED-FOR text structure. OtherScientificTerm are stochastic process of interest, and latent plan. ","This paper studies the problem of language models for text generation tasks. The authors consider structural flaws such as meandering, incoherent content, and text infilling. To address these structural flaws, the authors propose Time Control (TC), a language model based on the latent stochastic process of interest. TC uses a representation to represent the language model as a representation of the language, and then uses this representation to learn a document plan from the latent plan.  The authors show that TC is able to learn the text structure in long text generation settings, and that fine-tuning GPT2 on text domains outperforms other domain-specific methods and discourse coherence.","This paper proposes a new language model called Time Control (TC) that is based on the latent stochastic process of interest. The authors show that TC is able to capture structural flaws such as meandering, incoherent content, and text infilling in text generation tasks. The paper also shows that TC can capture text structure and discourse coherence in text domains, and fine-tuning GPT2 outperforms other domain-specific methods. "
5077,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"representation of objects USED-FOR higher - level concepts. framework USED-FOR object - centric representation. single 2D images USED-FOR object - centric representation. object - centric models COMPARE model. model COMPARE object - centric models. model USED-FOR segmenting objects. network USED-FOR latent code space. geometric shape CONJUNCTION texture / color. texture / color CONJUNCTION geometric shape. supervision USED-FOR model. specificity FEATURE-OF object segmentation. predictive learning USED-FOR models. approach USED-FOR symbolic representation. OtherScientificTerm are prediction error of future sensory input, moving objects, latent causes, 3D environment, and clustering colors. Material is synthetic dataset. ","This paper proposes a framework for learning object-centric representation from single 2D images. The key idea is to learn a symbolic representation of objects that can be used for higher-level concepts such as geometric shape, texture/color, etc. The model is trained by predicting the prediction error of future sensory input from moving objects. The network is then used to map the latent code space to the 3D environment, and the model is used for segmenting objects based on the specificity of the object segmentation. The proposed approach is evaluated on a synthetic dataset and shows that the proposed model performs better than existing object-centric models in terms of predictive learning.","This paper proposes a framework for learning an object-centric representation of objects for higher-level concepts. The framework is based on single 2D images, where the object-centric representation is learned from a single image. The key idea is to use the prediction error of future sensory input to predict moving objects in the 3D environment. The model is trained in a latent code space, where a network is used to predict the latent causes of the moving objects. The proposed model is used for segmenting objects into geometric shape, texture/color, and clustering colors. The specificity of object segmentation is measured using predictive learning, and the model is evaluated on a synthetic dataset. The paper shows that the proposed approach is able to learn a symbolic representation of the objects."
5093,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,"traffic management CONJUNCTION public safety. public safety CONJUNCTION traffic management. Spatio - temporal ( ST ) prediction task USED-FOR traffic management. Spatio - temporal ( ST ) prediction task USED-FOR public safety. mobility forecasting USED-FOR traffic management. mobility forecasting HYPONYM-OF Spatio - temporal ( ST ) prediction task. spatial and temporal domains FEATURE-OF features. independent variables PART-OF latent representation. semantic factors FEATURE-OF independent variables. It USED-FOR mobility forecasting models. It USED-FOR spatial and temporal features. VAE - based architecture USED-FOR disentangled representation. real spatio - temporal data USED-FOR mobility forecasting. real spatio - temporal data USED-FOR disentangled representation. deep generative model USED-FOR latent representation. temporal dynamics CONJUNCTION spatially varying component. spatially varying component CONJUNCTION temporal dynamics. deep generative model USED-FOR reconstructions. non - informative features USED-FOR method. Task is mobility forecasting problems. Generic are they, and models. Method are dynamic and static components, and Disentangled representation learning. Material is spatio - temporal datasets. ","This paper studies the problem of disentangled representation learning in the context of mobility forecasting problems. The authors propose a new Spatio-temporal (ST) prediction task for traffic management and public safety called mobility forecasting. It is based on a VAE-based architecture, which is able to disentangle the spatial and temporal features in the latent representation from the independent variables in the semantic factors. It can then be used to train mobility forecasting models. The proposed method uses a deep generative model to learn a latent representation and reconstructions from the reconstructed features. The model is trained using both dynamic and static components, and the authors show that the proposed method can learn non-informative features from the reconstructions. ","This paper proposes a disentangled representation learning method for mobility forecasting. The authors propose a VAE-based architecture for disentangling the dynamic and static components of a latent representation. It is based on the Spatio-temporal (ST) prediction task for traffic management and public safety. It uses both spatial and temporal domains to learn the features of the latent representation and the semantic factors. It can be applied to mobility forecasting models and is shown to improve the performance of models. The proposed method uses a deep generative model to generate reconstructions of the spatial dynamics and the spatially varying component. The method also uses non-informative features to train the model. Experiments are conducted on a number of mobility forecasting problems. The results show that the proposed method is able to disentangle the temporal and spatial features, and is also able to learn real spatio-temporally data."
5109,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"deep learning framework USED-FOR probabilistic interpolation of irregularly sampled time series. temporal VAE architecture USED-FOR uncertainty. heteroscedastic output layer USED-FOR variable uncertainty. input layer CONJUNCTION temporal VAE architecture. temporal VAE architecture CONJUNCTION input layer. temporal VAE architecture CONJUNCTION heteroscedastic output layer. heteroscedastic output layer CONJUNCTION temporal VAE architecture. heteroscedastic output layer USED-FOR output interpolations. variable uncertainty FEATURE-OF output interpolations. input layer USED-FOR input observation sparsity. temporal VAE architecture PART-OF HeTVAE. input layer PART-OF HeTVAE. heteroscedastic output layer PART-OF HeTVAE. architecture COMPARE deep latent variable models. deep latent variable models COMPARE architecture. homoscedastic output layers USED-FOR deep latent variable models. Material is Irregularly sampled time series. Method are deep learning models, Heteroscedastic Temporal Variational Autoencoder ( HeTVAE ), and sparse and irregular sampling. OtherScientificTerm is input sparsity. ","This paper proposes a deep learning framework for probabilistic interpolation of irregularly sampled time series. The authors propose a Heteroscedastic Temporal Variational Autoencoder (HeTVAE), which uses a temporal VAE architecture with an input layer and a heteroscedastastic output layer to improve the uncertainty of output interpolations with variable uncertainty. The input layer is used for input observation sparsity and the output layer for sparse and irregular sampling. The proposed architecture is shown to outperform deep latent variable models on several datasets.","This paper proposes a deep learning framework for probabilistic interpolation of irregularly sampled time series. The authors propose a Heteroscedastic Temporal Variational Autoencoder (HeTVAE), which is a combination of a temporal VAE architecture with a hetero-parameterized input layer, and a heteroscedastastic output layer to capture the uncertainty of output interpolations with variable uncertainty. The input layer is used for input observation sparsity and the output layer is trained to capture input sparsity. Experiments show that the proposed architecture outperforms other deep latent variable models with homo-propagation of the input and output layers. "
5125,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"structure HYPONYM-OF computational graph. importance CONJUNCTION coherence. coherence CONJUNCTION importance. coherence HYPONYM-OF proxies. importance HYPONYM-OF proxies. statistical methods USED-FOR proxies. proxies USED-FOR partitionings. network weights CONJUNCTION correlations of activations. correlations of activations CONJUNCTION network weights. correlations of activations USED-FOR edges. spectrally clustering USED-FOR partitionings. network weights USED-FOR edges. ones HYPONYM-OF partitionings. weights USED-FOR partitionings. weights USED-FOR ones. graph - based partitioning USED-FOR modularity. graph - based partitioning USED-FOR deep neural networks. Method is neural network. OtherScientificTerm are functionality, and neurons. Task is non - runtime analysis. ","This paper studies the problem of partitioning a neural network into sub-graphs. The authors propose a graph-based partitioning method for partitioning the subgraph into subgraphs, where each subgraph is represented as a set of vertices, and each vertice is represented by a node in the graph. The partitioning is done by using a spectrally-clustering method, where the vertices are represented as nodes in a graph, and the edges are represented by nodes.  The authors show that the proposed method can be used to partition the graph in a non-time-consuming manner. ","This paper proposes a novel approach to partitioning a neural network into sub-graphs. The main idea is to use the structure of a computational graph as a proxy for the functionality of the neurons. The authors propose two statistical methods for partitioning the partitionings, namely importance and coherence, and use spectrally clustering for partitionings. The proposed partitionings are based on the network weights and correlations of activations for the edges and the ones for the rest of the nodes. They show that the proposed weights are better than the original ones in terms of modularity. They also show that graph-based partitioning improves modularity in deep neural networks. The paper is well-written and easy to follow, and the non-runtime analysis is clear."
5141,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"mobile devices FEATURE-OF speech - interactive features. lottery ticket hypothesis USED-FOR sparse subnetworks. lottery ticket hypothesis USED-FOR lightweight speech recognition models. noise FEATURE-OF speech. CTC CONJUNCTION RNN - Transducer, and Transformer models. RNN - Transducer, and Transformer models CONJUNCTION CTC. winning tickets COMPARE full models. full models COMPARE winning tickets. sparsity USED-FOR noise robustness. Method are Lightweight speech recognition models, and speech models. Generic are systems, and full model. Task is open - world personalization. OtherScientificTerm are structured sparsity, backbones, full model weights, and background noises. ",This paper proposes a lottery ticket hypothesis for lightweight speech recognition models with sparse subnetworks. The main idea is to use structured sparsity to improve the robustness of the system against noise in speech. The authors show that the lottery ticket can be used to train a full model with a small number of backbones. The paper also shows that the winning tickets outperform full models with sparsity. ,"The paper proposes a lottery ticket hypothesis for lightweight speech recognition models with sparse subnetworks with mobile devices. The paper shows that the structured sparsity improves the noise robustness to noise in speech. The authors also show that the full model weights are more robust to background noises. The main contribution of the paper is the open-world personalization of the systems. The experiments show that CTC and RNN-Transducer, and Transformer models outperform the full models."
5157,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"random weights USED-FOR Deep neural networks. skip connections CONJUNCTION Hadamard transforms. Hadamard transforms CONJUNCTION skip connections. skip connections USED-FOR ResNet architectures. Hadamard transforms USED-FOR ResNet architectures. batch normalization USED-FOR network training. random weights USED-FOR network initialization. image classification datasets EVALUATE-FOR ZerO. ImageNet HYPONYM-OF image classification datasets. OtherScientificTerm is stable signal propagation. Metric are variance, and reproducibility. Method are random weight initialization, and residual networks. ","This paper studies the problem of random weight initialization in deep neural networks. The authors propose to use skip connections and Hadamard transforms to train ResNet architectures with skip connections. They show that by using batch normalization, the network training with random weights can achieve better performance than network initialization with the same number of random weights. They also show that the variance of the random weights improves the reproducibility of the residual networks. Finally, they show that ZerO performs well on image classification datasets such as ImageNet.",This paper proposes to use random weights for Deep neural networks with skip connections and Hadamard transforms to improve the reproducibility of ResNet architectures. The authors propose to use batch normalization for network training. The variance of the random weight initialization depends on the number of residual networks. They show that random weights can be used for network initialization and for stable signal propagation. They evaluate ZerO on image classification datasets such as ImageNet and ImageNet.
5173,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,"minimax formulation USED-FOR backdoors. backdoors PART-OF poisoned model. minimax formulation USED-FOR poisoned model. clean data USED-FOR minimax formulation. backdoor removal PART-OF formulation. implicit hypergradient USED-FOR algorithm. robustness EVALUATE-FOR minimax. clean data USED-FOR minimax. I - BAU COMPARE state - ofart backdoor defenses. state - ofart backdoor defenses COMPARE I - BAU. state - ofart backdoor defenses USED-FOR backdoor attacks. I - BAU USED-FOR backdoor attacks. I - BAU COMPARE baseline. baseline COMPARE I - BAU. attack settings CONJUNCTION poison ratio. poison ratio CONJUNCTION attack settings. poison ratio CONJUNCTION clean data size. clean data size CONJUNCTION poison ratio. it COMPARE baseline. baseline COMPARE it. single - target attack setting EVALUATE-FOR baseline. single - target attack setting EVALUATE-FOR it. computation USED-FOR I - BAU. OtherScientificTerm is inner and outer optimization. Metric is convergence. Generic are its, and baselines. ","This paper proposes a minimax formulation for backdoors in a poisoned model. The formulation is based on backdoor removal in the form of an implicit hypergradient. The authors show that the minimax improves the robustness of the poisoned model by using clean data. They also show that I-BAU can defend against backdoor attacks in the single-target attack setting and against state-ofart backdoor defenses in the multi-target setting. Finally, the authors provide a computation for I- BAU and show its convergence. ",The paper proposes a minimax formulation for backdoors in a poisoned model. The formulation is based on backdoor removal with an implicit hypergradient. The authors show that the minimax is robust to backdoors with clean data. They also show that it is robust in the single-target attack setting and in the attack settings with a large poison ratio. The paper also shows convergence of the I-BAU to the state-of-art backdoor defenses. The computation is done by minimizing the inner and outer optimization. 
5189,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"random permutations COMPARE with - replacement sampling. with - replacement sampling COMPARE random permutations. permutations COMPARE random. random COMPARE permutations. smooth second derivatives FEATURE-OF 1 - dimensional strongly convex functions. random permutations USED-FOR strongly convex functions. easy - to - construct permutations USED-FOR accelerated convergence. easy - to - construct permutations COMPARE random. random COMPARE easy - to - construct permutations. easy - to - construct permutations USED-FOR quadratic, strongly - convex functions. convergence characterization USED-FOR optimal permutations. Method is permutation - based SGD. Metric is convergence gap. ","This paper studies the convergence of permutation-based SGD with smooth second derivatives of 1-dimensional strongly convex functions. The authors show that random permutations can converge faster than with-replacement sampling, while permutations with permutations are more efficient than random. They also show that easy-to-construct permutations lead to accelerated convergence for quadratic, strongly-convex functions, and that the convergence gap between the optimal permutations and random is larger than the one between the two. ","This paper proposes a new permutation-based SGD. The main idea is to combine random permutations with with-replacement sampling. The authors show that the permutations of 1-dimensional strongly convex functions with smooth second derivatives converge to the optimal permutations. They show accelerated convergence with easy-to-construct permutations for quadratic, strongly-convex functions. The convergence characterization is based on the convergence gap between optimal permutation and random."
5205,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,transformations USED-FOR flow models. method USED-FOR transformation layers. unique transformations USED-FOR transformation layers. mixed distribution USED-FOR architecture optimization. invertibility FEATURE-OF NF architecture. discrete space USED-FOR architecture optimization. global minimum FEATURE-OF approximate upper bound. approximate upper bound USED-FOR mixture NF. block - wise alternating optimization algorithm USED-FOR architecture optimization. block - wise alternating optimization algorithm USED-FOR deep flow models. architecture optimization USED-FOR deep flow models. Metric is performance - cost trade - offs. Method is flow architecture. ,"This paper proposes a new method for learning transformation layers for flow models with unique transformations. The main idea is to use a mixed distribution for architecture optimization with a block-wise alternating optimization algorithm for deep flow models. The authors show that the invertibility of NF architecture with the mixed distribution leads to a global minimum of the approximate upper bound for mixture NF. The paper also shows that the performance-cost trade-offs of the flow architecture can be reduced by using the block-wise alternating optimization method. Finally, the authors provide a theoretical analysis of the architecture optimization in a discrete space.",This paper proposes a method for learning transformation layers for flow models. The key idea is to use unique transformations for each of the transformation layers in the flow models to improve the performance-cost trade-offs. The paper also proposes a block-wise alternating optimization algorithm for the architecture optimization of deep flow models in the discrete space. The authors show that the invertibility of NF architecture can be improved by using a mixed distribution for architecture optimization. They also provide an approximate upper bound for mixture NF with a global minimum. 
5221,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,self - supervised learning ( SSL ) methods USED-FOR representations. Intrinsic Dimension ( ID ) USED-FOR representation space. Intrinsic Dimension ( ID ) USED-FOR expressiveness. KNN classifier USED-FOR Kmeans cluster labels. Kmeans cluster labels USED-FOR held - out representations. learning speed EVALUATE-FOR KNN classifier. KNN classifier USED-FOR Cluster Learnability ( CL ). learning speed EVALUATE-FOR Cluster Learnability ( CL ). contrastive losses CONJUNCTION pretext tasks. pretext tasks CONJUNCTION contrastive losses. ID CONJUNCTION CL. CL CONJUNCTION ID. model architecture CONJUNCTION human labels. human labels CONJUNCTION model architecture. data augmentation CONJUNCTION model architecture. model architecture CONJUNCTION data augmentation. ID USED-FOR downstream classification. CL USED-FOR downstream classification. CL COMPARE techniques. techniques COMPARE CL. ID COMPARE techniques. techniques COMPARE ID. contrastive losses USED-FOR techniques. pretext tasks USED-FOR techniques. DeepCluster USED-FOR representations. modification USED-FOR DeepCluster. ImageNet benchmarks EVALUATE-FOR DeepCluster. intermediate checkpoints USED-FOR SSL algorithms. framework USED-FOR SSL algorithms. framework USED-FOR intermediate checkpoints. Generic is architectures. ,This paper studies self-supervised learning (SSL) methods for learning representations from held-out representations. The authors propose Cluster Learnability (CL) based on Intrinsic Dimension (ID) to improve the expressiveness of the representation space. CL uses a KNN classifier to learn Kmeans cluster labels for the held-outs representations. CL improves the learning speed of an existing SSL classifier (KNN) by a factor of 2. The proposed framework also provides intermediate checkpoints for SSL algorithms. Experiments on ImageNet benchmarks show that CL outperforms existing techniques such as ID and CL in terms of contrastive losses and pretext tasks. ,"This paper proposes a new framework for self-supervised learning (SSL) methods to learn representations that are more expressive. Intrinsic Dimension (ID) is used to represent the representation space, and CL is used for expressiveness. The authors show that CL improves the learning speed of the KNN classifier for Kmeans cluster labels for held-out representations. They also show that the proposed framework can be applied to SSL algorithms with intermediate checkpoints. The proposed techniques include contrastive losses, pretext tasks, and data augmentation. DeepCluster is evaluated on ImageNet benchmarks, and the proposed techniques outperform ID and CL for downstream classification. The model architecture, human labels, and model architecture are also evaluated. "
5237,SP:4f5c00469e4425751db5efbc355085a5e8709def,"Deep neural networks USED-FOR adversarial examples. query efficiency EVALUATE-FOR black - box attacks. segmentation priors USED-FOR black - box attacks. salient region FEATURE-OF perturbations. query efficiency CONJUNCTION success rate. success rate CONJUNCTION query efficiency. imperceptibility performance EVALUATE-FOR blackbox attacks. segmentation priors USED-FOR blackbox attacks. Saliency Attack HYPONYM-OF gradient - free black - box attack. attacks USED-FOR perturbations. approach USED-FOR perturbations. approach USED-FOR detection - based defense. Task are blackbox setting, and adversarial attacks. Metric is imperceptibility. ","This paper studies the problem of adversarial examples in deep neural networks. The authors consider the blackbox setting, where the perturbations are in the salient region of the input, and the query efficiency of black-box attacks is measured in terms of query efficiency and success rate. They show that the imperceptibility performance of blackbox attacks with segmentation priors can be improved by the use of the attacks. They also show that this approach can be used to improve the detection-based defense against adversarial attacks. ","This paper studies adversarial examples in deep neural networks. The authors consider the blackbox setting, where adversarial attacks can be applied to any perturbations in the salient region of the input data. They show that the query efficiency and success rate of black-box attacks are correlated with the imperceptibility performance. They propose a novel approach for detection-based defense, which uses segmentation priors to detect blackbox attacks. They also propose a gradient-free black-Box attack, called Saliency Attack."
5253,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"Synthesis planning CONJUNCTION reaction outcome prediction. reaction outcome prediction CONJUNCTION Synthesis planning. Synthesis planning HYPONYM-OF computer - aided organic chemistry. reaction outcome prediction HYPONYM-OF computer - aided organic chemistry. Natural language approaches USED-FOR problem. Natural language approaches USED-FOR end - to - end formulation. Natural language approaches USED-FOR SMILES - to - SMILES translation. Transformer models USED-FOR text generation. Transformer models CONJUNCTION molecular graph encoders. molecular graph encoders CONJUNCTION Transformer models. molecular graph encoders USED-FOR Graph2SMILES model. Transformer models USED-FOR Graph2SMILES model. Graph2SMILES USED-FOR task. Transformer USED-FOR task. Graph2SMILES USED-FOR Transformer. molecule(s)-to - molecule(s ) transformations USED-FOR task. Graph2SMILES USED-FOR end - to - end architecture. global attention encoder USED-FOR long - range and intermolecular interactions. graph - aware positional embedding USED-FOR global attention encoder. top-1 accuracy EVALUATE-FOR Transformer baselines. Graph2SMILES COMPARE Transformer baselines. Transformer baselines COMPARE Graph2SMILES. reaction outcome prediction CONJUNCTION one - step retrosynthesis. one - step retrosynthesis CONJUNCTION reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Transformer baselines. USPTO_480k and USPTO_STEREO datasets USED-FOR reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Graph2SMILES. USPTO_50k dataset USED-FOR one - step retrosynthesis. top-1 accuracy EVALUATE-FOR Graph2SMILES. Method are data - driven approaches, machine translation model architectures, SMILES representations, SMILES augmentation, and encoder. Task is data preprocessing. OtherScientificTerm are molecular structures, input data augmentation, and local chemical environments. ","This paper proposes a novel end-to-end architecture, Graph2SMILES, for the task of synthesizing molecule(s)-to-molecule(s) transformations. Synthesis planning and reaction outcome prediction are two important problems in computer-assisted organic chemistry. Natural language approaches are used to solve this problem, and the authors propose a novel approach to solve the problem using Transformer models and molecular graph encoders. The authors show that the proposed model achieves better top-1 accuracy than the existing Transformer baselines, and is able to learn the molecular structures of the target molecule.  The authors also show that their model can be combined with other existing machine translation model architectures. ","This paper proposes a novel end-to-end architecture for SMILES translation. Synthesis planning is an important problem in computer-assisted organic chemistry. Natural language approaches are used to solve this problem, and the authors propose a novel approach to solve the problem. The proposed Graph2SMILES model is a combination of Transformer models and molecular graph encoders for text generation. The authors also propose a global attention encoder to capture long-range and intermolecular interactions between molecules, and a graph-aware positional embedding to capture the relationships between molecular structures. The main contribution of the paper is to propose a new architecture for data preprocessing, which is based on data-driven approaches. In particular, the authors introduce two new machine translation model architectures, which are based on the existing Transformer model and Graph2MILES. The paper also proposes a new end-of-end formulation, which uses Natural Language approaches to solve SMILes-to -SMILes translation. The key idea is to use SMILE representations as input data augmentation, and to use the input data augmentations in local chemical environments, where the input molecules are represented as a set of molecule(s)-to-molecule(s) transformations and the encoder is used to predict the molecular structures of the molecules. The results show that Graph2SmILES achieves better top-1 accuracy compared to Transformer baselines on both reaction outcome prediction and one-step retrosynthesis on the USPTO_50k dataset and on the PTO_20k dataset. The experimental results are also show that the Graph2smilES model performs better than the Transformer on the same task. "
5269,SP:ce3cde67564679a8d9a0539f1e12551390b91475,reinforcement learning ( RL ) methods USED-FOR task - oriented dialogues setting. task - oriented dialogues setting USED-FOR automatic disease diagnosis. reinforcement learning ( RL ) methods USED-FOR automatic disease diagnosis. RL tasks COMPARE action space. action space COMPARE RL tasks. action space FEATURE-OF disease diagnosis. approaches USED-FOR problem. hierarchical policy PART-OF dialogue policy learning. symptom checkers CONJUNCTION disease classifier. disease classifier CONJUNCTION symptom checkers. master model USED-FOR low level model. low level policy PART-OF high level policy. disease classifier PART-OF low level policy. symptom checkers PART-OF low level policy. master model PART-OF high level policy. hierarchical framework COMPARE systems. systems COMPARE hierarchical framework. accuracy CONJUNCTION symptom recall. symptom recall CONJUNCTION accuracy. symptom recall EVALUATE-FOR systems. accuracy EVALUATE-FOR systems. disease diagnosis EVALUATE-FOR systems. hierarchical framework USED-FOR disease diagnosis. symptom recall EVALUATE-FOR hierarchical framework. accuracy EVALUATE-FOR hierarchical framework. Task is offline consultation process. ,"This paper studies the problem of automatic disease diagnosis in the task-oriented dialogues setting in reinforcement learning (RL) methods in the context of the offline consultation process. The authors propose two approaches to solve this problem: 1) a hierarchical policy in dialogue policy learning, where a low level policy is trained with a master model and a high level policy trained with the symptom checkers and a disease classifier. 2) a low-level policy trained using a hierarchical framework with a hierarchical model. The paper shows that the proposed systems achieve better accuracy and symptom recall than existing systems in disease diagnosis. ","This paper proposes a new approach for automatic disease diagnosis using reinforcement learning (RL) methods in the task-oriented dialogues setting. The main idea is to use a hierarchical policy in dialogue policy learning, where a low level policy is trained with a master model and a high level policy trained with the symptom checkers and a disease classifier. The problem is formulated as an offline consultation process, where the goal is to find the best action space in the action space for the disease diagnosis. The paper shows that the proposed approaches can solve the problem better than existing approaches for the same problem. The proposed hierarchical framework is shown to improve the accuracy and symptom recall of the systems in disease diagnosis compared to other systems."
5285,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,Federated Learning ( FL ) USED-FOR ML training ecosystem. distributed training USED-FOR data privacy. edge devices USED-FOR distributed training. FL COMPARE centralized training. centralized training COMPARE FL. Addressing label deficiency PART-OF FL. framework USED-FOR algorithms. SSFL ) HYPONYM-OF self - supervised and personalized federated learning framework. centralized self - supervised learning methods USED-FOR FL setting. SimSiam networks COMPARE FedAvg algorithm. FedAvg algorithm COMPARE SimSiam networks. Ditto CONJUNCTION local fine - tuning. local fine - tuning CONJUNCTION Ditto. perFedAvg CONJUNCTION Ditto. Ditto CONJUNCTION perFedAvg. algorithms USED-FOR supervised personalization algorithms. algorithms USED-FOR self - supervised learning. supervised personalization algorithms USED-FOR self - supervised learning. local fine - tuning HYPONYM-OF self - supervised learning. perFedAvg HYPONYM-OF self - supervised learning. Ditto HYPONYM-OF self - supervised learning. personalization CONJUNCTION consensus. consensus CONJUNCTION personalization. Per - SSFL USED-FOR personalization. Per - SSFL HYPONYM-OF personalized federated self - supervised learning algorithm. distributed training system USED-FOR SSFL. distributed training system CONJUNCTION evaluation protocol. evaluation protocol CONJUNCTION distributed training system. evaluation protocol USED-FOR SSFL. supervised learning CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION supervised learning. unsupervised learning USED-FOR FL. evaluation accuracy EVALUATE-FOR unsupervised learning. synthetic non - I.I.D. dataset CONJUNCTION intrinsically non - I.I.D. dataset. intrinsically non - I.I.D. dataset CONJUNCTION synthetic non - I.I.D. dataset. training system USED-FOR synthetic non - I.I.D. dataset. evaluation accuracy EVALUATE-FOR supervised learning. supervised learning USED-FOR FL. CIFAR-10 USED-FOR synthetic non - I.I.D. dataset. batch size CONJUNCTION non-I.I.D.ness. non-I.I.D.ness CONJUNCTION batch,"This paper proposes Federated Learning (FL) as a federated learning framework for the ML training ecosystem.   The authors propose a self-supervised and personalized federated self supervised learning framework called Federated Self-Supervised Learning (FSFL).  The main idea of FL is to use edge devices as edge devices for distributed training to improve data privacy.  The FL framework is based on the idea of Addressing label deficiency in the FL setting, which is an important problem in ML training, and the authors propose two algorithms: perFedAvg and Ditto, which are supervised personalization algorithms that can be used to improve the performance of self- supervised learning and local fine-tuning. The authors show that the FedAvg algorithm outperforms SimSiam networks in terms of performance on CIFAR-10 as well as on synthetic non-I.I.D. dataset, and unsupervised learning outperforms supervised learning on the FL dataset. ","This paper proposes a federated learning (FL) framework for the ML training ecosystem. The FL framework is based on the self-supervised and personalized federated learnability framework (SSFL). The authors propose two algorithms: perFedAvg and Ditto, which are supervised personalization algorithms that are designed to address label deficiency in the FL setting. The authors also propose an evaluation protocol for SSFL and a distributed training system to improve the data privacy. Experiments are conducted on CIFAR-10, a synthetic non-I.I.D. dataset and on edge devices for distributed training with edge devices. The results show that FL outperforms centralized training and unsupervised learning in terms of evaluation accuracy. The paper also shows that the FedAvg algorithm outperforms SimSiam networks and the Ditto algorithm. "
5301,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"deep neural networks ( DNN ) CONJUNCTION partial differential equations ( PDEs ). partial differential equations ( PDEs ) CONJUNCTION deep neural networks ( DNN ). PDEs USED-FOR DNN architectures. PDEs USED-FOR ResNet - like DNN. adjustment operator USED-FOR DNN. adjustment operator USED-FOR ResNet - like DNN. adjustment operator USED-FOR PDEs. training method USED-FOR DNN models. PDEs theory USED-FOR training method. robustness EVALUATE-FOR training method. training method USED-FOR networks. PDEs USED-FOR networks. training method USED-FOR DNN. robustness EVALUATE-FOR DNN. generalization gap FEATURE-OF DNN. training method USED-FOR generalization gap. method USED-FOR DNN. DNN COMPARE baseline model. baseline model COMPARE DNN. method USED-FOR DNN. generalization EVALUATE-FOR DNN. OtherScientificTerm are neural network design space, overfitting, and adversarial perturbations. Method is neural network structures. ",This paper proposes a new training method for DNN models based on PDEs theory. The main idea is to learn a PDE operator for a ResNet-like DNN with an adjustment operator. The authors show that this training method can improve the robustness of the networks and reduce the generalization gap of the DNN. They also show that the proposed method can be used to train a DNN that is more robust than a baseline model.,This paper proposes a new training method for deep neural networks (DNN) and partial differential equations (PDEs) for DNN architectures. The training method is based on PDEs theory. The authors propose a ResNet-like DNN that uses an adjustment operator to adjust the weights of the DNN. They show that the proposed training method improves the generalization gap of DNN models in terms of robustness to adversarial perturbations. They also show that their method is more robust than a baseline model. 
5317,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,deep learning models USED-FOR emergence of language. emergent language USED-FOR task. simulated agents USED-FOR emergent language. language games FEATURE-OF emergence of language. expressivity FEATURE-OF emergent languages. expressivity FEATURE-OF emergent languages. contrastive loss COMPARE referential loss. referential loss COMPARE contrastive loss. Generic is languages. Metric is complexity. OtherScientificTerm is message type collapse. ,This paper studies the emergence of language from deep learning models in the context of emergent language. The authors consider the task of learning emergent languages from a set of simulated agents trained on a task where the goal is to learn a language that can be used in a variety of language games.  The authors show that the expressivity of the emergentanguages in terms of expressivity and the complexity of the learned languages can be measured by contrastive loss and referential loss. They also show that message type collapse can be a significant factor in the expressiveness of these languages. ,"This paper studies the emergence of language from deep learning models in language games. The authors show that emergent language can be used to solve a task in a language game, and that the complexity of emergent languages can be measured by their expressivity. They also show that the emergent Language can be learned from simulated agents. They show that contrastive loss is more robust to message type collapse than referential loss. "
5333,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"exploration - exploitation dilemma PART-OF reinforcement learning. them USED-FOR reinforcement learning setting. exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. bandit setting FEATURE-OF Uncertainty - based exploration strategies. method USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR method. exploration USED-FOR bandit problems. value predictions USED-FOR it. δ - exploration HYPONYM-OF exploration strategy. SAU USED-FOR sequential Reinforcement Learning scenario. bandits USED-FOR SAU. Deep Q - learning case EVALUATE-FOR δ - exploration. OtherScientificTerm are posterior distributions, and model posterior distributions. Task is generic sequential decision tasks. Method are reward models, and Reinforcement Learning. ","This paper studies the exploration-exploration dilemma in reinforcement learning in the bandit setting, where the goal is to learn reward models that perform well in a set of generic sequential decision tasks. Uncertainty-based exploration strategies are well-studied in this setting, but they are not well-suited for the reinforcement learning setting. The authors propose a method based on Sample Average Ununcertainty (SAU) to perform exploration and exploitation in this context. The proposed method, called SAU, is based on the idea that the posterior distributions of the model posterior distributions can be used to guide the exploration of the reward models.  The authors show that it is possible to use SAU in the sequential Reinforcement Learning scenario, and it can be applied to bandit problems with value predictions as well as in the Deep Q-learning case. ","This paper studies the exploration-exploitation dilemma in reinforcement learning. The authors propose two Uncertainty-based exploration strategies in the bandit setting. The first one is based on value predictions. The second one relies on posterior distributions. Both of them are useful in the reinforcement learning setting. However, the authors do not consider the sequential Reinforcement Learning scenario where the reward models are not available. To address this issue, they propose a new method that combines exploration with value predictions to solve bandit problems. The proposed method is called δ-exploration in the context of the Deep Q-learning case. "
5349,SP:2f6e266b03939c96434834579999707d3268c5d6,"spatio - temporal complexity CONJUNCTION continuity of videos. continuity of videos CONJUNCTION spatio - temporal complexity. deep learning era USED-FOR long video generation. implicit neural representations ( INRs ) USED-FOR continuous signal. generative adversarial network USED-FOR video generation. motion discriminator USED-FOR unnatural motions. INR - based video generator USED-FOR motion dynamics. video extrapolation CONJUNCTION non - autoregressive video generation. non - autoregressive video generation CONJUNCTION video extrapolation. long video synthesis CONJUNCTION video extrapolation. video extrapolation CONJUNCTION long video synthesis. datasets EVALUATE-FOR DIGAN. long video synthesis HYPONYM-OF datasets. video extrapolation HYPONYM-OF datasets. UCF-101 EVALUATE-FOR FVD score. 128×128 resolution FEATURE-OF 128 frame videos. FVD score EVALUATE-FOR DIGAN. 128 frame videos USED-FOR DIGAN. UCF-101 EVALUATE-FOR DIGAN. OtherScientificTerm are video distribution, 3D grids of RGB values, scale of generated videos, continuous dynamics, INRs of video, and space and time coordinates. Method is parameterized neural network. Material is long frame sequences. ","This paper studies the problem of long video generation in the deep learning era in the context of long frame sequences. The authors propose a generative adversarial network for video generation using implicit neural representations (INRs) to generate a continuous signal from the video distribution. The INR-based video generator is trained to capture the motion dynamics of the video generated by the INR. The video distribution is represented as 3D grids of RGB values, and the scale of generated videos is determined by the distance between the INRs of video and the space and time coordinates. The motion discriminator is used to detect unnatural motions. The proposed DIGAN is evaluated on three datasets: long video synthesis, video extrapolation, and non-autoregressive video generation. The FVD score of DIGan is shown on UCF-101 with 128 frame videos.","This paper proposes a new framework for long video generation in the deep learning era. The key idea is to use implicit neural representations (INRs) to represent the continuous signal in the video distribution. The authors propose a generative adversarial network for video generation, which is trained on 3D grids of RGB values. The INR-based video generator is trained to predict the motion dynamics of the video generated by a motion discriminator. The paper also proposes a parameterized neural network to learn the INRs of video, which can be applied to long frame sequences. Experiments are conducted on three datasets: long video synthesis, video extrapolation, and non-autoregressive video generation. The FVD score of DIGAN is shown on 128 frame videos with 128×128 resolution. "
5365,SP:878325384328c885ced7af0ebf31bbf79287c169,Private multi - winner voting USED-FOR revealing k - hot binary vectors. bounded differential privacy guarantee FEATURE-OF revealing k - hot binary vectors. task PART-OF machine learning literature. Binary HYPONYM-OF privacy - preserving multi - label mechanisms. Powerset voting HYPONYM-OF privacy - preserving multi - label mechanisms. composition USED-FOR Binary voting. ` 2 norm FEATURE-OF τ voting. binary vector USED-FOR Powerset voting. Powerset voting COMPARE Binary voting. Binary voting COMPARE Powerset voting. mechanisms USED-FOR privacy - preserving multi - label learning. canonical single - label technique USED-FOR mechanisms. PATE HYPONYM-OF canonical single - label technique. canonical single - label technique USED-FOR privacy - preserving multi - label learning. large real - world healthcare data CONJUNCTION multi - label benchmarks. multi - label benchmarks CONJUNCTION large real - world healthcare data. techniques COMPARE DPSGD. DPSGD COMPARE techniques. large real - world healthcare data EVALUATE-FOR DPSGD. multi - label benchmarks EVALUATE-FOR DPSGD. large real - world healthcare data EVALUATE-FOR techniques. multi - label benchmarks EVALUATE-FOR techniques. centralized setting EVALUATE-FOR techniques. mechanisms USED-FOR models. mechanisms USED-FOR multi - site ( distributed ) setting. multi - site ( distributed ) setting FEATURE-OF models. Material is healthcare. OtherScientificTerm is power set. Method is multi - label CaPC. ,"This paper studies the problem of private multi-winner voting for revealing k-hot binary vectors with bounded differential privacy guarantee in healthcare. The authors propose a new task in machine learning literature, where the goal is to learn a power set of labels for each patient in a multi-label CaPC. The paper proposes two privacy-preserving multi-labelling mechanisms, Binary and Powerset voting, which are based on the canonical single-label technique PATE. Binary voting is based on composition of a binary vector, and the authors show that the `2 norm of Binary voting can be computed as a function of the power set. The proposed mechanisms are shown to outperform DPSGD on large real-world healthcare data and multiple-label benchmarks in the multi-site (distributed) setting. ","The paper proposes a new task in machine learning literature, Private multi-winner voting for revealing k-hot binary vectors with bounded differential privacy guarantee. The paper proposes two privacy-preserving multi-label mechanisms, Binary and Powerset voting, which are based on composition of the power set. Binary voting is based on the `2 norm' of τ voting, where the `1 norm’ is a function of the ‘2 norm‘ of a binary vector. The authors also propose a canonical single-label technique called PATE, which is a variant of Binary voting. Experiments show that the proposed techniques outperform DPSGD on large real-world healthcare data and multi-labels benchmarks, and the proposed mechanisms outperform existing models in the multi-site (distributed) setting and in the centralized setting. "
5381,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"dataset CONJUNCTION regularization. regularization CONJUNCTION dataset. architecture CONJUNCTION optimizer. optimizer CONJUNCTION architecture. batch size CONJUNCTION dataset. dataset CONJUNCTION batch size. optimizer CONJUNCTION batch size. batch size CONJUNCTION optimizer. optimizer CONJUNCTION learning rate schedule. learning rate schedule CONJUNCTION optimizer. tuned optimizer COMPARE optimizer. optimizer COMPARE tuned optimizer. grafting USED-FOR non - adaptive learning rate correction. non - adaptive learning rate correction USED-FOR SGD. non - adaptive learning rate correction USED-FOR BERT model. Method are large neural networks, optimizer grafting, optimizer hyperparameter search, and deep learning. OtherScientificTerm are implicit step size schedule, and empirical performance. Task is optimizer comparisons. ","This paper studies the problem of learning large neural networks with large batch sizes. The authors propose a new architecture, an optimizer, a batch size, and a learning rate schedule. The main idea is to learn the implicit step size schedule of the optimizer and the learning rate for the batch size. The paper also proposes a non-adaptive learning rate correction for SGD, which can be applied to the BERT model. Experiments show that the proposed method outperforms the state-of-the-art.","This paper studies the problem of learning large neural networks. The authors propose a new architecture, an optimizer, a batch size, and a learning rate schedule. The main contribution of the paper is to introduce an implicit step size schedule, which can be used to improve the empirical performance of the optimizer compared to a tuned optimizer. The paper also proposes a non-adaptive learning rate correction for SGD, which is based on the idea of optimizer grafting. Experiments show that the proposed BERT model can outperform a tuned optimalizer and a regularized optimizer in terms of empirical performance. "
5397,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"RL algorithms USED-FOR policy. algorithm USED-FOR online RL. algorithm USED-FOR offline demonstration data. sub - optimal behavior policy USED-FOR offline demonstration data. sparse reward settings FEATURE-OF online RL. policy improvement step CONJUNCTION policy guidance step. policy guidance step CONJUNCTION policy improvement step. offline demonstration data USED-FOR policy guidance step. LOGO USED-FOR policy. algorithm USED-FOR incomplete observation setting. censored version of the true state observation PART-OF demonstration data. sparse rewards CONJUNCTION censored state. censored state CONJUNCTION sparse rewards. algorithm COMPARE approaches. approaches COMPARE algorithm. censored state FEATURE-OF benchmark environments. sparse rewards FEATURE-OF benchmark environments. benchmark environments EVALUATE-FOR approaches. benchmark environments EVALUATE-FOR algorithm. LOGO USED-FOR obstacle avoidance. LOGO USED-FOR trajectory tracking. trajectory tracking CONJUNCTION obstacle avoidance. obstacle avoidance CONJUNCTION trajectory tracking. mobile robot USED-FOR trajectory tracking. mobile robot USED-FOR obstacle avoidance. mobile robot USED-FOR LOGO. LOGO USED-FOR approach. Task is real - world reinforcement learning ( RL ). OtherScientificTerm are sparsity of reward feedback, sparse reward function, fine grain feedback, exploration actions, feedback, guidance, sub - optimal policy, and learning episode. Material is offline data. Generic is it. ","This paper proposes a new algorithm for offline demonstration data in sparse reward settings in online RL, where the sparsity of reward feedback is high. The algorithm is based on a sub-optimal behavior policy, which is learned by a policy improvement step and a policy guidance step. The policy is then used to learn a policy in the incomplete observation setting. The goal is to maximize the reward function of the sparse reward function, and the policy is trained by fine-grained exploration actions. The paper shows that the proposed algorithm can achieve better performance than existing approaches in several benchmark environments with sparse rewards and a censored version of the true state observation in the demonstration data. The proposed approach, called LOGO, is evaluated on trajectory tracking and obstacle avoidance using a mobile robot.","This paper proposes a new algorithm for offline demonstration data in sparse reward settings in online RL, where the sparsity of reward feedback is high. The algorithm is based on existing RL algorithms for learning a policy in an incomplete observation setting. The key idea is to use a sub-optimal behavior policy to guide the offline learning of the policy in the sparse reward function. The policy improvement step and the policy guidance step are based on fine grain feedback. The goal of the guidance is to encourage the policy to learn the optimal behavior in the censored version of the true state observation of the demonstration data. The authors show that the proposed algorithm outperforms existing approaches in several benchmark environments with sparse rewards and a censored state. The proposed approach is evaluated on a mobile robot for trajectory tracking and obstacle avoidance, and on a LOGO for learning the policy. "
5413,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"scalarization problem USED-FOR Pareto front. Linear Scalarization CONJUNCTION domain decomposition. domain decomposition CONJUNCTION Linear Scalarization. Multi - Task Learning ( MTL ) solvers USED-FOR Pareto solutions. Linear Scalarization PART-OF Multi - Task Learning ( MTL ) solvers. domain decomposition PART-OF Multi - Task Learning ( MTL ) solvers. Linear Scalarization USED-FOR Pareto solutions. MTL solvers USED-FOR real - world applications. non - convex functions CONJUNCTION constraints. constraints CONJUNCTION non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR constraints. Hybrid Neural Pareto Front ( HNPF ) HYPONYM-OF two stage Pareto framework. Stage-1 neural network USED-FOR weak Pareto front. Fritz - John Conditions ( FJC ) USED-FOR Stage-1 neural network. FJC guided diffusive manifold USED-FOR weak Pareto front. low - cost Pareto filter USED-FOR strong Pareto subset. strong Pareto subset PART-OF weak front. low - cost Pareto filter USED-FOR weak front. Method is Fixed - point iterative strategies. OtherScientificTerm are convexity assumptions, Pareto definition, and convexity. Task is benchmarking and verification. Generic is approach. ","This paper studies the scalarization problem of the Pareto front in the context of multi-task learning (MTL) solvers such as Linear Scalarization and domain decomposition. The authors propose a two stage Pareta framework, Hybrid Neural Paretopole Front (HNPF) and Stage-1 Neural Network (FJJ) for the weak Paret front. The HNPF is based on the FJC guided diffusive manifold, and the Stage-2 neural network uses the low-cost FJC-guided diffusive manifolds to learn the weak front.  The authors show that the proposed method is able to achieve state-of-the-art performance on benchmarking and verification tasks. ","This paper studies the scalarization problem of the Pareto front, which is an important problem in benchmarking and verification. The authors propose a two stage Paretop-based approach, Hybrid Neural Paretto Front (HNPF) and Hybrid Neural Front (HPF) to solve the convexity assumption. The main idea of the proposed approach is to use Fixed-point iterative strategies. The paper also proposes to use Linear Scalarization and domain decomposition in Multi-Task Learning (MTL) solvers to find Paredto solutions. The proposed method is evaluated on a variety of real-world applications where the proposed method outperforms existing MTL solvers.   "
5429,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"library of diverse expert models COMPARE single generalist model. single generalist model COMPARE library of diverse expert models. library of diverse expert models USED-FOR task. single generalist model USED-FOR task. related task - specific teachers USED-FOR recognition tasks. related task - specific teachers USED-FOR consolidated image feature representation. student model PART-OF knowledge distillation. downstream transferability EVALUATE-FOR task - agnostic generalist model. distillation of task - specific teachers COMPARE task - agnostic generalist model. task - agnostic generalist model COMPARE distillation of task - specific teachers. teacher representations USED-FOR distillation of task - specific teachers. generalist teacher USED-FOR representations. generalist teacher PART-OF task - specific teacher(s ). unlabeled proxy dataset USED-FOR multihead, multi - task distillation method. task - specific teacher(s ) USED-FOR representations. multi - task joint training oracle EVALUATE-FOR method. Generic is library. OtherScientificTerm are teacher, and ImageNet pre - trained features. ","This paper proposes a multi-task joint training oracle that distills the knowledge from a library of diverse expert models into a single generalist model for each task. The task-agnostic teacher is trained by distilling the student model into a knowledge distillation. The student model is then used to learn a consolidated image feature representation from the related task-specific teachers for recognition tasks. The teacher representations are then used as teacher representations for the distillation of the distilled representations from the generalist teacher. The proposed method is evaluated on the unlabeled proxy dataset and shows that the proposed multihead, multi-tasks distillation method outperforms the state-of-the-art.","This paper proposes a novel multi-task joint training oracle. The authors propose to use a library of diverse expert models for each task, and a single generalist model for the task. The student model is used for knowledge distillation, and related task-specific teachers are used for recognition tasks. The proposed method is evaluated on ImageNet pre-trained features, and is shown to improve downstream transferability. The teacher representations are used in the distillation of task -specific teachers, and the representations are learned by a generalist teacher (s). The authors also propose an unlabeled proxy dataset for the multihead, multi-tasks distillation method."
5445,SP:ab0d024d4060235df45182dab584c36db16d8e31,"Quantifying the data uncertainty USED-FOR learning tasks. valid coverage CONJUNCTION efficiency. efficiency CONJUNCTION valid coverage. valid coverage FEATURE-OF prediction sets. low length HYPONYM-OF efficiency. constrained empirical risk minimization ( ERM ) problem USED-FOR prediction set. empirical coverage FEATURE-OF prediction set. approximate valid population coverage CONJUNCTION near - optimal efficiency. near - optimal efficiency CONJUNCTION approximate valid population coverage. function class PART-OF conformalization step. meta - algorithm USED-FOR conformal prediction algorithms. near - optimal efficiency EVALUATE-FOR it. approximate valid population coverage EVALUATE-FOR it. it USED-FOR ERM problem. non - differentiable coverage constraint PART-OF it. differentiable surrogate losses CONJUNCTION Lagrangians. Lagrangians CONJUNCTION differentiable surrogate losses. gradient - based algorithm USED-FOR it. constrained ERM USED-FOR gradient - based algorithm. Lagrangians USED-FOR gradient - based algorithm. Lagrangians USED-FOR constrained ERM. differentiable surrogate losses USED-FOR gradient - based algorithm. differentiable surrogate losses USED-FOR constrained ERM. minimum - volume prediction sets CONJUNCTION label prediction sets. label prediction sets CONJUNCTION minimum - volume prediction sets. prediction intervals CONJUNCTION minimum - volume prediction sets. minimum - volume prediction sets CONJUNCTION prediction intervals. algorithm COMPARE approaches. approaches COMPARE algorithm. label prediction sets USED-FOR image classification. efficiency EVALUATE-FOR approaches. minimum - volume prediction sets USED-FOR multi - output regression. algorithm USED-FOR applications. approaches USED-FOR applications. image classification HYPONYM-OF applications. efficiency EVALUATE-FOR algorithm. prediction intervals HYPONYM-OF applications. minimum - volume prediction sets HYPONYM-OF applications. label prediction sets HYPONYM-OF applications. OtherScientificTerm is prediction interval. Method are Conformal prediction, and conformal prediction. ","This paper studies the problem of quantifying the data uncertainty in learning tasks. The authors consider the constrained empirical risk minimization (ERM) problem, where the prediction set is constrained by a non-differentiable coverage constraint, and the goal is to maximize the empirical coverage of a prediction set with low length. Conformal prediction is a popular meta-algorithm for conformal prediction algorithms, and this paper proposes a new algorithm that is able to achieve near-optimal efficiency, approximate valid population coverage, and efficiency with valid coverage. The proposed algorithm is based on differentiable surrogate losses, Lagrangians, and a gradient-based algorithm. The efficiency is shown to be better than existing approaches in three different applications: image classification, minimum-volume prediction sets, and label prediction sets for multi-output regression. ","Quantifying the data uncertainty in learning tasks is an important problem. Conformal prediction is a meta-algorithm that is used to train conformal prediction algorithms. The paper considers the constrained empirical risk minimization (ERM) problem, where the prediction set has empirical coverage and low length. The authors propose a conformalization step that is based on the function class of the prediction interval. It is a non-differentiable coverage constraint, and it can be solved by a constrained ERM problem with Lagrangians or a gradient-based algorithm with differentiable surrogate losses. The proposed algorithm is evaluated on three different applications: image classification, label prediction sets, and minimum-volume prediction sets for multi-output regression. The results show that the proposed algorithm achieves better efficiency and approximate valid population coverage compared to other approaches, and near-optimal efficiency compared to the valid coverage. "
5461,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"reinforcement learning based approach USED-FOR query object localization. ordinal metric learning USED-FOR exemplary set. exemplary set USED-FOR transferable reward signal. ordinal metric learning USED-FOR transferable reward signal. method COMPARE fine - tuning approaches. fine - tuning approaches COMPARE method. method USED-FOR test - time policy adaptation. annotated images USED-FOR fine - tuning approaches. corrupted MNIST CONJUNCTION CU - Birds. CU - Birds CONJUNCTION corrupted MNIST. CU - Birds CONJUNCTION COCO datasets. COCO datasets CONJUNCTION CU - Birds. COCO datasets EVALUATE-FOR approach. corrupted MNIST EVALUATE-FOR approach. OtherScientificTerm are reward signals, and transferable reward. ",This paper proposes a reinforcement learning based approach for query object localization. The proposed method is based on ordinal metric learning to learn the exemplary set for the transferable reward signal. The authors show that the proposed method can achieve better test-time policy adaptation than fine-tuning approaches based on annotated images. The experimental results on corrupted MNIST and CU-Birds demonstrate the effectiveness of the proposed approach.,"This paper proposes a reinforcement learning based approach for query object localization. The key idea is to learn a transferable reward signal using ordinal metric learning. The proposed method is evaluated on corrupted MNIST, CU-Birds, and COCO datasets and compared to fine-tuning approaches based on annotated images."
5477,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"Transformers USED-FOR vision tasks. feature matching CONJUNCTION stereo. stereo CONJUNCTION feature matching. object detection CONJUNCTION feature matching. feature matching CONJUNCTION object detection. them USED-FOR vision tasks. dense predictions USED-FOR vision tasks. object detection HYPONYM-OF vision tasks. feature matching HYPONYM-OF vision tasks. quadtree transformer USED-FOR attention. token pyramids USED-FOR quadtree transformer. flops reduction EVALUATE-FOR stereo matching. quadtree attention USED-FOR vision tasks. top-1 accuracy EVALUATE-FOR ImageNet classification. ScanNet USED-FOR feature matching. feature matching EVALUATE-FOR quadtree attention. flops reduction EVALUATE-FOR quadtree attention. feature matching HYPONYM-OF vision tasks. Metric are quadratic computational complexity, and computational complexity. Method is QuadTree Attention. OtherScientificTerm is attention scores. Task is COCO object detection. ","This paper studies the quadratic computational complexity of quadtree transformer for vision tasks with dense predictions. The authors propose to use token pyramids as the attention for the attention, and use them to perform vision tasks such as feature matching, stereo, and object detection. The paper shows that quadtree attention improves the top-1 accuracy for ImageNet classification and feature matching on ScanNet. ","This paper proposes QuadTree Attention, an extension of the quadratic computational complexity of attention. The idea is to use token pyramids as a quadtree transformer for attention, and use them for vision tasks such as object detection, feature matching, stereo, and dense predictions. Experiments on COCO object detection show that quadtree attention improves top-1 accuracy on ImageNet classification and feature matching on ScanNet. The paper also shows that the attention scores are more computationally efficient."
5493,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"reusable options USED-FOR unknown task distribution. options USED-FOR transfer learning. options CONJUNCTION state transitions. state transitions CONJUNCTION options. MI USED-FOR method. scalable approximation USED-FOR MI maximization. scalable approximation USED-FOR InfoMax Termination Critic ( IMTC ) algorithm. gradient ascent USED-FOR scalable approximation. gradient ascent USED-FOR MI maximization. extrinsic rewards CONJUNCTION intrinsic rewards. intrinsic rewards CONJUNCTION extrinsic rewards. IMTC USED-FOR diversity of learned options. IMTC USED-FOR quick adaptation. IMTC USED-FOR complex domains. Method are reinforcement learning, and mutual information ( MI ) based skill learning. OtherScientificTerm is reusable building blocks. Task is learning reusable options. ",This paper studies the problem of learning reusable options for unknown task distribution in reinforcement learning. The authors propose a scalable approximation for InfoMax Termination Critic (IMTC) algorithm based on the mutual information (MI) based skill learning framework. The proposed method is based on MI and uses gradient ascent to achieve MI maximization. The paper shows that IMTC improves the diversity of learned options and the intrinsic rewards in order to improve the transfer learning performance of options and state transitions.  The authors also show that the IMTC can be used for quick adaptation in complex domains. ,"This paper proposes a method to learn reusable options for unknown task distribution. The idea is to use mutual information (MI) based skill learning. The authors propose a scalable approximation to the InfoMax Termination Critic (IMTC) algorithm, which is based on MI. They show that the MI maximization can be achieved by gradient ascent. They also show that IMTC can be used for diversity of learned options, extrinsic rewards, and intrinsic rewards. Finally, they show that using IMTC for quick adaptation can be useful for complex domains. "
5509,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"Open - World Object Detection HYPONYM-OF object detection paradigm. openworld object detector USED-FOR unknown objects. independent modules USED-FOR unknown categories. independent modules USED-FOR incremental learning. node PART-OF semantic topology. discriminative feature representations USED-FOR constraint. semantic topology COMPARE open - world object detectors. open - world object detectors COMPARE semantic topology. semantic topology USED-FOR open - world object detection. well - trained language model USED-FOR semantic topology. OtherScientificTerm are Semantic Topology, and features. Method are open - world object detector, and detector. Metric is absolute open - set error. ","This paper proposes a new object detection paradigm called Open-World Object Detection (OWOD), which uses an open-world object detector to detect unknown objects. The key idea is to use independent modules to learn the unknown categories, and then use these independent modules for incremental learning. Semantic Topology is a node in the semantic topology of the detector, and the discriminative feature representations are used to enforce a constraint on the open-set error. The authors show that open-World object detectors are able to achieve better performance in terms of absolute open-sets error compared to the state-of-the-art semantic topologies. The paper also shows that the well-trained language model can be used to learn semantic topological representations for open- world object detection.","This paper proposes a new object detection paradigm called Open-World Object Detection (OWOD), which is based on Semantic Topology. The main idea is to use an openworld object detector to detect unknown objects from a set of independent modules for unknown categories. The key idea of OOD is to learn a node in the semantic topology of the object detector, which is then used for incremental learning. Semantic topology is learned using a well-trained language model, and the constraint is enforced by discriminative feature representations. The paper shows that open-world object detectors are more robust to the absolute open-set error, and that the features learned by the detector are more likely to be close to the original object. The authors also show that semantic topologies are better than those learned by open-World object detectors."
5525,SP:97f618558f4add834e5930fd177f012a753247dc,Deep learning USED-FOR vision and natural language processing. computation CONJUNCTION human labeling effort. human labeling effort CONJUNCTION computation. deep learning models COMPARE ones. ones COMPARE deep learning models. dataset USED-FOR ones. Prior methods USED-FOR submodular objective functions. predicted class labels CONJUNCTION decision boundaries. decision boundaries CONJUNCTION predicted class labels. balancing constraints FEATURE-OF predicted class labels. balancing constraints FEATURE-OF decision boundaries. matroids HYPONYM-OF algebraic structure. algebraic structure USED-FOR linear independence. vector spaces FEATURE-OF linear independence. constant approximation guarantees FEATURE-OF greedy algorithm. matroids USED-FOR constraints. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. ImageNet CONJUNCTION long - tailed datasets. long - tailed datasets CONJUNCTION ImageNet. long - tailed datasets HYPONYM-OF classification datasets. CIFAR-100 - LT HYPONYM-OF long - tailed datasets. long - tailed datasets EVALUATE-FOR baselines. ImageNet HYPONYM-OF classification datasets. CIFAR10 HYPONYM-OF classification datasets. classification datasets EVALUATE-FOR baselines. CIFAR-100 HYPONYM-OF classification datasets. Generic is models. ,"This paper studies the problem of deep learning for vision and natural language processing. Prior methods for submodular objective functions are limited to a single dataset. The authors propose a new dataset for training deep learning models that can be combined with human labeling effort. They show that these new models can achieve better performance than existing ones. They also show that their greedy algorithm has constant approximation guarantees with respect to the number of iterations.    The authors also provide theoretical guarantees for the linear independence of linear independence in vector spaces under the algebraic structure of matroids. They provide a theoretical guarantee for the constraints of the predicted class labels and decision boundaries under the balancing constraints. Finally, they show that the new models are able to perform better than the existing ones on several classification datasets (CIFAR-100, CIFAR10, ImageNet, and long-tailed datasets).","This paper proposes a new dataset for learning submodular objective functions in deep learning for vision and natural language processing. Prior methods do not allow for submodularity objective functions. The authors show that deep learning models trained on this dataset outperform ones trained on the original dataset. The main contribution of the paper is to propose a greedy algorithm with constant approximation guarantees in the vector spaces. The proposed constraints are based on the algebraic structure of matroids, which allows for linear independence in vector spaces, and balancing constraints on the predicted class labels and decision boundaries. Experiments are conducted on classification datasets such as CIFAR-10, ImageNet, and long-tailed datasets. "
5541,SP:e0432ff922708c6c6e59124d27c1386605930346,"models USED-FOR semantic segmentation. adaptive inference strategy USED-FOR semantic segmentation. Instance - adaptive Batch Normalization ( IaBN ) USED-FOR normalization layers. feature statistics USED-FOR normalization layers. test - time training ( TTT ) approach USED-FOR semantic segmentation. Seg - TTT HYPONYM-OF test - time training ( TTT ) approach. self - supervised loss USED-FOR Seg - TTT. self - supervised loss USED-FOR model parameters. techniques COMPARE baseline. baseline COMPARE techniques. accuracy EVALUATE-FOR generalization methods. techniques COMPARE generalization methods. generalization methods COMPARE techniques. generalization USED-FOR semantic segmentation. accuracy EVALUATE-FOR techniques. Metric is Out - of - distribution robustness. Generic are model, and complementary techniques. ","This paper proposes a new adaptive inference strategy for semantic segmentation based on Instance-adaptive Batch Normalization (IaBN) for normalization layers based on feature statistics. The proposed test-time training (TTT) approach, Seg-TTT, is based on the self-supervised loss to learn the model parameters. The authors show that the proposed techniques achieve better accuracy than the baseline while maintaining Out-of-distribution robustness. ","This paper proposes a new adaptive inference strategy for semantic segmentation. The authors propose a test-time training (TTT) approach, Seg-TTT, which is based on Instance-adaptive Batch Normalization (IaBN) to normalize layers based on feature statistics. The main idea is to use a self-supervised loss to optimize the model parameters. Out-of-distribution robustness is achieved by using complementary techniques. Experimental results show that the proposed techniques outperform the baseline in terms of accuracy and generalization to semantic segmentations."
5557,SP:427100edad574722a6525ca917e84f817ff60d7e,contrastive loss USED-FOR mappings. mappings USED-FOR contrastive loss. Material is tabular data. Generic is method. Method is default set rule of hyperparameters selection. ,"This paper proposes a new method for hyperparameters selection in tabular data. The proposed method is based on contrastive loss for mappings between mappings. The main idea is to use the default set rule of hyperparameter selection to select the best mappings from a set of mappings, and then use the mappings to select mappings with the best performance. The authors show that the proposed method outperforms existing methods.",This paper proposes a contrastive loss for mappings in tabular data. The proposed method is based on the default set rule of hyperparameters selection. 
5573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,dimensional approach USED-FOR psychiatric classification. mining corresponded nosological relations USED-FOR low - dimensional embedding space. diagnostic information USED-FOR optimal embedding space. dual utilisation of diagnostic information PART-OF conditional variational auto - encoder. major depressive disorder CONJUNCTION schizophrenia. schizophrenia CONJUNCTION major depressive disorder. autism spectrum disorder CONJUNCTION major depressive disorder. major depressive disorder CONJUNCTION autism spectrum disorder. approaches USED-FOR synthetic functional connectivity features. autism spectrum disorder HYPONYM-OF nosological relation. major depressive disorder HYPONYM-OF nosological relation. empirical neuropsychiatric neuroimaging datasets EVALUATE-FOR approach. OtherScientificTerm is neuropsychiatric disorders. ,"This paper proposes a new dimensional approach for psychiatric classification based on mining corresponded nosological relations in the low-dimensional embedding space. The authors propose a conditional variational auto-encoder that uses dual utilisation of diagnostic information in order to find the optimal embedding for each patient. The proposed approach is evaluated on a variety of empirical neuropsychiatric neuroimaging datasets. The results show that the proposed approaches are able to recover synthetic functional connectivity features for a variety (i.e., major depressive disorder, schizophrenia, autism spectrum disorder) and can be applied to a wide range of neuropsychiatiatric disorders.","This paper proposes a novel dimensional approach for psychiatric classification based on mining corresponded nosological relations in the low-dimensional embedding space. The authors propose a conditional variational auto-encoder based on the dual utilisation of diagnostic information in the optimal embeddingspace. The proposed approach is evaluated on two empirical neuropsychiatric neuroimaging datasets, where it is shown that the proposed approach outperforms existing approaches in terms of synthetic functional connectivity features. "
5589,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,quantum neural networks USED-FOR quantum learning tasks. trainable quantum tensor network ( QTN ) USED-FOR quantum embedding. variational quantum circuit ( VQC ) USED-FOR quantum embedding. QTN - VQC HYPONYM-OF end - to - end learning framework. parametric tensor - train network CONJUNCTION tensor product encoding. tensor product encoding CONJUNCTION parametric tensor - train network. tensor product encoding USED-FOR quantum embedding. parametric tensor - train network USED-FOR feature extraction. architecture USED-FOR QTN. parametric tensor - train network PART-OF architecture. parametric tensor - train network PART-OF QTN. tensor product encoding PART-OF QTN. tensor product encoding PART-OF architecture. QTN USED-FOR quantum embedding. QTN USED-FOR end - to - end parametric model pipeline. QTN - VQC HYPONYM-OF end - to - end parametric model pipeline. QTN USED-FOR quantum embedding. MNIST dataset EVALUATE-FOR QTN. QTN COMPARE quantum embedding approaches. quantum embedding approaches COMPARE QTN. ,"This paper proposes a new end-to-end learning framework called QTN-VQC, which uses a trainable quantum tensor network (QTN) to perform quantum embedding using a variational quantum circuit (VQ). The architecture consists of a parametric tensor-train network, a tensor product encoding, and a QTN for feature extraction. The authors show that QTN is able to achieve state-of-the-art performance on the MNIST dataset. ","This paper proposes a novel end-to-end learning framework, QTN-VQC, for quantum learning tasks. The authors propose a trainable quantum tensor network (QTN) for quantum embedding, which is a variational quantum circuit (vQC). The architecture of QTN consists of a parametric tensor-train network and a tensor product encoding to encode the quantum embeddings. The QTN is used in an end- to-end parametric model pipeline, and is evaluated on the MNIST dataset. The experimental results show that QTN outperforms other state-of-the-art QTN for quantum encoding and feature extraction. "
5605,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"networks USED-FOR task. networks USED-FOR algorithms. neural networks USED-FOR high - level computational processes. algorithm USED-FOR low - dimensional manifolds. meta - model USED-FOR hidden states. meta - model USED-FOR DYNAMO. model PART-OF meta - model. pre - trained neural networks USED-FOR DYNAMO. model embedding vectors USED-FOR manifold. model embedding vector USED-FOR model. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. DYNAMO USED-FOR RNNs. DYNAMO USED-FOR CNNs. model embedding spaces USED-FOR applications. neural networks USED-FOR operable neural network. model embedding spaces USED-FOR clustering of neural networks. neural networks USED-FOR task. high - level computational processes USED-FOR clustering of neural networks. semi - supervised learning HYPONYM-OF applications. clustering of neural networks HYPONYM-OF applications. optimization USED-FOR semi - supervised learning. high - level computational processes FEATURE-OF topology of RNN dynamics. RNNs USED-FOR fixed - point analysis of meta - models. Method are deep neural networks, and neural network model. Generic is models. OtherScientificTerm are reparameterization, and model embedding space. ","This paper proposes a new algorithm for learning low-dimensional manifolds. The main idea is to use deep neural networks to solve the task. The authors propose DYNAMO, a meta-model that learns the hidden states of the manifold using a pre-trained neural networks. The model embedding vectors for the manifold are learned by reparameterization, and then the model is used to train a neural network model to predict the hidden state. The proposed algorithm is evaluated on a variety of applications, including semi-supervised learning, clustering of neural networks, and high-level computational processes in the topology of RNN dynamics. The experiments show that the proposed method outperforms existing RNNs and CNNs. ","This paper proposes a new algorithm for learning low-dimensional manifolds. The main idea is to use deep neural networks for the task. The authors propose two algorithms: DYNAMO, which uses a meta-model to model hidden states of the manifold, and a model embedding vector to represent the manifold. The model is trained using pre-trained neural networks, and the authors show that the proposed algorithms outperform existing models in terms of reparameterization. The paper also shows that the model is able to predict the topology of RNN dynamics with respect to high-level computational processes for clustering of neural networks and for applications such as semi-supervised learning, where the optimization of a neural network model is performed using neural networks. The experiments are conducted on RNNs and CNNs, and on fixed-point analysis of meta-models."
5621,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"constraint - based approach COMPARE direct prediction. direct prediction COMPARE constraint - based approach. direct prediction USED-FOR simulation engines. constraint - based approach USED-FOR simulation engines. framework USED-FOR constraint - based learned simulation. trainable function approximator USED-FOR scalar constraint function. gradient descent USED-FOR constraint solver. graph neural network USED-FOR constraint function. graph neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION graph neural network. gradient descent USED-FOR method. graph neural network USED-FOR method. backpropagation USED-FOR architecture. colliding irregular shapes CONJUNCTION splashing fluids. splashing fluids CONJUNCTION colliding irregular shapes. bouncing balls CONJUNCTION colliding irregular shapes. colliding irregular shapes CONJUNCTION bouncing balls. simulated ropes CONJUNCTION bouncing balls. bouncing balls CONJUNCTION simulated ropes. physical domains EVALUATE-FOR model. colliding irregular shapes HYPONYM-OF physical domains. splashing fluids HYPONYM-OF physical domains. simulated ropes HYPONYM-OF physical domains. bouncing balls HYPONYM-OF physical domains. model COMPARE simulators. simulators COMPARE model. forward learned simulators USED-FOR constraint - based framework. numerical methods USED-FOR learned models. Method are physical simulators, forward model, and forward approaches. Task is constraint satisfaction problem. Metric is simulation accuracy. OtherScientificTerm is hand - designed constraints. ","This paper proposes a framework for constraint-based learned simulation. The authors consider the constraint satisfaction problem, where the goal is to find a solution that satisfies a set of hand-designed constraints. The constraint solver is based on gradient descent with a graph neural network and gradient descent is used to learn the constraint solvers.  The authors show that the proposed method can achieve state-of-the-art performance on three physical domains: colliding irregular shapes, bouncing balls, and splashing fluids. ","This paper proposes a framework for constraint-based learned simulation. The authors propose a new constraint solver based on gradient descent and a graph neural network. The proposed method is based on backpropagation, and the authors show that the proposed method can achieve better simulation accuracy than direct prediction for simulation engines. They also show that their method can be applied to physical simulators. The paper also shows that their model outperforms other simulators on several physical domains (e.g. colliding irregular shapes, splashing fluids, bouncing balls, and simulated ropes). "
5637,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"exploration CONJUNCTION few - shot adaptation. few - shot adaptation CONJUNCTION exploration. policies USED-FOR exploration. diverse behaviors FEATURE-OF policies. controlling robots HYPONYM-OF real - world scenarios. iterative reproduction CONJUNCTION selection of policies. selection of policies CONJUNCTION iterative reproduction. iterative reproduction PART-OF evolutionary techniques. selection of policies USED-FOR methods. iterative reproduction USED-FOR methods. evolutionary techniques USED-FOR methods. EDO - CS HYPONYM-OF Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR EDO - CS. EDO - CS COMPARE methods. methods COMPARE EDO - CS. EDO - CS USED-FOR policies. continuous control tasks EVALUATE-FOR EDO - CS. EDO - CS COMPARE EDO - CS. EDO - CS COMPARE EDO - CS. Method is Reinforcement Learning ( RL ). Generic is task. OtherScientificTerm are selection mechanisms, and clusters. Task are reproduction, and reproduction process. ","This paper studies the problem of Reinforcement Learning (RL) where the goal is to learn a set of policies for exploration and few-shot adaptation. The authors propose a new task, called Evolutionary Diversity Optimization (EDO-CS), which is based on Clusteringbased Selection. The main idea is to select policies that have diverse behaviors in the environment, and then use iterative reproduction and selection of policies to learn these policies. The paper shows that EDO- CS outperforms existing methods on a variety of continuous control tasks.","This paper proposes a new task of Reinforcement Learning (RL) where the goal is to learn a set of policies that can be used for exploration and few-shot adaptation. The authors propose two methods: iterative reproduction and selection of policies based on evolutionary techniques. The selection mechanisms are based on Clusteringbased Selection, which is a variant of the Evolutionary Diversity Optimization algorithm. Experiments are conducted on real-world scenarios such as controlling robots, where diverse behaviors of policies can be observed. The paper shows that EDO-CS outperforms other methods on continuous control tasks, and that the selection of the policies is more efficient than other methods. "
5653,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"Markovian noise CONJUNCTION general consensus - type interaction. general consensus - type interaction CONJUNCTION Markovian noise. general consensus - type interaction USED-FOR multi - agent linear stochastic approximation algorithm. Markovian noise USED-FOR multi - agent linear stochastic approximation algorithm. time - varying directed graph USED-FOR interconnection structure. doubly stochastic matrices USED-FOR interconnection. finite - time bounds USED-FOR mean - square error. interaction matrices FEATURE-OF uniformly strongly connected graph sequences. stochastic matrices USED-FOR consensus - type algorithm. OtherScientificTerm are local stochastic approximation process, interconnection matrix, ordinary differential equation, interconnection matrices, local equilibria, communication, constant and time - varying step - sizes, and convex combination. Method are consensus - based stochastic approximation algorithms, and push - type distributed stochastic approximation algorithm. Generic is algorithm. ","This paper proposes a new consensus-based stochastic approximation algorithms. The main idea is to use Markovian noise and general consensus-type interaction as the interconnection matrix in a multi-agent linear stochastically approximation algorithm. The interconnection structure is modeled as a time-varying directed graph, where the local equilibria are defined as a local function of the global function. The local function is defined as an ordinary differential equation, and the local function can be represented as a convex combination of the local and global functions. The authors show that under certain assumptions on the interconnectivity matrices of uniformly strongly connected graph sequences with interaction matrices, the mean-square error of the proposed algorithm is bounded by finite-time bounds.  The authors also provide some theoretical guarantees for the convergence of their algorithm. ","The paper proposes a multi-agent linear stochastic approximation algorithm with Markovian noise and general consensus-type interaction. The main idea is to learn a time-varying directed graph with an interconnection structure. The interconnection matrices are doubly stochastically matrices, where the interconnection matrix is defined as the local equilibria between two agents, and the local convex approximation process is performed on top of the local differential equation. The authors provide finite-time bounds for the mean-square error of the algorithm, and show that the algorithm is equivalent to a push-type distributed stochatic approximation algorithm. They also show that their algorithm is more robust to the interaction matrices of uniformly strongly connected graph sequences, and that the communication between agents can be reduced to constant and time-deterministic step-size."
5669,SP:f7f96d545a907887396393aba310974f4d3f75ff,"ones HYPONYM-OF methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR ones. forward kinematics information FEATURE-OF structural object. it USED-FOR forward kinematics information. generalized coordinates USED-FOR forward kinematics information. generalized coordinates USED-FOR it. forward kinematics USED-FOR geometrical constraints. dynamics of constrained systems COMPARE unconstrained counterparts. unconstrained counterparts COMPARE dynamics of constrained systems. equivariant message passing USED-FOR GMN. orthogonality - equivariant functions USED-FOR equivariant message passing. molecular dynamics prediction CONJUNCTION human motion capture. human motion capture CONJUNCTION molecular dynamics prediction. sticks CONJUNCTION hinges. hinges CONJUNCTION sticks. particles CONJUNCTION sticks. sticks CONJUNCTION particles. prediction accuracy CONJUNCTION constraint satisfaction. constraint satisfaction CONJUNCTION prediction accuracy. GMN COMPARE GNNs. GNNs COMPARE GMN. constraint satisfaction CONJUNCTION data efficiency. data efficiency CONJUNCTION constraint satisfaction. real - world datasets USED-FOR molecular dynamics prediction. real - world datasets USED-FOR human motion capture. particles PART-OF simulated systems. sticks PART-OF simulated systems. simulated systems EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GNNs. data efficiency EVALUATE-FOR GMN. data efficiency EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GMN. Task are machine learning, and interacting systems. OtherScientificTerm is constrained systems. Method are Graph Mechanics Network ( GMN ), and equivariant formulation. ","This paper studies the problem of graph mechanics network (GMN) in the context of molecular dynamics prediction and human motion capture. The authors propose two methods, namely, ones based on equivariant Graph Neural Networks (GNNs) and one based on orthogonality-equivariant functions. The main idea is to use the forward kinematics information of a structural object to represent the dynamics of constrained systems, and then use the generalized coordinates to encode the geometrical constraints on it. Theoretically, the authors show that GMN can achieve better prediction accuracy, constraint satisfaction, and data efficiency than GNNs on simulated systems with particles, sticks, and hinges. ","This paper proposes a new method for learning constrained systems in the context of machine learning. The authors propose a Graph Mechanics Network (GMN), which is an equivariant formulation of GNNs. The main idea is to use orthogonality-equivariant functions to learn the dynamics of constrained systems compared to their unconstrained counterparts. Experiments show that GMN is able to achieve better prediction accuracy, constraint satisfaction, and data efficiency compared to other methods such as ones. The paper also shows that equivariance of message passing can be achieved by equivariants of orthogonal functions, which can be applied to any GNN. Experimental results on simulated systems and real-world datasets for molecular dynamics prediction and human motion capture show the effectiveness of GMN. "
5685,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"framework USED-FOR federated learning. partial model personalization USED-FOR federated learning. partial model personalization USED-FOR framework. full model personalization COMPARE partial model personalization. partial model personalization COMPARE full model personalization. domain knowledge USED-FOR model. domain knowledge USED-FOR partial model personalization. federated optimization algorithms USED-FOR partially personalized models. them USED-FOR deep learning models. algorithms USED-FOR minimizing smooth nonconvex functions. alternating update algorithm COMPARE simultaneous update algorithm. simultaneous update algorithm COMPARE alternating update algorithm. partial model personalization USED-FOR full model personalization. personalized parameters USED-FOR partial model personalization. personalized parameters USED-FOR full model personalization. OtherScientificTerm are on - device memory footprint, shared and personal parameters, shared parameters, and smooth nonconvex functions. Material is real - world image and text datasets. ","This paper proposes a new framework for federated learning with partial model personalization. The proposed framework is based on the idea that the on-device memory footprint of a model is shared across all clients, and that the shared and personal parameters can be used to improve the performance of the model. The authors propose two federated optimization algorithms for partially personalized models, one of which uses domain knowledge and the other one that uses personalized parameters. Both algorithms are based on minimizing smooth nonconvex functions, and both of them are shown to perform well on real-world image and text datasets. The main contribution of the paper is to show that the proposed alternating update algorithm performs better than the existing simultaneous update algorithm. ","This paper proposes a framework for federated learning with partial model personalization. The model is trained with domain knowledge, and the shared and personal parameters are computed separately. The authors propose two federated optimization algorithms for partially personalized models, one for minimizing smooth nonconvex functions and the other for minimizing them for deep learning models. Experiments are conducted on real-world image and text datasets. The proposed alternating update algorithm outperforms the conventional simultaneous update algorithm. "
5701,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"contrastive learning HYPONYM-OF unsupervised representation learning. contrastive learning PART-OF machine learning. augmentation operations USED-FOR representations. framework USED-FOR unsupervised learning of object representations. CLTT USED-FOR viewing sequences. CLTT USED-FOR supervised learning. ThreeDWorld ( TDW ) USED-FOR near - photorealistic training environment. ThreeDWorld ( TDW ) USED-FOR data set. near - photorealistic training environment USED-FOR data set. CLTT USED-FOR linear classification. OtherScientificTerm are supervision, positive pairs, temporal structure, and representational similarity. Material are biology, and natural videos. Method are object representations, Contrastive Learning Through Time ( CLTT ), contrastive learning methods, and fully supervised setting. Generic is data sets. ","This paper proposes a framework for unsupervised learning of object representations in the context of contrastive learning in machine learning, i.e., learning representations with augmentation operations. The proposed framework, Contrastive Learning Through Time (CLTT), is based on the observation that the representation learned by CLTT can be used for viewing sequences that are highly similar to natural videos. The authors show that CLTT is able to learn representations that are more similar than those learned by supervised learning in a fully supervised setting. They also show that the learned representations can be applied to a data set from ThreeDWorld (TDW) in a near-photorealistic training environment. ","This paper proposes a new framework for unsupervised learning of object representations. The proposed method, Contrastive Learning Through Time (CLTT), is based on contrastive learning methods. The key idea is to use augmentation operations to learn representations that are more similar to positive pairs than negative pairs. The authors show that CLTT is able to learn viewing sequences that are similar to biology, and natural videos. The experiments are conducted on a data set from ThreeDWorld (TDW), a near-photorealistic training environment, and on a fully supervised setting where CLTT can be used for linear classification."
5717,SP:2fb4af247b5022710b681037faca2420207a507a,deterministic transition model USED-FOR goal - directed planning. Monte Carlo Tree Search USED-FOR deterministic control problems. function approximators USED-FOR MCTS. MCTS USED-FOR continuous domains. MCTS USED-FOR AlphaZero family of algorithms. function approximators USED-FOR tree. algorithms USED-FOR control problems. sparse rewards FEATURE-OF control problems. goal - directed domains HYPONYM-OF sparse rewards. AlphaZero USED-FOR goal - directed planning tasks. Hindsight Experience Replay USED-FOR AlphaZero. application USED-FOR quantum compiling domain. application HYPONYM-OF simulated domains. quantum compiling domain HYPONYM-OF simulated domains. simulated domains EVALUATE-FOR approach. OtherScientificTerm is positive reward. ,This paper proposes a deterministic transition model for goal-directed planning using Monte Carlo Tree Search to solve deterministic control problems with sparse rewards. The proposed MCTS is based on the AlphaZero family of algorithms and is able to solve continuous domains. The authors show that the function approximators for the tree can be used to learn a tree with a positive reward. They also show that AlphaZero can be combined with Hindsight Experience Replay to improve the performance of the proposed AlphaZero on goal-direct planning tasks. They demonstrate the effectiveness of their approach on two simulated domains (goal-directed domains and quantum compiling domain).,This paper proposes a deterministic transition model for goal-directed planning. The authors propose Monte Carlo Tree Search to solve deterministic control problems with sparse rewards. The main idea is to use MCTS for continuous domains and function approximators for the tree. The proposed approach is evaluated on two simulated domains (goal-directed domains and a quantum compiling domain) and one application to the quantum compressing domain (Hindsight Experience Replay). The authors show that AlphaZero outperforms other algorithms for control problems that have sparse rewards in both goal-direct domains and quantum compiling domain. 
5733,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"network capacity CONJUNCTION data replay. data replay CONJUNCTION network capacity. network capacity USED-FOR methods. data replay USED-FOR methods. virtual Feature Encoding Layer ( FEL ) USED-FOR network structures. iteratively updated optimizer CONJUNCTION virtual Feature Encoding Layer ( FEL ). virtual Feature Encoding Layer ( FEL ) CONJUNCTION iteratively updated optimizer. iteratively updated optimizer USED-FOR gradient. task descriptors USED-FOR virtual Feature Encoding Layer ( FEL ). iteratively updated optimizer PART-OF RGO. virtual Feature Encoding Layer ( FEL ) PART-OF RGO. task descriptors USED-FOR network structures. 20 - split - CIFAR100 CONJUNCTION 20 - split - miniImageNet. 20 - split - miniImageNet CONJUNCTION 20 - split - CIFAR100. RGO COMPARE baselines. baselines COMPARE RGO. continual classification benchmarks EVALUATE-FOR baselines. 20 - split - miniImageNet EVALUATE-FOR RGO. 20 - split - CIFAR100 EVALUATE-FOR RGO. continual classification benchmarks EVALUATE-FOR RGO. method USED-FOR continual learning capabilities. average accuracy EVALUATE-FOR Single - Task Learning ( STL ). Single - Task Learning ( STL ) COMPARE method. method COMPARE Single - Task Learning ( STL ). average accuracy EVALUATE-FOR method. continual learning capabilities USED-FOR learning models. gradient descent USED-FOR learning models. Method are Continual Learning ( CL ), neural networks, and Recursive Gradient Optimization ( RGO ). Generic is approach. ",This paper proposes a new method for continual learning (CL) based on the Recursive Gradient Optimization (RGO) framework. The main idea is to use the virtual Feature Encoding Layer (FEL) and the iteratively updated optimizer in RGO to learn the gradient of the network structures using task descriptors. The proposed method is evaluated on continual classification benchmarks such as 20-split-CIFAR100 and 20-cut-miniImageNet and shows that RGO achieves better average accuracy than Single-Task Learning (STL) and outperforms other baselines. The authors also show that the proposed method can achieve continual learning capabilities for learning models with gradient descent.,"This paper proposes a new approach to continuous learning (CL), which is an extension of Continual Learning (CL) to neural networks. The authors propose Recursive Gradient Optimization (RGO), which uses an iteratively updated optimizer and a virtual Feature Encoding Layer (FEL) to optimize the network structures and the network capacity. The gradient of the gradient is computed by the iteratively update the gradient. The proposed method is evaluated on continual classification benchmarks such as 20-split-CIFAR100 and 20-splitting-miniImageNet, and shows that RGO outperforms the baselines in terms of average accuracy and the continual learning capabilities of learning models with gradient descent."
5749,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"fine - tuning USED-FOR them. aligned StyleGAN models USED-FOR image - to - image translation. human faces CONJUNCTION churches. churches CONJUNCTION human faces. latent spaces FEATURE-OF child model. churches HYPONYM-OF distant data domains. human faces HYPONYM-OF distant data domains. aligned models USED-FOR tasks. image translation CONJUNCTION automatic cross - domain image morphing. automatic cross - domain image morphing CONJUNCTION image translation. child domain USED-FOR zero - shot vision tasks. fine - tuning CONJUNCTION inversion. inversion CONJUNCTION fine - tuning. fine - tuning USED-FOR approach. inversion USED-FOR approach. Method are aligned generative models, and StyleGAN. Generic are models, and they. OtherScientificTerm are architecture, and supervision. Task are transfer learning, and model alignment. ",This paper studies the problem of transfer learning with aligned generative models. The authors propose to use aligned StyleGAN models for image-to-image translation and automatic cross-domain image morphing. The proposed approach uses fine-tuning and inversion to improve the performance of the models. They show that the proposed approach is able to achieve better performance than the state-of-the-art. ,"This paper proposes a new architecture for image-to-image translation, which is based on aligned StyleGAN models. StyleGAN is a family of aligned generative models, and the authors propose to use them to improve them by fine-tuning them in the latent spaces of the child model. The authors also propose to fine-tune their approach using inversion, which improves the model alignment. The proposed approach is evaluated on zero-shot vision tasks in the child domain, including image translation, automatic cross-domain image morphing, and human faces. "
5765,SP:0e13f831c211626195c118487f2fff36a6e293f6,Comparing structured objects PART-OF learning tasks. graphs HYPONYM-OF Comparing structured objects. Optimal Transport ( OT ) USED-FOR Gromov - Wasserstein ( GW ) distance. nodes connectivity relations USED-FOR GW. GW USED-FOR graphs. probability measures USED-FOR GW. conservation of mass USED-FOR OT. graph dictionary CONJUNCTION partition learning. partition learning CONJUNCTION graph dictionary. semi - relaxed Gromov - Wasserstein divergence USED-FOR it. partition learning HYPONYM-OF tasks. graph dictionary HYPONYM-OF tasks. it USED-FOR graph dictionary learning algorithm. partitioning CONJUNCTION clustering. clustering CONJUNCTION partitioning. clustering CONJUNCTION completion. completion CONJUNCTION clustering. graphs USED-FOR complex tasks. completion HYPONYM-OF complex tasks. completion HYPONYM-OF graphs. partitioning HYPONYM-OF graphs. partitioning HYPONYM-OF complex tasks. clustering HYPONYM-OF graphs. clustering HYPONYM-OF complex tasks. OtherScientificTerm is nodes. ,"This paper proposes Optimal Transport (OT) to reduce the Gromov-Wasserstein (GW) distance between two nodes in a graph. The OT is based on the conservation of mass, which is a well-studied result in the literature. The authors show that it can be used as a graph dictionary learning algorithm, and it can also be applied to other tasks such as graph dictionary, partition learning, and clustering. The paper also shows that OT can be combined with other probability measures to improve the performance of GW.","This paper proposes Optimal Transport (OT) to reduce the Gromov-Wasserstein (GW) distance between two nodes in a graph. OT is based on the conservation of mass between the nodes. GW is used to learn graphs with nodes connectivity relations. The authors show that it can be used as a graph dictionary learning algorithm. They also show that OT can be learned with probability measures. The paper also shows that OT is equivalent to semi-relaxed Wasserstein divergence. Experiments are conducted on several tasks including graph dictionary, partitioning, clustering, and completion."
5781,SP:d6d144be11230070ae9395db70b7c7743540bad4,"prediction CONJUNCTION collaboration. collaboration CONJUNCTION prediction. Models USED-FOR prediction. Models USED-FOR collaboration. ones HYPONYM-OF categories. ones HYPONYM-OF categories. imitation learning USED-FOR ones. predicting policies USED-FOR systematic suboptimality. Bayesian inference USED-FOR systematic deviations. Boltzmann policy distribution ( BPD ) USED-FOR human policies. sampling CONJUNCTION inference. inference CONJUNCTION sampling. generative and sequence models USED-FOR sampling. generative and sequence models USED-FOR inference. high - dimensional continuous space FEATURE-OF policies. BPD USED-FOR prediction of human behavior. prediction of human behavior CONJUNCTION human - AI collaboration. human - AI collaboration CONJUNCTION prediction of human behavior. BPD USED-FOR human - AI collaboration. human - AI collaboration CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION human - AI collaboration. prediction of human behavior CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION prediction of human behavior. BPD USED-FOR imitation learning - based human models. OtherScientificTerm are human behavior, reward function, Boltzmann rationality, and trajectories. Generic are former, and models. Material is human data. ","This paper studies the problem of systematic suboptimality in the setting where the goal is to predict human behavior in a high-dimensional continuous space. The authors consider two categories: (1) prediction of human behavior and (2) collaboration. In the former case, the authors propose to use Boltzmann policy distribution (BPD) to model human policies in the high-dimensionality continuous space, and then use Bayesian inference to identify systematic deviations in the BPD. They show that BPD can improve the performance of imitation learning-based human models and human-AI collaboration. They also show that using BPD improves the performance in the case of sampling and inference. ","This paper proposes a new way of modeling human behavior. The authors propose to use Boltzmann policy distribution (BPD) to model human policies in a high-dimensional continuous space, where the goal is to learn a reward function that maximizes the sum of the reward function and the state of the environment. The former is the former, and the latter is the latter. The latter is an extension of Bayesian inference, which allows for systematic suboptimality in the case of the former. Experiments are conducted on two categories of human data (prediction and collaboration) and two models (i.e. ones trained with imitation learning) are used to evaluate the performance of the two models. The sampling and inference are performed using generative and sequence models, and BPD is used for the prediction of human behavior and human-AI collaboration. The experiments show that the BPD can be used to improve the performance on both prediction of humans and imitation learning-based human models. "
5797,SP:401ef5fe2022e926b0321258efac1f369f186ace,"Quantization of deep neural networks ( DNN ) USED-FOR compressing and accelerating DNN models. Data - free quantization ( DFQ ) HYPONYM-OF approach. accuracy EVALUATE-FOR DFQ solutions. sub - second quantization time FEATURE-OF on - the - fly DFQ framework. inference - only devices USED-FOR networks. SQuant HYPONYM-OF on - the - fly DFQ framework. discrete domain FEATURE-OF data - free optimization objective. computation complexity EVALUATE-FOR objective solver. algorithm USED-FOR objective solver. back - propagation USED-FOR algorithm. computation complexity EVALUATE-FOR algorithm. fine - tuning CONJUNCTION synthetic datasets. synthetic datasets CONJUNCTION fine - tuning. SQuant COMPARE data - free post - training quantization. data - free post - training quantization COMPARE SQuant. SQuant USED-FOR data - free quantization process. SQuant USED-FOR sub - second level. synthetic datasets EVALUATE-FOR SQuant. accuracy EVALUATE-FOR models. sub - second level FEATURE-OF data - free quantization process. 4 - bit quantization FEATURE-OF models. accuracy EVALUATE-FOR SQuant. Method are DNN models, and network architecture. OtherScientificTerm are privacy - sensitive and confidential scenarios, DNN task loss, Hessian - based optimization objective, diagonal sub - items, and weight tensor. Material is synthetic data. Metric is computation and memory requirements. ","This paper studies the problem of quantization of deep neural networks (DNN) for compressing and accelerating DNN models in privacy-sensitive and confidential scenarios. The authors propose a new approach called Data-free quantization (DFQ) which is based on the on-the-fly DFQ framework with sub-second quantization time. The main idea of DFQ is to use inference-only devices to train the networks, and then use a Hessian-based optimization objective on the discrete domain. The objective solver is trained using back-propagation, and the authors show that the proposed algorithm has a lower computation complexity than the objective Solver with SQuant. SQuant outperforms SQuant in terms of fine-tuning, synthetic datasets, and accuracy on models with 4-bit quantization. ","The paper proposes a new approach to quantization of deep neural networks (DNN) for compressing and accelerating DNN models. The authors propose Data-free quantization (DFQ) which is a data-free optimization objective in the discrete domain. The main idea of DFQ is to use inference-only devices for training the networks. The paper shows that DFQ solutions with sub-second quantization time reduce the computation and memory requirements and improve the accuracy of the DNNs. The algorithm is based on a Hessian-based optimization objective with back-propagation, where the diagonal sub-items of the weight tensor are computed as a function of the network architecture. Experiments on fine-tuning and synthetic datasets show that SQuant improves the performance of models with 4-bit quantization. "
5813,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"stock market partitioning CONJUNCTION sleep stage labelling. sleep stage labelling CONJUNCTION stock market partitioning. sleep stage labelling CONJUNCTION human activity recognition. human activity recognition CONJUNCTION sleep stage labelling. Time series USED-FOR tasks. human activity recognition HYPONYM-OF tasks. stock market partitioning HYPONYM-OF tasks. sleep stage labelling HYPONYM-OF tasks. sliding window USED-FOR sub - sequences. overlapping stride FEATURE-OF sliding window. sliding window USED-FOR time series. accuracy EVALUATE-FOR segmentation. approach USED-FOR approximate breakpoints. it USED-FOR long - term dependencies. bi - pass architecture USED-FOR SegTime. Task are time series segmentation, and classification. OtherScientificTerm are precise breakpoints, sliding windows, and label changing frequency. ","This paper studies the problem of time series segmentation. Time series are commonly used in many tasks such as stock market partitioning, sleep stage labelling, and human activity recognition. The authors propose a sliding window for sub-sequences of the time series, where the sliding window has an overlapping stride. The sliding window is used to learn the approximate breakpoints for each sub-sequence, which are then used to train a bi-pass architecture for SegTime. The proposed approach is shown to be able to learn approximate breakpoint for long-term dependencies, and it is also shown to improve the accuracy of segmentation in terms of label changing frequency.","This paper proposes a new approach to learn approximate breakpoints for time series segmentation. The authors propose a bi-pass architecture for SegTime, which is a sliding window for sub-sequences with overlapping stride. The idea is to learn the precise breakpoints from the sliding windows. The proposed approach is evaluated on three tasks: stock market partitioning, sleep stage labelling, and human activity recognition. The accuracy of segmentation is measured by the label changing frequency, and it is shown that it is able to learn long-term dependencies. "
5829,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,Graph Neural Networks ( GNNs ) USED-FOR graph data. faithfulness problems CONJUNCTION unnatural artifacts. unnatural artifacts CONJUNCTION faithfulness problems. methods USED-FOR approximation based and perturbation based approaches. faithful explanation USED-FOR GNN predictions. subgraph level interpretation algorithm USED-FOR complex interactions. GNN characteristics USED-FOR algorithm. synthetic and real - world datasets EVALUATE-FOR DEGREE. node classification and graph classification tasks EVALUATE-FOR DEGREE. Method is GNNs. Generic is models. OtherScientificTerm is graph nodes. ,"Graph Neural Networks (GNNs) are widely used for graph data. However, GNNs are not always faithful explanation for GNN predictions. This paper proposes two methods for approximation based and perturbation based approaches. The first is a subgraph level interpretation algorithm for complex interactions between graph nodes. The second is an algorithm based on GNN characteristics. DEGREE is evaluated on both synthetic and real-world datasets. The experimental results show the effectiveness of the proposed method on node classification and graph classification tasks.","Graph Neural Networks (GNNs) are widely used for graph data. However, GNNs suffer from faithfulness problems and unnatural artifacts. The authors propose two methods to address these issues: approximation based and perturbation based approaches. The first is a faithful explanation for GNN predictions. The second is a subgraph level interpretation algorithm for complex interactions between graph nodes. The proposed algorithm is based on GNN characteristics. The experimental results on both synthetic and real-world datasets show the effectiveness of DEGREE on node classification and graph classification tasks."
5845,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"strided convolutions CONJUNCTION pooling layers. pooling layers CONJUNCTION strided convolutions. downsampling operators USED-FOR resolution of intermediate representations. downsampling operators PART-OF Convolutional neural networks. strided convolutions HYPONYM-OF downsampling operators. pooling layers HYPONYM-OF downsampling operators. computational complexity EVALUATE-FOR architecture. hyperparameter FEATURE-OF layers. stride FEATURE-OF hyperparameter. crossvalidation CONJUNCTION discrete optimization. discrete optimization CONJUNCTION crossvalidation. architecture search HYPONYM-OF discrete optimization. gradient descent USED-FOR search space. DiffStride HYPONYM-OF downsampling layer. learnable strides FEATURE-OF downsampling layer. layer USED-FOR resizing. layer USED-FOR cropping mask. Fourier domain FEATURE-OF cropping mask. DiffStride USED-FOR downsampling layers. audio and image classification EVALUATE-FOR solution. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. layer PART-OF ResNet-18 architecture. computational complexity EVALUATE-FOR architecture. regularization term USED-FOR architecture. computational complexity EVALUATE-FOR regularization term. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR regularization. accuracy EVALUATE-FOR regularization. ImageNet EVALUATE-FOR regularization. OtherScientificTerm are shift - invariance, integer factor of downsampling, strides, random stride configurations, and learnable variables. Metric is computational cost. Generic is them. ","This paper studies the problem of downsampling operators in Convolutional neural networks with strided convolutions and pooling layers. The authors propose a new architecture with a new regularization term to reduce the computational complexity of the proposed architecture. The proposed architecture is based on DiffStride, a downampling layer with learnable strides. The main idea is to use gradient descent in the search space to find the optimal solution for each step of the search.  The authors show that the proposed method is computationally efficient in terms of the number of steps needed to find a good solution. They also show that their method can be combined with crossvalidation and discrete optimization. ","This paper introduces a new downsampling operators for the resolution of intermediate representations in Convolutional neural networks. The downsamplings operators are strided convolutions, pooling layers, and DiffStride. The authors show that the shift-invariance of the downsampledges is a function of the number of steps, and the integer factor of downsamples. They also show that these steps are independent of the hyperparameter of the layers, which is a measure of the distance between the input and the output of each layer. They show that they can be decomposed into two parts: (1) a step-wise regularization term, and (2) a regularization of the weights of the downsampling layer, which depends on the learnable strides of the learningable variables.  The authors also propose a new layer for resizing. The cropping mask is learned in the Fourier domain. The proposed layer is based on the ResNet-18 architecture. The main idea is to use gradient descent in the search space, which can be done in the form of architecture search, crossvalidation, and discrete optimization. The paper shows that the proposed solution can be used for audio and image classification. The computational complexity of the proposed architecture is shown to be much lower than the computational cost of the original architecture.  "
5861,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"image segmentation CONJUNCTION node classification. node classification CONJUNCTION image segmentation. Models USED-FOR image segmentation. Models USED-FOR node classification. node classification CONJUNCTION tasks. tasks CONJUNCTION node classification. Models USED-FOR tasks. method USED-FOR strictly local models. collective certificate USED-FOR softly local models. localized randomized smoothing approach USED-FOR certificate. collective guarantees FEATURE-OF locally smoothed model. image segmentation and node classification tasks EVALUATE-FOR locally smoothed model. prediction quality EVALUATE-FOR locally smoothed model. Metric is collective robustness certificate. OtherScientificTerm are small receptive field, and random perturbation strength. ","This paper proposes a method to certify the robustness of strictly local models to perturbations in a small receptive field. The certificate is based on a localized randomized smoothing approach, where the perturbation strength of the locally smoothed model is computed using the collective robustness certificate. The authors show that the proposed method can certify the performance of softly local models with the proposed collective certificate. They also provide theoretical guarantees on the performance and prediction quality of the proposed locally smoothhed model on image segmentation and node classification tasks.",The paper proposes a method for training strictly local models with a small receptive field. The proposed certificate is based on a localized randomized smoothing approach. The key idea is to use a collective robustness certificate that guarantees that the locally smoothed model is robust to random perturbation strength. The method is evaluated on both image segmentation and node classification tasks. The results show that the proposed method can be used to fine-tune softly local models. 
5877,SP:aacc31e83886c4c997412a1e51090202075eda86,"Normalizing flows USED-FOR general - purpose density estimators. domain - specific knowledge USED-FOR real world applications. general - purpose transformations CONJUNCTION structured layers. structured layers CONJUNCTION general - purpose transformations. general - purpose transformations PART-OF embedded - model flows ( EMF ). equivalent bijective transformations USED-FOR user - specified differentiable probabilistic models. EMFs USED-FOR desirable properties. multimodality CONJUNCTION hierarchical coupling. hierarchical coupling CONJUNCTION multimodality. hierarchical coupling CONJUNCTION continuity. continuity CONJUNCTION hierarchical coupling. EMFs USED-FOR multimodality. EMFs USED-FOR hierarchical coupling. continuity HYPONYM-OF desirable properties. multimodality HYPONYM-OF desirable properties. hierarchical coupling HYPONYM-OF desirable properties. EMFs USED-FOR variational inference. structure of the prior model PART-OF variational architecture. approach COMPARE alternative methods. alternative methods COMPARE approach. common structured inference problems EVALUATE-FOR alternative methods. common structured inference problems EVALUATE-FOR approach. Method are normalizing flows, gated structured layers, and prior model. OtherScientificTerm is domain - specific inductive biases. Generic are layers, and models. ","This paper studies the problem of normalizing flows for general-purpose density estimators. The authors propose to use embedded-model flows (EMF), which are equivalent bijective transformations to user-specified differentiable probabilistic models, to learn domain-specific knowledge for real world applications. Empirical results show that EMFs can learn desirable properties such as multimodality, hierarchical coupling, continuity, and structured layers. The proposed approach is also shown to perform better than alternative methods on common structured inference problems. ","This paper proposes a new normalizing flows for general-purpose density estimators. The idea is to use the domain-specific knowledge in real world applications. The authors propose to use embedded-model flows (EMF) as equivalent bijective transformations for user-specified differentiable probabilistic models. The key idea of the paper is to replace the gated structured layers of the prior model with a variational architecture that is based on the structure of a prior model. The proposed approach is evaluated on several common structured inference problems and compared to alternative methods. Empirical results show that the proposed EMFs achieve desirable properties such as multimodality, hierarchical coupling, and continuity. "
5893,SP:825a254c0725008143b260ead840ae35f9f096d1,"entities CONJUNCTION objects. objects CONJUNCTION entities. conceptual structure FEATURE-OF rich conceptual structure. LMs USED-FOR grounded world representation. LMs USED-FOR conceptual domain. it USED-FOR concepts. grid world FEATURE-OF concepts. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. GPT-2 HYPONYM-OF generative language models. GPT-3 HYPONYM-OF generative language models. text - only models USED-FOR rich conceptual structure. Method are text - only language models ( LMs ), and grounded language models. Generic are representation, and model. OtherScientificTerm is conceptual structure of language. ","This paper studies the problem of grounding language models in a grid world. The authors propose to use text-only language models (LMs) to learn a grounded world representation of the concepts and objects in the grid world, and then use LMs to represent the concepts in the conceptual domain. The representation is then used to train a model that is able to capture the conceptual structure of language. The proposed generative language models, such as GPT-2 and GPT3, are shown to be able to learn the rich conceptual structure, and it is shown that it can be used to represent concepts as well as objects.","This paper proposes a novel way to model the rich conceptual structure of language. The authors propose text-only language models (LMs) for the grounded world representation. LMs are used for the conceptual domain, and it is shown that it can capture concepts in the grid world as well as entities and objects. The representation is then used to train a GPT-2 and GPT3, which are generative language models. The model is shown to capture the conceptual structure. The paper is well-written and easy to follow. "
5909,SP:702029739062693e3f96051cbb38f20c53f2a223,"Reinforcement learning USED-FOR phenomena. biases PART-OF learning process. biases USED-FOR shaped rewards. shaped rewards USED-FOR emergent phenomena. sender - receiver navigation game USED-FOR shaped rewards. Generic are they, and rewards. OtherScientificTerm are base reward, inductive bias, semantics, and environmental variables of interest. Task is emergent language experimentation. Material is emergent language. Metric is entropy. ","This paper studies the problem of emergent language experimentation. The authors propose a new reinforcement learning method for learning the shape of a language. The key idea is to learn a set of learned representations of the environment, and then use these representations to train a language model. The learned representations are then used to train the language model, which is then used as a proxy for the environment. The proposed method is evaluated on a variety of tasks, and shows that it outperforms the baselines.","This paper studies emergent language experimentation in the context of reinforcement learning. The authors propose to use biases in the learning process to learn shaped rewards for emergent phenomena such as shaped rewards in a sender-receiver navigation game. They show that they can be learned in the form of an inductive bias, where the base reward is a function of the semantics of the environment and the environment variables of interest. They also show that the entropy of the learned rewards can be reduced to zero."
5925,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"deep multilingual language models USED-FOR cross - lingual knowledge. neural modeling USED-FOR deep multilingual language models. unparallel texts USED-FOR cross - lingual knowledge. invariance FEATURE-OF feature representations. invariance FEATURE-OF transfer. prior shift estimation CONJUNCTION correction. correction CONJUNCTION prior shift estimation. representation alignment CONJUNCTION prior shift estimation. prior shift estimation CONJUNCTION representation alignment. unsupervised cross - lingual learning method USED-FOR representation alignment. unsupervised cross - lingual learning method USED-FOR prior shift estimation. importance - weighted domain alignment ( IWDA ) HYPONYM-OF unsupervised cross - lingual learning method. method COMPARE semi - supervised learning techniques. semi - supervised learning techniques COMPARE method. Method are cross - lingually shared representations, and multilingual representations. OtherScientificTerm are distributional shift in class priors, and prior shifts. ","This paper proposes a new unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), to improve the performance of deep multilingual language models trained with neural modeling. IWDA is based on prior shift estimation and representation alignment. The authors show that the proposed method can achieve better performance than semi-supervised learning techniques. ",This paper proposes an unsupervised cross-lingual learning method to improve the representation alignment and prior shift estimation in deep multilingual language models. The key idea is to use the distributional shift in class priors of unparallel texts as a prior for cross-laggingual knowledge in neural modeling. The authors show that the invariance of the feature representations of multilingual representations is a function of the distribution of the prior shifts. They also show that this invariance can be used to improve transfer between multilingual shared representations. The method is evaluated on importance-weighted domain alignment (IWDA) and representation alignment. The proposed method outperforms other semi-supervised learning techniques.
5941,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"Meta - learning USED-FOR artificial intelligence. algorithm USED-FOR problem. algorithm USED-FOR meta - learner. metric USED-FOR meta - optimisation. gradients USED-FOR meta - learning. bootstrapping mechanism USED-FOR meta - learning horizon. backpropagation USED-FOR bootstrapping mechanism. it USED-FOR multi - task meta - learning. Atari ALE benchmark EVALUATE-FOR model - free agents. it USED-FOR exploration. ε - greedy Q - learning agent USED-FOR exploration. Task is metaoptimisation problem. Method are metalearner, and bootstrapping. Metric is ( pseudo-)metric. OtherScientificTerm is update rule. ","This paper studies the metaoptimisation problem in the context of artificial intelligence. The authors propose a new algorithm to solve the problem by bootstrapping the meta-learner, which is an existing metric for meta-optimisation based on gradients of the metalearner. The bootstrapped mechanism is based on backpropagation, and the authors show that it can be applied to multi-task meta-learning.  The authors also show that the bootstraps can be used for exploration in the presence of a ε-greedy Q-learning agent, and it can also be used to train model-free agents on the Atari ALE benchmark.","This paper studies the metaoptimisation problem in the context of artificial intelligence. The authors propose a new algorithm to solve the problem. The main idea is to use a (pseudo-)metric to measure the gradients of the meta-optimisation, and then use a bootstrapping mechanism to map the meta -learning horizon to the gradient of the metalearner. This is done by backpropagation, and the authors show that it can be used for multi-task meta-learning, and it can also be used in exploration with a ε-greedy Q-learning agent. The paper also shows that the proposed algorithm can be applied to model-free agents on the Atari ALE benchmark. "
5957,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"model - free agents USED-FOR generalization. generalization ability EVALUATE-FOR model - based agents. model - based agents COMPARE model - free counterparts. model - free counterparts COMPARE model - based agents. generalization ability EVALUATE-FOR model - based agents. generalization ability EVALUATE-FOR model - free counterparts. MuZero HYPONYM-OF model - based agent. procedural and task generalization EVALUATE-FOR its. self - supervised representation learning CONJUNCTION procedural data diversity. procedural data diversity CONJUNCTION self - supervised representation learning. planning CONJUNCTION self - supervised representation learning. self - supervised representation learning CONJUNCTION planning. generalization CONJUNCTION data efficiency. data efficiency CONJUNCTION generalization. data efficiency EVALUATE-FOR Procgen. planning HYPONYM-OF procedural generalization. Procgen EVALUATE-FOR techniques. generalization EVALUATE-FOR techniques. data efficiency EVALUATE-FOR techniques. factors USED-FOR task generalization. Meta - World USED-FOR task generalization. single - task, model - free paradigm CONJUNCTION self - supervised model - based agents. self - supervised model - based agents CONJUNCTION single - task, model - free paradigm. rich, procedural, multi - task environments FEATURE-OF self - supervised model - based agents. single - task, model - free paradigm USED-FOR generalizable agents. Method is model - based reinforcement learning. OtherScientificTerm is internal model of the world. Task is transfer. ","This paper studies the generalization ability of model-free agents against model-based agents in the context of generalization. The authors propose MuZero, an agent-based agent that is able to generalize well in both procedural and task generalization environments. MuZero is based on a single-task model, which is trained by self-supervised representation learning, planning, and procedural data diversity. They show that MuZero can achieve better generalization performance than the state-of-the-art in both the procedural and the multi-task settings. They also show that the MuZero agent can generalize better than the best-performing models in both environments. ","This paper proposes a new paradigm for model-based reinforcement learning, called MuZero. MuZero is a model-free agent that is able to generalize to new tasks. The authors show that MuZero can achieve better generalization ability than the state-of-the-art models on both procedural and task generalization. They also show that the MuZero agent can generalize better than the standard models on the task of transfer. "
5973,SP:ba80e35d452d894181d51624183b60541c0f3704,"fixed graph USED-FOR relational inductive biases. graph neural networks HYPONYM-OF Machine learning frameworks. fixed graph USED-FOR Machine learning frameworks. network inverse ( deconvolution ) problem USED-FOR graph learning task. eigendecomposition - based spectral methods CONJUNCTION iterative optimization solutions. iterative optimization solutions CONJUNCTION eigendecomposition - based spectral methods. Graph Deconvolution Network ( GDN ) HYPONYM-OF parameterized neural network architecture. GDNs USED-FOR link prediction or edge - weight regression tasks. GDNs USED-FOR distribution of graphs. loss function USED-FOR GDNs. loss function USED-FOR link prediction or edge - weight regression tasks. layers USED-FOR graph objects. GDNs USED-FOR larger - sized graphs. graph objects COMPARE node features. node features COMPARE graph objects. GDN USED-FOR graph recovery. synthetic data USED-FOR GDN. synthetic data USED-FOR graph recovery. Human Connectome Project - Young Adult neuroimaging dataset EVALUATE-FOR model. inferring structural brain networks USED-FOR model. functional connectivity USED-FOR inferring structural brain networks. functional connectivity USED-FOR model. Material is network data. OtherScientificTerm are graphs, graph convolutional relationship, observed and latent graphs, and structural brain networks. Task is inferring graph structure. Method is proximal gradient iterations. ","This paper studies the problem of graph inverse (deconvolution) problem in the graph learning task, where the goal is to learn a fixed graph with relational inductive biases. The authors propose a parameterized neural network architecture, called Graph Deconvolvolution Network (GDN), which is based on the graph convolutional relationship between observed and latent graphs. GDNs are able to learn the distribution of graphs by learning a loss function for each node in a graph, which can be used for link prediction or edge-weight regression tasks. The proposed model is evaluated on the Human Connectome Project-Young Adult neuroimaging dataset, where it is shown that the proposed GDN is able to recover graph objects from synthetic data and perform graph recovery using functional connectivity.","This paper proposes a new parameterized neural network architecture called Graph Deconvolution Network (GDN) which is based on graph convolutional relationship between observed and latent graphs. The authors propose to use a fixed graph as a proxy for relational inductive biases in Machine learning frameworks. The graph learning task is modeled as a network inverse (decomposition-based deconvolution) problem, where the network data is represented as a set of graphs. GDNs are used to learn the distribution of graphs and the loss function for GDNs is used for link prediction or edge-weight regression tasks. The proposed model is evaluated on a Human Connectome Project-Young Adult neuroimaging dataset with synthetic data for graph recovery and functional connectivity for inferring structural brain networks. "
5989,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"Reward shaping ( RS ) USED-FOR reinforcement learning ( RL ). manually engineered shaping - reward functions USED-FOR RS. domain knowledge USED-FOR It. Reinforcement Learning Optimising Shaping Algorithm ( ROSA ) HYPONYM-OF automated RS framework. Markov game USED-FOR shaping - reward function. agent ( Controller ) USED-FOR optimal policy. agent ( Controller ) USED-FOR task. optimal policy USED-FOR task. shaped rewards USED-FOR agent ( Controller ). switching controls USED-FOR reward - shaping agent ( Shaper ). shaped rewards USED-FOR optimal policy. ROSA USED-FOR shapingreward function. shapingreward function USED-FOR task. RL algorithms USED-FOR ROSA. RS algorithms USED-FOR sparse reward environments. OtherScientificTerm are sparse or uninformative rewards, shaping rewards, and congenial properties. Task is autonomous learning. ","This paper proposes a new algorithm for reward shaping (RS) in reinforcement learning (RL). The authors propose a new automated RS framework, Reinforcement Learning Optimising Shaping Algorithm (ROSA), which uses a Markov game to learn a shaping-reward function for each task. The agent (Controller) learns an optimal policy for the task using shaped rewards from the agent (Shaper) and switching controls. It uses domain knowledge to learn the optimal policy from the shaping rewards. The authors show that the proposed ROSA is able to learn an optimal shapingreward for any task using a simple shaping reward function. They also show that their algorithm can be applied to sparse or uninformative rewards. Finally, the authors demonstrate the effectiveness of the proposed RS algorithms in sparse reward environments.","This paper proposes Reinforcement Learning Optimising Shaping Algorithm (ROSA), an automated RS framework for reinforcement learning (RL). It is based on manually engineered shaping-reward functions, where the agent (Controller) is trained on sparse or uninformative rewards, and the optimal policy is learned from domain knowledge. The agent (controller) learns an optimal policy for each task using a shapingreward function based on a Markov game. The optimal policy can be learned from a set of shaped rewards or from switching controls. The authors show that ROSA is able to learn the optimal task from a subset of sparse reward environments, and that it can learn the shaping rewards with congenial properties. The paper also shows that the proposed ROSA outperforms existing RL algorithms in sparse reward environment."
6005,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,Vertical Federated Learning ( VFL ) HYPONYM-OF distributed learning paradigm. backdoor attacks FEATURE-OF VFL. robustness EVALUATE-FOR VFL. horizontal federated learning COMPARE VFL. VFL COMPARE horizontal federated learning. RVFR HYPONYM-OF VFL training and inference framework. RVFR USED-FOR uncorrupted features. RVFR USED-FOR model. RVFR USED-FOR inferencephase adversarial and missing feature attacks. RVFR COMPARE baselines. baselines COMPARE RVFR. robustness EVALUATE-FOR baselines. robustness EVALUATE-FOR RVFR. Method is global model. OtherScientificTerm is features. Task is inference - phase attacks. Material is NUS - WIDE and CIFAR-10 datasets. ,"The paper proposes a distributed learning paradigm called Vertical Federated Learning (VFL) which is a variant of the VFL with backdoor attacks. The authors propose a VFL training and inference framework called RVFR, which is based on a global model. The model is trained using RVFR to recover uncorrupted features from the input data. The paper shows that the robustness of VFL against backdoor attacks is comparable to horizontal federated learning. In addition, the paper also shows that RVFR is robust to inferencephase adversarial and missing feature attacks. Finally, the authors conduct extensive experiments on NUS-WIDE and CIFAR-10 datasets to demonstrate the effectiveness of RVFR.","This paper proposes a distributed learning paradigm called Vertical Federated Learning (VFL) which is an extension of the distributed learning paradigms. The key idea of VFL is to avoid backdoor attacks on the global model. The authors propose a VFL training and inference framework called RVFR, which is based on the RVFR. The model is trained using RVFR to detect uncorrupted features and to prevent inferencephase adversarial and missing feature attacks. Experiments on NUS-WIDE and CIFAR-10 datasets show that RVFR is more robust to inference-phase attacks than other baselines. The paper also shows that VFL outperforms horizontal federated learning."
6021,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,question answering CONJUNCTION fact checking. fact checking CONJUNCTION question answering. Information retrieval USED-FOR knowledge intensive tasks. Information retrieval HYPONYM-OF natural language processing. natural language processing USED-FOR knowledge intensive tasks. fact checking HYPONYM-OF knowledge intensive tasks. question answering HYPONYM-OF knowledge intensive tasks. dense retrievers COMPARE sparse methods. sparse methods COMPARE dense retrievers. dense retrievers PART-OF information retrieval. neural networks USED-FOR dense retrievers. term - frequency USED-FOR sparse methods. datasets EVALUATE-FOR models. BM25 HYPONYM-OF term - frequency methods. contrastive learning USED-FOR unsupervised dense retrievers. it USED-FOR retrieval. model COMPARE BM25. BM25 COMPARE model. BEIR benchmark EVALUATE-FOR model. model COMPARE BM25. BM25 COMPARE model. MS MARCO dataset EVALUATE-FOR technique. pre - training USED-FOR technique. pre - training CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pre - training. fine - tuning EVALUATE-FOR technique. MS MARCO dataset EVALUATE-FOR pre - training. MS MARCO dataset EVALUATE-FOR fine - tuning. BEIR benchmark EVALUATE-FOR technique. Generic is they. Material is new domains. OtherScientificTerm is supervision. ,"This paper studies the problem of information retrieval for knowledge intensive tasks such as question answering, fact checking, and natural language processing. The authors propose a new method for unsupervised dense retrievers based on contrastive learning. The proposed method is based on the term-frequency method of BM25, which is a variant of the popular term-frequency methods such as BM25.  The authors show that the proposed method outperforms the state-of-the-art on the BEIR benchmark and the MS MARCO dataset. ",This paper presents a novel method for learning dense retrievers for knowledge retrieval in natural language processing. The authors propose to use contrastive learning to improve the performance of unsupervised dense retrivers. They show that the proposed method outperforms other sparse methods in terms of term-frequency. They also show that it can be used for retrieval in new domains. They evaluate the proposed model on the BEIR benchmark and the MS MARCO dataset. The proposed technique is evaluated on both pre-training and fine-tuning.
6037,SP:ed4e2896dc882bd089f420f719da232d706097c5,fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. fine - tuning HYPONYM-OF methods. linear probing HYPONYM-OF methods. accuracy in - distribution ( ID ) EVALUATE-FOR fine - tuning. accuracy EVALUATE-FOR fine - tuning. fine - tuning COMPARE linear probing. linear probing COMPARE fine - tuning. DomainNet CONJUNCTION CIFAR. CIFAR CONJUNCTION DomainNet. Breeds - Entity30 CONJUNCTION DomainNet. DomainNet CONJUNCTION Breeds - Entity30. distribution shift datasets EVALUATE-FOR fine - tuning. Breeds - Living17 CONJUNCTION Breeds - Entity30. Breeds - Entity30 CONJUNCTION Breeds - Living17. CIFAR10.1 CONJUNCTION FMoW. FMoW CONJUNCTION CIFAR10.1. accuracy OOD EVALUATE-FOR linear probing. CIFAR CONJUNCTION CIFAR10.1. CIFAR10.1 CONJUNCTION CIFAR. accuracy ID EVALUATE-FOR linear probing. CIFAR HYPONYM-OF distribution shift datasets. FMoW HYPONYM-OF distribution shift datasets. Breeds - Living17 HYPONYM-OF distribution shift datasets. CIFAR10.1 HYPONYM-OF distribution shift datasets. DomainNet HYPONYM-OF distribution shift datasets. Breeds - Entity30 HYPONYM-OF distribution shift datasets. accuracy OOD EVALUATE-FOR fine - tuning. accuracy ID EVALUATE-FOR fine - tuning. fine - tuning USED-FOR pretrained features. fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. linear probing COMPARE fine - tuning. fine - tuning COMPARE linear probing. ID and OOD accuracy EVALUATE-FOR fine - tuning. linear probing CONJUNCTION fine - tuning. fine - tuning CONJUNCTION linear probing. fine - tuning COMPARE fine - tuning. fine - tuning COMPARE fine - tuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. datasets EVALUATE-FOR fine - tuning. Method is pretrained model,"This paper studies the problem of fine-tuning a pretrained model in the presence of distribution shift datasets. The authors propose two methods, linear probing and linear probing, to improve the accuracy in-distribution (ID) of the training data. They show that the accuracy ID of linear probing is better than the accuracy OOD of the two methods. They also show that linear probing performs better than linear probing in terms of ID and OOD accuracy. ",This paper studies the fine-tuning of a pretrained model in the context of distribution shift datasets. The authors show that the accuracy in-distribution (ID) can be used to improve the accuracy of the final prediction of the model. They also show that fine-tuning can be applied to other methods such as linear probing and linear probing with a different number of parameters. The paper also shows that the ID and OOD accuracy can be improved in the presence of fine -tuning. 
6053,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"meta - learning COMPARE L2DNC. L2DNC COMPARE meta - learning. meta - learning USED-FOR L2DNC. meta - learning algorithms USED-FOR L2DNC problem. meta - learning - based methodology USED-FOR unlabeled data. meta - learning - based methodology USED-FOR it. L2DNC COMPARE labeling in causality. labeling in causality COMPARE L2DNC. unseen - class data USED-FOR seen - class data. Material are labeled data, and limited data. Method is clustering models. OtherScientificTerm is high - level semantic features. ","This paper studies the L2DNC problem in the context of clustering models. The authors propose a meta-learning-based methodology to learn unlabeled data from unseen-class data, and then use it to improve the performance of L2dNC. They show that the proposed method is able to achieve state-of-the-art performance in terms of labeling in causality. They also show that their method can be applied to other datasets with limited data.","This paper proposes a meta-learning-based approach to the L2DNC problem, where the goal is to improve the performance of clustering models on unlabeled data. The main idea is to learn a set of high-level semantic features that can be used as a surrogate for the labeled data. This is done by using meta-learned algorithms to solve the problem. The authors show that it is possible to learn the high level semantic features with a meta learning-based methodology. The paper also shows that it can be done with unseen-class data, and that it outperforms the labeling in causality. "
6069,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"POMDPs USED-FOR model - based RL agents. online and offline experiences USED-FOR causal transition model. offline experiences USED-FOR agent. reinforcement learning CONJUNCTION causality. causality CONJUNCTION reinforcement learning. causal inference problem USED-FOR model - based reinforcement learning. offline data USED-FOR learning. methodology USED-FOR offline data. latent - based causal transition model USED-FOR interventional and observational regimes. latent - based causal transition model USED-FOR method. latent variable USED-FOR deconfounding. deconfounding USED-FOR POMDP transition model. latent variable USED-FOR POMDP transition model. generalization guarantees EVALUATE-FOR it. synthetic toy problems EVALUATE-FOR it. generalization guarantees EVALUATE-FOR method. synthetic toy problems EVALUATE-FOR method. OtherScientificTerm are online experiences, privileged information, and learning agent. Material are interventional data, and observational data. Method is causal framework of do - calculus. ","This paper studies the problem of POMDPs for model-based RL agents trained with online and offline experiences. The authors consider the causal inference problem in the context of reinforcement learning and causality, where the agent is trained with offline experiences and online experiences are used to train a causal transition model.  The authors propose a methodology to learn offline data from offline data and use the offline data as a proxy for the online experiences to learn the privileged information about the learning agent. The method is based on a latent-based causal transfer model that can be used in both interventional and observational regimes. The proposed method uses the latent variable as a deconfounding for the POMD transition model, and it is evaluated on synthetic toy problems.","This paper proposes a causal transition model that combines online and offline experiences to train model-based RL agents. The agent is trained on both online experiences, where the online experiences are privileged information and the offline experiences are not. The authors propose a causal framework of do-calculus. The causal inference problem is formulated as an extension of the reinforcement learning and causality. The proposed method is based on a POMDP transition model with a latent variable that is used for deconfounding. The method is evaluated on synthetic toy problems and generalization guarantees. "
6085,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"textual knowledge corpus USED-FOR retriever. Wikipedia HYPONYM-OF textual knowledge corpus. retriever USED-FOR text generation systems. methods USED-FOR generator. retriever CONJUNCTION generator. generator CONJUNCTION retriever. methods USED-FOR retriever. generating informative utterances in conversations HYPONYM-OF open - ended generation tasks. retriever CONJUNCTION generator. generator CONJUNCTION retriever. informative conversations EVALUATE-FOR retriever. generator USED-FOR it. Wizard of Wikipedia dataset FEATURE-OF informative conversations. retriever USED-FOR it. posterior - guided training USED-FOR informative conversations. evidence lower bound ( ELBo ) USED-FOR it. posterior distribution Q USED-FOR guide retriever. OtherScientificTerm are top-10, and generator ’s responses. Method is end - to - end system. ",This paper proposes a new retriever for text generation systems based on the textual knowledge corpus of Wikipedia. The retriever is a combination of two existing methods: retriever and generator. The main idea is to use the top-10 of the corpus as a guide retriever to guide the generator’s responses. The authors provide an evidence lower bound (ELBo) for it and show that it can be trained with posterior-guided training to generate informative conversations from the Wizard of Wikipedia dataset. They also show that the retriever can generate informative utterances in conversations in open-ended generation tasks.,"This paper proposes a new retriever for text generation systems based on the textual knowledge corpus of Wikipedia. The authors propose two methods for training the retriever and the generator. The retriever is based on an end-to-end system, where the top-10 of the generator’s responses are used to guide the generator to generate the final output. The generator is trained using posterior-guided training on informative conversations generated by the Wizard of Wikipedia dataset. The guide retriever uses posterior distribution Q, and it is evaluated using the evidence lower bound (ELBo) on the number of informative utterances in conversations in open-ended generation tasks."
6101,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"machine learning USED-FOR combinatorial optimization. real - world problems USED-FOR large graphs. influence maximization HYPONYM-OF NP - hard problem. influence estimation HYPONYM-OF # P - hard counting problem. influence estimation CONJUNCTION influence maximization. influence maximization CONJUNCTION influence estimation. influence estimation HYPONYM-OF problems. influence maximization HYPONYM-OF problems. Graph Neural Network ( GNN ) USED-FOR upper bound of influence estimation. GLIE HYPONYM-OF Graph Neural Network ( GNN ). small simulated graphs USED-FOR it. GLIE USED-FOR influence estimation. real graphs USED-FOR influence estimation. it USED-FOR influence maximization. GLIE CONJUNCTION simulated influence estimation. simulated influence estimation CONJUNCTION GLIE. simulated influence estimation USED-FOR Lazy Forward optimization. GLIE USED-FOR Lazy Forward optimization. time complexity CONJUNCTION quality of influence. quality of influence CONJUNCTION time complexity. first HYPONYM-OF Q - network. GLIE ’s predictions USED-FOR Q - network. second USED-FOR provably submodular function. GLIE ’s representations USED-FOR provably submodular function. time efficiency CONJUNCTION influence spread. influence spread CONJUNCTION time efficiency. latter COMPARE SOTA benchmarks. SOTA benchmarks COMPARE latter. influence spread EVALUATE-FOR latter. time efficiency EVALUATE-FOR latter. Generic are perspective, and approaches. Task is small graph problems. OtherScientificTerm are predictions ranking, and computational overhead. Metric is accuracy. ","This paper studies the problem of combinatorial optimization in machine learning. The authors consider two problems: influence estimation and influence maximization in the NP-hard problem of influence estimation in the #P-hard counting problem, where the goal is to maximize the number of nodes in a graph. The main contribution of the paper is to propose a Graph Neural Network (GNN) for the upper bound of the influence estimation, which is based on GLIE. The GNN is trained on small simulated graphs, and it is shown that it can be combined with GLIE and simulated influence estimation for influence estimation on real graphs. The paper also shows that GLIE’s predictions for the Q-network can be used to learn a provably submodular function, which can then be used for Lazy Forward optimization. The latter is shown to perform better than the SOTA benchmarks in terms of time efficiency and influence spread. ","This paper presents a new perspective on combinatorial optimization in the context of machine learning. The main idea is to use a Graph Neural Network (GNN) to compute the upper bound of influence estimation, which is a NP-hard problem. The authors show that it is equivalent to a #P-hard counting problem (e.g., influence maximization) and influence estimation on real-world problems for large graphs. They also show that using GLIE and simulated influence estimation for Lazy Forward optimization and GLIE for influence estimation in small simulated graphs improves the time efficiency and the quality of influence. They show that the latter is better than the SOTA benchmarks in terms of time complexity and quality of the influence. "
6117,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"active learning strategies USED-FOR domain adaptation. localized class of functions USED-FOR labeling. Rademacher average CONJUNCTION localized discrepancy. localized discrepancy CONJUNCTION Rademacher average. localized discrepancy FEATURE-OF loss functions. generalization error bounds USED-FOR active learning strategies. regularity condition FEATURE-OF loss functions. Kmedoids algorithm USED-FOR large data set. theoretical bounds USED-FOR Kmedoids algorithm. algorithm COMPARE active learning techniques. active learning techniques COMPARE algorithm. active learning techniques USED-FOR domain adaptation. algorithm USED-FOR domain adaptation. large data sets EVALUATE-FOR algorithm. OtherScientificTerm are assumption of Lipschitz functions, and discrepancy distance. ","This paper studies the problem of domain adaptation with active learning strategies. The authors consider the assumption of Lipschitz functions, and propose a Kmedoids algorithm for the large data set. The proposed algorithm is based on theoretical bounds on the Rademacher average and the localized discrepancy between the two loss functions with a regularity condition. Theoretical results show that the proposed algorithm outperforms active learning techniques for domain adaptation on large data sets.",This paper proposes a novel Kmedoids algorithm for domain adaptation on a large data set. The proposed algorithm is based on the assumption of Lipschitz functions. The authors provide theoretical bounds on the Rademacher average and the localized discrepancy of loss functions with respect to the regularity condition of the loss functions. They also provide generalization error bounds for active learning strategies. They show that the proposed algorithm outperforms active learning techniques in domain adaptation in large data sets.
6133,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,variational approximation USED-FOR Bayesian neural networks. singular statistical models USED-FOR neural networks. posterior distribution FEATURE-OF singular model. desingularization map HYPONYM-OF algebraic - geometrical transformation. generalized gamma mean - field variational family USED-FOR leading order term. leading order term FEATURE-OF model evidence. generalized gamma mean - field variational family USED-FOR model evidence. desingularization USED-FOR generalized gamma mean - field variational family. Affine coupling layers USED-FOR unknown desingularization map. source distribution USED-FOR normalizing flow. normalizing flow USED-FOR methodology. generalized gamma USED-FOR normalizing flow. Generic is approximation. Method is singular learning theory. OtherScientificTerm is mixture of standard forms. ,"This paper studies the problem of variational approximation for Bayesian neural networks with singular statistical models. The authors propose a new algebraic-geometrical transformation, called desingularization map, which is based on Affine coupling layers. The proposed methodology uses a normalizing flow based on a generalized gamma mean-field variational family to normalize the leading order term of the model evidence. The paper also proposes a new approximation based on singular learning theory.","The paper proposes a variational approximation for Bayesian neural networks based on singular statistical models. The approximation is based on the singular learning theory, where the posterior distribution of a singular model is the sum of the mixture of standard forms. The paper shows that the leading order term of the model evidence is the generalized gamma mean-field variational family. The authors also show that the normalizing flow of the proposed methodology is the source distribution, and that the unknown desingularization map is the algebraic-geometrical transformation. Affine coupling layers are used to map the source and target distributions."
6149,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"model USED-FOR domain generalization ( DG ) problem setting. known data distributions USED-FOR model. methods COMPARE empirical risk minimisation baseline. empirical risk minimisation baseline COMPARE methods. learning - theoretic generalisation bound USED-FOR DG. learning - theoretic generalisation bound USED-FOR domain generalisation. Rademacher complexity FEATURE-OF model. empirical risk - predictor complexity trade - off EVALUATE-FOR methods. regularised ERM USED-FOR domain generalisation. Task are general purpose DG, and DG problem. Material is DomainBed benchmark. ","This paper proposes a new model for the domain generalization (DG) problem setting. The model is based on known data distributions. The authors show that the learning-theoretic generalisation bound for DG with Rademacher complexity is equivalent to the empirical risk minimisation baseline. They also show that regularised ERM can be used for domain generalisation with empirical risk-prediction complexity trade-off. Finally, they provide a DomainBed benchmark for the DG problem.",This paper proposes a model for the domain generalization (DG) problem setting. The model is based on known data distributions. The authors propose a learning-theoretic generalisation bound for the DG. The proposed model has Rademacher complexity that is lower than the empirical risk minimisation baseline. The main contribution of the paper is to propose a regularised ERM for domain generalisation. Experiments on the DomainBed benchmark show that the proposed methods outperform empirical risk-prediction complexity trade-off.
6165,SP:b1f622cbc827e880f98de9e99eca498584efe011,overlays USED-FOR maximum n - times coverage problem. multi - set multi - cover problem USED-FOR Maximum n - times coverage. integer linear programming CONJUNCTION sequential greedy optimization. sequential greedy optimization CONJUNCTION integer linear programming. solutions USED-FOR n - times coverage. integer linear programming USED-FOR solutions. sequential greedy optimization USED-FOR solutions. it USED-FOR pan - strain COVID-19 vaccine design. pan - strain COVID-19 vaccine design COMPARE designs. designs COMPARE pan - strain COVID-19 vaccine design. maximum n - times coverage USED-FOR peptide vaccine design. predicted population coverage EVALUATE-FOR pan - strain COVID-19 vaccine design. predicted population coverage EVALUATE-FOR designs. Task is min - cost n - times coverage problem. OtherScientificTerm is HLA molecules. ,"This paper studies the min-cost n-times coverage problem in the multi-set multi-cover problem. The authors propose two overlays for the maximum n-time coverage problem, which is a multi-sets multi-coverage problem.  The solutions are based on integer linear programming and sequential greedy optimization.   The authors show that it is possible to find solutions that achieve n-teams coverage with a small number of HLA molecules, and that it can be used for pan-strain and pan-strain COVID-19 vaccine design with predicted population coverage. ","This paper studies the min-cost n-times coverage problem, which is a multi-set multi-cover problem. The authors propose a set of overlays to solve the maximum n-time coverage problem. They propose two solutions to the n- times coverage problem: integer linear programming and sequential greedy optimization. They show that it is equivalent to a pan-strain COVID-19 vaccine design with predicted population coverage and a peptide vaccine design that has maximum n times coverage. "
6181,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"supervised learning algorithms USED-FOR SNNs. power consumption CONJUNCTION computational capability. computational capability CONJUNCTION power consumption. information encoding CONJUNCTION power consumption. power consumption CONJUNCTION information encoding. information encoding EVALUATE-FOR spiking neural networks ( SNNs ). weight initialization USED-FOR SNN training. It USED-FOR gradient generation. asymptotic formula USED-FOR response curve. asymptotic formula USED-FOR spiking neurons. initialization method USED-FOR gradient vanishing. slant asymptote USED-FOR initialization method. coding schemes USED-FOR classification tasks. method COMPARE deep learning initialization methods. deep learning initialization methods COMPARE method. method COMPARE SNN initialization methods. SNN initialization methods COMPARE method. deep learning initialization methods CONJUNCTION SNN initialization methods. SNN initialization methods CONJUNCTION deep learning initialization methods. model accuracy EVALUATE-FOR deep learning initialization methods. model accuracy EVALUATE-FOR SNN initialization methods. coding schemes EVALUATE-FOR method. training speed CONJUNCTION model accuracy. model accuracy CONJUNCTION training speed. MNIST and CIFAR10 dataset EVALUATE-FOR coding schemes. MNIST and CIFAR10 dataset USED-FOR classification tasks. training speed EVALUATE-FOR method. model accuracy EVALUATE-FOR method. Method is backpropagation. OtherScientificTerm are neuron response distribution, and training hyperparameters. Generic is methods. ","This paper studies the problem of spiking neural networks (SNNs) with information encoding and power consumption. The authors propose a new initialization method based on slant asymptote. The main idea is to use weight initialization for SNN training. It is shown that this initialization method is able to avoid gradient vanishing in the case of backpropagation. The paper also shows that the asymptonotic formula for the response curve for spiking neurons can be used to estimate the neuron response distribution. The proposed method is evaluated on the MNIST and CIFAR10 dataset for classification tasks and compared with other coding schemes and deep learning initialization methods. The results show that the proposed method achieves better training speed and model accuracy than SNN initialization methods, while also being more computationally efficient.","This paper proposes a novel initialization method for spiking neural networks (SNNs) based on a slant asymptote. The main idea is to use weight initialization for SNN training, where the neuron response distribution is learned by backpropagation. The authors show that the initialization method is able to avoid gradient vanishing, which is common in deep learning initialization methods. It is shown that the weight initialization can be used for gradient generation, and that the response curve can be approximated by an asymmptotic formula. The proposed method is evaluated on MNIST and CIFAR10 dataset for classification tasks, where it outperforms other coding schemes and model accuracy in terms of training speed."
6197,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"Federated learning USED-FOR machine learning models. mobile devices FEATURE-OF decentralized client data. decentralized client data USED-FOR machine learning models. model USED-FOR federated learning systems. mixture of distributions USED-FOR it. block - cyclic pattern USED-FOR distribution shift. light - weight branches USED-FOR network. image classification CONJUNCTION next word prediction. next word prediction CONJUNCTION image classification. algorithm USED-FOR distribution shift. image classification EVALUATE-FOR algorithm. model EVALUATE-FOR algorithm. Stack Overflow dataset USED-FOR next word prediction. EMNIST and CIFAR datasets USED-FOR image classification. OtherScientificTerm are periodically shifting distributions, and daytime and nighttime modes. Method are Federated Expectation - Maximization algorithm, and mixture model. ","This paper proposes a Federated Expectation-Maximization algorithm for federated learning with distributed client data from mobile devices. The authors propose a mixture of distributions that can be used to train a model for all clients in the federation. The proposed algorithm is based on a block-cyclic pattern, which allows the network to be trained with light-weight branches. The algorithm is evaluated on image classification and next word prediction on the Stack Overflow dataset. ","This paper proposes a Federated Expectation-Maximization algorithm for federated learning of decentralized client data from mobile devices. The authors propose a mixture of distributions that can be used in federated training of machine learning models. The model can be applied to a variety of settings of different sizes of client data, and it is shown that it is able to learn the distribution shift with a block-cyclic pattern. The network is trained with light-weight branches, and the authors show that the proposed algorithm can learn distribution shift in both the daytime and nighttime modes. The algorithm is evaluated on image classification and next word prediction on the Stack Overflow dataset, and on the EMNIST and CIFAR datasets."
6213,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"accuracy EVALUATE-FOR winning ticket ” subnetwork. pruning USED-FOR DN ’s decision boundary. pruning DN nodes USED-FOR decision boundary. spline interpretation of DNs USED-FOR theory and visualization tools. DN spline theory CONJUNCTION lottery ticket hypothesis of DNs. lottery ticket hypothesis of DNs CONJUNCTION DN spline theory. early - bird ( EB ) phenomenon FEATURE-OF DN ’s spline mappings. pruning strategy USED-FOR DN nodes. spline partition regions FEATURE-OF DN nodes. spline - based DN pruning approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE spline - based DN pruning approach. accuracy EVALUATE-FOR state - of - the - art methods. networks EVALUATE-FOR spline - based DN pruning approach. spline - based DN pruning approach USED-FOR training FLOPs. accuracy EVALUATE-FOR spline - based DN pruning approach. Method are deep network ( DN ) training, and pruning technique. Generic is model. OtherScientificTerm is spline ’s partition. ","This paper studies the problem of deep network (DN) training, where the goal is to improve the accuracy of the “winning ticket” subnetwork by pruning the DN’s decision boundary. The authors propose a spline interpretation of DNs for theory and visualization tools, and propose a pruning strategy to prune DN nodes in the spline partition regions of the DN nodes. They show that the pruning technique can improve the performance of the model in terms of accuracy on a variety of networks. The spline-based DN pruning approach is also shown to be effective in training FLOPs. ","This paper proposes a new pruning technique for deep network (DN) training. The authors propose to prune the DN’s decision boundary by pruning DN nodes that are close to the “winning ticket” subnetwork. The pruning strategy is based on the DN spline theory and lottery ticket hypothesis of DNs, and the authors show that the pruning of DN nodes in the spline partition regions leads to better accuracy in training FLOPs compared to state-of-the-art methods.  The authors also propose a spline interpretation of the DNs for theory and visualization tools, and show the early-bird (EB) phenomenon of DN ’s spline mappings. "
6229,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"data representation USED-FOR optimal predictors. inner - level USED-FOR optimal group predictors. bi - level optimization USED-FOR problem. inner optimization CONJUNCTION implicit differentiation. implicit differentiation CONJUNCTION inner optimization. implicit differentiation CONJUNCTION optimization path. optimization path CONJUNCTION implicit differentiation. implicit differentiation USED-FOR implicit path alignment algorithm. inner optimization USED-FOR implicit path alignment algorithm. sufficiency rule FEATURE-OF bi - level objective. error gap EVALUATE-FOR implicit approach. classification and regression settings EVALUATE-FOR method. Task are fair representation learning perspective, and fairness measurement. Generic is representation. Method are inner - level optimization, and fair representation learning. ","This paper studies the problem of fair representation learning from the perspective of bi-level optimization, where the goal is to learn a data representation for optimal predictors at the inner-level for optimal group predictors. The authors propose an implicit approach to this problem, which combines inner optimization with implicit differentiation and an optimization path. They show that the proposed implicit approach has a lower error gap than the state-of-the-art in both classification and regression settings. ","This paper proposes a new fair representation learning perspective, where the goal is to learn optimal group predictors at the inner-level. The authors propose to use bi-level optimization to solve the problem. The main idea is to use a data representation for optimal predictors, and the representation is learned at the outer-level, which is then used for the inner optimization. The implicit path alignment algorithm is based on implicit differentiation and the optimization path. The paper also proposes a sufficiency rule for the bi -level objective. The proposed method is evaluated on classification and regression settings, and shows that the proposed implicit approach is able to reduce the error gap. "
6245,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"value - based methods USED-FOR Offline reinforcement learning ( RL ). temporal difference ( TD ) learning USED-FOR value - based methods. supervised learning methods USED-FOR optimal policies. methods COMPARE value - based approximate dynamic programming algorithms. value - based approximate dynamic programming algorithms COMPARE methods. design decisions PART-OF methods. policy architectures HYPONYM-OF design decisions. large sequence models CONJUNCTION value - based weighting schemes. value - based weighting schemes CONJUNCTION large sequence models. RvS methods COMPARE prior methods. prior methods COMPARE RvS methods. optimal data FEATURE-OF datasets. datasets HYPONYM-OF offline RL benchmarks. offline RL benchmarks EVALUATE-FOR prior methods. offline RL benchmarks EVALUATE-FOR RvS methods. Task is offline RL problem. Generic is task. Material is suboptimal data. Method is reinforcement learning via supervised learning ( RvS ). OtherScientificTerm are conditioning variable, and model capacity. ","This paper studies the offline RL problem. Offline reinforcement learning (RL) is a challenging task with suboptimal data. The authors propose to use value-based methods based on temporal difference (TD) learning with supervised learning methods to learn optimal policies. The design decisions of these methods are based on design decisions in the form of policy architectures. The goal is to learn a conditioning variable that maximizes the performance of the agent in a given task. The main contribution of this paper is to propose reinforcement learning via supervised learning (RvS). The authors show that RvS methods perform better than prior methods on a variety of offline RL benchmarks on optimal data, including large sequence models, value based weighting schemes, etc. ","This paper proposes value-based methods for Offline reinforcement learning (RL) with temporal difference (TD) learning. The main idea is to use supervised learning methods to learn optimal policies from suboptimal data. This task is an offline RL problem, where the goal is to learn a policy that maximizes the performance of the current state of the environment. The authors propose two methods, value-by-value and value by weighting, to solve this task. The design decisions of the two methods are based on design decisions in the conditioning variable, and the authors show that both methods are able to learn better design decisions compared to prior methods on several offline RL benchmarks, including datasets with optimal data (e.g., large sequence models and value based weighting schemes). The authors also show that reinforcement learning via supervised learning (RvS) outperforms prior methods in terms of model capacity."
6261,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,computational cognitive mechanisms USED-FOR exploration algorithms. spatial information USED-FOR structure of unobserved spaces. Hierarchical Bayesian framework USED-FOR program induction. program induction USED-FOR compositional formation of proposed maps of complex spaces. Map Induction USED-FOR cognitive process. distribution of strong spatial priors USED-FOR model. computational framework USED-FOR human exploration behavior. computational framework COMPARE non - inductive models. non - inductive models COMPARE computational framework. Map Induction USED-FOR approximate planning algorithms. Task is behavioral Map Induction Task. Method is Map Induction framework. ,"This paper studies the problem of behavioral Map Induction Task, where the goal is to learn the structure of unobserved spaces from spatial information. The authors propose a Hierarchical Bayesian framework for program induction, which allows for compositional formation of proposed maps of complex spaces using program induction. The proposed model is based on a distribution of strong spatial priors, which is then used to learn a cognitive process. The paper shows that the proposed computational framework is able to learn human exploration behavior and approximate planning algorithms. ",This paper proposes a new computational framework for studying the human exploration behavior. The authors propose a Hierarchical Bayesian framework for program induction for compositional formation of proposed maps of complex spaces using spatial information. The proposed framework is based on the Map Induction framework. The main idea is to use spatial information to learn the structure of unobserved spaces and then use exploration algorithms based on these computational cognitive mechanisms to learn exploration algorithms. The model is built on the distribution of strong spatial priors. The computational framework is shown to outperform non-inductive models for approximate planning algorithms. 
6277,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"differentiable approach USED-FOR probabilistic factors. probabilistic factors USED-FOR inference. nonparametric belief propagation algorithm USED-FOR differentiable approach. nonparametric belief propagation algorithm USED-FOR inference. probabilistic factors PART-OF graphical model. domain - specific features FEATURE-OF probabilistic factors. domain - specific features USED-FOR nonparametric belief propagation methods. differentiable neural network USED-FOR factors. labeled data USED-FOR optimization routine. differentiable neural network USED-FOR crafted factor. optimization routine USED-FOR factors. differentiable neural networks CONJUNCTION belief propagation algorithm. belief propagation algorithm CONJUNCTION differentiable neural networks. method USED-FOR marginal posterior samples. differentiable neural networks USED-FOR method. end - to - end training USED-FOR marginal posterior samples. end - to - end training USED-FOR method. differentiable nonparametric belief propagation ( DNBP ) method COMPARE learned baselines. learned baselines COMPARE differentiable nonparametric belief propagation ( DNBP ) method. articulated pose tracking tasks EVALUATE-FOR differentiable nonparametric belief propagation ( DNBP ) method. learned factors USED-FOR tracking. Method are hand - crafted approaches, and Gradient Descent. OtherScientificTerm is Feature Extractor. ","This paper proposes a differentiable approach for learning probabilistic factors for inference using a nonparametric belief propagation algorithm. The key idea is to learn a crafted factor from labeled data, and then use the differentiable neural network to learn the factors from the labeled data. The authors show that the proposed method is able to obtain marginal posterior samples from end-to-end training, and that the learned factors can be used for tracking. They also show that their method outperforms learned baselines on articulated pose tracking tasks.","This paper proposes a differentiable approach for learning probabilistic factors for inference. The key idea is to use a nonparametric belief propagation algorithm for inference, which is a variant of the hand-crafted approaches. The main difference is that the generated factors are generated by a differentiability of the differentiable neural network, and the crafted factor is learned using an optimization routine on labeled data. The authors show that the proposed method can be applied to marginal posterior samples generated by end-to-end training. The paper also shows that the learned factors can be used for tracking. "
6293,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"deep learning - based modeling of molecules USED-FOR in silico drug discovery. scaffold USED-FOR drug discovery projects. MoLeR HYPONYM-OF graph - based model. MoLeR COMPARE approaches. approaches COMPARE MoLeR. MoLeR COMPARE methods. methods COMPARE MoLeR. MoLeR COMPARE them. them COMPARE MoLeR. them COMPARE approaches. approaches COMPARE them. scaffoldbased tasks EVALUATE-FOR MoLeR. scaffoldbased tasks EVALUATE-FOR them. unconstrained molecular optimization tasks EVALUATE-FOR methods. unconstrained molecular optimization tasks EVALUATE-FOR MoLeR. Method are generative models, and generative procedure. OtherScientificTerm are fragmentby - fragment, scaffolds, and generation history. Generic is constraint. ","This paper proposes a graph-based model called MoLeR, which is a deep learning-based modeling of molecules for in silico drug discovery. The main idea is to use a scaffold to generate drug discovery projects from a fragmentby-fragment, and then use generative models to generate the fragment by a generative procedure. The authors show that their methods perform better than existing approaches on a variety of scaffoldbased tasks, and that they are able to learn the scaffolds better than previous approaches. ","The paper proposes a deep learning-based modeling of molecules for in silico drug discovery. The authors propose a graph-based model called MoLeR, which uses a scaffold to model drug discovery projects. The scaffolds are generated from a fragmentby-fragment, and the generative models are trained to predict the fragment by fragment. The generative procedure is based on a constraint on the generation history. The proposed methods are evaluated on a number of unconstrained molecular optimization tasks, and they outperform other approaches."
6309,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"Imitation learning algorithms USED-FOR policy. deterministic experts USED-FOR imitation learning. stationary reward FEATURE-OF reinforcement learning. reduction USED-FOR continuous control tasks. Task is recovery of expert reward. OtherScientificTerm is total variation distance. Method are imitation learner, and adversarial imitation learning. ",This paper studies the problem of recovery of expert reward in reinforcement learning with deterministic experts. The authors propose two Imitation learning algorithms to learn a policy with total variation distance between the expert and the imitation learner. The main contribution of the paper is a reduction to continuous control tasks. The paper also provides a theoretical analysis of adversarial imitation learning.,This paper proposes a new method for imitation learning with deterministic experts. The key idea is to reduce the total variation distance between the expert and the imitation learner. The authors show that this reduction can be applied to continuous control tasks. The paper also proposes two Imitation learning algorithms to learn the policy. Experiments are conducted on adversarial imitation learning.
6325,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,reweighting algorithms USED-FOR machine learning models. reweighting algorithms USED-FOR fairness. machine learning models USED-FOR fairness. overparameterized setting FEATURE-OF algorithms. reweighting USED-FOR overparameterized model. worst - group test performance COMPARE ERM. ERM COMPARE worst - group test performance. overparameterized model USED-FOR ERM interpolator. reweighting algorithms USED-FOR interpolator. reweighting algorithms COMPARE ERM. ERM COMPARE reweighting algorithms. interpolator COMPARE ERM. ERM COMPARE interpolator. worst - group performance EVALUATE-FOR ERM. regularization CONJUNCTION data augmentation. data augmentation CONJUNCTION regularization. regularization USED-FOR reweighting algorithms. data augmentation USED-FOR reweighting algorithms. worst - group test performance EVALUATE-FOR reweighting algorithms. OtherScientificTerm is model parameters. Method is theoretical backing. Generic is model. ,"This paper studies the problem of reweighting algorithms for fairness in machine learning models in the overparameterized setting. The authors consider the case where the model parameters are not known, and the goal is to improve the worst-group test performance of the ERM interpolator. The main contribution of the paper is to show that the interpolator can be reweighted using the reweightsing algorithms. The paper also shows that regularization and data augmentation can improve the performance of these reweightings algorithms. ","This paper presents a theoretical analysis of reweighting algorithms for machine learning models for fairness in the overparameterized setting. The authors propose two algorithms in this setting: (1) an ERM interpolator based on the reweighted model, and (2) an interpolator that uses the regularization and data augmentation of the model parameters. They show that the ERM outperforms the other two reweightsing algorithms in terms of worst-group test performance. They also provide theoretical backing for their results."
6341,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"Multi - agent reinforcement learning ( MARL ) USED-FOR emergent behavior. emergent behavior PART-OF complex agent - based simulations. model USED-FOR human - irrationality. Rational Inattention ( RI ) model HYPONYM-OF model. model PART-OF human - like RL agents. RI USED-FOR cost of cognitive information processing. mutual information USED-FOR RI. mutual information USED-FOR cost of cognitive information processing. multi - timestep dynamics CONJUNCTION information channels. information channels CONJUNCTION multi - timestep dynamics. heterogeneous processing costs FEATURE-OF information channels. RI USED-FOR information asymmetry. RIRL USED-FOR equilibrium behaviors. RIRL USED-FOR AI agents. AI agents USED-FOR real human behavior. Method are RL agents, and RIRL framework. OtherScientificTerm are human behavior, rational assumptions, Principal ’s inattention, inattention, and rationality assumptions. Metric is Agent welfare. ","This paper studies emergent behavior in multi-agent reinforcement learning (MARL) in complex agent-based simulations. The authors propose a model based on Rational Inattention (RI) model, which is able to capture human-irrationality in human-like RL agents.  The authors show that RI can reduce the cost of cognitive information processing by using mutual information between the agent and the agent’s inattention. They also show that the multi-timestep dynamics and the information channels have heterogeneous processing costs.   The paper also shows that the RIRL framework can be used to train RL agents that are able to learn equilibrium behaviors.  Finally, the authors provide a theoretical analysis of the benefits of using RI for information asymmetry. ","This paper proposes Multi-agent reinforcement learning (MARL) for emergent behavior in complex agent-based simulations. The model is based on Rational Inattention (RI) model, which is a model for human-like RL agents. The authors propose to use mutual information to reduce the cost of cognitive information processing, which can be achieved by minimizing the mutual information between the multi-timestep dynamics and information channels. The main contribution of the paper is to propose a novel RIRL framework, where the human behavior is modeled by rational assumptions on the agent’s inattention and the inattentions of the other agents. Agent welfare is modeled as a function of the number of agents in the environment, and the rationality assumptions are assumed to be non-trivial.  The authors show that the information channels have heterogeneous processing costs, which leads to information asymmetry. They also show that RirL can be used to learn equilibrium behaviors for AI agents that are similar to real human behavior. "
6357,SP:100c91da177504d89f1819f4fdce72ebcf848902,perturbations USED-FOR Audio adversarial attacks. lp - norm CONJUNCTION auditory masking. auditory masking CONJUNCTION lp - norm. auditory masking USED-FOR magnitude spectrogram. lp - norm CONJUNCTION waveform. waveform CONJUNCTION lp - norm. perturbations USED-FOR audio adversarial attacks. phaseoriented algorithm USED-FOR imperceptible audio adversarial examples. energy dissipation FEATURE-OF imperceptible audio adversarial examples. PhaseFool HYPONYM-OF phaseoriented algorithm. energy patterns PART-OF spectrogram. spectrogram consistency USED-FOR short - time Fourier transform ( STFT ). weighted loss function USED-FOR PhaseFool. imperceptibility EVALUATE-FOR PhaseFool. weighted loss function USED-FOR imperceptibility. PhaseFool COMPARE imperceptible counterparts. imperceptible counterparts COMPARE PhaseFool. PhaseFool USED-FOR full - sentence imperceptible audio adversarial examples. realistic simulated environmental distortions USED-FOR adversarial examples. PhaseFool USED-FOR adversarial examples. phase - oriented energy dissipation FEATURE-OF audio adversarial example. PhaseFool COMPARE perturbations. perturbations COMPARE PhaseFool. perturbations FEATURE-OF audio waveform. phase - oriented energy dissipation COMPARE perturbations. perturbations COMPARE phase - oriented energy dissipation. phase - oriented energy dissipation USED-FOR PhaseFool. Method is automatic speech recognition ( ASR ) model. OtherScientificTerm is phase spectrogram. ,"This paper proposes a new approach for automatic speech recognition (ASR) model, called PhaseFool, which is based on a phase-oriented algorithm. The main idea is to learn a magnitude spectrogram with a lp-norm, a waveform, and an auditory masking. The spectrogram consistency is achieved by a short-time Fourier transform (STFT). The weighted loss function is then used to improve the imperceptibility of the proposed approach. Experiments show that the proposed method achieves state-of-the-art performance in terms of energy dissipation on a variety of imperceptible audio adversarial examples with realistic simulated environmental distortions.","This paper proposes a phaseoriented algorithm for generating imperceptible audio adversarial examples with energy dissipation. The authors propose a phase-oriented algorithm, called PhaseFool, which is based on the automatic speech recognition (ASR) model. The phase spectrogram is composed of a magnitude spectrogram, a lp-norm, and an auditory masking. The spectrogram consistency is achieved by a short-time Fourier transform (STFT). The authors also propose a weighted loss function to improve the imperceptibility of the phaseFool. Experiments are conducted on a range of environments, including realistic simulated environmental distortions, and the authors show that the phase fool outperforms other adversarial methods. "
6373,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,Non - contrastive methods USED-FOR representations. Non - contrastive methods USED-FOR self - supervised learning. BYOL HYPONYM-OF Non - contrastive methods. BYOL HYPONYM-OF self - supervised learning. augmentation process USED-FOR representation. DirectPred USED-FOR predictor. DirectSet(α ) USED-FOR projection matrix. sample complexity EVALUATE-FOR downstream tasks. DirectSet(α ) USED-FOR downstream tasks. DirectSet(α ) USED-FOR linear network. sample complexity EVALUATE-FOR DirectSet(α ). weight decay USED-FOR implicit threshold. eigen - decomposition step USED-FOR DirectPred. CIFAR-100 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-100. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DirectCopy COMPARE DirectPred. DirectPred COMPARE DirectCopy. CIFAR-10 EVALUATE-FOR DirectCopy. ImageNet EVALUATE-FOR DirectCopy. CIFAR-10 EVALUATE-FOR DirectPred. CIFAR-10 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-10. Generic is approaches. Task is augmentation. ,"This paper studies the problem of self-supervised learning with non-contrastive methods such as BYOL and BYOL. The authors propose a new augmentation process to improve the representation of the predictor. The proposed method, DirectPred, is based on the DirectSet(α) which is a projection matrix of a linear network. DirectPred uses weight decay to reduce the implicit threshold and the eigen-decomposition step in DirectPred. The experimental results show that DirectPred achieves better sample complexity than DirectCopy on CIFAR-10, STL-10 and ImageNet.","This paper introduces BYOL, an extension of Non-contrastive methods for learning representations for self-supervised learning. The main idea of BYOL is to use the augmentation process to improve the representation of the predictor. DirectSet(α) is used to learn the projection matrix of a linear network, and the sample complexity of the downstream tasks is measured using the Direct set(α). The implicit threshold is defined by weight decay. The eigen-decomposition step of DirectPred is also used. Experiments on CIFAR-10, STL-10 and ImageNet show that DirectCopy outperforms DirectPred in terms of sample complexity. "
6389,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,method USED-FOR learning long - term sequential dependencies. Long Expressive Memory ( LEM ) USED-FOR learning long - term sequential dependencies. it USED-FOR input - output maps. it USED-FOR sequential tasks. long - term dependencies FEATURE-OF sequential tasks. time - discretization USED-FOR system. system of multiscale ordinary differential equations CONJUNCTION time - discretization. time - discretization CONJUNCTION system of multiscale ordinary differential equations. system of multiscale ordinary differential equations USED-FOR LEM. rigorous bounds USED-FOR exploding and vanishing gradients problem. rigorous bounds USED-FOR LEM. LEM USED-FOR dynamical systems. accuracy EVALUATE-FOR LEM. gated recurrent units CONJUNCTION long short - term memory models. long short - term memory models CONJUNCTION gated recurrent units. recurrent neural networks CONJUNCTION gated recurrent units. gated recurrent units CONJUNCTION recurrent neural networks. LEM COMPARE recurrent neural networks. recurrent neural networks COMPARE LEM. image and time - series classification CONJUNCTION dynamical systems prediction. dynamical systems prediction CONJUNCTION image and time - series classification. LEM COMPARE long short - term memory models. long short - term memory models COMPARE LEM. dynamical systems prediction CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION dynamical systems prediction. LEM COMPARE gated recurrent units. gated recurrent units COMPARE LEM. image and time - series classification CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION image and time - series classification. Method is gradient - based recurrent sequential learning methods. ,"This paper proposes a method for learning long-term sequential dependencies in Long Expressive Memory (LEM). The system is based on a system of multiscale ordinary differential equations and time-discretization, and it is able to learn input-output maps. The authors provide rigorous bounds for the exploding and vanishing gradients problem, and show that LEM achieves better accuracy than long short-term memory models, gated recurrent units, and recurrent neural networks. LEM can also be used to learn dynamical systems. The paper also shows that it can be used for sequential tasks with long-time dependencies. ","This paper proposes Long Expressive Memory (LEM), a method for learning long-term sequential dependencies. The system is based on a system of multiscale ordinary differential equations and time-discretization, and it can be used to learn input-output maps. The authors provide rigorous bounds for the exploding and vanishing gradients problem, and show that LEM outperforms recurrent neural networks, gated recurrent units, and long short-term memory models in terms of accuracy. LEM is also applied to dynamical systems prediction, image and time series classification, and keyword spotting and language modeling. "
6405,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"symmetry USED-FOR architectures. architectures USED-FOR deep learning. rotation CONJUNCTION permutation equivariance. permutation equivariance CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. rotationand permutation - equivariant architectures USED-FOR deep learning. rotationand permutation - equivariant architectures USED-FOR small point clouds. products USED-FOR reductions. reductions PART-OF rotationand permutation - equivariant architectures. attention mechanism USED-FOR reductions. rotation invariance CONJUNCTION covariance. covariance CONJUNCTION rotation invariance. attention USED-FOR permutation equivariance. chemistry CONJUNCTION biology. biology CONJUNCTION chemistry. physics CONJUNCTION chemistry. chemistry CONJUNCTION physics. models USED-FOR architectures. Method are geometric deep learning, and geometric algebra. Task is physical sciences. OtherScientificTerm are twoor three - dimensional space, and vector. ",This paper studies the problem of geometric deep learning in the setting of physical sciences. The authors propose two architectures based on symmetry for deep learning: rotation and permutation-equivariant architectures for small point clouds. These two reductions are based on two products: rotation invariance and covariance. The attention mechanism in these two reductions is based on the geometric algebra. The paper also provides a theoretical analysis of the properties of these models.,"This paper proposes two new architectures for deep learning based on symmetry. The main idea is to use geometric deep learning, where the twoor three-dimensional space is represented as a vector. The authors propose two new reductions of the two products of these two products: rotation and permutation equivariance. The reductions of rotationand permutation-equivariant architectures for small point clouds are based on the attention mechanism. The proposed models are evaluated on three physical sciences: chemistry, biology, and chemistry."
6421,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,order fulfillment problem HYPONYM-OF combinatorial optimization problems. it USED-FOR online retailing. combinatorial optimization problems PART-OF supply chain management. order fulfillment problem HYPONYM-OF supply chain management. exact mathematical programming methods USED-FOR problem. machine learning method USED-FOR it. tripartite graph USED-FOR machine learning method. edge - featureembedded graph attention mechanism USED-FOR assignment policy. edge - feature - embedded graph attention USED-FOR optimization problem. edge - feature - embedded graph attention USED-FOR heterogeneous information. edge - feature - embedded graph attention USED-FOR high - dimensional edge features. model COMPARE baseline heuristic method. baseline heuristic method COMPARE model. optimality EVALUATE-FOR baseline heuristic method. optimality EVALUATE-FOR model. online inference time COMPARE mathematical programming methods. mathematical programming methods COMPARE online inference time. ,This paper studies the order fulfillment problem in combinatorial optimization problems in the context of supply chain management. The authors propose a machine learning method to solve it using a tripartite graph. The optimization problem is formulated as an edge-feature-embedded graph attention mechanism to learn the assignment policy. The paper shows that the proposed model can achieve better online inference time than the baseline heuristic method. ,"This paper proposes a novel approach to solve the order fulfillment problem, a combinatorial optimization problem in supply chain management. The problem is formulated as an exact mathematical programming methods, and it is used for online retailing. The paper proposes to solve it using a machine learning method on a tripartite graph. The authors propose an edge-featureembedded graph attention mechanism to learn the assignment policy. The optimization problem is solved by using edge-features-embeddings of high-dimensional edge features. The proposed model is shown to outperform the baseline heuristic method in terms of optimality and online inference time compared to other mathematical Programming methods. "
6437,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"natural language processing models USED-FOR inference process. inferring Concepts PART-OF dialogue summarization task. CODC inference module CONJUNCTION knowledge attention module. knowledge attention module CONJUNCTION CODC inference module. knowledge attention module PART-OF neural summarization model. CODC inference module PART-OF framework. knowledge attention module PART-OF framework. CODC USED-FOR evaluation metric. evaluation metric EVALUATE-FOR methods. automatic evaluation metrics EVALUATE-FOR out - of - context inference. automatic evaluation metrics EVALUATE-FOR natural language generation. CODC inference CONJUNCTION automatic evaluation metrics. automatic evaluation metrics CONJUNCTION CODC inference. natural language generation EVALUATE-FOR out - of - context inference. CODC inference EVALUATE-FOR summarization model. automatic evaluation metrics EVALUATE-FOR summarization model. CIDEr HYPONYM-OF automatic evaluation metrics. model EVALUATE-FOR model. Material are human dialogues, and WordNet. OtherScientificTerm are pragmatics studies, and Dialogue Context ( CODC ). ","This paper studies the problem of natural language processing models for the inference process in the context of dialogue summarization task. The authors propose a framework that combines the CODC inference module and the knowledge attention module in a neural summarization model. The evaluation metric is based on the COCO, and the authors show that the proposed methods perform better than existing methods in terms of the evaluation metric. The paper also shows that the automatic evaluation metrics for natural language generation and out-of-context inference (e.g. CIDEr) can be improved by the proposed model. ","This paper proposes a novel framework for learning natural language processing models for the inference process of dialogue summarization task. The framework consists of a CODC inference module, a knowledge attention module, and a neural summarization model. The evaluation metric of the proposed methods is based on the evaluation metric in the Dialogue Context (CODC). The evaluation metrics are based on CIDEr, a set of automatic evaluation metrics for natural language generation and out-of-context inference. The authors show that the proposed model outperforms the state of the art in terms of the number of human dialogues generated by WordNet."
6446,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"entity embeddings USED-FOR Artificial Intelligence. embedding models USED-FOR Horn rules. embedding strategies USED-FOR monotonic and non - monotonic attribute dependencies. OtherScientificTerm are embeddings, vectors, semantic dependencies, and attribute dependencies. Method is attribute embeddings. ","This paper studies the problem of learning entity embeddings for Artificial Intelligence. The authors propose two embedding strategies for monotonic and non-monotonic attribute dependencies. The main idea is to use embedding models to learn the Horn rules, and then use them to learn a set of attributes that are independent of the embedding. The key idea is that these attributes are learned as vectors, and that the semantic dependencies between the vectors and the attributes can be used to encode the attribute dependencies, which can be then used to train the attribute embedding model. Experiments show that the proposed approach can achieve better performance than existing methods.","This paper studies the problem of learning entity embeddings for Artificial Intelligence. The authors propose two embedding models for learning Horn rules, one for monotonic and one for non-monotonic attribute dependencies. The main idea of the paper is to use embedding strategies to learn both monotsonic and non-Monotonic attributions. The key idea is to learn a set of vectors that are independent of each other, and then use these vectors to encode the semantic dependencies between the attribute dependencies and the embedding vectors. "
6455,SP:794cca5205d667900ceb9a1332b6272320752ef4,transformer - based models USED-FOR natural language processing tasks. transformers USED-FOR natural language. commonsense reasoning CONJUNCTION logical reasoning. logical reasoning CONJUNCTION commonsense reasoning. mathematical reasoning CONJUNCTION commonsense reasoning. commonsense reasoning CONJUNCTION mathematical reasoning. transformers USED-FOR reasoning tasks. mathematical reasoning HYPONYM-OF reasoning tasks. logical reasoning HYPONYM-OF reasoning tasks. commonsense reasoning HYPONYM-OF reasoning tasks. ,"This paper proposes a transformer-based models for natural language processing tasks. The proposed transformers can be applied to natural language. The reasoning tasks considered include commonsense reasoning, mathematical reasoning, logical reasoning, etc. The experiments show the effectiveness of the proposed model.","This paper proposes to use transformer-based models for natural language processing tasks. The main idea is to use transformers to learn natural language. Experiments are conducted on reasoning tasks such as mathematical reasoning, commonsense reasoning, logical reasoning, etc."
6464,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"paradigms PART-OF machine learning. complexity FEATURE-OF tasks. algorithmic procedures USED-FOR representation transformations. domaingeneral framework USED-FOR algorithmic procedures. compositional recursive learner HYPONYM-OF domaingeneral framework. compositional recursive learner USED-FOR compositional generalization. compositional approach COMPARE baselines. baselines COMPARE compositional approach. compositional approach COMPARE learner. learner COMPARE compositional approach. Generic are it, and they. Task are generalization, and compositional generalization problem. OtherScientificTerm are compositional structure of the task distribution, compositional problem graph, and shared subproblems. ","This paper studies the problem of generalization in machine learning with paradigms in the compositional structure of the task distribution. The authors propose a compositional recursive learner based on the domaingeneral framework, which is a combination of algorithmic procedures for representation transformations. The compositional generalization problem is formulated in terms of compositional problem graph, where each task is represented as a set of shared subproblems, and it is assumed that each subproproblem has a similar structure. The main contribution of the paper is to show that the complexity of these tasks is bounded by the number of tasks, and that it is possible to generalize to new tasks by using the learned learner. The paper also shows that this compositional approach performs better than other baselines.","This paper proposes a new approach to generalization in machine learning. The authors propose a compositional recursive learner, where the compositional structure of the task distribution is represented as a compositionally problem graph. The compositional generalization problem is formulated as an optimization problem, and the authors propose an algorithmic procedures for representation transformations. The proposed method is based on the domaingeneral framework, which is an extension of the previous work. The paper shows that the proposed compositional approach outperforms other baselines in terms of generalization, and it is shown that it is able to generalize to more complex tasks with higher complexity. "
6468,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"weight pruning HYPONYM-OF network model compression. activation pruning CONJUNCTION weight pruning. weight pruning CONJUNCTION activation pruning. weight pruning PART-OF Integral Pruning ( IP ) technique. activation pruning PART-OF Integral Pruning ( IP ) technique. IPnet HYPONYM-OF network. execution efficiency EVALUATE-FOR network. activation functions FEATURE-OF network models. datasets EVALUATE-FOR network models. network models EVALUATE-FOR IPnet. datasets EVALUATE-FOR IPnet. testing accuracy EVALUATE-FOR IPnet. IPnet COMPARE dense models. dense models COMPARE IPnet. computation cost EVALUATE-FOR IPnet. Method are deep neural networks ( DNNs ), and DNNs. Task is compression. OtherScientificTerm are weights, connections, sparsity, and activation and weight numbers. ","This paper studies the problem of weight pruning in the context of network model compression. The authors consider the case of deep neural networks (DNNs), where the weights of the network are connected by connections, and the network is trained with activation pruning (i.e., the weights are multiplied by the number of connections). The authors propose a new Integral Pruning (IP) technique, which combines the benefits of the activation pruned and weight pruned versions of the same network.  The authors show that the proposed network, IPnet, can achieve better execution efficiency than the state-of-the-art in terms of computation cost. They also show that IPnet performs better than other network models on a variety of datasets.","This paper proposes a new method for training deep neural networks (DNNs) with weight pruning. The authors propose Integral Pruning (IP) technique, which combines activation pruning and weight Pruning to improve network model compression. They show that the proposed network can achieve better execution efficiency and lower computation cost compared to other network models with different activation functions. They also show that IPnet achieves better test accuracy compared to existing network models on a variety of datasets. "
6472,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"discovery of relevant features USED-FOR task. Machine learning driven feature selection USED-FOR discovery. methodology USED-FOR False Discovery Rate. Generative Adversarial Networks framework USED-FOR knockoffs. stability network CONJUNCTION power network. power network CONJUNCTION stability network. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. discriminator CONJUNCTION stability network. stability network CONJUNCTION discriminator. networks CONJUNCTION generator. generator CONJUNCTION networks. generator CONJUNCTION stability network. stability network CONJUNCTION generator. discriminator CONJUNCTION power network. power network CONJUNCTION discriminator. networks PART-OF model. discriminator PART-OF model. stability network PART-OF model. generator PART-OF model. power network PART-OF model. model USED-FOR feature selection. it COMPARE model. model COMPARE it. it COMPARE knockoff generation model. knockoff generation model COMPARE it. model USED-FOR non - Gaussian settings. knockoff generation model USED-FOR Gaussian setting. non - Gaussian settings EVALUATE-FOR it. real - world dataset EVALUATE-FOR it. real - world dataset EVALUATE-FOR model. Task are Feature selection, and overfitting in prediction. OtherScientificTerm are relevant genetic factors, features, and feature distribution. Metric is expert time. Method is Knockoff framework. ","This paper studies the problem of feature selection in machine learning. The authors propose a new method for feature selection based on the Generative Adversarial Networks framework. The key idea is to learn a set of relevant genetic factors, which are then used to train a discriminator, a stability network, a power network, and a generator. The discriminator and the generator are trained in parallel to the discriminator. The generator is trained in the same way as a knockoff generation model in the Gaussian setting, while the power network and the stability network are trained separately in the non-Gaussian setting. The model is evaluated on a real-world dataset and shows that it performs better than the previous state-of-the-art model in feature selection. ","This paper proposes a new method for feature selection in the context of machine learning. Feature selection is an important problem in machine learning, where the goal is to learn a set of relevant features for a given task. The key idea is to use a generative adversarial network (GAN) to learn the relevant genetic factors. The authors propose a new methodology to reduce the False Discovery Rate (FDR) of the GAN. The FDR is based on the Generative Adversarial Networks framework, which is used to generate knockoffs of the target feature distribution. The proposed method is evaluated on a real-world dataset, where it outperforms a knockoff generation model in the Gaussian setting. The model is also evaluated on non-Gaussian settings where it is shown to outperform the model in feature selection. "
6476,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"Pruning techniques CONJUNCTION Winograd convolution. Winograd convolution CONJUNCTION Pruning techniques. Winograd convolution USED-FOR CNN computation. Pruning techniques USED-FOR CNN computation. Winograd transformation USED-FOR sparsity. learning rates USED-FOR Winograd - domain retraining. ReLU function PART-OF Winograd domain. pruning method USED-FOR Winograd - domain weight sparsity. spatial - Winograd pruning HYPONYM-OF pruning method. importance factor matrix USED-FOR weight importance. importance factor matrix USED-FOR weight gradients. weight importance CONJUNCTION weight gradients. weight gradients CONJUNCTION weight importance. pruning PART-OF Winograd domain. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. models EVALUATE-FOR method. datasets EVALUATE-FOR method. datasets EVALUATE-FOR models. CIFAR10 HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. Winograddomain sparsities EVALUATE-FOR method. Method are Deep convolutional neural networks ( CNNs ), sparse Winograd convolution, and Winograd - domain network. Generic are they, and technique. OtherScientificTerm are weight sparsity, network structure, network structures, spatial - domain weights, and spatial - domain sparsity. ","This paper studies the problem of weight sparsity in Deep convolutional neural networks (CNNs). Pruning techniques and Winograd convolution are popular methods for CNN computation. However, they can be expensive to train, and they can lead to sparsity. To address this problem, the authors propose a new pruning method called spatial-Winograd pruning, which aims to improve the performance of the Winograddomain sparsities of the network structure. The main idea is to learn a ReLU function for each layer in the network, which is then used to learn the weights of each layer. The authors show that the weight gradients and the importance factor matrix can be used to estimate the weight importance of the weights, which can then be used in the weight-based pruning.  The authors also show that this technique can be applied to the Winodrad-domain network. ","This paper studies the problem of weight sparsity in Deep convolutional neural networks (CNNs). Pruning techniques and Winograd convolution are commonly used in CNN computation. However, they are expensive to compute, and the authors propose a novel pruning method for reducing the Winograddomain weight spep. The authors propose to use a sparse Winogrand convolution, where the network structure is sparse and the weights of the network structures are sparse. The sparsity is reduced by a simple and effective technique called spatial-Winograd pruning, which is based on the ReLU function in the Winovrad domain. The main contribution of the paper is to propose a Winovad transformation to reduce the sparsity, which can be applied to any sparsity. This technique is evaluated on two datasets, CIFAR-10 and ImageNet, and shows that the proposed method outperforms existing models on both datasets. "
6480,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,Adversarially Learned Mixture Model ( AMM ) HYPONYM-OF generative model. generative model USED-FOR unsupervised or semi - supervised data clustering. adversarially optimized method USED-FOR conditional dependence. AMM USED-FOR conditional dependence. AMM HYPONYM-OF adversarially optimized method. AMM USED-FOR semantic separation of complex data. MNIST and SVHN datasets EVALUATE-FOR AMM. unsupervised clustering error rates EVALUATE-FOR AMM. MNIST and SVHN datasets EVALUATE-FOR AMM. AMM USED-FOR semi - supervised extension. SVHN dataset EVALUATE-FOR semi - supervised extension. classification error rate EVALUATE-FOR semi - supervised extension. Material is labeled data. ,"This paper proposes a generative model for unsupervised or semi-supervised data clustering, called Adversarially Learned Mixture Model (AMM). AMM is an adversarially optimized method for conditional dependence. The authors show that AMM can be used for semantic separation of complex data. They also show that the unsupersupervised clustering error rates of AMM are lower than the classification error rate of the standard AMM on the MNIST and SVHN datasets. Finally, the authors demonstrate that the AMM with a semi -supervised extension on the SVN dataset is able to achieve a classification error of $O(\sqrt{T})$ for labeled data.","This paper proposes a generative model for unsupervised or semi-supervised data clustering, called Adversarially Learned Mixture Model (AMM). AMM is an adversarially optimized method for conditional dependence. The authors show that AMM can be used for semantic separation of complex data. They show that the proposed AMM has better unsupersupervised clustering error rates and better classification error rate on MNIST and SVHN datasets compared to AMM. They also demonstrate that the AMM achieves better semi-vised extension on the SVN dataset. "
6484,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"stochastic binary layers USED-FOR gradients. reparameterization USED-FOR ARM estimator. REINFORCE CONJUNCTION reparameterization. reparameterization CONJUNCTION REINFORCE. variable augmentation CONJUNCTION REINFORCE. REINFORCE CONJUNCTION variable augmentation. variable augmentation USED-FOR ARM estimator. REINFORCE USED-FOR ARM estimator. adaptive variance reduction USED-FOR Monte Carlo integration. ARM estimator USED-FOR adaptive variance reduction. REINFORCE estimator USED-FOR augmented space. antithetic sampling USED-FOR augmented space. variance - reduction mechanism USED-FOR ARM estimator. antithetic sampling USED-FOR variance - reduction mechanism. auto - encoding variational inference CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION auto - encoding variational inference. ARM estimator USED-FOR discrete latent variable models. ARM estimator USED-FOR maximum likelihood estimation. ARM estimator USED-FOR auto - encoding variational inference. stochastic binary layers FEATURE-OF discrete latent variable models. Metric is computational complexity. OtherScientificTerm are common random numbers, and Python code. ","This paper proposes a new estimator based on REINFORCE and reparameterization to improve the performance of the ARM estimator. The proposed estimator is based on adaptive variance reduction for Monte Carlo integration. The variance-reduction mechanism is a variant of the variance reduction mechanism used in the standard estimator, and the augmented space is augmented by antithetic sampling. The authors show that their estimator achieves better performance than the existing estimator in terms of the number of parameters and the computational complexity. They also show that the estimator can be used for auto-encoding variational inference and maximum likelihood estimation.","This paper proposes a new estimator based on reparameterization, REINFORCE, and a variance-reduction mechanism to reduce the computational complexity of an ARM estimator. The main idea is to add stochastic binary layers to the gradients of the input data, which are common random numbers. The authors show that the proposed estimator can be used in an augmented space with adaptive variance reduction for Monte Carlo integration. They also show that this estimator is more robust to adversarial perturbations than the original estimator in terms of the number of parameters and the variance of the estimator, and that the variance augmentation is more effective than the standard variable augmentation.    The authors also provide a theoretical analysis of their estimator for auto-encoding variational inference and maximum likelihood estimation in discrete latent variable models, which is an extension of the previous work of [Zhang et al., 2017]. "
6488,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,Modularity PART-OF deep learning libraries. probabilistic distributions CONJUNCTION probabilistic model. probabilistic model CONJUNCTION probabilistic distributions. modularity FEATURE-OF probabilistic programming language. Bayesian nonparametric models HYPONYM-OF probabilistic model. probabilistic modules HYPONYM-OF re - usable building blocks. re - usable building blocks PART-OF modular probabilistic programming language. probabilistic distributions FEATURE-OF random variables. random variables PART-OF probabilistic module. probabilistic distributions PART-OF probabilistic module. inference methods PART-OF probabilistic module. pre - specified inference methods USED-FOR probabilistic modules. pre - specified inference methods USED-FOR variational inference. probabilistic modules PART-OF MXFusion. Gaussian process models EVALUATE-FOR probabilistic modules. real data EVALUATE-FOR probabilistic modules. Method is probabilistic programming. ,"This paper studies the problem of modularity in deep learning libraries. The authors propose a modular probabilistic programming language with re-usable building blocks that can be combined with probabilistically-trained Bayesian nonparametric models. The main idea is to learn a probabilism module that is able to learn random variables in the form of random variables, and then use inference methods to infer the true distribution of these random variables. The probabilistics modules are then used to perform variational inference using pre-specified inference methods. Experiments on Gaussian process models demonstrate the effectiveness of the probabilists modules in MXFusion.","This paper proposes a modular probabilistic programming language with re-usable building blocks. The modularity of deep learning libraries is an important topic in the literature. The authors propose to use Bayesian nonparametric models to train a probabilistically model, which is based on probabilism distributions of random variables, and then use probabilistics modules, which are based on Bayesian Nonparametric Models (BNM), to learn the probablistic distributions of the random variables. They show that the probabilists modules can be used for variational inference using pre-specified inference methods. They also show that probabilisms can be learned using Gaussian process models. They evaluate the performance of the proposed modules on real data. "
6492,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"heuristically designed pruning schedules CONJUNCTION hyperparameters. hyperparameters CONJUNCTION heuristically designed pruning schedules. heuristically designed pruning schedules USED-FOR iterative optimization procedure. pruning PART-OF methods. iterative optimization procedure USED-FOR pruning. approach USED-FOR network. structurally important connections PART-OF network. connection sensitivity USED-FOR structurally important connections. network USED-FOR task. structurally important connections USED-FOR task. connection sensitivity USED-FOR saliency criterion. pretraining CONJUNCTION complex pruning schedule. complex pruning schedule CONJUNCTION pretraining. it USED-FOR architecture variations. pruning USED-FOR sparse network. method COMPARE reference network. reference network COMPARE method. accuracy EVALUATE-FOR reference network. method USED-FOR architectures. method USED-FOR sparse networks. sparse networks COMPARE reference network. reference network COMPARE sparse networks. Tiny - ImageNet classification tasks EVALUATE-FOR method. convolutional, residual and recurrent networks HYPONYM-OF architectures. MNIST EVALUATE-FOR method. Tiny - ImageNet classification tasks EVALUATE-FOR reference network. MNIST EVALUATE-FOR reference network. accuracy EVALUATE-FOR method. methods COMPARE approach. approach COMPARE methods. Task are Pruning large neural networks, and training. Metric is reduced space and time complexity. ","This paper studies the problem of Pruning large neural networks. The authors propose an iterative optimization procedure that uses heuristically designed pruning schedules and hyperparameters to optimize the network. The proposed approach is based on pruning PART of the network by using the connection sensitivity between the structurally important connections in the network for a given task. The saliency criterion is derived by using connection sensitivity for each task.  The authors show that the proposed method improves the performance of sparse networks compared to the reference network on MNIST and Tiny-ImageNet classification tasks. They also show that it can be applied to architecture variations such as convolutional, residual and recurrent networks. ","This paper proposes a novel approach for pruning large neural networks. The authors propose to use heuristically designed pruning schedules and hyperparameters, and an iterative optimization procedure to prune the network. The network is trained on a task with a set of structurally important connections. The saliency criterion is based on the connection sensitivity of the network to the task, and the authors show that the reduced space and time complexity leads to better performance. The proposed method is evaluated on MNIST and Tiny-ImageNet classification tasks, and compared to other architectures such as convolutional, residual and recurrent networks. It is shown that the proposed approach outperforms other methods for sparse networks and the reference network in terms of accuracy. "
6496,SP:986b9781534ffec84619872cd269ad48d235f869,"inference algorithm USED-FOR decoding neural sequence models. Beam search HYPONYM-OF inference algorithm. Beam search USED-FOR decoding neural sequence models. greedy search COMPARE beam search. beam search COMPARE greedy search. beam search USED-FOR non - greedy local decisions. beam widths USED-FOR beam search. beam search algorithm USED-FOR sequence synthesis tasks. evaluation score FEATURE-OF sequences. highly non - greedy decisions USED-FOR beam search. constrained beam search USED-FOR beam search degradation. methods USED-FOR search. OtherScientificTerm are beam width, early and highly non - greedy decisions, and ( conditional ) probability. Metric is evaluation scores. Material is copies. ","This paper proposes a new inference algorithm, Beam search, for decoding neural sequence models. The main idea is to use beam widths in beam search for non-greedy local decisions. The authors show that beam search performs better than greedy search for both early and highly non-guaranteed decisions. They also show that the evaluation score for sequences with high evaluation score can be used to improve the performance of the beam search algorithm for sequence synthesis tasks. ","This paper proposes a new inference algorithm for decoding neural sequence models, called Beam search. The main idea is to use beam widths to learn non-greedy local decisions. The authors show that beam search is more efficient than greedy search, and that the evaluation scores of the sequences with high evaluation score are better than the evaluation score of sequences with low evaluation score. They also show that the beam search algorithm can be used for sequence synthesis tasks. The paper also shows that the proposed methods are more robust to beam search degradation compared to other methods for search. "
6500,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"method USED-FOR sample efficiency. demonstrations USED-FOR method. curriculum USED-FOR task. Backplay COMPARE competitive methods. competitive methods COMPARE Backplay. Backplay USED-FOR large grid worlds. sample efficiency EVALUATE-FOR competitive methods. training speed EVALUATE-FOR Backplay. sample efficiency EVALUATE-FOR Backplay. reward shaping CONJUNCTION behavioral cloning. behavioral cloning CONJUNCTION reward shaping. behavioral cloning CONJUNCTION reverse curriculum generation. reverse curriculum generation CONJUNCTION behavioral cloning. Task is Model - free reinforcement learning ( RL ). OtherScientificTerm are policy, and sparse rewards. Generic is approach. ","This paper proposes a method for improving sample efficiency in Model-free reinforcement learning (RL). The proposed method is based on demonstrations, where the goal is to learn a curriculum for a given task. The curriculum is a combination of a policy and a set of demonstrations. The goal of the approach is to maximize the sample efficiency of the learned policy. Backplay is shown to improve sample efficiency over competitive methods in large grid worlds. The authors also show that Backplay improves the training speed in terms of the number of demonstrations required. The proposed approach is evaluated on reward shaping, behavioral cloning, and reverse curriculum generation.","This paper proposes a method for improving sample efficiency in Model-free reinforcement learning (RL). The method is based on demonstrations, where the goal is to learn a curriculum for a given task. The goal is not to learn the policy, but to learn sparse rewards. Backplay is shown to improve the sample efficiency of large grid worlds compared to competitive methods in terms of training speed. The proposed approach is evaluated on reward shaping, behavioral cloning, and reverse curriculum generation."
6504,SP:426c98718b2dbad640380ec4ccb2b656958389bc,"Model compression USED-FOR large neural networks. model size CONJUNCTION accuracy. accuracy CONJUNCTION model size. hand - crafted heuristics USED-FOR compression techniques. AlexNet CONJUNCTION VGG16. VGG16 CONJUNCTION AlexNet. hyper - parameters USED-FOR method. expert knowledge USED-FOR method. OtherScientificTerm are compression ratio, compression ratios, Hessian, and pruning criterion. Method are Multi - Layer Pruning method ( MLPrune ), and Kroneckerfactored Approximate Curvature method. Generic is state - of - theart. Material is ImageNet. ","This paper proposes a Multi-Layer Pruning method (MLPrune) for large neural networks. The proposed method is based on hand-crafted heuristics, where the compression ratio is determined by the Hessian. The authors show that the proposed method can be combined with hyper-parameters from expert knowledge. They also provide a pruning criterion for the compression ratios. ","This paper proposes a method for model compression for large neural networks. The authors propose a Multi-Layer Pruning method (MLPrune), which uses hand-crafted heuristics to improve the compression techniques. The compression ratio is based on the Hessian of the model size and accuracy. The proposed method uses hyper-parameters and expert knowledge to optimize the compression ratios. The pruning criterion is a Kroneckerfactored Approximate Curvature method, which is a variant of the state-of-theart. Experiments are performed on ImageNet, AlexNet and VGG16."
6508,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"sample quality CONJUNCTION training stability. training stability CONJUNCTION sample quality. Generative Adversarial Networks ( GANs ) USED-FOR real - world applications. training stability EVALUATE-FOR GAN variants. sample quality EVALUATE-FOR GAN variants. architectural choices USED-FOR GAN learning. analytic framework USED-FOR GANs. unit-, object-, and scene - level FEATURE-OF GANs. object concepts FEATURE-OF interpretable units. segmentation - based network dissection method USED-FOR interpretable units. object concepts PART-OF images. models CONJUNCTION datasets. datasets CONJUNCTION models. internal representations CONJUNCTION models. models CONJUNCTION internal representations. artifact - causing units USED-FOR GANs. framework USED-FOR applications. open source interpretation tools USED-FOR GAN models. Generic are they, and units. Method is GAN. OtherScientificTerm is interventions. ","This paper studies the problem of GANs in real-world applications. The authors propose a new analytic framework for GAN learning based on architectural choices. They show that GAN variants with different sample quality and training stability can have different performance in terms of sample quality, training stability, and sample quality. They also show that they can be interpretable at the unit-, object-object, and scene-level. They then propose a segmentation-based network dissection method to learn interpretable units based on object concepts in images, models, datasets, and internal representations. Finally, they propose a framework for open source interpretation tools to train GAN models. ","This paper proposes a new analytic framework for GANs for real-world applications. Generative Adversarial Networks (GANs) can be applied to a wide range of applications, where they can be used at the unit-, object-, and scene-level. GAN learning is based on architectural choices, where the goal is to maximize the sample quality and training stability of the GAN variants. The authors propose a novel segmentation-based network dissection method to generate interpretable units based on object concepts in images, models, datasets, and internal representations. The GAN models are trained using open source interpretation tools, and they are evaluated on a variety of applications. The experiments show that the proposed framework is able to achieve state-of-the-art performance on several applications. "
6512,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"parameter ` distances COMPARE function L distances. function L distances COMPARE parameter ` distances. space FEATURE-OF networks. L distance USED-FOR optimization. L - space FEATURE-OF network. loss curvature HYPONYM-OF local approximations. Method is neural network. OtherScientificTerm are L Hilbert space, optimization trajectory, catastrophic forgetting, learning rule, and function distances. Generic are distances, and applications. Metric is L/ ` ratio. Task is multitask learning. ","This paper studies the problem of multitask learning in the L-space of a neural network, where the L Hilbert space of the network is the space of a function L distance between the parameters of the neural network and the function L distances of the functions. The authors show that the L distance can be used for optimization in the optimization trajectory, and that it can also be used as a parameter for the learning rule. They also show that local approximations such as loss curvature can be useful for the L/`ratio of the function distances. ","This paper proposes a new way to measure the L-space of a neural network. The authors define the L Hilbert space as the space of networks in which the parameters of the network are in the L/` ratio, and the function L distances are the parameter ` distances. The L distance is used for optimization in the optimization trajectory. The paper also introduces a new learning rule, called catastrophic forgetting, which is used to quantify the distance between the parameters and the functions of the networks in the space. Experiments show that the proposed loss curvature of local approximations can be used as a measure of the distance of a network to the L -space. "
6516,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,measurements of dynamic entities PART-OF Biological data. DyMoN HYPONYM-OF neural network framework. probability distribution FEATURE-OF deep generative Markov model. deep generative Markov model USED-FOR neural network framework. deep generative Markov model USED-FOR DyMoN. Dynamics Modeling Network HYPONYM-OF deep neural network framework. DyMoN USED-FOR idiosyncrasies of biological data. noise CONJUNCTION sparsity. sparsity CONJUNCTION noise. data USED-FOR probability distributions. trajectories HYPONYM-OF probability distributions. dimensionality reduction methods USED-FOR trajectories. probability distributions USED-FOR DyMoN. training efficiency CONJUNCTION accuracy. accuracy CONJUNCTION training efficiency. Kalman filters CONJUNCTION hidden Markov models. hidden Markov models CONJUNCTION Kalman filters. deep models COMPARE shallow models. shallow models COMPARE deep models. hidden Markov models HYPONYM-OF shallow models. Kalman filters HYPONYM-OF shallow models. DyMoN USED-FOR biological systems. DyMoN USED-FOR features of the dynamics. OtherScientificTerm is longitudinal measurements. Material is biological data. Generic is model. ,"This paper proposes a deep generative Markov model, DyMoN, which is a neural network framework based on the Dynamics Modeling Network (DynamicsNet). The authors propose to use the probability distribution of a Deep Generative Model (DyMoN) to learn the features of the dynamics of biological data, which are measurements of dynamic entities in biological data. The authors show that the probability distributions obtained by DyMoNs can be used to learn probability distributions of the trajectories, which can then be used as dimensionality reduction methods to learn trajectories of biological systems.  The authors also show that DyMoIs can be trained with noise and sparsity, and that the training efficiency and accuracy can be improved by training with Kalman filters, hidden Markov models, and shallow models.  ","This paper proposes a deep generative Markov model for biological data, called DyMoN, which is a neural network framework based on the Dynamics Modeling Network. The model is based on a probability distribution of the probability distribution over a set of biological data (measurements of dynamic entities in Biological data). The probability distributions are derived from the data using dimensionality reduction methods. The authors show that the probability distributions over the data can be represented as trajectories, and that the model is robust to noise and sparsity. The paper also shows that the training efficiency and accuracy of the model can be improved compared to shallow models such as Kalman filters and hidden Markov models. "
6520,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"graph Laplacian USED-FOR unsupervised classification method. classification method COMPARE architecture. architecture COMPARE classification method. approximate linear map CONJUNCTION spectral clustering theory. spectral clustering theory CONJUNCTION approximate linear map. spectral clustering theory USED-FOR generative adversarial networks. approximate linear map USED-FOR generative adversarial networks. spectral clustering theory USED-FOR dimension reduced spaces. framework USED-FOR images. approximate linear connector network C USED-FOR cerebral cortex. spectral clustering HYPONYM-OF unsupervised learning. unsupervised classification method USED-FOR method. Method are human visual recognition system, and connector network. OtherScientificTerm is human brains. ",This paper proposes a graph Laplacian for unsupervised classification method based on the graph neural network. The proposed method is based on an existing framework that uses an approximate linear map and spectral clustering theory for generative adversarial networks. The authors show that the proposed framework is able to generate images that are similar to the human visual recognition system. They also show that this framework can be applied to images generated by the approximate linear connector network C in the cerebral cortex. ,"This paper proposes a graph Laplacian for unsupervised classification method. The proposed classification method is compared to an existing architecture based on an approximate linear map and spectral clustering theory for generative adversarial networks. The paper also proposes a framework for generating images from a human visual recognition system. The framework is based on the idea that the images are generated from a set of human brains, where each brain is represented by a different type of connector network. The cerebral cortex is represented as a set, and the approximate linear connector network C is used to map the images from the human brains to the corresponding dimension reduced spaces. The method is evaluated on a set (1) and (2) of tasks in the context of Unsupervised learning."
6524,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,permuted set USED-FOR permutation - invariant representation. bottleneck PART-OF set models. model USED-FOR permutations and set representations. number sorting CONJUNCTION image mosaics. image mosaics CONJUNCTION number sorting. classification CONJUNCTION visual question answering. visual question answering CONJUNCTION classification. image mosaics CONJUNCTION classification. classification CONJUNCTION image mosaics. image mosaics CONJUNCTION visual question answering. visual question answering CONJUNCTION image mosaics. image mosaics USED-FOR classification. explicit or implicit supervision USED-FOR model. explicit or implicit supervision USED-FOR permutations and set representations. OtherScientificTerm is Representations of sets. Method is Permutation - Optimisation module. ,"This paper proposes a permutation-invariant representation of a permuted set. Representations of sets can be represented as permutations of sets. The authors propose a Permutation-Optimisation module, which is based on the bottleneck in set models. The model learns permutations and set representations using explicit or implicit supervision. The proposed model is tested on classification, number sorting, image mosaics, and visual question answering.","This paper proposes a permutation-invariant representation of a permuted set. Representations of sets can be represented by a Permutation-Optimisation module. The bottleneck of set models is the number of permutations in the set models. The authors propose a model to learn permutations and set representations with explicit or implicit supervision. The model is evaluated on classification, number sorting, image mosaics, and visual question answering."
6528,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"Projective Subspace Networks ( PSN ) HYPONYM-OF deep learning paradigm. deep learning paradigm USED-FOR non - linear embeddings. limited supervision USED-FOR non - linear embeddings. PSN approach USED-FOR end - to - end learning. Method are learning techniques, and PSN. OtherScientificTerm are dynamical environments, affine subspace, projective subspace, and higher - order information datapoints. Task is lifelong learning. Generic is modeling. ","This paper proposes a new deep learning paradigm called Projective Subspace Networks (PSN) for learning non-linear embeddings with limited supervision in dynamical environments. The main idea is to learn a subspace of the affine subspace, which is then used to train a PSN. The PSN approach is then applied to end-to-end learning, where the goal is to improve the performance of the learning techniques. Experiments show that the proposed PSN is able to achieve state-of-the-art performance on a variety of tasks.","The paper proposes a deep learning paradigm called Projective Subspace Networks (PSN) for learning non-linear embeddings with limited supervision. The main idea is to learn a subspace of the affine subspace, which is then used for lifelong learning. Experiments on dynamical environments show that PSN outperforms other learning techniques. The PSN approach can be used for end-to-end learning. The paper also shows that the projective subspace can be extended to higher-order information datapoints, and the modeling can be applied to other tasks."
6532,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"meta - learning method USED-FOR fast adaptation. CAML HYPONYM-OF meta - learning method. context parameters CONJUNCTION shared parameters. shared parameters CONJUNCTION context parameters. gradient steps USED-FOR task - specific loss. gradient steps USED-FOR context parameters. approaches COMPARE method. method COMPARE approaches. method USED-FOR networks. memory writes CONJUNCTION network communication. network communication CONJUNCTION memory writes. memory writes USED-FOR training. approach COMPARE MAML. MAML COMPARE approach. approach USED-FOR task embeddings. task - specific learning rate EVALUATE-FOR approach. context parameters USED-FOR task embeddings. OtherScientificTerm are model parameters, overfitting, and partitionings of the parameter vectors. Generic are model, network, and task. Method is distributed machine learning systems. ","This paper proposes a meta-learning method for fast adaptation in distributed machine learning systems, CAML. CAML uses gradient steps to learn the context parameters and shared parameters for the task-specific loss. The key idea is that the model parameters can be partitioned into two parts: the context and the shared parameters. The shared parameters are then used to train the network, while the shared parameter is used for the context. The authors show that the proposed method outperforms existing approaches in terms of performance on a variety of tasks. The proposed approach is also able to learn task embeddings with context parameters in a more efficient manner than MAML. ","This paper proposes a meta-learning method for fast adaptation to distributed machine learning systems. CAML is an extension of CAML, where the model parameters are shared across different tasks. The key idea is to use gradient steps for the task-specific loss, which is based on the partitionings of the parameter vectors. The authors show that the proposed method outperforms other approaches for training networks with memory writes and network communication. The proposed approach is evaluated on task embeddings with context parameters and shared parameters. The approach is shown to outperform MAML in terms of learning rate. "
6536,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"Utility providers USED-FOR data privacy. natural informationtheoretic bounds FEATURE-OF utility - privacy trade - off. explicit learning architectures USED-FOR privacypreserving representations. utility algorithms USED-FOR privacy requirements. gender CONJUNCTION emotion detection. emotion detection CONJUNCTION gender. use cases EVALUATE-FOR framework. face recognition USED-FOR mobile devices. mobile devices HYPONYM-OF application. subject - withinsubject HYPONYM-OF use cases. Task are privacy protecting challenges, and facial verification. Generic are paradigm, bound, and algorithm. Method are machine learning algorithms, privacy - preserving representations, sanitization process, space - preserving transformations, and face identity detector. OtherScientificTerm are gender - and - subject, gender attribute, emotion - and - gender, and independent variables. ","This paper studies the problem of privacy preserving representations in machine learning algorithms. The authors propose a new paradigm for privacy protecting challenges where the utility providers are not able to provide data privacy. The utility-privacy trade-off is defined as the natural informationtheoretic bounds on the utility-private trade-offs between the gender-and-subject and the gender attribute. The paper shows that explicit learning architectures can be used to learn privacypreserving representations by using utility algorithms that satisfy the privacy requirements. The main contribution of the paper is a new bound on the bound. The bound is based on the sanitization process, which is a space-preserving transformations of the data. The proposed framework is evaluated in three use cases: face recognition on mobile devices, subject-withinsubject, emotion detection, and face identity detector. The experimental results show that the proposed algorithm can achieve state-of-the-art performance.","The paper proposes a new paradigm for privacy protecting challenges. The paper proposes to use utility providers for data privacy. The utility-privacy trade-off is based on natural informationtheoretic bounds on the privacy-preserving trade-offs between the gender-and-subject, the gender attribute and the emotion- and-gender, and the independent variables of the gender and the subject. The authors propose to use explicit learning architectures to learn privacypreserving representations that are independent of gender and gender. They show that the utility algorithms are able to satisfy privacy requirements under certain conditions. They also show that under certain assumptions, the proposed framework can be applied to a variety of use cases, including mobile devices and subject-withinsubject, and facial verification with face identity detector. "
6540,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"training instability FEATURE-OF Generative Adversarial Networks ( GANs ). backpropagation signal USED-FOR generator. task difficulty FEATURE-OF discriminator. progressive augmentation USED-FOR generator. progressive augmentation USED-FOR GAN objective. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION CELEBA. CELEBA CONJUNCTION CIFAR10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. image generation task EVALUATE-FOR approach. Method are hyper - parameter tuning, progressive augmentation of GANs ( PAGAN ), and GAN training. OtherScientificTerm are fragile training behaviour, and input space. ","This paper studies the problem of training instability of Generative Adversarial Networks (GANs) in the presence of hyper-parameter tuning. The authors propose a new GAN objective based on progressive augmentation of GANs (PGAN). The generator is trained with backpropagation signal, and the task difficulty of the discriminator depends on the input space. The proposed approach is evaluated on MNIST, Fashion-MNIST, CIFAR10 and CELEBA.","This paper studies the training instability of Generative Adversarial Networks (GANs) in the context of hyper-parameter tuning. The authors show that the fragile training behaviour of GANs (PAGAN) is due to the backpropagation signal of the generator, and that the discriminator is sensitive to the task difficulty of training the generator. To address this issue, the authors propose to use progressive augmentation of the GAN objective, which is based on the idea of PAGAN training. The proposed approach is evaluated on MNIST, Fashion-MNIST, CIFAR10, and CELEBA on the image generation task."
6544,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"models USED-FOR neural responses. models USED-FOR primary visual cortex ( V1 ). convolutional neural networks ( CNNs ) USED-FOR V1 activity. orientation selectivity CONJUNCTION phase invariance. phase invariance CONJUNCTION orientation selectivity. orientation selectivity FEATURE-OF V1 neurons. phase invariance FEATURE-OF V1 neurons. V1 neurons USED-FOR features. framework USED-FOR common features. rotation - equivariant convolutional neural network USED-FOR framework. mouse primary visual cortex FEATURE-OF natural images. two - photon imaging USED-FOR rotation - equivariant CNN. rotation - equivariant network COMPARE regular CNN. regular CNN COMPARE rotation - equivariant network. rotation - equivariant network USED-FOR common features. feature maps USED-FOR regular CNN. Method is energy models. Task are V1 computations, and nonlinear functional organization of visual cortex. OtherScientificTerm is neural activity. ","This paper studies the problem of learning neural responses in the presence of energy models. The authors propose a new framework for learning the common features of V1 computations. The proposed framework is based on a rotation-equivariant convolutional neural network, which is trained with two-photon imaging. The key idea is to learn the features of the V1 neurons in the form of orientation selectivity and phase invariance. The experiments show that the proposed framework performs better than a regular CNN on natural images from a mouse primary visual cortex.","This paper proposes a new framework for learning neural responses from images. The authors propose to use models for learning the primary visual cortex (V1) and convolutional neural networks (CNNs) to model the V1 activity. The main idea is to use energy models to model V1 computations. The proposed framework is based on a rotation-equivariant neural network with two-photon imaging. The paper shows that the rotation-Equivariant network is able to learn common features of V1 neurons, including orientation selectivity and phase invariance, and can be used to learn the nonlinear functional organization of visual cortex. Experiments are conducted on natural images from a mouse primary vision cortex, showing that the proposed framework can learn the common features from the feature maps of a regular CNN."
6548,SP:f17090812ace9c83d418b17bf165649232c223e3,"large datasets USED-FOR neural networks. algorithm USED-FOR robust, communication - efficient learning. algorithm USED-FOR SIGNSGD. algorithm COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE algorithm. communication COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE communication. communication USED-FOR algorithm. convergence USED-FOR parameter regime. parameter regime FEATURE-OF ADAM. large and mini - batch settings FEATURE-OF SIGNSGD. majority vote USED-FOR sign gradients. SGD COMPARE majority vote. majority vote COMPARE SGD. Pytorch USED-FOR distributed training system. time EVALUATE-FOR resnet50. collective communications library ( NCCL ) COMPARE framework. framework COMPARE collective communications library ( NCCL ). Imagenet USED-FOR resnet50. parameter server PART-OF framework. time EVALUATE-FOR framework. AWS p3.2xlarge machines USED-FOR resnet50. Generic is networks. OtherScientificTerm are communicating gradients, machine counts, network faults, gradient vector, server, class of adversaries, and gradient estimate. ","This paper proposes a new algorithm for robust, communication-efficient learning on large datasets. The algorithm is based on the idea of communicating gradients between the server and the clients. The authors show that the proposed algorithm is better than full-precision, distributed SGD and communication in both large and mini-batch settings. The main contribution of the paper is the convergence of ADAM in the parameter regime. The paper also shows that the majority vote for sign gradients in SGD can be better than majority vote in a distributed training system using Pytorch.   The authors also show that their framework is able to learn a parameter server in the same amount of time as a collective communications library (NCCL) and that resnet50 with Imagenet on AWS p3 outperforms NCCL in terms of time. ","The paper proposes a new algorithm for robust, communication-efficient learning on large datasets. The algorithm is based on ADAM, which is an extension of SIGNSGD in both large and mini-batch settings. The main difference between the two networks is that the communication gradients are computed by computing the majority vote of the sign gradients, while the full-precision, distributed SGD uses communication. The authors show that the convergence of the parameter regime of ADAM is better than that of SGD, which uses majority vote. The paper also proposes a distributed training system based on Pytorch, where the network counts are computed as a function of the number of network faults, and the gradient vector is computed by the class of adversaries. The proposed framework consists of a parameter server and a collective communications library (NCCL). The authors evaluate the proposed framework on resnet50 on Amazon p3 on 2xlarge machines."
6552,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,approach USED-FOR realistic and high - fidelity stock market data. generative adversarial networks USED-FOR approach. conditional Wasserstein GAN USED-FOR history dependence of orders. stock market FEATURE-OF history dependence of orders. finite history dependence FEATURE-OF stochastic process. stochastic process USED-FOR order stream. actual market and synthetic data EVALUATE-FOR approach. Material is real data. ,"This paper proposes a new approach to generate realistic and high-fidelity stock market data from generative adversarial networks. The proposed approach is based on conditional Wasserstein GAN, where the history dependence of orders in the stock market is modeled as a stochastic process with finite history dependence. The authors show that the proposed approach can be applied to both actual market and synthetic data.","This paper proposes a novel approach to generate realistic and high-fidelity stock market data from generative adversarial networks. The approach is based on conditional Wasserstein GAN, where the history dependence of orders in the stock market is modeled as a stochastic process with finite history dependence. The proposed approach is evaluated on both actual market and synthetic data."
6556,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"deep reinforcement learning ( DRL ) algorithm USED-FOR arbitrary errors. deep reinforcement learning ( DRL ) algorithm USED-FOR applications. robotics HYPONYM-OF applications. RL agents USED-FOR observed rewards. reward confusion matrix USED-FOR observed rewards. RL agents USED-FOR noisy environments. approaches USED-FOR supervised learning. supervised learning USED-FOR framework. noisy data USED-FOR approaches. noisy data USED-FOR supervised learning. approaches USED-FOR framework. sample complexity EVALUATE-FOR approach. convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION convergence. convergence EVALUATE-FOR approach. policies COMPARE baselines. baselines COMPARE policies. expected rewards EVALUATE-FOR policies. estimated surrogate reward USED-FOR policies. Atari games EVALUATE-FOR PPO algorithm. Method are reinforcement learning ( RL ) models, and unbiased reward estimator aided robust RL framework. OtherScientificTerm are noises, observed reward channel, perturbed rewards, and unbiased surrogate rewards. Task is noisy RL problems. Generic is solution. Material is DRL platforms. Metric is error rates. ","This paper studies the problem of learning policies with noisy rewards in reinforcement learning (RL) models. The authors propose a novel approach to learn policies that are robust to noisy rewards. The proposed approach is based on the PPO algorithm, which is an unbiased reward estimator. The paper shows that the proposed approach achieves better convergence and sample complexity than existing approaches for supervised learning on noisy data.","The paper proposes a deep reinforcement learning (DRL) algorithm for arbitrary errors in applications such as robotics. The framework is based on the unbiased reward estimator aided robust RL framework. The proposed approach is evaluated on Atari games and shows better sample complexity and better convergence compared to other approaches for supervised learning on noisy data. The authors also propose a novel solution to noisy RL problems. The key idea is to use RL agents to predict observed rewards from the reward confusion matrix. The observed reward channel is modeled as a perturbed rewards, and the observed rewards are modeled as unbiased surrogate rewards. The paper shows that the proposed policies outperform baselines in terms of expected rewards and error rates. "
6560,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"network structure USED-FOR halting time. overparametrization USED-FOR weight space traversal. power - law - like relationship USED-FOR average step size. Method is gradient descent. OtherScientificTerm are computational requirements, model ’s width, gradient vectors, and traversal. Generic are larger models, and applications. ",This paper studies the problem of weight space traversal with overparametrization in a network structure. The main idea is to use the power-law-like relationship between the average step size and the model’s width to reduce the computational requirements for gradient descent. The authors show that this can lead to larger models with better performance in a variety of applications. ,"The paper proposes a new method for learning a network structure for halting time. The main idea is to use overparametrization for weight space traversal, which is a power-law-like relationship between the average step size and the model’s width. The authors show that this can be used to reduce the computational requirements of gradient descent. They also show that larger models can be trained with overparameterized weights. The paper is well-written and easy to follow, and the applications are interesting. "
6564,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"learning framework USED-FOR setting. reinforcement learning USED-FOR online optimization problems. reinforcement learning USED-FOR theoretically optimal algorithms. theoretically optimal algorithms USED-FOR online optimization problems. algorithms CONJUNCTION complexity theory. complexity theory CONJUNCTION algorithms. universal and high - entropy training sets HYPONYM-OF adversarial distributions. online knapsack problem CONJUNCTION secretary problem. secretary problem CONJUNCTION online knapsack problem. AdWords problem CONJUNCTION online knapsack problem. online knapsack problem CONJUNCTION AdWords problem. online knapsack problem EVALUATE-FOR ideas. secretary problem EVALUATE-FOR ideas. AdWords problem EVALUATE-FOR ideas. optimal algorithms USED-FOR problems. online primal - dual framework USED-FOR problems. OtherScientificTerm are distributions, and worst case. Method is learner. Generic is models. ","This paper proposes a learning framework for online optimization problems where the goal is to learn a set of distributions that are optimal in the worst case. The authors propose a new setting where the learner has access to an online primal-dual framework and can learn the optimal algorithms for these problems. The algorithms are theoretically optimal algorithms based on algorithms and complexity theory. The paper shows that the best algorithms can be found in both universal and high-entropy training sets, and that the worst-case algorithms are also theoretically optimal.  The paper also shows that these ideas can be used to solve the online knapsack problem and the AdWords problem.","This paper proposes a learning framework for the setting of online optimization problems, where the goal is to learn theoretically optimal algorithms for the online optimization problem. The proposed algorithms are based on algorithms and complexity theory. The authors also propose an online primal-dual framework to solve the problems, including the online knapsack problem, the secretary problem, and the AdWords problem. In the worst case, the learner is not allowed to change the distribution of the adversarial distributions in the universal and high-entropy training sets."
6568,SP:b99732087f5a929ab248acdcd7a943bce8671510,"inductive biases PART-OF deep reinforcement learning algorithms. domain knowledge CONJUNCTION pretuned hyperparameters. pretuned hyperparameters CONJUNCTION domain knowledge. domain knowledge HYPONYM-OF inductive biases. pretuned hyperparameters HYPONYM-OF inductive biases. fixed components COMPARE adaptive solutions. adaptive solutions COMPARE fixed components. components COMPARE adaptive components. adaptive components COMPARE components. learning EVALUATE-FOR systems. continuous control problems EVALUATE-FOR systems. adaptive components USED-FOR system. tasks EVALUATE-FOR system. deep RL algorithms USED-FOR tasks. deep reinforcement learning ( RL ) community USED-FOR tasks. Go CONJUNCTION Chess. Chess CONJUNCTION Go. board - games CONJUNCTION video - games. video - games CONJUNCTION board - games. video - games CONJUNCTION 3D navigation tasks. 3D navigation tasks CONJUNCTION video - games. Atari HYPONYM-OF video - games. Go HYPONYM-OF board - games. Chess HYPONYM-OF board - games. tuning USED-FOR these. these USED-FOR new domains. tuning USED-FOR new domains. inductive biases FEATURE-OF agents. AlphaZero COMPARE AlphaGo algorithm. AlphaGo algorithm COMPARE AlphaZero. Go - specific inductive biases CONJUNCTION human data. human data CONJUNCTION Go - specific inductive biases. Chess CONJUNCTION Shogi. Shogi CONJUNCTION Chess. AlphaZero USED-FOR Chess. AlphaZero USED-FOR Shogi. biases USED-FOR AlphaZero. Go USED-FOR AlphaZero. generality CONJUNCTION performance. performance CONJUNCTION generality. inductive biases PART-OF algorithms. domain knowledge CONJUNCTION pretuned learning parameters. pretuned learning parameters CONJUNCTION domain knowledge. domain knowledge PART-OF Inductive biases. pretuned learning parameters PART-OF Inductive biases. domain knowledge CONJUNCTION pretune parameters. pretune parameters CONJUNCTION domain knowledge. Generic are problems, and approach. OtherScientificTerm is hyper - parameters. Method are domain - specific components, RL algorithms, AlphaZero algorithm, and learning algorithm. ","This paper studies the problem of continuous control problems in deep reinforcement learning, where the goal is to learn a system that is able to solve a continuous control problem. The authors propose a novel approach to solve these problems by learning a set of domain-specific components, which are then used to train a system with adaptive components. The proposed approach is evaluated on a variety of tasks from the deep RL (RL) community, including board-games, video-games and 3D navigation tasks. The results show that the proposed approach outperforms the state-of-the-art AlphaZero algorithm on these tasks. ","This paper presents a new approach for learning deep reinforcement learning (RL) algorithms with inductive biases. The idea is to learn a set of domain-specific components for each task, and then use these to learn new domains. The proposed approach is evaluated on a number of continuous control problems, and the results show that the proposed approach outperforms the state-of-the-art methods. "
6572,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,photo - realistic unknown environments FEATURE-OF navigational instruction. visual - textual co - grounding module CONJUNCTION progress monitor. progress monitor CONJUNCTION visual - textual co - grounding module. complementary components PART-OF self - monitoring agent. visual - textual co - grounding module HYPONYM-OF complementary components. progress monitor HYPONYM-OF complementary components. ablation studies USED-FOR primary components. ablation studies USED-FOR approach. benchmark EVALUATE-FOR selfmonitoring agent. method USED-FOR state. method USED-FOR art. Generic is task. OtherScientificTerm is navigation progress. Metric is success rate. ,This paper proposes a self-monitoring agent with complementary components: a visual-textural co-grounding module and a progress monitor. The proposed approach is based on ablation studies on the primary components. The authors show that the proposed method can achieve state-of-the-art performance on the benchmark. ,This paper proposes a novel approach to learn navigational instruction in photo-realistic unknown environments. The authors propose a self-monitoring agent with complementary components: a visual-textural co-grounding module and a progress monitor. The primary components are based on ablation studies. The proposed approach is evaluated on a benchmark and shows that the proposed method is able to achieve state-of-the-art results in terms of navigation progress. 
6576,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,input - output examples USED-FOR Neural program synthesis. encoder USED-FOR embedding. decoder USED-FOR program. syntax FEATURE-OF embedding. encoder USED-FOR encoder - decoder architecture. embedding USED-FOR decoder. encoder - decoder architecture USED-FOR neural program synthesis approaches. approaches USED-FOR tasks. approaches USED-FOR tasks. tasks EVALUATE-FOR state - of - the - art approach. tasks HYPONYM-OF tasks. accuracy EVALUATE-FOR state - of - the - art approach. Karel HYPONYM-OF tasks. Karel HYPONYM-OF tasks. FlashFill HYPONYM-OF tasks. execution - guided synthesis CONJUNCTION synthesizer ensemble. synthesizer ensemble CONJUNCTION execution - guided synthesis. techniques USED-FOR semantic information. synthesizer ensemble HYPONYM-OF techniques. execution - guided synthesis HYPONYM-OF techniques. synthesizer ensemble HYPONYM-OF semantic information. execution - guided synthesis HYPONYM-OF semantic information. techniques CONJUNCTION encoder - decoder - style neural program synthesizer. encoder - decoder - style neural program synthesizer CONJUNCTION techniques. accuracy EVALUATE-FOR techniques. Karel dataset EVALUATE-FOR techniques. ,"This paper proposes a new method for learning neural program synthesis from input-output examples. The key idea is to use the encoder-decoder architecture of neural program synthesizer as an encoder for embedding the embedding of the program into a decoder, and then use the decoder to generate the program. The embedding is composed of two parts: (1) the syntax of the embeddings, and (2) the semantic information extracted from the encoders. The authors show that the proposed state-of-the-art approach achieves better accuracy on a variety of tasks such as Karel, FlashFill, and execution-guided synthesis compared to existing approaches on these tasks. They also show that their techniques are able to learn semantic information from the Karel dataset.","This paper presents a new approach to learning a program from input-output examples for Neural program synthesis. The authors propose an encoder-decoder architecture that combines the embedding of the program with the decoder to learn the syntax of the input program. The proposed approaches are evaluated on a variety of tasks (e.g. Karel, FlashFill, etc). The authors show that the proposed state-of-the-art approach is able to achieve better accuracy on these tasks compared to other approaches for different tasks. They also demonstrate the effectiveness of the proposed techniques on semantic information and execution-guided synthesis on the Karel dataset."
6580,SP:dc7dfc1eec473800580dba309446871122be6040,"stability FEATURE-OF batch normalization ( BN ). BN USED-FOR simplified model. ordinary least squares ( OLS ) HYPONYM-OF simplified model. modeling approach USED-FOR problem. scaling law CONJUNCTION convergence. convergence CONJUNCTION scaling law. gradient descent USED-FOR OLS. arbitrary learning rates FEATURE-OF weights. convergence FEATURE-OF arbitrary learning rates. convergence CONJUNCTION acceleration effects. acceleration effects CONJUNCTION convergence. BN USED-FOR gradient descent. mathematical principles USED-FOR batch normalization. OtherScientificTerm is learning rates. Task are OLS problem, and supervised learning problems. ","This paper studies the stability of batch normalization (BN) under arbitrary learning rates. The authors propose a simplified model, called ordinary least squares (OLS), which is based on BN. The main contribution of the paper is a modeling approach to solve the problem. The learning rates of the OLS problem can be approximated by the scaling law and convergence of BN, and the authors show that the convergence and acceleration effects of gradient descent for OLS can be obtained by BN under arbitrary weights. They also show that BN can be used for gradient descent in supervised learning problems. Finally, the authors provide mathematical principles to explain the performance of the BN in batch normalisation.","This paper studies the stability of batch normalization (BN) under arbitrary learning rates. The authors propose a simplified model called ordinary least squares (OLS) which is based on BN. The problem is formulated as a modeling approach to solve the problem, where learning rates can be arbitrarily large. The OLS problem can be solved by gradient descent for OLS, and the scaling law and convergence of the OLS can be computed by BN for gradient descent. In particular, the authors show that BN can be decomposed into two parts: (1) the learning rates of the weights and (2) the acceleration effects. The main contribution of the paper is to introduce mathematical principles for batch normalisation, which can be applied to supervised learning problems."
6584,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,data noising USED-FOR recurrent neural network language models. theoretical perspective USED-FOR data noising. data noising HYPONYM-OF Bayesian recurrent neural networks. variational distribution FEATURE-OF Bayesian recurrent neural networks. variational framework USED-FOR data noising. variational smoothing CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION variational smoothing. tied input and output embedding matrices CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION tied input and output embedding matrices. tied input and output embedding matrices USED-FOR variational smoothing. benchmark language modeling datasets EVALUATE-FOR data noising methods. Method is mixture of Gaussians. OtherScientificTerm is unigram distribution. Generic is method. ,"This paper studies the problem of data noising in recurrent neural network language models with a theoretical perspective. The authors propose a variational framework based on the mixture of Gaussians. They show that the variational distribution of Bayesian recurrent neural networks with variational noising can be expressed as a mixture of the unigram distribution of the input and output embeddings. They also show that variational smoothing with tied input andoutput embedding matrices can be combined with element-wise variational smoothhing method. Finally, they show that their method can be applied to a variety of benchmark language modeling datasets.","This paper proposes a theoretical perspective on data noising in recurrent neural network language models. The authors propose a variational framework for learning data-noising in Bayesian recurrent neural networks with variational distribution. The main idea is to use a mixture of Gaussians, where the unigram distribution of the input and the output is the same. The variational smoothing is based on the tied input and output embedding matrices, the element-wise variational smoothhing method, and the variational approximations. The proposed method is evaluated on several benchmark language modeling datasets."
6588,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"saliency maps USED-FOR classification. classifier - dependent methods USED-FOR Extracting saliency maps. approach USED-FOR saliency maps. approach COMPARE weakly - supervised localization techniques. weakly - supervised localization techniques COMPARE approach. Method are classifier - agnostic saliency map extraction, and classifier. Material is ImageNet dataset. ",This paper proposes a classifier-agnostic saliency map extraction method for classification. The proposed approach is based on the idea of extracting saliency maps for classification from the input image. The authors propose to do so by using classifier dependent methods to extract the saliencymaps. The approach is shown to outperform weakly-supervised localization techniques. ,This paper proposes a classifier-agnostic saliency map extraction method for classification. Extracting saliency maps from the ImageNet dataset is a challenging problem. The authors propose a new approach to extract saliencymaps from the image. The approach is based on classifier dependent methods. The proposed approach is compared to weakly-supervised localization techniques. 
6592,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,deep nets USED-FOR task. net USED-FOR task. net USED-FOR model. deep nets USED-FOR task. knowledge flow USED-FOR deep net model. teachers HYPONYM-OF deep nets. tasks EVALUATE-FOR they. output spaces FEATURE-OF tasks. fine - tuning COMPARE knowledge exchange ’ methods. knowledge exchange ’ methods COMPARE fine - tuning. supervised and reinforcement learning tasks EVALUATE-FOR fine - tuning. approach COMPARE fine - tuning. fine - tuning COMPARE approach. supervised and reinforcement learning tasks EVALUATE-FOR approach. ,"This paper proposes a new deep net model based on knowledge flow. The model is trained on a set of deep nets (e.g., teachers) and then fine-tuned on a new task using the learned net. The proposed approach is evaluated on supervised and reinforcement learning tasks, and shows that the proposed approach performs better than standard ‘knowledge exchange’ methods. ","This paper proposes a deep net model based on knowledge flow. The model is trained on a set of deep nets (e.g., teachers) and then fine-tuned on a new task. The proposed approach is evaluated on supervised and reinforcement learning tasks, and they show that the proposed approach outperforms ‘knowledge exchange’ methods in terms of output spaces."
6596,SP:a72072879f7c61270d952f06d9ce995e8150632c,"spatial and temporal dimensions FEATURE-OF higher - order inter - dependencies. higher - order inter - dependencies FEATURE-OF data streams. soft - clustering USED-FOR compact dynamical model. dynamics USED-FOR compact dynamical model. predictive accuracy EVALUATE-FOR mathematical representation. stochastic calculus PART-OF information theory inspired approach. predictive ability CONJUNCTION causal interdependence ( relatedness ) constraints. causal interdependence ( relatedness ) constraints CONJUNCTION predictive ability. data streams CONJUNCTION compact model. compact model CONJUNCTION data streams. maximization of the compression of the state variables USED-FOR model construction. convergence FEATURE-OF learning algorithm. iterative scheme USED-FOR model parameters. high - dimensional Gaussian case study EVALUATE-FOR framework. compression and prediction accuracy EVALUATE-FOR dynamical systems. algorithm USED-FOR prediction. reduced dimensions FEATURE-OF prediction. real - world dataset of multimodal sentiment intensity EVALUATE-FOR algorithm. Task are Extracting relevant information, modeling complex systems, and causal inference. Metric is accuracy. Generic is tasks. Material is high - dimensional heterogeneous data streams. ","This paper proposes an information theory inspired approach based on the stochastic calculus. The authors propose a compact dynamical model based on soft-clusterering, where the dynamics are modeled by the dynamics of the data streams with spatial and temporal dimensions of higher-order inter-dependences between data streams. The model construction is based on maximizing the maximization of the compression of the state variables in the model construction. The proposed framework is evaluated on a high-dimensional Gaussian case study and shows that the proposed algorithm achieves better compression and prediction accuracy for dynamical systems with reduced dimensions. ",This paper proposes an information theory inspired approach based on stochastic calculus. The key idea is to use soft-clustering to learn a compact dynamical model with higher-order inter-dependencies in the spatial and temporal dimensions of data streams. The compact model is learned by minimizing the dynamics between the data streams and the data points. The model construction is done by maximizing the maximization of the compression of the state variables. The authors show that the proposed learning algorithm converges to convergence in terms of the convergence of the model parameters. The proposed framework is evaluated on a high-dimensional Gaussian case study and on a real-world dataset of multimodal sentiment intensity. The algorithm is shown to improve the prediction and prediction accuracy of dynamical systems with reduced dimensions. 
6600,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"variational autoencoder USED-FOR single neural probabilistic model. stochastic variational Bayes USED-FOR model. feature imputation CONJUNCTION image inpainting problems. image inpainting problems CONJUNCTION feature imputation. synthetic data CONJUNCTION feature imputation. feature imputation CONJUNCTION synthetic data. synthetic data EVALUATE-FOR approach. OtherScientificTerm are observed features, and features. ",This paper proposes a variational autoencoder for a single neural probabilistic model. The model is based on stochastic variational Bayes and is trained with observed features. The proposed approach is evaluated on synthetic data and feature imputation.,"This paper proposes a variational autoencoder for a single neural probabilistic model. The model is based on stochastic variational Bayes. The proposed approach is evaluated on synthetic data, feature imputation and image inpainting problems. The authors show that the observed features can be used to train the model. "
6604,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"architecture design USED-FOR neural network models. tasks EVALUATE-FOR neural network models. intermediate layer activations USED-FOR back - propagation. memory footprint FEATURE-OF models. spatial resolutions FEATURE-OF layers. approximation strategy USED-FOR network. memory footprint FEATURE-OF network. training EVALUATE-FOR approximation strategy. lower - precision approximations USED-FOR activations. approximate activations USED-FOR backward pass. approximation USED-FOR gradients. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. ImageNet EVALUATE-FOR approach. CIFAR EVALUATE-FOR approach. 32 - bit floating - point activations USED-FOR approach. training and validation performance EVALUATE-FOR approach. OtherScientificTerm are Limited GPU memory, memory, and forward and backward pass. Generic are architecture, and forward pass. Metric are computational expense, and memory usage. ","This paper proposes a new architecture design for neural network models for a variety of tasks. The architecture is based on the notion of Limited GPU memory. The authors propose a new approximation strategy for training a network with a large memory footprint in terms of spatial resolutions of the layers. The main idea is to use intermediate layer activations for back-propagation in the forward pass, and then use lower-precision approximations for the activations in the backward pass. The proposed approach is evaluated on CIFAR and ImageNet and achieves state-of-the-art training and validation performance.","This paper proposes a new architecture design for neural network models for a variety of tasks. The architecture is based on Limited GPU memory. The authors propose to use intermediate layer activations for back-propagation and forward pass to reduce the computational expense. The network is trained with a lower-precision approximations for the activations. The memory footprint of the models is measured by the spatial resolutions of the layers, and the authors propose an approximation strategy for the network to minimize the memory footprint for the forward and backward pass. The proposed approach is evaluated on CIFAR and ImageNet with 32-bit floating-point activations, and is shown to improve the training and validation performance. "
6608,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"Face completion HYPONYM-OF task. complexity CONJUNCTION controllable attributes of filled - in fragments. controllable attributes of filled - in fragments CONJUNCTION complexity. end - to - end framework USED-FOR generative adversarial networks ( GANs ). conditional vectors USED-FOR controllable attributes. end - to - end framework USED-FOR system. mean inference time FEATURE-OF images. system USED-FOR structural and appearance variations. mean inference time FEATURE-OF feed - forward pass of computation. 1024× 1024 resolution FEATURE-OF images. feed - forward pass of computation USED-FOR system. approach COMPARE face completion methods. face completion methods COMPARE approach. OtherScientificTerm are high resolution, and frequency components. Generic are model, and network. ","This paper proposes a new task called Face completion, which is an extension of the face completion task. The authors propose a new end-to-end framework for generative adversarial networks (GANs) by using conditional vectors to represent the controllable attributes of the model. The system is able to capture structural and appearance variations in the input images, and the mean inference time of the images with 1024×1024 resolution is reduced by a feed-forward pass of computation. The proposed approach is shown to perform better than existing face completion methods.","This paper proposes an end-to-end framework for generative adversarial networks (GANs) for the task of face completion, where the model is trained on high resolution images with high resolution and low resolution images, and the goal is to learn the controllable attributes of the generated images (e.g., complexity, controllability of filled-in fragments, etc.). The system is based on conditional vectors, which can be represented as a set of conditional vectors. The system learns structural and appearance variations of the images in a feed-forward pass of computation with mean inference time in terms of images with 1024×1024 resolution. The authors show that the proposed approach outperforms existing face completion methods on the face completion tasks. "
6612,SP:a300122021e93d695af85e158f2b402d21525bc8,"high - precision accumulators USED-FOR systems. precision requirements FEATURE-OF partial sum accumulations. reduced accumulation precision USED-FOR deep learning training. statistical approach USED-FOR reduced accumulation precision. analysis USED-FOR benchmark networks. ImageNet ResNet 18 CONJUNCTION ImageNet AlexNet. ImageNet AlexNet CONJUNCTION ImageNet ResNet 18. single precision floating - point baseline FEATURE-OF networks. OtherScientificTerm are numerical precision, multiply - accumulate units, accumulation precision, ensemble of partial sums, accumulation, and computation hardware. Metric is quality of convergence. Generic are variance, and equations. Task is areaand power - optimal systems. ","This paper studies the problem of learning high-precision accumulators for systems with high-probability accumulators. The authors propose a statistical approach to reduce the reduced accumulation precision in deep learning training. The main contribution of the paper is to show that the precision requirements of partial sum accumulations are not satisfied by numerical precision, but rather by multiply-accumulate units. The paper also shows that the quality of convergence is not guaranteed by the variance of the accumulation, but by the number of multiplications of the multiplications.  The authors also provide a theoretical analysis of the performance of two benchmark networks, ImageNet ResNet 18 and ImageNet AlexNet, on the single precision floating-point baseline. Theoretically, the authors prove that the accumulation precision of the two networks is bounded by the sum of the multiply-accentuate units, and that the ensemble of partial sums is not optimal.  ","This paper proposes a statistical approach to reduce accumulation precision in deep learning training. The authors propose to use numerical precision to reduce the variance of partial sum accumulations in systems with high-precision accumulators. The main idea is to use multiply-accumulate units, where the accumulation precision is the sum of the ensemble of partial sums, and the variance is a function of the number of multiplications of the multiplications. The paper shows that the quality of convergence can be reduced to a single precision floating-point baseline, which can be used in the context of computation hardware. Experiments on ImageNet ResNet 18 and ImageNet AlexNet show that the proposed analysis improves the performance of the benchmark networks compared to the original networks. The analysis is also applied to areaand power-optimal systems."
6616,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"gradient flow CONJUNCTION gradient descent. gradient descent CONJUNCTION gradient flow. risk convergence CONJUNCTION asymptotic weight matrix alignment. asymptotic weight matrix alignment CONJUNCTION risk convergence. gradient descent USED-FOR deep linear networks. asymptotic weight matrix alignment HYPONYM-OF implicit regularization. gradient flow USED-FOR deep linear networks. asymptotic weight matrix alignment USED-FOR gradient flow. linearly separable data USED-FOR deep linear networks. gradient flow USED-FOR strictly decreasing loss functions. normalized ith weight matrix COMPARE rank-1 approximation. rank-1 approximation COMPARE normalized ith weight matrix. decreasing step sizes FEATURE-OF gradient descent. linear function COMPARE maximum margin solution. maximum margin solution COMPARE linear function. binary cross entropy FEATURE-OF logistic loss. weight matrices USED-FOR linear function. network USED-FOR linear function. Metric is risk. OtherScientificTerm are rank-1 matrices, and alignment phenomenon. ","This paper studies implicit regularization with gradient flow and gradient descent in deep linear networks with linearly separable data. The authors consider asymptotic weight matrix alignment and risk convergence in the context of gradient flow. They show that gradient flow can be used to approximate strictly decreasing loss functions with decreasing step sizes. They also show that the normalized ith weight matrix can be approximated by a rank-1 approximation, and that the maximum margin solution of a linear function with weight matrices can be obtained by a network that is trained on the linear function. ","This paper studies the problem of implicit regularization with asymptotic weight matrix alignment and risk convergence in deep linear networks with linearly separable data. The authors show that gradient flow and gradient descent are strictly decreasing loss functions with decreasing step sizes. They also show that the normalized ith weight matrix is equivalent to the rank-1 approximation, and that the risk is bounded by the weight matrices. Finally, the authors provide a maximum margin solution for the logistic loss with binary cross entropy. "
6620,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,dialogue topic CONJUNCTION speaker sentiments. speaker sentiments CONJUNCTION dialogue topic. speaker identity CONJUNCTION dialogue topic. dialogue topic CONJUNCTION speaker identity. hredGAN architecture USED-FOR utterance attributes. speaker sentiments HYPONYM-OF utterance attributes. speaker identity HYPONYM-OF utterance attributes. dialogue topic HYPONYM-OF utterance attributes. persona - based HRED generator ( PHRED ) CONJUNCTION conditional discriminator. conditional discriminator CONJUNCTION persona - based HRED generator ( PHRED ). persona - based HRED generator ( PHRED ) PART-OF phredGAN. conditional discriminator PART-OF phredGAN. persona - based HRED generator ( PHRED ) PART-OF system. conditional discriminator PART-OF system. approaches USED-FOR conditional discriminator. phredGANd HYPONYM-OF dual discriminator system. dual discriminator system CONJUNCTION adversarial discriminator. adversarial discriminator CONJUNCTION dual discriminator system. phredGANa HYPONYM-OF approaches. phredGANd HYPONYM-OF approaches. Ubuntu Dialogue Corpus ( UDC ) CONJUNCTION TV series transcripts. TV series transcripts CONJUNCTION Ubuntu Dialogue Corpus ( UDC ). phredGAN COMPARE persona SeqSeq model. persona SeqSeq model COMPARE phredGAN. Big Bang Theory and Friends FEATURE-OF TV series transcripts. conversational datasets EVALUATE-FOR phredGAN. Ubuntu Dialogue Corpus ( UDC ) HYPONYM-OF conversational datasets. TV series transcripts HYPONYM-OF conversational datasets. quantitative measures CONJUNCTION crowd - sourced human evaluation. crowd - sourced human evaluation CONJUNCTION quantitative measures. datasets CONJUNCTION ones. ones CONJUNCTION datasets. attribute modalities FEATURE-OF customer - agent interactions. Ubuntu dataset FEATURE-OF customer - agent interactions. ones EVALUATE-FOR phredGAN. attribute modalities FEATURE-OF datasets. attribute modalities FEATURE-OF ones. datasets EVALUATE-FOR phredGAN. weak attribute modalities FEATURE-OF datasets. Big Bang Theory HYPONYM-OF weak attribute modalities. Task is multi - turn dialogue scenario. Method is attribute representation. ,"This paper studies multi-turn dialogue scenario. The authors propose a hredGAN architecture to learn utterance attributes such as speaker identity, dialogue topic, speaker sentiments, and speaker sentiments. The proposed approaches are based on phredGANd, a dual discriminator system, and a conditional discriminator. The system consists of a persona-based HRED generator (PHRED) that predicts the attributes of the speaker and the listener, and an adversarial discriminator which predicts the speaker's attributes. Experiments on several conversational datasets including Big Bang Theory and Friends, TV series transcripts, and Ubuntu Dialogue Corpus (UDC) demonstrate the effectiveness of the proposed approach. The quantitative measures and the crowd-sourced human evaluation show that the proposed ones outperform the existing ones in terms of attribute modalities and customer-agent interactions on the Ubuntu dataset. ","This paper presents a multi-turn dialogue scenario, where the goal is to learn a set of utterance attributes from a hredGAN architecture. The key idea of the paper is to use a conditional discriminator, which is a combination of two approaches, phredGANd and phredgana, to learn the attributes of a given utterance attribute. The authors show that the proposed approach can outperform the dual discriminator system and the adversarial discriminator. The proposed system is based on the conditional discriminators, which consists of a persona-based HRED generator (PHRED) and a phREDGAN. Experiments are conducted on several conversational datasets, including Big Bang Theory and Friends, TV series transcripts, and the Ubuntu Dialogue Corpus (UDC). The authors also show that their approach outperforms the persona SeqSeq model. The quantitative measures are based on crowd-sourced human evaluation and quantitative measures of the customer-agent interactions on the Ubuntu dataset. "
6624,SP:017b66d6262427cca551ef50006784498ffc741d,"language CONJUNCTION vision. vision CONJUNCTION language. vision CONJUNCTION action. action CONJUNCTION vision. virtual environment FEATURE-OF action. language PART-OF goal - driven collaborative task. vision PART-OF goal - driven collaborative task. action PART-OF goal - driven collaborative task. movable clip art objects PART-OF virtual world. virtual world USED-FOR game. natural language USED-FOR two - way communication. protocols CONJUNCTION metrics. metrics CONJUNCTION protocols. imitation learning CONJUNCTION goal - driven training. goal - driven training CONJUNCTION imitation learning. nearest - neighbor techniques CONJUNCTION neural network approaches. neural network approaches CONJUNCTION nearest - neighbor techniques. models USED-FOR task. nearest - neighbor techniques PART-OF models. neural network approaches PART-OF models. imitation learning USED-FOR neural network approaches. goal - driven training USED-FOR neural network approaches. game EVALUATE-FOR models. live human agents USED-FOR game. fully automated evaluation EVALUATE-FOR models. Task is Collaborative image - Drawing game. Method is CoDraw. OtherScientificTerm are clip art pieces, and crosstalk condition. Material is CoDraw dataset. Generic is testbed. ","This paper studies the Collaborative image-Draw game where the goal is to draw a set of clip art pieces in a virtual world with movable clip art objects. The goal-driven collaborative task consists of a language, vision, and an action in the virtual environment. The two-way communication is done using natural language and vision. The authors show that the proposed CoDraw dataset is a good testbed to evaluate the performance of the proposed models on the task. The proposed protocols and metrics are evaluated on a variety of tasks, and the models are compared with state-of-the-art neural network approaches and nearest-neighbor techniques. The game is played with live human agents. The models are evaluated using fully automated evaluation.","This paper presents a new collaborative image-Drawing game, where the goal is to draw a set of clip art pieces from a virtual world with movable clip art objects. The game is a two-way communication, where a natural language and vision are used to guide the action in the virtual environment. The goal-driven collaborative task is a combination of language, vision, and action in a virtual environment with a crosstalk condition. Experiments are conducted on the CoDraw dataset and show that the proposed testbed outperforms the state-of-the-art models on the task, as well as on a number of protocols and metrics. The authors also show that models trained on this task can outperform other neural network approaches, such as imitation learning, goal-directed training, and nearest-neighbor techniques. Finally, the authors conduct a fully automated evaluation of the models on a game with live human agents."
6628,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,model spaces USED-FOR machine learning. neural networks USED-FOR potential functions. potential functions USED-FOR undirected models. neural networks USED-FOR Neural random fields ( NRFs ). approach USED-FOR NRFs. gradient information USED-FOR model sampling. inclusive - NRF approach USED-FOR continuous data. inclusive - divergence minimized auxiliary generator USED-FOR approach. inclusive - divergence minimized auxiliary generator USED-FOR NRFs. images HYPONYM-OF continuous data. unsupervised / supervised image generation CONJUNCTION semi - supervised classification. semi - supervised classification CONJUNCTION unsupervised / supervised image generation. inclusive - NRFs USED-FOR unsupervised / supervised image generation. inclusive - NRFs USED-FOR semi - supervised classification. random fields USED-FOR tasks. inclusive - NRFs USED-FOR random fields. CIFAR-10 EVALUATE-FOR sample generation quality. unsupervised and supervised settings EVALUATE-FOR inclusiveNRFs. sample generation quality EVALUATE-FOR inclusiveNRFs. CIFAR-10 EVALUATE-FOR inclusiveNRFs. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. Semi - supervised inclusive - NRFs COMPARE generative model based semi - supervised learning methods. generative model based semi - supervised learning methods COMPARE Semi - supervised inclusive - NRFs. classification EVALUATE-FOR generative model based semi - supervised learning methods. generation EVALUATE-FOR Semi - supervised inclusive - NRFs. classification EVALUATE-FOR Semi - supervised inclusive - NRFs. ,"This paper proposes a novel approach to learn the potential functions of undirected models in model spaces. Neural random fields (NRFs) are trained with neural networks to generate potential functions for undirectED models. The authors propose an inclusive-NRF approach to generate continuous data from images, such as images HYPONYM-of continuous data. The approach is based on the inclusive-divergence minimized auxiliary generator for NRFs. The gradient information is then used for model sampling. The proposed approach is evaluated on unsupervised/supervised image generation, semi-supervised classification, and sample generation quality on CIFAR-10, SVHN, and MNIST. The results show that the proposed Semi-Supervised inclusive-Nrf outperforms other generative model based semi-studied learning methods in terms of generation and classification. ","This paper proposes a novel approach to learn potential functions in model spaces for machine learning. Neural random fields (NRFs) are used in undirected models, and the authors propose an approach to train NRFs with neural networks. The approach is based on an inclusive-divergence minimized auxiliary generator. The gradient information is used for model sampling. The authors evaluate the proposed approach on continuous data such as images, MNIST, SVHN, CIFAR-10, and semi-supervised classification. The results show that in both unsupervised and supervised settings, the sample generation quality of the inclusiveNRFs is better than that of the generative model based semi -supervised learning methods in terms of generation and classification. "
6632,SP:0841febf2e95da495b41e12ded491ba5e9633538,"Deep learning models USED-FOR graphs. Deep learning models USED-FOR tasks. graph neural networks USED-FOR node classification. training time attacks FEATURE-OF graph neural networks. meta - gradients USED-FOR bilevel problem. meta - gradients USED-FOR training - time attacks. small graph perturbations USED-FOR graph convolutional networks. they COMPARE baseline. baseline COMPARE they. relational information USED-FOR baseline. algorithm USED-FOR perturbations. Metric is robustness. OtherScientificTerm are discrete graph structure, and graph. Task is unsupervised embeddings. Generic is attacks. Method is classifiers. ","This paper studies the problem of unsupervised embeddings in graph convolutional networks with small graph perturbations. In particular, the authors consider the case where the target graph has a discrete graph structure, and the target node is a graph with a small number of nodes. The authors propose a new algorithm for learning the perturbation of the target nodes, which is based on meta-gradients. They show that the proposed algorithm is robust against the attacks. ","This paper studies the problem of learning graph convolutional networks with small graph perturbations. In particular, the authors focus on the case of unsupervised embeddings where the discrete graph structure of the graph is not known. The authors propose to use meta-gradients to solve the bilevel problem with meta-grains to improve the robustness of graph neural networks to node classification attacks. They show that the proposed algorithm is robust to the perturbation. They also show that they are more robust than the baseline, which is based on relational information."
6636,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"Wasserstein Autoencoder ( WAE ) HYPONYM-OF generative models. Sliced - Wasserstein Autoencoders ( SWAE ) CONJUNCTION WAE - MMD ( WAE. WAE - MMD ( WAE CONJUNCTION Sliced - Wasserstein Autoencoders ( SWAE ). Cramer - Wold AutoEncoder ( CWAE ) HYPONYM-OF generative model. maximum mean discrepancy based distance function USED-FOR WAE - MMD ( WAE. CramerWold kernel HYPONYM-OF characteristic kernel. characteristic kernel USED-FOR CWAE cost function. CWAE COMPARE WAE - MMD. WAE - MMD COMPARE CWAE. OtherScientificTerm are normal prior, and distance function. Method are optimization procedure, and SWAE. ","This paper proposes a new generative model called Wasserstein autoencoder (WAE) which is a combination of Sliced-Wasserstein Autoencoders (SWAE) and WAE-MMD (Wae). The main idea is to use a maximum mean discrepancy based distance function between the normal prior and the output of the WAE as a normal prior. The distance function is then used to estimate the CWAE cost function using a characteristic kernel called the CramerWold kernel. The optimization procedure is similar to the one in SWAE, except that the distance function in WAE is different. The theoretical results show that CWAE is more efficient than WAE - MMD.","This paper proposes a new generative model, called Wasserstein Autoencoder (WAE) for generative models such as Sliced-Wasserstein autoencoders (SWAE) and WAE-MMD (WAE). The main idea is to use the maximum mean discrepancy based distance function as the normal prior. The authors propose a new optimization procedure, called Cramer-Wold AutoEncoder (CWAE) which uses the CramerWold kernel as the characteristic kernel of the CWAE cost function. The distance function is defined as the difference between the output of a normal prior and the distance function of the WAE. CWAE is shown to be more efficient than WAE - MMD, and the authors show that CWAE outperforms WAE in terms of the number of samples. "
6640,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"It USED-FOR many - class ” problem. It USED-FOR class hierarchy. coarse - class label HYPONYM-OF class hierarchy. convolutional neural network ( CNN ) USED-FOR features. memory - augmented attention module CONJUNCTION multi - layer perceptron ( MLP ). multi - layer perceptron ( MLP ) CONJUNCTION memory - augmented attention module. multi - layer perceptron ( MLP ) USED-FOR MahiNet. memory - augmented attention module PART-OF MahiNet. convolutional neural network ( CNN ) USED-FOR MahiNet. attention module CONJUNCTION KNN classifier. KNN classifier CONJUNCTION attention module. linear classifier USED-FOR MLP. training strategies USED-FOR supervised learning. MahiNet USED-FOR supervised learning. training strategies USED-FOR MahiNet. mcfsOmniglot ” ( re - splitted Omniglot ) USED-FOR MCFS problem. benchmark datasets USED-FOR MCFS problem. mcfsOmniglot ” ( re - splitted Omniglot ) HYPONYM-OF benchmark datasets. MahiNet COMPARE models. models COMPARE MahiNet. MCFS classification tasks EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR MahiNet. MCFS classification tasks EVALUATE-FOR MahiNet. Task are MCFS, MCFS learning, and few - shot ” problem. OtherScientificTerm is fine class. Material is ImageNet. ","This paper studies the “many-class” problem in MCFS, where the goal is to learn a class hierarchy from a coarse-class label to a fine class. The authors propose MahiNet, a convolutional neural network (CNN) trained with a memory-augmented attention module and a multi-layer perceptron (MLP) to extract features from the features. The attention module is a linear classifier, and the KNN classifier is an MLP. The proposed model is evaluated on several benchmark datasets for the MCFS problem, including “mcfsOmniglot” (re-splitted Omniglot) and “MCFS learning,” where it is shown to perform better than other models on MCFS classification tasks. The paper also shows that the proposed training strategies for supervised learning and meta-learning scenarios can be used to improve the performance of the proposed model.","This paper proposes a novel approach to the “many-class” problem, which is a variant of MCFS. It is a “few-shot” setting where the class hierarchy is a coarse-class label, and the goal is to learn a class hierarchy. The authors propose a convolutional neural network (CNN) to learn features, and a memory-augmented attention module and a multi-layer perceptron (MLP) for learning features. The attention module is a linear classifier, while the KNN classifier is an MLP. MahiNet is evaluated on MCFS classification tasks on several benchmark datasets, including “mcfsOmniglot” (re-splitted Omniglot) and “MCFS learning,” where it outperforms other models in both supervised learning and meta-learning scenarios."
6644,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,Recurrent neural networks ( RNNs ) USED-FOR natural language. neural speed reading HYPONYM-OF inference. Structural - Jump - LSTM HYPONYM-OF neural speed reading model. LSTM CONJUNCTION agents. agents CONJUNCTION LSTM. one CONJUNCTION one. one CONJUNCTION one. one HYPONYM-OF agents. one HYPONYM-OF agents. one PART-OF model. one PART-OF model. LSTM PART-OF model. agents PART-OF model. model COMPARE neural reading models. neural reading models COMPARE model. accuracy EVALUATE-FOR vanilla LSTM. Structural - Jump - LSTM COMPARE vanilla LSTM. vanilla LSTM COMPARE Structural - Jump - LSTM. model COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE model. neural reading models COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE neural reading models. accuracy EVALUATE-FOR Structural - Jump - LSTM. floating point operations ( FLOP ) reduction EVALUATE-FOR Structural - Jump - LSTM. Method is RNNs. Metric is inference time. OtherScientificTerm is end of text markers. ,"This paper proposes a new neural speed reading model, Structural-Jump-LSTM, for the task of inference in recurrent neural networks (RNNs) for natural language. The proposed model consists of two agents: one for the end of text markers, and one for a set of nodes. The authors show that the proposed model achieves better accuracy than vanilla LSTM with floating point operations (FLOP) reduction. They also show that their model performs better than other neural reading models.","This paper proposes a new neural speed reading model, Structural-Jump-LSTM, for natural language inference. The model consists of two parts: one for learning the end of text markers, and one for training the agents. The authors show that the proposed model outperforms vanilla LSTM in terms of accuracy and floating point operations (FLOP) reduction. The paper also shows that the model is more accurate than other neural reading models. "
6648,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"Mahalanobis distance function USED-FOR convolutional features. Mahalanobis distance function USED-FOR tight clusters. Mahalanobis distance function USED-FOR nonlinear radial basis convolutional feature transformation. MNIST CONJUNCTION ISBI ISIC skin lesion. ISBI ISIC skin lesion CONJUNCTION MNIST. ISBI ISIC skin lesion CONJUNCTION NIH ChestX - ray14. NIH ChestX - ray14 CONJUNCTION ISBI ISIC skin lesion. NIH ChestX - ray14 HYPONYM-OF image classification and segmentation data - sets. MNIST HYPONYM-OF image classification and segmentation data - sets. ISBI ISIC skin lesion HYPONYM-OF image classification and segmentation data - sets. image classification and segmentation data - sets EVALUATE-FOR method. robustness EVALUATE-FOR method. it COMPARE non - gradient masking defense strategies. non - gradient masking defense strategies COMPARE it. method USED-FOR deep convolutional neural networks. deep convolutional neural networks USED-FOR adversarial perturbations. Method is deep convolutional models. Generic is them. OtherScientificTerm are carefully crafted adversarial perturbations, clusters, small adversarial perturbations, and decision boundary. Material is clean data. ","This paper proposes a new Mahalanobis distance function for learning convolutional features with tight clusters. The main idea is to learn a nonlinear radial basis convolutionally feature transformation using the Mahalanov distance function. The proposed method is evaluated on image classification and segmentation data-sets such as MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14. The results show that the proposed method improves robustness against adversarial perturbations and is competitive with other non-gradient masking defense strategies. ","This paper proposes a Mahalanobis distance function for learning convolutional features from clean data. The proposed method is evaluated on image classification and segmentation data-sets including MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14. It is shown that it outperforms non-gradient masking defense strategies on adversarial perturbations. "
6652,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"exploratory behaviors USED-FOR local optima. exploratory behaviors USED-FOR Reinforcement learning agents. immediate dithering perturbation CONJUNCTION temporally consistent exploration. temporally consistent exploration CONJUNCTION immediate dithering perturbation. immediate dithering perturbation PART-OF behaviors. temporally consistent exploration PART-OF behaviors. sparse rewards CONJUNCTION long term information. long term information CONJUNCTION sparse rewards. stochastic policy model USED-FOR tasks. sparse rewards FEATURE-OF tasks. long term information FEATURE-OF tasks. global random variable USED-FOR conditional distribution. dropout USED-FOR reinforcement learning policies. inherent temporal consistency FEATURE-OF them. factors USED-FOR NADPEx policy. gradients ’ alignment HYPONYM-OF factors. naive exploration CONJUNCTION parameter noise. parameter noise CONJUNCTION naive exploration. NADPEx USED-FOR tasks. sparse reward USED-FOR NADPEx. naive exploration USED-FOR NADPEx. parameter noise USED-FOR NADPEx. sparse reward USED-FOR tasks. mujoco benchmark USED-FOR continuous control. mujoco benchmark EVALUATE-FOR It. Method are on - policy temporally consistent exploration strategy, and deep reinforcement learning agents. OtherScientificTerm are reward signals, and policy space. ","This paper proposes a new on-policy temporally consistent exploration strategy for deep reinforcement learning agents. The main idea is to use exploratory behaviors to learn local optima. The proposed behaviors include immediate dithering perturbation (i.e., a global random variable is used to update the conditional distribution of the reward signals in the policy space) as well as temporal consistent exploration (e.g., a local random variable). The authors show that the proposed dropout can improve the performance of reinforcement learning policies with dropout. The authors also show that these factors can be used to train a NADPEx policy with “gradients’ alignment”. It is shown that it can achieve state-of-the-art performance on the mujoco benchmark for continuous control.","This paper proposes a novel on-policy temporally consistent exploration strategy for deep reinforcement learning agents. The authors propose to use exploratory behaviors for local optima. The proposed behaviors include immediate dithering perturbation, temporally inconsistent exploration, and sparse rewards. The key idea is to learn a conditional distribution over a global random variable, where the reward signals are sampled from the policy space. This conditional distribution is learned using dropout. The main contribution of the paper is to introduce two factors to the NADPEx policy: “Gradients’ alignment” and “parameter noise”. The paper shows that the two factors have inherent temporal consistency, and that they can be used to learn reinforcement learning policies. It also shows that NADPE can be applied to tasks with sparse rewards and long term information. It is evaluated on a mujoco benchmark for continuous control."
6656,SP:304930c105cf036ab48e9653926a5f61879dfea6,"gradient - based metric USED-FOR network. nonlinearity coefficient ( NLC ) HYPONYM-OF metric. NLC USED-FOR test error. NLC USED-FOR architecture search and design. Method are neural architectures, expert hand - tuning, and fully - connected feedforward networks. Task is architecture design. OtherScientificTerm is exploding or vanishing gradients. Generic are guideline, and it. ",This paper proposes a new metric called nonlinearity coefficient (NLC) to measure the distance between two neural architectures. NLC is a gradient-based metric for measuring the distance of a network to a point in the input space. The authors show that NLC can be used to estimate the test error of an architecture search and design. They also provide a guideline for expert hand-tuning for fully-connected feedforward networks. ,This paper proposes a new metric called nonlinearity coefficient (NCL) for architecture search and design. NLC is a gradient-based metric for measuring the network’s robustness to exploding or vanishing gradients. The authors show that NLC can be used to measure the test error of an architecture search or design. They also show that it can be applied to fully-connected feedforward networks. 
6660,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"bias amplification FEATURE-OF classifiers. inductive bias in gradient descent methods USED-FOR bias amplification. feature - wise bias amplification HYPONYM-OF bias. features FEATURE-OF model. targeted feature selection USED-FOR feature - wise bias amplification. feature selection algorithms USED-FOR bias amplification. they USED-FOR convolutional neural networks. feature selection algorithms USED-FOR linear models. bias amplification PART-OF linear models. algorithms USED-FOR reduced bias. synthetic and real data EVALUATE-FOR algorithms. Method is machine learning model. OtherScientificTerm are moderately - predictive “ weak ” features, and predictive bias. Material is insufficient training data. Metric is accuracy. ","This paper studies the problem of bias amplification in classifiers with inductive bias in gradient descent methods. The authors propose a new bias amplification called feature-wise bias amplification, which uses targeted feature selection to improve the performance of a machine learning model with moderately-predictive “weak” features. The main idea is to use feature selection algorithms for bias amplification of convolutional neural networks, where the features of the model depend on the number of training samples. The proposed algorithms are shown to achieve reduced bias on both synthetic and real data. The paper also provides a theoretical analysis of the predictive bias of the proposed algorithms.","This paper studies the problem of bias amplification in classifiers. The authors propose to use inductive bias in gradient descent methods to reduce the bias amplification. The bias amplification is defined as the difference between the “moderate-predictive” and “weak” features of a machine learning model. The model is trained on a set of features that are “moderately-predictive” weak”, and the bias is reduced by using targeted feature selection for the feature-wise bias amplification of the model. They show that they can reduce bias in convolutional neural networks by using feature selection algorithms for linear models. The algorithms are evaluated on synthetic and real data, and show that the reduced bias leads to better accuracy."
6664,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"over - parametrization USED-FOR generalization. over - parametrization USED-FOR neural networks. neural networks USED-FOR generalization. normalized margin CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION normalized margin. global minimizer FEATURE-OF weakly - regularized cross - entropy loss. generalization error bounds FEATURE-OF deep networks. maximum normalized margin FEATURE-OF global minimizer. two - layer networks FEATURE-OF infinite - width neural network. generalization guarantees EVALUATE-FOR infinite - width neural network. neural net margin COMPARE kernel methods. kernel methods COMPARE neural net margin. kernel methods HYPONYM-OF infinite feature methods. generalization guarantees EVALUATE-FOR kernel methods. infinite - neuron viewpoint USED-FOR analyzing optimization. perturbed gradient flow USED-FOR global optimizer. perturbed gradient flow USED-FOR infinite - size networks. polynomial time FEATURE-OF global optimizer. Method are margin - based perspective, and multi - layer feedforward relu networks. Material is natural instances. ","This paper studies the problem of over-parametrization in neural networks for generalization. The authors show that the global minimizer of weakly-regularized cross-entropy loss with a maximum normalized margin is a good generalization error bound for deep networks with two-layer networks. They also provide a margin-based perspective for multi-layer feedforward relu networks. Finally, they show that a global optimizer with polynomial time can be obtained with perturbed gradient flow for infinite-size networks. ","This paper proposes a new perspective on the generalization of neural networks based on over-parametrization. The authors show that the global minimizer of weakly-regularized cross-entropy loss is the maximum normalized margin of a deep networks, and that the normalized margin can be used to improve generalization error bounds of deep networks. They also show that this margin-based perspective is applicable to multi-layer feedforward relu networks. Finally, they show that for infinite-width neural network with two-layer networks, the global optimizer can be computed in polynomial time, which is much faster than the neural net margin compared to other infinite feature methods such as kernel methods. The paper also provides an infinite-neuron viewpoint for analyzing optimization."
6668,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"image captions HYPONYM-OF natural language descriptions. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. object pathway USED-FOR generator. object pathway USED-FOR discriminator. semantic layout USED-FOR approach. image background CONJUNCTION image layout. image layout CONJUNCTION image background. global pathway USED-FOR image background. global pathway USED-FOR image layout. Multi - MNIST CONJUNCTION CLEVR. CLEVR CONJUNCTION Multi - MNIST. CLEVR CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION CLEVR. Multi - MNIST CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION Multi - MNIST. global image characteristics CONJUNCTION image background. image background CONJUNCTION global image characteristics. object pathway USED-FOR features. global pathway USED-FOR image background. global pathway USED-FOR global image characteristics. object pathway COMPARE global pathway. global pathway COMPARE object pathway. Method is Generative Adversarial Networks ( GANs ). OtherScientificTerm are bounding boxes, and complex scenes. ","This paper proposes a novel approach to generate image captions from natural language descriptions. The approach is based on the semantic layout of GANs. The key idea is to use an object pathway between the generator and the discriminator to guide the generator. The object pathway is then used to learn the image background, image layout, and image characteristics. The global pathway is used to train the image layout and image background. The proposed method is evaluated on Multi-MNIST, CLEVR, and MSCOCO data set. The results show the effectiveness of the proposed method.","This paper proposes a novel approach to generate natural language descriptions of images, i.e., image captions, from bounding boxes. Generative Adversarial Networks (GANs) are a family of GANs. The proposed approach is based on the semantic layout. The generator and discriminator are based on an object pathway, where the generator generates a generator and the discriminator generates a discriminator. The global pathway is used to generate image background, image layout, image background and global image characteristics. Experiments are conducted on Multi-MNIST, CLEVR, MSCOCO data set, and the CLEVR data set with complex scenes."
6672,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,models USED-FOR policy improvement. learning models USED-FOR global model methods. local model methods USED-FOR system dynamics. representations USED-FOR simple dynamics. local models USED-FOR policy learning in complex systems. real Sawyer robotic arm USED-FOR manipulation task. manipulation task HYPONYM-OF robotics tasks. robotics tasks EVALUATE-FOR approach. camera images USED-FOR manipulation task. OtherScientificTerm is local improvements. ,"This paper proposes a new method for policy learning in complex systems, where the goal is to improve the performance of global models by learning local models for policy improvement. The authors propose to use local model methods to learn the system dynamics by learning representations for simple dynamics, and then use these representations to learn local improvements for complex systems. The proposed approach is evaluated on a variety of robotics tasks, including a manipulation task using camera images and a real Sawyer robotic arm.",This paper proposes a new method for learning models for policy improvement. The main idea is to use local model methods to learn system dynamics and representations for simple dynamics. The local models are then used for policy learning in complex systems. The authors show that the local improvements are more effective than global model methods. The proposed approach is evaluated on a variety of robotics tasks including manipulation task on camera images and a real Sawyer robotic arm.
6676,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"reinforcement learning algorithms USED-FOR real experience. models USED-FOR Learning policies. POMDPs FEATURE-OF learning policies. off - policy experience USED-FOR learning policies. structural causal models USED-FOR counterfactual evaluation of arbitrary policies. It USED-FOR counterfactual evaluation of arbitrary policies. structural causal models USED-FOR It. CF - GPS COMPARE vanilla model - based RL algorithms. vanilla model - based RL algorithms COMPARE CF - GPS. logged data USED-FOR CF - GPS. off - policy algorithms COMPARE CF - GPS. CF - GPS COMPARE off - policy algorithms. model USED-FOR CF - GPS. experience data USED-FOR algorithm. Importance Sampling USED-FOR off - policy algorithms. CF - GPS USED-FOR Guided Policy Search. counterfactual methods USED-FOR reparameterization - based algorithms. Stochastic Value Gradient HYPONYM-OF reparameterization - based algorithms. Task are simulating plausible experience de novo, modelbased policy evaluation and search, and de novo synthesis of data. OtherScientificTerm are logged, real experience, counterfactual actions, and off - policy episodes. ","This paper studies the problem of simulating plausible experience de novo in reinforcement learning algorithms for real experience. Learning policies from POMDPs with off-policy experience is a common problem in RL, and models trained on these models are not always able to capture the true experience of the environment. The authors propose a new model called CF-GPS, which uses structural causal models to perform counterfactual evaluation of arbitrary policies. It is based on the idea of Importance Sampling, which is a popular technique in the literature for model-based policy evaluation and search. The proposed algorithm uses experience data from the logged data to train the model, and then uses the model to perform Guided Policy Search using the experience data. The experimental results show that CF- GPS outperforms vanilla model -based RL algorithms in terms of Stochastic Value Gradient and reparameterization-based algorithms.","This paper proposes a new approach to learning from real-world data. The authors propose a new model, called CF-GPS, which is based on the notion of plausible experience de novo (POMDPs), which is an extension of Reinforcement learning algorithms for real experience. It uses structural causal models to perform the counterfactual evaluation of arbitrary policies. It is shown to outperform vanilla model-based RL algorithms in terms of the Stochastic Value Gradient. The main contribution of the paper is that the authors propose to use off-policy experience for learning policies that are not based on real experience (e.g., the ones that are based on logged, real experience). The authors also propose a method for modelbased policy evaluation and search based on Importance Sampling. The proposed algorithm is evaluated on a set of experience data, where the authors show that the proposed model outperforms CF-PPS on Guided Policy Search on the logged data. They also show that CF-PSG outperforms other reparameterization-based algorithms on the de novevo synthesis of data. "
6680,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"flat local minima of loss surface USED-FOR generalization. parameter space FEATURE-OF flat local minima of loss surface. parameter space FEATURE-OF loss surface. generalization EVALUATE-FOR loss surface. parameter space CONJUNCTION input space. input space CONJUNCTION parameter space. input space FEATURE-OF decision surfaces. parameter space FEATURE-OF decision surfaces. input space FEATURE-OF decision surface. adversarial robustness indicator USED-FOR neural network ’s intrinsic robustness property. method USED-FOR network ’s intrinsic adversarial robustness. Method are neural network generalization, robust training method, and adversarial training. OtherScientificTerm are adversarial settings, and adversarial attacks. Metric is adversarial robustness. ","This paper studies the problem of neural network generalization in adversarial settings. The authors propose a robust training method that uses the flat local minima of loss surface in the parameter space of the loss surface to improve the generalization of a loss surface. The parameter space and the input space of a decision surface are then used to train a neural network’s intrinsic robustness property, which is then used as an adversarial robustness indicator. The proposed method is then applied to train the network “sintrinsically robustness” to adversarial attacks. ","The paper proposes a new robust training method for neural network generalization. The main idea is to use flat local minima of loss surface for generalization in parameter space and input space. The loss surface is defined in the parameter space, and the input space is the decision surface. In adversarial settings, adversarial attacks can be applied to the decision surfaces. The authors propose to use an adversarial robustness indicator to measure the neural network’s intrinsic robustness property. The proposed method is evaluated on a variety of datasets and shows that the proposed method improves the network “sintrinsic robustness”. "
6684,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"autonomous drones CONJUNCTION wearable devices. wearable devices CONJUNCTION autonomous drones. energy consumption PART-OF DNN training. weighted sparse projection CONJUNCTION input masking. input masking CONJUNCTION weighted sparse projection. end - to - end DNN training framework USED-FOR quantitative energy consumption guarantees. weighted sparse projection USED-FOR end - to - end DNN training framework. input masking USED-FOR end - to - end DNN training framework. weighted sparse projection USED-FOR quantitative energy consumption guarantees. input masking USED-FOR quantitative energy consumption guarantees. optimization problem USED-FOR DNN training. DNN training process USED-FOR constrained optimization. quantitative DNN energy estimation PART-OF DNN training process. approximate algorithm USED-FOR optimization problem. framework USED-FOR DNNs. prior energy - saving methods COMPARE framework. framework COMPARE prior energy - saving methods. Method is Deep Neural Networks ( DNNs ). OtherScientificTerm are energy budget, optimization constraint, and energy budgets. ","This paper studies the energy consumption of Deep Neural Networks (DNNs) in the context of autonomous drones and wearable devices. The authors propose a new end-to-end DNN training framework based on weighted sparse projection and input masking to obtain quantitative energy consumption guarantees for DNNs. The optimization problem of the optimization problem is formulated as a constrained optimization problem, and the authors propose an approximate algorithm to solve the problem. The proposed framework is shown to outperform prior energy-saving methods. ","This paper proposes a new framework for optimizing Deep Neural Networks (DNNs) with energy consumption. The main idea is to use weighted sparse projection and input masking for quantitative energy consumption guarantees in the end-to-end DNN training framework. The proposed framework is evaluated on autonomous drones and wearable devices. The authors show that the proposed framework outperforms prior energy-saving methods on DNNs. The optimization problem is formulated as a constrained optimization problem where the energy budget is a function of the optimization constraint, and the authors propose an approximate algorithm to solve the optimization problem. The paper also provides a theoretical analysis of the energy budgets of DNN learning with quantitative DNN energy estimation."
6688,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"Addressing uncertainty USED-FOR autonomous systems. latent model parameters FEATURE-OF posterior distribution. belief distribution USED-FOR expected long - term reward. universal policy USED-FOR Bayesian value function. policy optimization algorithms USED-FOR universal policy. universal policy USED-FOR exploration - exploitation trade - off. policy optimization algorithms USED-FOR Bayesian Policy Optimization. policy optimization algorithms USED-FOR algorithm. policy network architecture USED-FOR belief distribution. observable state FEATURE-OF belief distribution. method COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE method. method COMPARE algorithms. algorithms COMPARE method. algorithms USED-FOR model uncertainty. algorithms COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE algorithms. method USED-FOR model uncertainty. OtherScientificTerm are continuous latent parameter space, and belief distributions. ","This paper studies the problem of addressing uncertainty in autonomous systems. The authors propose a new algorithm based on policy optimization algorithms for Bayesian Policy Optimization. The main idea is to learn a belief distribution over the expected long-term reward from the latent model parameters of the posterior distribution of a continuous latent parameter space. The belief distribution is learned using a policy network architecture, which is then used to learn the universal policy for the Bayesian value function. The universal policy is used for exploration-exploration trade-off. The proposed method is shown to outperform existing algorithms in terms of model uncertainty. The experimental results show that the proposed method outperforms Partially Observable Markov Decision Process solvers.","This paper proposes a new method for addressing uncertainty in autonomous systems. The authors consider the problem of estimating uncertainty in a continuous latent parameter space, where the posterior distribution of the latent model parameters is a function of the expected long-term reward. The paper proposes an algorithm based on two policy optimization algorithms for Bayesian Policy Optimization, where a universal policy is used to estimate the Bayesian value function. The belief distribution is learned using a policy network architecture, which is based on the belief distribution in the observable state. The proposed method is evaluated on a variety of datasets and compared to other algorithms for model uncertainty. The results show that the proposed method outperforms Partially Observable Markov Decision Process solvers."
6692,SP:3823faee83bc07a989934af5495dafd003c27921,"unified framework USED-FOR unsupervised representations of entities. optimal transport USED-FOR representations. method USED-FOR uncertainty. sentence similarity CONJUNCTION word entailment detection. word entailment detection CONJUNCTION sentence similarity. unsupervised representations USED-FOR text. tasks EVALUATE-FOR it. word entailment detection HYPONYM-OF tasks. sentence similarity HYPONYM-OF tasks. approach USED-FOR unsupervised or supervised problem. co - occurrence structure FEATURE-OF unsupervised or supervised problem. sequence data HYPONYM-OF co - occurrence structure. Wasserstein distances CONJUNCTION Wasserstein barycenters. Wasserstein barycenters CONJUNCTION Wasserstein distances. Wasserstein distances USED-FOR framework. OtherScientificTerm are histogram ( or distribution ), entities, distributions, and optimal transport map. Method is rich and powerful feature representations. ","This paper proposes a unified framework for unsupervised representations of entities. The proposed method is based on the idea that the histogram (or distribution) of entities can be represented as an optimal transport map, and that the representations can be learned using optimal transport. The authors show that it can be applied to a variety of tasks, such as sentence similarity and word entailment detection. The framework is built on top of the Wasserstein distances and the use of a combination of sequence data. The paper shows that the proposed approach can be used to solve an unstructured or supervised problem with co-occurrence structure, where the distribution of the entities is learned by the distributions. ","This paper proposes a unified framework for learning unsupervised representations of entities, where the histogram (or distribution) of entities is represented as a set of distributions, and the representations are learned using optimal transport. The proposed framework is based on the Wasserstein distances, which can be computed from sequence data. The authors show that the proposed approach can be used to solve an unsupersupervised or supervised problem with co-occurrence structure, and that it can be applied to a variety of tasks, including sentence similarity and word entailment detection. The method is evaluated on text, and it is shown that the uncertainty of the proposed method is lower than the uncertainty in rich and powerful feature representations."
6696,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,model - based reinforcement learning methods COMPARE model - free approaches. model - free approaches COMPARE model - based reinforcement learning methods. model - based reinforcement learning methods USED-FOR complex simulated environments. dynamics model CONJUNCTION planner. planner CONJUNCTION dynamics model. recursions USED-FOR long - range planning. recursions USED-FOR state estimates. model accuracy CONJUNCTION performance. performance CONJUNCTION model accuracy. task reward EVALUATE-FOR model - free approaches. Material is MuJoCo environments. OtherScientificTerm is long planning horizons. ,This paper studies the problem of long-range planning in MuJoCo environments. The authors propose to use model-based reinforcement learning methods to learn complex simulated environments with long planning horizons. They show that model-free approaches can achieve better model accuracy and better performance than model -free approaches in terms of task reward. They also show that recursions can be used to improve the state estimates of the dynamics model and the planner.,"This paper proposes a new model-based reinforcement learning methods for complex simulated environments. The authors show that the proposed model-free approaches are more robust to long-range planning, and that the model accuracy and performance can be improved by using recursions in the long range planning. The paper also shows that the learned dynamics model and planner can be used for long planning horizons. Experiments are conducted on MuJoCo environments, and the proposed approach is shown to improve model accuracy, performance, and task reward."
6700,SP:da14205470819495a3aad69d64de4033749d4d3e,2or 3 - bit precision HYPONYM-OF ultra - low precision. precision highway USED-FOR ultralow - precision computation. precision highway USED-FOR convolutional and recurrent neural networks. precision highway USED-FOR accumulated quantization error. accumulated quantization error FEATURE-OF convolutional and recurrent neural networks. hardware accelerator EVALUATE-FOR overhead. method COMPARE quantization methods. quantization methods COMPARE method. 3 - bit weight / activation quantization CONJUNCTION 2 - bit quantization. 2 - bit quantization CONJUNCTION 3 - bit weight / activation quantization. top-1 accuracy loss EVALUATE-FOR ResNet-50. 3 - bit weight / activation quantization EVALUATE-FOR method. ResNet-50 EVALUATE-FOR 2 - bit quantization. top-1 accuracy loss EVALUATE-FOR 2 - bit quantization. accuracy loss EVALUATE-FOR method. top-1 accuracy loss EVALUATE-FOR method. method COMPARE method. method COMPARE method. LSTM USED-FOR language modeling. 2 - bit quantization FEATURE-OF LSTM. 2 - bit quantization USED-FOR method. 2 - bit quantization FEATURE-OF method. Method is Neural network quantization. ,"This paper studies the problem of ultra-low precision, i.e. 2or 3-bit precision, in the context of ultralow-precision computation. The authors propose a precision highway to reduce the accumulated quantization error of convolutional and recurrent neural networks by using the precision highway. The proposed method is evaluated on ResNet-50 with top-1 accuracy loss and 2-bit quantization. The paper shows that the proposed method outperforms existing quantization methods in terms of accuracy loss. ","This paper proposes a new method for quantization of neural networks. The proposed method is based on the precision highway for ultralow-precision computation, i.e. 2or 3-bit precision. The authors show that the accumulated quantization error of convolutional and recurrent neural networks can be reduced by using the proposed precision highway. The paper also shows that the proposed method outperforms other quantization methods in terms of top-1 accuracy loss for ResNet-50 and 2-bit quantization for LSTM for language modeling. "
6704,SP:0355b54430b39b52df94014d78289dd6e1e81795,"method USED-FOR image restoration problems. deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION deblurring. denoising CONJUNCTION deblurring. deblurring CONJUNCTION denoising. denoising HYPONYM-OF image restoration problems. super - resolution HYPONYM-OF image restoration problems. deblurring HYPONYM-OF image restoration problems. constrained optimization problem USED-FOR problem. Generative Adversarial Network ( GAN ) USED-FOR density estimation model. OtherScientificTerm are posteriori probability of latent variables, latent variables, and degraded image. Material is MNIST dataset. ","This paper proposes a method for image restoration problems such as denoising, deblurring, super-resolution. The problem is formulated as a constrained optimization problem, where the posteriori probability of latent variables is known. The goal is to estimate the density estimation model using a Generative Adversarial Network (GAN). The latent variables are generated from the degraded image. The authors show that the proposed method can achieve state-of-the-art performance on MNIST dataset.","This paper proposes a method for image restoration problems such as denoising, deblurring, super-resolution. The problem is formulated as a constrained optimization problem, where the posteriori probability of latent variables is computed. The authors propose a density estimation model based on Generative Adversarial Network (GAN) to estimate the posterior of the latent variables. The proposed method is evaluated on the MNIST dataset. "
6708,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"softmax outputs CONJUNCTION feature responses. feature responses CONJUNCTION softmax outputs. large "" teacher "" network CONJUNCTION compact "" student "" network. compact "" student "" network CONJUNCTION large "" teacher "" network. full training data USED-FOR knowledge distillation methods. method USED-FOR knowledge distillation. conv - layer PART-OF student - net. layer PART-OF conv - layer. layer PART-OF conv - layer. computation cost FEATURE-OF conv - layer. teacher - net CONJUNCTION student - net constructing. student - net constructing CONJUNCTION teacher - net. OtherScientificTerm are human cognition, feature map sizes, and block - level outputs. ","This paper proposes a new method for knowledge distillation based on full training data. The main idea is to use a large ""teacher"" network and a compact ""student"" network with softmax outputs and feature responses. The conv-layer of the student-net consists of a layer that maps the feature map sizes to the block-level outputs. The computation cost of the conv -layer is reduced by a factor of $\mathcal{O}(\sqrt{T})$ for each block. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of datasets. ","This paper proposes a method for knowledge distillation based on full training data. The idea is to use a large ""teacher"" network and a compact ""student"" network to learn softmax outputs and feature responses. The conv-layer of the student-net consists of a layer that maps the feature map sizes to the block-level outputs. The computation cost of the conv - layer is proportional to the number of blocks. The teacher-net is used for student-nets constructing, and the compact student-network is used to learn the blocks. Experiments show that the proposed method is able to improve the performance of knowledge distillation methods."
6712,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,task relatedness USED-FOR transferability. transferred representations USED-FOR classification problems. H - score HYPONYM-OF evaluation function. evaluation function EVALUATE-FOR transferred representations. asymptotic error probability FEATURE-OF decision function. information theoretic approach USED-FOR H - score. asymptotic error probability FEATURE-OF H - score. transferred feature USED-FOR asymptotic error probability. transferred feature USED-FOR H - score. source tasks USED-FOR task transfer learning problems. transferability USED-FOR source tasks. it USED-FOR inference problems. recognition tasks USED-FOR 3D indoor - scene understanding. classification HYPONYM-OF inference problems. recognition tasks HYPONYM-OF inference problems. Task is task transfer learning. Metric is task transferability. OtherScientificTerm is representations. Generic is metric. Method is transfer learning policies. Material is synthetic and real image data. ,"This paper studies the problem of task transferability in the context of classification problems. The authors propose a metric called H-score, which is based on the asymptotic error probability of the decision function of the transferred representations of the source task. They show that the transferred feature can be used to improve the H-scores of the original source tasks. They also show that it can improve the performance of the transfer learning policies. Finally, they demonstrate that it is useful for inference problems such as classification and recognition tasks in 3D indoor-scene understanding.","This paper proposes a new metric to measure the transferability of task relatedness in the context of task transfer learning. The metric is based on the H-score, which is the evaluation function of a decision function with asymptotic error probability of the transferred representations for classification problems and recognition problems. The authors propose an information theoretic approach to compute H-scores using the transferred feature of the decision function. The proposed metric is evaluated on synthetic and real image data, and it is shown that it can be applied to source tasks as well as to inference problems such as classification and recognition tasks for 3D indoor-scene understanding. "
6716,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"Planning USED-FOR complex languages. planning phase USED-FOR global sentence structure. planning phase PART-OF neural machine translation. discrete structural representations USED-FOR syntactic information. approach USED-FOR discrete structural representations. beam search USED-FOR structural codes. discrete codes USED-FOR word generation. codes USED-FOR pure structural variations. codes USED-FOR translation. structural planning USED-FOR global sentence structure. Method is language generation models. Metric are structural diversity metric, and diversity scores. Material is sampled paraphrase translations. ","This paper proposes a new structural diversity metric for language generation models. The proposed approach uses discrete structural representations to capture syntactic information in the planning phase of neural machine translation. The structural codes are learned by beam search, and are then used for word generation. The authors show that the codes can be used for pure structural variations in the translation, as well as for more complex languages. The diversity scores are also used to evaluate the performance of sampled paraphrase translations. ",This paper proposes a novel approach to learn discrete structural representations for syntactic information in complex languages. The idea is to use the planning phase in neural machine translation to learn global sentence structure. The authors propose a structural diversity metric to measure the diversity scores of the language generation models. The proposed approach uses beam search to find structural codes for word generation from discrete codes for pure structural variations. These codes are then used for translation and are used for sampling paraphrase translations. 
6720,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"discriminator PART-OF generative adversarial network ( SGAN ). SGAN COMPARE integral probability metric ( IPM ) GANs. integral probability metric ( IPM ) GANs COMPARE SGAN. real data COMPARE randomly sampled fake data. randomly sampled fake data COMPARE real data. real data COMPARE fake data. fake data COMPARE real data. approaches USED-FOR GAN loss functions. Relativistic GANs ( RGANs ) CONJUNCTION Relativistic average GANs ( RaGANs ). Relativistic average GANs ( RaGANs ) CONJUNCTION Relativistic GANs ( RGANs ). Relativistic GANs ( RGANs ) HYPONYM-OF them. Relativistic average GANs ( RaGANs ) HYPONYM-OF them. IPM - based GANs HYPONYM-OF RGANs. identity function USED-FOR RGANs. identity function USED-FOR IPM - based GANs. RaGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RaGANs. RGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RGANs. images COMPARE ones. ones COMPARE images. WGAN - GP CONJUNCTION SGAN. SGAN CONJUNCTION WGAN - GP. RGANs CONJUNCTION RaGANs. RaGANs CONJUNCTION RGANs. GAN CONJUNCTION LSGAN. LSGAN CONJUNCTION GAN. RaGAN COMPARE WGAN - GP. WGAN - GP COMPARE RaGAN. WGAN - GP CONJUNCTION spectral normalization. spectral normalization CONJUNCTION WGAN - GP. discriminator update CONJUNCTION generator update. generator update CONJUNCTION discriminator update. spectral normalization USED-FOR SGAN. SGAN USED-FOR images. WGAN - GP USED-FOR images. gradient penalty USED-FOR RaGAN. SGAN USED-FOR ones. WGAN - GP USED-FOR ones. spectral normalization USED-FOR ones. Method are generator G, divergence minimization, and relativistic discriminator. OtherScientificTerm is priori knowledge. Generic is code.","This paper proposes a new discriminator for the generative adversarial network (SGAN). The proposed discriminator is based on the integral probability metric (IPM) GAN, which is an extension of the SGAN. The key idea is to use the identity function of the generator G as the discriminator. The authors show that this discriminator can be used to improve the performance of the GAN loss functions. They also show that their discriminator improves the performance in terms of divergence minimization. ","This paper proposes a new discriminator in the generative adversarial network (SGAN) framework. The proposed discriminator is based on the integral probability metric (IPM) GAN, which is an extension of SGAN. The authors show that the generator G can be decomposed into two parts: divergence minimization, and the identity function of the discriminator. The identity function is used to estimate the divergence of the generator and discriminator, which can be used as a priori knowledge. Experiments are performed on real data and randomly sampled fake data. The results show that SGAN outperforms SGAN and WGAN-GP in terms of image quality and spectral normalization. "
6724,SP:8df1599919dcb3329553e75ffb19059f192542ea,"catastrophic forgetting FEATURE-OF neural network architectures. approach USED-FOR problem. approach USED-FOR model. solver HYPONYM-OF model. Task is AI and lifelong learning systems. Method are continual learning methods, and Parameter Generation. ",This paper studies the problem of catastrophic forgetting in neural network architectures. The authors propose a new approach to solve the problem by learning a model with a solver. The model is trained using continual learning methods. Parameter Generation is used to train the model. Experimental results show the effectiveness of the proposed approach.,"This paper studies the problem of catastrophic forgetting in neural network architectures with continual learning methods. The authors propose a new approach to solve the problem, called Parameter Generation. The proposed model is based on a solver. The paper is well-written and well-motivated, and the paper is easy to follow. However, it is not clear to me why the paper focuses on AI and lifelong learning systems."
6728,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"artificial agents USED-FOR them. rich and orderly structure USED-FOR systems. behavioral dynamics FEATURE-OF multi - agent systems. rich and orderly structure FEATURE-OF behavioral dynamics. rich and orderly structure FEATURE-OF multi - agent systems. Relational Forward Models ( RFM ) USED-FOR multi - agent learning. they USED-FOR interpretable intermediate representations. discrete entities USED-FOR models. RFM modules USED-FOR learning systems. learning systems COMPARE non - augmented baselines. non - augmented baselines COMPARE learning systems. RFM modules PART-OF agents. RFM modules COMPARE non - augmented baselines. non - augmented baselines COMPARE RFM modules. Generic is networks. OtherScientificTerm are multi - agent environments, and social interactions. Method is autonomous systems. ","This paper proposes Relational Forward Models (RFM) for multi-agent learning with rich and orderly structure in systems with behavioral dynamics. The authors show that RFM modules can be integrated into existing learning systems, and that they can be used to learn interpretable intermediate representations of the agents. They also show that these networks are able to learn to interact with other agents in multi- agent environments, and can be trained with autonomous systems. ","This paper proposes Relational Forward Models (RFM) for multi-agent learning, where agents are trained with a set of artificial agents and they are trained on them. The authors show that RFM modules for learning systems with multiple agents can be learned with a rich and orderly structure of the behavioral dynamics of the systems. They also show that they are able to learn interpretable intermediate representations of discrete entities in the environment, and that they can learn networks that can be trained on multiple agents. They show that these learning systems are more interpretable than non-augmented baselines, and they show that their learning systems can learn to predict the behavior of other agents in the same environment. "
6732,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"reinforcement learning USED-FOR real world problems. expert behavior USED-FOR reward function. method USED-FOR rewards. prior USED-FOR approach. images USED-FOR rewards. OtherScientificTerm are oracle reward function, reward functions, and expressive reward functions. Method is Inverse reinforcement learning ( IRL ). Material is datasets of demonstrations. Task is IRL. Generic is tasks. ","This paper studies the problem of reinforcement learning in real world problems. Inverse reinforcement learning (IRL) is a well-studied problem in which the goal is to learn an oracle reward function that is expressive of the expert behavior of the reward function. The authors propose a new method to learn rewards that are expressive of expert behavior. The proposed approach is based on a prior that learns a prior for each reward function, and then uses this prior to learn the reward functions. The reward functions are learned using a set of demonstrations. The rewards are learned by using images from the demonstrations.  The authors show that the proposed IRL can achieve state-of-the-art performance on a variety of tasks.","This paper proposes a new method for learning rewards for real world problems. Inverse reinforcement learning (IRL) is an extension of the oracle reward function, where the reward functions are learned from expert behavior. The main idea of IRL is to learn a reward function that depends on expert behavior, and then use expert behavior to learn the reward function. The proposed approach is based on the prior, which is a prior to the rewards learned from images. The authors show that the proposed approach can be applied to a variety of datasets of demonstrations. They also show that it can be used to learn expressive reward functions. "
6736,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"framework USED-FOR data efficient and versatile learning. framework USED-FOR Meta - Learning approximate Probabilistic Inference. Meta - Learning approximate Probabilistic Inference USED-FOR Prediction. ML - PIP HYPONYM-OF framework. ML - PIP USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR ML - PIP. VERSA HYPONYM-OF framework. flexible and versatile amortization network USED-FOR framework. flexible and versatile amortization network USED-FOR VERSA. benchmark datasets EVALUATE-FOR method. benchmark datasets EVALUATE-FOR VERSA. few - shot ShapeNet view reconstruction task EVALUATE-FOR approach. Material is few - shot learning datasets. OtherScientificTerm are task - specific parameters, and second derivatives. Task are optimization, inference, and classification. Method is inference networks. ","This paper proposes a new framework for data efficient and versatile learning called Meta-Learning approximate Probabilistic Inference for Prediction. The proposed framework, called ML-PIP, is a combination of existing methods based on probabilistic interpretations of meta-learning and a flexible and versatile amortization network. The authors demonstrate the effectiveness of the proposed method on a few-shot ShapeNet view reconstruction task and on several benchmark datasets.","This paper proposes a framework for data efficient and versatile learning, called Meta-Learning approximate Probabilistic Inference for Prediction. The framework is based on ML-PIP, which is an extension of previous methods based on probabilistic interpretations of meta-learning. The proposed framework, called VERSA, uses a flexible and versatile amortization network to learn the task-specific parameters. The authors show that the proposed method outperforms existing methods on a few-shot ShapeNet view reconstruction task, and on several benchmark datasets. The main contribution of the paper is the optimization of the inference networks. The second contribution is the analysis of the second derivatives of the first derivatives. "
6740,SP:44e0f63ffee15796ba6135463134084bb370627b,"deep learning architecture USED-FOR classifying structured objects. ultrafine - grained datasets USED-FOR deep learning architecture. localvisual features CONJUNCTION neighboring class information. neighboring class information CONJUNCTION localvisual features. linear - chain CRFs USED-FOR images. visual features COMPARE class - structure information. class - structure information COMPARE visual features. convolutional layers USED-FOR visual features. CRF pairwise potential matrix USED-FOR class - structure information. parametrization USED-FOR nonlinear objective function. surrogate likelihood USED-FOR local likelihood approximation. local likelihood approximation USED-FOR CRF. surrogate likelihood USED-FOR CRF. integrated batch - normalization USED-FOR CRF. integrated batch - normalization USED-FOR local likelihood approximation. CRF methods USED-FOR contextual relationships. model COMPARE CRF methods. CRF methods COMPARE model. method COMPARE linear CRF parametrization. linear CRF parametrization COMPARE method. unnormalized likelihood optimization CONJUNCTION RNN modeling. RNN modeling CONJUNCTION unnormalized likelihood optimization. linear CRF parametrization CONJUNCTION unnormalized likelihood optimization. unnormalized likelihood optimization CONJUNCTION linear CRF parametrization. linear CRF parametrization COMPARE RNN modeling. RNN modeling COMPARE linear CRF parametrization. method COMPARE RNN modeling. RNN modeling COMPARE method. OtherScientificTerm are context - based semantic similarity space, visual similarities, and contextual information. Material is images of retail - store product displays. ",This paper proposes a new deep learning architecture for classifying structured objects on ultrafine-grained datasets. The key idea is to use the CRF pairwise potential matrix as a parametrization for the nonlinear objective function. The CRF is trained with integrated batch-normalization to improve the local likelihood approximation for CRF by using surrogate likelihood to approximate the surrogate likelihood of CRF. The authors show that the proposed model is able to learn contextual relationships better than existing CRF methods. The proposed method is shown to outperform linear CRF and RNN modeling in terms of unnormalized likelihood optimization and unnormalised likelihood optimization. ,This paper proposes a new deep learning architecture for classifying structured objects on ultrafine-grained datasets. The key idea is to use a CRF pairwise potential matrix to encode the class-structured information and the neighboring class information in the context-based semantic similarity space. The authors use linear-chain CRFs to generate images of images of retail-store product displays. The visual similarities between two images are computed using convolutional layers. The local likelihood approximation for CRF is based on integrated batch-normalization. The parametrization of the nonlinear objective function is done using surrogate likelihood. The proposed model outperforms other CRF methods in terms of contextual relationships and unnormalized likelihood optimization compared to RNN modeling. 
6744,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"global structure CONJUNCTION fine - scale waveform coherence. fine - scale waveform coherence CONJUNCTION global structure. audio synthesis HYPONYM-OF machine learning task. global structure FEATURE-OF human perception. fine - scale waveform coherence FEATURE-OF human perception. local structure FEATURE-OF Autoregressive models. WaveNet HYPONYM-OF Autoregressive models. global latent conditioning CONJUNCTION parallel sampling. parallel sampling CONJUNCTION global latent conditioning. global latent conditioning FEATURE-OF Generative Adversarial Networks ( GANs ). parallel sampling USED-FOR Generative Adversarial Networks ( GANs ). log magnitudes CONJUNCTION instantaneous frequencies. instantaneous frequencies CONJUNCTION log magnitudes. GANs USED-FOR high - fidelity and locally - coherent audio. log magnitudes USED-FOR GANs. spectral domain FEATURE-OF sufficient frequency resolution. sufficient frequency resolution FEATURE-OF log magnitudes. sufficient frequency resolution FEATURE-OF instantaneous frequencies. spectral domain FEATURE-OF instantaneous frequencies. sufficient frequency resolution USED-FOR GANs. GANs COMPARE WaveNet baselines. WaveNet baselines COMPARE GANs. NSynth dataset EVALUATE-FOR GANs. automated and human evaluation metrics EVALUATE-FOR WaveNet baselines. automated and human evaluation metrics EVALUATE-FOR GANs. Method is iterative sampling. OtherScientificTerm are global latent structure, and locally - coherent audio waveforms. ","This paper studies the problem of audio synthesis, an important machine learning task in audio synthesis. In this task, the goal is to synthesize high-fidelity and locally-coherent audio. The authors consider the problem in the context of human perception, where the global structure and fine-scale waveform coherence are important factors in human perception. Autoregressive models such as WaveNet and GANs are trained with global latent conditioning and parallel sampling, and the global latent structure is used to train Generative Adversarial Networks (GANs) with parallel sampling.  The authors show that the log magnitudes and the instantaneous frequencies in the spectral domain are sufficient frequency resolution for GAN, and that the resulting WaveNet baselines outperform WaveNet on the NSynth dataset.  ","This paper proposes a new machine learning task, audio synthesis, where the goal is to generate high-fidelity and locally-coherent audio. The authors propose to use iterative sampling to learn the global latent structure and fine-scale waveform coherence for human perception. Autoregressive models such as WaveNet and Generative Adversarial Networks (GANs) are based on global latent conditioning and parallel sampling. They show that GANs are able to produce high-quality and locally coherent audio with sufficient frequency resolution in the spectral domain and log magnitudes in the instantaneous frequencies. They also show that in the NSynth dataset, GAN's outperform WaveNet baselines on both automated and human evaluation metrics. "
6748,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"training accuracy EVALUATE-FOR Neural networks. mixed integer program USED-FOR piecewise - linear neural networks. verifier COMPARE state - of - the - art. state - of - the - art COMPARE verifier. finding minimum adversarial distortions EVALUATE-FOR verifier. tight formulations USED-FOR non - linearities. tight formulations CONJUNCTION presolve algorithm. presolve algorithm CONJUNCTION tight formulations. tight formulations USED-FOR computational speedup. verifier USED-FOR networks. adversarial accuracy EVALUATE-FOR MNIST classifier. adversarial accuracy EVALUATE-FOR perturbations. perturbations FEATURE-OF MNIST classifier. adversarial example USED-FOR classifier. bounded l∞ norm FEATURE-OF perturbations. robust training procedures CONJUNCTION network architectures. network architectures CONJUNCTION robust training procedures. adversarial examples COMPARE first - order attack. first - order attack COMPARE adversarial examples. Task are Verification of networks, and verification of piecewise - linear neural networks. OtherScientificTerm are minimum adversarial distortions, ReLUs, and norm - bounded perturbations. Method is convolutional and residual networks. Material is MNIST and CIFAR-10 datasets. ","This paper studies the problem of Verification of networks. The authors propose a mixed integer program for piecewise-linear neural networks, where the objective is to find the minimum adversarial distortions between the convolutional and residual networks. They show that the verifier performs better than the state-of-the-art in finding minimum discrimarial distortions. They also show that tight formulations for non-linearities (e.g., ReLUs) can be used to improve the computational speedup of networks trained with tight formulations and a presolve algorithm. Finally, they show that perturbations to the MNIST classifier can improve the adversarial accuracy of the classifier by a bounded l∞ norm.  ","This paper studies the problem of Verification of networks. The authors propose a mixed integer program for piecewise-linear neural networks. They show that the verifier is better than state-of-the-art for finding minimum adversarial distortions, and that the tight formulations for non-linearities and the presolve algorithm for nonlinearities are better for computational speedup. They also show that perturbations of the MNIST classifier with bounded l∞ norm are better than the perturbation of the ReLUs, and they show that adversarial examples are better at first-order attack than the adversarial example for the classifier. The paper also shows that robust training procedures and network architectures are more robust to adversarial attacks. "
6752,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,rich structure FEATURE-OF real world tasks. repeated structure USED-FOR learning. default policy HYPONYM-OF component. component PART-OF KL regularized expected reward objective. it USED-FOR reusable behaviours. reusable behaviours USED-FOR policy. information bottleneck approaches CONJUNCTION variational EM algorithm. variational EM algorithm CONJUNCTION information bottleneck approaches. default policy CONJUNCTION policy. policy CONJUNCTION default policy. default policy USED-FOR tasks. Method is fixed default policy. Generic is strategy. Material is discrete and continuous action domains. ,This paper studies the problem of learning from repeated structure in real world tasks with rich structure. The authors propose a strategy to learn a fixed default policy with a component of the KL regularized expected reward objective. The key idea is that it can be used to learn reusable behaviours that can be combined with the default policy. The paper shows that it is possible to learn the policy with reusable behaviours by combining information bottleneck approaches and a variational EM algorithm. Experimental results on discrete and continuous action domains demonstrate the effectiveness of the proposed strategy.,This paper proposes a new strategy for learning with a rich structure in real world tasks with repeated structure. The key idea is to use a fixed default policy that is a component of the KL regularized expected reward objective. The authors show that it can be used to learn reusable behaviours that can be applied to different tasks. The paper also proposes a variational EM algorithm and information bottleneck approaches. Experiments are conducted on discrete and continuous action domains.
6756,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,conversation history USED-FOR Conversational machine comprehension. single - turn models USED-FOR history. FLOW USED-FOR single - turn models. intermediate representations PART-OF FLOW. intermediate representations PART-OF mechanism. alternating parallel processing structure USED-FOR FLOW. alternating parallel processing structure USED-FOR intermediate representations. approaches COMPARE FLOW. FLOW COMPARE approaches. latent semantics PART-OF FLOW. CoQA CONJUNCTION QuAC. QuAC CONJUNCTION CoQA. FLOWQA HYPONYM-OF model. conversational challenges EVALUATE-FOR FLOWQA. conversational challenges EVALUATE-FOR model. tasks EVALUATE-FOR FLOW. FLOWQA COMPARE models. models COMPARE FLOWQA. sequential instruction understanding USED-FOR conversational machine comprehension. SCONE EVALUATE-FOR models. accuracy EVALUATE-FOR FLOWQA. SCONE EVALUATE-FOR FLOWQA. Metric is F1. ,"This paper studies the problem of conversational machine comprehension in the context of sequential instruction understanding. The authors propose FLOWQA, a model based on FLOW with an alternating parallel processing structure. FLOW uses the intermediate representations in FLOW as intermediate representations of the mechanism. The intermediate representations are the latent semantics of the FLOW. The paper shows that FLOW performs better than existing approaches on a variety of tasks. ","This paper proposes a new approach to learning the conversational history for Conversational machine comprehension. The authors propose FLOWQA, a model that uses an alternating parallel processing structure to learn intermediate representations in the FLOW mechanism. The proposed approach is based on single-turn models, where the history is encoded in the latent semantics. The paper shows that FLOW outperforms existing approaches on a number of tasks, and outperforms CoQA and QuAC. The model is evaluated on several conversational challenges, and is shown to outperform other models in terms of accuracy. "
6760,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,"Programming languages HYPONYM-OF machine learning. generative models USED-FOR generating static snapshots of code. source code HYPONYM-OF dynamic object. synthetic data USED-FOR edit patterns. synthetic data USED-FOR neural networks. generalization USED-FOR edit patterns. large - scale dataset USED-FOR models. fine - grained edits PART-OF large - scale dataset. Method are generative models of source code, and attentional and pointer network components. OtherScientificTerm are static snapshots of code, and source code files. Generic are it, and tools. Metric is scalability. ","This paper studies the problem of generating static snapshots of code from generative models of source code (i.e., source code). The authors propose a new dynamic object, called ""programming languages"", which is composed of static snapshots from the source code. The authors show that it is possible to generate such static snapshots by training neural networks on synthetic data with fine-grained edits. They also show that such edit patterns can be learned by generalization to different types of edit patterns. Finally, the authors provide a large-scale dataset to evaluate the performance of their models. ","This paper proposes to use generative models of source code for generating static snapshots of code. The static snapshots are generated from a dynamic object (e.g. source code) that can be represented as a programming languages. The authors propose to use the generated static snapshots as a large-scale dataset for training neural networks on synthetic data for generating edit patterns with generalization to edit patterns that are generated by fine-grained edits. The paper also proposes two tools to improve the scalability of the generated source code files. The first is to use attentional and pointer network components, and the second one is to add a scalability term to the output of the generator."
6764,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"strategy USED-FOR multi - class classification. pairwise similarity HYPONYM-OF annotation. pairwise similarity USED-FOR strategy. binary classifier USED-FOR pairwise similarity prediction. method USED-FOR binary classifier. submodule USED-FOR multi - class classifier. loss function USED-FOR neural network - based models. probabilistic graphical model USED-FOR it. method COMPARE state of the art. state of the art COMPARE method. learning paradigms EVALUATE-FOR state of the art. learning paradigms EVALUATE-FOR method. multi - class labels FEATURE-OF learning multi - class classification. accuracy EVALUATE-FOR state of the art. accuracy EVALUATE-FOR method. OtherScientificTerm is class - specific labels. Method is meta classification learning. Generic are approach, and framework. ",This paper proposes a new strategy for multi-class classification based on pairwise similarity between two classes. The proposed approach is based on a binary classifier trained with a submodule that predicts the class-specific labels for each class. The authors show that the proposed method can outperform the state of the art in terms of accuracy on several learning paradigms. ,"This paper proposes a new strategy for multi-class classification based on pairwise similarity in annotation. The proposed approach is based on meta classification learning, where each class-specific label is represented as a submodule of a binary classifier. The authors propose a new loss function for neural network-based models, which is a probabilistic graphical model. They show that the proposed method outperforms the state of the art in terms of accuracy and learning paradigms for learning multiple-class labels. "
6768,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"visual scene USED-FOR interpretation of quantifier statements. inference mechanisms USED-FOR interpretation of quantifier statements. cognitive concepts USED-FOR strategies. deep learning models USED-FOR visual question answering. spatial arrangement of the scene HYPONYM-OF confounding factors. Task is psycholinguistics. Method are FiLM visual question answering model, and approximate number system. OtherScientificTerm is Weber ’s law. Generic is system. ","This paper studies the problem of visual question answering in the context of psycholinguistics, where the goal is to answer a set of questions in a visual scene. The authors propose a FiLM-based approach to solve the problem. The proposed approach is based on the Weber’s law, which is a well-studied and well-motivated idea. The main contribution of the paper is a theoretical analysis of the effect of confounding factors (e.g., spatial arrangement of the scene) on the interpretation of quantifier statements, which are used as inference mechanisms for the interpretation. The paper also proposes two strategies based on cognitive concepts to improve the performance of the proposed strategies. ","This paper proposes a FiLM visual question answering model, which uses inference mechanisms for the interpretation of quantifier statements in a visual scene. The authors propose two strategies based on cognitive concepts. The first is based on the Weber’s law, and the second is on psycholinguistics. Both strategies are based on deep learning models. The main contribution of the paper is to study the confounding factors such as spatial arrangement of the scene, and to propose an approximate number system. The experimental results show that the proposed system is effective."
6772,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"embedding models USED-FOR link prediction in relational knowledge graphs. relational facts PART-OF Knowledge graphs. Bayesian framework USED-FOR knowledge graphs. it USED-FOR gradient based optimization. divergences FEATURE-OF non - Bayesian treatment. gradient based optimization USED-FOR divergences. hyperparameters USED-FOR gradient based optimization. Models COMPARE state - of - the - art. state - of - the - art COMPARE Models. hyperparameters COMPARE state - of - the - art. state - of - the - art COMPARE hyperparameters. hyperparameters USED-FOR Models. Material is relational knowledge graphs. Task is small data problem. OtherScientificTerm is parameter uncertainty. Method are variational inference, and Bayesian approach. ","This paper studies the problem of link prediction in relational knowledge graphs with embedding models. Knowledge graphs are relational facts that contain relational facts. The authors propose a Bayesian framework for learning knowledge graphs from these relational facts, and it is shown that it can be used for gradient based optimization with divergences in non-Bayesian treatment. Models trained with these hyperparameters are shown to perform better than state-of-the-art in terms of parameter uncertainty. The paper also shows that variational inference can be applied to the small data problem. ","This paper proposes a Bayesian framework for link prediction in relational knowledge graphs, where the relational facts in Knowledge graphs are represented by embedding models. The idea is to use variational inference to estimate the parameter uncertainty, and then use it for gradient based optimization to reduce the divergences in non-Bayesian treatment. Experiments on small data problem show that the proposed Bayesian approach outperforms state-of-the-art models in terms of hyperparameters in the case of the hyperparameter in the gradient-based optimization."
6776,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"dimension reduction USED-FOR visualization or prediction enhancement. online learning approaches USED-FOR supervised dimension reduction. algorithm USED-FOR supervised dimension reduction. sliced inverse regression ( SIR ) HYPONYM-OF algorithm. sliced inverse regression ( SIR ) USED-FOR algorithm. algorithm USED-FOR subspace of significant factors. intrinsic lower dimensionality FEATURE-OF subspace of significant factors. overlapping technique USED-FOR algorithm. real data applications EVALUATE-FOR algorithms. Method are Online learning, and incremental sliced inverse regression ( ISIR ). Task is online dimension reduction. Generic is it. ","This paper studies the problem of online dimension reduction for visualization or prediction enhancement. Online learning approaches for supervised dimension reduction have recently gained a lot of attention in recent years. The authors propose a new algorithm called sliced inverse regression (SIR) which is an extension of the existing algorithm, incremental sliced inverse regressions (ISIR), which is based on the overlapping technique. The main idea of the proposed algorithm is to learn a subspace of significant factors with intrinsic lower dimensionality, which is then used to reduce the dimension of the input data. The proposed algorithms are evaluated on a variety of real data applications and demonstrate the effectiveness of their algorithms.",This paper proposes a novel algorithm for supervised dimension reduction for visualization or prediction enhancement. The algorithm is based on sliced inverse regression (SIR) which is an extension of Online learning. The authors propose an overlapping technique to improve the algorithm. Experiments on real data applications show the effectiveness of the proposed algorithms. 
6780,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"models USED-FOR intra - modal and cross - modal interactions. models USED-FOR unexpected missing or noisy modalities. intra - modal and cross - modal interactions USED-FOR prediction. models USED-FOR prediction. multimodal data USED-FOR joint generative - discriminative objective. model USED-FOR representations. model USED-FOR independent factors. multimodal discriminative and modality - specific generative factors HYPONYM-OF independent factors. joint multimodal features USED-FOR discriminative tasks. Multimodal discriminative factors USED-FOR discriminative tasks. joint multimodal features PART-OF Multimodal discriminative factors. sentiment prediction HYPONYM-OF discriminative tasks. model USED-FOR multimodal representations. multimodal datasets EVALUATE-FOR multimodal representations. model USED-FOR missing modalities. factorized representations USED-FOR multimodal learning. Task is Learning multimodal representations. OtherScientificTerm are multiple modalities, and Modality - specific generative factors. ","This paper studies the problem of learning multimodal representations with multiple modalities. The authors propose a joint generative-discriminative objective based on multimodale data. The model learns representations that are independent of the missing modalities and the intermodal and cross-modal interactions. Multimodal discriminative factors are used to train the discriminator on a variety of tasks, including sentiment prediction, multi-modality classification, etc. The results show that the proposed model can learn to represent missing and/or noisy modalities with factorized representations.","The paper proposes a joint generative-discriminative objective based on multimodal data. The authors propose to use models for intra-modal and cross modal interactions for prediction. The model learns representations for missing or noisy modalities, and then uses the model to learn independent factors (multimodal discriminative and modality-specific generative factors) for each of these independent factors. The proposed model is evaluated on two discriminant tasks: sentiment prediction and multimodality datasets. The results show that the proposed model can learn the missing modalities with factorized representations, and that the model is able to learn multiple modalities at the same time. "
6784,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"shared conditional WaveNet core CONJUNCTION independent learned embeddings. independent learned embeddings CONJUNCTION shared conditional WaveNet core. independent learned embeddings USED-FOR multi - speaker model. shared conditional WaveNet core USED-FOR multi - speaker model. fixed weights USED-FOR neural network. stochastic gradient descent USED-FOR architecture. trained neural network encoder USED-FOR speaker embedding. sample naturalness CONJUNCTION voice similarity. voice similarity CONJUNCTION sample naturalness. approaches USED-FOR multi - speaker neural network. Method are meta - learning approach, and TTS system. Generic are network, and strategies. OtherScientificTerm is WaveNet core. Material is audio data. ","This paper proposes a meta-learning approach to learn a multi-speaker model with shared conditional WaveNet core and independent learned embeddings. The proposed architecture is based on stochastic gradient descent, where fixed weights are used to train the neural network. The network is trained using a trained neural network encoder, and the speaker embedding is learned using a TTS system. Experimental results show that the proposed approaches can improve the performance of the multi-Speaker neural network, especially when the audio data is diverse. ","This paper proposes a meta-learning approach to learn a multi-speaker model with shared conditional WaveNet core and independent learned embeddings. The architecture is based on stochastic gradient descent with fixed weights. The network is trained using a trained neural network encoder to learn speaker embedding. The authors show that the proposed approach is able to achieve better sample naturalness and voice similarity compared to other approaches for learning a multi -speaker neural network. The proposed TTS system is evaluated on audio data, and the authors show the effectiveness of the proposed strategies."
6788,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"Robust estimation HYPONYM-OF statistics. Huber ’s -contamination model USED-FOR Robust estimation. Tukey ’s median CONJUNCTION estimators. estimators CONJUNCTION Tukey ’s median. estimators HYPONYM-OF Statistically optimal procedures. depth functions USED-FOR estimators. Tukey ’s median HYPONYM-OF Statistically optimal procedures. f -GANs CONJUNCTION depth functions. depth functions CONJUNCTION f -GANs. f -Learning USED-FOR depth functions. f -Learning USED-FOR f -GANs. depth functions USED-FOR statistically optimal robust estimators. total variation distance FEATURE-OF variational lower bounds. f -Learning USED-FOR variational lower bounds. variational lower bounds USED-FOR depth functions. GANs USED-FOR computing robust estimators. Gaussian distribution CONJUNCTION elliptical distributions. elliptical distributions CONJUNCTION Gaussian distribution. statistically optimal robust location estimators USED-FOR Gaussian distribution. statistically optimal robust location estimators USED-FOR elliptical distributions. hidden layers PART-OF GANs. discriminator networks PART-OF GANs. hidden layers PART-OF discriminator networks. OtherScientificTerm are computational intractability, and first moment. Method is f GANs. ","This paper studies the problem of robust estimation in statistics. The authors propose a Huber’s-contamination model for Robust estimation, which is an extension of the “Robust estimation” framework of Huber et al. (2020). The main idea is to use the depth functions of the estimators (e.g., Tukey ’s median and estimators such as f-GANs and depth functions from f-Learning) as depth functions for statistically optimal robust estimators, such as Statistically optimal procedures such as Tukey's median or estimators based on the Gaussian distribution or elliptical distributions. The main contribution of the paper is to show that the variational lower bounds on the total variation distance between the two depth functions can be obtained by f-learning. The paper also shows that f GANs are computationally intractable, and that the computational intractability can be reduced by using discriminator networks that are trained with hidden layers. Theoretically, the paper shows that the first moment of the fGANs can be approximated by the first moments of a discriminator network. ","This paper proposes a new method for computing robust estimators. The authors propose a Huber’s-contamination model for Robust estimation of statistics. The main idea is to use the estimators of Statistically optimal procedures (e.g., Tukey ’s median, estimators based on f-GANs, depth functions, and f-Learning) to compute the depth functions for statistically optimal robust estimator of the Gaussian distribution and elliptical distributions.  The authors show that the variational lower bounds of depth functions with total variation distance can be computed by f GANs with hidden layers and discriminator networks. They also show the computational intractability of the first moment."
6792,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"symmetry CONJUNCTION repetition. repetition CONJUNCTION symmetry. higher - level, abstract regularities PART-OF scene. repetition HYPONYM-OF higher - level, abstract regularities. symmetry HYPONYM-OF higher - level, abstract regularities. vision recognition modules CONJUNCTION scene representations. scene representations CONJUNCTION vision recognition modules. symbolic program USED-FOR scene programs. model USED-FOR scene programs. hierarchical, object - based scene representation USED-FOR model. hierarchical, object - based scene representation USED-FOR scene programs. compositional structure FEATURE-OF real images. synthetic data EVALUATE-FOR model. complex visual analogy - making CONJUNCTION scene extrapolation. scene extrapolation CONJUNCTION complex visual analogy - making. scene programs USED-FOR applications. scene extrapolation HYPONYM-OF applications. complex visual analogy - making HYPONYM-OF applications. Task is Human scene perception. ","This paper studies the problem of human scene perception. The authors propose a symbolic program to learn scene programs from a hierarchical, object-based scene representation. The proposed model is able to generate scene programs with higher-level, abstract regularities such as symmetry, repetition, etc. The compositional structure of real images is also studied. The model is evaluated on synthetic data. The experiments show that the proposed model can generate complex visual analogy-making, scene extrapolation and scene representations.","This paper proposes a new model for learning scene programs from a symbolic program. The model is based on a hierarchical, object-based scene representation, which is composed of higher-level, abstract regularities in the scene, such as symmetry, repetition, etc. The compositional structure of real images is modeled as a compositional matrix, and the model is trained on synthetic data. The proposed model is evaluated on three applications: complex visual analogy-making, scene extrapolation, and scene representations. "
6796,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"long sequence data USED-FOR Recurrent neural networks. methods USED-FOR RNN state updates. memory FEATURE-OF network. methods PART-OF architectures. timing - gated LSTM RNN model USED-FOR reducing state updates. longer memory persistence CONJUNCTION error - gradient flow. error - gradient flow CONJUNCTION longer memory persistence. time gate USED-FOR longer memory persistence. model USED-FOR long temporal dependencies. model COMPARE LSTM. LSTM COMPARE model. non - optimal initialization USED-FOR time gate parameters. temporal curriculum learning schedule USED-FOR g - LSTM. computational budget term USED-FOR network. convergence time EVALUATE-FOR LSTM. computational budget term USED-FOR training loss. long sequences EVALUATE-FOR LSTM. Task are vanishing gradient problem, and network update. OtherScientificTerm are neuron, and neuron state. ",This paper studies the problem of long sequence data in Recurrent neural networks. The authors consider the vanishing gradient problem and propose a new method for reducing the memory of an RNN state updates. The proposed method is based on a timing-gated LSTM RNN model. The model is able to learn long temporal dependencies between each neuron in the network. The time gate parameters are learned using non-optimal initialization. The network is trained using a computational budget term to reduce the training loss. The convergence time of the proposed model is shown to be better than the state-of-the-art on long sequences.,This paper proposes a new method for learning long sequence data for Recurrent neural networks. The authors propose a timing-gated LSTM RNN model for reducing state updates. The proposed methods are based on existing methods for reducing RNN state updates in memory. The main contribution of the paper is to introduce a time gate to encourage longer memory persistence and error-gradient flow in the network. The paper also proposes a non-optimal initialization of the time gate parameters to encourage the network to learn long temporal dependencies. The model is evaluated on long sequences and shows better convergence time compared to LstM in terms of the computational budget term for the network and the training loss. 
6800,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"ensembling USED-FOR first and second order deep feature statistics. re - training CONJUNCTION pre - processing. pre - processing CONJUNCTION re - training. pre - processing CONJUNCTION model. model CONJUNCTION pre - processing. method COMPARE state - of - the - art. state - of - the - art COMPARE method. benchmarking tasks EVALUATE-FOR state - of - the - art. benchmarking tasks EVALUATE-FOR method. TinyImageNet resize FEATURE-OF out - of - distribution dataset. true negative rate EVALUATE-FOR method. DenseNet USED-FOR method. DenseNet USED-FOR in - distribution ( CIFAR-100 ). Method are deep neural networks, and plug - and - play detection procedure. OtherScientificTerm is feature maps. ","This paper proposes a new method for deep neural networks to detect first and second order deep feature statistics. The proposed method is based on a plug-and-play detection procedure, where the model is trained with re-training and pre-processing. The authors show that the proposed method achieves a true negative rate of 0.5% on benchmarking tasks, which is significantly better than state-of-the-art. The method is also able to detect the in-distribution (CIFAR-100) dataset with TinyImageNet resize.",This paper proposes a method for learning deep neural networks that can be used to generate first and second order deep feature statistics. The method is based on the plug-and-play detection procedure. The proposed method is evaluated on several benchmarking tasks and compared to state-of-the-art in terms of the true negative rate. The model is a combination of re-training and pre-processing. The authors show that the proposed method outperforms DenseNet for in-distribution (CIFAR-100) on TinyImageNet resize of the out-of -distribution dataset. 
6804,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,model recovery USED-FOR data classification. neural network USED-FOR weight vectors. Gaussian inputs USED-FOR empirical risk function. cross entropy USED-FOR empirical risk function. gradient descent USED-FOR one - hidden - layer neural networks. near - optimal sample CONJUNCTION computational complexity. computational complexity CONJUNCTION near - optimal sample. global convergence guarantee USED-FOR empirical risk minimization. computational complexity EVALUATE-FOR network input dimension. gradient descent USED-FOR cross entropy. cross entropy USED-FOR global convergence guarantee. cross entropy USED-FOR empirical risk minimization. gradient descent USED-FOR global convergence guarantee. gradient descent USED-FOR empirical risk minimization. OtherScientificTerm is sigmoid activations. Metric is sample complexity. Method is tensor method. ,This paper studies the problem of model recovery for data classification. The authors propose a tensor method that uses gradient descent to train one-hidden-layer neural networks with Gaussian inputs. They show that the cross entropy of the empirical risk function of the neural network can be used to estimate the weight vectors of a neural network. They also provide a global convergence guarantee for empirical risk minimization using gradient descent using cross entropy. The paper also shows that the near-optimal sample and the computational complexity of the network input dimension can be reduced by gradient descent. ,"This paper proposes a new model recovery method for data classification. The authors propose a tensor method that is based on gradient descent for one-hidden-layer neural networks with Gaussian inputs. The key idea is to use a neural network to learn weight vectors, and then use the empirical risk function based on the cross entropy of the Gaussian input and the global convergence guarantee for empirical risk minimization using gradient descent with cross entropy. They show that the near-optimal sample and the computational complexity of the network input dimension are the same, and that the sample complexity is the same for all sigmoid activations."
6808,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"method USED-FOR salient convolutional channels. convolutional layers USED-FOR features. small auxiliary connections PART-OF convolutional layers. small auxiliary connections USED-FOR FBS. it USED-FOR convolution. channel pruning methods COMPARE it. it COMPARE channel pruning methods. it USED-FOR full network structures. it USED-FOR CNNs. stochastic gradient descent USED-FOR FBS - augmented networks. channel pruning CONJUNCTION dynamic execution schemes. dynamic execution schemes CONJUNCTION channel pruning. FBS COMPARE channel pruning. channel pruning COMPARE FBS. FBS COMPARE dynamic execution schemes. dynamic execution schemes COMPARE FBS. ImageNet classification EVALUATE-FOR FBS. VGG-16 CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION VGG-16. VGG-16 EVALUATE-FOR FBS. ResNet-18 EVALUATE-FOR FBS. Method are deep convolutional neural networks, and feature boosting and suppression ( FBS ). Metric are computational and memory resources, and top-5 accuracy loss. OtherScientificTerm is channels. ","This paper proposes a method to improve the performance of salient convolutional channels by using small auxiliary connections in FBS. The main idea is to use the convolution layers of the FBS to learn features from the input data. The authors show that by using stochastic gradient descent, FBS-augmented networks can achieve better performance than channel pruning methods, and it can be applied to full network structures.  The authors also show that FBS can be used in combination with channel pruned and dynamic execution schemes to improve FBS performance on VGG-16 and ResNet-18.  ","This paper proposes a method for learning salient convolutional channels. The key idea is to add small auxiliary connections to the convolution layers, which are then used to enhance the features. The authors show that the proposed feature boosting and suppression (FBS) can improve the computational and memory resources. They show that FBS-augmented networks with stochastic gradient descent can achieve better performance on ImageNet classification and ResNet-18 compared to channel pruning and other dynamic execution schemes. They also show that it improves the performance of CNNs. "
6812,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,mixed Nash Equilibria ( NE ) perspective FEATURE-OF Generative Adversarial Networks ( GANs ). algorithmic framework USED-FOR GANs. prox methods USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR GANs. procedure USED-FOR prox methods. sampling routines USED-FOR prox methods. approach COMPARE methods. methods COMPARE approach. Adam CONJUNCTION RMSProp. RMSProp CONJUNCTION Adam. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. methods USED-FOR pure strategy equilibria. SGD HYPONYM-OF pure strategy equilibria. quality EVALUATE-FOR approach. OtherScientificTerm is mixed NE. ,"This paper studies the mixed Nash Equilibria (NE) perspective of Generative Adversarial Networks (GANs) from an algorithmic framework based on an infinite-dimensional two-player game. The prox methods in this framework are based on sampling routines. The authors propose a procedure to combine the prox methods to obtain a pure strategy equilibria such as SGD, Adam, RMSProp. The proposed approach is evaluated on a variety of benchmarks and shows improved quality over existing methods.","This paper proposes a mixed Nash Equilibria (NE) perspective for Generative Adversarial Networks (GANs) from an algorithmic framework based on an infinite-dimensional two-player game. The authors propose a procedure to combine prox methods with sampling routines to improve the performance of GANs in the mixed NE. The proposed approach is evaluated on a variety of datasets and compared to other methods for pure strategy equilibria such as Adam, RMSProp, and SGD."
6816,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"deep neural networks USED-FOR method. approach USED-FOR model. pretrained network USED-FOR problems. depth - wise convolutions HYPONYM-OF low - parameter layers. approach USED-FOR sequential transfer learning. multi - task learning problems EVALUATE-FOR logits - only fine - tuning. Generic are task, and network. Method are Single Shot MultiBox Detection ( SSD ) model, 1000 - class image classification model, and SSD feature extractor. Metric is transfer - learning accuracy. OtherScientificTerm is singletask. ","This paper proposes a new method for multi-task multi-box detection. The proposed method uses deep neural networks to train a model with a pretrained network to solve problems with low-parameter layers (e.g. depth-wise convolutions). The authors propose a new Single Shot MultiBox Detection (SSD) model, which is a 1000-class image classification model. The authors show that the proposed approach can improve the transfer-learning accuracy in sequential transfer learning. They also show that logits-only fine-tuning improves the performance of the logits in multi-tasks learning problems. ",The paper proposes a new method for multi-task multi-box detection. The proposed Single Shot MultiBox Detection (SSD) model consists of a 1000-class image classification model and a SSD feature extractor. The model is trained using deep neural networks. The authors propose a novel approach to train the model on problems with low-parameter layers (e.g. depth-wise convolutions). The authors show that the proposed approach can improve sequential transfer learning accuracy. The paper also shows that the logits-only fine-tuning can be used for multi task learning problems. The main contribution of the paper is to propose a new way to train a network that can be applied to multiple tasks. 
6820,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"Normalization methods PART-OF deep learning toolbox. batch normalization ( BN ) HYPONYM-OF normalization method. method COMPARE BN. BN COMPARE method. method COMPARE normalization techniques. normalization techniques COMPARE method. BN CONJUNCTION normalization techniques. normalization techniques CONJUNCTION BN. single and multi - task datasets EVALUATE-FOR method. Generic are They, and approach. OtherScientificTerm are manually tuned learning rate schedules, and multi - modal distributions. Method is normalization. ","This paper proposes a new normalization method called batch normalization (BN) for deep learning. BN is an extension of batch normalisation (BN), which is a popular normalization technique in the deep learning community. The main idea is to regularize the weights of the weights in BN so that they are closer to the true weights in the training data. The authors show that BN can achieve better performance than BN on both single and multi-task datasets. ","This paper proposes a new normalization method, batch normalization (BN), which is a variant of batch normalisation (BN) which is used in deep learning toolbox. The main idea of BN is to use manually tuned learning rate schedules. They show that BN outperforms BN and other normalization techniques on single and multi-task datasets. They also show that the proposed approach can be applied to multi-modal distributions."
6824,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"pruning technique USED-FOR subnetworks. test accuracy EVALUATE-FOR network. OtherScientificTerm are lottery ticket hypothesis, winning tickets, and initialization lottery. Task is training. ",This paper proposes a pruning technique for subnetworks. The main idea is to use lottery ticket hypothesis to train the winning tickets. The network is then evaluated on test accuracy. The authors show that training with the initialization lottery leads to better performance.,"This paper proposes a pruning technique for subnetworks. The main idea is to use lottery ticket hypothesis, where winning tickets are randomly assigned to different subnets, and the initialization lottery. The authors show that the proposed network improves test accuracy and training performance."
6828,SP:08c662296c7cf346f027e462d29184275fd6a102,"exploration FEATURE-OF reinforcement learning. attentive dynamics model ( ADM ) USED-FOR controllable elements of the observations. state representation USED-FOR exploration purposes. contingency information USED-FOR state representation. contingency information USED-FOR exploration purposes. high - level information CONJUNCTION supervisory data. supervisory data CONJUNCTION high - level information. actor - critic algorithm CONJUNCTION count - based exploration. count - based exploration CONJUNCTION actor - critic algorithm. expert demonstrations CONJUNCTION high - level information. high - level information CONJUNCTION expert demonstrations. representation USED-FOR count - based exploration. RAM states HYPONYM-OF high - level information. representation USED-FOR actor - critic algorithm. contingency - awareness USED-FOR exploration problems. exploration problems PART-OF reinforcement learning. contingency - awareness USED-FOR reinforcement learning. Method are Arcade Learning Element ( ALE ), and ADM. Material are Atari games, and MONTEZUMA ’S REVENGE. ","This paper studies the problem of exploration in reinforcement learning. The authors propose an attentive dynamics model (ADM) to learn the controllable elements of the observations. The ADM is based on Arcade Learning Element (ALE) and is trained on Atari games. The state representation of the state representation is then used for exploration purposes, including count-based exploration, actor-critic algorithm, and high-level information such as expert demonstrations and supervisory data.  The authors show that the ADM can be used as a contingency-awareness for reinforcement learning to solve exploration problems. The experiments are conducted on MONTEZUMA’S REVENGE.","This paper proposes a novel approach to exploration in reinforcement learning. The main idea is to use an attentive dynamics model (ADM) to learn controllable elements of the observations. The ADM is based on Arcade Learning Element (ALE), which is an extension of MONTEZUMA’S REVENGE. The state representation of the state representation can be used for exploration purposes, including count-based exploration, actor-critic algorithm, and high-level information (e.g., expert demonstrations and supervisory data). The authors show that the proposed representation is able to solve the exploration problems of reinforcement learning with the help of contingency-awareness."
6832,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"HyperGAN HYPONYM-OF generative network. generative network USED-FOR weight parameters. weight parameters PART-OF deep neural networks. generative network USED-FOR deep neural networks. HyperGAN USED-FOR latent space. HyperGAN USED-FOR low dimensional noise. architecture COMPARE generative adversarial networks. generative adversarial networks COMPARE architecture. generated network parameter distribution CONJUNCTION unknown true parameter distribution. unknown true parameter distribution CONJUNCTION generated network parameter distribution. KL - divergence FEATURE-OF generated network parameter distribution. KL - divergence FEATURE-OF unknown true parameter distribution. HyperGAN USED-FOR classification. HyperGAN COMPARE fully supervised learning. fully supervised learning COMPARE HyperGAN. HyperGAN USED-FOR MNIST and CIFAR-10 datasets. HyperGAN COMPARE ensembles. ensembles COMPARE HyperGAN. HyperGAN USED-FOR uncertainty. uncertainty EVALUATE-FOR ensembles. HyperGAN - generated ensembles USED-FOR out of distribution data. out of distribution data CONJUNCTION adversarial examples. adversarial examples CONJUNCTION out of distribution data. HyperGAN - generated ensembles USED-FOR adversarial examples. HyperGAN USED-FOR uncertainty estimates. OtherScientificTerm are classification loss, and rich distribution of effective parameters. Material is inlier data. ","This paper proposes HyperGAN, a generative network that learns weight parameters in the latent space of deep neural networks. The proposed architecture is different from existing generative adversarial networks in the sense that it is able to learn low dimensional noise. HyperGAN can be used for classification in the presence of inlier data. The authors show that the generated network parameter distribution and the unknown true parameter distribution with KL-divergence can be approximated by HyperGAN with respect to the classification loss. They also show that HyperGAN improves the performance of fully supervised learning in MNIST and CIFAR-10 datasets. ","This paper proposes HyperGAN, a generative network for learning weight parameters in deep neural networks. HyperGAN can be seen as an extension of the generative adversarial networks, where the classification loss is based on a rich distribution of effective parameters. The authors show that HyperGAN is able to learn the latent space with low dimensional noise. They also show that the generated network parameter distribution and the unknown true parameter distribution have KL-divergence, which is a measure of the difference between the true and the generated parameters. They compare HyperGAN to fully supervised learning on MNIST and CIFAR-10 datasets, and show that it outperforms the ensembles in terms of uncertainty for out of distribution data and adversarial examples. "
6836,SP:230b3e008e687e03a8b914084b93fc81609051c0,Variational Auto Encoder ( VAE ) HYPONYM-OF generative latent variable model. generative latent variable model USED-FOR representation learning. Variational Auto Encoder ( VAE ) USED-FOR representation learning. continuous valued latent variables USED-FOR VAEs. differentiable estimate USED-FOR ELBO. reparametrized sampling USED-FOR differentiable estimate. Stochastic Gradient Descend ( SGD ) USED-FOR it. discrete valued latent variables USED-FOR VAEs. binary or categorically valued latent representations USED-FOR VAEs. differentiable estimator USED-FOR ELBO. importance sampling USED-FOR differentiable estimator. benchmark datasets EVALUATE-FOR VAEs architectures. Bernoulli and Categorically distributed latent representations USED-FOR VAEs architectures. variational auto encoder ( VAE ) HYPONYM-OF generative model. it HYPONYM-OF generative model. It USED-FOR model. VAE USED-FOR tasks. data generation CONJUNCTION data interpolation. data interpolation CONJUNCTION data generation. density estimation CONJUNCTION data generation. data generation CONJUNCTION density estimation. data interpolation CONJUNCTION outlier and anomaly detection. outlier and anomaly detection CONJUNCTION data interpolation. VAE USED-FOR density estimation. outlier and anomaly detection CONJUNCTION clustering. clustering CONJUNCTION outlier and anomaly detection. VAE USED-FOR data generation. VAE USED-FOR outlier and anomaly detection. density estimation HYPONYM-OF tasks. data interpolation HYPONYM-OF tasks. clustering HYPONYM-OF tasks. outlier and anomaly detection HYPONYM-OF tasks. data generation HYPONYM-OF tasks. VAE HYPONYM-OF latent variable model. Generic is approach. Method is VARIATIONAL AUTO ENCODER. OtherScientificTerm is nonlinear dependent elements. ,"This paper proposes a new generative latent variable model called Variational Auto Encoder (VAE) for representation learning. It is a variational auto encoder that can be applied to any generative model, and it can be used for a variety of tasks such as data generation, data interpolation, outlier and anomaly detection, and density estimation. The proposed approach is based on Stochastic Gradient Descend (SGD) and it uses a differentiable estimator for ELBO based on importance sampling. The differentiable estimate is obtained by reparametrized sampling, and the ELBO is then used to train the VAE. Experiments on benchmark datasets show that the proposed VAEs architectures are able to achieve state-of-the-art performance on Bernoulli and Categorically distributed latent representations.",This paper presents Variational Auto Encoder (VAE) which is a generative latent variable model for representation learning. It is a variational auto encoder with nonlinear dependent elements. Stochastic Gradient Descend (SGD) is used to approximate it and reparametrized sampling is used for the differentiable estimate. ELBO is a differentiable estimator based on importance sampling. VAE is evaluated on a variety of benchmark datasets. It outperforms other VAE architectures on Bernoulli and Categorically distributed latent representations. 
6840,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,feed forward neural network COMPARE training approaches. training approaches COMPARE feed forward neural network. robustness EVALUATE-FOR training approaches. robustness EVALUATE-FOR feed forward neural network. mean field description of a Boltzmann machine USED-FOR pre - trained building block. MNIST dataset EVALUATE-FOR method. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial resistance EVALUATE-FOR method. OtherScientificTerm is adversarial attacks. Method is Boltzmann machine. ,This paper studies the problem of robustness of a feed forward neural network against adversarial attacks. The authors propose a new pre-trained building block based on the mean field description of a Boltzmann machine. The proposed method is evaluated on the MNIST dataset and shows that the proposed method can achieve better robustness than existing training approaches. The method also shows better data augmentation and adversarial training.,This paper presents a novel method to improve the robustness of a feed forward neural network against adversarial attacks. The authors propose a pre-trained building block based on the mean field description of a Boltzmann machine. The proposed method is evaluated on the MNIST dataset and shows that the proposed method can achieve better adversarial resistance than existing training approaches. 
6844,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"visible region PART-OF minimal image. deep neural networks ( DNNs ) COMPARE DNNs. DNNs COMPARE deep neural networks ( DNNs ). object location FEATURE-OF DNNs. DNN recognition ability FEATURE-OF natural images. robustness EVALUATE-FOR DNNs. natural images FEATURE-OF DNNs. Material is Minimal images. Metric are human recognition accuracy, and accuracy. OtherScientificTerm are invariance, and adversarial patterns. ",This paper studies the problem of human recognition accuracy in Minimal images. The authors show that deep neural networks (DNNs) perform better than DNNs in terms of object location in the minimal image. They also show that DNN recognition ability on natural images improves the robustness against adversarial patterns. ,"This paper studies the robustness of deep neural networks (DNNs) compared to DNNs in the context of object location in a minimal image. Minimal images are defined as a set of objects in the visible region of the minimal image, and the goal is to improve human recognition accuracy. The authors show that the DNN recognition ability of natural images with respect to natural images does not depend on the object location, but on the invariance to adversarial patterns. The paper also shows that the accuracy is not dependent on the location of the object, but rather on the number of objects. "
6848,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"multi - agent reinforcement learning ( MARL ) USED-FOR optimal collaboration. worker agents HYPONYM-OF self - interested agents. super agent USED-FOR them. super agent USED-FOR optimal coordination. manager HYPONYM-OF super agent. agent modeling CONJUNCTION policy learning. policy learning CONJUNCTION agent modeling. approach USED-FOR multi - agent management problems. environments EVALUATE-FOR approach. Resource Collection and Crafting EVALUATE-FOR approach. Resource Collection and Crafting HYPONYM-OF environments. approach USED-FOR optimal ad - hoc teaming. approach USED-FOR worker agents ’ minds. generalization FEATURE-OF optimal ad - hoc teaming. OtherScientificTerm are policy, and contracts. Generic is agents. Task is ad - hoc worker teaming. ",This paper proposes a multi-agent reinforcement learning (MARL) for optimal collaboration between worker agents and self-interested agents. The super agent is trained to learn optimal coordination between workers and the manager. The agent modeling is combined with policy learning. The proposed approach is evaluated on two environments: Resource Collection and Crafting. The results show that the proposed approach can achieve optimal ad-hoc teaming with high generalization. ,"This paper proposes multi-agent reinforcement learning (MARL) for optimal collaboration between self-interested agents and worker agents. The super agent is a super agent that learns the optimal coordination between them, and the manager is the super agent which learns the policy. The agent modeling is combined with policy learning. The proposed approach is evaluated in two environments, Resource Collection and Crafting, where the agent modeling and policy learning are applied. The approach is shown to improve the generalization of optimal ad-hoc teaming with respect to the agent’s “worker agents’ minds”. "
6852,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"time series USED-FOR recurrent neural network ( RNN ) solutions. asynchronous time series HYPONYM-OF series. unified RNN USED-FOR feature types. RNN framework USED-FOR sequential features. time features USED-FOR cell ’s memory state. sequential level FEATURE-OF time features. Method are RNN cells, and modeling framework. OtherScientificTerm are sparse and dense features, static ( whole sequence level ) features, and encoder output. Task is cell updates. ","This paper studies the problem of recurrent neural network (RNN) solutions with asynchronous time series. The authors propose a unified RNN for feature types, where sparse and dense features are represented by static (whole sequence level) features, and the encoder output is a sequence of time features at the sequential level. The proposed RNN framework is able to capture sequential features in the cell’s memory state, and can be used to model RNN cells. The experimental results demonstrate the effectiveness of the proposed modeling framework. ","This paper proposes a new model for recurrent neural network (RNN) solutions based on asynchronous time series. The authors propose a unified RNN for different feature types, including sparse and dense features, and static (whole sequence level) features. The proposed modeling framework is based on the RNN framework for sequential features at the sequential level, where the cell’s memory state is encoded in time features. This allows for cell updates without cell updates. "
6856,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"Method are neural model, feedforward neural network, and recurrent neural networks. OtherScientificTerm are propositional formula, and propositional atoms. Generic are network, and model. ","This paper studies the problem of learning a neural model with a feedforward neural network. The authors propose a new propositional formula that can be used to learn a neural network with a recurrent neural network, where each node in the network is represented as a set of propositional atoms. The network is trained to predict the next node’s position in the set. The model is then used to train the model. The proposed model is evaluated on a variety of datasets.","This paper proposes a new neural model, a feedforward neural network. The proposed model is based on a propositional formula, where the propositional atoms are represented as a set of atoms, and the network is trained to predict the value of each of these atoms. The authors show that the proposed model outperforms existing recurrent neural networks."
6860,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,random mini - batches USED-FOR Training neural networks. accuracy EVALUATE-FOR network. speed of learning EVALUATE-FOR network. difficulty FEATURE-OF mini - batches. curriculum learning USED-FOR problem. CNNs USED-FOR image recognition. CIFAR-10 CONJUNCTION CIFAR-100 datasets. CIFAR-100 datasets CONJUNCTION CIFAR-10. performance EVALUATE-FOR small and competitive networks. learning speed EVALUATE-FOR small and competitive networks. competitive ” teacher ” network USED-FOR transfer learning. Imagenet database USED-FOR competitive ” teacher ” network. CIFAR-100 datasets EVALUATE-FOR small and competitive networks. CIFAR-10 EVALUATE-FOR small and competitive networks. transfer learning USED-FOR difficulty. approach COMPARE Self - Paced Learning. Self - Paced Learning COMPARE approach. Metric is difficulty measure. OtherScientificTerm is ” teacher ” network. Generic is method. ,"This paper studies the problem of training neural networks on random mini-batches. The authors propose a new difficulty measure, called the “difficulty measure”, which measures the speed of learning of a network with respect to the accuracy of a “teacher” network. The problem is formulated as a curriculum learning problem, where the goal is to train CNNs to perform image recognition. The proposed method is based on the Imagenet database, where a competitive” teacher’ network is trained using transfer learning. The performance of small and competitive networks are evaluated on CIFAR-10 and CIFar-100 datasets, showing that the proposed approach performs better than Self-Paced Learning.","This paper proposes a novel method for training neural networks on random mini-batches. The problem is formulated as curriculum learning, where the goal is to improve the accuracy of the network by reducing the speed of learning. The difficulty measure is based on the difficulty of mini-batch, and the authors propose a “competitive” teacher “network” that is trained on the Imagenet database. The proposed method is evaluated on CIFAR-10 and CIFar-100 datasets, and shows that the proposed approach outperforms Self-Paced Learning in terms of learning speed and performance. The authors also show that transfer learning improves the difficulty for mini-batch learning. "
6864,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"generalization guarantees FEATURE-OF neural networks. pre - activation values FEATURE-OF network. Method are overparameterized deep networks, stochastic gradient descent ( SGD ), and PAC - Bayesian framework. OtherScientificTerm are small random noise, weight matrices, wide training loss minimum, and wide test loss minimum. Generic are approach, framework, matrices, and prior approaches. Metric is generalization guarantee. ","This paper studies the generalization guarantees of neural networks with overparameterized deep networks with pre-activation values. In particular, the authors consider stochastic gradient descent (SGD) with small random noise. The authors propose a PAC-Bayesian framework with a wide training loss minimum and a wide test loss minimum. The proposed approach is based on the assumption that the weight matrices of the network are the same for all matrices, and that the weights of the matrices are different for different prior approaches. The generalization guarantee of the proposed framework is shown to be tight.","This paper proposes a generalization guarantee for overparameterized deep networks with stochastic gradient descent (SGD). The framework is based on the PAC-Bayesian framework, where the weight matrices of the network are defined as the pre-activation values of a network with small random noise. The generalization guarantees for neural networks with large random noise are obtained by minimizing the wide training loss minimum and the wide test loss minimum. The authors show that the proposed approach is more robust than prior approaches."
6868,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"Deep neural networks USED-FOR symbolic reasoning. Deep neural networks USED-FOR learning abstractions. symbolic reasoning CONJUNCTION learning abstractions. learning abstractions CONJUNCTION symbolic reasoning. discrete latent variables FEATURE-OF Deep neural networks. perplexity EVALUATE-FOR VAE. CIFAR-10 EVALUATE-FOR VAE. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR VAE. training technique USED-FOR VQ - VAE. it CONJUNCTION sequence level knowledge distillation. sequence level knowledge distillation CONJUNCTION it. nonautoregressive machine translation model COMPARE greedy autoregressive baseline Transformer. greedy autoregressive baseline Transformer COMPARE nonautoregressive machine translation model. accuracy EVALUATE-FOR greedy autoregressive baseline Transformer. it USED-FOR nonautoregressive machine translation model. sequence level knowledge distillation USED-FOR nonautoregressive machine translation model. accuracy EVALUATE-FOR nonautoregressive machine translation model. EM USED-FOR discrete autoencoder. Method are discrete latent variable models, vector quantized autoencoders ( VQ - VAE ), and Expectation Maximization ( EM ) algorithm. Task is inference. ","This paper studies the problem of learning abstractions from discrete latent variables in Deep neural networks for symbolic reasoning. The authors propose a new training technique called Expectation Maximization (EM) algorithm for discrete latent variable models, which is based on vector quantized autoencoders (VQ-VAE). Empirical results on CIFAR-10 and VAE datasets show that the perplexity of VAE is reduced by the proposed training technique. Empirically, it outperforms the nonautoregressive machine translation model with sequence level knowledge distillation and the greedy autoregressive baseline Transformer in terms of accuracy. ","The paper proposes a new approach to learning abstractions in Deep neural networks for symbolic reasoning. The authors propose to use discrete latent variable models to learn representations of discrete latent variables. They use vector quantized autoencoders (VQ-VAE), which is an extension of the Expectation Maximization (EM) algorithm. They show that VAE is able to achieve better perplexity than the greedy autoregressive baseline Transformer in terms of accuracy on CIFAR-10. They also propose a training technique to improve the performance of VAE by using a discrete autoencoder with EM. Finally, they propose a nonautoregressive machine translation model based on sequence level knowledge distillation."
6872,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"Multi - view learning USED-FOR self - supervision. Distributional hypothesis USED-FOR self - supervision. multi - view frameworks USED-FOR sentence representations. learning architectures USED-FOR sentence meaning. generative objective CONJUNCTION discriminative one. discriminative one CONJUNCTION generative objective. generative objective USED-FOR framework. multi - view frameworks COMPARE single - view learnt counterparts. single - view learnt counterparts COMPARE multi - view frameworks. multi - view frameworks USED-FOR representations. single - view learnt counterparts USED-FOR representations. Material is large unlabelled corpora. Generic are frameworks, and representation. Method are Recurrent Neural Network ( RNN ), and linear model. Task is downstream tasks. ",This paper studies the problem of multi-view learning for self-supervision in large unlabelled corpora. The authors propose a new framework based on Distributional hypothesis to learn sentence representations from multi-views. The framework is based on a generative objective and a discriminative one. The learning architectures are designed to learn the sentence meaning from multiple views. The proposed frameworks are based on Recurrent Neural Network (RNN). The authors show that the representations learned by the multi- view frameworks are better than the single-view learnt counterparts for downstream tasks. ,"This paper proposes a framework for multi-view learning for self-supervision based on Distributional hypothesis. The framework is based on a generative objective and a discriminative one. The authors propose two learning architectures for learning sentence meaning. The first one is a Recurrent Neural Network (RNN), which is a linear model. The second one is large unlabelled corpora. The two frameworks are evaluated on two downstream tasks. The results show that the proposed representations are better than single-view learnt counterparts for learning representations."
6876,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"Distributed optimization USED-FOR large - scale machine learning problems. stragglers HYPONYM-OF slow nodes. Anytime Minibatch HYPONYM-OF online distributed optimization method. fixed communication time FEATURE-OF minibatch gradients. consensus USED-FOR minibatch gradients. dual averaging USED-FOR primal variables. approach COMPARE it. it COMPARE approach. Amazon EC2 EVALUATE-FOR approach. Method are distributed optimization techniques, and convergence analysis. OtherScientificTerm are gradients, and compute node performance. Metric is wall time. ","This paper studies distributed optimization for large-scale machine learning problems with slow nodes. The authors propose a new online distributed optimization method called Anytime Minibatch, which is based on the idea of stragglers. The main idea is to use a consensus to compute minibatch gradients with fixed communication time in the form of a weighted average of the gradients of the slow nodes, and then use dual averaging to compute primal variables. The proposed approach is evaluated on Amazon EC2 and shows that it outperforms existing distributed optimization techniques in terms of convergence analysis.","This paper proposes a new online distributed optimization method, Anytime Minibatch, for large-scale machine learning problems with stragglers and slow nodes. The main idea is to use consensus to compute minibatch gradients with fixed communication time. The authors show that the proposed approach outperforms it on Amazon EC2. The paper also provides a convergence analysis. "
6880,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,"approach USED-FOR reinforcement learning. task - independent intrinsic reward function USED-FOR approach. peripheral pulse measurements USED-FOR task - independent intrinsic reward function. reward functions USED-FOR sparse and skewed rewards. reward functions USED-FOR reinforcement learning settings. reward functions USED-FOR sample efficiency. sparse and skewed rewards FEATURE-OF reinforcement learning settings. it USED-FOR learning. simulated driving environment EVALUATE-FOR this. OtherScientificTerm are intrinsic feedback, Physiological changes, biological preparations, and human autonomic nervous system responses. Task is learning stage. ","This paper proposes a novel approach for reinforcement learning with a task-independent intrinsic reward function based on peripheral pulse measurements. The reward functions can be used to improve sample efficiency in both sparse and skewed rewards in reinforcement learning settings with intrinsic feedback. Physiological changes can be induced by biological preparations, and human autonomic nervous system responses can be altered during the learning stage. Experiments on a simulated driving environment demonstrate the effectiveness of this approach.","This paper proposes a novel approach for reinforcement learning with a task-independent intrinsic reward function based on peripheral pulse measurements. The idea is to use intrinsic feedback to guide the learning stage. Physiological changes in the environment (e.g. biological preparations) can be seen as biological preparations, and human autonomic nervous system responses can be viewed as intrinsic feedback. The authors propose to use reward functions for sparse and skewed rewards to improve sample efficiency in reinforcement learning settings. Experiments on a simulated driving environment show that it can improve the sample efficiency of learning in this setting."
6884,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"they USED-FOR predictive distributions. NPs USED-FOR conditional distributions. NPs USED-FOR observed data. attention PART-OF NPs. Method is Neural Processes ( NPs ). OtherScientificTerm are regression functions, and functions. Metric is linear complexity. Task is underfitting. Generic is this. ",This paper studies the problem of underfitting with Neural Processes (NPs) in the context of regression functions. The authors show that NPs can be used to learn predictive distributions for conditional distributions. NPs are trained with attention in the presence of observed data. The main contribution of this paper is to show that the linear complexity of NPs is bounded by the number of functions. ,"This paper proposes a new way to study the problem of underfitting. The main idea is to use Neural Processes (NNPs) to learn predictive distributions. NPs are a special case of regression functions, and they can be used to learn conditional distributions. The authors propose to use attention in NPs to learn the observed data. They show that the linear complexity of NPs is much lower than that of linear complexity in regression functions. They also show that underfitting can be reduced to linear complexity. "
6888,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,credit assignment USED-FOR pre - adaptation behavior. sample - efficiency EVALUATE-FOR metatraining. credit assignment USED-FOR gradient - based Meta - RL. meta - learning algorithm USED-FOR estimating meta - policy gradients. meta - learning algorithm USED-FOR poor credit assignment. algorithm USED-FOR meta - learning. statistical distance FEATURE-OF pre - adaptation and adapted policies. pre - adaptation and adapted policies USED-FOR meta - policy search. approach COMPARE Meta - RL algorithms. Meta - RL algorithms COMPARE approach. approach USED-FOR pre - adaptation policy behavior. wall - clock time CONJUNCTION asymptotic performance. asymptotic performance CONJUNCTION wall - clock time. sample - efficiency CONJUNCTION wall - clock time. wall - clock time CONJUNCTION sample - efficiency. asymptotic performance EVALUATE-FOR Meta - RL algorithms. wall - clock time EVALUATE-FOR Meta - RL algorithms. asymptotic performance EVALUATE-FOR approach. wall - clock time EVALUATE-FOR approach. sample - efficiency EVALUATE-FOR Meta - RL algorithms. sample - efficiency EVALUATE-FOR approach. Task is Credit assignment. Generic is it. Method is task identification strategies. OtherScientificTerm is meta - policy gradients. ,"This paper proposes a new meta-learning algorithm for estimating meta-policy gradients for gradient-based Meta-RL. Credit assignment is a popular method for pre-adaptation behavior in meta-RL, but it is not well-studied in practice. The authors propose a new algorithm to improve the sample-efficiency of meta-learned policies in the presence of poor credit assignment. The proposed approach is shown to improve sample-efficient and asymptotic performance compared to existing Meta- RL algorithms in terms of wall-clock time, sample-efficacy, and performance on different task identification strategies. ","This paper proposes a meta-learning algorithm for estimating meta-policy gradients. Credit assignment is an important problem in gradient-based Meta-RL, but it is not well-studied in the literature. The authors propose an algorithm to improve the sample-efficiency of meta-learned policies for pre-adaptation behavior in the context of metatraining. The key idea is to use pre-augmented and adapted policies for meta-adversarial search. The paper shows that the statistical distance between the two sets of policies can be reduced to the same statistical distance in the case of poor credit assignment. Experiments show that the proposed approach can improve the asymptotic performance and wall-clock time of the proposed meta-RL algorithms compared to standard Meta- RL algorithms. "
6892,SP:be5f2c827605914206f5645087b94a50f59f9214,"classifier USED-FOR satisfiability. NeuroSAT HYPONYM-OF message passing neural network. message passing neural network USED-FOR SAT problems. it COMPARE SAT solvers. SAT solvers COMPARE it. NeuroSAT USED-FOR problems. SAT solvers COMPARE NeuroSAT. NeuroSAT COMPARE SAT solvers. it COMPARE NeuroSAT. NeuroSAT COMPARE it. clique detection CONJUNCTION dominating set. dominating set CONJUNCTION clique detection. graph coloring CONJUNCTION clique detection. clique detection CONJUNCTION graph coloring. it USED-FOR SAT problems. dominating set CONJUNCTION vertex cover problems. vertex cover problems CONJUNCTION dominating set. NeuroSAT USED-FOR distributions. graph coloring HYPONYM-OF SAT problems. clique detection HYPONYM-OF SAT problems. OtherScientificTerm are random SAT problems, and small random graphs. ","This paper proposes NeuroSAT, a message passing neural network for solving SAT problems. The main idea is to learn a classifier for satisfiability in the presence of random SAT problems, and then use it to solve these problems in a more efficient manner than existing SAT solvers. The authors show that it can solve a variety of SAT problems such as graph coloring, clique detection, dominating set, and vertex cover problems. They also show that the distributions of the distributions learned by NeuroSat can be used to learn distributions for small random graphs.","This paper proposes NeuroSAT, a message passing neural network for solving SAT problems, which is a classifier for satisfiability. The authors show that it outperforms other SAT solvers on three problems: graph coloring, clique detection, and vertex cover problems. The paper also shows that it can learn distributions of small random graphs. "
6896,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,policy USED-FOR autonomous driving. imitation learning USED-FOR policy. behavior cloning USED-FOR complex driving scenarios. perception system CONJUNCTION controller. controller CONJUNCTION perception system. perturbations FEATURE-OF expert ’s driving. synthesized data USED-FOR learner. perturbations FEATURE-OF synthesized data. robustness EVALUATE-FOR model. losses USED-FOR imitation loss. causal factors FEATURE-OF model. OtherScientificTerm is collisions. ,This paper proposes a new policy for autonomous driving based on imitation learning. The policy is based on behavior cloning and can be applied to complex driving scenarios. The learner is trained using synthesized data and perturbations from expert’s driving. The model is evaluated on a variety of datasets and shows improved robustness against collisions and causal factors. The proposed imitation loss is also evaluated on synthetic data.,"This paper proposes a new policy for autonomous driving based on imitation learning. The policy is based on behavior cloning, which is an important problem in complex driving scenarios. The key idea is to use synthesized data to train the learner and the controller. The authors show that the perturbations of expert’s driving are correlated with perturbation of the policy. The model is evaluated on a set of synthetic data, and the robustness of the model is measured using causal factors. "
6900,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"language modeling CONJUNCTION content recommendation. content recommendation CONJUNCTION language modeling. machine learning models USED-FOR tasks. content recommendation CONJUNCTION advertising. advertising CONJUNCTION content recommendation. image classification CONJUNCTION language modeling. language modeling CONJUNCTION image classification. data USED-FOR machine learning models. data USED-FOR tasks. advertising HYPONYM-OF tasks. image classification HYPONYM-OF tasks. language modeling HYPONYM-OF tasks. content recommendation HYPONYM-OF tasks. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. dataset USED-FOR model. SVHN EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. Material are text, and images. Method are small proxy model, and large target model. ",This paper proposes a new proxy model for the task of image classification. The authors propose a small proxy model that can be used to train a large target model. The proposed model is trained on CIFAR10 and SVHN. The paper shows that the proposed model performs well on the dataset.,"This paper presents a novel approach to training a small proxy model for a large target model. The authors propose a new dataset, CIFAR10, which contains text, images, and videos. The proposed model is trained on the dataset. Experiments show that the proposed approach outperforms SVHN on CifAR10."
6904,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"formulation of planning USED-FOR it. probabilistic inference problem USED-FOR formulation of planning. probabilistic inference problem USED-FOR it. classical methods USED-FOR control. classical methods USED-FOR inference. Sequential Monte Carlo CONJUNCTION Bayesian smoothing. Bayesian smoothing CONJUNCTION Sequential Monte Carlo. Bayesian smoothing USED-FOR control. classical methods USED-FOR Sequential Monte Carlo. classical methods USED-FOR Bayesian smoothing. inference USED-FOR control. classical methods USED-FOR Sequential Monte Carlo Planning. classical methods USED-FOR algorithm. Sequential Monte Carlo Planning USED-FOR continuous control tasks. Sequential Monte Carlo Planning USED-FOR multimodal policies. Method is sampling methods. OtherScientificTerm are continuous domains, and fixed computational budget. ","This paper studies the problem of planning in continuous domains, and proposes a probabilistic inference problem to solve it. The authors use classical methods for inference and Bayesian smoothing for control, and propose Sequential Monte Carlo Planning for continuous control tasks. The proposed algorithm is based on classical methods, and the authors show that the proposed sampling methods are able to learn multimodal policies with a fixed computational budget.","This paper proposes a probabilistic inference problem for the formulation of planning, and it uses classical methods for control and Bayesian smoothing for inference. The proposed algorithm, Sequential Monte Carlo Planning, can be applied to continuous control tasks, and can be used to learn multimodal policies. The authors also propose sampling methods to reduce the computational budget."
6908,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"Neural networks USED-FOR small adversarial perturbations. Adversarial robustness COMPARE clean accuracy. clean accuracy COMPARE Adversarial robustness. adversarial training HYPONYM-OF robust training method. robustness EVALUATE-FOR adversarial trained model. semantics - preserving transformations FEATURE-OF data distribution. distribution USED-FOR adversarial trained model. clean accuracy CONJUNCTION robust accuracy. robust accuracy CONJUNCTION clean accuracy. clean accuracy EVALUATE-FOR Bayes classifier. robust accuracy EVALUATE-FOR Bayes classifier. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. semantically - identical variants USED-FOR CIFAR10. semantically - identical variants USED-FOR MNIST. robustness accuracies EVALUATE-FOR adversarially trained models. adversarial robustness EVALUATE-FOR neural networks. Generic are models, and them. Metric is clean accuracies. OtherScientificTerm is input data distribution. ","This paper studies the problem of small adversarial perturbations in neural networks. The authors propose a robust training method called adversarial training, which is based on adversarial robustness. The main idea is to train the model on the input data distribution with semantics-preserving transformations, and then train the adversarial trained model on this distribution. The proposed method is evaluated on MNIST and CIFAR10 with semantically-identical variants. The results show that the proposed method improves the robustness accuracies of adversarially trained models. ","This paper proposes a new robust training method called adversarial training, which aims to improve the robustness of neural networks against small adversarial perturbations. Adversarial robustness is compared to clean accuracy and robust accuracy of a Bayes classifier on MNIST and CIFAR10 using semantically-identical variants of the input data distribution. The authors show that the adversarial trained model is robust to semantics-preserving transformations of the data distribution, and that the clean accuracies of the models are better than those of the adversarially trained models."
6912,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,transformation of layer weights COMPARE layer outputs. layer outputs COMPARE transformation of layer weights. normalization technique USED-FOR batch normalization. transformation of layer weights USED-FOR normalization technique. positive and negative weights USED-FOR layer output. SVHN CONJUNCTION ILSVRC 2012 ImageNet. ILSVRC 2012 ImageNet CONJUNCTION SVHN. CIFAR-10/100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-10/100. CIFAR-10/100 HYPONYM-OF benchmarks. ILSVRC 2012 ImageNet HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR method. SVHN HYPONYM-OF benchmarks. Generic is technique. ,"This paper proposes a normalization technique for batch normalization based on the transformation of layer weights compared to layer outputs. The key idea is to use positive and negative weights to normalize the layer output. The proposed technique is evaluated on three benchmarks: SVHN, ILSVRC 2012 ImageNet, and CIFAR-10/100.","This paper proposes a normalization technique for batch normalization. The key idea is to use the transformation of layer weights compared to layer outputs. The layer output is computed using positive and negative weights. The proposed technique is evaluated on three benchmarks: CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet."
6916,SP:8188f15c8521099305aa8664e05f102ee6cea402,Memorization USED-FOR over - parameterized neural networks. Memorization USED-FOR generalization. implicit regularization effect USED-FOR stochastic gradient descent. learning rates FEATURE-OF implicit regularization effect. learning rates FEATURE-OF stochastic gradient descent. loss statistics USED-FOR mislabeled examples. algorithm USED-FOR mislabeled examples. artificial and real - world mislabeled examples FEATURE-OF datasets. datasets EVALUATE-FOR ODD. Method is DATA DENOISING ( ODD ). OtherScientificTerm is computational overhead. ,"This paper studies the problem of generalization in over-parameterized neural networks. The authors propose a new algorithm called DATA DENOISING (ODD), which uses the implicit regularization effect of stochastic gradient descent with learning rates to improve the generalization. They show that the loss statistics for mislabeled examples can be improved by the proposed algorithm. They also show that ODD performs well on a variety of datasets, both artificial and real-world. ","This paper proposes a new method to improve the generalization of over-parameterized neural networks. The main idea is to use Memorization to improve generalization. The authors propose to use the implicit regularization effect of stochastic gradient descent with respect to learning rates. The proposed method is called DATA DENOISING (ODD) and is evaluated on two datasets: artificial and real-world mislabeled examples. The paper shows that the proposed algorithm can improve the performance of ODD on both artificial datasets and on real datasets. In addition, the authors also show that the loss statistics for mislabeling examples can be used to reduce the computational overhead."
6920,SP:fbf023a772013e6eca62f92982aecf857c16a428,Pretrained language models USED-FOR downstream NLP task. analysis framework USED-FOR pretraining and downstream tasks. latent variables FEATURE-OF posterior distribution. latent variable generative model of text USED-FOR pretraining and downstream tasks. head tuning CONJUNCTION prompt tuning. prompt tuning CONJUNCTION head tuning. frozen pretrained model USED-FOR classifier. Hidden Markov Model ( HMM ) CONJUNCTION HMM. HMM CONJUNCTION Hidden Markov Model ( HMM ). latent memory component USED-FOR long - term dependencies. natural language FEATURE-OF long - term dependencies. Hidden Markov Model ( HMM ) HYPONYM-OF generative model. HMM HYPONYM-OF generative model. latent memory component USED-FOR HMM. classification heads USED-FOR downstream task. non - degeneracy conditions FEATURE-OF HMM. memory - augmented HMM COMPARE vanilla HMM. vanilla HMM COMPARE memory - augmented HMM. prompt tuning USED-FOR downstream guarantees. recovery guarantees FEATURE-OF memory - augmented HMM. non - degeneracy conditions FEATURE-OF downstream guarantees. long - term memory USED-FOR task - relevant information. HMMs USED-FOR synthetically generated data. Generic is models. Task is downstream classifier. ,"This paper proposes a new analysis framework for pretraining and downstream tasks using a latent variable generative model of text. The authors propose to use pretrained language models to train a downstream NLP task using a frozen pretrained model to train the classifier. The latent variables of the posterior distribution are modeled as latent variables in the latent variables generated by the latent model of the text, and the latent memory component is used to capture the long-term dependencies between the latent variable and the natural language. The HMM is trained with a memory-augmented HMM with non-degeneracy conditions and prompt tuning to improve the downstream guarantees. Experiments on synthetically generated data show that the proposed HMMs perform better than vanilla HMM. ","This paper proposes a new analysis framework for pretraining and downstream tasks using a latent variable generative model of text. The authors propose two models: Hidden Markov Model (HMM) and HMM with a latent memory component to capture long-term dependencies in natural language. The HMM has non-degeneracy conditions, and the memory-augmented HMM is able to achieve better recovery guarantees than vanilla HMM in terms of head tuning and prompt tuning. The proposed models are evaluated on synthetically generated data and on a downstream classifier trained with HMMs. "
6936,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,invariant features USED-FOR classifier. transferability USED-FOR domain generalization. total variation CONJUNCTION Wasserstein distance. Wasserstein distance CONJUNCTION total variation. algorithms USED-FOR domain generalization. feature embeddings USED-FOR domain generalization. transferability FEATURE-OF feature embeddings. algorithms USED-FOR feature embeddings. algorithms USED-FOR transferability. RotatedMNIST CONJUNCTION PACS. PACS CONJUNCTION RotatedMNIST. PACS CONJUNCTION Office - Home. Office - Home CONJUNCTION PACS. algorithm USED-FOR transferable features. benchmark datasets EVALUATE-FOR it. RotatedMNIST HYPONYM-OF benchmark datasets. Office - Home HYPONYM-OF benchmark datasets. PACS HYPONYM-OF benchmark datasets. algorithm COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithm. Task is Out - of - distribution generalization. Generic is model. OtherScientificTerm is transferable ” features. ,"This paper studies the problem of Out-of-distribution generalization in the presence of invariant features in a classifier. The authors propose two algorithms to improve the transferability of feature embeddings for domain generalization by improving the total variation and Wasserstein distance of the model. The proposed algorithm is evaluated on three benchmark datasets: RotatedMNIST, PACS and Office-Home. The experimental results show that the proposed algorithm achieves better transferable features than state-of the-art algorithms. ","This paper proposes a new model for Out-of-distribution generalization based on invariant features for classifier. The key idea is to use “transferable” features to improve the transferability of feature embeddings for domain generalization. The authors propose two algorithms for improving transferability for domain specific applications. The proposed algorithm is evaluated on three benchmark datasets: RotatedMNIST, PACS, and Office-Home. The algorithm is shown to improve transferable features in terms of total variation and Wasserstein distance. "
6952,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"Reward USED-FOR reinforcement - learning agents. partial ordering CONJUNCTION partial ordering. partial ordering CONJUNCTION partial ordering. reward USED-FOR tasks. polynomial - time algorithms USED-FOR Markov reward function. OtherScientificTerm are acceptable behaviors, and reward function. ","This paper proposes a new reward for reinforcement-learning agents. The proposed Markov reward function is based on polynomial-time algorithms. The authors show that the proposed reward can be applied to a variety of tasks, including partial ordering, partial ordering with partial ordering and partial ordering without partial ordering. The paper also shows that the reward function can be used to learn acceptable behaviors.","This paper proposes a new reward function for reinforcement-learning agents. The Markov reward function is based on polynomial-time algorithms. The authors show that the proposed reward function can be applied to a variety of tasks, including partial ordering, partial ordering and partial ordering. They also show that it can be used to learn acceptable behaviors."
6968,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"approaches USED-FOR generalization. sequential structure FEATURE-OF RL problem. approaches USED-FOR RL problem. approaches USED-FOR sequential structure. implicit partial observability USED-FOR generalization. generalization USED-FOR RL. epistemic POMDP HYPONYM-OF induced partially observed Markov decision process. induced partially observed Markov decision process USED-FOR generalization. ensemble - based technique USED-FOR partially observed problem. algorithm COMPARE methods. methods COMPARE algorithm. generalization EVALUATE-FOR methods. epistemic POMDP USED-FOR algorithm. generalization EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR methods. Task is Generalization. Method are reinforcement learning ( RL ) systems, supervised learning, supervised learning methods, and POMDPs. OtherScientificTerm are epistemic uncertainty, fullyobserved MDPs, failure modes of algorithms, and partial observability. ","This paper studies the problem of generalization in reinforcement learning (RL) systems. The authors consider the sequential structure of the RL problem with sequential structure and propose two approaches for generalization based on implicit partial observability. The first approach, epistemic POMDP, is based on the induced partially observed Markov decision process. The second approach, ensemble-based technique, considers the partially observed problem as an ensemble of fullyobserved MDPs with epistemic uncertainty. The proposed algorithm is evaluated on the Procgen benchmark suite and shows that the proposed algorithm achieves better generalization than existing methods. ","This paper proposes a novel approach to improve generalization in reinforcement learning (RL) systems. The authors propose an ensemble-based technique to solve the partially observed problem with sequential structure in the RL problem. The proposed approach is based on the epistemic POMDP, which is an induced partially observed Markov decision process. The generalization of the proposed algorithm is evaluated on the Procgen benchmark suite and compared to other methods. The algorithm is shown to outperform other methods in terms of generalization. "
6984,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,Hessian matrix of value functions USED-FOR Model - agnostic meta - reinforcement learning. framework USED-FOR higherorder derivatives of value functions. off - policy evaluation USED-FOR framework. framework USED-FOR prior approaches. framework USED-FOR estimates. auto - differentiation libraries USED-FOR estimates. OtherScientificTerm is biased Hessian estimates. ,"This paper proposes a new Hessian matrix of value functions for Model-agnostic meta-reinforcement learning. The framework is based on off-policy evaluation, and is able to learn higherorder derivatives of the value functions. The authors show that the proposed framework outperforms prior approaches in terms of biased Hessian estimates. The proposed framework is also able to obtain better estimates from auto-differentiation libraries.",This paper proposes a Hessian matrix of value functions for Model-agnostic meta-reinforcement learning. The framework is based on off-policy evaluation. The authors show that the proposed framework is able to estimate higherorder derivatives of value function. The proposed framework outperforms prior approaches in terms of biased Hessian estimates. The paper also shows that the estimates can be obtained from auto-differentiation libraries.
7000,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"approach USED-FOR communication constraints. approach USED-FOR distributed learning problem. communication constraints FEATURE-OF distributed learning problem. central server USED-FOR distributed learning problem. algorithm COMPARE algorithms. algorithms COMPARE algorithm. algorithm USED-FOR bidirectional compression. convergence rate EVALUATE-FOR algorithms. convergence rate EVALUATE-FOR algorithm. MCM HYPONYM-OF algorithm. gradients FEATURE-OF local servers. perturbed models USED-FOR gradients. model compression CONJUNCTION memory mechanism. memory mechanism CONJUNCTION model compression. memory mechanism PART-OF MCM. model compression PART-OF MCM. worker dependent randomized - models CONJUNCTION partial participation. partial participation CONJUNCTION worker dependent randomized - models. Method is downlink compression. OtherScientificTerm are local models, global model, and perturbation. Task is convergence proofs. ","This paper proposes a new approach for communication constraints in the distributed learning problem with a central server. The authors propose a new algorithm called MCM, which is based on the idea of downlink compression, where the local models are perturbed and the global model is updated by perturbation. They show that the proposed algorithm achieves better convergence rate than existing algorithms for bidirectional compression. They also show that MCM can be combined with model compression and a memory mechanism.","This paper proposes a novel approach to address the distributed learning problem with communication constraints. The authors propose a novel algorithm called MCM, which is a variant of bidirectional compression. The main idea is to use a central server as the central server to solve the distribution of gradients of the local models to the global model, and then perturbation of the gradients in the local servers to the perturbed models. The convergence rate of the proposed algorithm is better than other algorithms in terms of convergence rate. The algorithm is based on model compression, memory mechanism, and worker dependent randomized-models. "
7016,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"causal inference USED-FOR stress testing. counterfactual invariance USED-FOR outof - domain model. causal structure USED-FOR counterfactual invariance. regularization schemes USED-FOR counterfactual invariance. regularization schemes USED-FOR causal structures. causal structure FEATURE-OF domain shift guarantees. domain shift guarantees FEATURE-OF counterfactual invariance. OtherScientificTerm are spurious correlation, spurious correlations, and counterfactual examples. Method are model, and machine learning. Generic are models, and schemes. Task is text classification. ",This paper studies the problem of causal inference for stress testing. The authors propose a counterfactual invariance for an outof-domain model that is invariant to spurious correlation. They show that the causal structure can be used to improve the domain shift guarantees of the proposed model. They also propose regularization schemes for the causal structures. The experiments show that these schemes can improve the performance of the models.,"This paper proposes a new method for learning counterfactual invariance for out-of-domain models. The idea is to use causal inference for stress testing, where the model is trained with spurious correlation. The authors propose two regularization schemes for learning the causal structures of the model, and show that the proposed schemes are more robust to spurious correlations than existing methods. They also show that these schemes are robust to domain shift guarantees of the causal structure. "
7032,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"limited data USED-FOR GANs. data augmentations CONJUNCTION model regularization. model regularization CONJUNCTION data augmentations. generator USED-FOR real data distribution. APA USED-FOR overfitting. method COMPARE approaches. approaches COMPARE method. generator USED-FOR APA. model regularization USED-FOR approaches. data augmentations USED-FOR approaches. APA USED-FOR low - data regime. low - data regime FEATURE-OF synthesis quality. synthesis quality EVALUATE-FOR APA. It USED-FOR GANs. StyleGAN2 HYPONYM-OF GANs. Method are Generative adversarial networks ( GANs ), and Adaptive Pseudo Augmentation ( APA ). Material are high - fidelity images, and generated images. OtherScientificTerm are discriminator overfitting, and discriminator. Generic are strategy, and training strategy. ","This paper studies the problem of training GANs with limited data. The authors propose Adaptive Pseudo Augmentation (APA), a new method to improve the synthesis quality of the GAN. APA is based on the idea that the discriminator overfits the generated images, which is a common problem in GAN training. The main contribution of the paper is to show that APA can avoid overfitting in the low-data regime by using a generator to generate the real data distribution instead of a model regularization. The proposed strategy is evaluated on StyleGAN2 and StyleGAN3 and shows that the proposed method outperforms existing approaches.","This paper proposes Adaptive Pseudo Augmentation (APA), a method to improve the synthesis quality of Generative adversarial networks (GANs) with limited data. APA uses a generator to generate high-fidelity images, and a discriminator overfitting to the generated images. The method is evaluated on StyleGAN2, where it outperforms other approaches that use data augmentations or model regularization. It is shown that APA is able to reduce overfitting in the low-data regime. The paper also shows that the proposed strategy can be applied to other GANs. "
7048,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,Causal inference CONJUNCTION discovery. discovery CONJUNCTION Causal inference. observational data USED-FOR discovery. observational data USED-FOR Causal inference. formalization USED-FOR causal inference. Rubin ’s framework USED-FOR multivariate point processes. average treatment effect ( ATE ) CONJUNCTION propensity scores. propensity scores CONJUNCTION average treatment effect ( ATE ). Rubin ’s framework USED-FOR average treatment effect ( ATE ). Rubin ’s framework USED-FOR propensity scores. multivariate recurrent event streams USED-FOR causal inference. multivariate point process USED-FOR data. joint probability distribution USED-FOR i.i.d. data. joint probability distribution COMPARE multivariate point process. multivariate point process COMPARE joint probability distribution. causal inference framework COMPARE baseline pairwise causal association scores. baseline pairwise causal association scores COMPARE causal inference framework. synthetic and real - world event datasets EVALUATE-FOR causal inference framework. Method is point process causal framework. Generic is measure. ,"This paper proposes a point process causal framework for multivariate point processes. The authors propose a formalization for causal inference using observational data from observational data for Causal inference and discovery. The proposed “Rubin’s framework” is based on the “average treatment effect (ATE) and propensity scores, which are learned using the ‘Rubin-based’ framework. The main idea is to use multivariate recurrent event streams to perform causal inference on data from multivariate points. The joint probability distribution of the data is then used to learn the i.i.d. data. The experimental results show that the proposed causal inference framework outperforms baseline pairwise causal association scores on both synthetic and real-world event datasets.","This paper proposes a point process causal framework that combines observational data for Causal inference and discovery with formalization for causal inference. The authors propose a “Rubin’s framework” for learning multivariate point processes, which is based on the average treatment effect (ATE) and propensity scores. The proposed framework is evaluated on synthetic and real-world event datasets. The main contribution of the paper is a new measure, i.e., the joint probability distribution for i.i.d. data. The paper shows that the proposed causal inference framework is better than the baseline pairwise causal association scores. "
7064,SP:5db39fbba518e24a22b99c8256491295048ec417,Graph neural networks ( GNNs ) USED-FOR graph representation learning. residual connections PART-OF message passing. they USED-FOR GNNs. message passing USED-FOR GNNs. abnormal node features FEATURE-OF GNNs. node features PART-OF graphs. GNNs USED-FOR abnormal features. resilience FEATURE-OF GNNs. AirGNN1 HYPONYM-OF GNN. Adaptive residual USED-FOR GNN. Task is real - world applications. OtherScientificTerm is abnormal feature scenarios. Generic is algorithm. ,"This paper studies graph neural networks (GNNs) for graph representation learning in real-world applications. The authors consider the problem of message passing in GNNs with residual connections in the form of residual connections between nodes in the graph. They show that they can be used as a way to improve the resilience of the GNN to abnormal node features in graphs. They then propose a new GNN called AirGNN1, which is a GNN with Adaptive residual. The algorithm is well-written and easy to follow. ","Graph neural networks (GNNs) are an important tool for graph representation learning. However, they are not well-studied in real-world applications. This paper proposes a novel algorithm called AirGNN1, which uses residual connections in message passing to improve the performance of GNNs in abnormal feature scenarios. The authors show that abnormal node features in GNN can be caused by node features of graphs. The paper also shows that the resilience of the GNN to these abnormal features can be improved by Adaptive residual."
7080,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"Thompson sampling policy PART-OF Bayesian ‘ optimistic ’ policies. algorithm USED-FOR policies. optimistic set FEATURE-OF policies. zero - sum matrix games CONJUNCTION constrained bandits. constrained bandits CONJUNCTION zero - sum matrix games. regret analysis USED-FOR bilinear saddle - point problems. regret analysis USED-FOR optimistic policies. zero - sum matrix games HYPONYM-OF bilinear saddle - point problems. Thompson sampling USED-FOR policies. policy USED-FOR convex optimization problem. optimistic set FEATURE-OF policy. log - concavity CONJUNCTION unimodality. unimodality CONJUNCTION log - concavity. unimodality CONJUNCTION smoothness. smoothness CONJUNCTION unimodality. procedure USED-FOR posteriors. regularization CONJUNCTION constraints. constraints CONJUNCTION regularization. Task are online sequential decision problems, and stochastic multi - armed bandit case. Metric is Bayesian regret. Generic are problem, and it. OtherScientificTerm are linear regret, posterior, and exploration - exploitation tradeoff. Method is variational Bayesian optimistic sampling ’ ( VBOS ). ","This paper studies the problem of online sequential decision problems. The authors propose a Thompson sampling policy for Bayesian ‘optimistic’ policies. The algorithm is based on the Thompson sampling algorithm for policies with an optimistic set. The regret analysis for optimistic policies in bilinear saddle-point problems such as zero-sum matrix games and constrained bandits is provided. The policy is then used to solve a convex optimization problem with log-concavity, unimodality, and smoothness. The paper also provides a procedure for learning posteriors for the stochastic multi-armed bandit case. ","This paper proposes a variational Bayesian optimistic sampling (VBOS) algorithm for online sequential decision problems. The algorithm is based on the Thompson sampling policy in Bayesian‘optimistic’ policies. The regret analysis for bilinear saddle-point problems, such as zero-sum matrix games and constrained bandits, is provided. The authors show that the optimistic set of policies with Thompson sampling can be approximated by a convex optimization problem with Bayesian regret. The paper also shows that the posterior of the policy can be estimated by the exploration-exploitation tradeoff. Finally, the authors propose a procedure to learn posteriors with regularization and constraints. The proposed procedure is evaluated on the stochastic multi-armed bandit case and it is shown that the proposed policy can approximate the optimal optimal policy with the Thompson Sampling policy. "
7096,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"without - replacement sampling orders COMPARE uniform - iid - sampling. uniform - iid - sampling COMPARE without - replacement sampling orders. Without - replacement sampling USED-FOR SGD without variance reduction. convergence analysis CONJUNCTION rates of variance reduction. rates of variance reduction CONJUNCTION convergence analysis. without - replacement sampling orders USED-FOR composite finite - sum minimization. rates of variance reduction EVALUATE-FOR without - replacement sampling orders. random reshuffling CONJUNCTION cyclic sampling. cyclic sampling CONJUNCTION random reshuffling. Prox - DFinito HYPONYM-OF Finito. random reshuffling USED-FOR convergence rates. rates EVALUATE-FOR full - batch gradient descent. variance - reduction USED-FOR without - replacement sampling. cyclic order USED-FOR cyclic sampling. variance reduction USED-FOR uniform - iid - sampling. Prox - DFinito USED-FOR data - heterogeneous scenario. optimal cyclic sampling USED-FOR Prox - DFinito. sample - size - independent convergence rate EVALUATE-FOR Prox - DFinito. method USED-FOR optimal cyclic ordering. Method is stochastic algorithm. OtherScientificTerm are convex and strongly convex scenarios, and optimal fixed ordering. ","This paper proposes a new stochastic algorithm, Prox-DFinito, which is an extension of Finito to the convex and strongly convex scenarios. Without-replacement sampling orders are used in SGD without variance reduction, and the convergence analysis and rates of variance reduction are shown to be better than uniform-iid-sampling. The authors also show that without-replacing sampling orders can be used for composite finite-sum minimization. The convergence rates for full-batch gradient descent with random reshuffling and cyclic sampling with cyclic order are also shown to converge faster than the optimal fixed ordering.    The paper also shows that the proposed method can achieve optimal cyclic ordering in the data-heterogeneous scenario with sample-size-independent convergence rate. ","This paper proposes a stochastic algorithm for convex and strongly convex scenarios. Without-replacement sampling orders outperform uniform-iid-sampling and rates of variance reduction for SGD without variance reduction in convergence analysis and for composite finite-sum minimization under random reshuffling and cyclic sampling. Prox-DFinito is a variant of Finito, where the authors show that the convergence rates of the proposed algorithm are better than the rates of full-batch gradient descent. The authors propose a method for optimal cyclic ordering in the data-heterogeneous scenario, and show that optimal fixed ordering can be achieved with sample-size-independent convergence rate. "
7112,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"algorithmic components USED-FOR reinforcement learning ( RL ) algorithms. relative entropy policy search ( REPS ) USED-FOR policy learning. simulated and real - world robotic domains EVALUATE-FOR policy learning. stochastic, gradient - based solvers USED-FOR REPS. first - order optimization methods USED-FOR REPS objective. sub - optimality FEATURE-OF policy. convergence rates FEATURE-OF sub - optimality. convergence rates FEATURE-OF policy. first - order optimization methods USED-FOR policy. technique USED-FOR parameter updates. generative access USED-FOR parameter updates. generative access USED-FOR Markov decision process. favorable convergence FEATURE-OF parameter updates. Markov decision process USED-FOR technique. generative access USED-FOR technique. OtherScientificTerm are exact gradients, near - optimality, stochastic gradients, and optimal regularized policy. ","This paper studies the relative entropy policy search (REPS) for policy learning in the context of reinforcement learning (RL) algorithms with algorithmic components. The REPS objective is based on stochastic, gradient-based solvers with exact gradients. The authors show that the sub-optimality of the policy is bounded by the convergence rates of the optimal regularized policy. The technique uses a Markov decision process with generative access to perform parameter updates with favorable convergence. ","This paper introduces relative entropy policy search (REPS) for policy learning in the context of reinforcement learning (RL) algorithms. The REPS objective is based on stochastic, gradient-based solvers. The authors show that the sub-optimality of the policy is a function of the convergence rates of the exact gradients of the optimal gradients. The paper also shows that the optimal regularized policy converges to a near-optimal optimality. The technique uses a Markov decision process with generative access for parameter updates and favorable convergence. Experiments are conducted on simulated and real-world robotic domains."
7128,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,deep neural networks ( DNNs ) USED-FOR 3D point cloud processing. knowledge representations USED-FOR 3D point cloud processing. deep neural networks ( DNNs ) USED-FOR knowledge representations. translation CONJUNCTION scale. scale CONJUNCTION translation. scale CONJUNCTION local 3D structures. local 3D structures CONJUNCTION scale. rotation CONJUNCTION translation. translation CONJUNCTION rotation. representation complexity EVALUATE-FOR DNN. metrics CONJUNCTION representation complexity. representation complexity CONJUNCTION metrics. metrics USED-FOR spatial smoothness. metrics EVALUATE-FOR DNN. spatial smoothness FEATURE-OF encoding 3D structures. DNNs USED-FOR representation problems. Generic is method. Method is adversarial training. ,This paper proposes a new method for training deep neural networks (DNNs) for 3D point cloud processing with knowledge representations. The proposed method is based on adversarial training. The authors show that DNNs are able to achieve state-of-the-art spatial smoothness in encoding 3D structures and representation problems. They also show that the representation complexity of a DNN with the proposed metrics improves the performance of a standard DNN. ,"This paper proposes a method for learning knowledge representations for 3D point cloud processing using deep neural networks (DNNs). The proposed method is based on adversarial training. The key idea is to use DNNs to solve representation problems in the context of encoding 3D structures with respect to spatial smoothness, rotation, translation, and scale. The proposed metrics are used to measure the representation complexity of a DNN, and show that the proposed metrics can be used to evaluate the spatial smoothing of a 3D structure."
7144,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"economics CONJUNCTION game theory. game theory CONJUNCTION economics. game theory CONJUNCTION computer science. computer science CONJUNCTION game theory. economics USED-FOR design of optimal auctions. methods USED-FOR approximating optimal auctions. deep learning USED-FOR approximating optimal auctions. deep learning USED-FOR methods. allocation fairness CONJUNCTION diversity. diversity CONJUNCTION allocation fairness. auction mechanisms USED-FOR socially desirable constraints. diversity HYPONYM-OF socially desirable constraints. allocation fairness HYPONYM-OF socially desirable constraints. neural - network - based auction mechanisms USED-FOR constraints. neural - network - based auction mechanisms USED-FOR PreferenceNet. method COMPARE neural - network based auction designs. neural - network based auction designs COMPARE method. metric EVALUATE-FOR method. metric USED-FOR auction allocations. socially desirable constraints FEATURE-OF auction allocations. human subject research USED-FOR approach. Method is strategyproof, revenuemaximizing auction designs. OtherScientificTerm are restricted settings, optimal auctions, and real human preferences. Generic are baselines, and they. Metric is maximizing revenue. ","This paper studies the problem of strategyproof, revenuemaximizing auction designs. The authors consider the design of optimal auctions in the context of economics, game theory, and computer science. Previous methods for approximating optimal auctions are based on deep learning, and the authors propose two new methods based on neural-network-based auction mechanisms to solve the socially desirable constraints (i.e., allocation fairness and diversity) in the restricted settings. The proposed method, PreferenceNet, is evaluated on a variety of benchmark datasets and shows that it outperforms the state-of-the-art in terms of maximizing revenue. ","This paper proposes a new metric for measuring the social desirable constraints of auction allocations in the context of economics and game theory. The proposed metric is based on the strategyproof, revenuemaximizing auction designs. The authors propose two methods for approximating optimal auctions based on deep learning. The first method, PreferenceNet, uses neural-network-based auction mechanisms to estimate the constraints of the optimal auctions. The second method, Strategyproof, uses human subject research to identify the socially desirable constraints on the auction allocations, including allocation fairness, diversity, and diversity in the restricted settings. The method is shown to outperform other neural-net based auction designs in terms of maximizing revenue. "
7160,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"user - level differential privacy USED-FOR personalization of supervised learning. user - level privacy guarantees EVALUATE-FOR approach. non - private approaches USED-FOR algorithms. nearly optimal estimation error guarantees FEATURE-OF algorithms. exponential mechanism - based algorithm USED-FOR information - theoretic upper bound. OtherScientificTerm are shared structure, and joint, user - level differential privacy. Task is linear regression problems. ","This paper studies the problem of user-level differential privacy for personalization of supervised learning. The authors propose a new approach based on the joint, user-specific differential privacy guarantees. The proposed algorithms are based on non-private approaches and have nearly optimal estimation error guarantees for algorithms with shared structure. The information-theoretic upper bound is obtained by an exponential mechanism-based algorithm. Experiments on linear regression problems demonstrate the effectiveness of the proposed approach.","This paper proposes a new approach to personalization of supervised learning with user-level differential privacy. The idea is to use a shared structure between the user and the teacher, and to use joint, user-levels differential privacy guarantees. The proposed algorithms are based on non-private approaches, and provide nearly optimal estimation error guarantees for the algorithms. The information-theoretic upper bound is derived by an exponential mechanism-based algorithm. Experiments are conducted on linear regression problems."
7176,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"human - adversarial examples USED-FOR them. examples EVALUATE-FOR state - of - the - art models. Material are Visual Question Answering dataset ( VQA v2 ), adversarial examples, and Adversarial VQA ( AdVQA ) benchmark. Metric is human accuracy. Method are VQA models, and VQA model. Generic is model. ","This paper proposes a Visual Question Answering dataset (VQA v2), which contains human-adversarial examples that can be used to train VQA models. The authors show that these examples can improve the performance of state-of-the-art models by improving the human accuracy. The paper also provides a new Adversarial VQLA (AdVQLA) benchmark. ","The paper proposes a new benchmark for evaluating the human-adversarial VQA (AdVQA) benchmark. The paper proposes to use the Visual Question Answering dataset (VIQA v2) as a baseline for evaluating adversarial examples. The main idea of the paper is to evaluate the human accuracy of the proposed model on a set of state-of-the-art models. The proposed model is evaluated on a small set of examples, and the results show that it outperforms the state of the art models on these examples. "
7192,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"Medial entorhinal cortex ( MEC ) USED-FOR navigational and memory related behaviors. they USED-FOR MEC functionality. they USED-FOR behavior. response profiles FEATURE-OF heterogeneous ” cells. models USED-FOR response profiles. response profiles FEATURE-OF stereotypical and heterogeneous MEC cells. task - optimized neural network models COMPARE grid cell - centric models. grid cell - centric models COMPARE task - optimized neural network models. grid cell - centric models USED-FOR MEC neuronal response profiles. task - optimized neural network models USED-FOR MEC neuronal response profiles. gated nonlinearities CONJUNCTION intermediate place cell representation. intermediate place cell representation CONJUNCTION gated nonlinearities. intermediate place cell representation HYPONYM-OF network architecture. gated nonlinearities HYPONYM-OF network architecture. heterogeneous cells COMPARE grid and border cells. grid and border cells COMPARE heterogeneous cells. heterogeneous cells USED-FOR downstream functional outcomes. path integration HYPONYM-OF downstream functional outcomes. spatial response selectivity FEATURE-OF MEC cells. reward - modulated path integration USED-FOR MEC model. non - spatial rewards FEATURE-OF MEC cells. variable - reward conditions FEATURE-OF neural recordings. OtherScientificTerm are MEC, grid, stereotypical response profiles, MEC neurons, stereotypical firing patterns, heterogeneous MEC cells, and response patterns. Method are computational approach, statistical analysis, and goal - driven modeling approach. Generic is model. Task is Neural Information Processing Systems. ","This paper proposes a new neural network architecture, called Medial entorhinal cortex (MEC), for learning navigational and memory related behaviors. The authors propose a new computational approach to model the MEC, which is based on the notion of “heterogeneous” cells, which they call “heterogeneous MEC cells”, where each MEC is composed of a grid of neurons that are heterogeneous in their response patterns. The response profiles of these “homogeneous’ cells are learned by the models, and they are used to learn MEC functionality. The MEC neuronal response profiles are trained by task-optimized neural network models, which are compared to grid cell-centric models on a variety of tasks, and compared to heterogeneous cells on downstream functional outcomes such as path integration, gated nonlinearities, and intermediate place cell representation. The goal-driven modeling approach is shown to improve the performance of the model. ","This paper proposes a new computational approach to study the navigational and memory related behaviors of the Medial entorhinal cortex (MEC) in the context of Neural Information Processing Systems (NIS). The authors propose a goal-driven modeling approach, which is based on statistical analysis. The proposed network architecture consists of gated nonlinearities, an intermediate place cell representation, and heterogeneous “heterogeneous” cells. The response profiles of the “traditional” and “homogenous” MEC cells are modeled by models that are able to predict the response profiles for both the traditional and the heterogeneous MEC cell. The authors show that task-optimized neural network models can predict the MEC neuronal response profiles better than grid cell-centric models. They also show that heterogeneous cells can predict downstream functional outcomes such as path integration, spatial response selectivity, and non-spatial rewards. "
7208,SP:57f9812fa5e7d0c66d412beb035301684d760746,"them USED-FOR physical real - world tasks. KL - regularized reinforcement learning USED-FOR deep reinforcement learning algorithms. sample efficiency EVALUATE-FOR deep reinforcement learning algorithms. KL - regularized reinforcement learning USED-FOR them. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency EVALUATE-FOR KL - regularized reinforcement learning. behavioral reference policies USED-FOR KL - regularized reinforcement learning. expert demonstrations USED-FOR behavioral reference policies. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency CONJUNCTION online policy. online policy CONJUNCTION sample efficiency. KL - regularized reinforcement learning COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR state - of - the - art approaches. non - parametric behavioral reference policies USED-FOR pathology. OtherScientificTerm are pathological training dynamics, and behavioral policy classes. Method is online learning. ","This paper studies the problem of KL-regularized reinforcement learning for physical real-world tasks. The authors show that the sample efficiency of such deep reinforcement learning algorithms can be improved by using expert demonstrations to learn behavioral reference policies that can be used to guide them to perform well on them. They also show that such online learning can improve sample efficiency as well as the online policy. Finally, they show that non-parametric behavioral reference policy can be useful for pathology and can be applied to behavioral policy classes. ",This paper proposes a new method for learning behavioral reference policies for physical real-world tasks. The key idea is to use non-parametric behavior reference policies to model pathological training dynamics. The authors show that they can achieve better sample efficiency than state-of-the-art deep reinforcement learning algorithms in terms of sample efficiency and online policy. They also show that using expert demonstrations can improve the sample efficiency of KL-regularized reinforcement learning for them. They show that online learning can improve sample efficiency on locomotion and dexterous hand manipulation tasks. 
7224,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"teacher - student framework USED-FOR kernel regression. neural tangent kernel PART-OF convolutional architectures. neural tangent kernel USED-FOR convolutional ’ kernels. filter size FEATURE-OF convolutional architectures. teacher - student framework USED-FOR problem. convolutional ’ kernels USED-FOR teacher - student framework. convolutional ’ kernels USED-FOR problem. locality USED-FOR learning curve exponent β. physics USED-FOR heuristic methods. ridge USED-FOR kernel regression. Method is Convolutional neural networks. OtherScientificTerm are ridgeless case, translational invariance, natural universality assumption, and learning curve exponents. ","This paper proposes a teacher-student framework for kernel regression with convolutional neural networks with a neural tangent kernel. The problem is formulated as a ridgeless case where the filter size of the convolutionan architectures has a large filter size, and the goal is to find a solution with translational invariance. The authors propose to solve the problem using convolutiona’ kernels, which can be trained with a teacher -student framework. The learning curve exponent β is defined by the locality of the learning curve exponents. The ridge of the ridge is then used to train the kernel regression. Theoretical results on physics are provided to show that the heuristic methods can converge to the optimal solution. ","This paper proposes a teacher-student framework for kernel regression with convolutional neural networks. The problem is formulated as the problem of learning a neural tangent kernel in convolutionals with filter size larger than the filter size. The authors show that in the ridgeless case, the learning curve exponent β is invariant to the locality, and in the translational invariance case, it is non-invariant. They also show that the natural universality assumption is violated in the case where the learning curves exponents are smaller than the radius of the ridge. They propose two heuristic methods based on physics to solve the problem."
7240,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"Variational Autoencoders ( VAEs ) USED-FOR representations of complex data distributions. Variational Autoencoders ( VAEs ) HYPONYM-OF probabilistic models. probabilistic models USED-FOR representations of complex data distributions. model USED-FOR latent representations. uni - modal Gaussian distribution USED-FOR latent representations. regularized autoencoders USED-FOR deterministic autoencoding framework. ex - post density estimation step USED-FOR they. latent space FEATURE-OF model. deterministic autoencoding framework USED-FOR latent space. expressive multi - modal latent distributions USED-FOR deterministic autoencoding framework. latent distribution USED-FOR encoded data. sample quality EVALUATE-FOR model. continuous and discrete domains EVALUATE-FOR model. Method are VAEs, and variational training procedure. OtherScientificTerm is VAE objective. Generic are models, and training procedure. ","This paper studies the problem of learning representations of complex data distributions using probabilistic models such as Variational Autoencoders (VAEs). The authors propose a deterministic autoencoding framework based on regularized autoencopers with expressive multi-modal latent distributions. The model learns latent representations based on an uni-modular Gaussian distribution, and uses an ex-post density estimation step to train them. The authors show that the model is able to learn the latent space of the latent distribution of the encoded data, and that the sample quality of the model can be evaluated on continuous and discrete domains.  The authors also provide a variational training procedure for the VAE objective. ","This paper introduces Variational Autoencoders (VAEs), probabilistic models for learning representations of complex data distributions. The authors propose a deterministic autoencoding framework based on regularized autoencopers with an ex-post density estimation step. The model learns latent representations based on an uni-modal Gaussian distribution, where the VAE objective is to maximize the sample quality of the encoded data. The models are trained using variational training procedure, where they are trained on expressive multi -modal latent distributions. Experiments on continuous and discrete domains show that the model achieves better sample quality in terms of the latent space of the model."
7256,SP:6232d8738592c9728feddec4462e61903a17d131,adversarial examples USED-FOR Deep learning models. Autoencoder USED-FOR self - supervised ) adversarial detection. benign examples USED-FOR Autoencoder. autoencoder structure USED-FOR disentangled represen8 tations of images. disentangled represen8 tations of images USED-FOR adversarial examples. class features CONJUNCTION semantic features. semantic features CONJUNCTION class features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features USED-FOR autoencoder. paired class / semantic features USED-FOR autoencoder. discriminator network USED-FOR autoencoder. generalization ability FEATURE-OF autoencoder. AUC CONJUNCTION FPR. FPR CONJUNCTION AUC. FPR CONJUNCTION TPR. TPR CONJUNCTION FPR. method COMPARE self - supervised detection methods. self - supervised detection methods COMPARE method. adversarial attacks CONJUNCTION victim models. victim models CONJUNCTION adversarial attacks. method COMPARE it. it COMPARE method. AUC CONJUNCTION TPR. TPR CONJUNCTION AUC. victim models USED-FOR self - supervised detection methods. adversarial attacks USED-FOR method. adversarial attacks FEATURE-OF self - supervised detection methods. AUC HYPONYM-OF measurements. measurements EVALUATE-FOR it. TPR HYPONYM-OF measurements. AUC EVALUATE-FOR it. FPR HYPONYM-OF measurements. AUC EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. Autoencoder - based detectors COMPARE method. method COMPARE Autoencoder - based detectors. method USED-FOR adaptive adversary. Metric is reconstruction error. ,"This paper proposes a new autoencoder for (self-supervised) adversarial detection. The proposed method is based on the Autoencoding framework. The authors propose to use the disentangled represen8 tations of images to generate the adversarial examples, which are then used to train a discriminator network to predict the target class and semantic features of the target image. The model is trained using a combination of the paired class/semantic features and paired class / semantic features. The method is evaluated on CIFAR-10 and ImageNet. The results show that the proposed method outperforms existing self-supervision methods in terms of AUC, FPR, and TPR. ","This paper proposes an Autoencoder for (self-supervised) adversarial detection. The autoencoders are based on the disentangled represen8 tations of images, which are generated by a discriminator network. The authors propose to use benign examples to generate adversarial examples, which is an extension of benign examples that are generated from benign examples.  The authors show that the autoencoders are able to generalize better than self-supervision detection methods in terms of generalization ability. The proposed method is evaluated on CIFAR-10, AUC, FPR, TPR, and AUC. The method is shown to be able to detect an adaptive adversary that is more robust to adversarial attacks than adversarial models.  "
7272,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"it USED-FOR brain activity. embedding space FEATURE-OF syntactic structure. low signal - to - noise ratio FEATURE-OF neuroimaging tools. functional Magnetic Resonance Imaging ( fMRI ) HYPONYM-OF neuroimaging tools. multi - dimensional features USED-FOR syntactic structure. features CONJUNCTION fMRI recordings. fMRI recordings CONJUNCTION features. fMRI recordings USED-FOR brain representation of syntax. fMRI recordings USED-FOR natural text. features USED-FOR brain representation of syntax. syntactic structure - based features USED-FOR brain activity. complexity metrics USED-FOR processing load. language system FEATURE-OF brain activity. OtherScientificTerm are semantics, semantic processing load, semantic representation of the stimulus words, syntactic processing load, and syntactic features. Generic is approaches. Task is syntax. ","This paper proposes a new neuroimaging tool called functional Magnetic Resonance Imaging (fMRI) that is able to capture brain activity in a low signal-to-noise ratio. The authors propose to use multi-dimensional features to represent the syntactic structure in the embedding space, which is then used to encode the semantic processing load of the brain activity. The features are combined with fMRI recordings to capture the brain representation of syntax and natural text. The syntactic processing load is estimated by the complexity metrics of the processing load, and the semantic representation of the stimulus words is computed by the semantic structure-based features. The proposed approaches are evaluated on a variety of tasks, and show that the proposed approaches perform better than the state-of-the-art.","This paper proposes a new neuroimaging tools called functional Magnetic Resonance Imaging (fMRI) that is based on low signal-to-noise ratio. The authors propose to use it to measure brain activity in the embedding space of the syntactic structure of the language system. The syntactic processing load is defined as the semantic processing load, which is the semantic representation of the stimulus words. The main idea is to use multi-dimensional features to measure the brain representation of syntax using fMRI recordings and features from natural text. The proposed approaches are evaluated on a variety of tasks, and the results show that the proposed processing load can be measured using complexity metrics. "
7288,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,deep generative models USED-FOR real - world applications. deep generative models USED-FOR Controllable generation. compositional ability USED-FOR concept combinations. compositional ability FEATURE-OF models. energybased models ( EBMs ) USED-FOR compositional generation. attributes USED-FOR compositional generation. them USED-FOR high - resolution image generation. EBM USED-FOR pre - trained generative model. latent space FEATURE-OF pre - trained generative model. latent space FEATURE-OF EBM. EBM USED-FOR them. EBM USED-FOR high - resolution image generation. StyleGAN HYPONYM-OF pre - trained generative model. EBM formulation USED-FOR joint distribution. it USED-FOR sampling. ordinary differential equation ( ODE ) USED-FOR sampling. pre - trained generator USED-FOR controllable generation. controllable generation USED-FOR attribute classifier. latent space USED-FOR Sampling. ODEs USED-FOR Sampling. conditional sampling CONJUNCTION sequential editing. sequential editing CONJUNCTION conditional sampling. method COMPARE state - of - the - art. state - of - the - art COMPARE method. sequential editing EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR method. sequential editing EVALUATE-FOR method. method USED-FOR attribute combinations. method USED-FOR compositional generation. energy functions CONJUNCTION logical operators. logical operators CONJUNCTION energy functions. compositionality FEATURE-OF generating photo - realistic images. OtherScientificTerm is hyperparameters. Material is photo - realistic images. ,"This paper studies the problem of Controllable generation in deep generative models for real-world applications. The authors propose to use energybased models (EBMs) for compositional generation with compositional ability to generate concept combinations. They use them for high-resolution image generation and use them to pre-train a pre-trained generative model called StyleGAN. The EBM is trained in a latent space of the latent space, where the hyperparameters of the EBM are defined by the joint distribution of the energy functions and logical operators. Sampling is performed using an ordinary differential equation (ODE) and it is used for sampling from the ODEs.  The authors show that the proposed method performs better than state-of-the-art conditional sampling and sequential editing in terms of compositionality in generating photo-realistic images. They also show that their method is able to generate attribute combinations that are controllable to different attributes. ","This paper proposes a method for Controllable generation using deep generative models for real-world applications. The authors propose to use energybased models (EBMs) for compositional generation with compositional ability to generate concept combinations. They use them for high-resolution image generation and use them to train a pre-trained generative model, StyleGAN, in the latent space of an EBM. The EBM formulation is based on the joint distribution of the energy functions and logical operators. The hyperparameters of the EBM are defined as the sum of the two EBMs. Sampling is performed using the ordinary differential equation (ODE) and it is shown that it is equivalent to sampling using ODEs. The proposed method is evaluated on a set of photo-realistic images, where the authors show that the proposed method can achieve better compositionality than state-of-the-art, conditional sampling and sequential editing. The method is also evaluated on attribute combinations generated by the pre-train generator for controllable generation for attribute classifier. "
7304,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,common global parameters USED-FOR K - armed stochastic bandits. geometric structure USED-FOR collaborative algorithm. local feature vectors CONJUNCTION raw data. raw data CONJUNCTION local feature vectors. collaborative algorithm USED-FOR heterogeneity. geometric structure FEATURE-OF linear rewards. Fed - PE USED-FOR heterogeneity. Fed - PE HYPONYM-OF collaborative algorithm. near - optimal regrets FEATURE-OF disjoint and shared parameter cases. logarithmic communication costs FEATURE-OF near - optimal regrets. multi - client G - optimal design USED-FOR Fed - PE. tight minimax regret lower bound USED-FOR disjoint parameter case. collinearly - dependent policies HYPONYM-OF concept. synthetic and real - world datasets EVALUATE-FOR algorithms. Method is federated linear contextual bandits model. ,"This paper studies the problem of learning common global parameters for K-armed stochastic bandits with a federated linear contextual bandits model. The authors propose Fed-PE, a collaborative algorithm based on geometric structure for learning linear rewards with a local feature vectors and raw data. The main contribution of the paper is a multi-client G-optimal design that allows for the use of Fed- PE to overcome heterogeneity in the dataset. The paper also provides a tight minimax regret lower bound for the disjoint parameter case, and shows that the proposed algorithms can be used to achieve near-optimistic regrets with logarithmic communication costs. The theoretical results show that the concept of collinearly-dependent policies can be applied to both synthetic and real-world datasets.","This paper proposes a federated linear contextual bandits model, where the goal is to learn common global parameters for K-armed stochastic bandits. The proposed collaborative algorithm, Fed-PE, is based on the geometric structure of linear rewards with local feature vectors and raw data. The authors propose a multi-client G-optimal design, which is a variant of Fed-PPE. The main contribution of the paper is a tight minimax regret lower bound for the disjoint parameter case, and the authors show that the near-optimistic regrets of the disjunctive and shared parameter cases have logarithmic communication costs. The paper also proposes a new concept called collinearly-dependent policies. The experiments on synthetic and real-world datasets show the effectiveness of the proposed algorithms."
7320,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"model USED-FOR conservative planning. explicit uncertainty quantification USED-FOR incorporating conservatism. explicit uncertainty quantification USED-FOR model - based algorithms. deep neural networks USED-FOR Uncertainty estimation. complex models USED-FOR Uncertainty estimation. deep neural networks HYPONYM-OF complex models. uncertainty estimation USED-FOR offline model - based RL. offline dataset CONJUNCTION data. data CONJUNCTION offline dataset. COMBO USED-FOR value function. model - based offline RL algorithm USED-FOR value function. rollouts USED-FOR data. COMBO HYPONYM-OF model - based offline RL algorithm. data USED-FOR value function. offline dataset USED-FOR value function. policy improvement guarantee FEATURE-OF COMBO. COMBO COMPARE offline RL. offline RL COMPARE COMBO. offline RL USED-FOR problems. COMBO USED-FOR problems. COMBO COMPARE offline RL methods. offline RL methods COMPARE COMBO. generalization FEATURE-OF problems. offline RL benchmarks EVALUATE-FOR offline RL methods. image - based tasks HYPONYM-OF offline RL benchmarks. Method is dynamics model. Material is logged experience. Task are offline reinforcement learning ( offline RL ), and explicit uncertainty estimation. OtherScientificTerm is model rollouts. ","This paper studies the problem of offline reinforcement learning (Offline RL), where the goal is to learn a model that can be used for conservative planning in the presence of logged experience. Uncertainty estimation in complex models such as deep neural networks is an important problem, and explicit uncertainty quantification can be useful for incorporating conservatism in model-based algorithms such as COMBO. The authors propose a model based offline RL algorithm called COMBO, where the value function is learned from the offline dataset and the data, and the model rollouts are used to learn the data. COMBO has a policy improvement guarantee of $O(\sqrt{T})$ and is shown to outperform existing offline RL methods on a range of offline RL benchmarks such as image-based tasks.","This paper proposes a new model for offline reinforcement learning (Offline RL), which is based on explicit uncertainty quantification for incorporating conservatism. Uncertainty estimation is an important problem in complex models such as deep neural networks. The authors propose COMBO, a model-based offline RL algorithm that learns a value function from an offline dataset and a set of rollouts. The value function is learned from the data and the rollouts are used to train a dynamics model. The policy improvement guarantee of COMBO is shown to be better than offline RL methods in terms of generalization to new problems and on image-based tasks. "
7336,SP:ca6f11ed297290e487890660d9a9a088aa106801,"stochastic gradient descent USED-FOR neural networks. deep learning training USED-FOR evolution of features. stochastic differential equations ( SDEs ) USED-FOR evolution of features. backpropagation USED-FOR features. drift term PART-OF SDE. sharp phase transition phenomenon FEATURE-OF intra - class impact. neural collapse of the features HYPONYM-OF geometric structure. local elasticity USED-FOR neural networks. synthesized dataset of geometric shapes CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION synthesized dataset of geometric shapes. Method are deep learning models, modeling strategy, and SDEs. Generic is models. OtherScientificTerm are feature spaces, and vanishing training loss. ","This paper studies the problem of stochastic gradient descent for neural networks with local elasticity. The authors propose a new modeling strategy for deep learning models, where the goal is to learn features with backpropagation. In particular, the authors consider the evolution of features from stochastically differential equations (SDEs) using deep learning training. They show that the drift term in the SDE is a drift term of the intra-class impact, which is a sharp phase transition phenomenon. They also show that SDEs with vanishing training loss have a geometric structure (e.g., neural collapse of the features) similar to the geometric structure in a synthesized dataset of geometric shapes and CIFAR-10. They then show that these models can be trained with a local elasticite.","The paper proposes stochastic gradient descent for neural networks with local elasticity. The main idea is to use deep learning training for the evolution of features in stochastically differential equations (SDEs) with backpropagation. The authors show that the drift term in the SDE is the intra-class impact of the feature spaces, and the sharp phase transition phenomenon is a result of the intra -class impact. The paper proposes a modeling strategy to minimize the vanishing training loss, and shows that the proposed SDEs are more robust to sharp phase transitions than existing models. The geometric structure of the features is shown to be a neural collapse of the neural networks. Experiments are conducted on a synthesized dataset of geometric shapes and CIFAR-10."
7352,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"DRL methods USED-FOR neural network policies. learning programmatic policies USED-FOR generalization. input / output state pairs CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION input / output state pairs. decision trees CONJUNCTION state machines. state machines CONJUNCTION decision trees. state machines CONJUNCTION predefined program templates. predefined program templates CONJUNCTION state machines. expert demonstrations HYPONYM-OF supervision. predefined program templates HYPONYM-OF limited policy representations. input / output state pairs HYPONYM-OF supervision. state machines HYPONYM-OF limited policy representations. limited policy representations USED-FOR works. decision trees HYPONYM-OF limited policy representations. supervision USED-FOR works. framework USED-FOR program. program embedding space USED-FOR program. framework USED-FOR task - solving programs. framework COMPARE DRL and program synthesis baselines. DRL and program synthesis baselines COMPARE framework. methods USED-FOR program embedding. Method are deep reinforcement learning ( DRL ) methods, and two - stage learning scheme. Generic is task. OtherScientificTerm are reward signals, and programs. ","This paper proposes a two-stage learning scheme for deep reinforcement learning (DRL) methods to learn neural network policies. The main idea is to learn programmatic policies for generalization by learning the reward signals in the environment. The authors propose two works: limited policy representations (i.e., decision trees, state machines, and predefined program templates) and supervision (input/output state pairs, expert demonstrations). The framework is able to learn a program in a program embedding space that can be used for task-solving programs. The proposed framework is shown to outperform DRL and program synthesis baselines. ","This paper proposes a framework for learning deep reinforcement learning (DRL) methods for learning neural network policies. The main idea is to use learning programmatic policies for generalization. The framework is based on a two-stage learning scheme, where the first task is to learn a set of reward signals, and the second task is learning a program embedding space for each of the reward signals. The works are based on limited policy representations (input/output state pairs, expert demonstrations, state machines, predefined program templates, and decision trees) and supervision. The proposed framework is evaluated on task-solving programs and outperforms DRL and program synthesis baselines."
7368,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"scientific machine learning USED-FOR physicsinformed neural network ( PINN ) models. machine learning methodologies USED-FOR model. soft constraints FEATURE-OF empirical loss function. soft constraints FEATURE-OF physical domain knowledge. physical domain knowledge USED-FOR approach. PINN methodologies USED-FOR models. they USED-FOR physical phenomena. soft regularization USED-FOR subtle problems. soft regularization USED-FOR PINNs. PDE - based differential operators PART-OF soft regularization. PINN ’s setup USED-FOR loss landscape. solutions USED-FOR failure modes. curriculum regularization USED-FOR PINN ’s loss term. curriculum regularization USED-FOR approach. PDE regularization USED-FOR PINN ’s loss term. approach USED-FOR problem. sequence - to - sequence learning task USED-FOR problem. methods COMPARE regular PINN training. regular PINN training COMPARE methods. Task are physical interest, and learning differential equations. OtherScientificTerm is differential equations. Method are NN architecture, and NN. ",This paper studies the problem of physicsinformed neural network (PINN) models in the context of physical interest. The authors propose a new approach based on physical domain knowledge and soft constraints on the empirical loss function of a PINN with soft constraints. They show that PINNs trained with soft regularization with PDE-based differential operators can achieve state-of-the-art performance on a sequence-to-sequence learning task. They also show that the PINN’s setup can be used to learn the loss landscape of the NN architecture. ,"This paper proposes a new approach for physicsinformed neural network (PINN) models. PINN methodologies are used to train models on physical domain knowledge. The authors propose to use soft constraints on the empirical loss function of the NN architecture. The soft regularization of PINNs is based on PDE-based differential operators. The proposed approach is evaluated on a sequence-to-sequence learning task, and compared to other methods for regular PINN training. Results show that the proposed PINN’s setup improves the loss landscape in terms of learning differential equations. The paper also shows that solutions to failure modes can be learned using the proposed approach."
7384,SP:cfd501bca783590a78305f0592f537e8f20bce27,"domaininvariant representations USED-FOR domain shift. domaininvariant representations USED-FOR unsupervised domain adaptation ( UDA ). self - training USED-FOR UDA. self - training USED-FOR unlabeled target data. Cycle Self - Training ( CST ) HYPONYM-OF self - training algorithm. CST USED-FOR target pseudo - labels. source - trained classifier USED-FOR CST. source - trained classifier USED-FOR target pseudo - labels. shared representations USED-FOR classifier. CST USED-FOR classifier. target pseudo - labels USED-FOR CST. target pseudo - labels USED-FOR classifier. Tsallis entropy USED-FOR confidence - friendly regularization. invariant feature learning CONJUNCTION vanilla self - training. vanilla self - training CONJUNCTION invariant feature learning. CST COMPARE state - of - the - arts. state - of - the - arts COMPARE CST. visual recognition and sentiment analysis benchmarks EVALUATE-FOR state - of - the - arts. visual recognition and sentiment analysis benchmarks EVALUATE-FOR CST. OtherScientificTerm are hardness or impossibility theorems, distributional shift, and pseudo - labels. Generic is forward step. ","This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA). The authors propose to use domaininvariant representations for domain shift in order to avoid hardness or impossibility theorems in the forward step of UDA. To this end, the authors propose self-train on unlabeled target data using self-trained classifier and target pseudo-labels from shared representations. The authors also propose a confidence-friendly regularization based on Tsallis entropy. Experiments on visual recognition and sentiment analysis benchmarks show that CST outperforms state-of-the-arts in terms of invariant feature learning and vanilla self -training.","This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA). The key idea is to use domaininvariant representations to avoid domain shift. The authors show that the hardness or impossibility theorems of the domain shift can be controlled by self-train on unlabeled target data. The proposed method is based on a forward step, where the classifier is trained on shared representations. The classifier uses a source-trained classifier to predict the target pseudo-labels, and then uses a set of shared representations to train a classifier on the pseudo-label. The confidence-friendly regularization is achieved by using Tsallis entropy. Experiments on visual recognition and sentiment analysis benchmarks show that CST outperforms state-of-the-arts in terms of invariant feature learning and vanilla self -training."
7400,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"unsupervised representation learning CONJUNCTION structured network pruning. structured network pruning CONJUNCTION unsupervised representation learning. deep learning USED-FOR compact representations of features. neural network USED-FOR compact representations of features. DiscriminAtive Masking ( DAM ) HYPONYM-OF single - stage structured pruning method. graph representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION graph representation learning. recommendation system CONJUNCTION graph representation learning. graph representation learning CONJUNCTION recommendation system. dimensionality reduction CONJUNCTION recommendation system. recommendation system CONJUNCTION dimensionality reduction. structured pruning USED-FOR image classification. representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION representation learning. applications USED-FOR structured pruning. applications USED-FOR representation learning. applications EVALUATE-FOR DAM approach. graph representation learning HYPONYM-OF applications. dimensionality reduction HYPONYM-OF applications. recommendation system HYPONYM-OF applications. learning objective FEATURE-OF DAM. Generic are state - of - the - art methods, and systematic approach. Method are fine - tuning, and dam - pytorch. OtherScientificTerm is masking layer. ","This paper proposes a single-stage structured pruning method called DiscriminAtive Masking (DAM) that uses deep learning to learn compact representations of features from a neural network. The authors propose a systematic approach to fine-tune the masking layer, and show that DAM achieves better performance than state-of-the-art methods. The main contribution of the paper is a theoretical analysis of the learning objective of DAM, which shows that the DAM approach can achieve better performance in a variety of applications, including representation learning, graph representation learning and structured network pruning. ","This paper proposes a single-stage structured pruning method called DiscriminAtive Masking (DAM) which is based on state-of-the-art methods. The main idea is to use deep learning to learn compact representations of features in a neural network. The authors propose a systematic approach to fine-tune the masking layer, which is called dam-pytorch. The proposed DAM approach is evaluated on a variety of applications for representation learning, graph representation learning and a recommendation system, as well as for image classification. The learning objective of DAM is to improve the performance of the model."
7416,SP:f831d25830efa88434b43e900241a5ad81119360,"compositional reasoning CONJUNCTION reuse of knowledge. reuse of knowledge CONJUNCTION compositional reasoning. they USED-FOR systematic generalization. architecture USED-FOR inference. self - attention network USED-FOR inference. functions HYPONYM-OF system of modules. architecture USED-FOR capacity extension. architecture USED-FOR computation. image classification CONJUNCTION visual abstract reasoning. visual abstract reasoning CONJUNCTION image classification. settings EVALUATE-FOR Neural Interpreters. settings EVALUATE-FOR it. image classification EVALUATE-FOR it. Raven Progressive Matrices USED-FOR visual abstract reasoning. visual abstract reasoning HYPONYM-OF settings. image classification HYPONYM-OF settings. Neural Interpreters COMPARE vision transformer. vision transformer COMPARE Neural Interpreters. former EVALUATE-FOR Neural Interpreters. Neural Interpreters COMPARE state - of - the - art. state - of - the - art COMPARE Neural Interpreters. latter EVALUATE-FOR Neural Interpreters. systematic generalization EVALUATE-FOR state - of - the - art. systematic generalization EVALUATE-FOR Neural Interpreters. Method is neural network architectures. Generic are model, and task. ","This paper proposes a new architecture for neural network architectures. The architecture is based on a self-attention network, which is a system of modules with functions. The authors show that they are able to perform systematic generalization by combining compositional reasoning and reuse of knowledge. They also show that the architecture can be used for capacity extension by using the architecture for computation. They show that it performs well in a variety of settings, including image classification, visual abstract reasoning, and Raven Progressive Matrices. Neural Interpreters are shown to perform better than the vision transformer in the former and the state-of-the-art in the latter. ","This paper proposes a new architecture for learning compositional reasoning and reuse of knowledge in neural network architectures. The architecture is based on a self-attention network, which is a system of modules with functions. The authors show that they can achieve systematic generalization in two settings: image classification and visual abstract reasoning with Raven Progressive Matrices. They also show that the architecture can be used for capacity extension for computation. The paper also shows that Neural Interpreters outperform the vision transformer in the former and the latter in the latter."
7432,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"fine - tuning strategies USED-FOR transfer. transfer USED-FOR challenging domains. fine - tuning strategies USED-FOR challenging domains. pre - trained policies USED-FOR exploration. Behavior Transfer ( BT ) HYPONYM-OF technique. pre - trained policies USED-FOR technique. intrinsic motivation 10 objectives USED-FOR complex behaviors. large - scale pre - training CONJUNCTION intrinsic motivation 10 objectives. intrinsic motivation 10 objectives CONJUNCTION large - scale pre - training. BT CONJUNCTION fine - tuning strategies. fine - tuning strategies CONJUNCTION BT. BT USED-FOR pre - trained 11 policies. Generic is it. Task are reinforcement learning, unsupervised pre - training phase, reinforcement learning problem, and structured exploration. Method is fine3 tuning neural network weights. OtherScientificTerm are rewards, neural network weights, pre - training, and pre - trained 15 policies. Material is supervised domains. ","This paper studies the problem of fine-tuning neural network weights in reinforcement learning. The authors propose a technique called Behavior Transfer (BT) which uses pre-trained policies to guide exploration in the unsupervised pre-training phase. The key idea is to learn a set of policies that can be used to transfer from one task to another, and then fine-tune these policies in the post-training stage.  The authors show that this technique can be applied to a wide range of tasks, and that it can achieve state-of-the-art results. ","This paper proposes Behavior Transfer (BT), a technique for reinforcement learning where the goal is to transfer from one task to another. The key idea is to use fine-tuning strategies for transfer to challenging domains. The technique is based on the pre-trained policies for exploration. The main idea is that the unsupervised pre-training phase of the reinforcement learning problem can be treated as a structured exploration problem, where the rewards are learned by fine3 tuning neural network weights. The authors show that BT can be used to transfer to different domains with different reward functions. They also show that it can be applied to supervised domains. They show that the technique can transfer to complex behaviors with intrinsic motivation 10 objectives and large-scale pre-train and intrinsic motivation10 objectives. They demonstrate that BT is able to transfer the learned policies to pre-trained 11 policies and BT can transfer pre-learned 15 policies."
7448,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"machine learning approaches USED-FOR ranking. performance metrics of interest CONJUNCTION surrogate loss functions. surrogate loss functions CONJUNCTION performance metrics of interest. gradient - based methods USED-FOR surrogate loss functions. sorting operation PART-OF ranking metrics. ranking metrics EVALUATE-FOR surrogates. differentiable surrogates USED-FOR ranking. PiRank HYPONYM-OF differentiable surrogates. continuous, temperature - controlled relaxation USED-FOR sorting operator. NeuralSort USED-FOR continuous, temperature - controlled relaxation. continuous, temperature - controlled relaxation USED-FOR PiRank. continuous, temperature - controlled relaxation USED-FOR differentiable surrogates. NeuralSort USED-FOR differentiable surrogates. PiRank USED-FOR metrics. PiRank COMPARE approaches. approaches COMPARE PiRank. OtherScientificTerm are model parameters, and zero temperature. Task are real - world applications, and training. Method is divideand - conquer extension. ","This paper studies the problem of ranking differentiable surrogates in machine learning approaches for real-world applications. The ranking metrics of interest and surrogate loss functions are learned by gradient-based methods. The sorting operation in the ranking metrics is used as a sorting operator. The authors propose PiRank, a differentiable surrogate based on continuous, temperature-controlled relaxation, which is an extension of NeuralSort. The paper shows that PiRank outperforms existing approaches in terms of the performance metrics and the surrogate loss function. ","This paper proposes a new metric for ranking in machine learning approaches for real-world applications. The ranking metrics are based on differentiable surrogates, i.e., the performance metrics of interest and surrogate loss functions, which are learned by gradient-based methods. The sorting operation of the ranking metrics is based on a continuous, temperature-controlled relaxation of the sorting operator. The authors show that the differentiable surrogate surrogates can be used to improve the ranking. The paper also shows that PiRank is able to achieve better ranking metrics than other approaches. The main contribution of the paper is the divideand-conquer extension to the training. The model parameters of the surrogate loss function can be decomposed into zero temperature and zero temperature, and the authors also propose a differentiable method, NeuralSort, which uses continuous,temporally-controlled relaxation to compute the metrics."
7464,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"they USED-FOR near - term quantum devices. machine learning USED-FOR problem. methods USED-FOR VQE structure optimization. reinforcement learning algorithm USED-FOR ansatzes. reinforcement learning algorithm USED-FOR economic circuits. feedback - driven curriculum learning method USED-FOR learning problem. complexity EVALUATE-FOR learning problem. it USED-FOR circuit depth. feedback - driven curriculum learning method USED-FOR algorithm. chemical accuracy EVALUATE-FOR benchmark problem. Task are Variational Quantum Eigensolvers ( VQEs ), and optimization of the VQE ansatz. OtherScientificTerm are variational ansatz, near - term restrictions, VQE ansatz, low depth, and ground energy estimates. Method is learning algorithm. ",This paper studies the problem of VQE structure optimization in machine learning. The authors propose a new reinforcement learning algorithm for the optimization of the ansatzes of variational ansatz (VQEs). The main idea is to use a feedback-driven curriculum learning method to solve the learning problem in the context of economic circuits. The proposed learning algorithm is based on a learning algorithm that learns the near-term restrictions on the parameters of an ansatz. The paper shows that it is able to learn the circuit depth and the chemical accuracy of a benchmark problem. ,"This paper studies Variational Quantum Eigensolvers (VQEs), a family of near-term quantum devices. The authors propose two methods for VQE structure optimization based on machine learning. The first one is variational ansatz, which is based on the idea that the nearest-term restrictions on the QE ansatz can be reduced to a low depth. The second one is a reinforcement learning algorithm for the ansatzes. The learning problem is a learning problem with a feedback-driven curriculum learning method, where the goal is to learn a learning algorithm that can be applied to the learning problem. The paper shows that it can be used to reduce the circuit depth and reduce the complexity of the learning algorithm. The evaluation is performed on a benchmark problem with respect to chemical accuracy. The results show that the optimization of the VQEs ansatz leads to a reduction in the complexity."
7480,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"Transductive inference USED-FOR few - shot learning. it COMPARE inductive counterpart. inductive counterpart COMPARE it. statistics of the unlabeled query set USED-FOR few - shot task. it USED-FOR few - shot task. statistics of the unlabeled query set USED-FOR it. class - balanced tasks USED-FOR inference. few - shot benchmarks EVALUATE-FOR inference. class - balanced tasks FEATURE-OF few - shot benchmarks. arbitrary and unknown label marginals FEATURE-OF unlabeled query sets. few - shot tasks USED-FOR inference. arbitrary class distributions FEATURE-OF few - shot tasks. arbitrary class distributions USED-FOR inference. Dirichlet - distributed random variables USED-FOR marginal probabilities. arbitrary class distributions FEATURE-OF testing tasks. α - divergences USED-FOR mutual - information loss. transductive α - divergence optimization COMPARE state - of - the - art methods. state - of - the - art methods COMPARE transductive α - divergence optimization. OtherScientificTerm are artificial regularity, marginal label probability, uniform distribution, class - balance artefact, simplex, class - distribution variations, and few - shot settings. Method are transductive methods, and inductive methods. ","This paper studies the problem of few-shot inference in the context of Transductive inference. The authors show that it is possible to learn the statistics of the unlabeled query set for a few-shoot task using it as an inductive counterpart, and that it can be used for inference on class-balanced tasks. The main contribution of the paper is to show that the marginal label probability can be computed using Dirichlet-distributed random variables. The paper also shows that the mutual-information loss obtained by transductive α-divergence optimization is better than state-of-the-art methods. ","This paper proposes a new method for few-shot learning, called Transductive Inference (TIA), which is based on the idea of minimizing the mutual information loss between the training data and the test data. The authors show that TIA is able to achieve better performance than state-of-the-art methods on several few shot benchmarks. The main contribution of the paper is that it uses the statistics of the unlabeled query set for the few-shooting task, and it is shown to outperform the inductive counterpart. The paper also shows that TAI can achieve better results on a variety of class-balanced tasks. "
7496,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"imbalanced memory distribution FEATURE-OF convolutional neural network ( CNN ) designs. overlapping patches CONJUNCTION computation overhead. computation overhead CONJUNCTION overlapping patches. receptive field redistribution USED-FOR receptive field. receptive field redistribution USED-FOR FLOPs. receptive field redistribution USED-FOR computation overhead. neural architecture CONJUNCTION inference scheduling. inference scheduling CONJUNCTION neural architecture. peak memory usage EVALUATE-FOR networks. neural networks USED-FOR MCUNetV2. Patch - based inference USED-FOR networks. visual wake words dataset EVALUATE-FOR accuracy. MCU EVALUATE-FOR MCUNetV2. ImageNet accuracy EVALUATE-FOR MCUNetV2. peak memory usage EVALUATE-FOR Patch - based inference. accuracy EVALUATE-FOR MCUNetV2. visual wake words dataset EVALUATE-FOR MCUNetV2. MCUNetV2 USED-FOR object detection. tiny devices USED-FOR object detection. mAP EVALUATE-FOR MCUNetV2. memory bottleneck PART-OF tinyML. image classification HYPONYM-OF vision applications. Task are Tiny deep learning, and Manually redistributing the receptive field. OtherScientificTerm are limited memory size, feature map, and peak memory. Metric is memory usage. Generic is network. Method are generic patch - by - patch inference scheduling, naive implementation, and neural architecture search. Material is Pascal VOC. ","This paper studies the problem of patch-by-patch inference scheduling in Tiny deep learning. The authors consider the imbalanced memory distribution of convolutional neural network (CNN) designs with limited memory size. They show that overlapping patches and computation overhead are the main reasons for the high computation overhead of FLOPs with receptive field redistribution to the receptive field of the network. They then propose a naive implementation of this problem and propose a new neural architecture search. Patch-based inference improves the accuracy of MCUNetV2 on the visual wake words dataset and improves the peak memory usage of networks with different neural architecture and inference scheduling. They also show that the memory bottleneck in tinyML (e.g., Pascal VOC) is a major reason for poor performance in vision applications like image classification. ","This paper studies the problem of Tiny deep learning, where the number of neurons in the network is limited. The authors propose a novel approach to reduce the memory usage of the network. They propose a generic patch-by-patch inference scheduling, which is based on the naive implementation of Pascal VOC. The paper shows that the proposed approach can be applied to any convolutional neural network (CNN) designs with an imbalanced memory distribution. The main contribution of the paper is to introduce the notion of ""manually redistributing the receptive field"". The receptive field redistribution is used for FLOPs, and the computation overhead is reduced by minimizing the overlap between overlapping patches and computation overhead of the feature map. They show that the neural architecture and the inference scheduling of MCUNetV2 with the proposed neural networks can achieve better performance than MCU on the visual wake words dataset and ImageNet accuracy. They also show that patch-based inference can improve the performance of networks with peak memory usage. "
7512,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"unstructured dynamic environments FEATURE-OF Bayesian automated mechanism design. optimal mechanism USED-FOR principal ’s utility. algorithm USED-FOR optimal mechanisms. linear program formulation USED-FOR algorithm. constant factor FEATURE-OF principal ’s optimal utility. unstructured environments FEATURE-OF automated dynamic mechanism design. time complexity EVALUATE-FOR algorithm. optimality CONJUNCTION computational tractability. computational tractability CONJUNCTION optimality. solution USED-FOR problem. Markov decision processes USED-FOR memoryless mechanisms. algorithms USED-FOR synthetic dynamic environments. algorithms USED-FOR algorithms. OtherScientificTerm are self - interested strategic agent, payments, individual - rationality constraints, time horizon, and strategic behavior. Task is dynamic mechanism design. ","This paper studies the problem of Bayesian automated mechanism design in unstructured dynamic environments, where the self-interested strategic agent has access to a large number of resources. The authors propose an algorithm that learns optimal mechanisms with a linear program formulation. The algorithm is based on the assumption that the principal’s utility is a constant factor, and that the optimal mechanism is a function of the number of available resources.  The authors show that the proposed algorithm has a lower time complexity than existing algorithms, and achieves state-of-the-art performance on synthetic dynamic environments. ","This paper studies the problem of Bayesian automated mechanism design in unstructured dynamic environments. The authors propose an algorithm to learn optimal mechanisms for optimal mechanisms with a self-interested strategic agent. The algorithm is based on a linear program formulation, where the principal’s utility is defined as the constant factor of the optimal utility over the time horizon. The main contribution of the paper is to introduce individual-rationality constraints on the optimal mechanism design. The proposed algorithm is evaluated on synthetic dynamic environments, and the time complexity of the algorithm is shown to be lower than the optimality and computational tractability of existing algorithms. The paper also proposes a solution to the problem for memoryless mechanisms based on Markov decision processes. "
7528,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,Graph Neural Networks ( GNNs ) architectures USED-FOR tasks. Neural Architecture Search ( NAS ) USED-FOR GNN architectures. GNN architectures USED-FOR tasks. Neural Architecture Search ( NAS ) COMPARE manually designed architectures. manually designed architectures COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR tasks. NAS USED-FOR GNN architectures. NAS USED-FOR GNN structures. gradient based NAS methods USED-FOR architectures. gradient based NAS USED-FOR searching suboptimal GNN architectures. graph structure learning USED-FOR search procedure. graph structure learning USED-FOR denoising process. denoising process USED-FOR search procedure. Structure Optimization ( GASSO ) USED-FOR Graph differentiable Architecture Search model. graph structure learning USED-FOR graph neural architectures. real - world graph datasets EVALUATE-FOR GASSO model. GASSO model COMPARE baselines. baselines COMPARE GASSO model. real - world graph datasets EVALUATE-FOR baselines. OtherScientificTerm is graph. Method is gradient descent. ,This paper studies the problem of finding suboptimal GNNs in graph neural networks (GNNs) architectures. The authors propose a new search procedure based on graph structure learning based on the denoising process. The main idea is to use a graph differentiable Architecture Search model based on Structure Optimization (GASSO). The authors show that the proposed GASSO model outperforms existing baselines on several real-world graph datasets. ,Graph Neural Networks (GNNs) architectures are widely used for tasks. Neural Architecture Search (NAS) outperforms manually designed architectures on most tasks. NAS is used to search suboptimal GNN structures using gradient based NAS methods. The search procedure is based on graph structure learning and denoising process. The authors propose a Graph differentiable Architecture Search model based on Structure Optimization (GASSO). The GASSO model outperforms baselines on several real-world graph datasets. 
7544,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"Clustering HYPONYM-OF unsupervised learning problem. metric space FEATURE-OF clusters. fairness FEATURE-OF algorithm. cost FEATURE-OF clustering objective. upper bound USED-FOR clustering problem. upper bound USED-FOR constraint. constraint USED-FOR clustering problem. upper bound USED-FOR clustering objective. it USED-FOR equality of representation. group utilitarian objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group utilitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective CONJUNCTION group egalitarian objective. group leximin objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective HYPONYM-OF fairness objectives. group egalitarian objective HYPONYM-OF fairness objectives. group utilitarian objective HYPONYM-OF fairness objectives. algorithms USED-FOR them. lower bounds USED-FOR approximation of the utilitarian and egalitarian objectives. heuristic algorithm USED-FOR leximin objective. impossibility results USED-FOR natural fairness objectives. real - world datasets EVALUATE-FOR algorithms. Method is fair clustering. OtherScientificTerm are group membership, group fairness, and utilitarian and egalitarian objectives. Generic is model. ","This paper studies the problem of fair clustering in unsupervised learning. The authors propose a new algorithm based on fairness in the metric space of the clustering objective. The main idea is to use the upper bound on the cost of a clustering problem as an upper bound for the constraint on the number of groups in a cluster, and then use it to compute the equality of representation between the groups. The paper shows that this upper bound can be used to compute a group egalitarian objective and a group leximin objective, as well as a group utilitarian objective. Finally, the authors provide lower bounds for the approximation of the utilitarian and egalitarian objectives, and show that these algorithms converge to them in practice. ","This paper studies the problem of fair clustering in the context of unsupervised learning problem. The authors propose a new algorithm that is based on fairness in the metric space of clusters. The main contribution of the paper is to introduce a new upper bound for the clustering objective in the cost of the algorithm. The upper bound can be seen as a constraint on the group membership, and it allows for the equality of representation. The paper also introduces two fairness objectives, the group egalitarian objective and the group leximin objective, which are based on a heuristic algorithm. Experiments on real-world datasets show that the proposed algorithms can achieve better approximation of the utilitarian and egalitarian objectives, and lower bounds for the group utilitarian objective are also provided. Finally, the authors provide some impossibility results for natural fairness objectives."
7560,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"neural - network - based graph generative models USED-FOR real - world network characteristics. high triangle density HYPONYM-OF real - world network characteristics. variational graph autoencoders CONJUNCTION CELL. CELL CONJUNCTION variational graph autoencoders. NetGAN CONJUNCTION variational graph autoencoders. variational graph autoencoders CONJUNCTION NetGAN. Erdös - Rényi and stochastic block models CONJUNCTION generative models. generative models CONJUNCTION Erdös - Rényi and stochastic block models. CELL HYPONYM-OF generative models. NetGAN HYPONYM-OF generative models. variational graph autoencoders HYPONYM-OF generative models. generative models PART-OF models. Erdös - Rényi and stochastic block models PART-OF models. edge independent models USED-FOR graphs. real - world social networks CONJUNCTION graphs. graphs CONJUNCTION real - world social networks. generative model USED-FOR graph statistics. overlap CONJUNCTION accuracy. accuracy CONJUNCTION overlap. overlap EVALUATE-FOR generative model. accuracy EVALUATE-FOR generative model. Method is edge independent random graph models. OtherScientificTerm are graph, and bounded overlap condition. Generic is model. ","This paper studies edge independent random graph models. The authors consider the problem of graph generative models for real-world network characteristics (e.g., high triangle density). The authors propose a new model based on edge independent models for graphs and graphs. The generative model is able to capture the graph statistics and the overlap between the graph and the edge independent model.  The authors show that the proposed model achieves better overlap and accuracy than existing models such as NetGAN, variational graph autoencoders, CELL, Erdös-Rényi and stochastic block models. ","This paper proposes a novel approach to learning graph generative models for real-world network characteristics such as high triangle density. The key idea is to use edge independent random graph models to learn the graph, and then apply edge independent models to graph statistics. The authors show that the generative model is able to learn graph statistics with high overlap and high accuracy. The proposed model is evaluated on a variety of graph statistics, including graphs, e.g., CELL, variational graph autoencoders, NetGAN, Erdös-Rényi and stochastic block models."
7576,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"backpropagation CONJUNCTION training. training CONJUNCTION backpropagation. ReLU′(0 ) USED-FOR neural network. ReLU′(0 ) USED-FOR backpropagation. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. SVHN CONJUNCTION ImageNet. ImageNet CONJUNCTION SVHN. networks CONJUNCTION datasets. datasets CONJUNCTION networks. ReLU′(0 ) USED-FOR precision levels. ImageNet HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. ResNet HYPONYM-OF networks. VGG HYPONYM-OF networks. test accuracy EVALUATE-FOR ReLU′(0 ) = 1. batch - norm CONJUNCTION ADAM. ADAM CONJUNCTION batch - norm. batch - norm HYPONYM-OF reconditioning approaches. ADAM HYPONYM-OF reconditioning approaches. OtherScientificTerm are default precision, deep learning problems, backpropagation outputs, and double precision. Method are training methods, and vanilla SGD training. Task is algorithmic differentiation of nonsmooth problems. ","This paper studies the problem of algorithmic differentiation of nonsmooth problems, where the goal is to find the optimal solution to a given problem. The authors consider the problem in the context of backpropagation and training, and propose two training methods: ReLU′(0) and ReLU’(1) for training a neural network. The main contribution of the paper is to show that ReLU(0)-1 can be used to improve the precision levels of the training and test accuracy of the neural network during training. The paper also provides a theoretical analysis of the dual precision of the two methods. ","This paper studies the problem of algorithmic differentiation of nonsmooth problems. The authors propose ReLU′(0) for backpropagation and training, which is an extension of ReLU’(1) for neural network training. The main contribution of the paper is that the authors show that the default precision for deep learning problems can be reduced to ReLU`(0), which is a good way to improve the test accuracy of a neural network.  The authors also show that for vanilla SGD training, the number of parameters of the training methods can be significantly reduced. The paper also shows that the precision levels can be decreased to a single precision, and the authors provide some experiments on MNIST, CIFAR10, SVHN, ImageNet, and ResNet. The experiments are conducted on a variety of datasets and networks, including VGG, ResNet, VGG and VGG+ResNet.   The results show that, for ReLU´(0 ) = 1, the authors are able to achieve double precision. They also provide some experimental results on batch-norm and ADAM, showing that the proposed reconditioning approaches achieve better test accuracy. "
7592,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"generalization CONJUNCTION transfer. transfer CONJUNCTION generalization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. transfer CONJUNCTION computational efficiency. computational efficiency CONJUNCTION transfer. minimizing information USED-FOR supervised learning setting. method USED-FOR policies. RPC ) USED-FOR policies. method USED-FOR RPC ). information bottlenecks CONJUNCTION model - based RL. model - based RL CONJUNCTION information bottlenecks. model - based RL CONJUNCTION bits - back coding. bits - back coding CONJUNCTION model - based RL. latent - space model CONJUNCTION policy. policy CONJUNCTION latent - space model. method USED-FOR policy. method USED-FOR latent - space model. method COMPARE prior methods. prior methods COMPARE method. reward EVALUATE-FOR information bottleneck. compression EVALUATE-FOR prior methods. method COMPARE information bottleneck. information bottleneck COMPARE method. compression EVALUATE-FOR method. reward EVALUATE-FOR method. compression USED-FOR policies. method USED-FOR policies. Method are reinforcement learning ( RL ) algorithms, and RL algorithms. Task is RL setting. OtherScientificTerm are past information, and decision making. ","This paper studies the problem of minimizing information in reinforcement learning (RL) algorithms in the supervised learning setting, where the goal is to learn policies that are robust to changes in past information. The authors consider the RL setting in which the agent has access to a large amount of past information, and the goal of the agent is to maximize the transfer, generalization, and computational efficiency. They propose a method called RPC (Reinforcement learning with RPC) to minimize the amount of information needed to learn the policies. The method is based on a combination of information bottlenecks, model-based RL, and bits-back coding. The proposed method learns a latent-space model, a policy, and a policy that can be used to learn a policy in the future. They show that the proposed method can achieve better performance than prior methods in terms of reward and information bottleneck. ","This paper proposes a method for learning policies that minimize the information bottleneck in reinforcement learning (RL) algorithms. The authors show that minimizing information in the supervised learning setting can improve generalization, robustness, transfer, and computational efficiency in the RL setting. The proposed method is based on the idea of minimizing information (RPC) in RL algorithms. In particular, the authors propose a method to learn policies that maximize RPC in RL (e.g. RPC) by minimizing the information bottlenecks in model-based RL and bits-back coding. The method is then applied to a latent-space model and a policy. The paper shows that the proposed method outperforms prior methods in terms of reward and information bottleneck. "
7608,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"Transformer architecture USED-FOR sequence processing. graphs HYPONYM-OF data structures. node PART-OF graph. full Laplacian spectrum USED-FOR learned positional encoding ( LPE ). learned positional encoding ( LPE ) USED-FOR Spectral Attention Network ( SAN ). LPE PART-OF node features. LPE USED-FOR fully - connected Transformer. node features PART-OF graph. model USED-FOR similar sub - structures. model USED-FOR distinguishing graphs. heat transfer CONJUNCTION electric interaction. electric interaction CONJUNCTION heat transfer. Transformer USED-FOR modeling of physical phenomenons. over - squashing HYPONYM-OF information bottleneck. information bottleneck PART-OF GNNs. electric interaction HYPONYM-OF modeling of physical phenomenons. heat transfer HYPONYM-OF modeling of physical phenomenons. datasets EVALUATE-FOR model. model COMPARE state - of - theart GNNs. state - of - theart GNNs COMPARE model. model COMPARE attention - based model. attention - based model COMPARE model. graph benchmarks EVALUATE-FOR fully - connected architecture. graph benchmarks EVALUATE-FOR attention - based model. OtherScientificTerm are Laplacian, resonance, and physical phenomenons. ","This paper proposes a transformer architecture for sequence processing on graphs. The learned positional encoding (LPE) is used in the Spectral Attention Network (SAN). The LPE encodes node features in a graph as a full Laplacian spectrum. The model is able to identify similar sub-structures in the same graph. The authors show that the proposed model can be used for distinguishing graphs on several graph benchmarks. The proposed model is shown to outperform state-of-theart GNNs in terms of over-squashing, which is a common information bottleneck in GNN. The paper also shows that the modeling of physical phenomenons such as heat transfer and electric interaction can be learned using the proposed Transformer.","The paper proposes a transformer architecture for sequence processing. The proposed Transformer is based on the learned positional encoding (LPE) of the Spectral Attention Network (SAN). The LPE encodes the node features of a graph as a full Laplacian spectrum. The model is able to distinguish similar sub-structures in graphs with different data structures. The information bottleneck in GNNs (e.g., over-squashing) is reduced to a single information bottleneck. Experiments on three datasets show that the proposed model outperforms the state-of-theart GNN on distinguishing graphs and on several graph benchmarks. The paper also shows that the Transformer can be used for modeling of physical phenomenons such as heat transfer and electric interaction."
7624,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"Task is two - alternative elections. OtherScientificTerm are state variable, private information, and private signals. Method is Bayes Nash equilibrium. ",This paper studies two-alternate elections where the state variable is unknown and the private information is unknown. The authors show that Bayes Nash equilibrium holds for both private signals and public signals.,"This paper proposes two-alternative elections, where the state variable is the private information, and private information is a set of private signals. The paper proposes to use Bayes Nash equilibrium to solve the problem. Experiments show that the proposed method outperforms the state-of-the-art."
7640,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"Hessian USED-FOR parameter interactions. Hessian FEATURE-OF neural network. secondorder derivatives of the loss USED-FOR Hessian. secondorder derivatives of the loss USED-FOR parameter interactions. model design CONJUNCTION optimization. optimization CONJUNCTION model design. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. model design HYPONYM-OF deep learning. optimization HYPONYM-OF deep learning. low - rank approximations CONJUNCTION heuristics. heuristics CONJUNCTION low - rank approximations. theoretical tools USED-FOR Hessian map. Hessian rank FEATURE-OF deep linear networks. rectified and hyperbolic tangent networks HYPONYM-OF models. model architecture USED-FOR rank deficiency. Generic are It, and bounds. OtherScientificTerm are network structure, and numerical Hessian rank. Method is overparameterized neural networks. ","This paper studies the Hessian of a neural network with a Hessian as the parameter interactions. The Hessian is a secondorder derivatives of the loss of the original Hessian in the neural network. It can be seen as a function of the network structure. The authors propose a theoretical tools to construct a new Hessian map for deep linear networks with a numerical Hessian rank. It is shown that this new model architecture can be used to reduce the rank deficiency in deep learning (e.g., model design, optimization, and generalization). The authors also provide bounds on the number of iterations needed to achieve the new rank. ","This paper proposes a new Hessian for parameter interactions in a neural network. It is based on the secondorder derivatives of the loss of the Hessian in the network structure. The authors show that this Hessian map can be computed using theoretical tools. They also provide bounds on the numerical Hessian rank of deep linear networks with rectified and hyperbolic tangent networks. Finally, they show that the model architecture can be used to address the rank deficiency in deep learning with low-rank approximations and heuristics. "
7656,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,metric USED-FOR LSBD. metric EVALUATE-FOR LSBD methods. metric USED-FOR LSBD. semi - supervised 6 method USED-FOR LSBD representations. LSBD - VAE HYPONYM-OF semi - supervised 6 method. metric USED-FOR LSBD - VAE. LSBD - VAE USED-FOR LSBD representations. methods USED-FOR LSBD representations. LSBD - VAE CONJUNCTION methods. methods CONJUNCTION LSBD - VAE. common VAE - based disentanglement methods USED-FOR LSBD representations. disentanglement metrics EVALUATE-FOR LSBD representations. limited supervision USED-FOR methods. limited supervision USED-FOR LSBD representations. OtherScientificTerm is Linear Symmetry - Based Disentanglement ( LSBD ). Method is linearly disentangled representations. Task is disentanglement. Metric is DLSBD. ,"This paper proposes a new metric for disentanglement, Linear Symmetry-Based Disentanglements (LSBD), which is a new way to measure the performance of LSBD. LSBD is based on a semi-supervised 6 method, LSBD-VAE, which is an extension of the recently proposed LSBD - VAE. The authors show that LSBD representations trained with the new metric outperform the state-of-the-art LSBD methods in terms of the number of disentangled representations. They also show that the proposed methods are able to achieve better performance than existing methods when using limited supervision. ","This paper proposes a new metric for disentanglement, Linear Symmetry-based Disentanglements (LSBD). The authors propose a semi-supervised 6 method, LSBD-VAE, which is based on LSBD. The authors show that the proposed metric can be used to evaluate LSBD methods on disentangled representations. They also show that LSBD representations can be improved by LSBD - VAE and other methods that rely on limited supervision. Experiments are conducted on common VAE-based disentangler methods to show the effectiveness of the proposed LSBD representation. "
7672,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,Deep state - space models ( DSSMs ) USED-FOR temporal predictions. dynamics of observed sequence data USED-FOR Deep state - space models ( DSSMs ). dynamics of observed sequence data USED-FOR temporal predictions. evidence lower bound USED-FOR They. model USED-FOR dynamics. approach USED-FOR DSSMs. constrained optimisation framework USED-FOR DSSMs. constrained optimisation framework USED-FOR approach. amortised variational inference CONJUNCTION Bayesian filtering / smoothing. Bayesian filtering / smoothing CONJUNCTION amortised variational inference. extended Kalman VAE ( EKVAE ) COMPARE RNN - based DSSMs. RNN - based DSSMs COMPARE extended Kalman VAE ( EKVAE ). amortised variational inference PART-OF extended Kalman VAE ( EKVAE ). Bayesian filtering / smoothing PART-OF extended Kalman VAE ( EKVAE ). constrained optimisation framework USED-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR constrained optimisation framework. EKVAE COMPARE models. models COMPARE EKVAE. EKVAE USED-FOR dynamical systems. EKVAE USED-FOR state - space representations. static and dynamic features PART-OF state - space representations. prediction accuracy EVALUATE-FOR EKVAE. prediction accuracy EVALUATE-FOR models. Task is maximising the evidence lower bound. ,This paper studies the dynamics of observed sequence data in Deep state-space models (DSSMs) for temporal predictions based on the dynamical system dynamics. The authors propose a constrained optimisation framework for DSSMs based on amortised variational inference and Bayesian filtering/smoothing. They provide an evidence lower bound for the performance of the proposed approach. They show that the proposed extended Kalman VAE (EKVAE) outperforms RNN-based DSSM in terms of system identification and prediction accuracy. They also provide a theoretical analysis of their approach. ,This paper proposes a novel approach to learn temporal predictions from dynamics of observed sequence data for Deep state-space models (DSSMs) for temporal predictions. The approach is based on a constrained optimisation framework for DSSMs that maximises the evidence lower bound on the dynamics. The authors show that the proposed extended Kalman VAE (EKVAE) outperforms RNN-based DSSM with amortised variational inference and Bayesian filtering/smoothing. The paper also shows that the models achieve better system identification and prediction accuracy compared to other models. 
7688,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"Explanation techniques USED-FOR introspecting black - box models. deep inversion approach USED-FOR counterfactual explanations. deep models USED-FOR images. training distribution USED-FOR deep models. deep inversion methods USED-FOR counterfactuals. deep inversion methods USED-FOR conditional image synthesis. DISC USED-FOR Synthesizing Counterfactuals. deep inversion USED-FOR DISC. image priors USED-FOR DISC. counterfactuals USED-FOR classifier decision boundaries. DISC USED-FOR counterfactuals. counterfactuals USED-FOR visually meaningful explanations. Task is model prediction. OtherScientificTerm are discernible changes, data manifold, manifold consistency objective, and unknown test - time corruptions. Method are deep classifier, and progressive optimization strategy. ","This paper proposes a deep inversion approach to generate counterfactual explanations for black-box models. The main idea is to train deep models on images with discernible changes, and then use the training distribution of these deep models to generate explanations for the images. Synthesizing Counterfactuals (DISC) is an extension of the recent work on conditional image synthesis, which uses deep invert methods for generating counterfactuallys. DISC uses image priors to train the deep classifier, and the classifier decision boundaries are learned using the counterfactUALs. The authors show that DISC is able to generate visually meaningful explanations for images, and that the model prediction can be improved by using a progressive optimization strategy. ","This paper proposes a deep inversion approach to learn counterfactual explanations for black-box models. The authors propose to use DISC for conditional image synthesis, which is an extension of the DISC method for Synthesizing Counterfactuals. The main idea is to use a deep classifier to predict the classifier decision boundaries, and then use a progressive optimization strategy to optimize the training distribution of the deep models to produce images with discernible changes in the data manifold. The paper also proposes a manifold consistency objective to prevent unknown test-time corruptions. Experiments show that DISC is able to generate visually meaningful explanations for a wide range of image priors."
7704,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"causal inference problem USED-FOR this. empirical objective USED-FOR algorithm. algorithm USED-FOR region of heterogeneity. algorithm COMPARE baselines. baselines COMPARE algorithm. real - world healthcare datasets EVALUATE-FOR algorithm. OtherScientificTerm are drug - related offenses, inter - decision - maker disagreement, generalization bound, and clinical knowledge. ","This paper studies the problem of drug-related offenses in the context of causal inference problem. The authors propose a new algorithm for this problem, which is based on the empirical objective. The main idea is to use the region of heterogeneity as an empirical objective to train the algorithm. The algorithm is evaluated on a variety of real-world healthcare datasets and shows that the proposed algorithm outperforms existing baselines. ",This paper proposes an algorithm to reduce the region of heterogeneity in drug-related offenses. The main idea is to use the causal inference problem to solve this problem. The algorithm is based on the empirical objective of the algorithm. The authors provide a generalization bound on the inter-decision-maker disagreement. The proposed algorithm is evaluated on two real-world healthcare datasets. The results show that the proposed algorithm outperforms baselines. 
7720,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"perspective USED-FOR image synthesis. visual token generation problem USED-FOR task. paradigms COMPARE formulation. formulation COMPARE paradigms. flexible local manipulation USED-FOR image regions. formulation USED-FOR flexible local manipulation. latent tokens USED-FOR visual tokens. latent tokens USED-FOR it. constant content tokens CONJUNCTION style tokens. style tokens CONJUNCTION constant content tokens. style tokens PART-OF latent space. visual tokens USED-FOR TokenGAN. constant content tokens HYPONYM-OF visual tokens. style tokens HYPONYM-OF visual tokens. TokenGAN USED-FOR image synthesis. style tokens USED-FOR TokenGAN. Transformer USED-FOR attention mechanism. attention mechanism USED-FOR styles. attention mechanism USED-FOR TokenGAN. FFHQ CONJUNCTION LSUN CHURCH. LSUN CHURCH CONJUNCTION FFHQ. image synthesis benchmarks EVALUATE-FOR TokenGAN. LSUN CHURCH HYPONYM-OF image synthesis benchmarks. FFHQ HYPONYM-OF image synthesis benchmarks. generator USED-FOR high - fidelity images. 1024× 1024 size FEATURE-OF high - fidelity images. OtherScientificTerm are latent code, content tokens, and convolutions. Method is token - based generator. ","This paper proposes a new perspective for image synthesis based on the visual token generation problem. The key idea is to learn a token-based generator that can generate high-fidelity images with 1024×1024 size. The proposed formulation is based on flexible local manipulation of the image regions by using latent tokens to represent the visual tokens in the latent space. The latent code is then used to generate the content tokens, and the style tokens are used to represent visual tokens. The attention mechanism in TokenGAN is a Transformer-based attention mechanism. The authors show that TokenGAN outperforms existing paradigms on several image synthesis benchmarks, including FFHQ and LSUN CHURCH.","This paper proposes a new perspective for image synthesis, where the goal is to generate high-fidelity images of high-quality images of 1024×1024 size. The task is a visual token generation problem. The authors propose a novel formulation for flexible local manipulation of image regions. The key idea is to use a token-based generator to generate the latent code and the style tokens in the latent space, and then use it to generate visual tokens (constant content tokens, style tokens, etc) from the latent tokens. The attention mechanism is based on a Transformer, and the output of the attention mechanism can be used to generate different styles. Experiments are conducted on several image synthesis benchmarks, including FFHQ, LSUN, and CIFAR-10, and show that the proposed formulation outperforms other paradigms. "
7736,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"min - norm interpolators CONJUNCTION max - margin classifiers. max - margin classifiers CONJUNCTION min - norm interpolators. overparameterization USED-FOR min - norm interpolators. overparameterization USED-FOR variance. variance FEATURE-OF min - norm interpolators. overparameterization USED-FOR max - margin classifiers. ridge regularization USED-FOR high dimensions. generalization EVALUATE-FOR avoiding interpolation. ridge regularization USED-FOR avoiding interpolation. linear regression CONJUNCTION classification. classification CONJUNCTION linear regression. linear regression FEATURE-OF robust risk. classification FEATURE-OF robust risk. OtherScientificTerm are noise, and interpolation. Task is robust overfitting. ","This paper studies the problem of robust overfitting in the presence of noise in the training data. In particular, the authors consider the case of min-norm interpolators and max-margin classifiers with overparameterization. The authors show that the variance of the min-neighboring min norm interpolators is bounded by the ridge regularization of the high dimensions of the interpolation. They also show that avoiding interpolation is a good generalization in terms of generalization. Finally, they provide a robust risk for linear regression and classification.",This paper studies the problem of robust overfitting in the presence of noise. The authors propose to use overparameterization to reduce the variance of min-norm interpolators and max-margin classifiers. The main idea is to use ridge regularization for high dimensions to avoid interpolation. The paper shows that avoiding interpolation improves generalization and robust risk on linear regression and classification. 
7752,SP:09f080f47db81b513af26add851822c5c32bb94e,"canonical primitive USED-FOR arbitrarily ordered point cloud. sphere HYPONYM-OF canonical primitive. primitive USED-FOR unordered point clouds. unordered point clouds PART-OF canonical surface. annotation CONJUNCTION selfsupervised part segmentation network. selfsupervised part segmentation network CONJUNCTION annotation. method USED-FOR unaligned input point clouds. selfsupervised part segmentation network USED-FOR method. annotation USED-FOR method. rotation range FEATURE-OF unaligned input point clouds. 3D semantic keypoint transfer CONJUNCTION part segmentation transfer. part segmentation transfer CONJUNCTION 3D semantic keypoint transfer. model COMPARE correspondence learning methods. correspondence learning methods COMPARE model. 3D semantic keypoint transfer EVALUATE-FOR model. part segmentation transfer EVALUATE-FOR model. Method are canonical point autoencoder ( CPAE ), and autoencoder. Material is 3D shapes. OtherScientificTerm is primitive surface. Generic is models. ","This paper proposes a canonical point autoencoder (CPAE) for unordered point clouds. The canonical primitive is an arbitrarily ordered point cloud, which is a sphere. The authors propose a method for unaligned input point clouds with rotation range in the rotation range. The method is based on annotation and selfsupervised part segmentation network. The proposed model is evaluated on 3D semantic keypoint transfer and part segmentsation transfer.","This paper proposes a canonical primitive for arbitrarily ordered point cloud. The canonical primitive is the sphere, which is an extension of the canonical point autoencoder (CPAE). The authors show that the primitive can be applied to unordered point clouds in the canonical surface. The authors propose a method for learning unaligned input point clouds with rotation range in the rotation range. The method is based on an annotation and a selfsupervised part segmentation network. The proposed model is evaluated on 3D semantic keypoint transfer, 3D segmentation transfer, and part segmentsation transfer. The model outperforms other correspondence learning methods. "
7768,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"Domain generalization ( DG ) methods USED-FOR generalizability. empirical risk minimization ( ERM ) approach COMPARE methods. methods COMPARE empirical risk minimization ( ERM ) approach. complex, non - convex loss function USED-FOR ERM. sharp minima USED-FOR sub - optimal generalizability. domain generalization gap EVALUATE-FOR flat minima. method USED-FOR flat minima. SWAD COMPARE vanilla SWA. vanilla SWA COMPARE SWAD. SWAD USED-FOR flatter minima. VLCS CONJUNCTION OfficeHome. OfficeHome CONJUNCTION VLCS. OfficeHome CONJUNCTION TerraIncognita. TerraIncognita CONJUNCTION OfficeHome. TerraIncognita CONJUNCTION DomainNet. DomainNet CONJUNCTION TerraIncognita. PACS CONJUNCTION VLCS. VLCS CONJUNCTION PACS. DG benchmarks EVALUATE-FOR SWAD. outof - domain accuracy EVALUATE-FOR SWAD. PACS HYPONYM-OF DG benchmarks. OfficeHome HYPONYM-OF DG benchmarks. DomainNet HYPONYM-OF DG benchmarks. VLCS HYPONYM-OF DG benchmarks. TerraIncognita HYPONYM-OF DG benchmarks. SWAD COMPARE generalization methods. generalization methods COMPARE SWAD. data augmentation and consistency regularization methods HYPONYM-OF generalization methods. SWAD CONJUNCTION DG method. DG method CONJUNCTION SWAD. SWAD USED-FOR DG. DG method USED-FOR DG. SWAD USED-FOR DG methods. Method are DomainBed, and Stochastic Weight Averaging Densely ( SWAD ). OtherScientificTerm is overfitting. Metric is in - domain generalizability. ","This paper studies domain generalization (DG) methods for generalizability in the presence of sharp minima. The authors propose Stochastic Weight Averaging Densely (SWAD), a new method for finding flat minima in the domain generalisation gap. The proposed method is based on the empirical risk minimization (ERM) approach, where the ERM is a complex, non-convex loss function.  The authors show that SWAD is able to find flatter minima with outof-domain accuracy than vanilla SWA. They also show that the proposed SWAD outperforms other DG benchmarks such as PACS, VLCS, OfficeHome, DomainNet, and TerraIncognita.  Finally, the authors show how SWAD can outperform other DG methods such as the DG method and DG method with data augmentation and consistency regularization methods. ","Domain generalization (DGD) methods are used to improve generalizability. These methods are based on the empirical risk minimization (ERM) approach, where the ERM is a complex, non-convex loss function with sharp minima. The authors propose two variants of DomainBed, and Stochastic Weight Averaging Densely (SWAD). The main idea of SWAD is to reduce the domain generalization gap between flat minima and sharp ones, which is a result of overfitting.  The authors show that SWAD reduces the outof-domain accuracy of DG benchmarks on PACS, VLCS, OfficeHome, DomainNet, and TerraIncognita. SWAD outperforms other generalization methods such as data augmentation and consistency regularization methods. "
7784,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"initialization time CONJUNCTION query time. query time CONJUNCTION initialization time. evaluation metric CONJUNCTION optimization. optimization CONJUNCTION evaluation metric. weight - sharing CONJUNCTION supervised learning. supervised learning CONJUNCTION weight - sharing. supervised learning CONJUNCTION zero - cost proxies. zero - cost proxies CONJUNCTION supervised learning. learning curve extrapolation CONJUNCTION weight - sharing. weight - sharing CONJUNCTION learning curve extrapolation. zero - cost proxies HYPONYM-OF techniques. learning curve extrapolation HYPONYM-OF techniques. supervised learning HYPONYM-OF techniques. weight - sharing HYPONYM-OF techniques. technique USED-FOR predictor - based NAS frameworks. Task are neural architecture search ( NAS ), and performance predictors. Method are neural networks, neural architectures, and performance prediction methods. Metric are correlationand rank - based performance measures, and predictive power. OtherScientificTerm is predictors. Generic is code. ","This paper studies the problem of neural architecture search (NAS), where the goal is to find the best architecture for a given task. The authors propose a new evaluation metric, which is based on the correlation and rank-based performance measures, and propose two techniques, zero-cost proxies and weight-sharing, to improve the performance of neural networks. The proposed technique is applied to a variety of predictor-based NAS frameworks, and shows that the proposed technique can achieve better performance than other performance prediction methods. ","This paper proposes a new method for performing neural architecture search (NAS) in the context of performance prediction. The authors propose a new evaluation metric and an optimization metric to measure the correlation and rank-based performance measures between two neural networks. The evaluation metric measures the predictive power of the predictors, and the optimization measure measures the number of times that the predictor is able to predict the output of the model. The optimization measure is a function of the initialization time and query time. The performance predictors are trained on a set of different neural architectures. The proposed technique is applied to a number of predictor-based NAS frameworks, including zero-cost proxies, supervised learning, weight-sharing, and learning curve extrapolation. The experimental results show that the proposed method outperforms other performance prediction methods. "
7800,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,Dirichlet posterior sampling CONJUNCTION privacy 4 guarantees. privacy 4 guarantees CONJUNCTION Dirichlet posterior sampling. exponential families FEATURE-OF differential privacy of posterior sampling. truncated concentrated differential privacy ( tCDP ) USED-FOR privacy guarantee. privacy guarantee FEATURE-OF Dirichlet posterior sampling. Dirichlet posterior sampling USED-FOR Multinomial8 Dirichlet sampling. Dirichlet posterior sampling USED-FOR private normalized histogram publishing. accuracy guarantees EVALUATE-FOR Dirichlet posterior sampling. Metric is inherent privacy. OtherScientificTerm is Dirichlet posterior 1 distribution. Method is posterior sampling. ,"This paper studies the differential privacy of posterior sampling in exponential families with exponential families. The authors show that Dirichlet posterior sampling with truncated concentrated differential privacy (tCDP) provides a better privacy guarantee than the privacy guarantee obtained by Dirichlett posterior sampling. They also provide a theoretical analysis of the inherent privacy of such posterior sampling and show that the privacy guarantees obtained from Dirichle posterior 1 distribution are better than the ones obtained by posterior sampling using Dirichlets. Finally, the authors propose a new method for private normalized histogram publishing based on Dirichtle posterior sampling, which is based on Multinomial8Dirichlet sampling. ","This paper studies the differential privacy of posterior sampling in exponential families with exponential families. The authors show that Dirichlet posterior sampling with truncated concentrated differential privacy (tCDP) provides a privacy guarantee that is equivalent to the inherent privacy. The privacy guarantee is also extended to Multinomial8Dirichlet sampling, where the privacy guarantee can be extended to the Diriclet posterior 1 distribution. The paper also provides a theoretical analysis of the privacy guarantees of Dirichlett posterior sampling for private normalized histogram publishing. "
7816,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,clustering CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION clustering. Random walks USED-FOR machine learning algorithms. parallel algorithm USED-FOR random walks. random walks USED-FOR it. random walk USED-FOR algorithm. technique USED-FOR parallel local clustering algorithm. algorithm COMPARE approaches. approaches COMPARE algorithm. Generic is method. OtherScientificTerm is graph. ,This paper proposes a new method for clustering and semi-supervised learning based on random walks in machine learning algorithms. The proposed technique is based on a parallel local clustering algorithm that uses a random walk to learn a graph. The authors show that it can be combined with a parallel algorithm that learns random walks for random walks. They show that the proposed algorithm can achieve better performance than existing approaches.,"This paper proposes a new method for clustering and semi-supervised learning based on Random walks for machine learning algorithms. The authors propose a parallel local clustering algorithm based on the technique of random walks. The proposed algorithm is based on random walks, and it uses a parallel algorithm to learn random walks for each node in the graph. The algorithm is evaluated on several datasets and compared to other approaches."
7832,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,statistical mechanics USED-FOR replica method. typical sample complexity EVALUATE-FOR ` 1 - LinR. paramagnetic phase FEATURE-OF random regular graphs. ` 1 - LinR USED-FOR model selection. order of sample complexity EVALUATE-FOR model selection. order of sample complexity EVALUATE-FOR ` 1 - LinR. method USED-FOR nonasymptotic behavior. precision CONJUNCTION recall. recall CONJUNCTION precision. method USED-FOR ` 1 - LinR. nonasymptotic behavior FEATURE-OF ` 1 - LinR. precision HYPONYM-OF nonasymptotic behavior. recall HYPONYM-OF nonasymptotic behavior. ` 1 - LogR CONJUNCTION interaction screening. interaction screening CONJUNCTION ` 1 - LogR. method USED-FOR ` 1 - regularized M -estimators. interaction screening HYPONYM-OF ` 1 - regularized M -estimators. ` 1 - LogR HYPONYM-OF ` 1 - regularized M -estimators. Task is Ising model selection. OtherScientificTerm is model misspecification. Metric is M. Method is Ising model. ,"This paper proposes a new replica method based on statistical mechanics. The key idea is to use the typical sample complexity of `1-LinR with a paramagnetic phase in random regular graphs with paramagnetic phases. The authors propose a new method for model selection based on `1 - LinR with order of sample complexity. The proposed method is able to capture nonasymptotic behavior such as precision, recall, and interaction screening. The method is shown to improve the performance of ‘1-regularized M-estimators’ such as ` 1-LogR’ and “1-logR with interaction screening”. ","This paper proposes a replica method based on statistical mechanics to improve the Ising model selection. The key idea is to use the paramagnetic phase of random regular graphs as a surrogate for model misspecification. The authors show that `1-LinR with typical sample complexity is able to achieve better model selection with order of sample complexity than the “standard” “Ising model”. They also show that the proposed method can achieve nonasymptotic behavior in terms of precision, recall, and interaction screening. Finally, they show that their method can be applied to “1-regularized M-estimators” (e.g., 1-LogR, 2-logR, etc.)."
7848,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"k - means USED-FOR datasets. fuzzy or soft k - means objective HYPONYM-OF kmeans problem. framework USED-FOR clustering. similarity queries USED-FOR polynomial - time approximation algorithm. algorithms USED-FOR fuzzy clustering. k - means USED-FOR fuzzy k - means objective. non - negative matrix factorization HYPONYM-OF nonconvex problem. Lloyd - type algorithms CONJUNCTION alternating - minimization algorithms. alternating - minimization algorithms CONJUNCTION Lloyd - type algorithms. similarity queries USED-FOR problem. real - world applications EVALUATE-FOR algorithms. real - world datasets EVALUATE-FOR algorithms. Method is semisupervised active clustering framework. OtherScientificTerm are similarity, O(poly(k ) log n ) similarity queries, and local minima. Metric is polynomialtime - complexity. ","This paper proposes a semisupervised active clustering framework for the kmeans problem, which is a variant of the fuzzy or soft k-means objective. The authors propose two algorithms for fuzzy clustering based on k-mean for datasets where the similarity between the two datasets is known. The first algorithm is a polynomial-time approximation algorithm based on similarity queries. The second algorithm is based on a nonconvex problem called non-negative matrix factorization. The main idea is to use O(poly(k) log n) similarity queries to solve the problem. The proposed algorithms are evaluated on a variety of real-world applications and compared with Lloyd-type algorithms and alternating-minimization algorithms. ","This paper proposes a semisupervised active clustering framework for the kmeans problem, which is a variant of the fuzzy or soft k-means objective. The key idea of the framework is to use similarity queries as a polynomial-time approximation algorithm for the nonconvex problem, i.e., non-negative matrix factorization. The main idea is that the similarity queries are O(poly(k) log n) similarity queries. The authors show that algorithms for fuzzy clustering can be learned using these algorithms. The proposed algorithms are evaluated on real-world applications, and compared to Lloyd-type algorithms and alternating-minimization algorithms. "
7864,SP:a8057c4708dceb4f934e449080043037a70fabf7,"models USED-FOR planning. value functions CONJUNCTION policies. policies CONJUNCTION value functions. computation USED-FOR policies. computation USED-FOR value functions. model CONJUNCTION value function. value function CONJUNCTION model. approach COMPARE planning methods. planning methods COMPARE approach. Dyna HYPONYM-OF planning methods. self - consistency USED-FOR policy evaluation. self - consistency USED-FOR control. policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. tabular and function approximation settings EVALUATE-FOR these. Method are reinforcement learning ( RL ) agents, model - based RL, and self - consistency updates. OtherScientificTerm is environment interactions. ","This paper studies the problem of reinforcement learning (RL) agents in the presence of environment interactions. The authors propose a model-based RL, where the goal is to learn a set of policies and value functions that can be combined with a model and a value function. The proposed approach is based on self-consistency updates, and is shown to perform better than other planning methods such as Dyna. The experiments are conducted on tabular and function approximation settings, and show that the proposed approach performs better than existing planning methods.","This paper proposes a new model-based reinforcement learning (RL) agents that can be used for planning. The proposed approach is compared to other planning methods such as Dyna, where the value functions and policies are computed using computation. The authors show that the proposed model can be combined with a value function and a policy. They also show that self-consistency for policy evaluation and for control can be achieved by self-constraint updates. Experiments are conducted on tabular and function approximation settings and show that these agents are able to achieve better performance than other models. "
7880,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"Episodic training USED-FOR models. few - shot learning USED-FOR models. Episodic training PART-OF few - shot learning. method USED-FOR episode sampling distributions. difficulty USED-FOR method. curriculum HYPONYM-OF sampling schemes. few - shot learning accuracies EVALUATE-FOR episodic training algorithms. algorithms CONJUNCTION network architectures. network architectures CONJUNCTION algorithms. network architectures CONJUNCTION protocols. protocols CONJUNCTION network architectures. few - shot learning datasets CONJUNCTION algorithms. algorithms CONJUNCTION few - shot learning datasets. network architectures EVALUATE-FOR method. algorithms EVALUATE-FOR method. few - shot learning datasets EVALUATE-FOR method. protocols EVALUATE-FOR method. Material is limited labelled data. Method are episodic training, and sampling method. OtherScientificTerm is episode difficulty. ","This paper studies episodic training in few-shot learning, where the goal is to train models on limited labelled data. Episodic training is a popular technique for few-shots learning, and the authors propose a new sampling method based on episodic learning. The sampling scheme is based on a curriculum, where each episode is sampled from a set of episodes. The authors show that their method is able to learn episode sampling distributions with high difficulty. They also show that the sampling method can be used to improve the performance of few-shoot learning algorithms. The proposed method is evaluated on a variety of benchmarks, and compared to existing algorithms and network architectures.","This paper proposes a method for few-shot learning for models with limited labelled data. Episodic training is a popular method for training models. The authors propose a sampling method for learning episode sampling distributions. The sampling method is based on the curriculum, which is a collection of sampling schemes. The main idea of the method is to reduce the episode difficulty by increasing the number of episodes. Experiments show that the proposed method is able to improve the performance of few-shooting learning accuracies of episodic training algorithms and network architectures. "
7896,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"logistic bandits USED-FOR binary rewards. logistic bandits HYPONYM-OF ones. ones PART-OF generalized linear bandits. logistic bandits HYPONYM-OF generalized linear bandits. algorithms USED-FOR logistic bandits. unknown parameter FEATURE-OF MNL probabilistic model. MNL - UCB HYPONYM-OF upper confidence bound ( UCB)-based algorithm. MNL - UCB USED-FOR problem. Generic is extension. Method is multinomial logit ( MNL ). OtherScientificTerm are revenue parameter, problemdependent constants, and loose regret bounds. ","This paper proposes a new upper confidence bound (UCB) based on the multinomial logit (MNL) for the logistic bandits. The authors show that the UCB can be extended to the case where the revenue parameter is unknown, which is the case for generalized linear bandits. They also provide an extension to the problem where the unknown parameter of the MNL probabilistic model is unknown. Finally, the authors provide loose regret bounds for the proposed algorithm.","This paper proposes a new extension of multinomial logit (MNL) to the problem of binary rewards. The main idea is to add an unknown parameter to the MNL probabilistic model, which is the revenue parameter. The authors propose two algorithms for logistic bandits, one for generalized linear bandits, and one for ones that are not. The problem is formulated as an extension of multi-parameter multi-modal linear bandits. The proposed algorithm, MNL-UCB, is an upper confidence bound (UCB)-based algorithm, where the problem is solved by solving the problem with the unknown parameter.  The authors show that the proposed algorithm can be extended to the case where there is no revenue parameter and the problem dependent constants are non-negative. They also provide loose regret bounds."
7912,SP:0eaf058ed224464f6682cbbd80f716c89759f467,max - min entropy framework USED-FOR reinforcement learning ( RL ). maximum entropy RL USED-FOR model - free sample - based learning. soft actor - critic ( SAC ) algorithm USED-FOR maximum entropy RL. maximum entropy RL USED-FOR policies. entropy USED-FOR exploration. entropy FEATURE-OF low - entropy states. max - min entropy framework USED-FOR algorithm. algorithm USED-FOR Markov decision processes ( MDPs ). algorithm COMPARE RL algorithms. RL algorithms COMPARE algorithm. ,This paper proposes a max-min entropy framework for reinforcement learning (RL) with maximum entropy RL for model-free sample-based learning. The authors propose a soft actor-critic (SAC) algorithm to learn policies with the max-max entropy RL with the goal of maximizing the entropy for exploration. The algorithm is applied to Markov decision processes (MDPs) and is shown to outperform existing RL algorithms.,This paper proposes a max-min entropy framework for reinforcement learning (RL). Maximum entropy RL is used for model-free sample-based learning. The authors propose a soft actor-critic (SAC) algorithm for maximum entropy RL. The algorithm is applied to Markov decision processes (MDPs). The authors show that the proposed algorithm outperforms other RL algorithms on low-entropy states with high entropy. They also show that max entropy RL can be used to learn policies for exploration.
7928,SP:19107a648d3d23403a8693b065ee842833a0b893,"cross - sectional data USED-FOR learning task. continuous - time Markov chains USED-FOR problem. approximate likelihood maximization method USED-FOR continuous - time Markov chains. synthetic and real cancer data EVALUATE-FOR approach. OtherScientificTerm are genetic mutations, time order, and underspecification. Task is biomedical applications. Generic is methods. ",This paper studies the problem of learning cross-sectional data for a learning task from cross-sectional data. The problem is formulated as a continuous-time Markov chains with genetic mutations. The authors propose an approximate likelihood maximization method to solve the problem. The proposed approach is evaluated on synthetic and real cancer data.  ,This paper proposes a new learning task based on cross-sectional data. The problem is formulated as a continuous-time Markov chains with genetic mutations. The authors propose an approximate likelihood maximization method to optimize the time order. Experiments on synthetic and real cancer data show the effectiveness of the proposed approach. 
7944,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,Document intelligence USED-FOR business applications. self - supervised learning methods USED-FOR annotation efforts. large - scale unlabeled document datasets USED-FOR self - supervised learning methods. self - supervised objectives FEATURE-OF models. models USED-FOR annotation efforts. unified pretraining framework USED-FOR document understanding. UDoc HYPONYM-OF unified pretraining framework. UDoc USED-FOR document understanding tasks. Transformer USED-FOR UDoc. multimodal embeddings USED-FOR Transformer. semantic region USED-FOR words and visual features. it USED-FOR generic representation. representation USED-FOR similarities. self - supervised losses USED-FOR it. self - supervised losses USED-FOR generic representation. pretraining procedure USED-FOR joint representations. pretraining procedure USED-FOR downstream tasks. Material is documents. Method is document pretraining methods. ,"This paper proposes a unified pretraining framework for document understanding, called UDoc. UDoc is based on the Transformer, which is an extension of Transformer with multimodal embeddings. The authors show that UDoc can be applied to a variety of document understanding tasks, including document classification, document segmentation, and document summarization. They also show that it can be combined with self-supervised learning methods to improve the performance of annotation efforts.","This paper proposes a unified pretraining framework for document understanding, called UDoc. UDoc is based on the Transformer, which can be applied to a variety of document understanding tasks. The Transformer uses multimodal embeddings to encode words and visual features into a single semantic region. The authors show that UDoc can be used to learn a generic representation of a document, and that it can be trained with self-supervised objectives. The proposed pretraining procedure can also be used for learning joint representations for downstream tasks. "
7960,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"individual fairness FEATURE-OF data clustering problems. lp - norm objectives FEATURE-OF data clustering problems. k - MEDIAN HYPONYM-OF data clustering problems. k - MEDIAN HYPONYM-OF lp - norm objectives. objective guarantees FEATURE-OF l∞ or k - CENTER objective. clustering algorithm USED-FOR l∞ or k - CENTER objective. objective guarantees FEATURE-OF clustering algorithm. local - search algorithm USED-FOR lp - norms. kMEDIAN CONJUNCTION k - MEANS. k - MEANS CONJUNCTION kMEDIAN. algorithms USED-FOR problem. Linear Programming ( LP ) techniques USED-FOR algorithms. theoretical fairness guarantees COMPARE MV20. MV20 COMPARE theoretical fairness guarantees. sparsification technique USED-FOR algorithm. run - time EVALUATE-FOR algorithm. run - time EVALUATE-FOR sparsification technique. Generic are dataset, concept, and objective. OtherScientificTerm is individual fairness constraint. Metric are fairness, and worst - case guarantee. Method is LP rounding techniques. ","This paper studies the problem of individual fairness in data clustering problems such as k-MEDIAN and k-MEANS, where the goal is to ensure that the clustering algorithm converges to the l∞ or k-CENTER objective with high objective guarantees. The authors propose two algorithms to solve this problem using Linear Programming (LP) techniques. The first algorithm is based on a local-search algorithm to find the lp-norms of the dataset. The second algorithm uses a sparsification technique to improve the run-time of the algorithm. The theoretical fairness guarantees of the proposed algorithm are compared to MV20 and MV20. ","This paper studies the problem of individual fairness in data clustering problems such as k-MEDIAN and k-MEANS, which are lp-norm objectives. The authors propose two algorithms to solve this problem using Linear Programming (LP) techniques. The main idea is to use a local-search algorithm to compute lp norms for each dataset, and then apply a clustering algorithm to achieve the l∞ or k-CENTER objective. The paper provides theoretical fairness guarantees for the lp–norm objective, as well as the best-case guarantee for the individual fairness constraint. The proposed algorithm is evaluated in terms of run-time and sparsification technique.  The paper shows that the proposed algorithm outperforms MV20 and kMEDIAN on the dataset. "
7976,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"MAX - K - CUT CONJUNCTION correlation clustering. correlation clustering CONJUNCTION MAX - K - CUT. MAX - K - CUT HYPONYM-OF graph partitioning problems. correlation clustering HYPONYM-OF graph partitioning problems. MAX - K - CUT CONJUNCTION MAX - AGREE variant of correlation clustering. MAX - AGREE variant of correlation clustering CONJUNCTION MAX - K - CUT. methods USED-FOR MAX - K - CUT. methods USED-FOR approximation guarantees. methods USED-FOR SDPs. approximation guarantees CONJUNCTION MAX - K - CUT. MAX - K - CUT CONJUNCTION approximation guarantees. O(n ) constraints USED-FOR SDPs. polynomial - time Gaussian sampling - based algorithms USED-FOR problems. O(n + |E| ) memory USED-FOR polynomial - time Gaussian sampling - based algorithms. approach CONJUNCTION sparsification. sparsification CONJUNCTION approach. OtherScientificTerm are memory bottleneck, and dense graphs. Metric are storage complexity, and approximation ratio. ","This paper studies the problem of graph partitioning problems such as MAX-K-CUT and correlation clustering, where the storage complexity is large and the memory bottleneck is large. The authors propose two methods to improve the approximation guarantees for these two SDPs under O(n) constraints. The main idea is to use polynomial-time Gaussian sampling-based algorithms to solve these problems with O(N + |E|) memory. The proposed approach is based on sparsification, and the authors show that the approximation ratio of the proposed methods is better than the previous methods. ","This paper studies the problem of graph partitioning problems such as MAX-K-CUT and correlation clustering. The authors propose two methods to improve the approximation guarantees of SDPs under O(n) constraints. The main contribution of the paper is to study the storage complexity of the memory bottleneck in dense graphs. The paper proposes a new approach to reduce the size of the storage and sparsification of dense graphs, and proposes polynomial-time Gaussian sampling-based algorithms for solving these problems. "
7992,SP:cfd6cf88a823729c281059e179788248238a6ed7,predicting inter - frame motion information USED-FOR video prediction tasks. Motion - Aware Unit ( MAU ) USED-FOR inter - frame motion information. temporal receptive field FEATURE-OF predictive units. attention module CONJUNCTION fusion module. fusion module CONJUNCTION attention module. modules PART-OF MAU. fusion module HYPONYM-OF modules. attention module HYPONYM-OF modules. attention module PART-OF MAU. fusion module PART-OF MAU. attention module USED-FOR attention map. historical temporal states PART-OF augmented motion information ( AMI ). attention map USED-FOR historical temporal states. predictive unit USED-FOR temporal dynamics. receptive field USED-FOR predictive unit. receptive field USED-FOR temporal dynamics. fusion module USED-FOR augmented motion information ( AMI ). unit USED-FOR predictive models. encoders CONJUNCTION decoders. decoders CONJUNCTION encoders. information recalling scheme USED-FOR encoders. information recalling scheme USED-FOR decoders. video prediction CONJUNCTION early action recognition tasks. early action recognition tasks CONJUNCTION video prediction. early action recognition tasks EVALUATE-FOR MAU. video prediction EVALUATE-FOR MAU. MAU COMPARE methods. methods COMPARE MAU. tasks EVALUATE-FOR methods. tasks EVALUATE-FOR MAU. OtherScientificTerm is historical spatial states. ,"This paper proposes a new method for predicting inter-frame motion information for video prediction tasks. The proposed Motion-Aware Unit (MAU) is a combination of two modules: a fusion module and an attention module. The attention module maps the temporal receptive field of the predictive units into historical spatial states, and the fusion module predicts the temporal dynamics from the receptive field. The predictive unit is trained to predict temporal dynamics using a predictive unit trained with the attention map. The unit is then used to train predictive models. Experimental results on video prediction and early action recognition tasks show that MAU outperforms other methods on these tasks. ","This paper proposes a Motion-Aware Unit (MAU) for predicting inter-frame motion information for video prediction tasks. The proposed MAU consists of three modules: a fusion module, an attention module, and an attention map that maps historical temporal states to historical spatial states. The temporal receptive field of the predictive units is modeled by a predictive unit that maps the temporal dynamics to the receptive field. The predictive unit is then used to train predictive models. The encoders and decoders are trained using an information recalling scheme. Experiments on video prediction and early action recognition tasks show that MAU outperforms other methods on both tasks."
8008,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"neural net approximation of the Q function USED-FOR Deep Reinforcement Learning ( RL ). neural net approximations USED-FOR nonlinear RL. ReLU and polynomial activation functions PART-OF two - layer neural networks. two - layer neural networks USED-FOR function approximation. algorithm USED-FOR generative model setting. algebraic dimension FEATURE-OF sample complexity. Task is RL. OtherScientificTerm are neural net function class, and deterministic dynamics. Method is linear ( or eluder dimension ) methods. ",This paper proposes a neural net approximation of the Q function in Deep Reinforcement Learning (RL) for nonlinear RL. The function approximation is based on two-layer neural networks with ReLU and polynomial activation functions. The authors propose an algorithm for the generative model setting where the neural net function class is a deterministic dynamics and the sample complexity is bounded by an algebraic dimension. They show that the proposed linear (or eluder dimension) methods perform better than previous work.,"This paper proposes a neural net approximation of the Q function for Deep Reinforcement Learning (RL). The authors propose to use neural net approximations for nonlinear RL, where the neural net function class is a deterministic dynamics. The function approximation is done using two-layer neural networks with ReLU and polynomial activation functions. The authors show that the sample complexity of the function approximation has an algebraic dimension with respect to the number of parameters of the neural network function class, which is lower than linear (or eluder dimension) methods. The algorithm is applied to the generative model setting."
8024,SP:cac881243abde92a28c110f5bd84d115ed189bda,"representations USED-FOR zero - shot transfer. Deep Metric Learning ( DML ) USED-FOR representations. priori unknown test distributions USED-FOR zero - shot transfer. ooDML benchmark USED-FOR generalization. ooDML USED-FOR generalization. ooDML USED-FOR train - to - test distribution shifts. benchmark EVALUATE-FOR DML methods. few - shot DML USED-FOR generalization. unknown test shifts FEATURE-OF generalization. OtherScientificTerm are distribution shifts, train - test splits, out - of - distribution shifts, and distribution shift. Method are DML, and ooDML1. Generic is methods. ","This paper studies the problem of zero-shot transfer with priori unknown test distributions in Deep Metric Learning (DML) from the perspective of representations. The authors propose a new ooDML benchmark to evaluate the generalization performance of the proposed OODML on the train-to-test distribution shifts. The main contribution of the paper is to show that the few-shot DML can achieve better generalization on unknown test shifts than other DML methods on the benchmark.  The authors also provide a theoretical analysis of the distribution shifts in DML, showing that train-test splits are the main cause of out-of-distribution shifts, and that the distribution shift can be used to improve the performance of other methods. ","This paper proposes a new representation learning method for zero-shot transfer based on Deep Metric Learning (DML). The key idea is to use priori unknown test distributions to learn representations that are invariant to distribution shifts in the training data. The authors propose two methods for training DML, namely ooDML1 and oOdML2. The main contribution of the paper is the introduction of a new oo-DML benchmark for evaluating the generalization of DML methods on unknown test shifts. The paper shows that few-shot DML can achieve better generalization on the unknown test shift compared to other methods. "
8040,SP:bacff3685476855a32549d03095375649fd89df2,"outlier detection algorithm CONJUNCTION hyperparameter(s ). hyperparameter(s ) CONJUNCTION outlier detection algorithm. model HYPONYM-OF hyperparameter(s ). data - driven approach USED-FOR UOMS. METAOD HYPONYM-OF data - driven approach. meta - learning USED-FOR data - driven approach. meta - learning USED-FOR METAOD. model selection USED-FOR clustering. UOMS problem COMPARE model selection. model selection COMPARE UOMS problem. model evaluations CONJUNCTION model comparisons. model comparisons CONJUNCTION model evaluations. model USED-FOR dataset. METAOD USED-FOR model. detection models USED-FOR METAOD. historical outlier detection benchmark datasets EVALUATE-FOR detection models. meta - learning framework USED-FOR task similarity. metafeatures USED-FOR task similarity. meta - learning techniques USED-FOR UOMS. model COMPARE model selection. model selection COMPARE model. METAOD COMPARE model selection. model selection COMPARE METAOD. model selection COMPARE meta - learning techniques. meta - learning techniques COMPARE model selection. METAOD USED-FOR model. meta-)training EVALUATE-FOR METAOD. METAOD CONJUNCTION meta - learning database. meta - learning database CONJUNCTION METAOD. METAOD USED-FOR UOMS problem. meta - learning database USED-FOR UOMS problem. Task are unsupervised outlier detection task, unsupervised outlier model selection ( UOMS ) problem, model evaluation, and model comparison. OtherScientificTerm is universal objective function. Generic is task. ","This paper studies the unsupervised outlier detection task. The authors propose a data-driven approach called METAOD, which combines the outlier detector algorithm and the hyperparameter(s) of the model with meta-learning to improve the performance of UOMS. The proposed approach is based on the idea that the model should be able to learn a universal objective function, which can be used for clustering. The model is trained on a dataset with a set of different detection models, and the goal is to find the best model for each of these detection models. The paper shows that METAod is able to outperform the state-of-the-art in (meta-)training, and outperforms the state of the art in (unsupervised) outlier model selection (UOMS) problem. ","The paper proposes a new unsupervised outlier detection task, which is an extension of the outlier model selection (UOMS) problem. The authors propose a data-driven approach, called METAOD, which combines meta-learning with the data-based approach, UOMS, to learn a universal objective function for the task. The paper presents a dataset and model evaluation, model comparisons, and model selection for clustering. The model is trained on the dataset and the model is evaluated on the meta-learned dataset. The results show that the proposed model outperforms the model selection in terms of model selection and model comparison.  The paper also provides a meta-training framework to improve the task similarity between the two tasks by using the metafeatures of the two task similarity. The experiments are conducted on two historical outlier detector benchmark datasets, where the detection models are trained on two different types of detection models. The experimental results show the proposed UOMs problem can be solved with a meta -learning database, and the proposed method is able to outperform model selection compared to model selection. "
8056,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"Prediction+optimization HYPONYM-OF real - world paradigm. SPO+ CONJUNCTION direct optimization. direct optimization CONJUNCTION SPO+. direct optimization HYPONYM-OF decision - focused prediction approaches. SPO+ HYPONYM-OF decision - focused prediction approaches. max operator USED-FOR real - world objectives. max operator USED-FOR soft constraints. framework USED-FOR closed - form solution. predictive parameters CONJUNCTION gradients. gradients CONJUNCTION predictive parameters. predictive parameters USED-FOR closed - form solution. gradients USED-FOR closed - form solution. synthetic linear programming CONJUNCTION portfolio optimization. portfolio optimization CONJUNCTION synthetic linear programming. portfolio optimization CONJUNCTION resource provisioning. resource provisioning CONJUNCTION portfolio optimization. method COMPARE decision - focused approaches. decision - focused approaches COMPARE method. two - staged methods CONJUNCTION decision - focused approaches. decision - focused approaches CONJUNCTION two - staged methods. method COMPARE two - staged methods. two - staged methods COMPARE method. applications EVALUATE-FOR method. method COMPARE method. method COMPARE method. soft constraints USED-FOR method. soft constraints FEATURE-OF applications. applications EVALUATE-FOR method. resource provisioning HYPONYM-OF applications. synthetic linear programming HYPONYM-OF applications. portfolio optimization HYPONYM-OF applications. Task are optimization problem, and downstream optimization problem. Method are prediction model, and analytically differentiable surrogate objective framework. Generic is they. OtherScientificTerm are soft linear and non - negative hard constraints, and theoretical bounds. ","This paper proposes a new real-world paradigm called Prediction+optimization, which combines SPO+ and direct optimization with decision-focused prediction approaches such as SPO+. The main idea is to use the prediction model as a surrogate objective framework to solve the optimization problem, where the goal is to find a closed-form solution with predictive parameters and gradients that satisfy soft constraints on the soft linear and non-negative hard constraints. The authors provide theoretical bounds on the theoretical bounds and show that they are consistent with the theoretical results. They also show that the proposed method can be applied to a variety of applications such as synthetic linear programming, portfolio optimization, resource provisioning, and two-staged methods. ","This paper proposes a new real-world paradigm, Prediction+optimization, which is a combination of SPO+ and direct optimization. The main idea is to use a prediction model to solve a downstream optimization problem. The authors propose an analytically differentiable surrogate objective framework, where the prediction model is a closed-form solution with predictive parameters and gradients. The max operator is used to define soft constraints on the real -world objectives, which are soft linear and non-negative hard constraints. The theoretical bounds on the soft constraints are also provided. The proposed framework is evaluated on two applications: synthetic linear programming and portfolio optimization. Results show that the proposed method outperforms decision-focused approaches and two-staged methods."
8072,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"graph USED-FOR GNN. GNN USED-FOR DropGNNs. DropGNNs USED-FOR graph neighborhoods. GNN benchmarks EVALUATE-FOR DropGNNs. Method are Dropout Graph Neural Networks ( DropGNNs ), GNN frameworks, and message passing GNNs. Generic is approach. OtherScientificTerm is theoretical bounds. Metric is expressiveness. ",This paper proposes a new approach to improve the expressiveness of Dropout Graph Neural Networks (DropGNNs) by using graph as a GNN. DropGNN is a family of GNNs that can be used as graph neighborhoods for graph neighborhoods. The main idea is to use graph as the GNN for the dropout graph. The authors show that the proposed approach is able to achieve better expressiveness than existing GNN frameworks. The theoretical bounds are also provided. The experimental results show the effectiveness of the proposed dropout GNN on various GNN benchmarks.,"This paper proposes Dropout Graph Neural Networks (DropGNNs), a new approach to learning graph GNNs. DropGNN is a GNN that can be used to learn graph neighborhoods. The authors propose two GNN frameworks, DropGnns and GNN with message passing GNN. They show theoretical bounds on the expressiveness of the proposed approach. Experiments on several GNN benchmarks show the effectiveness of the DropgNNs on graph neighborhoods, and show that the proposed dropout graph neural networks achieve better expressiveness. "
8088,SP:090dc0471d54e237f423034b1e1c46a510202807,"Transformers USED-FOR visual tasks. global representation capacities FEATURE-OF Transformers. local and global pattern features USED-FOR image classification. representation capacity FEATURE-OF local and global pattern features. DS - Net USED-FOR fine - grained and integrated features. DS - Net USED-FOR them. Inter - Scale Alignment module USED-FOR information interaction. Intra - scale Propagation module CONJUNCTION Inter - Scale Alignment module. Inter - Scale Alignment module CONJUNCTION Intra - scale Propagation module. Intra - scale Propagation module USED-FOR resolutions. Intra - scale Propagation module USED-FOR information interaction. contextual information USED-FOR downstream dense predictions. Vision Transformers CONJUNCTION ResNets. ResNets CONJUNCTION Vision Transformers. DS - Net COMPARE DeiT - Small. DeiT - Small COMPARE DS - Net. DS - Net COMPARE Vision Transformers. Vision Transformers COMPARE DS - Net. DS - Net COMPARE ResNets. ResNets COMPARE DS - Net. ImageNet-1k EVALUATE-FOR DeiT - Small. top-1 accuracy EVALUATE-FOR DeiT - Small. ImageNet-1k EVALUATE-FOR DS - Net. top-1 accuracy EVALUATE-FOR DS - Net. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. DS - Net - Small COMPARE ResNet-50. ResNet-50 COMPARE DS - Net - Small. DS - Net - Small USED-FOR object detection. mAP EVALUATE-FOR DS - Net - Small. DS - Net - Small USED-FOR instance segmentation. OtherScientificTerm are high - level local pattern information, features, and dual scales. Material is MSCOCO 2017. Generic is state - of - the - art scheme. Task is vision tasks. ",This paper proposes a new state-of-the-art scheme for image classification based on local and global pattern features. The key idea is to combine the high-level local pattern information with the global representation capacities of Transformers to improve the performance of visual tasks. The proposed DS-Net learns fine-grained and integrated features and uses them to train them. The Intra-scale Propagation module and the Inter-Scale Alignment module are used to learn the resolutions and the information interaction between the two features.  The authors show that DS-net-Small improves the top-1 accuracy on ImageNet-1k and ResNets compared to Vision Transformers and DeiT-Small on mAP.  ,"This paper proposes a new state-of-the-art scheme for image classification, which combines local and global pattern features in image classification. The key idea is to learn high-level local pattern information, which is then used to generate fine-grained and integrated features, and then use them in Transformers for visual tasks. The representation capacity of the local-and-global pattern features is measured by the sum of the global representation capacities of the Transformers. The proposed resolutions are defined by the Intra-scale Propagation module and the Inter-Scale Alignment module for information interaction. The features are learned at dual scales. The contextual information is used for downstream dense predictions. Experiments on ImageNet-1k, DeiT-Small, and ResNets show that DS-Net achieves better top-1 accuracy than Vision Transformers, Vision Transformers on mAP and ResNet-50 on object detection and instance segmentation. "
8104,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,unified framework USED-FOR visual concepts. unified framework USED-FOR physics models of objects. visual concepts CONJUNCTION physics models of objects. physics models of objects CONJUNCTION visual concepts. videos USED-FOR physics models of objects. visual perception module CONJUNCTION concept learner. concept learner CONJUNCTION visual perception module. concept learner CONJUNCTION differentiable physics engine. differentiable physics engine CONJUNCTION concept learner. differentiable physics engine PART-OF components. visual perception module PART-OF components. concept learner HYPONYM-OF components. latent scene representations USED-FOR them. color CONJUNCTION shape. shape CONJUNCTION color. shape CONJUNCTION material. material CONJUNCTION shape. prior knowledge USED-FOR physics engine. concept learner USED-FOR visual concepts. language USED-FOR object - centric representations. object - centric representations USED-FOR visual concepts. material HYPONYM-OF visual concepts. color HYPONYM-OF visual concepts. shape HYPONYM-OF visual concepts. mass CONJUNCTION restitution. restitution CONJUNCTION mass. restitution CONJUNCTION velocity. velocity CONJUNCTION restitution. differentiable physical simulation USED-FOR physical properties. differentiable physical simulation USED-FOR differentiable physics model. video observations USED-FOR simulated trajectories. impulse - based differentiable rigid - body simulator USED-FOR differentiable physics model. grounded concepts USED-FOR differentiable physical simulation. velocity HYPONYM-OF physical properties. mass HYPONYM-OF physical properties. restitution HYPONYM-OF physical properties. concepts CONJUNCTION physical models. physical models CONJUNCTION concepts. differentiable physics PART-OF dynamic reasoning framework. VRDP COMPARE counterpart. counterpart COMPARE VRDP. accuracy EVALUATE-FOR predictive and counterfactual questions. physics models USED-FOR dynamics prediction. predictive and counterfactual questions EVALUATE-FOR VRDP. synthetic and real - world benchmarks EVALUATE-FOR dynamics prediction. accuracy EVALUATE-FOR VRDP. VRDP USED-FOR concepts. physical parameters USED-FOR VRDP. OtherScientificTerm is object - centric trajectories. Metric is interpretability. ,"This paper proposes a unified framework for learning visual concepts and physics models of objects from videos. The proposed framework consists of three components: a differentiable physics engine, a concept learner, a visual perception module, and a visual recognition module. The physics engine is based on prior knowledge from prior knowledge of the physics engine. The visual perception modules are based on latent scene representations, and the visual concepts are learned from object-centric representations. The physical parameters of the VRDP are learned by differentiable physical simulation, which is a dynamic reasoning framework based on grounded concepts. Experiments on synthetic and real-world benchmarks show that the proposed VRDP achieves better accuracy than its counterpart. ","This paper proposes a unified framework for learning visual concepts and physics models of objects from videos. The key idea is to use object-centric representations as object-centric representations, and then use them as latent scene representations. The proposed dynamic reasoning framework is based on differentiable physics in a differentiable physical simulation, where prior knowledge is used to train the physics engine and the concept learner is used as a visual perception module. Experiments on synthetic and real-world benchmarks show that the proposed VRDP is able to learn concepts and physical models, and can also learn the physical parameters of the VRDP. The authors also conduct experiments on simulated trajectories from video observations, showing that VRDP achieves better interpretability and accuracy on predictive and counterfactual questions. "
8120,SP:c511066c38f9793bacb4986c564eafa36e032f39,"Active learning USED-FOR minimizing labeling costs. out - of - distribution data CONJUNCTION redundancy. redundancy CONJUNCTION out - of - distribution data. submodular information measures ( SIM ) USED-FOR acquisition functions. acquisition functions USED-FOR unified active learning framework. submodular information measures ( SIM ) USED-FOR unified active learning framework. one - stop solution USED-FOR active learning. SIMILAR USED-FOR active learning. SIMILAR COMPARE active learning algorithms. active learning algorithms COMPARE SIMILAR. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification tasks. CIFAR-10 HYPONYM-OF image classification tasks. MNIST HYPONYM-OF image classification tasks. DISTIL toolkit USED-FOR SIMILAR. Method are active learning methods, and Submodular Information Measures based actIve LeARning. OtherScientificTerm are imbalance or rare classes, and rare classes. Material is large real - world datasets. ","This paper proposes a unified active learning framework based on submodular information measures (SIM) for minimizing labeling costs in the presence of imbalance or rare classes. The authors propose a one-stop solution for active learning, which is based on the one-stopper solution in the DISTIL toolkit. SimILAR is shown to outperform existing active learning algorithms on several image classification tasks such as MNIST, CIFAR-10, and ImageNet. ","This paper proposes a unified active learning framework based on submodular information measures (SIM) for minimizing labeling costs. SIMILAR is a one-stop solution for active learning with out-of-distribution data and redundancy. The authors propose to use the DISTIL toolkit to learn active learning algorithms that are more robust to imbalance or rare classes. They also propose a new active learning methods based on Submodular Information Measures based actIve LeARning. Experiments on large real-world datasets are conducted on MNIST, ImageNet, and CIFAR-10."
8136,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"identity tests USED-FOR ranking data. Mallows model USED-FOR asymptotic and non - asymptotic settings. Mallows model USED-FOR ranking data. algorithms USED-FOR spread parameter. spread parameter FEATURE-OF Mallows model. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR asymptotic setting. sample - optimal non - asymptotic identity test USED-FOR it. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR one. distribution of the sufficient statistic USED-FOR it. optimal learning algorithm USED-FOR Mallows model. optimal learning algorithm USED-FOR nonasymptotic test. Mallows models USED-FOR unknown central ranking case. asymptotic setting USED-FOR case. OtherScientificTerm is central ranking. Generic are test, and tests. Material is medium sized data. ","This paper proposes a new identity test for ranking data. The test is based on the Uniformly Most Powerful Unbiased (UMPU) test for the asymptotic setting, and it uses a sample-optimal non-asymptotic identity test to evaluate the performance of the Mallows model on ranking data in both the standard and non-symmetric settings. The authors show that the spread parameter of a Mallows-based model can be approximated by algorithms based on a distribution of the sufficient statistic. They also show that this test can be applied to the unknown central ranking case, and that it can be used as an optimal learning algorithm for a nonasymic test. ","This paper proposes a new test for identity tests for ranking data. The test is based on the Uniformly Most Powerful Unbiased (UMPU) test for asymptotic and non-asymptotic settings. The main idea is to use a Mallows model to estimate the distribution of the sufficient statistic for the ranking data, and then use algorithms to compute the spread parameter of the Mallows network. The authors also propose a sample-optimal non-asymptotics identity test to evaluate it. The results show that the test outperforms other tests on medium sized data. "
8152,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"sparse multi - view cameras USED-FOR free - viewpoint video. pixel - aligned features USED-FOR radiance fields. heavy occlusions CONJUNCTION dynamic articulations of body parts. dynamic articulations of body parts CONJUNCTION heavy occlusions. parametric human body model USED-FOR robust performance capture. approach USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR approach. temporal transformer USED-FOR tracked visual features. skeletal body motion USED-FOR temporal transformer. skeletal body motion USED-FOR tracked visual features. multi - view transformer USED-FOR cross - attention. temporally - fused features CONJUNCTION pixel - aligned features. pixel - aligned features CONJUNCTION temporally - fused features. ZJU - MoCap and AIST datasets EVALUATE-FOR method. method COMPARE generalizable NeRF methods. generalizable NeRF methods COMPARE method. ZJU - MoCap and AIST datasets EVALUATE-FOR generalizable NeRF methods. OtherScientificTerm is appearance. Method are generalization approaches, and Neural Human Performer. ",This paper proposes a new approach for robust performance capture using sparse multi-view cameras for free-view video. The proposed approach is based on a parametric human body model to learn generalizable neural radiance fields using pixel-aligned features and pixel-aligned features. The paper also proposes a temporal transformer to capture tracked visual features using skeletal body motion and dynamic articulations of body parts. The cross-attention is achieved by using a multi- view transformer. The method is evaluated on ZJU-MoCap and AIST datasets and outperforms other generalizable NeRF methods. ,This paper proposes a new approach to learn generalizable neural radiance fields from sparse multi-view cameras for free-view video. The approach is based on a parametric human body model for robust performance capture. The key idea is to use a temporal transformer to generate tracked visual features from a skeletal body motion and dynamic articulations of body parts. The proposed method is evaluated on the ZJU-MoCap and AIST datasets and outperforms other generalizable NeRF methods. The authors also show that the proposed approach is able to learn temporally-fused features as well as pixel-aligned features. 
8168,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"recognition CONJUNCTION detection. detection CONJUNCTION recognition. Vision Transformer USED-FOR vision tasks. detection HYPONYM-OF vision tasks. recognition HYPONYM-OF vision tasks. neural architecture search USED-FOR process. architecture CONJUNCTION search space. search space CONJUNCTION architecture. E - T Error USED-FOR search dimensions. weight - sharing supernet USED-FOR E - T Error. weight - sharing supernet USED-FOR search dimensions. Swin CONJUNCTION DeiT. DeiT CONJUNCTION Swin. DeiT CONJUNCTION ViT. ViT CONJUNCTION DeiT. searched models COMPARE models. models COMPARE searched models. searched space COMPARE models. models COMPARE searched space. searched space USED-FOR searched models. S3 HYPONYM-OF searched models. ViT HYPONYM-OF models. Swin HYPONYM-OF models. DeiT HYPONYM-OF models. ImageNet EVALUATE-FOR models. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. semantic segmentation CONJUNCTION visual question answering. visual question answering CONJUNCTION semantic segmentation. S3 USED-FOR vision and vision - language tasks. semantic segmentation EVALUATE-FOR S3. visual question answering EVALUATE-FOR S3. object detection EVALUATE-FOR S3. Generic is architectures. Method is vision transformers. Task are space searching process, and vision transformer. OtherScientificTerm is Search Space. ","This paper studies the space searching process of vision transformers. The main idea is to use neural architecture search to learn a process that can be applied to a variety of vision tasks such as detection, detection, and detection with semantic segmentation. The proposed architecture and the search space are trained using a weight-sharing supernet to learn the search dimensions of the E-T Error. The search space is then used to train the architecture and search space. Experiments on ImageNet show that the proposed models perform better than other searched models such as S3, ViT, DeiT, and Swin. ","This paper proposes a Vision Transformer for vision tasks such as detection, detection, and semantic segmentation. The proposed process is based on neural architecture search. The architecture and the search space are modeled by a weight-sharing supernet. The search space is modeled by an E-T Error. The space searching process is performed by a vision transformer. The architectures are evaluated on ImageNet and compared to other models such as Swin, DeiT, ViT, and searched models. The results show that Search Space outperforms other models in both vision and vision-language tasks."
8184,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"learning from label proportions ( LLP ) framework USED-FOR linear threshold functions ( LTFs ). algorithm USED-FOR LTF. algorithm USED-FOR LTF. d - dimensional boolean vectors USED-FOR OR. accuracy EVALUATE-FOR LTF. LTFs USED-FOR monotone ORs. LTF HYPONYM-OF algorithm. unit - sized bags HYPONYM-OF supervised learning setup. accuracy EVALUATE-FOR algorithm. linear programming USED-FOR LTFs. techniques USED-FOR LLP setting. LTFs USED-FOR LLP learning of LTFs. LLP CONJUNCTION supervised learning. supervised learning CONJUNCTION LLP. inapproximability EVALUATE-FOR LLP learning LTFs. OtherScientificTerm are bags of feature - vectors, label proportions, bags, labeled feature - vectors, non - monochromatic bags, monotone OR, and non - monochromatic bags case. Metric are algorithmic bounds, and complexity. Generic is bound. ","This paper proposes a new learning from label proportions (LLP) framework for linear threshold functions (LTFs). The algorithm is based on the LTF with d-dimensional boolean vectors, where the bags of feature-vices are the labels of the function. The authors show that the algorithm is able to learn an LTF in a supervised learning setup with unit-sized bags. They also provide algorithmic bounds on the complexity of the bound.   ","The paper proposes a learning from label proportions (LLP) framework for linear threshold functions (LTFs). The algorithm is based on LTF with d-dimensional boolean vectors. The authors propose a supervised learning setup with unit-sized bags, where the bags of feature-vices are monotone, and the labels of the bags are non-monochromatic bags. They show that the algorithm can learn LTFs with monotsone ORs. They also provide algorithmic bounds on the complexity of the algorithm. The paper also shows that the proposed algorithm can be used in the LLP setting with linear programming. Finally, the authors show that their bound is inapproximable for LLP learning of LTF."
8200,SP:2eb193c76355aac08003c9b377895202fd3bd297,extreme computational resources USED-FOR neural architecture search ( NAS ). benchmarks EVALUATE-FOR multi - fidelity techniques. learning curve extrapolation HYPONYM-OF multi - fidelity techniques. NAS - Bench-111 CONJUNCTION NAS - Bench-311. NAS - Bench-311 CONJUNCTION NAS - Bench-111. method USED-FOR surrogate benchmarks. NAS - Bench-311 CONJUNCTION NAS - Bench - NLP11. NAS - Bench - NLP11 CONJUNCTION NAS - Bench-311. singular value decomposition and noise modeling USED-FOR method. NAS - Bench-111 HYPONYM-OF surrogate benchmarks. NAS - Bench - NLP11 HYPONYM-OF surrogate benchmarks. NAS - Bench-311 HYPONYM-OF surrogate benchmarks. learning curve extrapolation framework USED-FOR single - fidelity algorithms. it COMPARE single - fidelity algorithms. single - fidelity algorithms COMPARE it. Material is tabular and surrogate benchmarks. Generic is architecture. OtherScientificTerm is architectures. Metric is validation accuracy. ,"This paper studies the problem of neural architecture search (NAS) with extreme computational resources. The authors propose a novel method for learning surrogate benchmarks, NAS-Bench-111 and NAS-bench-311, which are multi-fidelity techniques such as learning curve extrapolation. The proposed method is based on singular value decomposition and noise modeling, and it is shown that it outperforms single-fibre algorithms in terms of validation accuracy. ",This paper proposes a novel framework for evaluating the performance of neural architecture search (NAS) on extreme computational resources. The proposed method is based on singular value decomposition and noise modeling. The authors show that the proposed method outperforms existing multi-fidelity techniques such as learning curve extrapolation and NAS-Bench-NLP11 on a variety of benchmarks. They also show that it outperforms single-faithfulness algorithms on tabular and surrogate benchmarks. 
8216,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"user - centred method USED-FOR example - based explanations. SimplEx HYPONYM-OF user - centred method. corpus USED-FOR SimplEx. Integrated Jacobian HYPONYM-OF approach. mortality prediction CONJUNCTION image classification. image classification CONJUNCTION mortality prediction. tasks EVALUATE-FOR decompositions. image classification HYPONYM-OF tasks. mortality prediction HYPONYM-OF tasks. Method are machine learning models, convoluted latent representations, mixture of corpus latent representations, and model representations. Generic are latent representations, model, and mixture. OtherScientificTerm are latent space, post - hoc explanations, and features. ","This paper proposes a user-centred method for example-based explanations based on SimplEx. SimplEx is an approach based on Integrated Jacobian, where the latent space is represented as a mixture of two separate latent representations, and the model is trained on a corpus of these two latent representations. The model is then used to generate post-hoc explanations. The decompositions are then used for a variety of tasks, including mortality prediction, image classification, etc. The authors show that SimplEx can achieve better performance than existing machine learning models on all of these tasks.","This paper proposes a user-centred method for example-based explanations, SimplEx. The approach is based on Integrated Jacobian, where the latent space is represented as a mixture of post-hoc explanations, and the model is trained on the mixture. SimplEx uses a corpus to learn the model representations. The decompositions are evaluated on three tasks: mortality prediction, image classification, and word embedding. "
8232,SP:c8f82ec90f891d7394933483b7f926155ac363ef,image - text pairs USED-FOR multi - modal representations. Transformer USED-FOR images. CNN USED-FOR images. CNN - Transformer architecture USED-FOR VLP models. Visual relationship between visual contents USED-FOR image understanding. CNNs USED-FOR visual relation learning. visual relation CONJUNCTION inter - modal alignment. inter - modal alignment CONJUNCTION visual relation. objectives PART-OF Transformer network. learning visual relation PART-OF Transformer network. inter - modal alignment PART-OF Transformer network. learning visual relation HYPONYM-OF objectives. inter - modal alignment HYPONYM-OF objectives. design USED-FOR inter - modal alignment learning. Transformer USED-FOR inter - modal alignment learning. fully Transformer visual embedding USED-FOR inter - modal alignment. fully Transformer visual embedding USED-FOR VLP. fully Transformer visual embedding USED-FOR visual relation. Inter - Modality Flow ( IMF ) HYPONYM-OF metric. masking optimization mechanism USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR Transformer. masking optimization mechanism PART-OF Transformer. Masked Feature Regression ( MFR ) HYPONYM-OF masking optimization mechanism. Transformer USED-FOR visual feature learning in VLP. Visual Entailment CONJUNCTION Visual Reasoning. Visual Reasoning CONJUNCTION Visual Entailment. Visual Question Answering ( VQA ) CONJUNCTION Visual Entailment. Visual Entailment CONJUNCTION Visual Question Answering ( VQA ). Image - Text Retrieval CONJUNCTION Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) CONJUNCTION Image - Text Retrieval. vision - language tasks EVALUATE-FOR method. Visual Reasoning HYPONYM-OF vision - language tasks. Image - Text Retrieval HYPONYM-OF vision - language tasks. Visual Question Answering ( VQA ) HYPONYM-OF vision - language tasks. Visual Entailment HYPONYM-OF vision - language tasks. approach COMPARE V,"This paper proposes a CNN-Transformer architecture for VLP models. Transformer is a multi-modal representation of image-text pairs, which can be used to learn multi- modal representations of images. Visual relationship between visual contents can be important for image understanding. The authors propose a new metric called Inter-Modality Flow (IMF) to measure the inter-modality flow between images and text pairs. The paper also proposes a masking optimization mechanism to improve the performance of the Transformer network in terms of learning visual relation, learning visual relations between images, and inter-Modal alignment. The proposed method is evaluated on vision-language tasks such as Visual Entailment, Visual Question Answering (VQA) and Image-Text Retrieval. ","This paper proposes a CNN-Transformer architecture for learning multi-modal representations from image-text pairs. The authors propose a new metric called Inter-Modality Flow (IMF) which measures the relationship between visual contents between images and text pairs. They show that the proposed model can learn the inter-modality alignment between two images and a text pair. The proposed method is evaluated on vision-language tasks such as Visual Entailment, Visual Question Answering (VQA), Visual Reasoning, and Image-Text Retrieval. "
8248,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"information leakage FEATURE-OF iterative randomized learning algorithm. model USED-FOR information leakage. noisy gradient descent algorithms USED-FOR problem. Rényi divergence FEATURE-OF probability distributions. probability distributions FEATURE-OF models. smooth and strongly convex loss functions COMPARE composition theorems. composition theorems COMPARE smooth and strongly convex loss functions. noisy gradient descent algorithms USED-FOR optimal utility. gradient complexity FEATURE-OF optimal utility. Generic is algorithm. OtherScientificTerm are dynamics of Rényi differential privacy loss, privacy loss, and intermediate gradient computations. ","This paper studies the problem of information leakage in an iterative randomized learning algorithm with noisy gradient descent algorithms. The authors consider the dynamics of Rényi differential privacy loss, where the model is trained to minimize the information leakage. They show that the probability distributions of the models with the same distribution of the Rénye divergence are differentiable, and that the optimal utility of the algorithm is a function of the gradient complexity. They also show that smooth and strongly convex loss functions are more efficient than composition theorems. Finally, they show that for intermediate gradient computations, the privacy loss can be reduced.","This paper studies the problem of information leakage in an iterative randomized learning algorithm. The authors propose a new model to mitigate information leakage. The proposed algorithm is based on the dynamics of Rényi differential privacy loss, where the privacy loss is defined as the divergence between the probability distributions of the two models with respect to the Rényis divergence. The paper shows that the proposed algorithm can achieve the optimal utility with a small gradient complexity. The main contribution of the paper is to study the problem with noisy gradient descent algorithms for the problem. The results show that smooth and strongly convex loss functions are more robust to information leakage than composition theorems. "
8264,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,large - scale machine learning CONJUNCTION embedded optimal control. embedded optimal control CONJUNCTION large - scale machine learning. First - order methods USED-FOR large - scale machine learning. First - order methods USED-FOR quadratic optimization. First - order methods USED-FOR embedded optimal control. OSQP HYPONYM-OF First - order methods. OSQP HYPONYM-OF quadratic optimization. manual hyperparameter tuning CONJUNCTION convergence time. convergence time CONJUNCTION manual hyperparameter tuning. Reinforcement Learning ( RL ) USED-FOR policy. RL policy COMPARE QP solvers. QP solvers COMPARE RL policy. RLQP COMPARE QP solvers. QP solvers COMPARE RLQP. RLQP HYPONYM-OF RL policy. RLQP USED-FOR problems. RLQP USED-FOR applications. Maros - Mészáros problems HYPONYM-OF applications. QPLIB HYPONYM-OF applications. Generic is methods. Material is QP benchmarks. ,This paper studies the problem of quadratic optimization with First-order methods such as OSQP and RLQP. The authors propose a new policy based on Reinforcement Learning (RL) that is able to learn a policy that is more efficient than QP solvers. They show that the RL policy can achieve state-of-the-art performance on the QP benchmarks. They also show that their RL policy outperforms the state of the art in a variety of applications such as Maros-Mészáros problems and QPLIB.,"This paper proposes a new method for large-scale machine learning with embedded optimal control. First-order methods for quadratic optimization (e.g., OSQP) and RLQP are proposed. The main idea is to use Reinforcement Learning (RL) to learn a policy that maximizes the convergence time and manual hyperparameter tuning. The RL policy outperforms QP solvers on QPLIB and other QP benchmarks. The paper also presents a number of applications of RLQPs on problems such as Maros-Mészáros problems."
8280,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"convergence rate EVALUATE-FOR model. PC - bias USED-FOR linear and non8 linear networks. PC - bias USED-FOR early stopping. early stopping USED-FOR PCA. random labels USED-FOR deep networks. Method are convolutional neural networks, and over - parametrized deep linear network model. OtherScientificTerm are asymptotic analysis, hidden layers, principal components, singular 6 values, convergence pattern, Principal Components bias ( PC - bias ), and spectral bias. Task is learning. Generic is biases. ","This paper studies the convergence rate of a deep linear network model with random labels. The authors consider the case of convolutional neural networks with hidden layers, where the principal components have singular 6 values and the asymptotic analysis is performed. They show that PC-bias in linear and non8 linear networks can be used as an early stopping for PCA. They also show that the convergence pattern of the model can be approximated by random labels in deep networks. ","This paper studies the convergence rate of convolutional neural networks. The authors propose an over-parametrized deep linear network model with asymptotic analysis. They show that the model converges to a singular 6 values with a high convergence rate. They also show that PC-bias in linear and non8 linear networks leads to early stopping for PCA, which is a variant of the Principal Components bias (PC-Bias), which is used for early stopping in PCA. The paper also shows that the convergence pattern of the model is a function of the number of hidden layers in the network and the principal components of the network. "
8296,SP:1598bad835a657e56af3261501c671897b7e9ffd,"Backdoor attack HYPONYM-OF deep neural networks ( DNNs ). defense methods USED-FOR detecting or erasing backdoors. anti - backdoor learning USED-FOR clean models. backdoor - poisoned data USED-FOR clean models. dual - task USED-FOR learning process. models USED-FOR backdoored data. backdoored data USED-FOR model. Anti - Backdoor Learning ( ABL ) USED-FOR backdoor attacks. learning scheme USED-FOR backdoor attacks. Anti - Backdoor Learning ( ABL ) HYPONYM-OF learning scheme. two - stage gradient ascent mechanism USED-FOR ABL. ABL - trained models COMPARE they. they COMPARE ABL - trained models. backdoor - poisoned data USED-FOR ABL - trained models. clean data USED-FOR ABL - trained models. clean data USED-FOR they. OtherScientificTerm are backdoor triggers, backdoor task, and backdoor target class. ","This paper studies the problem of detecting or erasing backdoors in deep neural networks (DNNs). The authors propose a new defense methods to defend against backdoor attacks. The proposed learning scheme, called Anti-Backdoor Learning (ABL) is based on the two-stage gradient ascent mechanism. The main idea is to learn a dual-task to guide the learning process. The backdoor triggers are learned by the backdoor task, and the model is trained on backdoored data. The authors show that ABL-trained models are able to detect backdoor-poisoned data and clean models with backdoor-protected data. ","This paper proposes a novel defense against backdoor attack in deep neural networks (DNNs). The authors propose two defense methods for detecting or erasing backdoors. The first defense method, Anti-Backdoor Learning (ABL), is a learning scheme for detecting backdoor attacks. ABL is based on the two-stage gradient ascent mechanism. The authors show that ABL outperforms clean models trained on backdoor-poisoned data and ABL-trained models on clean data. The second defense method is a dual-task, where the model is trained on backdoored data and the backdoor triggers are used to guide the learning process. The backdoor task is done in two stages: the first stage is to learn the backdoor target class, and the second stage is the backdoor task, which is to train the model on the models that have been trained on the backdoor data. "
8312,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"generative radiance fields USED-FOR 3Daware image synthesis. multi - view constraint USED-FOR 3D radiance fields. regularization USED-FOR 3D radiance fields. methods USED-FOR 3D radiance fields. multi - view constraint USED-FOR methods. 2D images USED-FOR 3D radiance fields. they USED-FOR 3D shapes. shading - guided generative implicit model USED-FOR shape representation. 3D shape USED-FOR realistic rendering. lighting conditions FEATURE-OF realistic rendering. lighting conditions FEATURE-OF shading. discriminator USED-FOR Gradients. surface tracking USED-FOR volume rendering strategy. approach USED-FOR photorealistic 3D - aware image synthesis. approach USED-FOR 3D shapes. approach COMPARE methods. methods COMPARE approach. approach USED-FOR image relighting. approach USED-FOR 3D shape reconstruction. OtherScientificTerm are shapecolor ambiguity, multi - lighting constraint, illumination, computational burden, and surface normals. Metric is training and inference time. ","This paper proposes a new approach for photorealistic 3D-aware image synthesis. The approach is based on a multi-view constraint on the 3D radiance fields of a 3D image. The authors propose to use a shading-guided generative implicit model to learn the shape representation. Gradients are generated by a discriminator that is trained on a set of 2D images, and the shapecolor ambiguity is used as a regularization. The paper shows that the lighting conditions for realistic rendering under lighting conditions are different for different 3D shapes, and that the proposed approach can achieve better performance than existing methods for 3D shape reconstruction.  The paper also proposes a volume rendering strategy based on surface tracking, which can be used to reduce the computational burden of training and inference time. ","This paper proposes a new approach for 3D shape reconstruction based on a multi-view constraint on 3D radiance fields. The authors propose to regularize the 3D Radiance fields of 2D images with respect to 3D images, which is an important step in 3Daware image synthesis. The proposed method is based on the shading-guided generative implicit model, where the shape representation is learned by a shapecolor ambiguity. Gradients are learned using a discriminator, and the lighting conditions of realistic rendering are modeled as lighting conditions. The paper also proposes a novel volume rendering strategy based on surface tracking. The approach is evaluated on a variety of 3D shapes, and is shown to outperform existing methods in terms of training and inference time. "
8328,SP:4b3dad77d79507c512877867dfea6db87a78682d,"flexible machine learning models USED-FOR instrumental variable ( IV ) regression. quasi - Bayesian procedure USED-FOR IV regression. kernelized IV models USED-FOR quasi - Bayesian procedure. Bayesian modeling USED-FOR IV. Bayesian modeling COMPARE approach. approach COMPARE Bayesian modeling. approach USED-FOR approximate inference algorithm. approximate inference algorithm COMPARE point estimation methods. point estimation methods COMPARE approximate inference algorithm. time cost EVALUATE-FOR point estimation methods. time cost EVALUATE-FOR approximate inference algorithm. algorithm USED-FOR neural network models. Method is uncertainty quantification methodology. OtherScientificTerm are data generating process, and quasi - posterior. Generic is method. ","This paper proposes a new uncertainty quantification methodology for instrumental variable (IV) regression with flexible machine learning models. The authors propose a quasi-Bayesian procedure for IV regression with kernelized IV models, which is based on Bayesian modeling of the data generating process. The proposed approach is shown to have a better approximate inference algorithm than existing point estimation methods in terms of time cost. The algorithm is also shown to improve the performance of neural network models.",This paper proposes a new uncertainty quantification methodology for instrumental variable (IV) regression with flexible machine learning models. The authors propose a quasi-Bayesian procedure for IV regression with kernelized IV models. They show that the proposed approach outperforms Bayesian modeling for IV with respect to the time cost of the data generating process. They also show that their approximate inference algorithm outperforms other point estimation methods in terms of time cost. The proposed algorithm can be applied to neural network models. 
8344,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,language - specific annotated data CONJUNCTION knowledge sources. knowledge sources CONJUNCTION language - specific annotated data. translation CONJUNCTION in - language retrieval modules. in - language retrieval modules CONJUNCTION translation. multilingual autoregressive generation model CONJUNCTION CORA. CORA CONJUNCTION multilingual autoregressive generation model. annotated data USED-FOR iterative training method. high - resource languages FEATURE-OF annotated data. multilingual open QA benchmarks EVALUATE-FOR CORA. cross - lingual retrieval CONJUNCTION generation. generation CONJUNCTION cross - lingual retrieval. Method is dense passage retrieval algorithm. OtherScientificTerm is low - resource ones. Material is low - resource settings. Generic is model. ,"This paper proposes a dense passage retrieval algorithm for multi-lingual language models. The authors propose a new iterative training method based on annotated data from high-resource languages and knowledge sources. The model is based on a multilingual autoregressive generation model, CORA, and in-language retrieval modules. The proposed model is evaluated on multilingual open QA benchmarks.","This paper proposes a novel iterative training method that uses annotated data and knowledge sources to train a multilingual autoregressive generation model and CORA on multilingual open QA benchmarks. The model is based on a dense passage retrieval algorithm. The authors show that the annotated language-specific annotated and high-resource languages are better than low-resource ones, and that the model is able to generalize to low-reward settings. The paper also shows that cross-lingual retrieval, generation, and in-language retrieval modules can be used to train the model."
8360,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"specialized training algorithms USED-FOR domain generalization. deep neural networks COMPARE specialized training algorithms. specialized training algorithms COMPARE deep neural networks. distribution shift FEATURE-OF deep neural networks. Empirical Risk Minimization ( ERM ) USED-FOR deep neural networks. domain generalization datasets USED-FOR ERM models. Fisher information CONJUNCTION predictive entropy. predictive entropy CONJUNCTION Fisher information. predictive entropy CONJUNCTION maximum mean discrepancy. maximum mean discrepancy CONJUNCTION predictive entropy. measures USED-FOR out - of - distribution generalization. measures CONJUNCTION predictive entropy. predictive entropy CONJUNCTION measures. out - of - distribution generalization EVALUATE-FOR ERM models. Fisher information FEATURE-OF measures. maximum mean discrepancy FEATURE-OF measures. deep networks USED-FOR out - of - distribution. ERM USED-FOR deep networks. Method are domain adaptation theory, and ERMs. Generic is theory. Task are out - of - domain generalization, and generalization. ","This paper studies the problem of out-of-domain generalization in deep neural networks with distribution shift. The authors propose Empirical Risk Minimization (ERM) to improve the generalization performance of ERM models on domain generalization datasets. They show that ERM improves the performance of deep networks on out- of-distribution generalization under Fisher information, predictive entropy, and maximum mean discrepancy. They also show that the ERM can be used to train deep networks to generalize better.","This paper proposes Empirical Risk Minimization (ERM) to improve the generalization performance of deep neural networks in the context of distribution shift. The main contribution of the paper is a theoretical analysis of ERM models on domain generalization datasets. The theory is based on the domain adaptation theory, and the authors show that ERMs can improve out-of-domain generalization. The authors also show that deep networks can generalize better in the case of distributions shift, and that ERM can be used to improve deep networks in terms of predictive entropy, Fisher information, predictive entropy and maximum mean discrepancy. The experimental results show that the proposed measures are able to improve ERM generalization in the sense that they can improve the performance of the deep networks."
8376,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,backdoor data poisoning attack HYPONYM-OF adversarial attack. watermarked examples USED-FOR model. backdoor data poisoning attacks USED-FOR classification problems. formal theoretical framework USED-FOR backdoor data poisoning attacks. this USED-FOR statistical and computational issues. statistical and computational issues FEATURE-OF attacks. intrinsic vulnerability FEATURE-OF learning problem. learning problem USED-FOR backdoor attack. memorization capacity HYPONYM-OF parameter. robustness FEATURE-OF natural learning problems. backdoor attacks FEATURE-OF natural learning problems. natural problem settings USED-FOR backdoor attacks. adversarial training USED-FOR backdoors. backdoor filtering CONJUNCTION robust generalization. robust generalization CONJUNCTION backdoor filtering. robust generalization HYPONYM-OF problems. backdoor filtering HYPONYM-OF problems. Generic is assumptions. Method is learning algorithm. ,"This paper proposes a new adversarial attack, backdoor data poisoning attack, which attacks the model with watermarked examples. The authors propose a formal theoretical framework to analyze backdoor attacks for classification problems, and show that this can be applied to both statistical and computational issues. They also show that the intrinsic vulnerability of the learning problem is a learning problem, and propose a new parameter, called the memorization capacity, which can be used to improve the performance of the proposed learning algorithm. Finally, the authors show that backdoor attacks can improve the robustness of natural learning problems, such as backdoor filtering and robust generalization, by adversarial training.","This paper proposes a novel adversarial attack, backdoor data poisoning attack, for classification problems. The authors propose a formal theoretical framework for detecting backdoor attacks on classification problems with watermarked examples. The main idea of the paper is to introduce a new parameter, the memorization capacity, which is used to train the model. The paper shows that this can be applied to both statistical and computational issues, and that the intrinsic vulnerability of the learning problem to the backdoor attack can be reduced by the learning algorithm. They also show that adversarial training can be used to prevent backdoors from being detected. They show that the proposed backdoor attacks can improve the robustness of natural learning problems with natural problem settings, such as backdoor filtering and robust generalization."
8392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"networks COMPARE ones. ones COMPARE networks. Large width limits PART-OF deep learning research. representational power FEATURE-OF networks. capacity CONJUNCTION width. width CONJUNCTION capacity. neural networks USED-FOR Deep Gaussian Processes ( Deep GP ). Deep Gaussian Processes ( Deep GP ) HYPONYM-OF nonparametric hierarchical models. neural nets HYPONYM-OF nonparametric hierarchical models. they USED-FOR modeling task. width USED-FOR neural networks. nonparametric Deep GP USED-FOR Gaussian processes. mixture of data - adaptable basis functions FEATURE-OF posterior. width CONJUNCTION depth. depth CONJUNCTION width. depth USED-FOR model. non - Gaussianity FEATURE-OF model. hidden units USED-FOR neural networks. L2 regularization USED-FOR neural networks. OtherScientificTerm are computational practicalities, GP behavior, adaptability, and Gaussian prior on parameters. Method are Deep GP, and hierarchical models. ","This paper studies the problem of large width limits in deep learning research. The authors consider the case of nonparametric hierarchical models such as Deep Gaussian Processes (Deep GP) and neural nets, where the representational power of the networks is bounded by the width and the width of the neural networks. Deep GP is a special case of Deep GP, and the authors show that they can be used for any modeling task where the number of hidden units in the network is large.  The authors also show that the width in neural networks can be reduced by L2 regularization, and that the depth of the model can be improved by using a mixture of data-adaptable basis functions in the posterior.  Finally, the authors provide a theoretical analysis of the computational practicalities of the GP behavior. ","This paper studies the problem of large width limits in deep learning research. The authors propose to use neural networks for Deep Gaussian Processes (Deep GP) and nonparametric hierarchical models such as neural nets. Deep GP is an extension of Deep GP, but the authors show that they can be applied to any modeling task, and that they are more robust to computational practicalities. They also show that Deep GP can be used to model the GP behavior, and they show that the adaptability of the posterior is due to the mixture of data-adaptable basis functions. They further show that neural networks with hidden units can be trained with L2 regularization. They show that their model is robust to non-Gaussianity and that the width and depth of the neural networks do not depend on the Gaussian prior on parameters. "
8408,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"algorithmic framework USED-FOR challenges. systems heterogeneity CONJUNCTION infrequent and imprecise communication. infrequent and imprecise communication CONJUNCTION systems heterogeneity. objective heterogeneity CONJUNCTION systems heterogeneity. systems heterogeneity CONJUNCTION objective heterogeneity. challenges PART-OF FL. FedLin HYPONYM-OF algorithmic framework. infrequent and imprecise communication HYPONYM-OF challenges. objective heterogeneity HYPONYM-OF challenges. systems heterogeneity HYPONYM-OF challenges. speed - accuracy conflict FEATURE-OF FL algorithms. FedLin USED-FOR linear convergence. matching upper and lower bounds FEATURE-OF convergence rate. convergence rate FEATURE-OF FedLin. matching upper and lower bounds FEATURE-OF FedLin. compression level FEATURE-OF convergence rate. gradient sparsification USED-FOR FedLin. linear convergence rates FEATURE-OF FedLin. gradient sparsification USED-FOR FL. Task is federated learning ( FL ) setup. Method is statistical model. Generic are framework, and they. OtherScientificTerm are global minimum, sub - linear rate, fast convergence, clients ’ local loss functions, objective and systems heterogeneity, infrequent, periodic communication, and tight linear convergence rate guarantees. Metric is accuracy. ","This paper proposes a new algorithmic framework for federated learning (FL) setup, FedLin. FedLin is a statistical model that learns a global minimum of the clients’ local loss functions, where each client’s local loss function is a subset of the global minimum, and the goal is to converge to a sub-linear rate. The authors propose two challenges in FL: infrequent and imprecise communication, and systems heterogeneity. The main contribution of the framework is that they provide tight linear convergence rate guarantees for FedLin, with matching upper and lower bounds on the convergence rate at the compression level. The speed-accuracy conflict of FL algorithms is also studied, and they show that FedLin achieves linear convergence rates with gradient sparsification for FL.","This paper proposes a new algorithmic framework for solving challenges in federated learning (FL) setup. The framework is based on FedLin, a statistical model where the global minimum is the sub-linear rate, and the clients’ local loss functions are the “local loss functions”. The authors propose two challenges in FL: infrequent and imprecise communication, and systems heterogeneity. The speed-accuracy conflict of FL algorithms is studied in terms of speed and accuracy conflict, and they provide tight linear convergence rate guarantees. The convergence rate of FedLin with matching upper and lower bounds on the convergence rate at the compression level is shown to be a function of the number of clients. The paper also shows that FedLin converges to linear convergence rates with gradient sparsification for FL."
8424,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,Sliced - Wasserstein distance ( SW ) USED-FOR machine learning applications. Sliced - Wasserstein distance ( SW ) COMPARE Wasserstein distance. Wasserstein distance COMPARE Sliced - Wasserstein distance ( SW ). Monte Carlo USED-FOR SW. perspective USED-FOR SW. one - dimensional projections FEATURE-OF highdimensional random vector. concentration of measure phenomenon USED-FOR perspective. concentration of measure phenomenon USED-FOR SW. deterministic approximation USED-FOR SW. method COMPARE Monte Carlo approximation. Monte Carlo approximation COMPARE method. weak dependence condition FEATURE-OF data distribution. nonasymptotical guarantees USED-FOR approach. generative modeling problem EVALUATE-FOR approximation. Generic is it. OtherScientificTerm is random projections. Metric is approximation error. Material is synthetic datasets. ,"This paper studies the Sliced-Wasserstein distance (SW) for machine learning applications. The main idea is to use the concentration of measure phenomenon in the perspective of a highdimensional random vector with one-dimensional projections. The authors propose a deterministic approximation for SW, which is based on Monte Carlo. The paper shows that the proposed method achieves better performance than Monte Carlo approximation with nonasymptotical guarantees. ",This paper proposes a new perspective for machine learning applications based on Sliced-Wasserstein distance (SW) which is a variant of Wasserstein Distance (SW). The main idea is to use a deterministic approximation to SW instead of Monte Carlo. The perspective is based on the concentration of measure phenomenon in the highdimensional random vector of the one-dimensional projections of the random projections. The authors show that it can be approximated with nonasymptotical guarantees on the data distribution under the weak dependence condition. The paper also shows that the proposed method is more robust than Monte Carlo approximation in the generative modeling problem. Experiments are conducted on synthetic datasets.
8440,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,neural language models CONJUNCTION translation models. translation models CONJUNCTION neural language models. translation models CONJUNCTION language tagging tasks. language tagging tasks CONJUNCTION translation models. language tagging tasks USED-FOR representations. neural language models USED-FOR representations. translation models USED-FOR representations. networks USED-FOR language tasks. hidden representations USED-FOR networks. computer vision USED-FOR encoder - decoder transfer learning method. hidden representations USED-FOR feature spaces. language models CONJUNCTION translation models. translation models CONJUNCTION language models. word embeddings CONJUNCTION syntactic and semantic tasks. syntactic and semantic tasks CONJUNCTION word embeddings. syntactic and semantic tasks CONJUNCTION word embeddings. word embeddings CONJUNCTION syntactic and semantic tasks. method USED-FOR low - dimensional structure. it USED-FOR NLP ( natural language processing ) tasks. language representation embedding USED-FOR low - dimensional structure. feature space USED-FOR human brain responses. representation embedding USED-FOR feature space. natural language stimuli USED-FOR human brain responses. fMRI USED-FOR natural language stimuli. fMRI USED-FOR human brain responses. metric USED-FOR brain ’s natural language processing hierarchy. principal dimension USED-FOR metric. principal dimension FEATURE-OF structure. structure USED-FOR metric. embedding USED-FOR brain ’s natural language representation structure. ,"This paper proposes an encoder-decoder transfer learning method based on computer vision. The proposed method learns a low-dimensional structure from a language representation embedding, which is then used to learn representations from neural language models and translation models. The network is trained with hidden representations that are used to represent feature spaces in the feature spaces. The networks are trained on a variety of language tasks, including word embeddings, translation models, and language tagging tasks. The method is evaluated on NLP (natural language processing) tasks, and it is shown that it is able to learn a “brain’s natural language representation structure” from the embedding. The structure is defined as the principal dimension of the structure, and the authors show that it can be used as a metric to evaluate the performance of the network. ","This paper proposes an encoder-decoder transfer learning method based on computer vision. The proposed method learns a low-dimensional structure based on a language representation embedding. The representations are learned by neural language models and translation models. The networks are trained on a set of hidden representations for each of the feature spaces. The authors show that the proposed metric can be used to measure the brain’s natural language processing hierarchy. They also show that it can be applied to NLP (natural language processing) tasks such as word embeddings, translation models, and language tagging tasks. "
8456,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"paradigm USED-FOR unconditional variational autoencoders ( VAEs ). unconditional variational autoencoders ( VAEs ) USED-FOR few - shot conditional image generation. Diffusion - Decoding models CONJUNCTION Contrastive representations ( D2C ). Contrastive representations ( D2C ) CONJUNCTION Diffusion - Decoding models. diffusion - based prior USED-FOR generation. diffusion - based prior USED-FOR latent representations. D2C USED-FOR generation. contrastive selfsupervised learning USED-FOR representation quality. contrastive selfsupervised learning USED-FOR D2C. diffusion - based prior USED-FOR D2C. D2C USED-FOR generation tasks. D2C COMPARE diffusion models. diffusion models COMPARE D2C. D2C USED-FOR conditional generation. D2C generations COMPARE StyleGAN2 ones. StyleGAN2 ones COMPARE D2C generations. double - blind study EVALUATE-FOR human evaluators. D2C generations USED-FOR conditional image manipulation. double - blind study EVALUATE-FOR D2C generations. Method are Conditional generative models of high - dimensional images, and d2c. OtherScientificTerm are supervision signals, and manipulation constraints. ",This paper proposes a new paradigm for unconditional variational autoencoders (VAEs) for few-shot conditional image generation. Diffusion-Decoding models and Contrastive representations (D2C) are commonly used for this purpose. D2C uses contrastive selfsupervised learning to improve the representation quality and the generation. The diffusion-based prior is used for the generation of latent representations. The authors show that the diffusion models perform better than diffusion models for conditional generation. They also show that d2c performs better than StyleGAN2 ones in a double-blind study for human evaluators. ,"This paper proposes a new paradigm for unconditional variational autoencoders (VAEs) for few-shot conditional image generation. Conditional generative models of high-dimensional images can be seen as an extension of Diffusion-Decoding models and Contrastive representations (D2C). D2C is a diffusion-based prior for generation of latent representations, and contrastive selfsupervised learning is used to improve the representation quality. The authors show that the generation of d2C outperforms diffusion models on a number of generation tasks. They also show that d2c generations outperform StyleGAN2 ones in the double-blind study of human evaluators for conditional image manipulation. "
8472,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"contrastive learning paradigm USED-FOR representations. Edges PART-OF graph. ground - truth classes PART-OF connected sub - graphs. contrastive learning objective USED-FOR neural net representations. population augmentation graph USED-FOR spectral decomposition. spectral decomposition USED-FOR loss. contrastive learning objective USED-FOR loss. objective USED-FOR features. linear probe evaluation FEATURE-OF features. generalization bounds USED-FOR accuracy guarantees. objective COMPARE baselines. baselines COMPARE objective. features COMPARE baselines. baselines COMPARE features. objective USED-FOR features. benchmark vision datasets EVALUATE-FOR objective. benchmark vision datasets EVALUATE-FOR baselines. Task is self - supervised learning. OtherScientificTerm are conditional independence of the positive pairs, correlated positive pairs, conditional independence of positive pairs, and augmentation graph. Method is contrastive learning. Metric is training contrastive loss. ",This paper proposes a contrastive learning paradigm for learning representations with conditional independence of the positive pairs. The proposed loss is based on spectral decomposition of the population augmentation graph as a population of positive pairs in a graph. The graph is composed of a set of connected sub-graphs with ground-truth classes. The authors show that the proposed loss can improve the performance of self-supervised learning in terms of generalization bounds on the accuracy guarantees. The main contribution of the paper is to propose a new objective for learning features with linear probe evaluation. The objective is shown to improve performance over existing baselines on benchmark vision datasets. ,"This paper proposes a contrastive learning paradigm for learning representations that are independent of the conditional independence of the positive pairs and correlated positive pairs. The proposed loss is based on the population augmentation graph, which is a spectral decomposition of connected sub-graphs with ground-truth classes. The authors show that the proposed loss has better generalization bounds for accuracy guarantees than the baselines on linear probe evaluation of the features. The main contribution of the paper is the proposed contrastive loss. Experiments on benchmark vision datasets show the effectiveness of the proposed objective compared to baselines. "
8488,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"parameterized complexity EVALUATE-FOR Bayesian Network Structure Learning ( BNSL ). complexity EVALUATE-FOR BNSL. parameterization USED-FOR fixed - parameter tractability. feedback edge set HYPONYM-OF parameterization. lower bounds USED-FOR complexity classification of BNSL. complexity classification EVALUATE-FOR BNSL. complexity EVALUATE-FOR BNSL. additive representation USED-FOR BNSL. OtherScientificTerm are superstructure, graph parameters, and treewidth. Task are fixed - parameter tractable, and Polytree Learning. Method is non - zero representation. ","This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL). The authors show that BNSL has a fixed-parameter tractable, and that the parameterization of the feedback edge set, which is a key component of the traditional parameterization, is not optimal for fixed-Parameter tractability. The authors then propose a non-zero representation of the graph parameters, and show that this additive representation is sufficient to improve the complexity classification of BnsL.   Polytree Learning is an interesting topic, and the paper is well-written and well-motivated. However, I have some concerns about the theoretical results and the experimental results.","This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL). The authors show that the complexity of BNSL depends on the superstructure of the graph parameters and the treewidth of the feedback edge set. The authors also provide lower bounds for the complexity classification of BnsL under additive representation. The paper also provides a non-zero representation for Polytree Learning. Finally, the authors provide a theoretical analysis of the effect of parameterization on the fixed-parameter tractability."
8504,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,active learning algorithm USED-FOR binary classification tasks. active learning algorithm USED-FOR streaming setting. streaming setting USED-FOR binary classification tasks. model USED-FOR surrogate loss. algorithm USED-FOR model. labeled and weak - labeled points USED-FOR surrogate loss. weak labels USED-FOR algorithm. theoretical guarantees FEATURE-OF general agnostic setting. Uncertainty Sampling HYPONYM-OF active learning algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. Margin Algorithm CONJUNCTION Uncertainty Sampling. Uncertainty Sampling CONJUNCTION Margin Algorithm. generalization and label complexity bounds EVALUATE-FOR algorithm. Margin Algorithm HYPONYM-OF baselines. Uncertainty Sampling HYPONYM-OF baselines. Material is real - world datasets. ,This paper proposes an active learning algorithm for binary classification tasks in the streaming setting. The authors propose a surrogate loss that uses a model trained with labeled and weak-label points to learn the surrogate loss. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling. The theoretical guarantees on the general agnostic setting are provided. Empirical results show that the proposed algorithm achieves better generalization and label complexity bounds than existing baselines. ,This paper proposes an active learning algorithm for binary classification tasks in the streaming setting. The authors provide theoretical guarantees for the general agnostic setting. They propose a surrogate loss between labeled and weak-labeled points. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling. They show that the proposed algorithm outperforms the baselines in terms of generalization and label complexity bounds on real-world datasets.
8520,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"classifiers USED-FOR invariant feature representations. classifier ’s function space USED-FOR generalization. complexity USED-FOR generalization. complexity FEATURE-OF classifier ’s function space. KC FEATURE-OF functions. measure USED-FOR generalization error bounds. complexity EVALUATE-FOR measure. complexity USED-FOR generalization error bounds. Kolmogorov Growth ( KG ) HYPONYM-OF measure. Occam ’s razor USED-FOR neural networks. generalization ability EVALUATE-FOR classifiers. approach USED-FOR classifiers. generalization ability EVALUATE-FOR approach. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. KG CONJUNCTION test accuracies. test accuracies CONJUNCTION KG. Kolmogorov Growth HYPONYM-OF function complexity prior. Method are classifier, complexity theory, network - to - network regularization, N2N regularization, and cross - entropy baselines. Metric is Kolmogorov complexity ( KC ). OtherScientificTerm are classification function, network trajectory, low KG zone, and training data sizes. Generic is bounds. Task is learning. ","This paper proposes a new measure for measuring the Kolmogorov complexity (KC) of a classifier’s function space for invariant feature representations. The measure is based on Occam's razor, which is a well-studied measure for generalization error bounds on the complexity of the classifier's function space. The authors show that the KG can be used to measure the generalization ability of classifiers in terms of their ability to generalize to new classes. The KG is a function complexity prior of the function complexity due to the fact that it can be computed as a function of the number of parameters of the classification function.  The authors also show that under certain conditions, the KGs are invariant to changes in the network trajectory. ","The paper proposes a new measure of generalization error bounds based on Kolmogorov complexity (KC) for invariant feature representations. The measure is based on the complexity of the classifier’s function space in the context of learning. The authors show that under certain assumptions on the classification function and the network trajectory, the generalization ability of neural networks is bounded by the complexity theory. The main contribution of the paper is to introduce a new function complexity prior based on a measure of the complexity. The paper also proposes a novel approach to regularize the classifiers for better generalization. The proposed approach is evaluated on MNIST and CIFAR-10, and on test accuracies. The results show that the proposed approach outperforms the state-of-the-art in terms of complexity. "
8536,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"self - supervised methods USED-FOR image representation learning. self - supervised methods USED-FOR embedding vectors. encoders USED-FOR constant or non - informative vectors. regularizations terms USED-FOR embeddings. term CONJUNCTION term. term CONJUNCTION term. variance FEATURE-OF term. threshold FEATURE-OF variance. term HYPONYM-OF regularizations terms. term HYPONYM-OF regularizations terms. batch normalization CONJUNCTION feature - wise normalization. feature - wise normalization CONJUNCTION batch normalization. feature - wise normalization CONJUNCTION output quantization. output quantization CONJUNCTION feature - wise normalization. output quantization CONJUNCTION stop gradient. stop gradient CONJUNCTION output quantization. stop gradient CONJUNCTION memory banks. memory banks CONJUNCTION stop gradient. weight sharing CONJUNCTION batch normalization. batch normalization CONJUNCTION weight sharing. approaches USED-FOR problem. approaches COMPARE VICReg. VICReg COMPARE approaches. output quantization CONJUNCTION memory banks. memory banks CONJUNCTION output quantization. techniques USED-FOR VICReg. downstream tasks EVALUATE-FOR VICReg. stop gradient HYPONYM-OF techniques. weight sharing HYPONYM-OF techniques. memory banks HYPONYM-OF techniques. feature - wise normalization HYPONYM-OF techniques. output quantization HYPONYM-OF techniques. batch normalization HYPONYM-OF techniques. variance regularization term USED-FOR methods. Generic is method. OtherScientificTerm are collapse problem, and branches. ","This paper studies self-supervised methods for image representation learning with embedding vectors. The authors consider the collapse problem, where the embeddings of encoders are constant or non-informative vectors, and the goal is to find the embedding vector with the smallest variance under the threshold. They propose a method called VICReg, which is based on the variance regularization term. The variance of the term is defined as the difference between the mean and variance of a set of regularizations terms (e.g. term, weight sharing, batch normalization, feature-wise normalization). The authors show that the proposed methods are able to achieve better performance than existing techniques such as stop gradient, output quantization, and memory banks on several downstream tasks. ","This paper proposes a new self-supervised methods for image representation learning. The proposed method is based on the collapse problem, where the embedding vectors of the encoders are constant or non-informative vectors. The authors propose two regularizations terms for embeddings: term and term-wise normalization. The variance of the term is defined as the threshold of the variance between the output quantization and the feature-wise regularization, and the term of the regularization term is used to define the threshold. The paper shows that the proposed approaches are able to solve the problem, and that they are more efficient than existing approaches for VICReg on downstream tasks. "
8552,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"model USED-FOR RL algorithms. model USED-FOR reward. expected returns EVALUATE-FOR RL algorithms. Bayesian model USED-FOR reward. Bayesian model USED-FOR Information Directed Reward Learning ( IDRL ). prior active reward learning methods COMPARE IDRL. IDRL COMPARE prior active reward learning methods. reward model USED-FOR policy. Task are reinforcement learning ( RL ) applications, and RL setting. OtherScientificTerm are binary preferences, expert queries, and reward approximation error. Metric is information gain. Generic is it. ","This paper proposes a Bayesian model for Information Directed Reward Learning (IDRL), a model for RL algorithms with high expected returns. IDRL is an extension of prior active reward learning methods, where the goal is to learn a policy that maximizes the reward for a set of binary preferences from expert queries. In the RL setting, this is done by learning a reward model that predicts the reward given the expert queries, and then using the reward approximation error to estimate the information gain. The authors show that IDRL can achieve better performance than the state-of-the-art in a variety of reinforcement learning (RL) applications.","This paper proposes a Bayesian model for RL algorithms with expected returns. The model is based on the idea of Information Directed Reward Learning (IDRL), where binary preferences are represented as expert queries, and the goal is to maximize the information gain. IDRL is compared to prior active reward learning methods, and it is shown to outperform IDRL in the RL setting. The paper also shows that the reward model can be used to learn a policy that maximizes the reward approximation error."
8568,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,Deep learning USED-FOR features. Deep learning USED-FOR machine learning pipelines. features PART-OF machine learning pipelines. algorithms USED-FOR neural network parameters. deep learning USED-FOR parameters. it USED-FOR parameter prediction. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. large - scale dataset USED-FOR parameter prediction. diverse computational graphs of neural architectures FEATURE-OF large - scale dataset. DEEPNETS-1 M HYPONYM-OF large - scale dataset. ImageNet USED-FOR parameter prediction. CIFAR-10 USED-FOR parameter prediction. DEEPNETS-1 M HYPONYM-OF diverse computational graphs of neural architectures. graph neural networks USED-FOR hypernetwork. accuracy EVALUATE-FOR it. CIFAR-10 EVALUATE-FOR it. ImageNet EVALUATE-FOR networks. top-5 accuracy EVALUATE-FOR networks. task CONJUNCTION model. model CONJUNCTION task. model USED-FOR neural architectures. OtherScientificTerm is CPU. Method is ResNet-50. Task is training networks. ,"This paper studies the problem of learning features in machine learning pipelines by deep learning. The authors propose two algorithms for learning neural network parameters. The first algorithm, ResNet-50, uses deep learning to learn the features of the parameters and then uses it to perform parameter prediction on a large-scale dataset, CIFAR-10 and ImageNet. The second algorithm, DEEPNETS-1 M, uses graph neural networks to train a hypernetwork. Empirically, the authors show that it achieves better top-5 accuracy than the state-of-the-art on ImageNet, and is able to generalize well to new tasks. ","This paper proposes a new way to learn features in machine learning pipelines. The key idea is to use deep learning to learn the features in the parameters of the network. The authors propose two algorithms for learning the neural network parameters. The first one is ResNet-50, which is a hypernetwork trained on graph neural networks. The second one is DEEPNETS-1 M, which uses a large-scale dataset of diverse computational graphs of neural architectures, including CIFAR-10 and ImageNet, for parameter prediction. Experiments show that it improves the top-5 accuracy of networks trained on ImageNet. The paper also shows that the proposed model can be used to train neural architectures on a new task or a different task. "
8584,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"distortion EVALUATE-FOR estimator. perception constraint FEATURE-OF minimal distortion. closed form expression USED-FOR distortion - perception ( DP ) function. mean squared - error ( MSE ) distortion CONJUNCTION Wasserstein-2 perception index. Wasserstein-2 perception index CONJUNCTION mean squared - error ( MSE ) distortion. closed form expression USED-FOR Wasserstein-2 perception index. mean squared - error ( MSE ) distortion EVALUATE-FOR distortion - perception ( DP ) function. closed form expression USED-FOR estimators. closed form expression USED-FOR Gaussian setting. global MSE minimizer CONJUNCTION minimizer. minimizer CONJUNCTION global MSE minimizer. global MSE minimizer CONJUNCTION MSE. MSE CONJUNCTION global MSE minimizer. minimizer FEATURE-OF MSE. perfect perceptual quality constraint FEATURE-OF minimizer. perfect perceptual quality constraint FEATURE-OF MSE. minimizer HYPONYM-OF tradeoff. global MSE minimizer HYPONYM-OF tradeoff. estimators USED-FOR estimators. stochastic transformation of the former USED-FOR latter. Metric are perception - distortion tradeoff, fidelity, and perceptual quality. Task is image restoration. OtherScientificTerm are statistics of natural images, perception - distortion plane, DP function, DP curve, and geodesic in Wasserstein space. ","This paper studies the perception-distortion tradeoff in image restoration. The authors propose a closed form expression for estimating the distortion-perception (DP) function in the Gaussian setting, which is based on the Wasserstein-2 perception index and the mean squared-error (MSE) distortion. The DP function is defined as the difference between the two statistics of natural images in the DP curve. The paper shows that the estimator can be trained with the same amount of distortion as the original estimator, but with a different perception constraint on the minimal distortion (the perception constraint of minimal distortion). The paper also shows that this tradeoff can be reduced to a global MSE minimizer and a minimizer with a perfect perceptual quality constraint on MSE. Finally, the paper provides a stochastic transformation of the former to the latter.","This paper studies the perception-distortion tradeoff between fidelity and perceptual quality in image restoration. The main idea is to use a closed form expression for the distortion-perception (DP) function and the Wasserstein-2 perception index as estimator of the estimator. The DP function is defined as the difference between the statistics of natural images and the DP function of the DP curve. The paper shows that the latter is a stochastic transformation of the former, while the former is a Gaussian setting. The authors show that the proposed tradeoff is equivalent to the minimizer of the MSE and the global MSE minimizer under the perfect perceptual quality constraint. "
8600,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,textual features CONJUNCTION neighbourhood information. neighbourhood information CONJUNCTION textual features. low - dimensional embeddings USED-FOR nodes. textual features USED-FOR low - dimensional embeddings. neighbourhood information USED-FOR low - dimensional embeddings. pretrained language models CONJUNCTION graph neural networks. graph neural networks CONJUNCTION pretrained language models. graph neural networks USED-FOR techniques. textual features FEATURE-OF nodes. language models USED-FOR textual features. graph neural networks USED-FOR textual embeddings. layerwise GNN components CONJUNCTION transformer blocks of language models. transformer blocks of language models CONJUNCTION layerwise GNN components. layerwise GNN components PART-OF GraphFormers. text encoding CONJUNCTION graph aggregation. graph aggregation CONJUNCTION text encoding. graph aggregation PART-OF iterative workflow. text encoding PART-OF iterative workflow. manipulated data CONJUNCTION original data. original data CONJUNCTION manipulated data. model PART-OF progressive learning strategy. manipulated data USED-FOR model. original data USED-FOR model. GraphFormers COMPARE SOTA baselines. SOTA baselines COMPARE GraphFormers. large - scale benchmark datasets EVALUATE-FOR GraphFormers. running efficiency EVALUATE-FOR SOTA baselines. running efficiency EVALUATE-FOR GraphFormers. OtherScientificTerm is textual graph. Method is cascaded model architecture. Generic is architecture. Task is independent modeling of textual features. ,"This paper proposes a cascaded model architecture for the independent modeling of textual features. The architecture is based on the idea that low-dimensional embeddings of nodes can be represented by textual features and neighbourhood information. The key idea is to use language models and graph neural networks to learn the textual features of the nodes. The proposed GraphFormers are composed of layerwise GNN components and transformer blocks of language models. The authors propose an iterative workflow based on text encoding, graph aggregation, and graph aggregation. The model is trained using manipulated data and original data, and the model is used as a progressive learning strategy. The experimental results on large-scale benchmark datasets show that GraphFormer outperforms SOTA baselines in terms of running efficiency.","This paper proposes a novel cascaded model architecture for the independent modeling of textual features. The architecture is based on the idea that low-dimensional embeddings of nodes in a textual graph can be represented as a set of nodes with textual features and neighbourhood information. The proposed techniques are based on graph neural networks and pretrained language models. GraphFormers consists of layerwise GNN components, transformer blocks of language models, and text encoding and graph aggregation. The model is trained using a progressive learning strategy, where the original data and the manipulated data are used to train the model. The experimental results on large-scale benchmark datasets show that GraphFormer outperforms SOTA baselines in terms of running efficiency. "
8616,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,algorithms USED-FOR learning tasks. userlevel differential privacy constraints FEATURE-OF learning tasks. empirical risk minimization CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION empirical risk minimization. stochastic convex optimization CONJUNCTION learning hypothesis classes. learning hypothesis classes CONJUNCTION stochastic convex optimization. high - dimensional mean estimation CONJUNCTION empirical risk minimization. empirical risk minimization CONJUNCTION high - dimensional mean estimation. smooth losses FEATURE-OF empirical risk minimization. finite metric entropy FEATURE-OF learning hypothesis classes. O(1 / n ) rate FEATURE-OF privacy cost. mean estimation CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION mean estimation. algorithms USED-FOR mean estimation. algorithms USED-FOR stochastic convex optimization. minimax optimality FEATURE-OF algorithms. lower bounds USED-FOR minimax optimality. lower bounds USED-FOR algorithms. techniques USED-FOR private mean estimation. techniques USED-FOR arbitrary dimension. error scaling FEATURE-OF techniques. private mean estimation USED-FOR algorithms. arbitrary dimension FEATURE-OF private mean estimation. techniques USED-FOR algorithms. Method is user - level DP. OtherScientificTerm is information leaks. ,"This paper studies user-level differential privacy constraints in the setting of stochastic convex optimization with smooth losses. The authors show that the O(1/n) rate of the privacy cost depends on the number of samples in the dataset. The paper also provides lower bounds for the minimax optimality of the algorithms for mean estimation, stochastically convex optimality, and empirical risk minimization. Finally, the paper shows that the techniques for private mean estimation with arbitrary dimension and error scaling can be used to improve the performance of these algorithms. ","This paper studies the privacy of algorithms for learning tasks with userlevel differential privacy constraints under user-level DP. The privacy cost is measured by the O(1/n) rate, which is a measure of how much information leaks are available to the user. The authors propose two algorithms for mean estimation, stochastic convex optimization, and empirical risk minimization with smooth losses. The learning hypothesis classes have finite metric entropy, and the authors show that these algorithms can achieve lower bounds on the minimax optimality of these algorithms with error scaling. They also propose two techniques for private mean estimation with arbitrary dimension."
8632,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"they USED-FOR deep learning. infinite width / channel limit FEATURE-OF Deep neural networks ( DNNs ). deep learning USED-FOR finite DNNs. self - consistent Gaussian Process theory USED-FOR finite - DNN and feature learning effects. noisy gradient descent USED-FOR DNNs. this USED-FOR toy model. feature learning regime CONJUNCTION lazy learning regime. lazy learning regime CONJUNCTION feature learning regime. CIFAR-10 USED-FOR Myrtle5 CNN. self - consistent theory USED-FOR finite - DNN effects. self - consistent theory USED-FOR feature learning. feature learning HYPONYM-OF finite - DNN effects. Method is Gaussian Processes ( GPs ). Generic are model, and theory. ","This paper studies the infinite width/channel limit of Deep neural networks (DNNs) with infinite width /channel limit. The authors propose a self-consistent Gaussian Process theory for the finite-DNN and feature learning effects. The main idea is to use noisy gradient descent to train DNNs with finite width and channel limit, and then use this to train a toy model. The model is trained using CIFAR-10 for the Myrtle5 CNN. The theoretical results show that the feature learning regime and the lazy learning regime converge to the same solution. ",This paper proposes a new model for finite DNNs with infinite width/channel limit. The model is based on Gaussian Processes (GPs) and they can be used for deep learning. The authors show that the finite-DNN and feature learning effects can be learned using self-consistent Gaussian process theory. They also show that this can be applied to a toy model with noisy gradient descent. The paper also shows that the feature learning regime and the lazy learning regime can be combined to improve the performance of the model. The theory is tested on CIFAR-10 and Myrtle5 CNN.
8648,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"inductive biases USED-FOR compositional communication. training framework USED-FOR inductive biases. signaling games USED-FOR compositionality. noise levels USED-FOR compositionality. conflict count CONJUNCTION context independence. context independence CONJUNCTION conflict count. topographical similarity CONJUNCTION conflict count. conflict count CONJUNCTION topographical similarity. context independence HYPONYM-OF compositionality metrics. topographical similarity HYPONYM-OF compositionality metrics. conflict count HYPONYM-OF compositionality metrics. Task is Communication. OtherScientificTerm are complex signals, and noisy channel. Generic is model. ",This paper studies the problem of compositional communication with inductive biases in the context of signaling games. The authors propose a training framework to address the inductive bias in the setting where complex signals are generated by a noisy channel. They propose two compositionality metrics (conflict count and topographical similarity) as well as a context independence metric to measure the influence of different noise levels on the compositionality. They show that the proposed model is able to achieve better performance than the state-of-the-art.,"This paper proposes a training framework for compositional communication. The authors propose to use signaling games to measure the compositionality of the input signals. The compositionality metrics are topographical similarity, topographical independence, and conflict count. The noise levels are used to measure compositionality. Communication is done by sending complex signals through a noisy channel. The model is evaluated on three datasets."
8664,SP:9d326254d77a188baf5bde39229c09b3966b5418,"ResMLP HYPONYM-OF architecture. multi - layer perceptrons USED-FOR image classification. architecture USED-FOR image classification. multi - layer perceptrons USED-FOR ResMLP. multi - layer perceptrons USED-FOR architecture. It HYPONYM-OF residual network. heavy data - augmentation CONJUNCTION distillation. distillation CONJUNCTION heavy data - augmentation. training strategy USED-FOR it. distillation USED-FOR training strategy. heavy data - augmentation USED-FOR training strategy. ImageNet EVALUATE-FOR it. self - supervised setup USED-FOR ResMLP models. labelled dataset USED-FOR priors. model USED-FOR machine translation. Timm library CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Timm library. OtherScientificTerm are image patches, and channels. Method is two - layer feed - forward network. ","This paper proposes a new architecture, ResMLP, which uses multi-layer perceptrons for image classification. It is a two-layer feed-forward network, where each layer is a residual network. The training strategy is based on heavy data-augmentation and distillation, and it is trained using a self-supervised setup. The model is trained on a labelled dataset, and the priors are trained on the labelled dataset. The proposed model is evaluated on ImageNet, where it outperforms the state-of-the-art on the Timm library and pre-trained models.","This paper proposes a new architecture for image classification based on multi-layer perceptrons for ResMLP. It is a two-layer feed-forward network with a residual network. The training strategy is based on heavy data-augmentation and distillation, and it is evaluated on ImageNet. The authors propose a self-supervised setup for the training of ResMLPs models on a labelled dataset. The model is trained on a Timm library and pre-trained models. The priors are learned on the labelled dataset, and the output of the model is used for machine translation. "
8680,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"regret guarantees USED-FOR geometric problem of contextual search. multiclass classification CONJUNCTION binary classification. binary classification CONJUNCTION multiclass classification. Task is multi - class classification. Metric is misclassification rate. Method are nearest neighbor partition, and reduction technique. OtherScientificTerm is Euclidean distance. ","This paper studies the geometric problem of contextual search with regret guarantees for multi-class classification and binary classification. The authors propose a new reduction technique to reduce the misclassification rate. The nearest neighbor partition is defined as the Euclidean distance between the nearest neighbors of the two classes. The proposed reduction technique is evaluated on multiclass classification, binary classification, and multiclass clustering.","This paper studies the geometric problem of contextual search with regret guarantees for multi-class classification, multiclass classification and binary classification. The main idea is to reduce the misclassification rate by minimizing the nearest neighbor partition. The authors propose a reduction technique based on the Euclidean distance between the nearest neighbors. "
8696,SP:5c0114535065d5125349f00bafdbccc911461ede,"Methods USED-FOR Visual Question Anwering ( VQA ). dataset biases COMPARE reasoning. reasoning COMPARE dataset biases. attention layers PART-OF VQA model. attention layers FEATURE-OF reasoning patterns. perfect ( oracle ) visual inputs USED-FOR they. method USED-FOR knowledge transfer. regularization term PART-OF loss function. regularization term USED-FOR method. sample complexity EVALUATE-FOR program prediction. PAC - learning USED-FOR theoretical analysis. GQA dataset EVALUATE-FOR approach. Task is generalization. Method is deep neural networks. Generic are models, and transfer. OtherScientificTerm is reasoning operations. ","This paper proposes a new method for Visual Question Anwering (VQA) based on PAC-learning. The proposed method is based on a regularization term in the loss function of the VQA model, where the attention layers in the VQLA model are replaced by a set of reasoning patterns. The authors show that the proposed method improves the sample complexity of program prediction by a large margin. They also provide theoretical analysis on the performance of the proposed approach on the GQA dataset.",This paper proposes a new method for Visual Question Anwering (VQA) that is based on PAC-learning. The main idea is to use the attention layers in a VQA model to learn the reasoning patterns of reasoning patterns from perfect (oracle) visual inputs. The authors show that the proposed method improves the sample complexity of program prediction and improves the generalization of deep neural networks. The proposed method also improves the knowledge transfer between different models. The paper also provides theoretical analysis of the proposed approach on the GQA dataset. 
8712,SP:40fd96105e77063de4a07d4b36fe19385434c533,"neurons of fixed precision FEATURE-OF dynamically growing memory module. 54 - neuron bounded - precision RNN USED-FOR Universal Turing Machine. growing memory modules PART-OF 54 - neuron bounded - precision RNN. Turing completeness FEATURE-OF unbounded - precision and boundedprecision RNNs. Method are recurrent neural networks ( RNNs ), RNNs, memory module, and stack - augmented RNNs. OtherScientificTerm are unbounded precision, simulated machine ’s time, and memory size. Metric is time complexity. ","This paper studies recurrent neural networks (RNNs), where the memory module is a dynamically growing memory module with neurons of fixed precision. The authors propose a 54-neuron bounded-precision RNN for the Universal Turing Machine with growing memory modules. Theoretically, unbounded precision and boundedprecision of RNNs have been shown to have Turing completeness, and the authors show that unbounded and bounded precision RNN have the same time complexity. The memory module of a stack-augmented RNN is also shown to be computationally efficient.  The authors also show that the simulated machine’s time depends on the memory size. ","This paper studies recurrent neural networks (RNNs), where the memory module is a dynamically growing memory module. The main idea is to use a 54-neuron bounded-precision RNN for the Universal Turing Machine, where the unbounded precision is the number of neurons in the memory, and the bounded precision is a function of the simulated machine’s time. The authors show that the growing memory modules of the 54-nerd bounded-probabilistic RNN are more computationally efficient than those of the conventional RNNs. The memory module of the stack-augmented RNN is much smaller than that of the standard RNN. The paper also shows that the memory size of the stacked RNN does not depend on the time complexity, but rather on the computational complexity of the memory. Turing completeness of unbounded-principal and boundedprecision rNNs is shown."
8728,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"Estimating the data uncertainty USED-FOR regression tasks. quantile function USED-FOR Estimating the data uncertainty. vanilla algorithm USED-FOR quantiles. uncertainty estimation algorithms USED-FOR quantiles. vanilla setting USED-FOR realizable linear quantile function. under - coverage bias FEATURE-OF quantile regression. α CONJUNCTION d / n. d / n CONJUNCTION α. quantile regression USED-FOR α - quantile. highdimensional parameter estimation error USED-FOR under - coverage bias. sample size CONJUNCTION model capacity. model capacity CONJUNCTION sample size. simulated and real data EVALUATE-FOR theory. model capacity FEATURE-OF under - coverage bias. OtherScientificTerm are asymptotic guarantees, coverage level, and noise distribution. ","This paper studies the problem of estimating the data uncertainty in regression tasks. Estimating the quantile function for regression tasks is a challenging problem because the asymptotic guarantees on the coverage level are not known. The authors propose uncertainty estimation algorithms for quantiles, which are based on the vanilla algorithm for estimating the quantiles. The main contribution of this paper is to propose a realizable linear Quantile function in the vanilla setting. The paper shows that the under-coverage bias of quantile regression with highdimensional parameter estimation error can be explained by the α-quantile and d/n of the noise distribution. The under-cover coverage bias can be understood as the difference between the sample size and the model capacity. The theory is validated on simulated and real data.","This paper studies the under-coverage bias of quantile regression under the assumption of asymptotic guarantees on the coverage level. The authors propose a vanilla algorithm for estimating the quantiles of the data uncertainty for regression tasks, which is based on uncertainty estimation algorithms for quantiles. The main idea is to use a realizable linear quantile function in the vanilla setting, where the α-quantile is defined as the sum of α and d/n, where α is the number of samples and n is the noise distribution. The paper shows that under-covering bias is due to the highdimensional parameter estimation error, and that the sample size and the model capacity are the main factors of under-comverage bias. Experiments on simulated and real data validate the theory."
8744,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"strict memory budget USED-FOR classifiers. learning USED-FOR incremental phase. static and ad hoc strategy USED-FOR memory allocation. dynamic memory management strategy USED-FOR incremental phases. incremental phases CONJUNCTION object classes. object classes CONJUNCTION incremental phases. dynamic memory management strategy USED-FOR object classes. reinforcement learning USED-FOR reinforced memory management ( RMM ). it USED-FOR tasks. RMM USED-FOR pseudo CIL tasks. policy function USED-FOR pseudo CIL tasks. policy function USED-FOR RMM. tasks HYPONYM-OF pseudo CIL tasks. it USED-FOR replaying - based CIL method. it USED-FOR memory management. LUCIR+AANets CONJUNCTION POD+AANets. POD+AANets CONJUNCTION LUCIR+AANets. RMM USED-FOR top - performing baselines. POD+AANets HYPONYM-OF top - performing baselines. LUCIR+AANets HYPONYM-OF top - performing baselines. ImageNet - Subset CONJUNCTION ImageNet - Full. ImageNet - Full CONJUNCTION ImageNet - Subset. CIFAR-100 CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION CIFAR-100. CIFAR-100 HYPONYM-OF benchmarks. ImageNet - Subset HYPONYM-OF benchmarks. ImageNet - Full HYPONYM-OF benchmarks. Method are Class - Incremental Learning ( CIL ), and RMM training. Task are replaying, and CIL. ","This paper proposes a new reinforcement learning method for reinforced memory management (RMM). The main idea is to use a static and ad hoc strategy to optimize the memory allocation between the incremental phase and the final phase of the learning. The authors propose a dynamic memory management strategy for the incremental phases and the object classes. The proposed method is evaluated on a variety of tasks, including CIFAR-100, ImageNet-Subset, POD+AANets, and LUCIR+Aanets. The results show that the proposed RMM outperforms the state-of-the-art top-performing baselines. ","This paper proposes Class-Incremental Learning (CIL), a method for replaying the incremental phase of a classifier with a strict memory budget. The authors propose a static and ad hoc strategy for memory allocation, and a dynamic memory management strategy for the incremental phases and the object classes. It is based on reinforcement learning for reinforced memory management (RMM) and it is applied to a variety of tasks, including pseudo CIL tasks, where the policy function is used as an RMM. The paper also proposes a replaying-based CIL method, where it is used for memory management. Experiments on CIFAR-100 and ImageNet-Full show that CIL outperforms top-performing baselines such as LUCIR+AANets and POD+Aanets. "
8760,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"it USED-FOR speeding up stochastic gradient descent ( SGD ). Ω ( √ T ) communications USED-FOR local gradient steps. Ω ( √ T ) communications USED-FOR it. linear speed - up EVALUATE-FOR √ N or N communications. optimal convergence rate EVALUATE-FOR one - shot averaging. Method are stochastic gradient descent ( SGD ), Local SGD method, Local SGD, and Local SGD scheme. OtherScientificTerm are SGD steps, stochastic gradients, communication, parallelism, Ω(N ) communications, and twice differentiability. Task is linear reduction in the variance. Metric is error. ","This paper proposes a Local SGD method for speeding up stochastic gradient descent (SGD) by using local gradient steps with Ω (√ T) communications. The local SGD scheme is based on parallelism between the SGD steps. The authors show that it can achieve linear speed-up in the case of either √ N or N communications. They also show that the optimal convergence rate for one-shot averaging can be obtained by using the LocalSGD scheme. Finally, they provide a linear reduction in the variance. ","This paper proposes a local SGD method for speeding up stochastic gradient descent (SGD), where it uses local gradient steps and Ω (√ T) communications to speed up SGD steps. The Local SGD scheme is based on the LocalSGD method. The main idea is to use parallelism in the communication between the local and the local gradient gradients. The authors show that the linear speed-up for √ N or N communications is equivalent to the linear reduction in the variance. The paper also shows that the optimal convergence rate for one-shot averaging is the same as the one-hot averaging.  The authors also show that twice differentiability of the two SGD schemes is a result of the parallelism. "
8776,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"Online Lazy Gradient Descent USED-FOR optimisation. strongly convex domain FEATURE-OF optimisation. O ( √ N ) regret EVALUATE-FOR algorithm. expected regret EVALUATE-FOR it. pseudo - regret CONJUNCTION expected regret. expected regret CONJUNCTION pseudo - regret. order bounds FEATURE-OF strongly convex domains. order bounds FEATURE-OF expected regret. order bounds FEATURE-OF pseudo - regret. OtherScientificTerm are adversarial opponents, i.i.d opponents, O(logN ) bounds, and simplex. Method is metaalgorithm. ",This paper studies online Lazy Gradient Descent for optimisation in the strongly convex domain. The authors show that the algorithm achieves O (√N) regret with respect to the order bounds of the pseudo-regret and the expected regret. They also provide O(logN) bounds for simplex. ,"This paper proposes an online Lazy Gradient Descent for optimisation in the strongly convex domain. The algorithm has O (√ N) regret and O (logN) regret, and it has pseudo-regret and expected regret. The authors show that the order bounds of the expected regret in the strong convex domains are O(logN). The authors also show that for adversarial opponents, i.i.d opponents, the order bound of the pseudo-probability of the final solution of the metaalgorithm is the same as that of simplex."
8792,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"Affine - Invariant ( AI ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry COMPARE Affine - Invariant ( AI ) geometry. Affine - Invariant ( AI ) geometry COMPARE Bures - Wasserstein ( BW ) geometry. symmetric positive definite ( SPD ) matrix manifold FEATURE-OF Riemannian optimization. linear dependence FEATURE-OF BW metric. BW metric USED-FOR Riemannian optimization problems. ill - conditioned SPD matrices USED-FOR Riemannian optimization problems. non - negative curvature FEATURE-OF BW geometry. OtherScientificTerm are SPD matrices, AI metric, and non - positively curved AI geometry. Metric is convergence rates. Method are cost functions, and AI geometry. Generic is applications. ","This paper studies the problem of Riemannian optimization on a symmetric positive definite (SPD) matrix manifold with non-negative curvature. The authors propose a new Bures-Wasserstein (BW) geometry, which is a variant of Affine-Invariant (AI) geometry. The main contribution of the paper is to introduce a new BW metric, called Affine Invariant-BW, to measure the convergence rates of the cost functions in the case of non-positively curved AI geometry. They show that the BW metric has a linear dependence on the number of iterations of the algorithm, and that it can be used to estimate the cost function in a variety of applications. The paper also provides a theoretical analysis of the convergence rate of the proposed AI metric. ","This paper proposes a new Bures-Wasserstein (BW) geometry for Riemannian optimization in the symmetric positive definite (SPD) matrix manifold. The authors show that the BW metric has linear dependence with respect to the cost functions, and that it is equivalent to the Affine-Invariant (AI) geometry in terms of convergence rates. They also show that BW geometry has non-negative curvature, which is similar to the non-positive curved AI geometry. They show that this is a useful metric for applications where the number of cost functions is large, and they show that it can be applied to a wide range of applications. "
8808,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"Dynaboard HYPONYM-OF evaluation - as - a - service framework. evaluation - as - a - service framework USED-FOR holistic model comparison. Dynaboard CONJUNCTION Dynabench platform. Dynabench platform CONJUNCTION Dynaboard. platform USED-FOR NLP models. reproducibility CONJUNCTION accessibility. accessibility CONJUNCTION reproducibility. accessibility CONJUNCTION backwards compatibility. backwards compatibility CONJUNCTION accessibility. benchmarking USED-FOR NLP. memory use CONJUNCTION throughput. throughput CONJUNCTION memory use. throughput CONJUNCTION robustness. robustness CONJUNCTION throughput. robustness HYPONYM-OF metrics. memory use HYPONYM-OF metrics. throughput HYPONYM-OF metrics. Dynascore HYPONYM-OF utility - based aggregation. NLP models COMPARE benchmarks. benchmarks COMPARE NLP models. OtherScientificTerm is selfreported metrics. Generic are paradigm, models, and task. Material is leaderboards. ","This paper proposes Dynaboard, an evaluation-as-a-service framework for holistic model comparison. The paper proposes a new paradigm for evaluating the performance of NLP models on leaderboards. The authors show that the proposed platform can be used to evaluate NLP model performance on a variety of metrics such as reproducibility, accessibility, backwards compatibility, and throughput. They also show that Dynascore, a utility-based aggregation of selfreported metrics, is a good benchmarking tool for NLP. ","This paper proposes a new evaluation-as-a-service framework, Dynaboard, for holistic model comparison. The paradigm is based on the Dynabench platform, which is used to evaluate NLP models on a set of selfreported metrics (e.g., reproducibility, accessibility, backwards compatibility, throughput, robustness, memory use, etc.). The authors show that the proposed benchmarking improves the performance of NLP on these benchmarks compared to other benchmarks. The authors also show that Dynascore, a utility-based aggregation, can be used to compare different models on the same task."
8824,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"filmmaking CONJUNCTION video production. video production CONJUNCTION filmmaking. re - recording actors ’ dialogues USED-FOR filmmaking. re - recording actors ’ dialogues USED-FOR video production. re - recording actors ’ dialogues USED-FOR Dubbing. Neural Dubber HYPONYM-OF neural network model. Neural Dubber USED-FOR automatic video dubbing ( AVD ) task. neural network model USED-FOR automatic video dubbing ( AVD ) task. Neural Dubber USED-FOR synthesizing human speech. synthesizing human speech HYPONYM-OF automatic video dubbing ( AVD ) task. lip movement USED-FOR Neural Dubber. Neural Dubber USED-FOR speech. timbre FEATURE-OF Neural Dubber. timbre FEATURE-OF speech. chemistry lecture single - speaker dataset CONJUNCTION LRS2 multi - speaker dataset. LRS2 multi - speaker dataset CONJUNCTION chemistry lecture single - speaker dataset. Neural Dubber COMPARE TTS models. TTS models COMPARE Neural Dubber. chemistry lecture single - speaker dataset EVALUATE-FOR Neural Dubber. LRS2 multi - speaker dataset EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR speech audios. speech audios COMPARE TTS models. TTS models COMPARE speech audios. speech quality EVALUATE-FOR TTS models. speech quality EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR high - fidelity speech. qualitative and quantitative evaluations EVALUATE-FOR Neural Dubber. Generic is It. Material are pre - recorded videos, and multi - speaker setting. ","This paper proposes Neural Dubber, a neural network model for the automatic video dubbing (AVD) task, synthesizing human speech. Dubbing involves re-recording actors’ dialogues for filmmaking and video production. It is an important task in the multi-speaker setting, where pre-recorded videos are used to generate high-fidelity speech, and the goal is to synthesize speech with timbre that matches the timbre of the actual speech. Neural dubber is based on lip movement, and is able to generate speech with high-quality timbre. The paper provides qualitative and quantitative evaluations of Neural DubBER on the chemistry lecture single-spear dataset and the LRS2 multi-Speaker dataset, and shows that it performs better than TTS models on speech audios.","This paper proposes Neural Dubber, a neural network model for the automatic video dubbing (AVD) task. It is based on re-recording actors’ dialogues for filmmaking and video production. The authors show that it can synthesize human speech in a multi-speaker setting. It can also synthesize pre-recorded videos. The paper also provides qualitative and quantitative evaluations on the timbre of speech and lip movement of the speech. Experimental results on the chemistry lecture single-speakers dataset and the LRS2 multi-Speaker dataset show the superiority of Neural DubBER over other speech audios and TTS models in terms of speech quality."
8840,SP:24ea12428bd675459f0509aa7cee821fa236382e,"Federated learning USED-FOR healthcare sector. neural network training USED-FOR COVID-19 diagnosis. chest X - ray ( CXR ) images USED-FOR neural network training. chest X - ray ( CXR ) images USED-FOR COVID-19 diagnosis. Vision Transformer USED-FOR split learning. Vision Transformer HYPONYM-OF deep learning architecture. decomposable configuration FEATURE-OF deep learning architecture. framework COMPARE data - centralized training. data - centralized training COMPARE framework. CXR datasets USED-FOR framework. CXR datasets USED-FOR non - independent and identically distributed data distribution. framework CONJUNCTION heterogeneous multi - task clients. heterogeneous multi - task clients CONJUNCTION framework. Transformer USED-FOR collaborative learning. collaborative learning USED-FOR medical imaging. Transformer USED-FOR medical imaging. Method are neural network, and network architecture. Generic are it, network, and methods. Material are decentralized data, and patient CXR data. OtherScientificTerm are data privacy, and network bandwidth. Task is diagnosis of COVID-19. ","This paper proposes a new deep learning architecture called Vision Transformer for split learning. The proposed method is based on the idea of federated learning in the healthcare sector, where the goal is to train a neural network to perform COVID-19 diagnosis using chest X-ray (CXR) images. The authors propose to use decentralized data to train the network, where each patient CXR data is distributed across multiple clients and the data privacy is preserved. The paper shows that the proposed framework is able to achieve better performance than data-centralized training on CXRs and heterogeneous multi-task clients. ",This paper proposes a new deep learning architecture called Vision Transformer for healthcare sector. The proposed framework is based on a decomposable configuration of the neural network. The authors show that it can be used for COVID-19 diagnosis using chest X-ray (CXR) images. They also show that the proposed framework outperforms data-centralized training and heterogeneous multi-task clients on CXR datasets for non-independent and identically distributed data distribution. 
8856,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"neural implicit representations USED-FOR 3D reconstruction. expressiveness CONJUNCTION flexibility. flexibility CONJUNCTION expressiveness. flexibility EVALUATE-FOR neural implicit representations. expressiveness EVALUATE-FOR neural implicit representations. oriented point cloud USED-FOR indicator function. differentiable PSR layer USED-FOR explicit 3D point representation. 3D mesh USED-FOR explicit 3D point representation. Chamfer distance HYPONYM-OF surface reconstruction metrics. oriented point clouds USED-FOR shapes. points CONJUNCTION patches. patches CONJUNCTION points. patches CONJUNCTION meshes. meshes CONJUNCTION patches. SAP USED-FOR topology - agnostic, watertight manifold surfaces. explicit representations COMPARE SAP. SAP COMPARE explicit representations. meshes HYPONYM-OF explicit representations. points HYPONYM-OF explicit representations. patches HYPONYM-OF explicit representations. SAP USED-FOR surface reconstruction. SAP USED-FOR learning - based reconstruction. unoriented point clouds CONJUNCTION learning - based reconstruction. learning - based reconstruction CONJUNCTION unoriented point clouds. learning - based reconstruction USED-FOR surface reconstruction. unoriented point clouds USED-FOR surface reconstruction. Metric are slow inference time, and inference time. Method are ubiquitous point cloud representation, differentiable point - to - mesh layer, and Poisson Surface Reconstruction ( PSR ). OtherScientificTerm is implicit indicator field. ","This paper studies the problem of 3D reconstruction with neural implicit representations. The authors propose a differentiable point-to-mesh layer, called Poisson Surface Reconstruction (PSR), which can be applied to any 3D point cloud representation. The key idea is to use an oriented point cloud as an indicator function, which is then used to predict the shape of the 3D mesh. The paper shows that the differentiable PSR layer can be used to learn an explicit 3d point representation, which can then be used as a point cloud for a 2D point representation.  The paper also proposes a new surface reconstruction metrics called Chamfer distance, which measures the difference between the expressiveness and the flexibility of the learned representations.   The authors show that the performance of SAP on topology-agnostic, watertight manifold surfaces is comparable to the state-of-the-art on unoriented point clouds and learning-based reconstruction. ","This paper studies the problem of learning neural implicit representations for 3D reconstruction. The authors propose a differentiable point-to-mesh layer, called Poisson Surface Reconstruction (PSR), which is an extension of the ubiquitous point cloud representation. The key idea is to learn an explicit 3D point representation from a 3D mesh. The paper shows that this paper is able to reduce the slow inference time by a factor of 2.5, which is a significant improvement over existing methods. The main contribution of the paper is to introduce an indicator function based on an oriented point cloud, which can be used to predict the shape of the surface reconstruction metrics (e.g., Chamfer distance). The paper also provides a theoretical analysis of the implicit indicator field. The results show that the proposed SAP can be applied to topology-agnostic, watertight manifold surfaces, and that it outperforms other explicit representations such as meshes, points, patches, and unoriented point clouds in learning-based reconstruction."
8872,SP:76b64e6b104818ed26e9331d134df0125d84291c,inverse problems USED-FOR recovering representations of corrupted data. pre - trained representation learning network R(x ) USED-FOR clean images. CLIP HYPONYM-OF clean images. representations USED-FOR corrupted images. supervised inversion method USED-FOR representations. forward operator USED-FOR corrupted version A(x ). contrastive objective USED-FOR supervised inversion method. blurring CONJUNCTION additive noise. additive noise CONJUNCTION blurring. linear probe USED-FOR robust representations. additive noise CONJUNCTION random pixel masking. random pixel masking CONJUNCTION additive noise. end - to - end supervised baselines USED-FOR classifying images. accuracy EVALUATE-FOR end - to - end supervised baselines. linear probe USED-FOR classifying images. linear probe COMPARE end - to - end supervised baselines. end - to - end supervised baselines COMPARE linear probe. distortions FEATURE-OF classifying images. random pixel masking HYPONYM-OF distortions. blurring HYPONYM-OF distortions. additive noise HYPONYM-OF distortions. ImageNet EVALUATE-FOR method. distortion FEATURE-OF method. method COMPARE end - to - end baselines. end - to - end baselines COMPARE method. forward operators FEATURE-OF labeled data. labeled data USED-FOR method. Material is images. ,"This paper studies the problem of recovering representations of corrupted data from a pre-trained representation learning network R(x) from clean images (e.g., CLIP). The authors propose a supervised inversion method to recover corrupted representations from corrupted images by using the forward operator of the corrupted version A(x). The proposed method is based on the contrastive objective, and is able to recover robust representations using a linear probe. The method is evaluated on ImageNet and compared with end-to-end supervised baselines on classifying images with distortions such as random pixel masking, additive noise, and blurring. The authors show that the proposed method can recover representations from labeled data with forward operators.","This paper studies the problem of recovering representations of corrupted data from a pre-trained representation learning network R(x) trained on clean images such as CLIP. The authors propose a supervised inversion method to recover corrupted images from the corrupted version A(x). The proposed method is based on a contrastive objective, and the authors show that the proposed method outperforms end-to-end supervised baselines on classifying images with a linear probe for robust representations. The method is evaluated on ImageNet with labeled data and with forward operators on labeled data. The paper shows that the method is able to recover images with distortion and random pixel masking."
8888,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,backpropagation USED-FOR local training of nodes. neural networks USED-FOR Structural credit assignment. reinforcement learning method USED-FOR node. REINFORCE USED-FOR node. global reward signal USED-FOR reinforcement learning method. REINFORCE HYPONYM-OF reinforcement learning method. reinforcement learning approaches USED-FOR learning. finite - horizon reinforcement learning problem USED-FOR neural network. off - policy learning HYPONYM-OF reinforcement learning. on - policy REINFORCE approach USED-FOR suboptimal solutions. on - policy REINFORCE approach CONJUNCTION variance reduction approaches. variance reduction approaches CONJUNCTION on - policy REINFORCE approach. networks of agents USED-FOR correlated samples. Generic is approach. Method is off - policy approach. OtherScientificTerm is stochasticity. ,"This paper proposes a new reinforcement learning method, REINFORCE, for local training of nodes in neural networks for Structural credit assignment. The proposed approach is based on backpropagation, where each node receives a global reward signal, and the goal is to learn a node that is close to the target node. The authors consider the finite-horizon reinforcement learning problem for a neural network, and propose an off-policy approach to solve the problem. The main contribution of the paper is the use of reinforcement learning approaches for learning the local learning of nodes, which is an important problem in reinforcement learning. The on-policy REINforCE approach is able to find suboptimal solutions, while the variance reduction approaches can only find correlated samples from networks of agents. The paper also provides a theoretical analysis of stochasticity of the proposed approach.","This paper proposes a new reinforcement learning method, REINFORCE, for the task of Structural credit assignment in neural networks. The proposed approach is based on backpropagation for local training of nodes. The main idea is to use a global reward signal to encourage the node to learn a node with high stochasticity. The authors propose a finite-horizon reinforcement learning problem to train a neural network. They show that the proposed approach outperforms the on-policy REINEFORCE approach and other reinforcement learning approaches for learning suboptimal solutions. They also propose an off-policy approach for reinforcement learning, which uses networks of agents to learn correlated samples."
8904,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"parallel, hierarchical specialized pathways PART-OF visual system. pathways USED-FOR behaviours. visual recognition and movement FEATURE-OF behaviours. deep neural networks USED-FOR ventral, recognition pathway. deep ANN USED-FOR pathways. model USED-FOR ventral and the dorsal pathways. loss function USED-FOR ventral and the dorsal pathways. loss function USED-FOR model. models USED-FOR mouse visual cortex. self - supervised predictive loss function USED-FOR parallel pathways. parallel pathways USED-FOR deep neural network architecture. self - supervised predictive loss function USED-FOR deep neural network architecture. self - supervised predictive learning approach USED-FOR functional specialization. self - supervised predictive learning approach USED-FOR parallel pathway architectures. functional specialization FEATURE-OF mammalian visual systems. Material is mice. Task is recognition and movement behaviours. OtherScientificTerm is dorsal and ventral pathways. ","This paper studies the problem of functional specialization in mammalian visual systems. The authors propose a novel deep neural network architecture based on parallel pathways for the ventral, recognition pathway. The main idea is to use a self-supervised predictive loss function to learn the parallel pathways of the mouse visual cortex. The proposed model is able to learn both ventral and the dorsal pathways in a deep ANN. The paper shows that the proposed model can achieve better performance than existing models.","This paper proposes a novel model for learning the ventral and the dorsal pathways of a mouse visual cortex. The authors propose to use deep neural networks to learn a ventral, recognition pathway and a dorsal, movement pathway. The model is trained using a self-supervised predictive loss function to learn the pathways. The proposed model is evaluated on a variety of mouse recognition and movement behaviours. "
8920,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"deep hierarchical topic models USED-FOR semantically meaningful topics. them PART-OF topic hierarchy. text corpus USED-FOR deep hierarchical topic models. text corpus USED-FOR semantically meaningful topics. prior belief USED-FOR learning of the topic hierarchy. knowledge graph HYPONYM-OF prior belief. TopicNet HYPONYM-OF deep hierarchical topic model. symmetric and asymmetric similarities FEATURE-OF Gaussian embedding vectors. TopicNet USED-FOR symmetric and asymmetric similarities. Gaussian - distributed embedding vector USED-FOR TopicNet. evidence lower bound CONJUNCTION regularization term. regularization term CONJUNCTION evidence lower bound. auto - encoding variational inference network USED-FOR model parameters. stochastic gradient descent USED-FOR regularization term. regularization term USED-FOR model parameters. evidence lower bound USED-FOR model parameters. stochastic gradient descent USED-FOR model parameters. deep topic models USED-FOR discovering deeper interpretable topics. TopicNet COMPARE deep topic models. deep topic models COMPARE TopicNet. TopicNet USED-FOR discovering deeper interpretable topics. TopicNet USED-FOR document representations. deep topic models USED-FOR document representations. OtherScientificTerm are prior structural knowledge, inductive bias, shared embedding space, and prior semantic hierarchies. Task is learning. ","This paper proposes TopicNet, a deep hierarchical topic model based on a text corpus for learning semantically meaningful topics from the text corpus. TopicNet uses prior belief as a prior belief for the learning of the topic hierarchy. The prior belief is based on the knowledge graph. The authors show that the prior structural knowledge can be used as a regularization term for the model parameters, which are learned by an auto-encoding variational inference network. The paper also shows that the symmetric and asymmetric similarities of the Gaussian embedding vectors of TopicNet can be computed using a Gaussian-distributed embedding vector. The main contribution of the paper is to provide a lower bound on the inductive bias of the learned model parameters using stochastic gradient descent. The empirical results show that TopicNet performs better than other deep topic models for discovering deeper interpretable topics and document representations.","This paper proposes TopicNet, a deep hierarchical topic model based on a text corpus for learning semantically meaningful topics from text corpus. TopicNet is based on prior structural knowledge, where prior belief is used to guide the learning of the topic hierarchy in a knowledge graph. The authors show that TopicNet can learn symmetric and asymmetric similarities between Gaussian embedding vectors, which is a special case of them in the context of topic hierarchy. They also provide an evidence lower bound and a regularization term for the model parameters, which are learned by an auto-encoding variational inference network. They show that the proposed TopicNet outperforms other deep topic models for discovering deeper interpretable topics and document representations. "
8936,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,Image - level contrastive representation learning USED-FOR transfer learning. generality USED-FOR transfer learning. specificity FEATURE-OF downstream task. design principle USED-FOR alignment. self - supervised pretext task CONJUNCTION downstream task. downstream task CONJUNCTION self - supervised pretext task. alignment USED-FOR self - supervised pretext task. alignment USED-FOR downstream task. pretraining method USED-FOR object detection. object - level translation invariance CONJUNCTION scale invariance. scale invariance CONJUNCTION object - level translation invariance. dedicated modules USED-FOR detection pipeline. selective search bounding boxes USED-FOR object proposals. object detection properties FEATURE-OF pretraining. FPN HYPONYM-OF detection pipeline. dedicated modules PART-OF pretraining network architecture. FPN HYPONYM-OF dedicated modules. scale invariance HYPONYM-OF object detection properties. object - level translation invariance HYPONYM-OF object detection properties. selective search bounding boxes USED-FOR object - level representations. COCO detection EVALUATE-FOR transfer. Selective Object COntrastive learning ( SoCo ) HYPONYM-OF method. transfer EVALUATE-FOR method. Mask R - CNN framework USED-FOR COCO detection. ,"This paper proposes a novel method for image-level contrastive representation learning for transfer learning. The proposed method, Selective Object COntrastive learning (SoCo), is based on the design principle of alignment between the self-supervised pretext task and the downstream task. The authors propose a pretraining method for object detection based on FPN and dedicated modules in the pretraining network architecture. The object detection properties of pretraining include object-level translation invariance and scale invariance, and selective search bounding boxes for object proposals. The method is evaluated on COCO detection with Mask R-CNN framework.","This paper proposes Image-level contrastive representation learning for transfer learning with generality. The proposed method is based on Selective Object COntrastive learning (SoCo). The authors propose a pretraining method for object detection with two dedicated modules in the detection pipeline: FPN and the selective search bounding boxes for object proposals. The key idea is to use the design principle for alignment between the self-supervised pretext task and the downstream task to improve the specificity of the object detection properties of the pretraining network architecture, such as object-level translation invariance and scale invariance. The method is evaluated on COCO detection using the Mask R-CNN framework."
8952,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"Vehicle routing problems ( VRPs ) HYPONYM-OF combinatorial problems. heuristic or learning - based works USED-FOR decent solutions. small problem instances EVALUATE-FOR decent solutions. learning - augmented local search framework USED-FOR large - scale VRP. linear number of subproblems COMPARE exponential. exponential COMPARE linear number of subproblems. spatial locality USED-FOR linear number of subproblems. regression USED-FOR subproblem selection. method USED-FOR VRPs. solution qualities EVALUATE-FOR VRPs. method USED-FOR VRP solvers. solution qualities EVALUATE-FOR method. subproblem selection COMPARE heuristic or random selection. heuristic or random selection COMPARE subproblem selection. variants CONJUNCTION solvers. solvers CONJUNCTION variants. VRP distributions CONJUNCTION variants. variants CONJUNCTION VRP distributions. OtherScientificTerm are subproblems, and black box subsolver. Method is Transformer. ","This paper proposes a learning-augmented local search framework for large-scale VRP. The proposed method is based on the Transformer. The main idea is to learn a set of subproblems that are sub-problems in a black box subsolver, and then use regression to select the best subproblem selection for each subproproblem. The authors show that the proposed method improves the performance of VRPs in terms of solution qualities and solvers. ","This paper proposes a learning-augmented local search framework for large-scale VRP. The main idea is to learn a black box subsolver for each subproblems, and then use a Transformer to find decent solutions for small problem instances. The authors show that the proposed method outperforms existing heuristic or learning-based works for finding decent solutions. They also show that linear number of subproproblems in the spatial locality is better than exponential. The proposed method is evaluated on a variety of VRPs with different solution qualities and solvers."
8968,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"dynamic data distributions USED-FOR Continual learning. learning - triggered synaptic expansion CONJUNCTION synaptic convergence. synaptic convergence CONJUNCTION learning - triggered synaptic expansion. learning - triggered synaptic expansion USED-FOR biological neural networks. synaptic Expansion - Convergence ( AFEC ) FEATURE-OF Active Forgetting. visual classification tasks CONJUNCTION Atari reinforcement tasks. Atari reinforcement tasks CONJUNCTION visual classification tasks. CIFAR-10 regression tasks CONJUNCTION visual classification tasks. visual classification tasks CONJUNCTION CIFAR-10 regression tasks. AFEC USED-FOR learning of new tasks. continual learning benchmarks EVALUATE-FOR AFEC. continual learning benchmarks EVALUATE-FOR AFEC. Atari reinforcement tasks HYPONYM-OF continual learning benchmarks. visual classification tasks HYPONYM-OF continual learning benchmarks. CIFAR-10 regression tasks HYPONYM-OF continual learning benchmarks. OtherScientificTerm are knowledge transfer, and forward knowledge transfer. Task is continual learning. Method are biological active forgetting, and Bayesian continual learning. Generic are approach, method, and them. ","This paper studies the problem of continual learning with dynamic data distributions. The authors propose Active Forgetting (AFEC), a new approach to continuous learning with active forgetting. AFEC combines learning-triggered synaptic expansion and synaptic convergence in biological neural networks. The proposed AFEC can be applied to continual learning benchmarks such as CIFAR-10 regression tasks, visual classification tasks, and Atari reinforcement tasks. Experimental results show the effectiveness of AFEC in the learning of new tasks.","This paper proposes a new approach to continuous learning with dynamic data distributions. Active Forgetting (AFEC) is a variant of biological active forgetting, where the goal of continual learning is to learn a set of new tasks that can be used for future knowledge transfer. The authors propose to use learning-triggered synaptic expansion and synaptic convergence for biological neural networks. The proposed method is evaluated on continual learning benchmarks such as CIFAR-10 regression tasks, visual classification tasks, and Atari reinforcement tasks. The results show that AFEC is able to improve the learning of new task and improve the performance of Bayesian continual learning."
8984,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,Zeroth - order ( ZO ) optimization USED-FOR tasks. query - based black - box adversarial attacks CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION query - based black - box adversarial attacks. query - based black - box adversarial attacks HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. prior information PART-OF gradient estimation procedure. finite differences USED-FOR gradient estimation procedure. finite differences USED-FOR prior information. greedy descent framework CONJUNCTION gradient estimators. gradient estimators CONJUNCTION greedy descent framework. convergence FEATURE-OF prior - guided ZO algorithms. greedy descent framework USED-FOR prior - guided ZO algorithms. prior information USED-FOR accelerated random search ( ARS ) algorithm. convergence analysis USED-FOR accelerated random search ( ARS ) algorithm. numerical benchmarks CONJUNCTION adversarial attacks. adversarial attacks CONJUNCTION numerical benchmarks. OtherScientificTerm is convergence guarantee. Method is greedy descent methods. ,This paper studies the problem of Zeroth-order (ZO) optimization for tasks such as query-based black-box adversarial attacks and reinforcement learning. The authors propose a greedy descent framework and gradient estimators based on the prior information in the gradient estimation procedure. The convergence analysis shows that the proposed greedy descent methods converge faster than the state-of-the-art.,"This paper proposes a new greedy descent algorithm for ZO (Zeroth-order) optimization for tasks such as query-based black-box adversarial attacks, reinforcement learning, and numerical benchmarks. The authors propose a greedy descent framework that combines the prior information in the gradient estimation procedure with finite differences to improve the convergence guarantee. The convergence analysis of the proposed accelerated random search (ARS) algorithm is also provided. Experiments show that the proposed greedy descent methods achieve better convergence than other prior-guided ZO algorithms."
9000,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ",This paper studies the lottery ticket hypothesis (LTH) in the context of pruning multi-layer neural networks. The authors propose a new algorithm for pruning a pruned neural network with non-pruned weights in the hidden layer. The algorithm is based on (an accelerated) stochastic gradient descent algorithm. The paper shows that the pruned network performs better than an unpruned network in terms of test accuracy and sample complexity compared to the prune network.  The authors also show that the proposed algorithm achieves a guaranteed generalization of the winning ticket. ,"This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that a pruned network outperforms an unpruned network in terms of test accuracy and generalization error. The paper also shows that the LTH is applicable to applications such as computer vision and natural language processing. The proposed algorithm is based on an accelerated (stochastic gradient descent algorithm, where non-pruned weights are added to the hidden layer of a deep neural network (DNN). The authors also show that the pruned neural network is more robust to zero-generalization error compared to the original neural network model. The main contribution of the paper is the generalization of the winning ticket to the convex region. The model achieves guaranteed generalization with respect to the objective function and sample complexity."
9016,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"private synthetic data generation USED-FOR query release. differential privacy FEATURE-OF sensitive dataset. algorithmic framework USED-FOR iterative algorithms. computational bottlenecks PART-OF algorithms. generative networks CONJUNCTION exponential mechanism ( GEM ). exponential mechanism ( GEM ) CONJUNCTION generative networks. MWEM HYPONYM-OF algorithms. neural networks USED-FOR generative models. MWEM USED-FOR private entropy projection ( PEP ). PEP COMPARE algorithms. algorithms COMPARE PEP. GEM COMPARE algorithms. algorithms COMPARE GEM. GEM CONJUNCTION PEP. PEP CONJUNCTION GEM. prior information USED-FOR GEM. public data USED-FOR prior information. public data USED-FOR state - of - the - art method. OtherScientificTerm is statistical queries. Generic are framework, methods, and method. Method are gradient - based optimization, and PMWPub. Metric is accuracy. ","This paper proposes a framework for private synthetic data generation for query release. The authors propose an algorithmic framework for iterative algorithms based on computational bottlenecks. The proposed algorithms, MWEM and GEM, are based on generative networks and an exponential mechanism (GEM). The authors show that the proposed algorithms are more efficient than PEP, GEM and PEP in terms of private entropy projection (PEP). They also show that GEM can be combined with prior information from public data to improve the performance of GEM. ",This paper proposes a new framework for private synthetic data generation for query release. The authors propose an algorithmic framework for iterative algorithms with computational bottlenecks. The proposed algorithms are generative networks and exponential mechanism (GEM). The authors also propose a private entropy projection (PEP) based on MWEM. The method is based on gradient-based optimization and is evaluated on PMWPub. The results show that the proposed algorithms perform better than GEM and PEP on public data. The paper also proposes a state-of-the-art method that uses public data to generate the prior information for GEM. 
9032,SP:d789e92c1e4f6a44de373210cd732198a6f809be,per - pixel classification task USED-FOR semantic segmentation. mask classification USED-FOR instance - level segmentation. mask classification USED-FOR semanticand instance - level segmentation tasks. model USED-FOR mask classification. training procedure USED-FOR mask classification. MaskFormer HYPONYM-OF mask classification model. mask classification model USED-FOR binary masks. single global class label prediction USED-FOR binary masks. approaches USED-FOR semantic and panoptic segmentation tasks. mask classification - based method USED-FOR approaches. mask classification - based method USED-FOR semantic and panoptic segmentation tasks. MaskFormer COMPARE per - pixel classification baselines. per - pixel classification baselines COMPARE MaskFormer. ,"This paper proposes MaskFormer, a new mask classification model for semantic segmentation tasks. MaskFormer is based on a training procedure for mask classification, where the model is trained on the per-pixel classification task, and then used for instance-level segmentation. The authors show that MaskFormer outperforms the state-of-the-art in terms of performance on both semantic and panoptic segmentation benchmarks. The main contribution of the paper is a mask classification-based method that is able to learn binary masks with single global class label prediction. The proposed MaskFormer achieves better performance than the state of the art on both per-pixels classification baselines. ","The paper proposes a new per-pixel classification task for semantic segmentation and instance-level segmentation. The model is based on MaskFormer, a mask classification model that predicts binary masks based on single global class label prediction. The training procedure for mask classification is similar to that of MaskFormer. The proposed approaches are evaluated on both semantic and panoptic segmentation tasks with the proposed mask classification-based method outperforming other approaches."
9048,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,gradient descent USED-FOR random undercomplete two - layers ReLU neural networks. smooth activation function FEATURE-OF subexponential width random neural network. Material is overcomplete case. OtherScientificTerm is dimension. ,"This paper studies the problem of gradient descent for random undercomplete two-layer ReLU neural networks. The authors consider the overcomplete case, where the dimension of the network is bounded by a smooth activation function of a subexponential width random neural network. They show that gradient descent converges to the optimal solution of the random overcomplete undercomplete case. ",This paper proposes to use gradient descent to train random undercomplete two-layer ReLU neural networks. The main idea is to use the smooth activation function of a subexponential width random neural network. This is an overcomplete case where the dimension of the network is larger than the number of layers. 
9064,SP:220db9ed147bbe67de5d82778720a1549656e48d,"sample quality CONJUNCTION distribution coverage. distribution coverage CONJUNCTION sample quality. distribution coverage EVALUATE-FOR Score - based generative models ( SGMs ). sample quality EVALUATE-FOR Score - based generative models ( SGMs ). network evaluations USED-FOR sampling. data space FEATURE-OF they. approach USED-FOR SGMs. latent space FEATURE-OF SGMs. variational autoencoder framework USED-FOR approach. non - continuous data USED-FOR SGMs. score - matching objective USED-FOR LSGM setting. SGM USED-FOR mismatch of the target distribution. Normal one USED-FOR mismatch of the target distribution. LSGM COMPARE generative results. generative results COMPARE LSGM. dataset EVALUATE-FOR LSGM. dataset EVALUATE-FOR generative results. FID score EVALUATE-FOR LSGM. CIFAR-10 EVALUATE-FOR LSGM. LSGM COMPARE SGMs. SGMs COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR LSGM. LSGM COMPARE them. them COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR SGMs. sampling time EVALUATE-FOR them. sample quality EVALUATE-FOR SGMs. sample quality EVALUATE-FOR LSGM. LSGM USED-FOR binary images. binarized OMNIGLOT dataset EVALUATE-FOR LSGM. Method are generative models, and LSGMs. OtherScientificTerm is score function. Task is variance reduction of the training objective. Generic is implementation. ","This paper studies the sample quality and distribution coverage of Score-based generative models (SGMs) under the assumption that they are trained on non-continuous data. The authors propose a score-matching objective for the LSGM setting, which is based on the SGM. The score function is defined as the variance reduction of the training objective, where the score function depends on the score of the target distribution. The approach is inspired by the variational autoencoder framework, and the authors show that the approach can improve the performance of SGMs in the latent space of the data space.  The authors also show that LSGM performs better than SGMs on the binarized OMNIGLOT dataset and on the CelebA-HQ-256 dataset. LSGM also performs better on the FID score compared to SGMs. ","This paper proposes a novel approach to improve the sample quality and distribution coverage of Score-based generative models (SGMs) by using network evaluations for sampling. The approach is based on the variational autoencoder framework. The authors propose a score-matching objective for the LSGM setting, where the score function of the SGM is computed as the variance reduction of the training objective. They show that they can achieve better sample quality in the data space than SGMs in the latent space. They also show that LSGMs can achieve good sample quality on non-continuous data. The paper also shows that LSGM outperforms SGMs on CIFAR-10 and CelebA-HQ-256 on the binarized OMNIGLOT dataset. "
9080,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"Neural networks COMPARE kernel methods. kernel methods COMPARE Neural networks. neural tangent kernels HYPONYM-OF kernel methods. hypothesis class USED-FOR realistic data. noise FEATURE-OF sparse signal. convolutional neural network USED-FOR noise. high - variance noise FEATURE-OF sparse signal. convolutional neural network USED-FOR data distribution. sparse signal FEATURE-OF data distribution. stochastic gradient descent USED-FOR convolutional neural network. predetermined features USED-FOR neural tangent kernel. CNN COMPARE neural tangent kernel. neural tangent kernel COMPARE CNN. CIFAR-10 and MNIST images EVALUATE-FOR CNN. neural networks COMPARE kernel methods. kernel methods COMPARE neural networks. OtherScientificTerm are complex hypothesis class, background noise, and local signal adaptivity ( LSA ) phenomenon. Task is image classification setting. ","This paper studies the problem of image classification setting where the data distribution is sparse and the input is a complex hypothesis class with high-variance noise. The authors propose to use a convolutional neural network to model the noise in the sparse signal, and then use stochastic gradient descent to approximate the data with a neural tangent kernel with predetermined features. The proposed method is shown to outperform the state-of-the-art neural networks on CIFAR-10 and MNIST images. ","This paper proposes a new model for image classification, called neural tangent kernels (NTK). NTK is an extension of neural networks, which is a family of kernel methods. NTK uses stochastic gradient descent to train a convolutional neural network on a data distribution with high-variance noise, where the data distribution is a sparse signal generated by a complex hypothesis class. The authors show that NTK can generalize well to realistic data with high noise. They also show that the NTK performs better than CNN on CIFAR-10 and MNIST images. "
9096,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"network USED-FOR decentralized machine learning. model USED-FOR local loss functions. known convergence rates EVALUATE-FOR GT algorithms. negative eigenvalues FEATURE-OF connectivity matrix. Method are stochastic model updates, gradient tracking ( GT ) algorithms, and GT method. OtherScientificTerm are workers ’ local data distributions, mixing parameter p, and O(p−3/2 ). Material are noiseless case, and stochastic case. ","This paper studies the problem of stochastic model updates in the setting where the workers’ local data distributions are unknown. The authors propose a new network for decentralized machine learning. The network is trained by gradient tracking (GT) algorithms. The model learns local loss functions for each worker’s local data distribution, and the mixing parameter p is used to update the weights of each worker. The connectivity matrix of the connectivity matrix has negative eigenvalues. The paper shows that the known convergence rates of the GT algorithms are O(p−3/2) for the noiseless case, and O(1/\sqrt{O(p+3/1)) for the stochastically case. The GT method is also shown to converge faster than existing GT method.","The paper proposes a new network for decentralized machine learning. The main idea is to use stochastic model updates to update the workers’ local data distributions. The authors propose gradient tracking (GT) algorithms, which are based on the GT method. The model learns local loss functions that are independent of the mixing parameter p, where p is the number of workers, and O(p−3/2) is the size of the network. In the noiseless case, the authors show that the known convergence rates of the GT algorithms can be approximated by negative eigenvalues of the connectivity matrix. "
9112,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"Upper Confidence Bound ( UCB ) policy HYPONYM-OF optimism - based MAB algorithms. O ( log n ) regret EVALUATE-FOR it. arm - sampling behavior FEATURE-OF UCB. UCB USED-FOR arm - sampling rates. O p n log n minimax regret EVALUATE-FOR UCB. process - level characterization FEATURE-OF MAB problem. diffusion scaling FEATURE-OF UCB. diffusion scaling FEATURE-OF MAB problem. UCB USED-FOR MAB problem. UCB CONJUNCTION Thompson Sampling. Thompson Sampling CONJUNCTION UCB. incomplete learning phenomenon FEATURE-OF latter. Metric are complexity, and problem complexity. OtherScientificTerm are mean rewards, instance gap, and small ” gap worst - case lens. ","This paper studies the upper Confidence Bound (UCB) policy in optimism-based MAB algorithms. The authors show that UCB has O (log n) regret with respect to the mean rewards, and that it has O p n log n minimax regret. They also show the arm-sampling behavior of UCB in terms of the number of arms, and the amount of time it takes to sample from a set of arms. Finally, they show that the UCB can be used to improve the sample complexity of the MAB problem by diffusion scaling. ","This paper proposes a new optimism-based MAB algorithms, the Upper Confidence Bound (UCB) policy. UCB is based on the O (log n) regret, and it is shown to have O p n log n minimax regret with respect to the mean rewards. The authors show that UCB can be used to reduce the arm-sampling behavior of the MAB problem with diffusion scaling, which is a process-level characterization of the problem. They also show that the complexity of UCB does not depend on the number of arms, but rather on the size of the instance gap, and that the “small” gap worst-case lens is a measure of problem complexity. The latter is a result of the incomplete learning phenomenon, and UCB has been shown to be able to reduce this by a factor of 1.5. "
9128,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"Cross - Domain Recommendation ( CDR ) USED-FOR cold - start problem. domain knowledge USED-FOR cold - start problem. cold - start problem PART-OF recommender systems. domain knowledge USED-FOR Cross - Domain Recommendation ( CDR ). cold - start CONJUNCTION CDR. CDR CONJUNCTION cold - start. approaches USED-FOR CDR. approaches USED-FOR cold - start. cross - domain recommendation framework USED-FOR CDCSR problem. DisAlign HYPONYM-OF cross - domain recommendation framework. rating and auxiliary representations USED-FOR recommendation. rating and auxiliary representations USED-FOR DisAlign. Stein path alignment USED-FOR latent embedding distributions. proxy Stein path HYPONYM-OF version. DisAlign COMPARE models. models COMPARE DisAlign. Douban and Amazon datasets EVALUATE-FOR DisAlign. CDCSR setting EVALUATE-FOR models. CDCSR setting EVALUATE-FOR DisAlign. OtherScientificTerm are latent embedding discrepancy, and model degradation. Metric is efficiency. ","This paper studies the cold-start problem in recommender systems with domain knowledge. The authors propose a cross-domain recommendation framework, called DisAlign, to solve the cold start problem in CDR. The key idea is to use the Stein path alignment between the latent embedding distributions of the recommender and the target domain. The recommendation is based on a combination of rating and auxiliary representations. Experiments on Douban and Amazon datasets show that the proposed models perform better than existing models in the CDCSR setting.","This paper proposes a cross-domain recommendation framework for the cold-start problem in recommender systems, where the goal is to reduce the latent embedding discrepancy between the target domain and the source domain. The authors propose to use domain knowledge as a proxy for the Cold-Start problem in the cross-Domain Recommendation (CDR) framework, and propose two approaches for solving the CDR. The first approach, DisAlign, uses the Stein path alignment between the source and target domain to learn the latent representations of the target and target domains. The second approach, the proxy Stein path, uses rating and auxiliary representations to guide the recommendation. Experiments on the Douban and Amazon datasets show that the proposed models outperform other models in the CDCSR setting. "
9144,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,self - attention CONJUNCTION multi - layer perceptrons ( MLP ) models. multi - layer perceptrons ( MLP ) models CONJUNCTION self - attention. multi - layer perceptrons ( MLP ) models USED-FOR vision. self - attention USED-FOR vision. self - attention CONJUNCTION MLP. MLP CONJUNCTION self - attention. complexity EVALUATE-FOR MLP. complexity EVALUATE-FOR self - attention. Global Filter Network ( GFNet ) HYPONYM-OF architecture. Global Filter Network ( GFNet ) USED-FOR long - term spatial dependencies. architecture USED-FOR long - term spatial dependencies. frequency domain FEATURE-OF long - term spatial dependencies. 2D discrete Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication CONJUNCTION 2D discrete Fourier transform. global filters CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION global filters. self - attention layer PART-OF vision transformers. element - wise multiplication USED-FOR frequency - domain features. element - wise multiplication CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication USED-FOR global filters. operations PART-OF architecture. self - attention layer PART-OF architecture. 2D discrete Fourier transform HYPONYM-OF operations. global filters HYPONYM-OF operations. element - wise multiplication HYPONYM-OF operations. 2D inverse Fourier transform HYPONYM-OF operations. accuracy / complexity trade - offs EVALUATE-FOR models. ImageNet and downstream tasks EVALUATE-FOR models. efficiency CONJUNCTION generalization ability. generalization ability CONJUNCTION efficiency. generalization ability CONJUNCTION robustness. robustness CONJUNCTION generalization ability. GFNet COMPARE transformer - style models. transformer - style models COMPARE GFNet. GFNet COMPARE CNNs. CNNs COMPARE GFNet. transformer - style models CONJUNCTION CNNs. CNNs CONJUNCTION transformer - style models. efficiency EVALUATE-FOR CNNs. generalization ability EVALUATE-FOR CNNs. robustness EVALUATE-FOR CNN,"This paper proposes a new architecture for learning long-term spatial dependencies in the frequency domain. The architecture is based on Global Filter Network (GFNet) and consists of three operations: 2D discrete Fourier transform, 2D inverse Fourier transformer, and element-wise multiplication. The key idea of the proposed architecture is to learn a self-attention layer in vision transformers, which can be used to improve the performance of vision models.  The authors show that the proposed model achieves better accuracy/complexity trade-offs than existing models on ImageNet and downstream tasks. The authors also show that GFNet achieves better generalization ability than CNNs and transformer-style models.",This paper proposes a new architecture for long-term spatial dependencies in the frequency domain. Global Filter Network (GFNet) is an extension of multi-layer perceptrons (MLP) models for vision. The proposed architecture consists of two operations: 2D discrete Fourier transform and element-wise multiplication. The global filters are based on the 2D inverse Fourier transformer and the global filters on the frequency-domain features. The accuracy/complexity trade-offs of the proposed models are evaluated on ImageNet and downstream tasks. Results show that the proposed architecture outperforms CNNs and transformer-style models in terms of generalization ability and robustness.
9160,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"high - dimensional features CONJUNCTION visual concepts. visual concepts CONJUNCTION high - dimensional features. real - world large - scale datasets USED-FOR predicting trustworthiness. focal loss CONJUNCTION true class probability confidence loss. true class probability confidence loss CONJUNCTION focal loss. cross entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross entropy loss. cross entropy loss HYPONYM-OF trustworthiness predictors. focal loss HYPONYM-OF trustworthiness predictors. cross entropy loss HYPONYM-OF prior - art loss functions. true class probability confidence loss HYPONYM-OF prior - art loss functions. focal loss HYPONYM-OF prior - art loss functions. prior - art loss functions USED-FOR trustworthiness predictors. steep slope loss USED-FOR features. steep slope loss USED-FOR trustworthiness predictors. Vision Transformer CONJUNCTION ResNet. ResNet CONJUNCTION Vision Transformer. deep learning models USED-FOR trustworthiness predictors. trustworthiness predictors EVALUATE-FOR loss. deep learning models EVALUATE-FOR loss. Vision Transformer HYPONYM-OF deep learning models. ResNet HYPONYM-OF deep learning models. loss USED-FOR trustworthiness predictors. Method are classifier, and AI models. Material are small - scale datasets, and ImageNet. Generic is task. Metric are data complexity, and generalizability of trustworthiness predictors. OtherScientificTerm is slide - like curves. ","This paper studies the problem of predicting trustworthiness on real-world large-scale datasets. The authors propose a new task where the classifier is trained to predict the trustworthiness of a set of high-dimensional features and visual concepts. The trustworthiness predictors are based on prior-art loss functions such as cross entropy loss, focal loss, true class probability confidence loss, and the focal loss. The main contribution of the paper is to show that the steep slope loss can be used to improve the performance of the features and the visual concepts in this task. The paper also shows that the proposed loss can improve the generalizability of trustworthiness predictions of deep learning models such as Vision Transformer and ResNet.","This paper proposes a new task of predicting trustworthiness on real-world large-scale datasets. The authors propose a new classifier, called ImageNet, which is a combination of two existing AI models, Vision Transformer and ResNet. The key idea is to use prior-art loss functions for trustworthiness predictors such as focal loss, cross entropy loss, true class probability confidence loss, and focal loss with respect to high-dimensional features and visual concepts. The main contribution of the paper is to reduce the data complexity of the task by reducing the slide-like curves. The proposed loss is shown to improve the generalizability of trustworthiness predictionsors in deep learning models. "
9176,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"adversarial examples USED-FOR Adversarial robustness. robustness FEATURE-OF adversarial attacks. robust models USED-FOR adversarial attacks. robustness EVALUATE-FOR robust models. linear components USED-FOR adversarial robustness. batch normalization CONJUNCTION maximum pooling. maximum pooling CONJUNCTION batch normalization. maximum pooling CONJUNCTION activation layers. activation layers CONJUNCTION maximum pooling. activation layers HYPONYM-OF non - linear components. maximum pooling HYPONYM-OF non - linear components. batch normalization HYPONYM-OF non - linear components. domain adaption CONJUNCTION robustness boosting. robustness boosting CONJUNCTION domain adaption. it USED-FOR tasks. it USED-FOR domain adaption. it USED-FOR robustness boosting. robustness boosting HYPONYM-OF tasks. domain adaption HYPONYM-OF tasks. OtherScientificTerm are statistical properties, and linearized sub - networks. Method is clustering strategy. ","This paper studies the problem of adversarial robustness against adversarial examples. The authors propose a clustering strategy to improve the robustness of robust models to adversarial attacks. The main idea is to use linear components in the training of the robust models in order to reduce the statistical properties of the models. The non-linear components such as batch normalization, maximum pooling, and activation layers are used to train the models to be more robust.  The authors show that it improves robustness on a variety of tasks such as domain adaption, robustness boosting, and domain adaptation. ","This paper studies adversarial examples for Adversarial robustness. The authors propose a clustering strategy to improve the robustness of robust models against adversarial attacks. The key idea is to use linearized sub-networks, where the statistical properties of the adversarial perturbations are known, and the linear components of the robust models are learned. The non-linear components are the batch normalization, maximum pooling, and activation layers. Experiments are conducted on three tasks: domain adaption, robustness boosting, and domain adaptation."
9201,SP:590b67b1278267e966cf0b31456d981441e61bb1,approach USED-FOR end - to - end reconstruction operators. unpaired training data USED-FOR ill - posed inverse problems. unpaired training data USED-FOR approach. expected distortion CONJUNCTION Wasserstein-1 distance. Wasserstein-1 distance CONJUNCTION expected distortion. variational framework CONJUNCTION iterative unrolling. iterative unrolling CONJUNCTION variational framework. measurement space FEATURE-OF expected distortion. variational framework USED-FOR method. regularizer USED-FOR variational setting. deep neural network USED-FOR regularizer. unrolled reconstruction operator USED-FOR regularizer. reconstruction network USED-FOR variational problem. it COMPARE variational methods. variational methods COMPARE it. unrolled operator USED-FOR initialization. initialization USED-FOR it. well - posedness CONJUNCTION noise - stability guarantees. noise - stability guarantees CONJUNCTION well - posedness. noise - stability guarantees FEATURE-OF variational setting. well - posedness FEATURE-OF variational setting. end - to - end unrolled reconstruction USED-FOR approach. well - posedness FEATURE-OF approach. well - posedness FEATURE-OF end - to - end unrolled reconstruction. it COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE it. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. approach COMPARE it. it COMPARE approach. approach COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE approach. OtherScientificTerm is reconstruction. Material is X - ray computed tomography ( CT ). ,"This paper proposes a new approach to learn end-to-end reconstruction operators from unpaired training data for ill-posed inverse problems. The proposed method is based on a variational framework and iterative unrolling. The reconstruction network is trained to solve the variational problem using a reconstruction network with a deep neural network. The authors show that the expected distortion and Wasserstein-1 distance between the reconstruction and the measurement space can be computed by a regularizer trained on the training data. The paper also shows that the proposed approach has well-posedness and noise-stability guarantees in the end- to-end unrolled reconstruction setting, and that it performs better than supervised data-driven reconstruction approaches and unsupervised methods.","This paper proposes a novel approach to learn end-to-end reconstruction operators for ill-posed inverse problems with unpaired training data. The proposed method is based on a variational framework and iterative unrolling. The variational setting is defined in a measurement space with expected distortion and Wasserstein-1 distance in the measurement space, and the regularizer is learned using a deep neural network. The reconstruction network is then used to solve the variational problem. The initialization is done using an unrolled reconstruction operator. The authors show that the proposed approach improves well-posedness and noise-stability guarantees in the variular setting, and that it outperforms other variational methods in terms of well-positedness. The approach is also compared to other supervised data-driven reconstruction approaches. "
9226,SP:115d679338ab35829dbc594472d13cc02be5ed4c,Large - scale vision and language representation learning USED-FOR vision - language tasks. transformer - based multimodal encoder USED-FOR word tokens. transformer - based multimodal encoder USED-FOR methods. multimodal encoder USED-FOR image - text interactions. visual tokens CONJUNCTION word tokens. word tokens CONJUNCTION visual tokens. contrastive loss USED-FOR image and text representations. cross - modal attention USED-FOR vision and language representation learning. methods COMPARE method. method COMPARE methods. bounding box annotations CONJUNCTION high - resolution images. high - resolution images CONJUNCTION bounding box annotations. high - resolution images USED-FOR method. momentum distillation HYPONYM-OF self - training method. pseudo - targets USED-FOR self - training method. momentum model USED-FOR pseudo - targets. momentum model USED-FOR self - training method. downstream visionlanguage tasks EVALUATE-FOR ALBEF. ALBEF COMPARE methods. methods COMPARE ALBEF. image - text retrieval EVALUATE-FOR ALBEF. image - text retrieval EVALUATE-FOR methods. NLVR2 EVALUATE-FOR ALBEF. VQA EVALUATE-FOR ALBEF. ALBEF COMPARE state - ofthe - art. state - ofthe - art COMPARE ALBEF. VQA CONJUNCTION NLVR2. NLVR2 CONJUNCTION VQA. Material is noisy web data. Task is training tasks. OtherScientificTerm is image - text pair. ,This paper proposes a self-training method based on momentum distillation. The authors propose a transformer-based multimodal encoder to capture image-text interactions between visual tokens and word tokens. The contrastive loss is used to learn the image and text representations by cross-modal attention in vision and language representation learning. The proposed method is evaluated on downstream visionlanguage tasks with bounding box annotations and high-resolution images. ALBEF outperforms existing methods such as VQA and NLVR2.,This paper proposes a self-training method based on momentum distillation. The authors propose a transformer-based multimodal encoder for image-text interactions and word tokens. The contrastive loss is used to learn image and text representations. The cross-modal attention is used for vision and language representation learning. The proposed method is evaluated on several downstream visionlanguage tasks and compared to other methods such as VQA and NLVR2.
9251,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,Markov decision processes ( MDPs ) USED-FOR offline policy evaluation ( OPE ). static datasets USED-FOR decisionmaking policies. OPE COMPARE realizable setting. realizable setting COMPARE OPE. unrealizability USED-FOR OPE. unrealizability USED-FOR OPE method. linear direct method ( DM ) HYPONYM-OF OPE method. doubly robust form FEATURE-OF OPE error. nonparametric consistency FEATURE-OF tile - coding estimators. OtherScientificTerm is approximate ) realizability assumptions. Method is hypothetical models. Task is real - world applications. ,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) where the goal is to evaluate the performance of a policy on a set of static datasets. The authors propose a linear direct method (DPM) based on the unrealizability of the OPE method, which has a doubly robust form. The paper shows that OPE can achieve better performance than the realizable setting under (approximate) realizability assumptions. They also show that the nonparametric consistency of tile-coding estimators can be improved. ",This paper proposes a new offline policy evaluation (OPE) method based on Markov decision processes (MDPs) for offline decision evaluation (OPE). OPE uses static datasets to evaluate decisionmaking policies on static datasets with (approximate) realizability assumptions. The authors show that OPE outperforms the realizable setting in terms of unrealizability. The OPE method is based on the linear direct method (DM) with a doubly robust form of the OPE error. The paper also shows that the nonparametric consistency of tile-coding estimators can be improved by using the (almost) approximate (realizability assumption. 
9276,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"stochastic 1 first - order methods USED-FOR large - scale machine learning models. theoretical guarantees FEATURE-OF expectation of the objective value. algorithms USED-FOR small objective residual. Existing methods USED-FOR non - smooth stochastic convex optimization. complexity 7 bounds FEATURE-OF Existing methods. logarithmic dependence FEATURE-OF high - probability convergence. stepsize rules USED-FOR stochastic methods. gradient 14 clipping USED-FOR stochastic methods. extension USED-FOR strongly convex problems. Hölder - continuous gradients FEATURE-OF generalized smooth objectives. extension USED-FOR methods. non - smooth setting FEATURE-OF one. iteration and oracle complexity EVALUATE-FOR accelerated ) 17 method. OtherScientificTerm are Random behavior, suboptimal objective value, confidence level, and negative - power. Generic is algorithm. Task are NLP tasks, and non - smooth convex stochastic 12 optimization problems. ","This paper studies stochastic 1 first-order methods for large-scale machine learning models. Existing methods for non-smooth convex optimization with complexity 7 bounds have theoretical guarantees on the expectation of the objective value. The main contribution of this paper is to extend these theoretical guarantees to the case of strongly convex problems with gradient 14 clipping. The authors show that under certain assumptions on the suboptimal objective value, the high-probability convergence has logarithmic dependence on the number of stepsize rules in the stoanchastic methods. They also provide an extension to the generalized smooth objectives with Hölder-continuous gradients. Finally, the authors provide an algorithm for the accelerated (17 method with iteration and oracle complexity) 17 method. ","This paper proposes a new algorithm for non-smooth stochastic 1 first-order methods for large-scale machine learning models with theoretical guarantees on the expectation of the objective value. Existing methods are based on the complexity 7 bounds of the original algorithms for the small objective residual. The main difference is that the authors propose an extension to strongly convex problems with Hölder-continuous gradients, where the suboptimal objective value is a function of the confidence level. The authors show that the high-probability convergence with logarithmic dependence can be obtained by using stepsize rules. They also show that this algorithm can be applied to NLP tasks where the goal is to find the optimal solution. They show that their (adversarially accelerated) 17 method can achieve better iteration and oracle complexity than existing methods in the non-slooth setting. They provide theoretical guarantees of the expectation for the objective values of the proposed algorithm. They evaluate their algorithm on non-sparse convex 12 optimization problems."
9301,SP:a22a893e25ce739dc757861741014764e78aa820,"extreme weather early warning CONJUNCTION long - term energy consumption planning. long - term energy consumption planning CONJUNCTION extreme weather early warning. self - attention mechanisms USED-FOR long - range dependencies. Transformerbased models USED-FOR long - range dependencies. self - attention mechanisms USED-FOR Transformerbased models. point - wise self - attentions USED-FOR long series efficiency. point - wise self - attentions USED-FOR Transformers. decomposition architecture USED-FOR Autoformer. Auto - Correlation mechanism USED-FOR decomposition architecture. Auto - Correlation mechanism USED-FOR Autoformer. pre - processing convention PART-OF series decomposition. design USED-FOR Autoformer. Autoformer USED-FOR complex time series. progressive decomposition capacities USED-FOR complex time series. progressive decomposition capacities FEATURE-OF Autoformer. dependencies discovery CONJUNCTION representation aggregation. representation aggregation CONJUNCTION dependencies discovery. stochastic process theory USED-FOR Auto - Correlation mechanism. series periodicity USED-FOR dependencies discovery. representation aggregation PART-OF sub - series level. series periodicity USED-FOR Auto - Correlation mechanism. Auto - Correlation COMPARE self - attention. self - attention COMPARE Auto - Correlation. efficiency EVALUATE-FOR Auto - Correlation. accuracy EVALUATE-FOR Auto - Correlation. traffic CONJUNCTION economics. economics CONJUNCTION traffic. energy CONJUNCTION traffic. traffic CONJUNCTION energy. energy CONJUNCTION economics. economics CONJUNCTION energy. long - term forecasting EVALUATE-FOR Autoformer. practical applications FEATURE-OF benchmarks. energy HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR Autoformer. accuracy EVALUATE-FOR Autoformer. traffic HYPONYM-OF benchmarks. economics HYPONYM-OF benchmarks. energy HYPONYM-OF practical applications. traffic HYPONYM-OF practical applications. economics HYPONYM-OF practical applications. OtherScientificTerm are forecasting time, and information utilization bottleneck. Task is long - term forecasting problem of time series. Generic are model, and it. Method is deep models. ","This paper studies the long-term forecasting problem of time series. The authors propose Autoformer, a new decomposition architecture based on the Auto-Correlation mechanism based on stochastic process theory. Autoformer is able to decompose complex time series with progressive decomposition capacities. The proposed design is based on a pre-processing convention in the series decomposition, which allows the model to handle long-range dependencies. The self-attention mechanisms in Transformerbased models can be used to learn long-distance dependencies.  The authors show that Auto-correlation can achieve better efficiency than self-tattles in terms of long series efficiency with point-wise self- attentions. They also show that the representation aggregation in the sub-series level can be combined with dependencies discovery and representation aggregation to improve the performance of the model.  Finally, the authors demonstrate the effectiveness of Autoformer on a variety of benchmarks such as traffic, economics, and energy. ","This paper proposes Autoformer, a decomposition architecture for long-term forecasting problem of time series. Autoformer is based on Transformerbased models with point-wise self-attention mechanisms to learn long-range dependencies. The decomposition is done using a pre-processing convention, which is a preprocessing convention that is used in series decomposition. The authors show that Autoformer can learn complex time series with progressive decomposition capacities. Auto-Correlation mechanism is proposed based on stochastic process theory. The proposed Autoformer achieves better efficiency and accuracy compared to Self-Attention and self-supervised learning. The paper also shows that Auto-correlation can be used to learn sub-series level dependencies, including representation aggregation, dependencies discovery, and dependencies discovery with series periodicity. "
9326,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,NLP systems USED-FOR compositional language. Cryptic crosswords HYPONYM-OF dominant crossword variety. character - level manipulations USED-FOR wordplay cipher. creative intelligence USED-FOR cryptics. NLP systems USED-FOR compositional language. cryptic clues USED-FOR NLP systems. dataset USED-FOR NLP systems. dataset USED-FOR compositional language. cryptic clues FEATURE-OF dataset. model USED-FOR tasks. T5 HYPONYM-OF neural language model. non - neural approaches CONJUNCTION T5. T5 CONJUNCTION non - neural approaches. unscrambling words HYPONYM-OF tasks. meta - linguistic capabilities FEATURE-OF subword - tokenized models. T5 COMPARE human solving strategies. human solving strategies COMPARE T5. wordplay part of clues USED-FOR model systematicity. curricular approach COMPARE T5 baseline. T5 baseline COMPARE curricular approach. cryptic crosswords PART-OF NLP systems. OtherScientificTerm is Cryptic clues. Method is curriculum approach. ,"This paper presents a new dataset for learning compositional language from cryptic clues in NLP systems. Cryptic crosswords are a dominant crossword variety in the literature, and the authors propose a curriculum approach to learn the cryptics from these clues. The key idea is to use character-level manipulations to encode the wordplay cipher in the cryptic clues, and then use creative intelligence to learn cryptics. The authors show that the proposed dataset is able to learn compositional languages from these cryptic clues. They also show that their model can be applied to a variety of tasks, including unscrambling words, T5, and non-neural approaches such as T5 and T5 CONJUNCTION. Finally, they show that subword-tokenized models have better meta-lingual capabilities than human solving strategies. ","This paper presents a new dataset for learning compositional language from cryptic clues. The authors propose to use the Cryptic Crosswords, a dominant crossword variety that is composed of character-level manipulations of the wordplay cipher. The key idea is to use creative intelligence to generate cryptics that are more interpretable than the dominant ones, and then use these cryptics to train NLP systems on the new dataset. The proposed curriculum approach is evaluated on three tasks: unscrambling words, T5, and non-neural approaches. The results show that T5 outperforms human solving strategies in terms of meta-linguistic capabilities of subword-tokenized models. The paper also shows that the proposed curricular approach outperforms the T5 baseline on all three tasks. The main contribution of the paper is the use of wordplay part of clues to improve model systematicity."
9351,SP:7693974b70806d9b67920b8ddd2335afc4883319,"Convolutional neural networks ( CNNs ) USED-FOR visual data. image classification tasks EVALUATE-FOR ( Vision ) Transformer models ( ViT ). Vision Transformers USED-FOR tasks. ViTs CONJUNCTION CNNs. CNNs CONJUNCTION ViTs. internal representation structure USED-FOR ViTs. internal representation structure USED-FOR CNNs. uniform representations USED-FOR ViT. image classification benchmarks EVALUATE-FOR internal representation structure. image classification benchmarks EVALUATE-FOR ViTs. ViT HYPONYM-OF architectures. image classification benchmarks EVALUATE-FOR CNNs. self - attention CONJUNCTION ViT residual connections. ViT residual connections CONJUNCTION self - attention. self - attention USED-FOR aggregation of global information. ViTs USED-FOR input spatial information. intermediate features CONJUNCTION transfer learning. transfer learning CONJUNCTION intermediate features. ( pretraining ) dataset scale USED-FOR transfer learning. ( pretraining ) dataset scale USED-FOR intermediate features. MLP - Mixer HYPONYM-OF architectures. Generic is they. Method are convolutional networks, and classification methods. OtherScientificTerm are visual representations, and features. Task are early aggregation of global information, and spatial localization. ","This paper studies the problem of early aggregation of global information in convolutional neural networks (CNNs) for visual data. The authors propose Vision Transformers (ViT) for the tasks of image classification tasks. ViT is an internal representation structure for CNNs, and they are trained with uniform representations. ViTs are evaluated on several image classification benchmarks, and compared with other architectures such as MLP-Mixer. The results show that ViTs can aggregate input spatial information, and can learn intermediate features and transfer learning on a (pretraining) dataset scale. ","This paper proposes a new architecture for (Vision) Transformer models (ViT) for image classification tasks. ViTs are a family of convolutional networks (CNNs) for visual data, and they can be seen as an extension of Vision Transformers. The main difference between ViTs and CNNs is that ViTs have an internal representation structure that allows for the aggregation of global information, while CNNs have uniform representations. The authors propose two architectures, ViT and MLP-Mixer, which are based on the (pretraining) dataset scale for intermediate features and transfer learning, and self-attention and ViT residual connections are used for aggregating the input spatial information. Experiments show that ViT outperforms CNNs on image classification benchmarks. "
9376,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Thompson sampling ( TS ) USED-FOR bandit area. approximation oracle USED-FOR TS. convergence analysis USED-FOR TS. exact oracle PART-OF CMAB. greedy oracle HYPONYM-OF common ( approximation ) oracle. theoretical guarantees FEATURE-OF common ( approximation ) oracle. TS USED-FOR CMAB problems. problemdependent regret lower bound USED-FOR TS. greedy oracle USED-FOR TS. TS USED-FOR CMAB. approximation oracles USED-FOR TS. Generic are It, and oracle. OtherScientificTerm are optimal solutions, reward gap, and almost matching regret upper bound. Task is combinatorial optimization problems. ","This paper studies Thompson sampling (TS) in the bandit area. It provides a convergence analysis for TS in the context of the exact oracle in CMAB. It also provides theoretical guarantees for the common (approximation) oracle, such as greedy oracle.  The authors show that TS can be used to solve CAMAB problems with approximation oracles. They also provide an almost matching regret upper bound for TS with a problemdependent regret lower bound. It is also shown that TS is able to find optimal solutions with a small reward gap. ","This paper proposes Thompson sampling (TS) for the bandit area. It is based on the convergence analysis of TS in the context of combinatorial optimization problems. TS is a common (approximation) oracle with theoretical guarantees. It can be seen as an exact oracle in CMAB, where the optimal solutions are known and the reward gap is small. TS can be used to solve CMABER problems with a problemdependent regret lower bound, and a greedy oracle for TS with almost matching regret upper bound. "
9401,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"Federated learning HYPONYM-OF distributed learning paradigm. accuracy rates EVALUATE-FOR federated learning. total error HYPONYM-OF social good properties. hedonic game USED-FOR federated learning. average error rates USED-FOR optimality. algorithm USED-FOR optimal ( error minimizing ) arrangement of players. stability CONJUNCTION optimality. optimality CONJUNCTION stability. stability EVALUATE-FOR arrangement. optimality EVALUATE-FOR arrangement. Method are global model, and game - theoretic approach. OtherScientificTerm are error - minimizing players, federating coalitions, stable coalition partitions, stable arrangements, Price of Anarchy, and constant - factor bound. Generic is stable solutions. ","This paper studies the problem of federated learning, a distributed learning paradigm where the goal is to learn a global model from a set of players. The authors propose a game-theoretic approach to solve the problem, where each player is given a subset of the players in the global model, and the objective is to find the optimal (error minimizing) arrangement of players that minimizes the total error of all the players. They show that the average error rates for the optimal optimality of the optimal arrangement are the same for all players, and that the stability of the arrangement is a function of the stability and the optimality. They also provide a constant-factor bound for the stability. ","The paper proposes a distributed learning paradigm called Federated learning, which is a variant of federated learning with error-minimizing players. The authors propose a game-theoretic approach where the goal is to minimize the total error of the social good properties (e.g., total error, total error) of the federating coalitions. The paper proposes an algorithm to find the optimal (error minimizing) arrangement of players that maximizes the average error rates of the optimality of the global model. The algorithm is based on a hedonic game, where the objective is to find stable coalitions with stable coalition partitions. The stability of the arrangement is measured in terms of stability and optimality, and the stability is measured by the Price of Anarchy. The main contribution of the paper is to provide stable solutions to the constant-factor bound."
9426,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"self - supervised capsule architecture USED-FOR 3D point clouds. capsule decompositions of objects USED-FOR capsule decompositions. permutation - equivariant attention USED-FOR capsule decompositions of objects. these USED-FOR decomposition. capsule invariance / equivariance properties FEATURE-OF decomposition. canonicalization operation USED-FOR object - centric reasoning. classification labels CONJUNCTION manually - aligned training datasets. manually - aligned training datasets CONJUNCTION classification labels. classification labels USED-FOR neural network. manually - aligned training datasets USED-FOR neural network. canonicalization CONJUNCTION unsupervised classification. unsupervised classification CONJUNCTION canonicalization. method COMPARE state - of - the - art. state - of - the - art COMPARE method. 3D point cloud reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION 3D point cloud reconstruction. object - centric representation USED-FOR method. canonicalization EVALUATE-FOR method. unsupervised classification EVALUATE-FOR method. 3D point cloud reconstruction EVALUATE-FOR state - of - the - art. 3D point cloud reconstruction EVALUATE-FOR method. self - supervised manner USED-FOR object - centric representation. Generic is process. OtherScientificTerm are attention masks, and semantic keypoints. Method is semantically consistent decomposition. ",This paper proposes a self-supervised capsule architecture for 3D point clouds. The capsule decompositions of objects are learned by permutation-equivariant attention. The decomposition is based on capsule invariance/equivariance properties of the objects. The authors propose a canonicalization operation for object-centric reasoning. The neural network is trained with classification labels and manually-aligned training datasets. The proposed method is shown to outperform the state-of-the-art in the case of canonicalization and unsupervised classification. ,This paper proposes a self-supervised capsule architecture for 3D point clouds. The capsule decompositions of objects are decomposed using capsule invariance/equivariance properties of the decomposition. The decomposition is done using permutation-equivariant attention. The authors propose a canonicalization operation for object-centric reasoning. The main idea is to use attention masks to encode semantic keypoints. The proposed method is compared with state-of-the-art in 3D Point cloud reconstruction and canonicalization and unsupervised classification on classification labels and manually-aligned training datasets for a neural network. 
9451,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conformal method USED-FOR prediction intervals. prediction intervals USED-FOR nonparametric regression. conformal method USED-FOR nonparametric regression. black - box machine learning algorithms USED-FOR conditional distribution. approximate conditional coverage FEATURE-OF prediction intervals. histograms USED-FOR black - box machine learning algorithms. conditional coverage CONJUNCTION optimal length. optimal length CONJUNCTION conditional coverage. finite samples FEATURE-OF marginal coverage. marginal coverage FEATURE-OF prediction intervals. conformalized quantile regression CONJUNCTION distributional conformal prediction approaches. distributional conformal prediction approaches CONJUNCTION conformalized quantile regression. simulated and real data EVALUATE-FOR state - of - the - art alternatives. distributional conformal prediction approaches HYPONYM-OF state - of - the - art alternatives. conformalized quantile regression HYPONYM-OF state - of - the - art alternatives. Material is skewed data. Method is black - box model. ,This paper proposes a conformal method for estimating prediction intervals for nonparametric regression using black-box machine learning algorithms based on histograms. The authors show that the approximate conditional coverage of the prediction intervals is a function of the marginal coverage on finite samples and the optimal length of the conditional distribution. The marginal coverage is defined as the difference between the conditional coverage and optimal length between the two prediction intervals. The paper also provides a theoretical analysis of the performance of conformalized quantile regression and distributional conformal prediction approaches on simulated and real data. ,"This paper proposes a conformal method for learning prediction intervals for nonparametric regression. The authors use black-box machine learning algorithms to learn a conditional distribution over histograms. The conditional coverage and optimal length of the prediction intervals are obtained from finite samples. The marginal coverage of prediction intervals is obtained from the conditional coverage of the distribution over finite samples, and the optimal coverage is obtained by minimizing the marginal coverage over the distribution of the conditional distribution. The proposed method is evaluated on simulated and real data, and compared to state-of-the-art alternatives such as conformalized quantile regression and distributional conformal prediction approaches. "
9476,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"invariance USED-FOR generalisation. incorporating invariance PART-OF kernel ridge regression. effective dimension USED-FOR generalisation. feature averaging USED-FOR invariance. reproducing kernel Hilbert space CONJUNCTION kernel. kernel CONJUNCTION reproducing kernel Hilbert space. Generic is approach. OtherScientificTerm are function space perspective, and group. ",This paper studies the problem of incorporating invariance in kernel ridge regression by incorporating the effective dimension into the generalisation of the function space perspective. The authors propose a new approach based on feature averaging to improve the invariance of the kernel. The key idea is to use the reproducing kernel Hilbert space and the kernel as a surrogate for the group. The experimental results show the effectiveness of the proposed approach.,"This paper proposes an approach to incorporate invariance to generalisation in kernel ridge regression by incorporating invariance in the function space perspective. The invariance is achieved by feature averaging, where the effective dimension of the generalisation is a function of the number of samples in the group. The authors show that the invariance can be obtained by reproducing kernel Hilbert space and kernel."
9501,SP:97fac361b69ed5871a60dc40e51900747a453df9,"assertion statements USED-FOR erroneous behavior. software programs USED-FOR they. applications EVALUATE-FOR deep learning programs. generative model USED-FOR neural network activations. DecNN HYPONYM-OF Decodable Neural Network. DecNN USED-FOR ensemble - like model. compositionality FEATURE-OF neural networks. uncertainty FEATURE-OF ensemble - like model. out - of - distribution detection CONJUNCTION adversarial example detection. adversarial example detection CONJUNCTION out - of - distribution detection. adversarial example detection CONJUNCTION calibration. calibration CONJUNCTION adversarial example detection. uncertainty USED-FOR out - of - distribution detection. uncertainty USED-FOR adversarial example detection. uncertainty USED-FOR calibration. accuracy EVALUATE-FOR neural networks. DecNN CONJUNCTION pretrained models. pretrained models CONJUNCTION DecNN. protected features USED-FOR neural networks. OtherScientificTerm is program logic. Generic are programs, and design. ","This paper proposes a Decodable Neural Network, DecNN, which is a generative model for neural network activations. The authors show that DecNN is a good ensemble-like model with high uncertainty in terms of the compositionality of the neural networks, and that they can be used in a variety of applications such as out-of-distribution detection, adversarial example detection, and calibration. They also show that the DecNN can be combined with pretrained models with protected features to improve the performance of neural networks. ","This paper proposes a Decodable Neural Network (DecNN), a generative model for learning deep learning programs. DecNN can be seen as an ensemble-like model with uncertainty and compositionality. The authors show that DecNN is able to learn programs that are more robust to erroneous behavior than other software programs, and they show that they can learn programs with different types of program logic. They also show that their design can be applied to a variety of applications, including out-of-distribution detection, adversarial example detection, and calibration. They show that the DecNN has better accuracy than other pretrained models, and that it can learn protected features for neural networks."
9526,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"Optimal transport maps USED-FOR machine learning and statistics. probability distributions FEATURE-OF Optimal transport maps. Plugin estimators USED-FOR transport maps. Plugin estimators USED-FOR computational optimal transport. rates of convergences EVALUATE-FOR plug - in estimators. barycentric projections USED-FOR plug - in estimators. stability estimate USED-FOR plug - in estimators. stability estimate USED-FOR barycentric projections. rates of convergence EVALUATE-FOR plug - in estimators. rates of convergence FEATURE-OF Wasserstein barycenter. asymptotic detection thresholds USED-FOR optimaltransport based tests of independence. probability distributions FEATURE-OF Wasserstein barycenter. Generic is maps. OtherScientificTerm are minimal smoothness assumptions, smoothness assumptions, curse of dimensionality, and Wasserstein distance. ",This paper studies the problem of computing optimal transport maps for machine learning and statistics. The authors propose to use Plugin estimators for computing computational optimal transport using barycentric projections of the probability distributions of Optimal transport maps. They show that the stability estimate of plug-in estimators can be used to improve the rates of convergences of these plug- in estimators. They also provide asymptotic detection thresholds for optimaltransport based tests of independence.   The authors also provide a theoretical analysis of the Wasserstein barycenter and its probability distributions. The main result is that the maps can be computed with minimal smoothness assumptions. ,"This paper proposes to use Optimal transport maps for machine learning and statistics. The authors propose to use Plugin estimators for computing computational optimal transport maps, which are based on minimal smoothness assumptions. The main idea is to use a stability estimate for plug-in estimators and rates of convergences of the Wasserstein barycenter with respect to the probability distributions of Optimal Transport maps.  The authors show that the stability estimate can be used for barycentric projections. The paper also provides asymptotic detection thresholds for optimaltransport based tests of independence. "
9551,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,training efficiency CONJUNCTION useful feature extraction. useful feature extraction CONJUNCTION training efficiency. useful feature extraction EVALUATE-FOR dataset distillation methods. training efficiency EVALUATE-FOR dataset distillation methods. distributed kernel - based meta - learning framework USED-FOR dataset distillation. infinitely wide convolutional neural networks USED-FOR distributed kernel - based meta - learning framework. infinitely wide convolutional neural networks USED-FOR dataset distillation. test accuracy EVALUATE-FOR CIFAR10 image classification task. Fashion - MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Fashion - MNIST. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Fashion - MNIST CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION Fashion - MNIST. MNIST HYPONYM-OF settings. Fashion - MNIST HYPONYM-OF settings. CIFAR-100 HYPONYM-OF settings. CIFAR-10 HYPONYM-OF settings. they COMPARE naturally occurring data. naturally occurring data COMPARE they. distilled datasets COMPARE naturally occurring data. naturally occurring data COMPARE distilled datasets. Method is machine learning algorithms. ,This paper proposes a distributed kernel-based meta-learning framework for dataset distillation using infinitely wide convolutional neural networks. The authors show that the training efficiency and useful feature extraction can be improved by distillation methods. They also show that distillation improves test accuracy on the CIFAR10 image classification task. They show that distilled datasets are more efficient than naturally occurring data. ,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The authors show that the training efficiency and useful feature extraction can be improved by using infinitely wide convolutional neural networks. Experiments are conducted on CIFAR10 image classification task, showing that the proposed machine learning algorithms can achieve better test accuracy compared to other distillation methods. They also show that they can improve the test accuracy on naturally occurring data compared to distilled datasets. Experimental results are provided on three settings: MNIST, Fashion-MNIST, and SVHN."
9576,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,Semi - supervised learning ( SSL ) USED-FOR model. unlabeled data USED-FOR model. unlabeled data USED-FOR Semi - supervised learning ( SSL ). label space FEATURE-OF labeled and unlabeled data. FixMatch HYPONYM-OF SSL methods. Learning representations of inliers USED-FOR OSSL. FixMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION FixMatch. OpenMatch USED-FOR FixMatch. OpenMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION OpenMatch. threshold USED-FOR outliers. OVA - classifier USED-FOR confidence score. open - set soft - consistency regularization loss USED-FOR outlier detection. smoothness FEATURE-OF OVA - classifier. open - set soft - consistency regularization loss USED-FOR smoothness. OpenMatch COMPARE supervised model. supervised model COMPARE OpenMatch. CIFAR10 EVALUATE-FOR supervised model. Method is SSL algorithms. ,"This paper proposes a new semi-supervised learning (SSL) model for unlabeled data. The model is based on Semi-Supervised learning, where the label space is a mixture of labeled and unlabelled data, and the goal is to learn a model that is able to generalize well to unlabeling data. Previous SSL methods such as FixMatch and OpenMatch are based on learning representations of inliers. The main contribution of this paper is to introduce a new threshold for outlier detection based on the OVA-classifier. The authors show that OpenMatch and FixMatch are able to achieve better performance than a supervised model on CIFAR10. They also show that the smoothness achieved by the open-set soft-consistency regularization loss of an OVA -classifier can be used to improve the confidence score. ","This paper proposes a new model for Semi-supervised learning (SSL) with unlabeled data in the label space. The authors propose two SSL methods, FixMatch and OpenMatch, which are based on Learning representations of inliers. The main difference between the two SSL algorithms is that FixMatch uses a threshold to detect outliers, while OpenMatch uses an OVA-classifier to estimate the confidence score. The proposed supervised model is evaluated on CIFAR10, and the authors show that OpenMatch improves smoothness of the outlier detection with an open-set soft-consistency regularization loss."
9601,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"it USED-FOR achiever policy. it USED-FOR explorer. explorer CONJUNCTION achiever policy. achiever policy CONJUNCTION explorer. Latent Explorer Achiever ( LEXA ) HYPONYM-OF agent. imagined rollouts USED-FOR it. image inputs USED-FOR world model. imagined rollouts USED-FOR achiever policy. prior methods COMPARE explorer. explorer COMPARE prior methods. LEXA USED-FOR tasks. goal images zero - shot FEATURE-OF tasks. approaches USED-FOR unsupervised goal reaching. LEXA COMPARE approaches. approaches COMPARE LEXA. LEXA USED-FOR unsupervised goal reaching. prior benchmarks CONJUNCTION benchmark. benchmark CONJUNCTION prior benchmarks. test tasks EVALUATE-FOR benchmark. robotic manipulation and locomotion domains FEATURE-OF test tasks. benchmark EVALUATE-FOR approaches. test tasks EVALUATE-FOR LEXA. benchmark EVALUATE-FOR LEXA. prior benchmarks EVALUATE-FOR approaches. prior benchmarks EVALUATE-FOR LEXA. Method is artificial agents. OtherScientificTerm are complex visual environments, supervision, and achiever. ","This paper proposes a new agent called Latent Explorer Achiever (LEXA) which is an agent that uses imagined rollouts to learn an explorer and an achiever policy. LEXA learns to navigate in complex visual environments where the goal images zero-shot are not available. The world model is trained using image inputs from the world model, and it is trained to learn the explorer and the achiever. The agent is evaluated on a variety of test tasks in robotic manipulation and locomotion domains, and compared to prior methods on these tasks. The results show that LexA performs better than prior benchmarks and a benchmark on unsupervised goal reaching. ","This paper proposes a Latent Explorer Achiever (LEXA) agent, which is an agent that is able to explore complex visual environments without supervision. It can be seen as a combination of an explorer and an achiever policy, and it can be viewed as a world model with image inputs. The authors show that LEXA outperforms prior methods on a number of tasks on goal images zero-shot, and outperforms other approaches for unsupervised goal reaching on two prior benchmarks and two test tasks on robotic manipulation and locomotion domains."
9626,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"trainable parameters USED-FOR Language models. networks USED-FOR tasks. task EVALUATE-FOR networks. parameter sharing CONJUNCTION factorized representations. factorized representations CONJUNCTION parameter sharing. model compression CONJUNCTION parameter sharing. parameter sharing CONJUNCTION model compression. factorized representations CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION factorized representations. reshaped and rearranged original matrix USED-FOR low - rank factorized representation. expressiveness EVALUATE-FOR low - rank layers. embedding CONJUNCTION attention. attention CONJUNCTION embedding. attention CONJUNCTION feed - forward layers. feed - forward layers CONJUNCTION attention. approach USED-FOR Transformer models. OtherScientificTerm are lottery ticket hypothesis, parameter space, self - attention layers, parameter matrix, and architecture of the network. Generic is models. Method is factorized representations of matrices. Task is deep networks. Metric is on - task performance. ","This paper studies the problem of training language models with trainable parameters in the lottery ticket hypothesis. The authors propose a new approach to train Transformer models with parameterized representations of matrices. The proposed method is based on a reshaped and rearranged original matrix, which can be used as a low-rank factorized representation of the parameter space. Theoretical results show that the proposed method can improve on-task performance on a variety of tasks. ","This paper proposes a new approach to learning Transformer models that can be applied to language models. Language models are trained with trainable parameters. The authors propose to use the lottery ticket hypothesis, which is based on the fact that the parameter space can be represented as a low-rank factorized representation of a reshaped and rearranged original matrix, which can be used to learn networks for different tasks. The key idea of the paper is to use factorized representations of matrices to learn deep networks that are able to perform on-task performance on a new task. This is done by learning self-attention layers that are parameterized by the parameter matrix, and feed-forward layers that learn the architecture of the network. Experiments show that the proposed approach can improve the expressiveness of the lower-rank layers in terms of embedding, attention, and model compression."
9651,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"structured source code representations USED-FOR models. syntax trees HYPONYM-OF structured source code representations. them PART-OF attention module of Transformer. path encoding methods PART-OF attention module of Transformer. path encoding PART-OF them. them PART-OF unified Transformer framework. TPTrans COMPARE baselines. baselines COMPARE TPTrans. code summarization EVALUATE-FOR approaches. Task is Learning distributed representation of source code. Method is positional encoding. OtherScientificTerm are pairwise path, tree root, and syntax tree. Generic is paths. ","This paper studies the problem of learning a distributed representation of source code from structured source code. The authors propose a unified Transformer framework that combines them into a unified attention module of Transformer, where the pairwise path is represented as a tree root, and a positional encoding is used to encode the paths. They show that the proposed TPTrans outperforms existing approaches in code summarization.","This paper proposes a new model for learning a distributed representation of source code. The authors propose a unified Transformer framework that combines them with existing path encoding methods in the attention module of Transformer. The proposed models are based on structured source code representations such as syntax trees. The idea is to learn a pairwise path from source code to source code, where the source code is represented as a tree root, and the target code is encoded in a positional encoding. The paths are then used as input to a Transformer, which outputs the output of the Transformer on the tree root. Experiments on code summarization show that the proposed approaches outperform other baselines."
9676,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"them USED-FOR high - resolution image generation. Attention - based models USED-FOR long range dependency. Transformer HYPONYM-OF Attention - based models. Generative Adversarial Networks ( GANs ) USED-FOR high - resolution image generation. multi - axis blocked self - attention USED-FOR mixing of local and global attention. global self - attention COMPARE multi - axis blocked self - attention. multi - axis blocked self - attention COMPARE global self - attention. implicit neural function FEATURE-OF multi - layer perceptrons. cross - attention USED-FOR self - modulation component. model USED-FOR synthesizing high definition images. linear computational complexity EVALUATE-FOR model. unconditional ImageNet CONJUNCTION FFHQ 256 × 256. FFHQ 256 × 256 CONJUNCTION unconditional ImageNet. FFHQ 256 × 256 EVALUATE-FOR HiT. FID scores EVALUATE-FOR HiT. unconditional ImageNet EVALUATE-FOR HiT. OtherScientificTerm are quadratic complexity of self - attention operation, low - resolution stages of the generative process, self - attention, image size, and convolutions. Method are generative process, and GANs. Task is high - resolution stages. ","This paper studies the problem of high-resolution image generation with Attention-based models such as Transformer. The authors propose Generative Adversarial Networks (GANs) for high-resolution image generation. They show that GANs can achieve linear computational complexity with respect to multi-axis blocked self-attention for mixing of local and global attention. They also show that multi-layer perceptrons with an implicit neural function can achieve quadratic complexity of self- attention operation. Finally, they show that the model can be used for synthesizing high definition images by cross-modulating the self-modulation component of the generative process. ","This paper proposes a new generative adversarial network for high-resolution image generation. The proposed model is based on Generative Adversarial Networks (GANs) and is able to generate high-res images with quadratic complexity of self-attention operation. The authors show that GANs are able to achieve better performance than Attention-based models for long range dependency. The main contribution of the paper is to introduce a multi-axis block of local and global attention, which is a variant of the Transformer. The multi-layer perceptrons of the implicit neural function of the GAN can be decomposed into two parts. The first part is the low-resolution stages of the generative process, and the second part is a self-modulation component, where cross-attentive attention is applied to the image size and convolutions. The model can be used for synthesizing high definition images with linear computational complexity. The experimental results on unconditional ImageNet and FFHQ 256 × 256 show that the proposed model achieves better FID scores than HiT."
9701,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"query complexity EVALUATE-FOR learning geodesically convex halfspaces on graphs. Geodesic convexity HYPONYM-OF Euclidean convexity. treewidth CONJUNCTION minimum hull set size. minimum hull set size CONJUNCTION treewidth. query complexity CONJUNCTION VC dimension. VC dimension CONJUNCTION query complexity. query complexity EVALUATE-FOR Radon number. cut size FEATURE-OF labelling. approach COMPARE active learning algorithms. active learning algorithms COMPARE approach. ground - truth communities PART-OF real - world graphs. OtherScientificTerm are convex sets, diameter, and separation axioms. Material is unlabelled graph. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs with Geodesic convexity (e.g., Euclidean convexities). The authors consider convex sets with treewidth and minimum hull set size, where the diameter is a function of the number of vertices. The authors show that the Radon number of queries is much smaller than the VC dimension, which is a good indicator of query complexity. They also show that this approach is more efficient than active learning algorithms. Finally, they show that their approach can be applied to real-world graphs with ground-truth communities. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs. Geodesic convexity is a special case of Euclidean convexities, where the convex sets are of different sizes (e.g. treewidth, minimum hull set size, etc.). The authors show that the Radon number of a Radon with respect to the query dimension and the VC dimension is the same as that of an unlabelled graph. The authors also show that this approach is more efficient than active learning algorithms on real-world graphs with ground-truth communities. The main contribution of the paper is the analysis of the cut size in labelling, which is a measure of the difference between the diameter and the length of the separation axioms."
9726,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"action localization dataset EVALUATE-FOR TAL head. large action classification dataset EVALUATE-FOR video encoder. transfer learning pipeline USED-FOR temporal action localization ( TAL ) methods. video encoder USED-FOR action classification. video encoder USED-FOR task discrepancy problem. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. TAL head USED-FOR joint optimization. video encoder USED-FOR joint optimization. this USED-FOR TAL. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. temporal, spatial or spatio - temporal resolution FEATURE-OF mini - batch composition. LoFi optimization approach USED-FOR TAL methods. ResNet18 based video encoder USED-FOR method. single RGB stream FEATURE-OF ResNet18 based video encoder. OtherScientificTerm are encoder, GPU memory constraints, mid - range hardware budget, gradients, and TAL supervision loss. Method are TAL learning, and feature representations. ","This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods. The video encoder is trained on a large action classification dataset and the TAL head is used to solve the task discrepancy problem. The proposed method is based on a ResNet18 based videoencoder with a single RGB stream. The authors show that the encoder can be trained with GPU memory constraints and that this can be used to improve TAL. They also show that mini-batch composition with temporal, spatial or spatio-temporal resolution can improve the performance of TAL learning. Finally, the authors propose a LoFi optimization approach to optimize TAL methods. ","This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods. The video encoder is trained on a large action classification dataset, and the TAL head is used to solve the task discrepancy problem. The proposed method is based on a ResNet18 based videoencoder with a single RGB stream. The authors show that the encoder can be trained with GPU memory constraints, and that the mid-range hardware budget can be reduced. The paper also shows that TAL learning can be done with mini-batch composition with temporal, spatial or spatio-temporal resolution. The main contribution of the paper is to propose a LoFi optimization approach for TAL methods, which can be applied to both video and tAL head for joint optimization. In addition, the paper also proposes a TAL supervision loss to improve the performance of TAL."
9751,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"Gaussian design matrix CONJUNCTION arbitrary 2 noise distribution. arbitrary 2 noise distribution CONJUNCTION Gaussian design matrix. Gaussian design matrix FEATURE-OF convex penalty in linear models. arbitrary 2 noise distribution FEATURE-OF convex penalty in linear models. gradient - Lipschitz loss function USED-FOR M - estimators. Huber loss CONJUNCTION Elastic - Net penalty. Elastic - Net penalty CONJUNCTION Huber loss. heavy - tails FEATURE-OF noise distribution. Elastic - Net penalty USED-FOR robust M - estimator. Huber loss USED-FOR robust M - estimator. differentiability structure FEATURE-OF convex regularized M - estimators. adaptive criterion USED-FOR regularized M - estimators. criterion USED-FOR out - of - sample error. noise distribution CONJUNCTION covariance of the design. covariance of the design CONJUNCTION noise distribution. criterion USED-FOR out - of - sample error. OtherScientificTerm are differentiation, intermediate high - dimensional 9 regime, dimension, distribution of the residuals, and out - of - sample 14 error. Generic is derivatives. Material is Simulated data. Method is M - estimator. ","This paper studies the convex penalty in linear models with a Gaussian design matrix and an arbitrary 2 noise distribution. The authors propose a gradient-Lipschitz loss function for M-estimators with a differentiability structure. They show that the noise distribution with heavy-tails can be approximated by a Huber loss and an Elastic-Net penalty for a robust M-iterator. They also provide an adaptive criterion for the out-of-sample error of convex regularized M- estimators with differentiability.    The main contribution of this paper is to provide a new definition of differentiation in the intermediate high-dimensional 9 regime. Simulated data is used to define the dimension of the residuals, and the derivatives are then used to train the M-averaged estimator. ","This paper studies the convex penalty in linear models with a Gaussian design matrix and an arbitrary 2 noise distribution. The authors propose a gradient-Lipschitz loss function for M-estimators with heavy-tailed noise distribution and a Huber loss for the robust M-iterator. The main contribution of the paper is to study the differentiability structure of convex regularized M-ensemblers with differentiability criterion for out-of-sample error under the intermediate high-dimensional 9 regime. Simulated data is generated from a set of derivatives of the original data, and the authors show that the density of the derivatives is the same as the dimension of the input data. The paper also shows that the distribution of the residuals of the data is similar to that of the initial data. "
9776,SP:be53bc4c064402489b644332ad9c17743502d73c,"calibrated beam - based algorithm USED-FOR neural abstractive summarization. calibrated beam - based algorithm USED-FOR local optimality problem. beam search USED-FOR local optimality problem. global attention distribution FEATURE-OF calibrated beam - based algorithm. attention distribution USED-FOR global protocol. global scoring mechanism USED-FOR beam search. global scoring mechanism USED-FOR beam search. global ( attention)-aware inference COMPARE summarization models. summarization models COMPARE global ( attention)-aware inference. empirical hyper - parameters USED-FOR summarization models. empirical hyper - parameters USED-FOR global ( attention)-aware inference. Generic are design, and algorithm. Task is inference. OtherScientificTerm is corrupted attention distributions. ",This paper proposes a calibrated beam-based algorithm for neural abstractive summarization. The proposed algorithm is based on the global attention distribution of the global protocol. The global scoring mechanism is used to perform beam search for the local optimality problem in beam search. The authors show that the global (attention)-aware inference performs better than summarization models with empirical hyper-parameters. ,This paper proposes a calibrated beam-based algorithm for neural abstractive summarization. The key idea is to use a global attention distribution for the local optimality problem. The proposed algorithm is based on a global scoring mechanism for beam search. The authors show that the proposed design is more robust to corrupted attention distributions. They also show that global (attention)-aware inference outperforms summarization models with empirical hyper-parameters.
9801,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"Attention mechanism USED-FOR deep learning models. relative position encoding PART-OF deep learning models. canonical local coordinate system USED-FOR neighborhoods. attention USED-FOR manifolds. method USED-FOR feature vectors. regular field of cyclic groups USED-FOR feature fields. regular field of cyclic groups USED-FOR intermediate layers. feature fields USED-FOR intermediate layers. feature vectors USED-FOR fields. method USED-FOR expressive ability. regular field of cyclic groups USED-FOR expressive ability. position vector USED-FOR orientation of the coordinate system. ambient space FEATURE-OF orientation of the coordinate system. local coordinate system USED-FOR position vector. global coordinate system HYPONYM-OF orientation of the coordinate system. gauge equivariance USED-FOR self - attention. triangle meshes USED-FOR Gauge Equivariant Transformer ( GET ). common recognition tasks EVALUATE-FOR GET. Method are equivariant transformer, and multi - head selfattention. OtherScientificTerm are orientation of local coordinate systems, gauge equivariant, position - based and content - based information, and rotation invariance. ","This paper proposes a new attention mechanism for deep learning models based on relative position encoding in the context of the orientation of local coordinate systems. Specifically, the canonical local coordinate system is used to map neighborhoods to manifolds. The authors propose a method to learn feature vectors for intermediate layers based on the regular field of cyclic groups and feature fields for the intermediate layers. The proposed equivariant transformer, called the Gauge Equivariant Transformer (GET), is a multi-head selfattention. The key idea is to learn a position vector for each orientation of the coordinate system in the ambient space, which is a global coordinate system. This position vector is then used as a gauge equivarianant for self-attention, and the authors show that the proposed method is able to learn an expressive ability by using the feature vectors of the fields. Experiments on a variety of common recognition tasks demonstrate the effectiveness of the proposed GET.","This paper proposes a new attention mechanism for deep learning models that is based on relative position encoding. The authors introduce a canonical local coordinate system that maps neighborhoods to manifolds. The proposed equivariant transformer (Gauge Equivariant Transformer) is a multi-head selfattention, where each layer takes as input the position-based and content-based information, and applies the attention to the manifolds in the direction of the orientation of local coordinate systems in the ambient space. The main idea is to use a regular field of cyclic groups for intermediate layers and feature fields for the intermediate layers. This method is shown to improve the expressive ability of the proposed method for learning feature vectors for different fields. In addition, the authors propose to use gauge equivariance for self-attention. The paper shows that the position vector of the coordinate system in ambient space is a global coordinate system with rotation invariance, and the authors show that the proposed gauge equivant can be used to learn a position vector that is invariant to rotation. Experiments on triangle meshes are conducted to demonstrate the effectiveness of the Gauge Equivarianant Transformers (GET) on common recognition tasks."
9826,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"methods USED-FOR unsupervised learning of finite mixture models. expectation maximization CONJUNCTION Metropolis - Hastings algorithm. Metropolis - Hastings algorithm CONJUNCTION expectation maximization. Metropolis - Hastings algorithm USED-FOR approach. expectation maximization PART-OF approach. it USED-FOR shallow and deep mixture 8 models. mixtures of normalizing flows CONJUNCTION sum - product ( transform ) networks. sum - product ( transform ) networks CONJUNCTION mixtures of normalizing flows. synthetic and real - data 10 contexts EVALUATE-FOR deep models. sum - product ( transform ) networks HYPONYM-OF deep models. mixtures of normalizing flows HYPONYM-OF deep models. synthetic and real - data 10 contexts EVALUATE-FOR method. Method is finite mixture models. OtherScientificTerm are mixture, and complex, and possibly nonlinear, transformations. Metric is computational cost. ","This paper proposes a new method for unsupervised learning of finite mixture models. The proposed approach combines expectation maximization and the Metropolis-Hastings algorithm. The authors show that it can be applied to shallow and deep mixture 8 models, including mixtures of normalizing flows, sum-product (transform) networks, and deep models with complex, and possibly nonlinear, transformations. Empirical results on synthetic and real-data 10 contexts demonstrate the effectiveness of the proposed method. ","This paper proposes two methods for unsupervised learning of finite mixture models. The approach is based on expectation maximization and the Metropolis-Hastings algorithm. The authors show that it outperforms shallow and deep mixture 8 models in both synthetic and real-data 10 contexts for deep models such as mixtures of normalizing flows, sum-product (transform) networks, and deep models with complex, and possibly nonlinear, transformations. The computational cost is also reduced."
9851,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"Sparse training USED-FOR deep neural networks. dense computation USED-FOR backward propagation step. sparse forward and backward passes USED-FOR sparse training method. global sparsity constraint FEATURE-OF continuous minimization problem. continuous minimization problem USED-FOR training process. weight update CONJUNCTION structure parameter update. structure parameter update CONJUNCTION weight update. structure parameter update HYPONYM-OF steps. weight update HYPONYM-OF steps. chain rule USED-FOR step. sparse structure USED-FOR chain rule. variance reduced policy gradient estimator USED-FOR sparse training. forward passes CONJUNCTION backward propagation. backward propagation CONJUNCTION forward passes. chain rule based gradient estimators USED-FOR variance reduced policy gradient estimator. variance reduced policy gradient estimator USED-FOR step. chain rule based gradient estimators USED-FOR step. forward passes USED-FOR variance reduced policy gradient estimator. algorithm USED-FOR training process. real - world datasets EVALUATE-FOR algorithm. OtherScientificTerm is memory usage. Method are neural networks, and gradient estimator. Generic is methods. Task is optimization process. ","This paper proposes a new sparse training method based on sparse forward and backward passes for deep neural networks with dense computation. The training process is a continuous minimization problem with global sparsity constraint. The authors propose two steps: weight update and structure parameter update. The first step is a dense computation for the backward propagation step, and the second step is based on a chain rule for the forward pass. The gradient estimator is a combination of the chain rule and the sparse structure of the forward passes.  The authors show that the proposed step can be trained with the variance reduced policy gradient estimators for sparse training with forward passes and backward propagation. The proposed algorithm is evaluated on real-world datasets.",This paper proposes a new sparse training method that combines sparse forward and backward passes for deep neural networks. The authors propose a continuous minimization problem with global sparsity constraint for the training process. The paper proposes two steps: weight update and structure parameter update. The first step uses dense computation for the backward propagation step and the second step uses a chain rule for the forward propagation step. The variance reduced policy gradient estimator is used for sparse training. The algorithm is evaluated on two real-world datasets. 
9876,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"importance samplers ( IS ) CONJUNCTION Markov chain Monte Carlo ( MCMC ) samplers. Markov chain Monte Carlo ( MCMC ) samplers CONJUNCTION importance samplers ( IS ). iterated sampling - importance resampling mechanism USED-FOR π. NEO - IS CONJUNCTION iterated sampling - importance resampling mechanism. iterated sampling - importance resampling mechanism CONJUNCTION NEO - IS. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. NEO - MCMC USED-FOR π. NEO - IS USED-FOR NEO - MCMC. iterated sampling - importance resampling mechanism USED-FOR NEO - MCMC. NEO - MCMC USED-FOR multimodal targets. T USED-FOR conformal Hamiltonian system. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. discrete - time integrator USED-FOR conformal Hamiltonian system. T USED-FOR NEO - IS. T USED-FOR discrete - time integrator. NEO - MCMC USED-FOR explicit mixing time estimates. OtherScientificTerm are complex distribution π, intractable normalizing constant, invertible map T, forward and backward Orbits, proposal distribution ρ, map T, NEO, Non - Equilibrium Orbits, and normalizing constant. Generic are schemes, and methods. ","This paper studies the problem of estimating the importance samplers (IS) and the Markov chain Monte Carlo (MCMC) sampler (NEO-IS) for multimodal targets with complex distribution π. The authors propose two schemes: (1) a novel iterated sampling-importance resampling mechanism for π, and (2) an iterated estimator of the intractable normalizing constant for the invertible map T. Theoretical results show that the proposed methods outperform the state-of-the-art NEO-IS and NEO-MCMC in terms of both forward and backward Orbits. In particular, the authors show that T can be used as a discrete-time integrator for the conformal Hamiltonian system. ","This paper proposes two new schemes to improve the performance of importance samplers (IS) and Markov chain Monte Carlo (MCMC) sampler. The main idea is to learn a complex distribution π, where the invertible map T is the sum of forward and backward Orbits, and the proposal distribution ρ is the map T. The authors propose two methods to learn the complex distribution. The first is to use an iterated sampling-importance resampling mechanism to learn π. The second one uses a discrete-time integrator to learn explicit mixing time estimates. The proposed methods are evaluated on two multimodal targets. The results show that the proposed NEO-IS outperforms the proposed MCMC and the proposed T for the conformal Hamiltonian system. "
9901,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,training CONJUNCTION inference. inference CONJUNCTION training. permutation invariance CONJUNCTION equivariance. equivariance CONJUNCTION permutation invariance. permutation invariance FEATURE-OF set - function constraints. equivariance FEATURE-OF set - function constraints. property USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) HYPONYM-OF property. attention - based set encoding mechanism USED-FOR set representations. mini - batch processing of sets USED-FOR attention - based set encoding mechanism. symmetries of invariance CONJUNCTION equivariance. equivariance CONJUNCTION symmetries of invariance. MBC USED-FOR method. symmetries of invariance FEATURE-OF method. method USED-FOR rich set encoding representations. rich set encoding representations USED-FOR set - structured data. Method is set encoding algorithms. OtherScientificTerm is computational and memory resources. Generic is assumptions. Task is large - scale set encoding. ,"This paper proposes a new property called Mini-Batch Consistency (MBC) for large scale mini-batch set encoding, which is based on permutation invariance and equivariance of the set-function constraints. The authors show that the attention-based set encoding mechanism can be used to learn set representations from mini-batches of sets. The proposed method uses MBC to learn rich set encoding representations for set-structured data. The paper also shows that the proposed method is able to learn symmetries of invariance as well as equivariances. ","This paper proposes a new property for large scale mini-batch set encoding, Mini-Batch Consistency (MBC), which is based on permutation invariance and equivariance of set-function constraints. The authors propose an attention-based set encoding mechanism for set representations, which uses mini-batches of sets. The proposed method is evaluated on rich set encoding representations for set-structured data, and is shown to outperform existing set encoding algorithms on both computational and memory resources. "
9926,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"Diplomacy HYPONYM-OF game. human data USED-FOR policy. Diplomacy CONJUNCTION StarCraft. StarCraft CONJUNCTION Diplomacy. StarCraft CONJUNCTION Dota. Dota CONJUNCTION StarCraft. branching factors FEATURE-OF games. Diplomacy HYPONYM-OF branching factors. StarCraft HYPONYM-OF branching factors. Dota HYPONYM-OF games. Diplomacy HYPONYM-OF games. StarCraft HYPONYM-OF games. action exploration CONJUNCTION equilibrium approximation. equilibrium approximation CONJUNCTION action exploration. algorithm USED-FOR action exploration. algorithm USED-FOR equilibrium approximation. algorithm USED-FOR policy proposal network. value iteration USED-FOR algorithm. policy USED-FOR model training. equilibrium search procedure USED-FOR policy. equilibrium search procedure USED-FOR model training. algorithm USED-FOR agent. DORA USED-FOR two - player variant of Diplomacy. agent USED-FOR two - player variant of Diplomacy. DORA HYPONYM-OF agent. methods USED-FOR full - scale no - press Diplomacy. human data USED-FOR agent. agent COMPARE human - data bootstrapped agents. human - data bootstrapped agents COMPARE agent. self play USED-FOR superhuman performance. multiple equilibria FEATURE-OF Diplomacy. Diplomacy FEATURE-OF superhuman performance. Task is complex games. Method are handcrafted reward shaping, and double oracle step. OtherScientificTerm are combinatorial action spaces, and policy proposals. Generic is it. ","This paper studies the problem of handcrafted reward shaping in complex games. Diplomacy is a game with branching factors with multiple branching factors: Diplomacy, StarCraft, and Dota. The goal is to learn a policy from human data, where the policy is learned using human data and the policy proposal network is trained using a value iteration. The algorithm is based on an equilibrium search procedure to find the optimal policy for each action in the action exploration and an equilibrium approximation to the policy. The authors show that this algorithm can be used to train an agent with DORA and a two-player variant of Diplomacy. The agent is trained with human data as well as the DORa and the two player variant. The agents are trained using self play and self-play is shown to improve superhuman performance in Diplomacy with multiple equilibria. ","This paper studies the problem of full-scale no-press Diplomacy in complex games. Diplomacy is a game with multiple equilibria, where the goal is to learn a policy from human data. The paper proposes a novel algorithm for learning a policy proposal network, which is based on value iteration. The algorithm is a two-player variant of Diplomacy, where a handcrafted reward shaping is used to guide the action exploration and an equilibrium approximation is used for action exploration, and a double oracle step is used in the equilibrium search procedure for model training. The agent is trained on DORA, and the results show that the agent outperforms human-data bootstrapped agents on self play in terms of superhuman performance. "
9951,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,it USED-FOR sequence modeling. attention heads PART-OF Multi - head attention. positive transfer CONJUNCTION negative interference. negative interference CONJUNCTION positive transfer. Multilingual and multi - domain learning USED-FOR sequence modeling. generalization EVALUATE-FOR non - selective attention sharing. attention sharing strategies USED-FOR multilingual and multi - domain sequence modeling. approach USED-FOR shared and specialized attention heads. attention sharing strategies USED-FOR sequence models. tasks EVALUATE-FOR attention sharing strategies. tasks EVALUATE-FOR sequence models. speech recognition HYPONYM-OF tasks. multi - head attention USED-FOR sequence models. BLEU EVALUATE-FOR multi - domain setting. BLEU EVALUATE-FOR speech - to - text translation. approach USED-FOR speech - to - text translation. multilingual setting EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. ,"This paper proposes a multi-head attention-based multi-domain attention-sharing method for sequence modeling. The proposed method is based on the idea of multi-headed attention, where the attention heads are shared across multiple domains. The authors show that the proposed method can achieve better generalization performance than the existing attention sharing methods. The main contribution of the paper is the use of attention sharing strategies to improve the performance of sequence models on a variety of tasks, including speech recognition, speech-to-text translation, and video recognition.","This paper proposes a multi-head attention method for multi-domain learning. The authors propose to use attention heads in Multi-Head attention to improve the generalization performance of sequence modeling. The main idea is to use shared and specialized attention heads to encourage non-selective attention sharing. The proposed approach is evaluated on three tasks: speech recognition, speech-to-text translation, and BLEU. The results show the effectiveness of the proposed approach in the multilingual setting and in the multi-domains setting. "
9976,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,covariate shift HYPONYM-OF distribution shift. covariate shift FEATURE-OF real - world applications. high - dimensional asymptotics FEATURE-OF random feature regression. limiting test error CONJUNCTION bias. bias CONJUNCTION limiting test error. covariate shift USED-FOR random feature regression. robustness EVALUATE-FOR overparameterized models. Method is machine learning models. OtherScientificTerm is conditional label distributions. Task is machine learning. ,This paper studies the problem of distribution shift in machine learning models. The authors show that the covariate shift in real-world applications such as random feature regression with high-dimensional asymptotics can lead to a limiting test error and bias. They also show that overparameterized models can suffer from robustness issues. The main contribution of the paper is a theoretical analysis of conditional label distributions. ,"This paper studies the covariate shift in real-world applications of machine learning models, i.e., the distribution shift between conditional label distributions. The authors show that random feature regression with high-dimensional asymptotics can be performed with high variance, and that the limiting test error and bias can be reduced. The paper also shows that the robustness of overparameterized models can be improved by minimizing the variance of the covariated shift. "
10001,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"Thompson sampling CONJUNCTION Bayesian sequential decision - making algorithms. Bayesian sequential decision - making algorithms CONJUNCTION Thompson sampling. Bayesian sequential decision - making algorithms USED-FOR explore / exploit trade - offs. Thompson sampling USED-FOR explore / exploit trade - offs. explore / exploit trade - offs FEATURE-OF ( contextual ) bandits. prior USED-FOR algorithms. expected reward EVALUATE-FOR Thompson sampling ( TS ). misspecified prior USED-FOR Thompson sampling ( TS ). well - specified prior USED-FOR TS. parametric form FEATURE-OF prior. universal constants FEATURE-OF it. bounded support FEATURE-OF priors. algorithms USED-FOR Bayesian meta - learning setting. generic PAC guarantees USED-FOR algorithms. Bayesian POMDPs HYPONYM-OF Bayesian decision - making setting. knowledge gradient algorithm ( KG ) HYPONYM-OF Bayesian decision - making algorithms. multi - armed and contextual bandits USED-FOR meta - learning. structured and correlated priors FEATURE-OF multi - armed and contextual bandits. structured and correlated priors USED-FOR meta - learning. OtherScientificTerm are domain knowledge, misspecification, total - variation distance, learning horizon, cardinality or structure of the action space, sensitivity analysis, and prior misspecification. Generic is bound. Method are contextual bandits, and KG ). ","This paper studies Thompson sampling and Bayesian sequential decision-making algorithms for explore/exploit trade-offs in (contextual) bandits. The authors propose a new bound on the expected reward of Thompson sampling (TS) with a misspecified prior in a parametric form, which is a well-specified prior for TS with bounded support in the form of universal constants.  The authors show that these algorithms have generic PAC guarantees in the Bayesian meta-learning setting (e.g., Bayesian POMDPs) and in the multi-armed and contextual bandits setting (i.e., structured and correlated priors). The authors also provide a sensitivity analysis of the performance of these algorithms in the context of contextual bandits. ","This paper proposes a new bound on the expected reward of Thompson sampling and Bayesian sequential decision-making algorithms for (contextual) bandits with explore/exploit trade-offs. The bound is based on a well-specified prior in a parametric form. The authors show that Thompson sampling (TS) has a lower expected reward than Bayesian POMDPs in the Bayesian meta-learning setting. They also show that the proposed prior is more robust to misspecification, and that it has bounded support on the priors of the action space. They further show that their algorithms have generic PAC guarantees on the learning horizon, and they provide a sensitivity analysis on the cardinality or structure of the actions. Finally, the authors propose a knowledge gradient algorithm (KG) that can be applied to multi-armed and contextual bandits for meta-Learning with structured and correlated priors."
10026,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"PAC - learning model CONJUNCTION Equivalence - Query - learning model. Equivalence - Query - learning model CONJUNCTION PAC - learning model. sample / query complexity EVALUATE-FOR PAC - learning model. exponential separation FEATURE-OF sample / query complexity. adversarial training COMPARE training. training COMPARE adversarial training. adversarial training USED-FOR generalization. on - manifold adversarial examples USED-FOR adversarial training. Method are PAC model, Equivalence - Query model, adversarial model, and Equivalance - Query model. OtherScientificTerm are teacher, learner, PAC bound, adversarial examples, norm constraint, and adversary. Metric are adversarial robustness, and robustness. Generic is model. ","This paper studies the problem of adversarial robustness in PAC-learning model and Equivalence-Query-learning models. The authors show that the PAC model has exponential separation between the sample/query complexity of the adversarial model and the Equivalance-Query model. They also show that adversarial training on on-manifold adversarial examples can improve the generalization of the model. Finally, they provide a PAC bound for the robustness of the PAC bound. ",The paper proposes a PAC model and an Equivalence-Query-learning model. The PAC bound is defined as the distance between the teacher and the learner. The Equivalance-Query model defines the PAC bound as the difference between the sample/query complexity and the exponential separation. The authors show that adversarial training on on-manifold adversarial examples can improve the generalization of the model. They also provide a norm constraint to measure the robustness of the adversary.
10051,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"models USED-FOR transfer learning. benchmarks EVALUATE-FOR task. techniques USED-FOR algorithms. methods USED-FOR PARC. PARC COMPARE methods. methods COMPARE PARC. methods USED-FOR diverse model selection. diverse model selection EVALUATE-FOR PARC. model selection USED-FOR transfer learning. Method is pretrained deep learning models. Material is large model banks. OtherScientificTerm is diversity of off - the - shelf models. Generic are model, and setting. Task is Scalable Diverse Model Selection. ","This paper studies the problem of diverse model selection in the context of transfer learning, where the goal is to select the best model from a large set of models for transfer learning. In this setting, the authors consider the setting where the number of models is large and the diversity of off-the-shelf models is limited. The authors propose two techniques to select diverse models from the large model banks. They show that PARC outperforms existing methods for diverse modelselection in terms of performance on several benchmarks. ","This paper proposes a new task of learning diverse models for transfer learning from large model banks. The authors propose Scalable Diverse Model Selection (PARC), a new setting where the diversity of off-the-shelf models can be used to improve the performance of pretrained deep learning models. PARC is evaluated on a number of benchmarks and compared to existing methods for diverse model selection. The results show that PARC outperforms other methods for PARC in terms of transfer learning. However, the authors also show that the proposed algorithms are more computationally expensive than existing techniques."
10076,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"low - dimensional binary codes USED-FOR compression of high - dimensional neural representations. large bit - codes USED-FOR compression of high - dimensional neural representations. method USED-FOR Low - dimensional binary Codes ( LLC ). method USED-FOR low - dimensional binary codes. annotated attributes CONJUNCTION label meta - data. label meta - data CONJUNCTION annotated attributes. label meta - data HYPONYM-OF side - information. annotated attributes HYPONYM-OF side - information. it USED-FOR image retrieval. it USED-FOR codes. binary codes COMPARE 10 dimensional real representations. 10 dimensional real representations COMPARE binary codes. binary codes COMPARE HashNet. HashNet COMPARE binary codes. ImageNet-100 retrieval problem EVALUATE-FOR binary codes. Material is ImageNet-1 K. Metric is classification accuracy. Method is ResNet50. Task is OOD detection. Generic are baseline, and Code. OtherScientificTerm is threshold. ","This paper proposes a method for low-dimensional binary codes (LLC) for compression of high-dimensional neural representations with large bit-codes. The proposed method is based on ResNet50, which is a baseline for OOD detection. The main idea is to use the annotated attributes and label meta-data as side-information. The authors show that it improves classification accuracy on ImageNet-1K. The paper also shows that it can be used for image retrieval and it can learn codes that are more efficient than HashNet. ","This paper proposes a method for learning Low-dimensional binary Codes (LLC) for compression of high-dimensional neural representations with large bit-codes. The proposed method is based on ResNet50, which is a baseline for OOD detection. The main idea is to use annotated attributes and label meta-data for side-information. The authors show that the proposed method improves classification accuracy over HashNet on ImageNet-1 K. They also show that it improves the classification accuracy on image retrieval, and that it can be used to learn codes that are more similar to 10 dimensional real representations. "
10101,SP:07def8c80d05f86402ce769313480b30cd99af43,"computational / storage costs EVALUATE-FOR convolutional neural networks ( CNNs ). model compression techniques CONJUNCTION adversarial training. adversarial training CONJUNCTION model compression techniques. adversarial perturbations FEATURE-OF robustness. throughput ( frames - per - second ) EVALUATE-FOR methods. GDWS USED-FOR pre - trained network. throughput EVALUATE-FOR pre - trained network. real - life hardware FEATURE-OF pre - trained network. robustness EVALUATE-FOR GDWS. throughput EVALUATE-FOR GDWS. pre - trained models USED-FOR it. algorithms USED-FOR GDWS convolutions. 2D convolution approximator USED-FOR GDWS. complexity and error constraints USED-FOR algorithms. ImageNet datasets EVALUATE-FOR GDWS. CIFAR-10 EVALUATE-FOR GDWS. Task is robust model compression. Method are Generalized Depthwise - Separable ( GDWS ) convolution, and 2D convolution. ","This paper studies the computational/storage costs of convolutional neural networks (CNNs). The authors consider the problem of robust model compression and adversarial training. The authors propose two methods to improve the throughput (frames-per-second) of a pre-trained network on real-life hardware. The main contribution of the paper is the Generalized Depthwise-Separable (GDWS) convolution, which is based on the 2D convolution approximator.  The authors show that GDWS improves the robustness against adversarial perturbations. They also show that it can be used to train a large number of different algorithms for GDWS convolutions on ImageNet datasets. The algorithms are based on complexity and error constraints. ","This paper studies the computational/storage costs of convolutional neural networks (CNNs) for robust model compression and adversarial training. The authors propose two methods to improve the throughput (frames-per-second) of the pre-trained network on real-life hardware. The main contribution of the paper is to propose Generalized Depthwise-Separable (GDWS) convolution, which is a variant of 2D convolution approximator. GDWS can be applied to any pre-training network, and it is shown that it is robust to adversarial perturbations in terms of robustness. The algorithms for GDWS convolutions are evaluated on CIFAR-10 and ImageNet datasets, and the authors show that the algorithms are robust to complexity and error constraints."
10126,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"Retrosynthesis prediction HYPONYM-OF organic synthesis. neural models USED-FOR task. model USED-FOR graph edits. model USED-FOR synthons. top-1 accuracy EVALUATE-FOR model. OtherScientificTerm are precursor molecules, graph topology, and chemical reaction. Method are model design, graph - based approach, and manual correction. Generic is architecture. ","The paper proposes a new method for synthesizing synthetic molecules from synthetic data. The method is based on a graph-based approach, where the model is trained on a set of synthetic data, and the goal is to predict the chemical reaction of a molecule. The authors propose a new architecture that can be applied to a variety of synthetic and real-world problems. The proposed method is evaluated on a number of synthetic datasets, and shows that the proposed method outperforms the baselines.","This paper proposes a novel approach to synthesizing synthetic molecules. The authors propose a graph-based approach where the model design is based on the graph topology of the molecule, and the model is trained using neural models to solve the task of organic synthesis. The key idea of the architecture is to learn the graph edits of the molecules, and then use the model to synthesize synthons from the synthesized molecule. The model is evaluated on top-1 accuracy of the synthons, and on the top-2 accuracy of synthesized molecules. "
10151,SP:772277d969c95924755113c86663fb0e009f24cc,"Bayesian formulation of deconditioning USED-FOR reproducing kernel Hilbert space formulation. deconditioning USED-FOR downscaling setup. conditional mean embedding estimator USED-FOR multiresolution data. solution USED-FOR deconditioning problem. posterior USED-FOR deconditioning problem. posterior USED-FOR latent field. posterior USED-FOR solution. minimax optimal convergence rate FEATURE-OF it. its EVALUATE-FOR methods. OtherScientificTerm are high - resolution ( HR ) information, LR samples, mediating variable, conditional expectation, conditional expectations, and inter - domain features. Task are statistical downscaling, and recovery of the underlying fine - grained field. Material is spatial datasets. ","This paper proposes a Bayesian formulation of deconditioning for reproducing kernel Hilbert space formulation. The main idea is to use a conditional mean embedding estimator for multiresolution data with high-resolution (HR) information. The authors show that this solution can be used to solve the decongestioning problem with a posterior on the latent field, and that it has a minimax optimal convergence rate. They also show that their methods can recover the underlying fine-grained field. ","This paper proposes a Bayesian formulation of deconditioning for reproducing kernel Hilbert space formulation. The authors propose a downscaling setup where the high-resolution (HR) information is replaced by a set of LR samples and the low-resolution (LR) information are replaced by the residuals of the underlying fine-grained field. The main idea is to use a conditional mean embedding estimator for multiresolution data, where the conditional expectation of the mediating variable is the sum of the conditional expectations of the inter-domain features. The proposed solution to the decunditioning problem is based on the posterior of the latent field and the minimax optimal convergence rate of it. Experiments on spatial datasets show that the proposed methods outperform existing methods in terms of its performance."
10176,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"Deep sparse networks ( DSNs ) USED-FOR high - order feature interactions. highsparsity features FEATURE-OF prediction task. prediction task EVALUATE-FOR Deep sparse networks ( DSNs ). computation efficiency EVALUATE-FOR models. feature - interaction layer PART-OF DSNs. neural architecture search USED-FOR problem. distilled search space USED-FOR architectures. progressive search algorithm USED-FOR sparse prediction tasks. progressive search algorithm USED-FOR order - priority property. order - priority property FEATURE-OF sparse prediction tasks. Task is model inference. Material is real - world benchmark datasets. Metric are accuracy, and efficiency. Method is search algorithm. ","This paper studies the problem of high-order feature interactions in Deep sparse networks (DSNs) for a prediction task with highsparsity features. The authors propose a new progressive search algorithm for sparse prediction tasks with order-priority property. The proposed search algorithm is based on neural architecture search, and is able to solve the problem in a distilled search space. Empirically, the authors show that the proposed models achieve better computation efficiency than existing models with a feature-interaction layer in DSNs. ","This paper studies the problem of predicting high-order feature interactions in Deep sparse networks (DSNs) for high-sparsity features in a prediction task. The authors propose a novel progressive search algorithm for sparse prediction tasks with order-prior property. The proposed search algorithm is based on neural architecture search. The paper shows that models with high computation efficiency achieve better prediction task performance than models with low computation efficiency on real-world benchmark datasets. The main contribution of the paper is to introduce a feature-interaction layer in DSNs, which allows for better accuracy and efficiency. The architectures are trained in a distilled search space."
10201,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,algorithm USED-FOR transfer learning. pre - trained model USED-FOR task. fine - tuning HYPONYM-OF algorithm. labeled data USED-FOR pre - trained model. labeled data USED-FOR task. fine - tuning USED-FOR overfitting. noise FEATURE-OF robustness. generalization properties EVALUATE-FOR fine - tuning. noise stability FEATURE-OF fine - tuned model. self label - correction CONJUNCTION label - reweighting. label - reweighting CONJUNCTION self label - correction. layer - wise regularization CONJUNCTION self label - correction. self label - correction CONJUNCTION layer - wise regularization. interpolation between regularization and self - labeling methods PART-OF regularized self - labeling. layer - wise regularization PART-OF interpolation between regularization and self - labeling methods. self label - correction PART-OF regularized self - labeling. layer - wise regularization PART-OF regularized self - labeling. pre - trained model architectures USED-FOR image and text data sets. image and text data sets EVALUATE-FOR approach. pre - trained model architectures USED-FOR approach. image classification tasks CONJUNCTION few - shot classification task. few - shot classification task CONJUNCTION image classification tasks. approach COMPARE baseline methods. baseline methods COMPARE approach. few - shot classification task EVALUATE-FOR approach. image classification tasks EVALUATE-FOR baseline methods. image classification tasks EVALUATE-FOR approach. approach COMPARE baseline methods. baseline methods COMPARE approach. Metric is PAC - Bayes generalization bound. OtherScientificTerm is noisy labels. ,"This paper proposes a new algorithm for transfer learning based on fine-tuning. The main idea is to fine-tune a pre-trained model on labeled data for a new task using labeled data from the original dataset. The authors provide a PAC-Bayes generalization bound for this new approach, which is based on the interpolation between regularization and self-labeling methods, layer-wise regularization, self label-correction, and label-reweighting. They also provide a theoretical analysis of the noise stability of the fine-fine-tuned model, which shows that the noise improves the robustness against overfitting. The proposed approach is evaluated on a few image classification tasks and a few-shot classification task, and compared with several baseline methods on both image and text data sets.","This paper proposes a new algorithm for transfer learning based on fine-tuning. The main idea is to use labeled data to train a pre-trained model for a task, and then fine-tune the model on the labeled data for the task. The authors show that fine-fine tuning can reduce overfitting and improve robustness to noisy labels. The generalization properties of the proposed algorithm are based on the PAC-Bayes generalization bound. The interpolation between regularization and self-labeling methods (layer-wise regularization, self label-correction, and label-reweighting) are used to regularize self-labelling and layer-wise non-linearity is used to improve the noise stability of the fine -tuned model. The proposed approach is evaluated on several image classification tasks and a few-shot classification task and compared to baseline methods on both image and text data sets."
10226,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"value - at - risk ( VaR ) HYPONYM-OF tail - risk measures. finance and insurance industries HYPONYM-OF tail - risk measures. weighted sum of CVaR CONJUNCTION mean. mean CONJUNCTION weighted sum of CVaR. VaR CONJUNCTION weighted sum of CVaR. weighted sum of CVaR CONJUNCTION VaR. CVaR CONJUNCTION VaR. VaR CONJUNCTION CVaR. VaR CONJUNCTION mean. mean CONJUNCTION VaR. latter USED-FOR risk - return trade - off. risk - return trade - off FEATURE-OF finance. optimal δcorrect algorithm USED-FOR arms. heavy - tailed distributions FEATURE-OF arms. non - convex optimization problem USED-FOR algorithm. probability measures FEATURE-OF non - convex optimization problem. OtherScientificTerm are probability distributions, and arm. ","This paper studies the problem of value-at-risk (VaR) in the tail-risk measures in finance and insurance industries. The authors propose a non-convex optimization problem with probability measures, where the probability distributions of the arms are heavy-tailed distributions. They show that the optimal δcorrect algorithm for the two arms is the same for both the weighted sum of CVaR and the mean of VaR, and that the latter can be used for risk-return trade-off in finance. ","This paper proposes value-at-risk (VaR) and value-based risk (VAR) measures for tail-risk measures in the finance and insurance industries. The authors propose an optimal δcorrect algorithm for both arms with heavy-tailed distributions. The algorithm is based on a non-convex optimization problem with probability measures. The probability distributions of the two arms are computed using the latter for risk-return trade-off in finance, and the weights of the arms are weighted sum of CVaR, VaR, and mean. "
10251,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"Transformers USED-FOR computer vision tasks. Transformers USED-FOR modeling long - range dependency. self - attention mechanism USED-FOR modeling long - range dependency. intrinsic inductive bias ( IB ) USED-FOR modeling local visual structures. vision transformers USED-FOR image. 1D sequence of visual tokens USED-FOR image. 1D sequence of visual tokens USED-FOR vision transformers. training schedules USED-FOR IB. large - scale training data CONJUNCTION training schedules. training schedules CONJUNCTION large - scale training data. training schedules USED-FOR they. large - scale training data USED-FOR they. intrinsic IB USED-FOR Vision Transformer. ViTAE HYPONYM-OF convolutions. convolutions USED-FOR intrinsic IB. dilation rates FEATURE-OF convolutions. spatial pyramid reduction modules PART-OF ViTAE. it USED-FOR robust feature representation. it USED-FOR intrinsic scale invariance IB. convolution block PART-OF multi - head selfattention module. ViTAE PART-OF transformer layer. convolution block PART-OF transformer layer. convolution block PART-OF ViTAE. local features CONJUNCTION global dependencies. global dependencies CONJUNCTION local features. it USED-FOR global dependencies. it USED-FOR local features. intrinsic locality IB FEATURE-OF it. ImageNet EVALUATE-FOR ViTAE. downstream tasks EVALUATE-FOR ViTAE. ViTAE COMPARE concurrent works. concurrent works COMPARE ViTAE. ViTAE COMPARE baseline transformer. baseline transformer COMPARE ViTAE. downstream tasks EVALUATE-FOR concurrent works. downstream tasks EVALUATE-FOR baseline transformer. ImageNet CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet. baseline transformer CONJUNCTION concurrent works. concurrent works CONJUNCTION baseline transformer. ImageNet EVALUATE-FOR baseline transformer. ImageNet EVALUATE-FOR concurrent works. OtherScientificTerm are local visual structures, scale variance, and rich multi - scale context. Method are feed - forward network, and pretrained models. ","This paper proposes a self-attention mechanism for modeling long-range dependency in vision transformers for computer vision tasks. The authors use intrinsic inductive bias (IB) for modeling local visual structures in the image. The intrinsic IB is defined as the difference between the dilation rates of convolutions (ViTAE) and convolutions with convolutions that have the same intrinsic IB. They show that they are invariant to large-scale training data and training schedules. They also show that ViTAE has a convolution block in the transformer layer and a multi-head selfattention module. The spatial pyramid reduction modules are also incorporated into the ViTAe. The paper also shows that it is able to capture local features and global dependencies, and that it can achieve intrinsic scale invariance IB. Finally, the paper shows that the proposed baseline transformer outperforms the baseline transformer on ImageNet and several downstream tasks. ","This paper proposes a self-attention mechanism for modeling long-range dependency in Transformers for computer vision tasks. The intrinsic inductive bias (IB) is used for modeling local visual structures. The authors show that convolutions with intrinsic IB (ViTAE) are more robust to dilation rates than convolutions without intrinsic IB. They show that ViTAE is a transformer layer with a convolution block, a multi-head selfattention module, and spatial pyramid reduction modules. They also show that the IB is invariant to large-scale training data and training schedules, and they show that they are invariant even when training schedules are very different from the IB.  The authors propose a feed-forward network, and show that it is able to learn intrinsic scale invariance IB, and it can learn local features as well as global dependencies. The paper also shows that the intrinsic locality IB is more robust than the intrinsic IB for Vision Transformer.   "
10276,SP:5e3572a386f890c5864437985cf63b13844f338f,fine - tuning USED-FOR NLP fields. pre - trained language models USED-FOR NLP fields. pre - trained language models USED-FOR fine - tuning. adversarial examples USED-FOR it. synonyms USED-FOR word substitution attacks. word substitution attacks HYPONYM-OF adversarial examples. adversarial training HYPONYM-OF defense technique. adversarial training USED-FOR fine - tuning scenario. catastrophic forgetting FEATURE-OF it. pre - trained model USED-FOR generic and robust linguistic features. Robust Informative Fine - Tuning ( RIFT ) HYPONYM-OF adversarial fine - tuning method. objective model USED-FOR features. RIFT USED-FOR objective model. pre - trained model USED-FOR features. pre - trained weights USED-FOR one. sentiment analysis CONJUNCTION natural language inference. natural language inference CONJUNCTION sentiment analysis. RIFT COMPARE state - of - the - arts. state - of - the - arts COMPARE RIFT. NLP tasks EVALUATE-FOR state - of - the - arts. NLP tasks EVALUATE-FOR RIFT. natural language inference HYPONYM-OF NLP tasks. sentiment analysis HYPONYM-OF NLP tasks. Method is BERT - based sentiment analysis model. Task is fine - tuning process. ,This paper proposes a defense technique called Robust Informative Fine-tuning (RIFT) that uses adversarial examples to fine-tune pre-trained language models for NLP fields. The authors show that it is possible to use adversarial training to improve the performance of the pre-training language models in the fine-tuned scenario. The main contribution of the paper is a BERT-based sentiment analysis model that can be used as a pre-trainable objective model to learn the features of the objective model.  The authors also show that RIFT improves the performance on a variety of NLP tasks such as sentiment analysis and natural language inference.,"This paper proposes Robust Informative Fine-tuning (RIFT), a defense technique based on adversarial training against word substitution attacks. The idea is to use pre-trained language models to fine-tun the NLP fields. The authors propose a BERT-based sentiment analysis model, which can be used as a pre-training objective for the objective model. They show that RIFT is robust to catastrophic forgetting, and that it is more robust to adversarial examples than it is to synonyms. They also show that one can use one of the pre-train weights to improve the robustness of the final model. Experiments on several NLP tasks, including sentiment analysis and natural language inference, show that the proposed RIFT outperforms state-of-the-arts."
10301,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"acceleration method USED-FOR fixed - point iterations. Anderson mixing ( AM ) HYPONYM-OF acceleration method. convergence theory FEATURE-OF AM. Stochastic Anderson Mixing ( SAM ) scheme USED-FOR nonconvex stochastic optimization problems. damped projection and adaptive regularization USED-FOR AM. damped projection and adaptive regularization USED-FOR Stochastic Anderson Mixing ( SAM ) scheme. almost sure convergence CONJUNCTION worst - case iteration complexity. worst - case iteration complexity CONJUNCTION almost sure convergence. convergence theory FEATURE-OF SAM. almost sure convergence FEATURE-OF stationary points. almost sure convergence PART-OF convergence theory. worst - case iteration complexity PART-OF convergence theory. variance reduction technique PART-OF SAM. preconditioned mixing strategy USED-FOR SAM. faster convergence CONJUNCTION generalization ability. generalization ability CONJUNCTION faster convergence. preconditioned mixing strategy USED-FOR faster convergence. generalization ability EVALUATE-FOR preconditioned mixing strategy. DenseNet CONJUNCTION LSTM. LSTM CONJUNCTION DenseNet. ResNeXt CONJUNCTION DenseNet. DenseNet CONJUNCTION ResNeXt. vanilla CNN CONJUNCTION ResNets. ResNets CONJUNCTION vanilla CNN. WideResNet CONJUNCTION ResNeXt. ResNeXt CONJUNCTION WideResNet. ResNets CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNets. SAM method USED-FOR neural networks. LSTM HYPONYM-OF neural networks. DenseNet HYPONYM-OF neural networks. vanilla CNN HYPONYM-OF neural networks. ResNeXt HYPONYM-OF neural networks. WideResNet HYPONYM-OF neural networks. ResNets HYPONYM-OF neural networks. image classification and language model EVALUATE-FOR method. Task are scientific computing, and machine learning problems. Metric is complexity bound. ","This paper proposes a new acceleration method for fixed-point iterations in scientific computing. The authors propose Stochastic Anderson Mixing (SAM) scheme for nonconvex stochastic optimization problems. SAM is based on damped projection and adaptive regularization. The variance reduction technique in SAM is used to improve the convergence theory of AM. The proposed SAM method is evaluated on image classification and language model and neural networks such as DenseNet, LSTM, ResNeXt, WideResNet, vanilla CNN, ResNets, and DenseNets.  The authors show that the preconditioned mixing strategy improves the faster convergence and generalization ability of SAM. ","This paper proposes a new acceleration method for fixed-point iterations in scientific computing, and machine learning problems. Anderson mixing (AM) is an extension of the Stochastic Anderson Mixing (SAM) scheme for nonconvex stochastic optimization problems. AM is based on damped projection and adaptive regularization. The convergence theory of SAM is a convergence theory with almost sure convergence and worst-case iteration complexity. The variance reduction technique in SAM is used to reduce the complexity bound. SAM method is applied to neural networks such as DenseNet, LSTM, ResNets, WideResNet, and vanilla CNN. The preconditioned mixing strategy is used for faster convergence and generalization ability. The proposed method is evaluated on image classification and language model."
10326,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"posterior distribution USED-FOR linear inverse problem. SNIPS HYPONYM-OF stochastic algorithm. Langevin dynamics CONJUNCTION Newton ’s method. Newton ’s method CONJUNCTION Langevin dynamics. Newton ’s method USED-FOR solution. Langevin dynamics USED-FOR solution. singular value decomposition ( SVD ) USED-FOR degradation operator. singular value decomposition ( SVD ) PART-OF posterior score function. paradigm USED-FOR image deblurring. image deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION image deblurring. super - resolution CONJUNCTION compressive sensing. compressive sensing CONJUNCTION super - resolution. paradigm USED-FOR super - resolution. paradigm USED-FOR compressive sensing. OtherScientificTerm are additive white Gaussian noise, and noisy observation. Generic are approach, and algorithm. Method is iterative algorithm. Task is inverse problem. ","This paper studies the inverse problem with additive white Gaussian noise. The authors propose a stochastic algorithm called SNIPS, which is based on the Posterior distribution of the linear inverse problem. The proposed approach is an iterative algorithm where the objective is to find a solution with Langevin dynamics and Newton’s method. The posterior score function is defined as a singular value decomposition (SVD) of the degradation operator. The algorithm is evaluated on image deblurring, super-resolution, and compressive sensing.","This paper proposes a stochastic algorithm called SNIPS, which is based on the notion of additive white Gaussian noise. The idea is to use the posterior distribution to solve the linear inverse problem with respect to the inverse problem. The authors propose an iterative algorithm to solve this inverse problem using Langevin dynamics and Newton’s method. The proposed approach uses singular value decomposition (SVD) to decompose the degradation operator into the posterior score function, and the authors show that the proposed algorithm converges to the optimal solution. The paper also proposes a new paradigm for image deblurring, super-resolution, and compressive sensing."
10351,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"Instagram HYPONYM-OF social media. techniques USED-FOR illicit drug trades. meta - learning technique USED-FOR MetaHG. multimodal content CONJUNCTION relational structured information. relational structured information CONJUNCTION multimodal content. holistic framework USED-FOR illicit drug traffickers. MetaHG USED-FOR illicit drug trafficker detection. relational structured information USED-FOR illicit drug trafficker detection. MetaHG USED-FOR multimodal content. MetaHG USED-FOR relational structured information. MetaHG USED-FOR illicit drug traffickers. social media FEATURE-OF relational structured information. MetaHG HYPONYM-OF holistic framework. social media USED-FOR illicit drug traffickers. Instagram HYPONYM-OF social media. heterogeneous graph ( HG ) USED-FOR MetaHG. relation - based graph convolutional neural network USED-FOR node ( i.e., user ) representations. graph structure refinement USED-FOR sparse connection among entities. graph structure refinement USED-FOR node representation learning. sparse connection among entities PART-OF HG. graph structure refinement USED-FOR HG. HG USED-FOR relation - based graph convolutional neural network. HG USED-FOR node ( i.e., user ) representations. meta - learning algorithm USED-FOR model optimization. self - supervised module CONJUNCTION knowledge distillation module. knowledge distillation module CONJUNCTION self - supervised module. unlabeled data USED-FOR model. knowledge distillation module USED-FOR model. knowledge distillation module USED-FOR unlabeled data. self - supervised module USED-FOR model. self - supervised module USED-FOR unlabeled data. MetaHG COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MetaHG. real - world data EVALUATE-FOR MetaHG. real - world data EVALUATE-FOR state - of - the - art methods. Instagram FEATURE-OF real - world data. Task are crime of drug trafficking, online drug trafficking, and model training. Material is post content. ","This paper proposes MetaHG, a meta-learning technique for drug detection based on a heterogeneous graph (HG). The authors propose a holistic framework for the detection of illicit drug traffickers based on relational structured information from social media (e.g. Instagram). The proposed method is based on the idea of graph structure refinement to improve the sparse connection among entities in a graph convolutional neural network. The authors also propose a self-supervised module for unlabeled data and a knowledge distillation module for the model. The proposed model is evaluated on real-world data from Instagram as well as online drug trafficking. The results show that the proposed method performs better than state-of-the-art methods in terms of detection performance.","This paper proposes MetaHG, a meta-learning technique for detecting illicit drug trades. The authors propose a heterogeneous graph (HG) for model optimization. The HG is a relation-based graph convolutional neural network with sparse connection among entities. The relational structured information in the relational structure information is obtained from social media such as Instagram and social media for illicit drug trafficker detection. The model training is performed using a self-supervised module and a knowledge distillation module for unlabeled data. The paper shows the effectiveness of metaHG on real-world data as well as on real world data for the crime of drug trafficking, online drug trafficking and post content. The experiments are conducted on a variety of datasets and show the performance improvement over state-of-the-art methods."
10376,SP:242da1384f48260d58a0e7949438611c05079197,"ReLU activations CONJUNCTION architecture. architecture CONJUNCTION ReLU activations. architecture USED-FOR neural network. ReLU activations USED-FOR neural network. neural network USED-FOR class of functions. polyhedral theory CONJUNCTION tropical geometry. tropical geometry CONJUNCTION polyhedral theory. mixed - integer optimization CONJUNCTION polyhedral theory. polyhedral theory CONJUNCTION mixed - integer optimization. hidden layer USED-FOR learning tasks. techniques USED-FOR mathematical counterbalance. mathematical counterbalance USED-FOR universal approximation theorems. polyhedral theory USED-FOR techniques. mixed - integer optimization USED-FOR techniques. upper bounds FEATURE-OF neural networks. neural networks USED-FOR neural hypothesis classes. OtherScientificTerm are layers, and neural network literature. Task is algorithmic and statistical aspects. ","This paper studies the problem of algorithmic and statistical aspects of neural networks. The authors propose two techniques: (1) mixed-integer optimization and (2) polyhedral theory. Both techniques are based on mathematical counterbalance, which allows for universal approximation theorems. The main contribution of the paper is to show that these two techniques can be combined with ReLU activations and the architecture of a neural network to learn a class of functions. Theoretically, the authors show that the upper bounds of these two neural networks can be used to derive neural hypothesis classes. ",This paper proposes a novel architecture for learning a neural network with ReLU activations and architecture. The main idea is to learn a class of functions that can be approximated by a single neural network. The authors propose two techniques: mixed-integer optimization and polyhedral theory. The techniques are based on mathematical counterbalance and universal approximation theorems. They show that the upper bounds of the neural networks for learning neural hypothesis classes are close to those of neural network literature. They also propose a hidden layer for learning tasks with algorithmic and statistical aspects.
10401,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"worst - case training principle USED-FOR maximal adversarial loss. min - max optimization USED-FOR AT. min - max optimization PART-OF adversarial context. framework USED-FOR adversarial attacks. min - max optimization USED-FOR adversarial attacks. framework USED-FOR min - max optimization. probability simplex FEATURE-OF domain weights. unified framework USED-FOR attack generation problems. unified framework USED-FOR crafting attacks. crafting attacks USED-FOR data transformations. crafting attacks HYPONYM-OF attack generation problems. attacking model ensembles HYPONYM-OF attack generation problems. approach COMPARE heuristic strategies. heuristic strategies COMPARE approach. robustness EVALUATE-FOR defense methods. heuristic strategies COMPARE defense methods. defense methods COMPARE heuristic strategies. approach COMPARE defense methods. defense methods COMPARE approach. robustness EVALUATE-FOR heuristic strategies. robustness EVALUATE-FOR approach. self - adjusted domain weights USED-FOR difficulty level of attack. min - max framework USED-FOR self - adjusted domain weights. Method is adversarial training ( AT ). Task are adversarial robustness, and min - max problem. OtherScientificTerm are risk sources, and universal perturbation. Metric is worst - case attack loss. ","This paper studies adversarial training (AT) in the worst-case training principle, where the goal is to achieve maximal adversarial loss. The authors propose a framework for adversarial attacks based on min-max optimization in the adversarial context. The framework is based on a unified framework for attack generation problems such as crafting attacks, attacking model ensembles, and data transformations. The proposed approach is shown to achieve better robustness than heuristic strategies and other defense methods. ",This paper proposes a new adversarial training (AT) framework for improving adversarial robustness. AT is based on the worst-case training principle for maximal adversarial loss. The authors propose a new framework for adversarial attacks based on min-max optimization in the adversarial context. The main idea is to use a unified framework for attacking model ensembles and crafting attacks for data transformations.  The authors show that this approach is more robust to adversarial perturbations than heuristic strategies and other defense methods in terms of robustness to the difficulty level of attack. They also show that the domain weights of domain weights with probability simplex are more robust than the ones with universal perturbation. 
10426,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. model USED-FOR sparse PCA. model USED-FOR tensor PCA. Wigner form FEATURE-OF sparse PCA. polynomial - time algorithm CONJUNCTION exponential - time exhaustive search algorithm. exponential - time exhaustive search algorithm CONJUNCTION polynomial - time algorithm. polynomial - time algorithm USED-FOR algorithms. exponential - time exhaustive search algorithm USED-FOR algorithms. algorithms USED-FOR sparse vector. signal - tonoise ratio λ FEATURE-OF sparse vector. algorithms USED-FOR sparse vectors. algorithms COMPARE algorithms. algorithms COMPARE algorithms. λ FEATURE-OF sparse vectors. algorithms USED-FOR sparse PCA. signal - to - noise ratio CONJUNCTION running time. running time CONJUNCTION signal - to - noise ratio. sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. lower bound USED-FOR lower bounds. lower bounds USED-FOR sparse PCA. lower bound USED-FOR sparse PCA. lower bounds USED-FOR tensor PCA. Task is sparse tensor principal component analysis. OtherScientificTerm are i.i.d. Gaussian entries, k - sparse unit vector, k - sparse signals, and sparsity k. Generic is matrix settings. Metric is low - degree likelihood ratio. ","This paper studies the problem of sparse tensor principal component analysis. The authors propose a model for sparse PCA and tensor PCA in the Wigner form, where i.i.d. Gaussian entries are represented by k-sparse unit vector, and the k-sparse unit vector is represented by a k-strongly-sparsity k. The sparse vector is defined as the signal-tonoise ratio λ of the sparse vector in the low-degree likelihood ratio.  The authors show that existing algorithms for computing the sparse vectors of sparse vectors with a polynomial-time algorithm and an exponential-time exhaustive search algorithm can be used to find the optimal algorithms for these algorithms. The lower bounds on the number of iterations of the algorithms are also provided for the case where the number k is small. ","The paper proposes a new sparse tensor principal component analysis, which is based on sparse PCA and tensor PCA in the Wigner form. The authors propose two algorithms for computing the sparse vector in the signal-to-noise ratio λ of the k-sparse unit vector. The polynomial-time algorithm and the exponential-time exhaustive search algorithm are used to compute the algorithms for the sparse vectors. The lower bound for the lower bounds for the tensor and the lower bound of the lower-degree likelihood ratio is also provided. "
10451,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"Multilayer - perceptrons ( MLP ) USED-FOR learning functions of high - frequencies. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR MLP networks. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR them. feedback loop USED-FOR neural optimization process. feedback loop USED-FOR progressive exposure of frequencies. regression of low dimensional signals CONJUNCTION images. images CONJUNCTION regression of low dimensional signals. representation learning of occupancy networks CONJUNCTION geometric task of mesh transfer. geometric task of mesh transfer CONJUNCTION representation learning of occupancy networks. regression of low dimensional signals CONJUNCTION representation learning of occupancy networks. representation learning of occupancy networks CONJUNCTION regression of low dimensional signals. SAPE USED-FOR applications. geometric task of mesh transfer HYPONYM-OF applications. regression of low dimensional signals HYPONYM-OF applications. images HYPONYM-OF applications. representation learning of occupancy networks HYPONYM-OF applications. OtherScientificTerm are wide frequency bands, and 3D shapes. Metric is training stability. Method is domain specific preprocessing. ","This paper studies the problem of learning functions of high-frequencies in multilayer-pertrons (MLP) with wide frequency bands. The authors propose a spatially adaptive progressive encoding (SAPE) scheme to train MLP networks with them by using a feedback loop in the neural optimization process. SAPE can be applied to a variety of applications such as regression of low dimensional signals, images, representation learning of occupancy networks, and geometric task of mesh transfer. The paper shows that SAPE improves the training stability in terms of domain specific preprocessing. ","This paper proposes a new framework for learning functions of high-frequencies in multilayer-pertrons (MLP). The authors propose a spatially adaptive progressive encoding (SAPE) scheme for MLP networks, which allows them to learn them with a feedback loop in the neural optimization process. The authors show that the proposed SAPE can be applied to applications such as regression of low dimensional signals and images, representation learning of occupancy networks, and the geometric task of mesh transfer. The paper also shows that the training stability of the SAPE is not affected by domain specific preprocessing. "
10476,SP:b03063fa82d76db341076e5f282176f4c007a202,"probability simplex constraints FEATURE-OF constrained saddle - point optimization problem. constrained saddle - point optimization problem USED-FOR equilibrium of competitive games. methods USED-FOR constrained settings. unconstrained setting FEATURE-OF extragradient methods. entropy regularization USED-FOR single - agent reinforcement learning and game theory. entropy regularization FEATURE-OF zero - sum two - player matrix games. algorithms USED-FOR approximate Nash equilibrium. approximate Nash equilibrium FEATURE-OF unregularized matrix game. knob of entropy regularization USED-FOR algorithms. policy extragradient algorithms USED-FOR entropy - regularized zero - sum Markov games. methods USED-FOR policy extragradient algorithms. linear rate FEATURE-OF policy extragradient algorithms. entropy regularization USED-FOR accelerating convergence. logarithm factors FEATURE-OF state and action spaces. OtherScientificTerm are multiplicative updates, objective function, sublinear rate, and Nash equilibrium. Method is symmetric and multiplicative updates. Metric is convergence rates. ","This paper studies the problem of solving a constrained saddle-point optimization problem with probability simplex constraints in the context of the equilibrium of competitive games. The authors consider the case of zero-sum two-player matrix games with entropy regularization, where the objective function is a function of the number of multiplicative updates. In the unconstrained setting, extragradient methods in the constrained settings have been shown to converge in a linear rate in practice. The main contribution of this paper is to show that these algorithms can converge to an approximate Nash equilibrium in an unregularized matrix game with approximate logarithm factors in the state and action spaces.  The authors also show that such algorithms can also converge to a Nash equilibrium with a knob of entropy regularisation, which is an important result in single-agent reinforcement learning and game theory.  ","This paper studies the constrained saddle-point optimization problem for equilibrium of competitive games with probability simplex constraints. The authors propose two methods for solving the constrained settings, one in the unconstrained setting and the other in the constrained setting with symmetric and multiplicative updates. The main contribution of the paper is to study the convergence rates of these two methods in the context of single-agent reinforcement learning and game theory. The algorithms are based on the knob of entropy regularization in zero-sum two-player matrix games, where the objective function is the sublinear rate of the Nash equilibrium. The proposed algorithms are shown to converge to an approximate Nash equilibrium in an unregularized matrix game with logarithm factors in the state and action spaces. The convergence rates are also shown to be faster than the linear rate of policy extragradient algorithms for entropy-regularized zero-sigma Markov games."
10501,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"remote cooperation CONJUNCTION online education. online education CONJUNCTION remote cooperation. screen sharing CONJUNCTION remote cooperation. remote cooperation CONJUNCTION screen sharing. HR display USED-FOR super - resolution ( SR ). image SR methods USED-FOR natural images. image SR methods USED-FOR SCIs. pixel values USED-FOR continuous SR. implicit transformer USED-FOR image features. image features USED-FOR pixel values. LR and HR SCI pairs USED-FOR SCI1 K and SCI1K - compression datasets. continuous and discrete SR methods USED-FOR compressed and uncompressed SCIs. ITSRN COMPARE continuous and discrete SR methods. continuous and discrete SR methods COMPARE ITSRN. ITSRN USED-FOR compressed and uncompressed SCIs. Material is screen contents. OtherScientificTerm are limited terminal bandwidth, high - resolution ( HR ) screen contents, and image characteristics. Task is SCI browsing. Method are SCISR, and implicit position encoding scheme. ","This paper proposes SCISR, an implicit position encoding scheme for SCI browsing. SCI1K-compression datasets are obtained using LR and HR SCI pairs. The authors show that image SR methods can be used to compress natural images into SCIs with limited terminal bandwidth. They also show that continuous SR with pixel values extracted from the implicit transformer can compress the image features. Finally, they show that ITSRN can compress compressed and uncompressed SCIs. ","The paper proposes SCISR, an extension of SCI browsing to the context of super-resolution (SR) on the HR display. The main idea is to use a limited terminal bandwidth to store high-res (HR) screen contents, and then use image SR methods to capture natural images. The paper proposes an implicit transformer to encode image features into pixel values, which are then used to encode the pixel values. The proposed implicit position encoding scheme is evaluated on the SCI1K and SCI2K-compression datasets with LR and HR SCI pairs. The results show that the proposed ITSRN outperforms both continuous and discrete SR methods for compressed and uncompressed SCIs. "
10526,SP:3751625929b707ced417c3eb10064e4917866048,"probabilistic models USED-FOR causality. sumproduct networks ( SPNs ) USED-FOR learning interventional distributions. gate functions USED-FOR sumproduct networks ( SPNs ). neural networks HYPONYM-OF gate functions. gate function USED-FOR SPN. structural causal model USED-FOR interventional SPNs. personal health FEATURE-OF structural causal model. generative and causal modelling USED-FOR methods. Generic is so. Task is intractability of inference. Method is causal models. OtherScientificTerm are interventional distributions, arbitrarily intervened causal graph, and Pearl ’s do - operator. ","This paper studies the intractability of inference with probabilistic models in the context of causality. The authors consider the problem of learning interventional distributions with sumproduct networks (SPNs) with gate functions (e.g., neural networks). The authors show that the gate functions of a SPN with a gate function in the form of a neural networks are intractable in the presence of arbitrary intervened causal graph. To address this problem, the authors propose to use a structural causal model to learn interventional SPNs with personal health. The proposed methods are based on both generative and causal modelling. ","This paper studies the intractability of inference with probabilistic models in the context of causality. The authors propose to use sumproduct networks (SPNs) for learning interventional distributions. The main idea is to use gate functions of neural networks, i.e., neural networks with gate functions that are independent of the input distribution, to model the interventional distribution. This is done by using a structural causal model, which is based on the notion of personal health. The paper shows that the proposed methods outperform existing generative and causal modelling in terms of generalization. "
10551,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"Factor analysis methods USED-FOR low dimensional, ideally interpretable representations. Factor analysis methods USED-FOR neuroimaging. Factor analysis methods USED-FOR high dimensional imaging data. high dimensional imaging data USED-FOR low dimensional, ideally interpretable representations. deep Markov factor analysis ( DMFA ) HYPONYM-OF generative model. Markov property USED-FOR low dimensional temporal embeddings. Markov property CONJUNCTION spatial inductive assumptions. spatial inductive assumptions CONJUNCTION Markov property. temporal dynamics FEATURE-OF functional magnetic resonance imaging ( fMRI ) data. Markov property USED-FOR generative model. discrete latent USED-FOR DMFA. DMFA USED-FOR fMRI data. low dimensional temporal embedding USED-FOR DMFA. DMFA USED-FOR interpretable clusters. DMFA USED-FOR nonlinear temporal dependencies. synthetic and real fMRI data EVALUATE-FOR DMFA. nonlinear temporal dependencies FEATURE-OF high dimensional imaging data. Generic is methods. OtherScientificTerm are nonlinear and complex temporal dynamics of neural processes, high spatial dimensionality, and subject and cognitive state variability. Material is imaging data. Method is neural networks. Task are fMRI - driven neuroscientific hypotheses, and capturing nonlinear temporal dependencies. ","This paper proposes a new generative model based on deep Markov factor analysis (DMFA) based on the Markov property for low dimensional, ideally interpretable representations from high dimensional imaging data. Factor analysis methods for neuroimaging have been widely used in recent years to learn low dimensional representations from a wide range of imaging data, but these methods have been limited in their ability to capture nonlinear and complex temporal dynamics of neural processes. In this paper, the authors focus on the problem of capturing nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data with temporal dynamics. The authors propose to use DMFA to capture fMRI data with low dimensional temporal embedding using a discrete latent, and then use the DMFA in order to learn interpretable clusters from the low dimensional spatial embedding. They show that DMFA is able to capture the nonlinear spatial dependencies in both synthetic and realfMRI data. ","This paper proposes a generative model based on deep Markov factor analysis (DMFA) that is able to capture the nonlinear and complex temporal dynamics of neural processes. The authors show that DMFA can capture nonlinear temporal dependencies in fMRI-driven neuroscientific hypotheses. They also show that the Markov property of DMFA is sufficient to capture low dimensional temporal embeddings in high dimensional imaging data, and that the high spatial dimensionality can capture the subject and cognitive state variability in imaging data. They further show that using DMFA to capture interpretable clusters in DMFA improves the performance on both synthetic and real fMRI data."
10576,SP:855dcaa42868a29a14619d63221169495ed5dd54,"spheres CONJUNCTION tori. tori CONJUNCTION spheres. generative models USED-FOR complex geometries. tori CONJUNCTION implicit surfaces. implicit surfaces CONJUNCTION tori. manifolds USED-FOR complex geometries. implicit surfaces HYPONYM-OF manifolds. spheres HYPONYM-OF complex geometries. spheres HYPONYM-OF manifolds. tori HYPONYM-OF complex geometries. tori HYPONYM-OF manifolds. Moser Flow ( MF ) HYPONYM-OF generative models. continuous normalizing flows ( CNF ) FEATURE-OF generative models. MF USED-FOR CNF. source ( prior ) density CONJUNCTION divergence. divergence CONJUNCTION source ( prior ) density. CNF methods COMPARE model ( learned ) density. model ( learned ) density COMPARE CNF methods. divergence PART-OF neural network ( NN ). source ( prior ) density USED-FOR model ( learned ) density. divergence HYPONYM-OF local, linear differential operator. CNFs COMPARE MF. MF COMPARE CNFs. divergence USED-FOR model density. NN USED-FOR model density. MF USED-FOR universal density approximator. flow models USED-FOR sampling from general curved surfaces. training complexity EVALUATE-FOR CNFs. sample quality EVALUATE-FOR CNFs. sample quality CONJUNCTION training complexity. training complexity CONJUNCTION sample quality. synthetic geometries CONJUNCTION real - world benchmarks. real - world benchmarks CONJUNCTION synthetic geometries. density estimation CONJUNCTION sample quality. sample quality CONJUNCTION density estimation. flow models COMPARE CNFs. CNFs COMPARE flow models. density estimation EVALUATE-FOR CNFs. earth and climate sciences FEATURE-OF synthetic geometries. earth and climate sciences FEATURE-OF real - world benchmarks. earth and climate sciences EVALUATE-FOR CNFs. real - world benchmarks EVALUATE-FOR CNFs. synthetic geometries EVALUATE-FOR CNFs. Method are Euclidean ) generative models, and ODE solver. OtherScientificTerm are change - of -","This paper studies the problem of learning complex geometries using generative models. The authors propose a continuous normalizing flows (CNF) approach to this problem. The CNF is based on the Moser Flow (MF) framework. The main idea is to use the divergence in the neural network (NN) as a local, linear differential operator to estimate the model density using the divergence of the source (prior) density and the divergence between the source and the model (learned) density.  The authors show that the CNF outperforms existing CNF methods in terms of sample quality, training complexity, and sample quality. They also show that MF is a universal density approximator. ","This paper proposes a new generative model for complex geometries. The authors propose to use continuous normalizing flows (CNF) as a generative models, which is a variant of the Moser Flow (MF) that is used in many existing (e.g., Euclidean) generative methods. The main difference between CNF and MF is that CNF uses a local, linear differential operator, while MF uses a change-of-the-divergence. The divergence is computed by a neural network (NN) and the source (prior) density of the model (learned) density is computed using the divergence of the neural network. This divergence is then used to estimate the model density, which can be used as a universal density approximator. The ODE solver is used to compute the divergence. CNFs outperform MF in terms of training complexity, sample quality, and density estimation. The paper also shows that the CNF outperforms flow models for sampling from general curved surfaces. "
10601,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"framework USED-FOR unsupervised representation learning. Independent component analysis USED-FOR unsupervised representation learning. observed variables PART-OF generative process. independent causal mechanisms USED-FOR causality. assumptions USED-FOR independent causal mechanisms. approach USED-FOR nonidentifiability issues. nonidentifiability issues FEATURE-OF nonlinear blind source separation. OtherScientificTerm are latent code, mixing, statistical independence, Identifiability, and mixing process. Generic is model. Method is independent mechanism analysis. ","This paper proposes a framework for unsupervised representation learning based on Independent component analysis. Independent causal mechanisms are observed variables in the generative process, and the model is trained to predict the latent code. The authors show that independent causal mechanisms can be used to improve the causality of the model. They also show that the mixing process can improve the statistical independence. Finally, the authors propose an approach for nonidentifiability issues of nonlinear blind source separation. ","This paper proposes a framework for unsupervised representation learning based on Independent component analysis. The model is based on the idea of independent mechanism analysis, where the observed variables in the generative process are independent of the latent code. The authors show that independent causal mechanisms can be used to measure causality between observed variables and latent code, and that the mixing process is independent of statistical independence. Identifiability is measured by measuring the statistical independence between the observed and latent variables. The proposed approach is evaluated on nonidentifiability issues of nonlinear blind source separation. "
10626,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"Annealed Importance Sampling ( AIS ) HYPONYM-OF method. Hamiltonian MCMC USED-FOR Annealed Importance Sampling ( AIS ). non - differentiable transition kernels USED-FOR it. AIS - like procedure USED-FOR framework. Uncorrected Hamiltonian MCMC USED-FOR AIS - like procedure. Uncorrected Hamiltonian Annealing HYPONYM-OF AIS - like procedure. method COMPARE approaches. approaches COMPARE method. method USED-FOR tight and differentiable lower bounds. method COMPARE approaches. approaches COMPARE method. OtherScientificTerm are unnormalized target distribution, tight lower bound, and reparameterization gradients. ","This paper proposes a new method called Annealed Importance Sampling (AIS) based on the Hamiltonian MCMC. AIS-like procedure is a framework based on an unnormalized target distribution, and it is based on non-differentiable transition kernels. The proposed method is shown to achieve tight and differentiable lower bounds compared to other approaches. The tight lower bound is achieved by using reparameterization gradients, and the uncorrected Hamiltonian Annealing is used to improve the performance of the proposed method.","This paper proposes a method called Annealed Importance Sampling (AIS) which is based on Hamiltonian MCMC. AIS-like procedure is a framework based on the Uncorrected Hamiltonian Annealing, where it uses non-differentiable transition kernels to estimate the unnormalized target distribution. The proposed method is shown to achieve tight and differentiable lower bounds compared to other approaches. The tight lower bound is achieved by minimizing the reparameterization gradients."
10651,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"deep neural networks USED-FOR safetycritical applications. training algorithms USED-FOR neural network. training algorithms USED-FOR robustness. Certified robustness FEATURE-OF deep neural networks. robustness EVALUATE-FOR neural network. Lipschitz constant FEATURE-OF global bound. global bound USED-FOR training algorithms. non - convexity FEATURE-OF network. natural and certified accuracy EVALUATE-FOR tighter Lipschitz bound. activation functions CONJUNCTION weight matrices. weight matrices CONJUNCTION activation functions. induced norm FEATURE-OF weight matrix. global Lipschitz constant FEATURE-OF neural network. method USED-FOR plug - in module. plug - in module USED-FOR Lipschitz bound. method USED-FOR Lipschitz bound. Lipschitz bound FEATURE-OF certifiable training algorithms. upper threshold CONJUNCTION sparsity loss. sparsity loss CONJUNCTION upper threshold. ReLU CONJUNCTION MaxMin. MaxMin CONJUNCTION ReLU. sparsity loss USED-FOR network. network USED-FOR local Lipschitz bound. upper threshold USED-FOR activation functions. MaxMin HYPONYM-OF activation functions. ReLU HYPONYM-OF activation functions. method COMPARE methods. methods COMPARE method. network architectures USED-FOR method. TinyImageNet datasets EVALUATE-FOR methods. clean and certified accuracy EVALUATE-FOR methods. MNIST EVALUATE-FOR methods. TinyImageNet datasets EVALUATE-FOR method. clean and certified accuracy EVALUATE-FOR method. MNIST EVALUATE-FOR method. Generic are bound, and it. Metric is natural accuracy. OtherScientificTerm are local Lipschitz upper bound, and activation function. ","This paper studies the Lipschitz bound on the robustness of deep neural networks for safetycritical applications. The authors show that the global bound is tighter than the local bound for training algorithms for training the neural network for robustness in the presence of non-convexity in the training data. The paper also shows that the Lipchitz constant of a global bound for the global bounds of training algorithms is a function of the activation functions and weight matrices of the weight matrix, and that the induced norm of a weight matrix depends on the activation function.  The paper then proposes a plug-in module that can be used as a Lipshitz bound for a specific training algorithm. The proposed method is based on the upper threshold and the sparsity loss of the network. The method is evaluated on MNIST and TinyImageNet datasets and shows that it achieves better clean and certified accuracy than existing certifiable training algorithms.","This paper studies the robustness of deep neural networks for safetycritical applications. Certified robustness is an important metric for evaluating robustness to perturbations in the training algorithms for training algorithms to improve robustness. The paper proposes a new global bound on the global bound of the Lipschitz constant of a neural network with non-convexity. The bound is based on the local Lipsschitz upper bound, which is a local function of the activation function and weight matrices. The authors propose a plug-in module to improve the global bounds. The proposed method is evaluated on MNIST and TinyImageNet datasets. The method outperforms other certifiable training algorithms in terms of both natural and certified accuracy. "
10676,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"scalable methods USED-FOR conformal Bayesian predictive intervals. Bayesian posterior predictive distributions USED-FOR subjective beliefs. finite sample frequentist guarantees FEATURE-OF predictive confidence intervals. conformal inference USED-FOR predictive confidence intervals. conformal inference USED-FOR finite sample frequentist guarantees. add - one - in ’ importance sampling USED-FOR conformal Bayesian predictive intervals. re - weighted posterior samples of model parameters USED-FOR conformal Bayesian predictive intervals. refitting of models CONJUNCTION data - splitting. data - splitting CONJUNCTION refitting of models. approach COMPARE conformal methods. conformal methods COMPARE approach. refitting of models USED-FOR conformal methods. computational efficiency EVALUATE-FOR conformal methods. data - splitting USED-FOR conformal methods. hierarchical models HYPONYM-OF partially exchangeable settings. OtherScientificTerm are finite sample calibration guarantees, predictors, predictive intervals, and model fidelity. Method is Bayesian prediction. Metric is empirical coverage. Generic is examples. ",This paper studies the problem of Bayesian posterior predictive distributions for subjective beliefs in the presence of finite sample calibration guarantees. The authors propose two scalable methods for conformal Bayesian predictive intervals based on conformal inference with add-one-in’ importance sampling. The main idea is to use re-weighted posterior samples of model parameters to obtain conformalBayesian predictive interval with finite sample frequentist guarantees for the predictive confidence intervals. The proposed approach is shown to achieve better computational efficiency than conformal methods with refitting of models and data-splitting. ,"This paper proposes two new methods for learning conformal Bayesian predictive intervals for Bayesian posterior predictive distributions for subjective beliefs. The main idea is to use finite sample calibration guarantees for the predictors, and to use conformal inference for the predictive confidence intervals with finite sample frequentist guarantees. The authors propose to use add-one-in’ importance sampling for the conformals, and re-weighted posterior samples of model parameters. The proposed approach is evaluated on partially exchangeable settings (e.g., hierarchical models). The authors show that the proposed approach performs better than other conformal methods such as refitting of models and data-splitting, and achieves better computational efficiency. The paper also provides empirical coverage of the proposed model fidelity."
10701,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"denoisers USED-FOR general inverse problems. priors FEATURE-OF explicit likelihood functions. regularization - by - denoising ( RED ) HYPONYM-OF frameworks. RED USED-FOR imaging tasks. RED CONJUNCTION PnP. PnP CONJUNCTION RED. PnP USED-FOR imaging tasks. convolutional neural networks ( CNNs ) HYPONYM-OF denoisers. maximum a posteriori ( MAP ) CONJUNCTION minimum mean square error ( MMSE ) estimators. minimum mean square error ( MMSE ) estimators CONJUNCTION maximum a posteriori ( MAP ). convergence FEATURE-OF RED and PnP methods. CNN denoisers USED-FOR maximum a posteriori ( MAP ). Lipschitz constant FEATURE-OF CNN denoisers. denoisers PART-OF RED and PnP schemes. denoisers USED-FOR MAP and MMSE estimators interpretation. symmetric Jacobians FEATURE-OF denoisers. backtracking step size USED-FOR RED and PnP schemes. backtracking step size USED-FOR denoisers. denoisers USED-FOR inversion method. method COMPARE RED and PnP methods. RED and PnP methods COMPARE method. imaging experiments EVALUATE-FOR RED and PnP methods. imaging experiments EVALUATE-FOR method. Method are denoising algorithms, MAP or MMSE estimators, inverse algorithms, and image denoisers. Generic is they. OtherScientificTerm are potentials, and objective function. ","This paper studies denoisers for general inverse problems. The authors propose two frameworks, regularization-by-denoising (RED) and PnP, which are denoising algorithms that are based on MAP or MMSE estimators. The main contribution of the paper is to show that they can be used to approximate the priors of explicit likelihood functions with respect to the potentials. The paper also shows that the convergence of the proposed RED, PnPs, and RED for imaging tasks can be improved by backtracking step size. The method is evaluated on a variety of imaging experiments and shows that it can achieve better performance than the state-of-the-art inversion method. ","This paper proposes two new frameworks, regularization-by-denoising (RED) and PnP, for solving general inverse problems. The main idea is to use denoising algorithms to approximate the priors of explicit likelihood functions. The authors propose two denoisers, convolutional neural networks (CNNs) and denoiser-based inverse algorithms, and show that they converge to the MAP or MMSE estimators. They also show that the convergence of RED and PpP methods converges to the maximum a posteriori (MAP) and the minimum mean square error (MMSE) estimators, and that the denoising of MAP and MMSE can be interpreted as a function of the number of potentials. The paper also proposes an inversion method based on the backtracking step size. The method is evaluated on a variety of imaging experiments and compared to RED and other popular denoisering schemes, and outperforms the MAP and MES estimators interpretation. "
10726,SP:da92e936f88b3842ca82c2914413b129ca35890f,rhythmic features FEATURE-OF activities. rhythmic features FEATURE-OF musical soundtrack. system USED-FOR soundtrack. them USED-FOR rhythmic sounds. models USED-FOR rhythmic sounds. human movements USED-FOR RhythmicNet. skeleton keypoints USED-FOR RhythmicNet. rhythm CONJUNCTION melody. melody CONJUNCTION rhythm. natural process of music improvisation USED-FOR RhythmicNet. RhythmicNet USED-FOR rhythm. RhythmicNet USED-FOR style pattern. body keypoints USED-FOR rhythm. body keypoints USED-FOR RhythmicNet. body keypoints USED-FOR style pattern. U - net based model USED-FOR velocity. U - net based model USED-FOR it. transformerbased model USED-FOR it. inherit sound association FEATURE-OF body movements. body movements PART-OF large scale video datasets. dance HYPONYM-OF body movements. dance HYPONYM-OF inherit sound association. large scale video datasets EVALUATE-FOR RhythmicNet. Task is video. OtherScientificTerm is free body movements. Generic is method. ,"This paper proposes RhythmicNet, a system that learns the rhythmic features of activities in a musical soundtrack by using skeleton keypoints. The key idea is to use them to generate rhythmic sounds and then use them in the system to synthesize the soundtrack. The system is based on a natural process of music improvisation, and it uses human movements to learn the human movements. The style pattern is learned by using the body keypoints from the body movements and the rhythm from the skeleton keypoint. The method is evaluated on large scale video datasets with body movements in the inherit sound association between the rhythm and the melody, as well as dance. ","This paper proposes a method to learn a musical soundtrack with rhythmic features of activities. The system is based on the natural process of music improvisation. The authors propose to use skeleton keypoints in the RhythmicNet to learn the rhythm and melody of the musical soundtrack, and then use them to generate rhythmic sounds. The method is evaluated on large scale video datasets with a transformerbased model, and it is shown that it can learn a style pattern based on body keypoints and a U-net based model to predict velocity. The paper also shows that the method can learn the inherit sound association between body movements (e.g., dance) and free body movements."
10751,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"approaches USED-FOR offline reinforcement learning ( RL ). iterative actor - critic approach USED-FOR approaches. off - policy evaluation PART-OF iterative actor - critic approach. on - policy Q estimate USED-FOR behavior policy. on - policy Q estimate USED-FOR constrained / regularized policy improvement. onestep algorithm COMPARE iterative algorithms. iterative algorithms COMPARE onestep algorithm. D4RL benchmark EVALUATE-FOR iterative algorithms. D4RL benchmark EVALUATE-FOR onestep algorithm. one - step baseline COMPARE iterative algorithms. iterative algorithms COMPARE one - step baseline. OtherScientificTerm is hyperparameters. Method are iterative approaches, and one - step algorithm. Task is repeated optimization of policies. Generic is estimates. ",This paper proposes a new iterative actor-critic approach for offline reinforcement learning (RL) based on off-policy evaluation. The main idea is to use an on-policy Q estimate to estimate the behavior policy and then use a constrained/regularized policy improvement using an onestep algorithm. The authors show that the proposed iterative algorithms outperform the one-step baseline on the D4RL benchmark. ,"This paper presents a novel approach to offline reinforcement learning (RL) that combines off-policy evaluation with an iterative actor-critic approach. The key idea is to use an on-policy Q estimate for the behavior policy to guide the constrained/regularized policy improvement. The authors show that the onestep algorithm outperforms iterative algorithms on the D4RL benchmark. The main difference between iterative approaches and one-step algorithm is that the iterative approach is based on hyperparameters, while the authors propose to use estimates based on the learned policy. "
10776,SP:0346eba4f587acbe3492d039066f1737360fd870,"statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. tasks PART-OF machine learning. Low - rank and nonsmooth matrix optimization problems USED-FOR tasks. tasks PART-OF statistics. Low - rank and nonsmooth matrix optimization problems USED-FOR statistics. methods USED-FOR smooth low - rank optimization problems. convex relaxations USED-FOR problems. extragradient method USED-FOR optimal solution. maximum of smooth functions USED-FOR nonsmooth objective. initializations USED-FOR extragradient method. full - rank SVDs CONJUNCTION SVDs of rank. SVDs of rank CONJUNCTION full - rank SVDs. OtherScientificTerm are high - rank matrices, high - rank SVDs, natural generalized strict complementarity condition, low - rank SVDs, and SVDs. Task are nonsmooth problems, and nonsmooth low - rank matrix recovery tasks. Generic is method. ","This paper studies the problem of nonsmooth low-rank matrix recovery. The authors propose a novel method to recover the high-rank matrices of the low rank SVDs. The method is based on an extension of the extragradient method, where the optimal solution is obtained by maximizing the maximum of smooth functions of the nonsmoot objective. The main contribution of the paper is a theoretical analysis of the properties of the proposed method. ","This paper proposes a novel method for solving nonsmooth low-rank matrix recovery tasks. The authors propose an extragradient method for finding the optimal solution to the nonsmoot objective. The method is based on the natural generalized strict complementarity condition, which allows for the use of high-rank matrices to be used as high-ranking SVDs, while the low-ranking matrices are used as low-ranks. The main idea is to use convex relaxations to solve the problems, and the authors show that the maximum of smooth functions can be used to achieve the nonssmooth objective. Experiments are conducted on three different tasks, including statistics and machine learning. Results show the effectiveness of the proposed method. "
10801,SP:d39f1d77d9919f897ccf82958b71be8798523923,graphs CONJUNCTION images. images CONJUNCTION graphs. images CONJUNCTION texts. texts CONJUNCTION images. texts HYPONYM-OF structured treatments. graphs HYPONYM-OF structured treatments. images HYPONYM-OF structured treatments. arbitrary models USED-FOR learning. generalized Robinson decomposition USED-FOR causal estimand. mild assumptions FEATURE-OF quasi - oracle convergence guarantee. approach COMPARE prior work. prior work COMPARE approach. small - world and molecular graphs EVALUATE-FOR approach. prior work USED-FOR CATE estimation. CATE estimation EVALUATE-FOR approach. OtherScientificTerm is regularization bias. ,"This paper studies the problem of learning arbitrary models with structured treatments (e.g., texts, images, and graphs) from arbitrary data. The authors propose a generalized Robinson decomposition for causal estimand based on mild assumptions on the quasi-oracle convergence guarantee. The proposed approach is evaluated on small-world and molecular graphs and compared to prior work on CATE estimation. ","This paper proposes a novel approach to learning from arbitrary models for learning. The approach is based on the generalized Robinson decomposition of the causal estimand, which is a quasi-oracle convergence guarantee under mild assumptions. The authors show that the proposed approach outperforms prior work on CATE estimation on small-world and molecular graphs. "
10826,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"qualitative assumptions CONJUNCTION distributions. distributions CONJUNCTION qualitative assumptions. system USED-FOR distributions. causal graph HYPONYM-OF qualitative assumptions. probability axioms CONJUNCTION do - calculus. do - calculus CONJUNCTION probability axioms. do - calculus CONJUNCTION c - factorization. c - factorization CONJUNCTION do - calculus. probability axioms USED-FOR graphical criteria. graphical criteria USED-FOR identification algorithms. matrix equations USED-FOR proxy variables. graphical criteria CONJUNCTION matrix equations. matrix equations CONJUNCTION graphical criteria. graphical criteria USED-FOR causal identification algorithm. matrix equations USED-FOR causal identification algorithm. graphically - driven formulae CONJUNCTION matrix multiplications. matrix multiplications CONJUNCTION graphically - driven formulae. enriched matrix - based criteria PART-OF graphical identification approach. marginal, conditional, and interventional distributions USED-FOR causal effect identification algorithm. Task is Causal effect identification. OtherScientificTerm are causal effect, proxy variable based identification conditions, and intermediary criteria. ","This paper studies the problem of Causal effect identification. The authors propose a system based on qualitative assumptions on the causal graph and distributions. The graphical criteria for identification algorithms are based on probability axioms, do-calculus, and c-factorization. The proxy variable based identification conditions are defined as marginal, conditional, and interventional distributions, and the proxy variables are defined by matrix equations. The proposed graphical identification approach is based on enriched matrix-based criteria. The experimental results show that the proposed causal identification algorithm outperforms existing graphically-driven formulae and matrix multiplications.","This paper proposes a new framework for Causal effect identification. The framework is based on qualitative assumptions on the causal graph and distributions. The authors propose a system to model the distributions and the causal effect. The qualitative assumptions are based on probability axioms, do-calculus, and c-factorization, and the distributions are modeled as matrix equations. The proxy variable based identification conditions are modeled using matrix equations and matrix multiplications. The graphical criteria for identification algorithms based on graphical criteria and matrix equations are used to train the causal identification algorithm based on marginal, conditional, and interventional distributions. A graphical identification approach based on enriched matrix-based criteria is also proposed. "
10851,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"Unsupervised Environment Design ( UED ) HYPONYM-OF selfsupervised RL paradigm. random levels USED-FOR PLR. Dual Curriculum Design ( DCD ) HYPONYM-OF UED methods. PLR CONJUNCTION UED algorithm. UED algorithm CONJUNCTION PLR. UED algorithm CONJUNCTION PAIRED. PAIRED CONJUNCTION UED algorithm. PLR CONJUNCTION PAIRED. PAIRED CONJUNCTION PLR. PAIRED PART-OF DCD. PLR PART-OF DCD. UED algorithm PART-OF DCD. theory USED-FOR PLR. robustness guarantee FEATURE-OF Nash equilibria. theory USED-FOR PLR. Nash equilibria FEATURE-OF convergence. PLR⊥ USED-FOR PAIRED. PLR⊥ HYPONYM-OF method. Method are Deep reinforcement learning ( RL ) agents, Prioritized Level Replay ( PLR ), UED, and theoretical framework. OtherScientificTerm are environment and task configurations, diverse training environments, randomly - generated training levels, and theoretical guarantees. Generic is it. ","This paper proposes a selfsupervised environment design (UED) method for deep reinforcement learning (RL) agents. The authors propose Dual Curriculum Design (DCD), which is an extension of UED methods such as PLR and UED algorithm. The main idea of DCD is to learn a set of environment and task configurations, which are then used to train a prioritized level Replay (PLR) agent. The PLR agent is trained on random levels of the environment, and the goal is to maximize the robustness guarantee of the PLR. The paper provides theoretical guarantees for the convergence of the proposed method, PLR⊥, and shows that it can converge to a Nash equilibria. The theoretical framework is well-motivated and well-written. The experimental results show that PLR outperforms PAIRED and PLR in terms of robustness.","This paper proposes a selfsupervised environment design (UED) paradigm for Deep reinforcement learning (RL) agents. The authors propose two UED methods, Dual Curriculum Design (DCD) and Prioritized Level Replay (PLR). The authors show that PLR converges to a Nash equilibria with robustness guarantee under diverse training environments and random levels. PLR and the UED algorithm are equivalent to DCD and PAIRED. The theoretical framework for PLR is well-motivated, and it provides theoretical guarantees. The proposed method, PLR⊥, is evaluated on two datasets."
10876,SP:9ed528da4b67f22678303cfd975aafe678db6411,"( ε, δ)-differentially private algorithm USED-FOR multi - armed bandit ( MAB ) problem. shuffle model FEATURE-OF multi - armed bandit ( MAB ) problem. upper bound COMPARE regret. regret COMPARE upper bound. Metric are distribution - dependent regret, and distribution - independent regret. OtherScientificTerm is suboptimality gap. Method are centralized model, and local model. ","This paper studies the multi-armed bandit (MAB) problem with a shuffle model. The authors propose a (ε, δ)-differentially private algorithm to solve the MAB problem. They show that the suboptimality gap between the centralized model and the local model is bounded by a distribution-dependent regret. They also provide an upper bound on the regret.","This paper proposes a (ε, δ)-differentially private algorithm to solve the multi-armed bandit (MAB) problem with a shuffle model. The main idea is to use a centralized model to compute the suboptimality gap between the local model and the distribution-independent regret. The authors show that the upper bound of the regret is lower than the regret of the distribution."
10901,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"probabilistic forecasts USED-FOR decision rules. threshold calibration HYPONYM-OF calibration. algorithm USED-FOR threshold - calibrated forecaster. uncalibrated forecaster USED-FOR algorithm. threshold loss function USED-FOR threshold decision. hospital scheduling decisions CONJUNCTION resource allocation decisions. resource allocation decisions CONJUNCTION hospital scheduling decisions. threshold calibration USED-FOR decision loss prediction. real - world settings EVALUATE-FOR threshold calibration. resource allocation decisions HYPONYM-OF real - world settings. hospital scheduling decisions HYPONYM-OF real - world settings. OtherScientificTerm are forecasted probabilities, predicted losses, cutoff, decision loss, and threshold decisions. Task are regression setting, and loss of threshold decisions. Generic is procedure. ","This paper studies the problem of decision rules based on probabilistic forecasts. The authors consider the regression setting, where the goal is to find the best decision rule for a given task. The main idea is to use a threshold-calibrated forecaster with an uncalibrated pre-trained algorithm. The algorithm is based on the idea of threshold calibration, which is a calibration of the threshold loss function for the threshold decision. The proposed procedure is well-motivated and well-thought-out. Theoretically, the authors show that threshold calibration can be used for decision loss prediction in real-world settings such as hospital scheduling decisions and resource allocation decisions. ","This paper proposes a new algorithm to train a threshold-calibrated forecaster for decision rules based on probabilistic forecasts. The algorithm is based on the idea of threshold calibration, which is a calibration of the prediction of the loss of the decision rules. The authors propose to use the threshold loss function as the threshold decision function. The proposed algorithm is evaluated on two real-world settings (hospital scheduling decisions and resource allocation decisions) and one regression setting (the regression setting). The authors show that the proposed procedure outperforms the baseline. The main contribution of the paper is the proposed algorithm. "
10926,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,method USED-FOR centroid approximation. centroid approximation USED-FOR random function. argmax distribution HYPONYM-OF random function. method USED-FOR argmax distribution. centroid points USED-FOR argmax distribution. centroid points USED-FOR method. objective function USED-FOR method. objective function USED-FOR argmax distribution. personalized dialogue systems CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue systems. few - shot image classification CONJUNCTION personalized dialogue systems. personalized dialogue systems CONJUNCTION few - shot image classification. real - world multitask learning applications EVALUATE-FOR method. few - shot image classification HYPONYM-OF real - world multitask learning applications. multi - target domain adaptation HYPONYM-OF real - world multitask learning applications. personalized dialogue systems HYPONYM-OF real - world multitask learning applications. Task is machine learning. Method is argmax centroid method. OtherScientificTerm is Wasserstein distance. ,"This paper proposes a method for centroid approximation for a random function, the argmax distribution. The method is based on centroid points, which can be approximated by a method based on an objective function. The argmax centroid method can be applied to a wide range of machine learning tasks, including few-shot image classification, personalized dialogue systems, multi-target domain adaptation, and real-world multitask learning applications. The authors show that the proposed method is able to obtain a good approximation of argmax with Wasserstein distance.","This paper proposes a method for centroid approximation to the random function, the argmax distribution. The method is based on centroid points. The authors propose a new objective function to approximate the argmin distribution, which is a Wasserstein distance. The proposed method is evaluated on several real-world multitask learning applications, including few-shot image classification, personalized dialogue systems, and multi-target domain adaptation. "
10951,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"adversarial manner USED-FOR preferences. preference vector USED-FOR reward function. pre - specified multi - objective reward functions FEATURE-OF preference vector. episodic learning problem USED-FOR problem. Markov decision process USED-FOR episodic learning problem. nearly minimax optimal regret bound EVALUATE-FOR model - based algorithm. nearly optimal trajectory complexity EVALUATE-FOR algorithm. Task are multi - objective reinforcement learning, and online setting. OtherScientificTerm are transitions, ( adversarial ) preference, policies, and preference - free exploration. ","This paper studies the problem of multi-objective reinforcement learning in the online setting. The authors consider the episodic learning problem in the Markov decision process, where the goal is to learn a preference vector for a reward function in a pre-specified multi-order reward functions. The preferences are learned in an adversarial manner in an online manner, and the transitions between the (adversarial) preference and the reward function are learned by learning policies. The proposed algorithm achieves a nearly minimax optimal regret bound with nearly optimal trajectory complexity. ","This paper studies multi-objective reinforcement learning in the online setting. The authors propose an adversarial manner to learn preferences in an online setting, where transitions are represented by (adversarial) preference. The preference vector of the reward function is a pre-specified multi-order reward functions. The problem is formulated as an episodic learning problem with a Markov decision process, and the authors propose a model-based algorithm with a nearly minimax optimal regret bound. The algorithm has nearly optimal trajectory complexity, and is able to learn policies with preference-free exploration."
10976,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"classification models COMPARE explanation of sequence generation models. explanation of sequence generation models COMPARE classification models. model - agnostic explanations USED-FOR text generation task. dialogue response generation HYPONYM-OF text generation task. open - ended sentences USED-FOR Dialog response generation. LERG USED-FOR sequence prediction. LERG USED-FOR explanations. unbiased approximation CONJUNCTION consistency. consistency CONJUNCTION unbiased approximation. consistency CONJUNCTION cause identification. cause identification CONJUNCTION consistency. explanation USED-FOR text generation. LERG USED-FOR text generation. explanation USED-FOR LERG. consistency HYPONYM-OF explanation. automaticand humanevaluation metrics EVALUATE-FOR task. method COMPARE methods. methods COMPARE method. task EVALUATE-FOR methods. task EVALUATE-FOR method. automaticand humanevaluation metrics EVALUATE-FOR methods. automaticand humanevaluation metrics EVALUATE-FOR method. LERG USED-FOR explicit and implicit relations. Method are generation model, and local explanation of response generation ( LERG ). OtherScientificTerm is human response. ","This paper proposes a new text generation task based on model-agnostic explanations. Dialog response generation with open-ended sentences is an important text generation problem. The authors propose a local explanation of response generation (LERG), which is an extension of the LERG for sequence prediction. The explanation is a combination of unbiased approximation, consistency, and cause identification. The proposed method is evaluated on this task and compared with other methods on both automaticand humanevaluation metrics.","This paper proposes a new text generation task based on model-agnostic explanations for dialogue response generation. The authors propose a new generation model, called local explanation of response generation (LERG). Dialog response generation is an open-ended sentences generation task, where the goal is to predict the human response to a given sentence. LERG is used for sequence prediction and explanation of sequence generation models. The proposed method is evaluated on both automaticand humanevaluation metrics for the task. The method is shown to outperform existing methods in terms of unbiased approximation, consistency, and cause identification. "
11001,SP:965413b1726617006317bbbec55673dd5d21812a,"distributed methods USED-FOR compressor. contraction property FEATURE-OF compressor. RandK HYPONYM-OF biased compressors. error compensation CONJUNCTION error feedback. error feedback CONJUNCTION error compensation. gradient compression CONJUNCTION acceleration. acceleration CONJUNCTION gradient compression. error compensation CONJUNCTION acceleration. acceleration CONJUNCTION error compensation. error compensation USED-FOR gradient compression. method COMPARE error compensated algorithms. error compensated algorithms COMPARE method. communication rounds EVALUATE-FOR method. Method are Gradient compression, error compensated gradient compression methods, and error compensated loopless Katyusha method. Metric are communication cost, and accelerated linear convergence rate. OtherScientificTerm is divergence. ","This paper studies the problem of gradient compression with distributed methods. Gradient compression has been a hot topic in recent years, with biased compressors such as RandK being the most popular. The authors propose a distributed methods to compress a compressor with contraction property. The main idea is to use error compensated gradient compression methods, where error compensation and error feedback are combined with gradient compression and acceleration. The paper shows that the communication cost of the error compensated loopless Katyusha method is bounded by the accelerated linear convergence rate. The proposed method is shown to have better communication rounds than other error compensated algorithms.","This paper proposes a distributed methods for learning a compressor with contraction property. Gradient compression is an important problem in machine learning. The authors propose two variants of biased compressors, RandK and RandK-2, which are based on distributed methods. In RandK, the communication cost of the compressor is shared across all clients, while in RandK it is shared among all clients. The paper also proposes an error compensated loopless Katyusha method, which is based on the error compensated gradient compression methods. The proposed method is evaluated on several communication rounds, and the authors show that the proposed method outperforms error compensated algorithms in terms of error compensation, error feedback, acceleration, and gradient compression. "
11026,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"machine learning framework USED-FOR edge and neuromorphic computing paradigms. training complexity CONJUNCTION biological plausibility. biological plausibility CONJUNCTION training complexity. machine learning framework USED-FOR it. biological plausibility FEATURE-OF liquid state machine ( LSM ). training complexity EVALUATE-FOR liquid state machine ( LSM ). LSM USED-FOR internal weights. LSM COMPARE multi - layer neural networks. multi - layer neural networks COMPARE LSM. LSM USED-FOR model of brain computation. synaptic plasticity CONJUNCTION brain dynamics. brain dynamics CONJUNCTION synaptic plasticity. astrocytes USED-FOR synaptic plasticity. astrocytes USED-FOR brain dynamics. neuron - astrocyte liquid state machine ( NALSM)1 USED-FOR under - performance. self - organized near - critical dynamics USED-FOR neuron - astrocyte liquid state machine ( NALSM)1. self - organized near - critical dynamics USED-FOR under - performance. astrocyte model USED-FOR global feedback. NALSM COMPARE LSM methods. LSM methods COMPARE NALSM. accuracy EVALUATE-FOR LSM methods. accuracy EVALUATE-FOR NALSM. MNIST CONJUNCTION N - MNIST. N - MNIST CONJUNCTION MNIST. accuracy EVALUATE-FOR NALSM. N - MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION N - MNIST. braininspired machine learning methods USED-FOR deep learning. deep learning USED-FOR neuromorphic computing. Task is brain computation. Method are backpropagation of gradients, and backpropagation. OtherScientificTerm are brain networks, computationally optimal critical phase transition, neuronal activity, NALSM dynamics, branching factor, edge - of - chaos, and data - specific hand - tuning. ","This paper proposes a new machine learning framework for edge and neuromorphic computing paradigms. The authors propose a new model of brain computation based on LSM that combines synaptic plasticity, brain dynamics, and astrocytes. The main idea is to use a self-organized near-critical dynamics in the neuron-astrocyte liquid state machine (NALSM)1 to improve the under-performance of LSM by reducing the training complexity and biological plausibility of the LSM. The LSM is trained by backpropagation of gradients, and the authors show that the internal weights of the NALSM can be trained to be computationally optimal critical phase transition. The paper also shows that LSM outperforms multi-layer neural networks and N-MNIST on MNIST and Fashion MNIST.    The authors also show that by using the self-organized near-critic dynamics of the neuron -astrocyclic liquid-state machine (NSLM)1, the accuracy of the proposed LSM methods is better than the state-of-the-art in terms of accuracy. ","The paper proposes a new machine learning framework for edge and neuromorphic computing paradigms. The main idea is to learn a model of brain computation using LSM, which is an extension of the LSM framework. The authors show that LSM is able to achieve better training complexity and biological plausibility than the original LSM in terms of internal weights, synaptic plasticity, brain dynamics, and astrocytes. They also show that backpropagation of gradients leads to a computationally optimal critical phase transition. The paper also shows that the proposed LSM outperforms multi-layer neural networks on MNIST, Fashion-MNIST, and N-MIST. "
11051,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"class imbalance problem HYPONYM-OF learning node representations. asymmetric topological properties FEATURE-OF labeled nodes. graph data USED-FOR imbalance. Label Propagation algorithm USED-FOR node influence shift phenomenon. model - agnostic method ReNode USED-FOR topology - imbalance issue. influence conflict detection – based metric Totoro USED-FOR graph topology imbalance. method USED-FOR topology - imbalance issue. method USED-FOR semi - supervised node classification. topology - imbalance issue CONJUNCTION semi - supervised node classification. semi - supervised node classification CONJUNCTION topology - imbalance issue. graph neural networks ( GNNs ) USED-FOR topology imbalance. OtherScientificTerm are quantity imbalance, graph ( topology imbalance ), and class boundaries. Task are unknown topology - imbalance issue, semisupervised node classification learning, and quantityand topologyimbalance issues. ","This paper studies the class imbalance problem in learning node representations with asymmetric topological properties. The authors consider the unknown topology-imbalance issue in semisupervised node classification learning, and propose a Label Propagation algorithm to mitigate the node influence shift phenomenon. Theoretically, the authors show that the topology imbalance is caused by the quantity imbalance in the graph (topology imbalance), and that the influence conflict detection –based metric Totoro can be used to identify the graph topological imbalance.  The authors then propose a model-agnostic method ReNode to solve topology - imbalance issue and a method for semi-supervision node classification with graph neural networks (GNNs). Theoretical results on the quantityand topologyimbalance issues are provided.","This paper studies the class imbalance problem of learning node representations with asymmetric topological properties. The authors propose a Label Propagation algorithm to mitigate the node influence shift phenomenon. The paper also proposes a model-agnostic method ReNode to address the topology-imbalance issue and a method for semi-supervised node classification. The topology imbalance issue is measured by the influence conflict detection –based metric Totoro, and the quantity imbalance is measured using graph (topology imbalance), and the class boundaries are measured using the quantityand topologyimbalance issues. "
11076,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"d - dimensional lattice FEATURE-OF additive Gaussian noise. additive Gaussian noise USED-FOR piece - wise constant signals. partition recovery USED-FOR partition of the lattice. DCART - based procedure USED-FOR partition. regularity conditions USED-FOR DCART - based procedure. recursive dyadic partitions USED-FOR signal partition. recursive dyadic partitions USED-FOR rectangular sub - graphs. NP - hard exhaustive search method USED-FOR one. optimal regression tree estimator ( ORT ) USED-FOR partition estimator. DCART USED-FOR partition recovery. Task are signal detection or testing, and estimation. OtherScientificTerm are constancy regions of the unknown signal, and noise variance. Generic is method. ","This paper studies the problem of signal detection or testing in a d-dimensional lattice with additive Gaussian noise in the form of piece-wise constant signals. The authors propose to use partition recovery to recover the partition of the lattice using a DCART-based procedure under regularity conditions. The proposed method is based on a NP-hard exhaustive search method, where the signal partition is partitioned into recursive dyadic partitions for rectangular sub-graphs, and the constancy regions of the unknown signal are estimated by an optimal regression tree estimator (ORT). The authors show that the proposed partition estimator can recover the original signal partition using DCART. ",This paper proposes a new method for partition recovery of the lattice of additive Gaussian noise in a d-dimensional lattice. The authors propose a DCART-based procedure to recover the partition under regularity conditions. The proposed method is based on a NP-hard exhaustive search method. The paper also proposes a partition estimator based on an optimal regression tree estimator (ORT) to estimate the partition. The method is evaluated on rectangular sub-graphs with recursive dyadic partitions for the signal partition. 
11101,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"tasks EVALUATE-FOR deep learning models. causality - based training framework USED-FOR spurious correlations. interventional distribution COMPARE observational distribution. observational distribution COMPARE interventional distribution. Maximum Likelihood Estimation ( MLE ) USED-FOR interventional distribution. algorithms USED-FOR causal predictions. Implicit CMLE CONJUNCTION Explicit CMLE. Explicit CMLE CONJUNCTION Implicit CMLE. Implicit CMLE USED-FOR causal predictions. algorithms USED-FOR deep learning models. Explicit CMLE USED-FOR causal predictions. deep learning models USED-FOR causal predictions. Implicit CMLE HYPONYM-OF algorithms. Explicit CMLE HYPONYM-OF algorithms. observational data USED-FOR algorithms. observational data USED-FOR deep learning models. observational data USED-FOR causal predictions. observational data USED-FOR interventional distribution. Natural Language Inference ( NLI ) CONJUNCTION Image Captioning. Image Captioning CONJUNCTION Natural Language Inference ( NLI ). simulated data CONJUNCTION real - world tasks. real - world tasks CONJUNCTION simulated data. Natural Language Inference ( NLI ) HYPONYM-OF real - world tasks. Image Captioning HYPONYM-OF real - world tasks. CMLE methods COMPARE regular MLE method. regular MLE method COMPARE CMLE methods. CMLE methods USED-FOR spurious correlations. out - of - domain generalization EVALUATE-FOR regular MLE method. out - of - domain generalization EVALUATE-FOR CMLE methods. Generic is they. OtherScientificTerm are predictive clues, observed confounders, and expected negative log - likelihood. Method are general Structural Causal Model ( SCM ), and Counterfactual Maximum Likelihood Estimation ( CMLE ). ","This paper proposes a causality-based training framework to reduce spurious correlations in deep learning models. The authors propose a general Structural Causal Model (SCM), which is based on the observation that the interventional distribution of the observed confounders is not the same as the observational distribution, but rather a Maximum Likelihood Estimation (MLE) which is a weighted average of the two distributions. They propose two algorithms for making causal predictions based on observational data: Implicit CMLE and Explicit CMLE. They show that these CMLE methods achieve better out-of-domain generalization than the regular MLE method. They also show that they are more robust to spurious correlations than the CMLE method in practice.    The authors conduct experiments on simulated data and real-world tasks such as Natural Language Inference (NLI) and Image Captioning to demonstrate the effectiveness of the proposed algorithms.","This paper proposes a causality-based training framework to reduce spurious correlations. The authors propose a general Structural Causal Model (SCM), which is based on the Counterfactual Maximum Likelihood Estimation (CMLE) to estimate the interventional distribution of the predictive clues and observed confounders, and the expected negative log-likelihood of the predicted predictive clues. The proposed algorithms, Implicit CMLE and Explicit CMLE, are used to make causal predictions using observational data for deep learning models on a variety of real-world tasks (Natural Language Inference (NLI) and Image Captioning). The authors show that the proposed CMLE methods achieve better out-of-domain generalization than the regular MLE method."
11126,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"multi - task learning USED-FOR learning. multi - task learning COMPARE single task learning. single task learning COMPARE multi - task learning. learning COMPARE single task learning. single task learning COMPARE learning. multi - task learning objective USED-FOR average loss. gradients FEATURE-OF task objectives. heuristics USED-FOR problem. heuristics USED-FOR task gradients. Conflict - Averse Gradient descent ( CAGrad ) USED-FOR average loss function. regular gradient descent ( GD ) CONJUNCTION multiple gradient descent algorithm ( MGDA ). multiple gradient descent algorithm ( MGDA ) CONJUNCTION regular gradient descent ( GD ). multiple gradient descent algorithm ( MGDA ) PART-OF multi - objective optimization ( MOO ) literature. multiple gradient descent algorithm ( MGDA ) PART-OF It. regular gradient descent ( GD ) PART-OF It. CAGrad COMPARE multi - objective gradient manipulation methods. multi - objective gradient manipulation methods COMPARE CAGrad. OtherScientificTerm are model structures, conflicting gradients, average gradient direction, convergence guarantee, Pareto - stationary point, and worst local improvement. Generic is objective. Method is multi - task model. ",This paper studies the problem of multi-task learning in the context of learning with conflicting gradients. The authors propose a new objective called Conflict-Averse Gradient descent (CAGrad) to improve the average loss function of the multi-tasks learning objective. It is a combination of regular gradient descent (GD) and a multiple gradient descent algorithm (MGDA) from multi-objective optimization (MOO) literature. CAGrad is shown to converge faster than other multi- objective gradient manipulation methods. The convergence guarantee is based on the Pareto-stationary point. ,This paper proposes a new multi-task learning objective to improve the average loss function of the model structures. The main idea is to use Conflict-Averse Gradient descent (CAGrad) to reduce the number of task gradients for each of the different task objectives. It is an extension of the multi-objective optimization (MOO) literature with regular gradient descent (GD) and a multiple gradient descent algorithm (MGDA). The authors show that CAGrad outperforms other multi- objective gradient manipulation methods in terms of convergence guarantee. The authors also show that heuristics for the problem can be used to learn the task gradientients. 
11151,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"Large language models USED-FOR few - shot learning. language models USED-FOR simple algorithmic concepts. strong priors FEATURE-OF teaching problem. program induction systems CONJUNCTION humans. humans CONJUNCTION program induction systems. GPT architectures CONJUNCTION program induction systems. program induction systems CONJUNCTION GPT architectures. complexity EVALUATE-FOR concept. GPT architectures CONJUNCTION humans. humans CONJUNCTION GPT architectures. artificial intelligence CONJUNCTION machine learning. machine learning CONJUNCTION artificial intelligence. language models CONJUNCTION machine teaching. machine teaching CONJUNCTION language models. machine teaching USED-FOR artificial intelligence. OtherScientificTerm are patterns of algorithmic nature, and Occam ’s razor. Generic is models. Task is learning. ","This paper studies the problem of few-shot learning with large language models for simple algorithmic concepts, where the goal is to learn patterns of algorithmic nature. The authors consider the teaching problem with strong priors. The main contribution of the paper is a new concept called “Occam’s razor”, which is a combination of program induction systems, GPT architectures, humans, and language models. The paper shows that the complexity of the concept is bounded by the number of models, and that the learning of these models can be combined with artificial intelligence and machine learning.","This paper proposes a new method for few-shot learning with large language models. The main idea is to learn simple algorithmic concepts with language models, and then use these models to learn the patterns of algorithmic nature. The authors show that the teaching problem with strong priors can be solved by learning models that are robust to adversarial attacks. The paper also shows that the concept of “occam’s razor” can be reduced to a simple concept, and the complexity of the concept is reduced to the same complexity of program induction systems, humans, and GPT architectures. Experiments are conducted on artificial intelligence, machine learning, and language models to demonstrate the effectiveness of the proposed method."
11176,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"perturbation USED-FOR Adversarial examples. feature representation USED-FOR robust and non - robust features. Information Bottleneck USED-FOR feature representation. Information Bottleneck USED-FOR way. noise variation USED-FOR feature unit. information flow PART-OF feature representation. noise variation magnitude USED-FOR information flow. human - perceptible semantic information FEATURE-OF they. attack mechanism USED-FOR gradient of non - robust features. OtherScientificTerm are adversarial examples, feature space, and distilled features. Task are adversarial prediction, model prediction, and model robustness. ","This paper studies the problem of adversarial prediction. The authors propose a new way to improve the robustness of the feature representation of robust and non-robust features by using Information Bottleneck. The key idea is to use the noise variation magnitude of a feature unit as the information flow in the original feature representation, and then use the information Bottleneck to distill the feature space into the distilled features. The proposed attack mechanism is then applied to the gradient of non-observable features, which is then used to improve model robustness. Experiments show the effectiveness of the proposed method.","This paper proposes a new adversarial attack method for adversarial examples. The proposed method is based on the idea of adversarial prediction, where the adversarial example is generated by perturbation on the feature space of the model. The authors propose a way to attack the feature representation of robust and non-robust features by using the Information Bottleneck. The key idea is to use noise variation magnitude of the feature unit in the input feature space to change the information flow between the feature representations of the two features. The paper also proposes a novel attack mechanism for the gradient of non-robot features, which is a variant of the attack mechanism used for model robustness. Experiments show that the proposed method can improve the robustness of model prediction. "
11201,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"support vector machine ( SVM ) CONJUNCTION minimum Euclidean norm least squares regression. minimum Euclidean norm least squares regression CONJUNCTION support vector machine ( SVM ). models USED-FOR high - dimensional data. support vector machine ( SVM ) HYPONYM-OF approaches. approaches USED-FOR linear models. minimum Euclidean norm least squares regression HYPONYM-OF approaches. they PART-OF models. support vector proliferation USED-FOR independent feature models. super - linear lower bound USED-FOR support vector proliferation. dimension USED-FOR support vector proliferation. super - linear lower bound FEATURE-OF dimension. sharp phase transition FEATURE-OF Gaussian feature models. geometric characterization of the problem USED-FOR lp case. l1 variant PART-OF SVM. OtherScientificTerm are support vector, upper bounds, and phase transition. Generic is transition. ",This paper studies the problem of support vector machine (SVM) and minimum Euclidean norm least squares regression for high-dimensional data. The authors propose two approaches to train linear models with support vector. The main idea is to use the dimension of the support vector proliferation to train independent feature models with a super-linear lower bound on the dimension. They show that they can be used to train models with l1 variant of the SVM. They also provide upper bounds on the sharp phase transition of Gaussian feature models in the lp case. They provide geometric characterization of the problem.,"This paper proposes two approaches to train linear models on high-dimensional data, namely support vector machine (SVM) and minimum Euclidean norm least squares regression. The authors show that they can be used to train independent feature models with support vector proliferation. The main contribution of the paper is to show that the dimension of the support vector density is a super-linear lower bound of the dimension for the support vectors of the two models. They also show that for the lp case, the l1 variant of the SVM is equivalent to the l2 variant in the case of the L1 variant. The paper also shows that the upper bounds for the Lp case are the same as the ones in the L2 case. Finally, the authors provide a geometric characterization of the problem of Gaussian feature models in terms of sharp phase transition. "
11226,SP:99f226a63902863c429cb7baefab09626d13921e,"Markov Decision Processes USED-FOR active pure exploration problem. instance - specific sample complexity EVALUATE-FOR algorithm. algorithm USED-FOR communicating MDPs. reduced exploration rate CONJUNCTION faster convergence. faster convergence CONJUNCTION reduced exploration rate. reduced exploration rate FEATURE-OF variant. ergodicity assumption USED-FOR variant. online setting USED-FOR navigation constraints. ergodic theorem USED-FOR non - homogeneous Markov chains. ergodic theorem USED-FOR analysis. OtherScientificTerm are system trajectory, and problem - dependent lower bound. Task are generative setting, and analysis of Markov Decision Processes. ","This paper studies the active pure exploration problem of Markov Decision Processes in the generative setting. The authors propose a new algorithm for communicating MDPs with instance-specific sample complexity. The proposed variant is based on the ergodicity assumption and has a reduced exploration rate and faster convergence. The paper also provides a problem-dependent lower bound for the system trajectory. Finally, the authors provide an online setting for navigation constraints.    The authors provide a theoretical analysis of the non-homogeneous Markov chains, and an ergodic theorem for their analysis. ","This paper proposes a novel algorithm for communicating MDPs with instance-specific sample complexity. The authors propose to use Markov Decision Processes to solve the active pure exploration problem, which is a generative setting where the goal is to learn a system trajectory. The proposed algorithm is based on the ergodicity assumption on the system trajectory, which allows for a problem-dependent lower bound on the number of samples. The paper also proposes a variant of the proposed algorithm with reduced exploration rate and faster convergence. The main contribution of the paper is to provide an analysis of the non-homogeneous Markov chains based on an ergodic theorem. The analysis is done in an online setting with navigation constraints."
11251,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"entities CONJUNCTION first - order logical ( FOL ) queries. first - order logical ( FOL ) queries CONJUNCTION entities. low - dimensional spaces FEATURE-OF first - order logical ( FOL ) queries. low - dimensional spaces FEATURE-OF entities. conjunction CONJUNCTION disjunction. disjunction CONJUNCTION conjunction. disjunction CONJUNCTION negation. negation CONJUNCTION disjunction. geometry - based QE model USED-FOR FOL operations. query embedding model USED-FOR FOL operations. Cone Embeddings ( ConE ) HYPONYM-OF geometry - based QE model. Cone Embeddings ( ConE ) HYPONYM-OF query embedding model. negation HYPONYM-OF FOL operations. disjunction HYPONYM-OF FOL operations. conjunction HYPONYM-OF FOL operations. geometric complement operators USED-FOR negation operations. embedding space FEATURE-OF geometric complement operators. ConE COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ConE. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR ConE. Task is multi - hop reasoning over knowledge graphs. OtherScientificTerm are knowledge graphs, geometric shapes, Cartesian products of two - dimensional cones, conjunction and disjunction operations, closure of complement of cones, and cones. Method is geometry - based models. ","This paper studies multi-hop reasoning over knowledge graphs. The authors propose a query embedding model for FOL operations such as conjunction, disjunction, and negation, which are based on a geometry-based QE model called Cone Embeddings (ConE). ConE uses geometric complement operators in the embedding space to represent the geometric shapes of the nodes in the knowledge graph. ConE outperforms state-of-the-art methods on several benchmark datasets.","This paper proposes a new framework for multi-hop reasoning over knowledge graphs. The key idea is to use a geometry-based QE model to perform FOL operations on entities and first-order logical (FOL) queries in low-dimensional spaces. The authors propose Cone Embeddings (ConE) which is a query embedding model that can be applied to any FOL operation such as conjunction, disjunction, and negation. ConE is shown to outperform state-of-the-art methods on a number of benchmark datasets. The main contribution of the paper is the introduction of geometric shapes, Cartesian products of two-dimensional cones, and the closure of complement of cones. The geometric complement operators are used to perform negation operations in the embedding space. "
11276,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"linear - time Legendre transform USED-FOR numerical scheme. numerical scheme USED-FOR value iteration ( VI ) algorithm. linear - time Legendre transform USED-FOR value iteration ( VI ) algorithm. conjugate domain FEATURE-OF value iteration ( VI ) algorithm. time complexity EVALUATE-FOR algorithm. error EVALUATE-FOR algorithm. convergence EVALUATE-FOR algorithm. convergence CONJUNCTION time complexity. time complexity CONJUNCTION convergence. time complexity CONJUNCTION error. error CONJUNCTION time complexity. minimization operation PART-OF primal domain. discretization USED-FOR state and input spaces. discretization USED-FOR approach. time complexity EVALUATE-FOR approach. Task is stochastic nonlinear systems. OtherScientificTerm are state and input variables, and O(X + U ). Method is VI algorithm. ",This paper proposes a numerical scheme based on the linear-time Legendre transform for value iteration (VI) algorithm in the conjugate domain of stochastic nonlinear systems. The proposed approach uses discretization to represent the state and input spaces in the primal domain by minimization operation in the O(X + U) of the VI algorithm. The convergence and time complexity of the proposed algorithm are shown.,The paper proposes a numerical scheme for value iteration (VI) algorithm based on linear-time Legendre transform in the conjugate domain of stochastic nonlinear systems. The proposed approach is based on discretization of the state and input spaces. The minimization operation in the primal domain of the VI algorithm is O(X + U). The proposed algorithm is shown to achieve convergence and time complexity in terms of error.
11301,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"framework USED-FOR multimodal representations. unlabeled data USED-FOR framework. unlabeled data USED-FOR multimodal representations. convolution - free Transformer architectures USED-FOR framework. multimodal representations USED-FOR downstream tasks. VideoAudio - Text Transformer ( VATT ) USED-FOR multimodal representations. raw signals USED-FOR VideoAudio - Text Transformer ( VATT ). audio event classification CONJUNCTION image classification. image classification CONJUNCTION audio event classification. image classification CONJUNCTION text - to - video retrieval. text - to - video retrieval CONJUNCTION image classification. video action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION video action recognition. video action recognition HYPONYM-OF downstream tasks. text - to - video retrieval HYPONYM-OF downstream tasks. image classification HYPONYM-OF downstream tasks. audio event classification HYPONYM-OF downstream tasks. multimodal contrastive losses USED-FOR VATT. convolution - free VATT COMPARE ConvNet - based architectures. ConvNet - based architectures COMPARE convolution - free VATT. ConvNet - based architectures USED-FOR downstream tasks. convolution - free VATT USED-FOR downstream tasks. Kinetics-400 EVALUATE-FOR VATT ’s vision Transformer. top-1 accuracy EVALUATE-FOR VATT ’s vision Transformer. videos CONJUNCTION images. images CONJUNCTION videos. VATT ’s audio Transformer USED-FOR waveform - based audio event recognition. mAP EVALUATE-FOR VATT ’s audio Transformer. Method are modality - agnostic, single - backbone Transformer, and supervised pre - training. Material are Kinetics-600, Kinetics-700, ImageNet, and AudioSet. Generic are Transformer, and model. ","This paper proposes a new framework for learning multimodal representations from unlabeled data. The proposed VideoAudio-Text Transformer (VATT) is based on convolution-free Transformer architectures and uses the raw signals as the input to a single-backbone Transformer. The Transformer is trained with a modality-agnostic, single-head backbone Transformer, and is trained using supervised pre-training. The model is evaluated on Kinetics-600, Kinetics -700, ImageNet, and AudioSet. The results show that VATT outperforms ConvNet-based architectures on several downstream tasks such as audio event classification, image classification, text-to-video retrieval, and video action recognition. VATT’s audio Transformer also outperforms waveform-based audio event recognition on mAP. ","This paper proposes a framework for learning multimodal representations from unlabeled data. The framework is based on convolution-free Transformer architectures. The authors propose a VideoAudio-Text Transformer (VATT) that uses raw signals as input to learn multi-modal representations for downstream tasks such as audio event classification, image classification, text-to-video retrieval, and video action recognition. The main idea is to train a modality-agnostic, single-backbone Transformer, where each modality is represented by a single- backbone Transformer. The Transformer is trained using supervised pre-training. The model is trained on ImageNet, Kinetics-600, and AudioSet. The proposed VATT is evaluated on a set of downstream tasks (audio event classification and image classification). The authors show that the proposed model achieves top-1 accuracy compared to Convolution-Free VATT and other ConvNet-based architectures for these downstream tasks. They also show that VATT outperforms ConvNet with multimodals contrastive losses. "
11326,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"quadratic time and space complexity EVALUATE-FOR self - attention mechanism. computation bottleneck FEATURE-OF pairwise dot products. computation bottleneck FEATURE-OF kernel machines. computational cost EVALUATE-FOR approximation schemes. accuracy EVALUATE-FOR approximation schemes. computation methods USED-FOR kernel machines. Nyström method USED-FOR computation. Nyström method USED-FOR non - positive semidefinite matrix. softmax structure CONJUNCTION Gaussian kernel. Gaussian kernel CONJUNCTION softmax structure. computation methods USED-FOR computational cost. matrix approximation error EVALUATE-FOR method. spectral norm FEATURE-OF matrix approximation error. spectral norm EVALUATE-FOR method. method COMPARE full self - attention. full self - attention COMPARE method. Long Range Arena benchmark EVALUATE-FOR method. Long Range Arena benchmark EVALUATE-FOR full self - attention. Method are Transformers, and Skyformer. OtherScientificTerm is computation resources. ","This paper proposes a self-attention mechanism with quadratic time and space complexity. The authors show that the computation bottleneck of pairwise dot products in kernel machines with a computation bottleneck is a function of the number of computation resources. They then propose a Nyström method for computing the computation of a non-positive semidefinite matrix with a softmax structure and a Gaussian kernel. The proposed method achieves a spectral norm of the matrix approximation error, which is better than previous approximation schemes with high computational cost in terms of accuracy. They also show that their method performs better than full self- attention on the Long Range Arena benchmark. ",This paper proposes a self-attention mechanism with quadratic time and space complexity. The authors propose to use the Nyström method for computation of a non-positive semidefinite matrix with a computation bottleneck on the pairwise dot products. The computation bottleneck is a result of the computation bottleneck in kernel machines with high computational cost due to the computation resources. The proposed method is based on the softmax structure and Gaussian kernel. The method is evaluated on the Long Range Arena benchmark and shows that the proposed method achieves better matrix approximation error compared to full self- attention. 
11351,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"setting PART-OF problems. image - based data augmentation USED-FOR invariance. image - based data augmentation CONJUNCTION expert - aware offline data augmentation approach. expert - aware offline data augmentation approach CONJUNCTION image - based data augmentation. expert - aware offline data augmentation approach USED-FOR feedback - sensitivity. image - based data augmentation USED-FOR image perturbations. image - based data augmentation PART-OF augmented policy cloning ( APC ) approach. method USED-FOR transfer 12 of complex high - DoF behaviors. method USED-FOR policy cloning. data - efficiency EVALUATE-FOR policy cloning. approach USED-FOR algorithms. policy cloning USED-FOR transfer 12 of complex high - DoF behaviors. policy cloning PART-OF algorithms. data - efficiency EVALUATE-FOR method. Method are data - augmentation technique, and behavioral cloning. Task is policy 3 cloning setting. Metric is data efficiency. OtherScientificTerm are expert, student policy, and expert trajectories. ","This paper proposes a data-augmented policy cloning (APC) approach based on image-based data augmentation to improve the invariance of the learned policy. The proposed method is based on policy cloning, where the goal is to learn a policy that can be used in a policy 3 cloning setting. The authors show that the proposed method can achieve better data-efficiency and transfer 12 of complex high-DoF behaviors. ","This paper proposes an augmented policy cloning (APC) approach based on image-based data augmentation to improve invariance to image perturbations. The paper proposes a data-augmentation technique that is based on behavioral cloning. The idea is to learn a policy 3 cloning setting in which the expert, student policy, and the student policy share the same expert trajectories. This setting consists of two problems: (1) learning a policy that is invariant to the perturbation of the expert and (2) learning an expert-aware offline policy augmentation approach to improve feedback-sensitivity. The proposed method is evaluated on transfer 12 of complex high-DoF behaviors and shows improved data-efficiency over existing algorithms for policy cloning."
11376,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,benchmarks CONJUNCTION simulated robotics environment. simulated robotics environment CONJUNCTION benchmarks. simulated robotics environment EVALUATE-FOR framework. benchmarks EVALUATE-FOR framework. Task is computer vision settings. Method is deep networks. ,This paper proposes a new framework for computer vision settings. The framework is based on a combination of several benchmarks and a simulated robotics environment. The authors show that the proposed framework is able to achieve state-of-the-art performance on a variety of benchmarks. They also show that deep networks are able to perform well on these benchmarks.,This paper proposes a new framework for computer vision settings. The proposed framework is evaluated on several benchmarks and a simulated robotics environment. The authors show that the proposed framework outperforms state-of-the-art deep networks. 
11401,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"agents USED-FOR tasks. Reinforcement Learning ( RL ) USED-FOR agents. visual observations USED-FOR agents. visual observations USED-FOR tasks. data augmentation USED-FOR generalization. generalization FEATURE-OF RL. data augmentation USED-FOR off - policy RL algorithms. data augmentation USED-FOR instability. technique USED-FOR algorithms. ConvNets CONJUNCTION Vision Transformers ( ViT ). Vision Transformers ( ViT ) CONJUNCTION ConvNets. benchmarks CONJUNCTION robotic manipulation tasks. robotic manipulation tasks CONJUNCTION benchmarks. benchmarks EVALUATE-FOR Vision Transformers ( ViT ). benchmarks EVALUATE-FOR image - based RL. DeepMind Control Suite USED-FOR benchmarks. Vision Transformers ( ViT ) USED-FOR image - based RL. ConvNets USED-FOR image - based RL. state - of - the - art methods USED-FOR image - based RL. stability CONJUNCTION sample efficiency. sample efficiency CONJUNCTION stability. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. generalization EVALUATE-FOR state - of - the - art methods. stability FEATURE-OF ConvNets. sample efficiency FEATURE-OF ConvNets. method USED-FOR ConvNets. augmentation FEATURE-OF ConvNets. sample efficiency EVALUATE-FOR method. stability EVALUATE-FOR method. generalization EVALUATE-FOR method. method USED-FOR RL. ViT - based architectures USED-FOR method. ViT - based architectures USED-FOR RL. OtherScientificTerm are divergence, and high - variance Q - targets. Generic is problems. ","This paper studies Reinforcement Learning (RL) for agents trained with visual observations for tasks based on visual observations. The authors propose a technique to improve the generalization of off-policy RL algorithms using data augmentation to reduce instability and improve the stability of convolutional neural networks (convNets). The authors show that the proposed method improves the stability and sample efficiency of ConvNets and Vision Transformers (ViT) for image-based RL on several benchmarks from the DeepMind Control Suite. The proposed method also improves the performance of RL on ViT-based architectures for RL. The method is shown to be more stable than state-of-the-art methods in terms of stability, sample efficiency, and generalization. ","This paper proposes Reinforcement Learning (RL) for learning agents from visual observations for tasks based on visual observations. The authors propose a technique to improve the generalization of existing algorithms using data augmentation for off-policy RL algorithms. The main contribution of the paper is that the proposed method is able to generalize better than existing state-of-the-art methods for image-based RL using ConvNets and Vision Transformers (ViT) on a variety of benchmarks (DeepMind Control Suite) and robotic manipulation tasks. The proposed method outperforms the state of the art methods in terms of stability, sample efficiency, and generalization in RL. The method also outperforms ViT-based architectures for RL. "
11426,SP:f8ca9d92c45adc4512381035856b445029e3080a,"local data USED-FOR joint model. minibatch sizes CONJUNCTION number of local updates. number of local updates CONJUNCTION minibatch sizes. WNs ’ and the server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and the server ’s update directions. communication rounds USED-FOR WNs. WNs USED-FOR local updates. algorithm USED-FOR ✏ -stationary solution. Õ ( ✏ 3/2 ) samples CONJUNCTION Õ ( ✏ 1 ) communication rounds. Õ ( ✏ 1 ) communication rounds CONJUNCTION Õ ( ✏ 3/2 ) samples. stochastic momentum estimator USED-FOR WN ’s and the server ’s directions. Õ ( ✏ 1 ) communication rounds USED-FOR algorithm. Õ ( ✏ 3/2 ) samples USED-FOR algorithm. near - optimal sample and communication complexities EVALUATE-FOR FL algorithm. WNs ’ and server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and server ’s update directions. Method are Federated Learning ( FL ), stochastic algorithms, and FL algorithms. Task is non - convex FL problem. Metric is sample and communication complexities. OtherScientificTerm is trade - off curve. ","This paper studies Federated Learning (FL), a non-convex FL problem where the goal is to learn a joint model from local data. The authors propose a new algorithm for the ✏-stationary solution. The FL algorithm is based on a stochastic momentum estimator, where the WNs’ and the server’s update directions, minibatch sizes, and number of local updates depend on the number of WNs. The WNs are trained with communication rounds to learn the local updates. The algorithm is evaluated on a variety of datasets, and is shown to have near-optimal sample and communication complexities. ","This paper studies Federated Learning (FL), a non-convex FL problem where the joint model is trained on local data. The authors propose a new algorithm for the ✏-stationary solution. The FL algorithms are based on stochastic algorithms, where the WNs’ and the server’s update directions and minibatch sizes and number of local updates are computed using communication rounds.  The authors show that the FL algorithm has near-optimal sample and communication complexities with respect to the trade-off curve. The main contribution of the paper is the proposed algorithm, which is based on the use of two different types of communication rounds for WNs for local updates and for the server ’s directions. The algorithm is evaluated on a number of different datasets, including a set of Õ (✏ 3/2) samples and two different kinds of  communication rounds, and the authors show the FL algorithms outperform the state-of-the-art. "
11451,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"information - theoretical framework USED-FOR non - vacuous generalization bounds. non - vacuous generalization bounds USED-FOR large models. isotropic noise USED-FOR Stochastic Gradient Langevin Dynamics ( SGLD ). Stochastic Gradient Langevin Dynamics ( SGLD ) USED-FOR large models. noise structure USED-FOR SGLD. noise structure USED-FOR information - theoretical generalization bound. expected gradient covariance USED-FOR optimal noise covariance. optimal noise COMPARE empirical gradient covariance. empirical gradient covariance COMPARE optimal noise. information - theoretical bound USED-FOR optimization analysis. matrix analysis USED-FOR optimal noise covariance. OtherScientificTerm are constraint, and prior. ","This paper proposes a new information-theoretic framework for non-vacuous generalization bounds for large models with isotropic noise. Stochastic Gradient Langevin Dynamics (SGLD) is a well-studied family of large models, and the noise structure of SGLD is well-known. The authors show that the expected gradient covariance of the optimal noise covariance can be computed using matrix analysis, and that the information-trivial generalization bound for optimization analysis can be derived from this information. ",This paper proposes an information-theoretic framework for non-vacuous generalization bounds for large models with isotropic noise. Stochastic Gradient Langevin Dynamics (SGLD) is used to define large models. The noise structure of SGLD is modeled as the expected gradient covariance between the optimal noise covariance and the prior. The authors show that optimal noise can be approximated by a matrix analysis. They also show that the information-trivial generalization bound can be used for optimization analysis.
11476,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,Learned video compression methods COMPARE video codecs. video codecs COMPARE Learned video compression methods. prediction mode CONJUNCTION fixed network framework. fixed network framework CONJUNCTION prediction mode. prediction mode USED-FOR learned video compression schemes. fixed network framework USED-FOR learned video compression schemes. model USED-FOR prediction modes. 3D motion vector fields USED-FOR weighted 9 trilinear warping. voxel flows USED-FOR weighted 9 trilinear warping. spatial - temporal space FEATURE-OF weighted 9 trilinear warping. 3D motion vector fields USED-FOR motion compensation 8 module. motion compensation 8 module USED-FOR versatile compression. voxel flows HYPONYM-OF 3D motion vector fields. temporal reference position FEATURE-OF voxel flows. flow prediction module USED-FOR motion trajectories. flow prediction module USED-FOR multiple - reference - frame predic12 tion. unified polynomial function USED-FOR flow prediction module. unified polynomial function USED-FOR motion trajectories. flow prediction module USED-FOR voxel flows. VLVC USED-FOR versatile compression. VLVC COMPARE Versatile Video Coding 17 ( VVC ) standard. Versatile Video Coding 17 ( VVC ) standard COMPARE VLVC. R - D performance EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. MS - SSIM EVALUATE-FOR R - D performance. R - D performance EVALUATE-FOR VLVC. MS - SSIM EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. OtherScientificTerm is inter prediction modes. ,"This paper proposes a new model for learning prediction modes for video compression. The model is based on a combination of a prediction mode and a fixed network framework. The prediction mode is used to train learned video compression schemes. The motion compensation 8 module is used for versatile compression, and the 3D motion vector fields are used for weighted 9 trilinear warping in spatial-temporal space. The voxel flows are used to represent the temporal reference position of the voxell flows. The flow prediction module predicts the motion trajectories from the unified polynomial function. The VLVC achieves R-D performance on MS-SSIM compared to the Versatile Video Coding 17 (VVC) standard. ","This paper proposes a new model for learning different prediction modes for video codecs. The proposed prediction mode is based on a fixed network framework. The authors show that weighted 9 trilinear warping in spatial-temporal space can be solved using 3D motion vector fields (e.g., voxel flows). The authors also propose a motion compensation 8 module for versatile compression based on the flow prediction module for motion trajectories in the temporal reference position. Experiments on MS-SSIM show that VLVC outperforms the Versatile Video Coding 17 (VVC) standard in terms of R-D performance. "
11501,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"regret EVALUATE-FOR MT - OMD. geometry FEATURE-OF regularizer. geometry FEATURE-OF task 3 variance. OMDs USED-FOR √ NT bound. Online Gradient Descent CONJUNCTION Exponentiated 7 Gradient. Exponentiated 7 Gradient CONJUNCTION Online Gradient Descent. Exponentiated 7 Gradient HYPONYM-OF OMD. Method are Online Mirror 1 Descent ( OMD ), and closed - form updates. OtherScientificTerm are time horizon, and σ. Generic is them. Material is real - world datasets. ","This paper studies Online Mirror 1 Descent (OMD), a regularizer with geometry in the task 3 variance. OMDs are well-studied in the literature and have been shown to converge to the √ NT bound. The main contribution of this paper is to study them in the context of real-world datasets. The authors show that Online Gradient Descent and Exponentiated 7 Gradient are OMD with closed-form updates. ","This paper proposes Online Mirror 1 Descent (OMD), a regularizer with geometry in the task 3 variance. Online Gradient Descent and Exponentiated 7 Gradient are two variants of OMDs, and both of them are shown to converge to the √ NT bound with respect to the time horizon. The authors also show that MT-OMD is equivalent to OMD with closed-form updates. The experiments are conducted on real-world datasets."
11526,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,ε - error USED-FOR approximating d - dimensional ULD. finite summation of N smooth components PART-OF stronglyconvex potential. stronglyconvex potential USED-FOR underdamped Langevin diffusion ( ULD ). gradient evaluations USED-FOR discretization method. method USED-FOR strongly - log - concave distribution. gradient complexity EVALUATE-FOR gradient based sampling algorithms. method COMPARE gradient based sampling algorithms. gradient based sampling algorithms COMPARE method. gradient complexity EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. method COMPARE ULD approaches. ULD approaches COMPARE method. synthetic and real - world data EVALUATE-FOR ULD approaches. ,This paper proposes a new discretization method for underdamped Langevin diffusion (ULD) based on the stronglyconvex potential of a finite summation of N smooth components. The proposed method is based on discretizing the strongly-log-concave distribution. The authors show that the proposed method has a lower gradient complexity than gradient based sampling algorithms. They also show that their method outperforms existing ULD approaches on both synthetic and real-world data.,The paper proposes a new method for approximating d-dimensional ULD with a finite summation of N smooth components. The idea is to use the stronglyconvex potential of underdamped Langevin diffusion (ULD) to approximate the strongly-log-concave distribution. The discretization method is based on gradient evaluations. The method is evaluated on both synthetic and real-world data. The authors show that the proposed method has better gradient complexity compared to gradient based sampling algorithms. 
11551,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"recurrent networks USED-FOR neural dynamics. neural dynamics USED-FOR biological continual learning. feedforward and recurrent neural networks EVALUATE-FOR methods. weight regularization CONJUNCTION projected gradient descent. projected gradient descent CONJUNCTION weight regularization. projected gradient descent PART-OF method. weight regularization PART-OF method. catastrophic forgetting FEATURE-OF optimization. prior precision USED-FOR catastrophic forgetting. gradient projection USED-FOR NCL. prior precision USED-FOR gradient projection. Bayesian weight regularization USED-FOR NCL. projection based approaches USED-FOR continual learning problems. weight regularization techniques CONJUNCTION projection based approaches. projection based approaches CONJUNCTION weight regularization techniques. method USED-FOR continual learning problems. method COMPARE weight regularization techniques. weight regularization techniques COMPARE method. method COMPARE projection based approaches. projection based approaches COMPARE method. feedforward and recurrent networks USED-FOR continual learning problems. networks USED-FOR task - specific dynamics. Method are Biological agents, artificial agents, specific parameter regularizers, and Natural Continual Learning ( NCL ). OtherScientificTerm are parameter space, gradients, and biological circuits. Task is optimization journey. ","This paper studies the problem of continual learning in biological continual learning with recurrent neural networks. The authors propose a method that combines weight regularization, projected gradient descent, and gradient projection to improve the performance of NCL. The proposed method is evaluated on a variety of continuous learning problems with feedforward and recurrent networks and shows that the proposed method outperforms the state-of-the-art.","This paper proposes a novel method for learning biological continual learning with recurrent networks. The method is based on a combination of weight regularization, projected gradient descent, and prior precision to avoid catastrophic forgetting in the optimization. The authors show that the proposed method outperforms previous methods on both feedforward and recurrent neural networks for continual learning problems with task-specific dynamics. The main contribution of the paper is the introduction of specific parameter regularizers to Natural Continual Learning (NCL) and the optimization journey. The paper also provides a theoretical analysis of the parameter space and the gradients of the biological circuits. The proposed method is evaluated on a variety of continuous learning problems and compared to other projection based approaches. The results show that NCL is more robust to catastrophic forgetting than prior precision for gradient projection. "
11576,SP:26de056be14962312c759be5d284ef235d660f9c,"Normalizing flows HYPONYM-OF invertible neural networks. change - of - volume terms FEATURE-OF invertible neural networks. low - dimensional manifold PART-OF high - dimensional ambient space. heuristics USED-FOR approaches. heuristics USED-FOR term. methods USED-FOR gradient. automatic differentiation CONJUNCTION numerical linear algebra. numerical linear algebra CONJUNCTION automatic differentiation. gradient FEATURE-OF term. automatic differentiation USED-FOR methods. approaches USED-FOR end - to - end nonlinear manifold learning. manifolds CONJUNCTION distributions. distributions CONJUNCTION manifolds. Method is maximum likelihood. OtherScientificTerm are modelling mismatch, invertibility requirement, Injective flows, lowto high - dimensional spaces, and volume - change term. Generic are manifold, and model. Task is out - of - distribution detection. ","This paper studies the problem of out-of-distribution detection in invertible neural networks with normalizing flows. Invertible networks are defined as networks that have a low-dimensional manifold in the high-dimensional ambient space. The manifold is defined as a set of points in the ambient space, and the invertibility requirement is that the model is invariant to model mismatch. The authors propose a new term called ""volume-change term"" to describe the difference between the two manifolds and the distributions. The new term is based on heuristics, and they show that the gradient of the new term can be computed using automatic differentiation and numerical linear algebra. They also show that this new term leads to better performance than existing approaches for end-to-end nonlinear manifold learning. ","This paper studies the problem of out-of-distribution detection in the setting of invertible neural networks. The authors propose Normalizing flows (Normalizing flows), which is an extension of the invertibility requirement in the context of Invertible Neural Networks. The main idea is to define a low-dimensional manifold in the high-dimensional ambient space, where the modelling mismatch between the output of the model and the input manifold is minimized. Injective flows are defined in the lowto high-dimensions spaces, and a volume-change term is defined for the manifold. The proposed method is based on heuristics, and the authors show that the proposed term can be decomposed into two terms: (1) automatic differentiation and (2) numerical linear algebra. The gradient of the term is computed as the sum of the gradient of both methods. The paper also provides a theoretical analysis of the maximum likelihood of the proposed method. Experiments are performed on a variety of manifolds and distributions. The results show the effectiveness of the approaches for end-to-end nonlinear manifold learning."
11601,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"response time FEATURE-OF physical computational elements. inference CONJUNCTION learning. learning CONJUNCTION inference. framework USED-FOR inference. framework USED-FOR learning. inference CONJUNCTION learning. learning CONJUNCTION inference. networks of slow components USED-FOR learning. principle USED-FOR quasi - instantaneous inference. phased plasticity CONJUNCTION network relaxation phases. network relaxation phases CONJUNCTION phased plasticity. prospective energy function USED-FOR disentangled neuron and synapse dynamics. continuous - time, leaky neuronal dynamics CONJUNCTION continuously active, local plasticity. continuously active, local plasticity CONJUNCTION continuous - time, leaky neuronal dynamics. error backpropagation USED-FOR deep cortical networks. continuous - time, leaky neuronal dynamics USED-FOR error backpropagation. benchmark datasets EVALUATE-FOR learning. fully - connected and convolutional architectures USED-FOR learning. robustness EVALUATE-FOR model. model USED-FOR physical realization. spatio - temporal substrate imperfections FEATURE-OF robustness. spatio - temporal substrate imperfections FEATURE-OF model. OtherScientificTerm are neurons, response lag, delayed processing of stimuli, timing mismatch, instructive signals, biological neurons, membrane potential, and network depth. Method are hierarchical models of cortical networks, physical dynamical systems, and Latent Equilibrium. ","This paper studies the problem of learning hierarchical models of cortical networks. The authors propose a framework to combine inference and learning with networks of slow components in order to reduce the response time of physical computational elements in response time. The key idea is to use the principle of quasi-instantaneous inference, where the neurons have a response lag between the input and the output, and the response lag is a delayed processing of stimuli.  The authors show that this principle can be applied to quasi-imagenetic inference in the context of deep cortical networks with continuous-time, leaky neuronal dynamics, and network relaxation phases. In particular, they show that the disentangled neuron and synapse dynamics with a prospective energy function can be approximated by a simple yet effective model. The model is also shown to be robust to spatio-temporal substrate imperfections in terms of the robustness of the model to physical realization. Finally, the model is shown to perform well on benchmark datasets.  ","The paper proposes a framework for learning in physical computational elements with response time with respect to physical dynamical systems. The framework is based on hierarchical models of cortical networks. The authors propose a principle for quasi-instantaneous inference with networks of slow components. The key idea is to use a prospective energy function to model disentangled neuron and synapse dynamics. The paper shows that the proposed model is robust to spatio-temporal substrate imperfections and robustness to physical realization. The model is evaluated on two benchmark datasets and shows that it is able to learn deep cortical networks with continuous-time, leaky neuronal dynamics and continuous active, local plasticity. "
11626,SP:b937901e3230b14e36975fbab0658a52bdac4977,"Graph neural network ( GNN ) USED-FOR graph classification. node representation USED-FOR rooted subtree. GNN USED-FOR node representation. 1 - WL USED-FOR node representation. 1 - WL CONJUNCTION GNN. GNN CONJUNCTION 1 - WL. representation USED-FOR graph. rooted subtree representations PART-OF representation. rooted subtrees USED-FOR nontree graph. NGNN USED-FOR graph. rooted subgraphs COMPARE rooted subtrees. rooted subtrees COMPARE rooted subgraphs. rooted subgraphs FEATURE-OF graph. GNN USED-FOR subgraph representation. NGNN USED-FOR subgraph representation. GNN USED-FOR subgraph. NGNN USED-FOR local subgraph. GNN USED-FOR NGNN. subgraph representations USED-FOR whole - graph representation. NGNN COMPARE 1 - WL. 1 - WL COMPARE NGNN. NGNN USED-FOR r - regular graphs. NGNN COMPARE GNNs. GNNs COMPARE NGNN. GNNs COMPARE NGNN. NGNN COMPARE GNNs. time complexity EVALUATE-FOR GNNs. time complexity EVALUATE-FOR NGNN. NGNN HYPONYM-OF plug - and - play framework. plug - and - play framework CONJUNCTION base GNNs. base GNNs CONJUNCTION plug - and - play framework. NGNN CONJUNCTION base GNNs. base GNNs CONJUNCTION NGNN. NGNN COMPARE base GNNs. base GNNs COMPARE NGNN. benchmark datasets EVALUATE-FOR base GNNs. benchmark datasets EVALUATE-FOR NGNN. OtherScientificTerm are neighboring node features, subtrees, and subtree. Method is Nested Graph Neural Networks ( NGNNs ). ","This paper proposes a new graph neural network (GNN) for graph classification. The proposed Nested Graph Neural Networks (NGNNs) are based on the 1-WL. The main idea is to learn a node representation for a rooted subtree and a subgraph representation for each node in the subgraph. The subgraph representations are then used to represent the whole-graph representation. The GNN is trained to predict the node representation of each node. The representation is then used as a proxy for the whole graph. The root subgraphs of the graph are then mapped to the nodes in the subtree.   The authors show that the root subtree representations of a nontree graph can be used to learn the representation of a GNN.  The main contribution of the paper is to show that rooted subtrees can be learned to represent a graph as a set of nodes in a subtree, and that the representation can be combined with the GNN to generate a sub-graph. ","Graph neural network (GNN) is an important tool for graph classification. GNN is used to learn a node representation for each node in a graph, and a subgraph representation for the neighboring node features. The representation for a graph is a combination of the node representation and a rooted subtree representation. The root subgraphs of a graph can be represented as a subset of the original graph, or as a nontree graph with rooted subtrees. The authors propose a plug-and-play framework for learning the subgraph representations for the whole-graph representation. They show that the root subtree representations of the representation can be used to train a GNN to learn the local subgraph. They evaluate the proposed Nested Graph Neural Networks (NGNNs) on several benchmark datasets, and show that NGNN outperforms base GNNs and 1-WL in terms of time complexity. "
11651,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"nesting FEATURE-OF forward or reverse KL divergence. NVI USED-FOR importance sampling strategies. heuristics USED-FOR sampler. NVI USED-FOR intermediate densities. NVI USED-FOR multimodal distribution. amortized inference USED-FOR hierarchical deep generative models. heuristics USED-FOR amortized inference. heuristics USED-FOR hidden Markov model. learned annealing path USED-FOR NVI. log average weight CONJUNCTION effective sample size. effective sample size CONJUNCTION log average weight. log average weight FEATURE-OF sample quality. effective sample size FEATURE-OF sample quality. sample quality EVALUATE-FOR nested objectives. Method are nested variational inference ( NVI ), and nested importance samplers. ",This paper studies the problem of nested variational inference (NVI) in hierarchical deep generative models. NVI is an important technique for importance sampling strategies. The authors propose to use NVI to learn intermediate densities of the multimodal distribution by using heuristics to train a sampler. The learned annealing path is then used to train the hidden Markov model. The amortized inference is then applied to perform the NVI. The sample quality of the nested objectives is evaluated using log average weight and effective sample size. ,"This paper proposes a novel approach to learning importance sampling strategies for hierarchical deep generative models. The main idea is to use nested variational inference (NVI) to learn the forward or reverse KL divergence in the multimodal distribution. NVI can be used to learn importance samplers. The authors propose two heuristics for learning the sampler. First, they use a learned annealing path to learn a hidden Markov model. Second, they apply amortized inference to amortize inference. They show that the log average weight and effective sample size of the NVI are the best measures of sample quality for nested objectives."
11676,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"packing bound USED-FOR Piyavskii - Shubert algorithm. packing bound USED-FOR upper bound. local worst - case analysis USED-FOR learning tasks. instance - dependent lower bound COMPARE worst - case lower bounds. worst - case lower bounds COMPARE instance - dependent lower bound. Lipschitz setting FEATURE-OF worst - case lower bounds. local worst - case analysis USED-FOR instance - dependent lower bound. OtherScientificTerm is Lipschitz function f. Metric is optimal sample complexity. Method are computationally tractable DOO algorithm, and packing and integral bounds. ",This paper proposes a packing bound for the Piyavskii-Shubert algorithm. The packing bound is based on the upper bound on the Lipschitz function f. The authors show that the optimal sample complexity of the computationally tractable DOO algorithm is bounded by the packing and integral bounds. The paper also shows that the instance-dependent lower bound is better than the worst-case lower bounds in the LPschitz setting. ,"The paper proposes a packing bound for the Piyavskii-Shubert algorithm. The packing bound is the upper bound of the Lipschitz function f, where f is the optimal sample complexity. The authors show that the instance-dependent lower bound is better than the worst-case lower bounds for learning tasks. The paper also shows that the packing and integral bounds are better than those in the Lopschitz setting."
11701,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"Deep neural networks ( DNNs ) USED-FOR tasks. Credible uncertainty estimation USED-FOR risk - sensitive applications. network USED-FOR uncertainty estimation. attack COMPARE adversarial attacks. adversarial attacks COMPARE attack. white - box setting USED-FOR scenario. Deep Ensembles CONJUNCTION MC - Dropout. MC - Dropout CONJUNCTION Deep Ensembles. vanilla softmax score CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION vanilla softmax score. uncertainty estimation methods USED-FOR attacks. vanilla softmax score HYPONYM-OF uncertainty estimation methods. MC - Dropout HYPONYM-OF uncertainty estimation methods. Deep Ensembles HYPONYM-OF uncertainty estimation methods. SelectiveNet CONJUNCTION selective classification architecture. selective classification architecture CONJUNCTION SelectiveNet. SelectiveNet USED-FOR attack. MobileNetV2 CONJUNCTION EfficientNetB0. EfficientNetB0 CONJUNCTION MobileNetV2. architectures EVALUATE-FOR attack. EfficientNetB0 HYPONYM-OF architectures. MobileNetV2 HYPONYM-OF architectures. OtherScientificTerm are network ’s capacity, and uncertainty estimation damage. Method is DNN. Material are black - box regime, and ImageNet. Task is uncertainty estimations. ","This paper studies the problem of uncertainty estimation in deep neural networks (DNNs) for tasks where the network’s capacity is limited. Credible uncertainty estimation is important for risk-sensitive applications. The authors consider the black-box regime, where the model is trained in a white-box setting, and the goal is to minimize the uncertainty estimation damage. In this scenario, uncertainty estimation methods such as vanilla softmax score, Deep Ensembles, MC-Dropout, and SelectiveNet can be used to improve the performance of a DNN.  The authors propose a new attack on ImageNet, where they attack the network using uncertainty estimation from the network. They show that the attack performs better than adversarial attacks. They also show that different architectures such as MobileNetV2, EfficientNetB0, and a selective classification architecture can be targeted by the attack.","The paper proposes a new attack on deep neural networks (DNNs) for tasks where the network’s capacity is limited. The authors propose to use Credible uncertainty estimation for risk-sensitive applications. The attack is based on adversarial attacks on the network. The paper presents a white-box setting in which the scenario is similar to the black-box regime, but the network is trained on ImageNet. The attacks are based on uncertainty estimation methods such as vanilla softmax score, Deep Ensembles, MC-Dropout, and SelectiveNet. Experiments are conducted on three architectures: MobileNetV2, EfficientNetB0, and a selective classification architecture. "
11726,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"stochastic block models USED-FOR community detection. network information USED-FOR they. model USED-FOR networks. Task are community detection problem, and real - world applications. OtherScientificTerm are network, well - connected ‘ communities ’, and network structure. Method are detection algorithm, voting approaches, streaming stochastic block model ( StSBM ), voting algorithms, and streaming belief - propagation ( STREAMBP ) approach. Material is synthetic and real data. ","This paper studies the community detection problem with stochastic block models. The authors propose a new detection algorithm, STREAMBP, which is based on streaming belief-propagation (STREAMBP) approach. The main idea is to use the network information from the voting algorithms to train the model to identify the networks that are “well-connected” to the network. The network structure is then used as a proxy for the network’s “community”. The proposed method is evaluated on both synthetic and real data.","This paper proposes a new method for community detection based on stochastic block models. The authors propose a new detection algorithm based on voting approaches. The proposed method is based on the streaming stochedastic block model (StSBM) and voting algorithms. The main idea of the proposed model is to learn networks with well-connected ‘communities’, where each community is represented as a set of ‘well-connected‘ communities’. The network information is used to predict which communities belong to which community, and they are then used to train a model to predict the networks that belong to each community. Experiments on synthetic and real data show that the proposed streaming belief-propagation (STREAMBP) approach outperforms other methods in both real-world applications."
11751,SP:b1163857a6b06047c3531ab762642fcbed6dd294,predictor space FEATURE-OF regularization cost. l2 regularization USED-FOR regularization cost. l2 regularization USED-FOR predictor space. linear neural networks USED-FOR parameterizations of linear predictors. sparse linear ConvNets CONJUNCTION residual networks. residual networks CONJUNCTION sparse linear ConvNets. representation cost FEATURE-OF sparse linear ConvNets. representation cost FEATURE-OF residual networks. lp quasi - norms CONJUNCTION group quasi - norms. group quasi - norms CONJUNCTION lp quasi - norms. architecture CONJUNCTION parameterization. parameterization CONJUNCTION architecture. group quasi - norms CONJUNCTION k - support - norm. k - support - norm CONJUNCTION group quasi - norms. k - support - norm CONJUNCTION elastic net. elastic net CONJUNCTION k - support - norm. parameterization USED-FOR representation cost. regularizers USED-FOR linear predictors. architecture USED-FOR representation cost. group quasi - norms CONJUNCTION elastic net. elastic net CONJUNCTION group quasi - norms. l2 regularization USED-FOR representation cost. elastic net HYPONYM-OF regularizers. k - support - norm HYPONYM-OF regularizers. group quasi - norms HYPONYM-OF regularizers. lp quasi - norms HYPONYM-OF regularizers. Method is parameterizations. Task is reverse problem. ,"This paper studies the problem of regularizing linear neural networks with parameterizations of linear predictors. The authors consider the reverse problem, where the parameterizations are learned by a linear neural network. The regularization cost is computed by l2 regularization on the predictor space of the predicted model. The paper shows that the representation cost of sparse linear ConvNets and residual networks with the same representation cost is the same as that of residual networks without the parameterization.  The authors then propose two regularizers, the elastic net and the group quasi-norm. The architecture and parameterization of the proposed regularizers are shown to be able to achieve a similar representation cost. ","This paper studies the regularization cost in the predictor space of linear neural networks. The authors consider the problem of learning parameterizations of linear predictors in the context of the reverse problem. They show that the representation cost of sparse linear ConvNets and residual networks with l2 regularization in the predictor space is proportional to the number of parameters. They propose a new architecture and parameterization to reduce this representation cost. They also propose a number of regularizers to improve the performance of the regular predictors, including group quasi-norms, k-support-norm, and elastic net."
11776,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"approach USED-FOR knowledge graphs ( KGs ). approach COMPARE case - based reasoning. case - based reasoning COMPARE approach. case - based reasoning USED-FOR artificial intelligence ( AI ). non - parametric approach USED-FOR crisp logical rules. graph path patterns USED-FOR non - parametric approach. NELL-995 CONJUNCTION FB-122. FB-122 CONJUNCTION NELL-995. accuracy EVALUATE-FOR method. NELL-995 EVALUATE-FOR models. FB-122 EVALUATE-FOR models. model USED-FOR low data settings. OtherScientificTerm are binary relation, and relation. ","This paper proposes a novel approach to learn knowledge graphs (KGs) from graph path patterns. The authors propose a non-parametric approach to generate crisp logical rules that can be used in case-based reasoning for artificial intelligence (AI). The key idea is to learn a binary relation between two nodes in the graph, and then use this relation to train a model for low data settings. The proposed method achieves state-of-the-art accuracy on NELL-995 and FB-122.","This paper proposes a novel approach to learning knowledge graphs (KGs) from graph path patterns. The authors propose a non-parametric approach to learn crisp logical rules based on graph path paths. The proposed approach is compared to case-based reasoning in the context of artificial intelligence (AI). The authors show that the proposed method improves the accuracy of the proposed model in low data settings, and is shown to outperform existing models such as NELL-995 and FB-122. "
11780,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,Transformer architecture PART-OF entity linking model. CoNLL CONJUNCTION TAC - KBP. TAC - KBP CONJUNCTION CoNLL. entity linking datasets EVALUATE-FOR model. Transformer architecture CONJUNCTION input perturbations. input perturbations CONJUNCTION Transformer architecture. negative entity candidates CONJUNCTION Transformer architecture. Transformer architecture CONJUNCTION negative entity candidates. end - to - end entity linking CONJUNCTION entity linking. entity linking CONJUNCTION end - to - end entity linking. entity linking HYPONYM-OF settings. end - to - end entity linking HYPONYM-OF settings. in - domain training data USED-FOR settings. Material is Wikipedia links. ,"This paper proposes a Transformer architecture for entity linking model. The proposed model is evaluated on a variety of entity linking datasets. The authors show that the proposed model performs better than CoNLL and TAC-KBP.  The authors also show that in-domain training data can be used to improve the performance of the proposed settings: end-to-end entity linking, entity linking with negative entity candidates, and entity linking without Wikipedia links.","This paper proposes a Transformer architecture for the entity linking model. The model is evaluated on several entity linking datasets, including CoNLL, TAC-KBP, and Wikipedia links. The authors show the effectiveness of the proposed model in three settings: end-to-end entity linking, entity linking with negative entity candidates, and in-domain training data."
11784,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"ensemble Active Learning methods USED-FOR acquisition. intermediate training checkpoints USED-FOR ensembles. training data subset search USED-FOR large labeled datasets. acquisition functions CONJUNCTION ensemble configurations. ensemble configurations CONJUNCTION acquisition functions. initialization schemes CONJUNCTION acquisition functions. acquisition functions CONJUNCTION initialization schemes. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. ResNet-101 CONJUNCTION DenseNet121. DenseNet121 CONJUNCTION ResNet-101. data subsets USED-FOR deep models. ResNet-18 ensemble USED-FOR data subsets. DenseNet121 HYPONYM-OF deep models. ResNet-101 HYPONYM-OF deep models. training data distribution USED-FOR large scale vision tasks. Method are Deep Neural Networks ( DNNs ), and DNNs. Generic are datasets, they, approach, and dataset. Task is DNN ’s optimization. OtherScientificTerm is training distribution. Metric is training time. ","This paper studies ensemble Active Learning methods for acquisition in the context of large-scale image classification tasks. The authors propose a new approach to learn ensembles with intermediate training checkpoints, where the training data subset search is applied to large labeled datasets. The proposed approach is based on the notion of “DNN’s optimization”, which is a generalization of Deep Neural Networks (DNNs), where the goal is to learn a training distribution that is close to the training distribution of the original training data distribution. The key idea is to use ensemble configurations, acquisition functions, and initialization schemes to learn the acquisition functions and ensemble configurations. The paper shows that the proposed approach can achieve state-of-the-art performance on several image classification benchmarks, including CIFAR-10, CifAR-100, ImageNet, and ResNet-101, and DenseNet121. ","This paper presents ensemble Active Learning methods for acquisition of ensembles from intermediate training checkpoints for large labeled datasets. The authors propose a new approach to improve the DNN’s optimization. The key idea is to use ensemble configurations and initialization schemes to optimize the acquisition functions and ensemble configurations. They show that they can improve the training time of DNNs. They evaluate their approach on several image classification benchmarks, including CIFAR-10, Cifar-100, and ImageNet. They also show that the training data subsets of deep models trained on the ResNet-18 ensemble can be used to train deep models on the training distribution for large scale vision tasks."
11788,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"routing algorithm USED-FOR capsule networks. mechanism USED-FOR routing. sequential iterative routing CONJUNCTION concurrent iterative routing. concurrent iterative routing CONJUNCTION sequential iterative routing. inverted dot - product attention USED-FOR routing. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. routing algorithms COMPARE method. method COMPARE routing algorithms. method COMPARE CNN. CNN COMPARE method. it COMPARE CNN. CNN COMPARE it. method COMPARE it. it COMPARE method. CIFAR-10 HYPONYM-OF benchmark datasets. CIFAR-100 HYPONYM-OF benchmark datasets. benchmark datasets EVALUATE-FOR method. capsule model COMPARE CNNs. CNNs COMPARE capsule model. task EVALUATE-FOR CNNs. task EVALUATE-FOR capsule model. task EVALUATE-FOR recognizing digits. neurons per layer USED-FOR CNNs. capsule model USED-FOR recognizing digits. overlayed digit images USED-FOR task. overlayed digit images USED-FOR recognizing digits. capsule networks USED-FOR complex real - world tasks. Method are Layer Normalization, and Capsules - Inverted - Attention - Routing. OtherScientificTerm is normalization. ","This paper proposes Capsules-Inverted-Attention-Routing, a new routing algorithm for capsule networks. The proposed mechanism is based on the inverted dot-product attention, which is a common mechanism for routing. The authors show that the proposed method outperforms existing routing algorithms on several benchmark datasets, including CIFAR-10 and CifAR-100. They also show that it is able to perform better than CNN on the same task than CNNs with neurons per layer. Finally, they demonstrate that the capsule model can be used for recognizing digits from overlayed digit images for complex real-world tasks.","This paper proposes Capsules-Inverted-Attention-Routing, a new routing algorithm for capsule networks. The proposed mechanism is based on Layer Normalization, where each layer of the network is trained with a different normalization, and the routing is performed using inverted dot-product attention. The method is evaluated on three benchmark datasets (CIFAR-10, CifAR-100, and CIFAR10-100) and compared to other routing algorithms. The authors show that it outperforms CNN on the task of recognizing digits from overlayed digit images. They also show that the proposed capsule model outperforms other CNNs on the same task. "
11792,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"statistical physics FEATURE-OF parallel tempering technique. history PART-OF joint hyperparameter / model - parameter space. method USED-FOR dropout and learning rate optimization. Task are Hyperparameter optimization, and training of deep architectures. Method is deep architectures. Generic are methods, and model. OtherScientificTerm are hyperparameter space, nonlocal paths, hyperparameters, correlated noise, temperature, and overfitting. Metric are computational cost, resistance, and absolute validation error. ","This paper proposes a parallel tempering technique based on statistical physics. Hyperparameter optimization is an important problem in deep architectures. The authors propose a new method for dropout and learning rate optimization based on the history of the joint hyperparameter/model-parameter space. The proposed methods are based on two main contributions: (1) The authors show that hyperparameters can be represented as nonlocal paths, and (2) The proposed method is computationally efficient. ","The paper proposes a parallel tempering technique based on statistical physics. Hyperparameter optimization is an important problem in deep architectures. The authors propose a new method for dropout and learning rate optimization. The proposed method is based on the history of the joint hyperparameter/model-parameter space, where the hyperparameters are learned in nonlocal paths, and the model is trained in a nonlocal way. The paper provides a theoretical analysis of the computational cost, resistance, and absolute validation error of the proposed method. The method is evaluated on a variety of datasets, and is shown to outperform other methods in terms of computational cost and resistance. "
11796,SP:beba754d96cc441712a5413c41e98863c8abf605,"Minimum Risk Training ( MRT ) CONJUNCTION Generative Adversarial Networks ( GAN ). Generative Adversarial Networks ( GAN ) CONJUNCTION Minimum Risk Training ( MRT ). Reinforcement learning ( RL ) USED-FOR text generation tasks. machine translation ( MT ) HYPONYM-OF text generation tasks. methods USED-FOR MT. RL methods USED-FOR MT. OtherScientificTerm are expected reward, pre - trained parameters, translation, training signal, and distribution curve. ","This paper studies Reinforcement learning (RL) for text generation tasks such as machine translation (MT) and Generative Adversarial Networks (GAN). The authors consider the problem of maximizing the expected reward of a pre-trained parameters of a GAN. The authors show that MT can be achieved by a combination of two RL methods: Minimum Risk Training (MRT) and GAN, where the goal is to minimize the number of times the training signal is updated during the training. The main contribution of the paper is to provide a distribution curve for MT, which allows the authors to compare MT with other methods for MT.","This paper proposes Reinforcement learning (RL) for text generation tasks such as machine translation (MT) and Generative Adversarial Networks (GAN). The main idea is to learn the expected reward from the pre-trained parameters of the GAN. The authors propose two methods for MT: (1) Minimum Risk Training (MRT) and (2) Generalized Reinforcement Learning (RL). The authors show that MT can be learned using RL methods. The main contribution of the paper is to show that the translation, the training signal, and the distribution curve can be learnt."
11800,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,reinforcement learning algorithms CONJUNCTION applications. applications CONJUNCTION reinforcement learning algorithms. closed - form characterizations FEATURE-OF asymptotic variances. closed - form characterizations USED-FOR Q - value estimates. policies USED-FOR estimation errors. confidence regions USED-FOR Q - value and optimal value functions. exploration strategy COMPARE benchmark approaches. benchmark approaches COMPARE exploration strategy. Task is statistical inference. Method is policy exploration strategy. OtherScientificTerm is Q estimates. ,"This paper studies the problem of statistical inference with closed-form characterizations of asymptotic variances in the Q-value estimates. The authors propose a policy exploration strategy, where the Q estimates are obtained by learning policies that are close to the optimal value functions in confidence regions. The exploration strategy is shown to outperform other benchmark approaches in terms of exploration performance. ","This paper proposes a policy exploration strategy for statistical inference. The exploration strategy is based on closed-form characterizations of asymptotic variances of Q-value estimates, which are used in reinforcement learning algorithms and applications. The authors show that the Q estimates of the policies can be approximated by confidence regions of the Q-values and optimal value functions. They also show that this exploration strategy outperforms other benchmark approaches in terms of estimation errors."
11804,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"Top - k recommendation USED-FOR largescale recommender systems. Cold - start and efficiency issues USED-FOR largescale recommender systems. Cold - start and efficiency issues FEATURE-OF Top - k recommendation. hybrid recommendation methods USED-FOR cold - start issues. efficiency EVALUATE-FOR online recommendation. online recommendation EVALUATE-FOR they. real latent space FEATURE-OF similarity search. cold - start items CONJUNCTION warm - start ones. warm - start ones CONJUNCTION cold - start items. cold - start users CONJUNCTION cold - start items. cold - start items CONJUNCTION cold - start users. collaborative generated hashing ( CGH ) USED-FOR efficiency. it USED-FOR recommendation settings. CGH USED-FOR hash functions. Minimum Description Length ( MDL ) principle USED-FOR CGH. Minimum Description Length ( MDL ) principle USED-FOR hash functions. CGH USED-FOR marketing strategy. generative step USED-FOR CGH. MDL principle USED-FOR compact and informative binary codes. content data USED-FOR compact and informative binary codes. recommendations COMPARE baselines. baselines COMPARE recommendations. public datasets EVALUATE-FOR recommendations. application USED-FOR marketing. public datasets EVALUATE-FOR baselines. OtherScientificTerm are side information, and binary codes. ","This paper studies the problem of top-k recommendation in largescale recommender systems. Cold-start and efficiency issues arise in Top-K recommendation due to the lack of side information in the real latent space. The authors propose hybrid recommendation methods to address these cold-start issues. The efficiency of online recommendation is studied and they show that collaborative generated hashing (CGH) can improve efficiency by reducing the number of hash functions. CGH can also be used as a marketing strategy by using a generative step.  The authors also show that it can be used in recommendation settings where the binary codes are compact and informative. They show that using the Minimum Description Length (MDL) principle, CGH is able to learn hash functions that are more efficient than baselines on public datasets. ","This paper studies the Cold-start and efficiency issues of Top-k recommendation for largescale recommender systems. The authors propose two hybrid recommendation methods to address the cold-start issues. First, they propose collaborative generated hashing (CGH) to improve the efficiency of online recommendation. Second, they use the Minimum Description Length (MDL) principle to optimize hash functions in the real latent space for similarity search. The MDL principle is applied to compact and informative binary codes from the content data. The CGH is then used as a marketing strategy for the optimization of a generative step. The proposed application is evaluated on two public datasets and shows that the proposed recommendations outperform other baselines. "
11808,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"adversarial domain adaptation CONJUNCTION multi - task learning. multi - task learning CONJUNCTION adversarial domain adaptation. adversarial domain adaptation USED-FOR AITL. multi - task learning USED-FOR AITL. genomic information USED-FOR drug response. large pre - clinical pharmacogenomics datasets CONJUNCTION clinical datasets. clinical datasets CONJUNCTION large pre - clinical pharmacogenomics datasets. transfer learning USED-FOR large pre - clinical pharmacogenomics datasets. drug response outcome FEATURE-OF clinical data. cancer cell lines HYPONYM-OF large pre - clinical pharmacogenomics datasets. AITL USED-FOR input and output discrepancies. adversarial inductive transfer learning method USED-FOR input and output discrepancies. AITL HYPONYM-OF adversarial inductive transfer learning method. AITL USED-FOR precision oncology. AITL COMPARE pharmacogenomics and transfer learning baselines. pharmacogenomics and transfer learning baselines COMPARE AITL. Method is Adversarial Inductive Transfer Learning ( AITL ). Generic is method. OtherScientificTerm are input and output spaces, and output space. Task is pharmacogenomics. Material is pre - clinical and clinical datasets. ","This paper proposes Adversarial Inductive Transfer Learning (AITL), a method to transfer genomic information between input and output spaces. AITL is a combination of adversarial domain adaptation and multi-task learning. The proposed adversarial inductive transfer learning method is able to transfer input andoutput discrepancies between the input space and the output space. The authors demonstrate the effectiveness of the proposed method on large pre-clinical pharmacogenomics datasets such as cancer cell lines, as well as clinical datasets where the drug response outcome of clinical data is not available. The paper also shows that the proposed approach can improve precision oncology. ","This paper proposes Adversarial Inductive Transfer Learning (AITL), a method to improve the performance of adversarial domain adaptation and multi-task learning. AITL is based on the adversarial inductive transfer learning method, where the input and output spaces are the same and the output space is the same. The authors propose to use genomic information for drug response and transfer learning for large pre-clinical pharmacogenomics datasets (e.g., cancer cell lines) and clinical datasets (i.e., pre-clinically and in vitro). The authors show that the proposed method can improve the precision oncology. The proposed method is evaluated on a number of pre-Clinically- and clinical-data sets, and the results show that it is able to improve both the accuracy and the precision of the drug response outcome. "
11812,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,"sample complexity EVALUATE-FOR model - free approaches. fictitious trajectory rollouts USED-FOR dynamics model. fictitious trajectory rollouts USED-FOR model - free approaches. stochastic dynamics model USED-FOR uncertainty quantification. erroneously optimistic regions PART-OF dynamics model. uncertainty aware ensemble of dynamics models USED-FOR next state predictions. ensemble of dynamics models USED-FOR policy update. simulated robotic locomotion HYPONYM-OF benchmark tests. benchmark tests EVALUATE-FOR approach. approach COMPARE model - based one. model - based one COMPARE approach. approach COMPARE model - free algorithms. model - free algorithms COMPARE approach. model - free algorithms COMPARE model - based one. model - based one COMPARE model - free algorithms. learning rates CONJUNCTION asymptotic behaviour. asymptotic behaviour CONJUNCTION learning rates. asymptotic behaviour EVALUATE-FOR MBPGE. learning rates EVALUATE-FOR MBPGE. Method are RL methods, and policy gradient methods. OtherScientificTerm are next state prediction, and real and virtual total reward. ","This paper studies the problem of next state prediction in RL methods. The authors propose a stochastic dynamics model for uncertainty quantification of a dynamics model with fictitious trajectory rollouts. The uncertainty aware ensemble of dynamics models is used to make next state predictions for the dynamics model, and the authors show that the sample complexity of these model-free approaches is much lower than that of the model-based one. The proposed approach is evaluated on a variety of benchmark tests, including simulated robotic locomotion and asymptotic behaviour, and shows that the proposed approach has better learning rates compared to the model -based one, and is able to achieve better performance than the model free algorithms. ","This paper proposes a new approach to model-free approaches based on fictitious trajectory rollouts for learning a dynamics model with erroneously optimistic regions. The authors propose a stochastic dynamics model for uncertainty quantification of the next state prediction, which can be applied to RL methods. The key idea is to use an uncertainty aware ensemble of dynamics models to learn next state predictions for policy update. The approach is evaluated on two benchmark tests, simulated robotic locomotion and real and virtual total reward. The proposed approach is shown to outperform model-based one and other model free algorithms in terms of learning rates and asymptotic behaviour."
11816,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"neural network models USED-FOR visual features. adversarial examples CONJUNCTION corrupted images. corrupted images CONJUNCTION adversarial examples. class - conditional reconstruction of the input USED-FOR adversarial examples. class - conditional reconstruction of the input USED-FOR corrupted images. misclassification CONJUNCTION reconstruction error. reconstruction error CONJUNCTION misclassification. Reconstructive Attack USED-FOR detection mechanism. reconstructive attack USED-FOR undetected adversarial examples. success rate EVALUATE-FOR reconstructive attack. CapsNets COMPARE convolutional networks. convolutional networks COMPARE CapsNets. adversarial examples USED-FOR CapsNets. visual similarity USED-FOR reconstructive attack. features USED-FOR CapsNets. Material is Adversarial examples. Method is class - conditional reconstruction. Generic is attacks. OtherScientificTerm are perturbations, and human perception. ","This paper studies the problem of learning visual features from neural network models. The authors propose a new detection mechanism based on Reconstructive Attack, which uses a class-conditional reconstruction of the input to generate adversarial examples and corrupted images. They show that the reconstructive attack improves the success rate of the detection mechanism in terms of misclassification and reconstruction error. They also show that CapsNets with visual similarity are more robust than convolutional networks. ","This paper proposes a novel detection mechanism based on the Reconstructive Attack on visual features of neural network models. The authors propose a class-conditional reconstruction of the input to generate adversarial examples and corrupted images, which are then used as input for the class-conditioned reconstruction. The proposed attacks are based on perturbations to the input image, which is then used for the reconstruction error and misclassification. CapsNets are shown to outperform convolutional networks in terms of the success rate of the reconstructive attack. The paper also shows that the reconstructionive attack can be used to generate undetected adversarial example, and that the reconstructed images are similar to the original images. The main contribution of the paper is the use of visual similarity between images to improve human perception."
11820,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"gradient descent USED-FOR parameter space. function space FEATURE-OF kernel gradient descent. gradient descent USED-FOR neural network. linear model USED-FOR wide networks. linear model USED-FOR neural network. full batch gradient descent USED-FOR neural network. Edge of Chaos HYPONYM-OF initialization. initialization CONJUNCTION activation function. activation function CONJUNCTION initialization. activation function USED-FOR NTK. initialization USED-FOR NTK. OtherScientificTerm are Neural Tangent Kernel ( NTK ), and network depth. ","This paper studies the problem of kernel gradient descent in the function space of a neural network with a linear model. The authors propose a Neural Tangent Kernel (NTK) which is a generalization of the full batch gradient descent for neural network in the parameter space. The initialization of the NTK is based on Edge of Chaos, and the activation function is a function of the network depth. Experiments show that the proposed NTK can achieve state-of-the-art performance.","This paper proposes a new neural network architecture, the Neural Tangent Kernel (NTK), which is based on gradient descent in the parameter space of the kernel gradient descent over the function space. The neural network is trained using a linear model for wide networks. The NTK consists of two steps: initialization and activation function. The initialization consists of Edge of Chaos and the activation function of the NTK. The network depth consists of full batch gradient descent."
11824,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"self - supervised method USED-FOR sentence representations. injection of linguistic knowledge USED-FOR self - supervised method. sentence structures USED-FOR semantic meaning. Multiple linguistic frameworks USED-FOR sentence structures. compositional words operations USED-FOR semantic meaning. embeddings USED-FOR semantic. linguistic views USED-FOR embeddings. OtherScientificTerm are linguist diversity, views, and sentence outward form. ",This paper proposes a self-supervised method for learning sentence representations from the injection of linguistic knowledge. The key idea is to learn sentence structures that capture semantic meaning from compositional words operations. Multiple linguistic frameworks are used to learn the sentence structures. The embeddings are learned from linguistic views that capture the semantic. The authors show that the embedding of a sentence can capture the linguisticist diversity of the language. The views are then used to generate the sentence outward form.,"This paper proposes a self-supervised method for learning sentence representations from the injection of linguistic knowledge. Multiple linguistic frameworks are used to learn sentence structures for semantic meaning from compositional words operations. The embeddings are learned from linguistic views of the sentence, and the semantic meaning is extracted from these views. The authors also propose a way to increase the linguist diversity of the views, which is achieved by learning a sentence outward form. "
11828,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"methods USED-FOR niche domains. product or movie review datasets EVALUATE-FOR sentiment classification solutions. finance HYPONYM-OF niche domains. Transfer learning USED-FOR new domains. NLP transfer learning USED-FOR financial sentiment classification. FinBERT HYPONYM-OF language model. financial sentiment classification task EVALUATE-FOR language model. FinancialPhrasebank dataset USED-FOR financial sentiment classification task. BERT USED-FOR language model. OtherScientificTerm is domainspecific language. Generic is models. Material are labeled data, specific domain, and large training data sets. ","This paper proposes a new method for finding niche domains for sentiment classification solutions on product or movie review datasets. The proposed method is based on NLP transfer learning to learn new domains. The model is trained on a FinancialPhrasebank dataset, where the goal is to learn a domainspecific language. The language model is then trained on FinBERT, a language model based on BERT. The authors show that the proposed models are able to identify the labeled data in a specific domain, and can be used to train new models for new domains with large training data sets. ","This paper proposes two methods for learning niche domains (e.g., finance and product or movie review datasets) for sentiment classification solutions. The main idea is to learn a domainspecific language, where each domain is represented as a set of labeled data, and the goal is to train a language model on the specific domain. Transfer learning is used to learn new domains. The models are evaluated on two large training data sets. The language model is trained on the FinBERT dataset and the financial sentiment classification task on the FinancialPhrasebank dataset. "
11832,SP:31c9c3a693922d5c3448e80ade920391dce261f9,musical scores CONJUNCTION text lyrics. text lyrics CONJUNCTION musical scores. Generative models USED-FOR singing voice. Generative models USED-FOR singing voice synthesis. singing voice synthesis USED-FOR singing voice waveforms. musical scores USED-FOR singing voice waveforms. text lyrics USED-FOR singing voice waveforms. pre - assigned scores CONJUNCTION lyrics. lyrics CONJUNCTION pre - assigned scores. pre - assigned scores USED-FOR singing voice generation. singing voice generation HYPONYM-OF alternative. training and inference time EVALUATE-FOR singing voice generation. pipeline USED-FOR tasks. source separation and transcription models USED-FOR data preparation. adversarial networks USED-FOR audio generation. source separation and transcription models CONJUNCTION adversarial networks. adversarial networks CONJUNCTION source separation and transcription models. adversarial networks CONJUNCTION metrics. metrics CONJUNCTION adversarial networks. Method is unconditioned or weakly conditioned singing voice generation schemes. ,"This paper proposes a new method for singing voice synthesis based on Generative models. The authors propose a new alternative called singing voice generation, which uses pre-assigned scores and text lyrics to generate singing voice waveforms based on musical scores or text lyrics. The proposed pipeline can be applied to a variety of tasks such as source separation and transcription models for data preparation and adversarial networks for audio generation. The paper shows that the proposed method is able to achieve better training and inference time than unconditioned or weakly conditioned voice generation schemes. ","This paper proposes an alternative to singing voice synthesis based on Generative models for singing voice. The main idea is to use musical scores and text lyrics to generate singing voice waveforms based on pre-assigned scores or text lyrics. The proposed pipeline can be used for different tasks such as data preparation, source separation and transcription models for data generation, adversarial networks for audio generation, and other metrics. Experiments show that the proposed unconditioned or weakly conditioned singing voice generation schemes can achieve better training and inference time."
11836,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,computer vision CONJUNCTION audio - understanding. audio - understanding CONJUNCTION computer vision. deep neural networks USED-FOR computer vision. deep neural networks USED-FOR audio - understanding. adversarial attacks FEATURE-OF they. defensive tensorization HYPONYM-OF adversarial defence technique. latent high order factorization of the network USED-FOR adversarial defence technique. Randomization USED-FOR latent subspace. Randomization USED-FOR dense reconstructed weights. sparsity CONJUNCTION perturbations. perturbations CONJUNCTION sparsity. randomization USED-FOR perturbations. approach CONJUNCTION techniques. techniques CONJUNCTION approach. approach CONJUNCTION neural architecture. neural architecture CONJUNCTION approach. adversarial training HYPONYM-OF techniques. image classification benchmarks EVALUATE-FOR approach. audio classification task CONJUNCTION binary networks. binary networks CONJUNCTION audio classification task. binary networks EVALUATE-FOR approach. audio classification task EVALUATE-FOR approach. Generic is network. ,"This paper proposes a new adversarial defense technique called defensive tensorization, which is based on the latent high order factorization of the network. Randomization is used to reconstruct the latent subspace of the neural network, and dense reconstructed weights are used to defend against sparsity and perturbations. The proposed approach is evaluated on image classification benchmarks and audio classification task, and compared with other techniques such as adversarial training.","This paper proposes a new adversarial defence technique, called defensive tensorization. The idea is to use deep neural networks for computer vision and audio-understanding. The authors show that they can defend against adversarial attacks with sparsity, perturbations, and randomization in the latent subspace of the network. The proposed approach is evaluated on image classification benchmarks and on the audio classification task, and compared to other techniques such as adversarial training."
11840,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,Deep learning based approaches USED-FOR urban spatiotemporal forecasting problems. graph attention network CONJUNCTION transformer. transformer CONJUNCTION graph attention network. clustered graph transformer framework USED-FOR unsmoothness issue. transformer PART-OF encoder - decoder architecture. transformer PART-OF clustered graph transformer framework. graph attention network PART-OF clustered graph transformer framework. structural components USED-FOR architectures. structural components USED-FOR deep learning models. architectures PART-OF deep learning models. gradient - based clustering method USED-FOR feature extractors. gradient - based clustering method USED-FOR spatial domain. multi - view position encoding USED-FOR periodicity and closeness of urban time series data. multi - view position encoding USED-FOR temporal domain. real datasets EVALUATE-FOR method. real datasets EVALUATE-FOR baselines. method COMPARE baselines. baselines COMPARE method. ride - hailing business FEATURE-OF real datasets. OtherScientificTerm is unsmoothness issue of urban data. Material is urban data. ,"This paper studies the unsmoothness issue of urban data. The authors propose a clustered graph transformer framework to solve the unsmoothness problem. The proposed framework consists of a graph attention network, a transformer, and an encoder-decoder architecture. The structural components of the proposed architectures can be used to train deep learning models. The paper also proposes a gradient-based clustering method to learn feature extractors for the spatial domain and a multi-view position encoding to improve the periodicity and closeness of urban time series data. Experiments on two real datasets show that the proposed method performs better than other baselines on the ride-hailing business.",This paper presents a novel approach to solving the unsmoothness issue of urban spatiotemporal forecasting problems. The authors propose a clustered graph transformer framework that combines a graph attention network and a transformer in an encoder-decoder architecture. The proposed architectures are used to train deep learning models with structural components. The paper also proposes a gradient-based clustering method for feature extractors in the spatial domain and a multi-view position encoding for the temporal domain. The method is evaluated on two real datasets and compared to baselines in the ride-hailing business.
11844,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. deep neural networks USED-FOR fitted Q iteration ( FQI ) algorithm. algorithmic and statistical rates of convergence FEATURE-OF action - value functions. action - value functions FEATURE-OF iterative policy sequence. FQI USED-FOR iterative policy sequence. geometric rate FEATURE-OF algorithmic error. deep neural network USED-FOR action - value function. experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. Minimax - DQN algorithm USED-FOR zero - sum Markov game. DQN USED-FOR Minimax - DQN algorithm. Method are deep reinforcement learning, and deep Q - network ( DQN ) algorithm. OtherScientificTerm is statistical error. ",This paper studies the problem of deep reinforcement learning. The authors propose a fitted Q iteration (FQI) algorithm based on deep neural networks trained with experience replay and target network. They show that the iterative policy sequence of FQI has algorithmic and statistical rates of convergence for the action-value functions of iterative policies. The algorithmic error has a geometric rate of $O(\sqrt{T})$ and the statistical error is $\Omega(T)$. The authors then propose a deep Q-network (DQN) algorithm that uses the deep neural network to learn an action-valued function of the iterate. The DQN is trained with the target network and the experience replay. The Minimax-DQNs algorithm is then applied to the zero-sum Markov game.,"This paper proposes a new algorithm for deep reinforcement learning. The authors propose a fitted Q iteration (FQI) algorithm based on deep neural networks. The iterative policy sequence of FQI can be decomposed into two steps: (1) an action-value function that is computed by a deep neural network, and (2) an experience replay and target network that is learned by DQN. The algorithmic and statistical rates of convergence of the action-values functions are computed by the geometric rate of the algorithmic error, and the statistical error is computed using the deep Q-network (DQN) algorithm. The Minimax-dQN algorithm is used to solve the zero-sum Markov game. The paper shows that DQNs can be trained with experience replay or a target network."
11848,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"weighted sum USED-FOR semantics. attention mechanism USED-FOR natural language. PhraseTransformer HYPONYM-OF attention architecture. second phase USED-FOR inductive bias. WMT16 English - German translation task EVALUATE-FOR Transformer. WMT16 English - German translation task EVALUATE-FOR PhraseTransformer. BLEU EVALUATE-FOR Transformer. BLEU EVALUATE-FOR PhraseTransformer. Transformer COMPARE PhraseTransformer. PhraseTransformer COMPARE Transformer. WMT16 English - German translation task EVALUATE-FOR BLEU. OtherScientificTerm are self - attention, word compositions, compositional attentions, hypernodes, and non - linearity. Task is attention. Generic is first phase. Method is non - linear attention. ","This paper proposes a new attention architecture called PhraseTransformer, which is based on the attention mechanism in natural language. The main idea is to learn a weighted sum of the semantics of the input word compositions. The first phase learns the self-attention, and the second phase learns compositional attentions. The authors show that the inductive bias of the first phase is due to the non-linearity of the attention. They also show that by using hypernodes in the second stage, the Transformer can achieve better performance on the WMT16 English-German translation task and BLEU than the original Transformer. ","This paper proposes a new attention architecture called PhraseTransformer, which is based on the attention mechanism in natural language. The main idea is to use a weighted sum of the semantics of the word compositions as the first phase of the attention. The second phase is the inductive bias. The authors show that the self-attention can be reduced to non-linearity and that compositional attentions can be replaced by hypernodes. Experiments on the WMT16 English-German translation task and on the BLEU show that Transformer performs better than the original Transformer. "
11852,SP:622b0593972296a95b630a4ece1e959b60fec56c,"modular neural network architecture MAIN USED-FOR algorithms. input - output examples USED-FOR algorithms. neural controller USED-FOR variable - length input tape. neural controller PART-OF MAIN. general domain - agnostic mechanism USED-FOR selection of modules. general domain - agnostic mechanism USED-FOR MAIN. input tape layout CONJUNCTION parallel history tape. parallel history tape CONJUNCTION input tape layout. parallel history tape USED-FOR It. input tape layout USED-FOR It. memoryless controller USED-FOR it. input - output examples USED-FOR reinforcement learning. reinforcement learning USED-FOR MAIN architecture. it USED-FOR policies. algorithmic tasks EVALUATE-FOR MAIN. OtherScientificTerm are modules, random access, and tape locations. ","This paper proposes a modular neural network architecture MAIN for learning algorithms with input-output examples. It uses a neural controller to learn a variable-length input tape and a general domain-agnostic mechanism for the selection of modules. It also uses an input tape layout and a parallel history tape to learn policies. The authors show that it can be used as a memoryless controller for reinforcement learning, and it can learn policies that can be applied to a variety of algorithmic tasks. ",This paper proposes a modular neural network architecture MAIN for learning algorithms with input-output examples. MAIN consists of a neural controller for variable-length input tape and a general domain-agnostic mechanism for selection of modules. It uses a memoryless controller and a parallel history tape to learn the modules. The authors show that it can learn policies that are robust to random access to different tape locations. They also show that the MAIN architecture can be applied to reinforcement learning with input -output examples for reinforcement learning. The experimental results on several algorithmic tasks demonstrate the effectiveness of MAIN.
11856,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"Quantization USED-FOR Deep Neural Networks. Quantized floating point representations COMPARE fixed point representations. fixed point representations COMPARE Quantized floating point representations. Quantized floating point representations USED-FOR dynamic range. fixed point representations USED-FOR dynamic range. technique USED-FOR Deep Neural Networks. floating point arithmetic FEATURE-OF quantization. quantization FEATURE-OF Deep Neural Networks. floating point arithmetic USED-FOR Deep Neural Networks. Monte Carlo Arithmetic USED-FOR inference computation. relative standard deviation FEATURE-OF neural network loss. CIFAR-10 and ImageNet datasets EVALUATE-FOR pre - trained image classification models. pre - trained image classification models EVALUATE-FOR method. CIFAR-10 and ImageNet datasets EVALUATE-FOR method. loss of significance FEATURE-OF weight parameter sets. Metric is accuracy. OtherScientificTerm are parameter distributions, network topology, Monte Carlo trials, and network topologies. Method is MCDA. ",This paper proposes a new technique for quantization in Deep Neural Networks based on floating point arithmetic. Quantized floating point representations can be used to represent the dynamic range of the parameter distributions of the network topology. The authors use Monte Carlo Arithmetic for inference computation and show that the relative standard deviation of the neural network loss with respect to the number of parameters is a good measure of the accuracy. The proposed method is evaluated on CIFAR-10 and ImageNet datasets for pre-trained image classification models.,This paper proposes a new technique for quantization of Deep Neural Networks based on floating point arithmetic. Quantized floating point representations are used to represent the dynamic range of the parameter distributions of the network topology. The authors show that the proposed method is able to improve the accuracy of the model on CIFAR-10 and ImageNet datasets compared to the state-of-the-art pre-trained image classification models. The main contribution of the paper is the use of Monte Carlo Arithmetic for inference computation. The paper also shows that the relative standard deviation of the neural network loss is a measure of the loss of significance of the weight parameter sets. 
11860,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"Model - based reinforcement learning ( MBRL ) USED-FOR data - efficiently learning control of continuous tasks. function approximators CONJUNCTION planning schemes. planning schemes CONJUNCTION function approximators. function approximators USED-FOR MBRL. planning schemes USED-FOR MBRL. dynamics models USED-FOR task. method USED-FOR mismatch issue. dynamics model training USED-FOR method. Method are MBRL framework, and forward dynamics model. Task are objective mismatch issue, and downstream control task. OtherScientificTerm are Objective mismatch, and objective mismatch. Metric is downstream control performance. ","This paper studies the problem of data-efficient learning control of continuous tasks using model-based reinforcement learning (MBRL) in the context of objective mismatch issue. The authors propose a new MBRL framework that combines function approximators and planning schemes for MBRL. The main idea is to learn a forward dynamics model, and then use the learned dynamics models to perform a downstream control task using dynamics models trained on the task. The proposed method is able to solve the mismatch issue by using dynamics model training. Objective mismatch is defined as the difference between the downstream control performance and the objective mismatch of the forward dynamics models.","This paper proposes a new model-based reinforcement learning (MBRL) framework for data-efficient learning control of continuous tasks. The MBRL framework is based on a forward dynamics model, where the objective mismatch issue is defined as the difference between the downstream control task and the target task. The authors propose a method to address the mismatch issue by using dynamics model training. They show that MBRL with function approximators and planning schemes can achieve better performance than MBRL without the objective mismatch. They also show that the proposed method improves downstream control performance."
11864,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"CNN classifiers USED-FOR adversarial attacks. targeted blackbox transfer - based attack EVALUATE-FOR undefended ImageNet models. intermediate feature distributions FEATURE-OF CNNs. adversarial attacks USED-FOR intermediate feature distributions. Generic are network, and methodology. Task are adversarial attack, and attacking process. Method is CNN architecture. Metric is transferability. ",This paper studies the problem of adversarial attacks on CNN classifiers against undefended ImageNet models under targeted blackbox transfer-based attack. The authors propose a novel methodology to attack the network by attacking the intermediate feature distributions of the CNNs. They show that the adversarial attack can be applied to any CNN architecture and that the transferability of the network can be improved by the attacking process. ,"This paper proposes a new adversarial attack against CNN classifiers for adversarial attacks on undefended ImageNet models. The authors propose a targeted blackbox transfer-based attack against undefending ImageNet model. The proposed methodology is based on the idea that the network is not transferable across different layers of the network, and that the attacker can only attack one layer at a time. The paper also proposes a novel way to attack the CNN architecture. The attacking process is simple, and the authors show that the transferability of the attack can be reduced to a single layer. They also show that CNNs with intermediate feature distributions that are not transferably transferable can be attacked."
11868,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,approach USED-FOR reinforcement learning policies. latent behavioral space FEATURE-OF Wasserstein distances ( WDs ). Wasserstein distances ( WDs ) USED-FOR approach. dual formulation USED-FOR score functions. dual formulation USED-FOR WD. score functions USED-FOR policy optimization. dual formulation USED-FOR algorithms. smoothed WDs CONJUNCTION dual formulation. dual formulation CONJUNCTION smoothed WDs. WD regularizers USED-FOR algorithms. Behavior - Guided Policy Gradient CONJUNCTION Behavior - Guided Evolution Strategies. Behavior - Guided Evolution Strategies CONJUNCTION Behavior - Guided Policy Gradient. regularizers PART-OF on - policy algorithms. Behavior - Guided Evolution Strategies COMPARE methods. methods COMPARE Behavior - Guided Evolution Strategies. Behavior - Guided Policy Gradient HYPONYM-OF on - policy algorithms. Behavior - Guided Evolution Strategies HYPONYM-OF on - policy algorithms. Method is demo1. ,"This paper proposes a novel approach for learning reinforcement learning policies in the latent behavioral space of Wasserstein distances (WDs). The approach is based on a dual formulation of the score functions of the WD and the dual formulation for the WD for policy optimization. The authors show that the two algorithms, smoothed WDs and dual formulation, can be combined with WD regularizers to achieve state-of-the-art on-policy algorithms such as Behavior-Guided Policy Gradient and Behavior-GUIDED Evolution Strategies. The experimental results demonstrate that the proposed methods perform better than existing methods.",This paper proposes a novel approach for learning reinforcement learning policies in the latent behavioral space. The approach is based on Wasserstein distances (WDs) in the sense that they can be decomposed into two parts: (1) a dual formulation of the WD and (2) score functions for policy optimization. The authors propose two algorithms based on the dual formulation: smoothed WDs and the WD regularizers. Experiments show that the proposed methods outperform other on-policy algorithms such as Behavior-Guided Policy Gradient and Behavior-GUID Evolution Strategies. 
11872,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"deep neural networks PART-OF supervised learning. optimization algorithm USED-FOR deep learning. interpolation property USED-FOR optimization algorithm. it USED-FOR adaptive learning - rate. SGD USED-FOR ALI - G. learning - rate EVALUATE-FOR ALI - G. SGD COMPARE ALI - G. ALI - G COMPARE SGD. constant hyper - parameter USED-FOR ALI - G. constant hyper - parameter USED-FOR learning - rate. convergence guarantees FEATURE-OF ALI - G. ALI - G USED-FOR stochastic convex setting. convergence guarantees FEATURE-OF stochastic convex setting. wide residual networks CONJUNCTION densely connected networks. densely connected networks CONJUNCTION wide residual networks. architectures CONJUNCTION tasks. tasks CONJUNCTION architectures. SVHN data set EVALUATE-FOR wide residual network. CIFAR data sets EVALUATE-FOR wide residual networks. SNLI data set EVALUATE-FOR Bi - LSTM. CIFAR data sets EVALUATE-FOR densely connected networks. ALI - G COMPARE SGD. SGD COMPARE ALI - G. ALI - G COMPARE adaptive methods. adaptive methods COMPARE ALI - G. manually tuned learning - rate schedules USED-FOR SGD. ALI - G USED-FOR drop - in replacement. ALI - G PART-OF deep learning framework. OtherScientificTerm are empirical loss, Adaptive Learning - rates, and decay schedule. Method is differentiable neural computer. ","This paper proposes a new optimization algorithm for deep learning with deep neural networks in the context of supervised learning. The optimization algorithm is based on the interpolation property of the empirical loss. Adaptive Learning-rates (ALI-G) is a differentiable neural computer. The authors show that it can be used as an adaptive learning-rate in the stochastic convex setting with convergence guarantees in the setting where the number of parameters is large. The main contribution of the paper is the theoretical analysis of the convergence guarantees of ALI-GAN with constant hyper-parameter for the learning-rates. The theoretical analysis shows that ALI -GAN is more efficient than SGD in the same setting, and that it is also able to perform drop-in replacement. The paper also provides a theoretical analysis on the performance of Bi-LSTM on the SNLI data set. ","This paper proposes a new optimization algorithm for deep learning. The main idea is to use the interpolation property of the optimization algorithm to improve the performance of deep neural networks in the context of supervised learning. Adaptive Learning-rates (ALI-G) is an extension of SGD to the stochastic convex setting, and it is shown that it is more efficient than SGD in terms of adaptive learning-rate. The authors also show that ALI-g is more robust to drop-in replacement and convergence guarantees of ALI -G in the stochy convex settings with constant hyper-parameter. The paper also proposes a differentiable neural computer. The experiments show that the proposed algorithms outperform SGD on the Bi-LSTM on the SVHN data set and on the CIFAR data sets, and on a variety of architectures and tasks. "
11876,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,Forming perceptual groups CONJUNCTION individuating objects in visual scenes. individuating objects in visual scenes CONJUNCTION Forming perceptual groups. Forming perceptual groups USED-FOR visual intelligence. computations USED-FOR ability. connections USED-FOR perceptual grouping. high - level object cues USED-FOR perceptual grouping. synthetic visual tasks EVALUATE-FOR neural network architectures. learning USED-FOR networks. bottom - up connections USED-FOR networks. Horizontal connections USED-FOR straining. top - down connections USED-FOR learning. high - level object cues USED-FOR learning. model USED-FOR perceptual groups. interactions USED-FOR perceptual groups. interactions PART-OF model. Material is visual scenes. Generic is task. OtherScientificTerm is Gestalt cues. Task is incremental grouping. ,"This paper studies the problem of learning perceptual groups and individuating objects in visual scenes. Forming perceptual groups is an important problem in visual intelligence. The authors propose a new task called Gestalt cues, where the goal is to learn a set of connections that can be used to perform perceptual grouping using high-level object cues. Theoretical results show that the ability of these computations can improve the performance of neural network architectures on a variety of synthetic visual tasks. The main contribution of the paper is a model that learns perceptual groups from interactions between objects in the scene, and then uses these top-down connections to perform learning for networks with bottom-up connections for straining. ","This paper proposes a new task of learning perceptual groups for visual intelligence. Forming perceptual groups and individuating objects in visual scenes is an important problem in the field of visual intelligence, and the ability to learn such computations is not well understood. The authors propose to use top-down connections for perceptual grouping and high-level object cues for learning for networks with bottom-up connections for straining. Gestalt cues are used for incremental grouping. Experiments on synthetic visual tasks demonstrate the effectiveness of the proposed neural network architectures. The proposed model learns perceptual groups from interactions between objects and interactions between perceptual groups."
11880,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"` 1 or ` 0 regularizers USED-FOR weight sparsity. ` 0 regularizer USED-FOR parameter sparsity. complex optimization techniques USED-FOR it. gradient descent USED-FOR ` 1 regularizer. Hoyer measure USED-FOR compressed sensing problems. DeepHoyer HYPONYM-OF sparsity - inducing regularizers. DeepHoyer regularizers USED-FOR sparser neural network models. DeepHoyer USED-FOR element - wise and structural pruning. Task is sparse and efficient neural network models. Method is neural network models. OtherScientificTerm are scaling of parameter values, gradients, and sparsity. Metric are shrinking rate, and accuracy level. ","This paper studies the problem of sparse and efficient neural network models. The authors propose a new Hoyer measure for compressed sensing problems, called DeepHoyer, to measure the scaling of parameter values. The main idea is to use `1 or `0 regularizers to reduce the weight sparsity of parameter sparsity in the `1 regularizer by gradient descent, and then use complex optimization techniques to improve it. The paper shows that the shrinking rate of the gradient descent can be used to improve the accuracy level of the sparsity-inducing regularizers, and that the resulting deepHoyer regularizers can be applied to sparser neural network model models.    The paper also shows that using DeepHoser can improve the performance of element-wise and structural pruning. ","This paper proposes a new measure for parameter sparsity in sparse and efficient neural network models. The measure is based on the Hoyer measure for compressed sensing problems. The authors propose to use `1 or `0 regularizers for weight sparsity. The `1 regularizer uses gradient descent, and it is a simple extension of complex optimization techniques. The scaling of parameter values depends on the gradients of the weights.  The authors show that the shrinking rate is proportional to the accuracy level, and that the sparsity can be controlled by DeepHoyer, which is a sparsity-inducing regularizers.   The paper also shows that DeepHoser can be used for element-wise and structural pruning. "
11884,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"data - dependent O(log T ) regret bound FEATURE-OF strongly convex functions. Adam USED-FOR data - dependent O(log T ) regret bound. controlled step size USED-FOR strong convexity. hyperparameters USED-FOR SAdam. RMSprop USED-FOR strongly convex functions. SC - RMSprop HYPONYM-OF RMSprop. SC - RMSprop USED-FOR SAdam. optimizing strongly convex functions CONJUNCTION deep networks. deep networks CONJUNCTION optimizing strongly convex functions. OtherScientificTerm are O ( √ T ) regret bound, and time horizon. Metric is data - dependent logarithmic regret bound. Generic is method. ","This paper studies the data-dependent logarithmic regret bound for strongly convex functions with O(log T) regret bound. The authors propose Adam, a new method that uses hyperparameters in SAdam to reduce the O (√T) regret by controlling step size of the strong convexity. The proposed method is based on SC-RMSprop, which is a variant of RMSprop that can be used to optimize strongly and/or deep networks. The main contribution of the paper is that the time horizon of the regret bound can be extended to a fixed number of iterations.","This paper proposes Adam, a new data-dependent O(log T) regret bound for strongly convex functions. The main idea of Adam is to use RMSprop to optimize strong convex function with controlled step size. The proposed method is based on SC-RMSprop, which is an extension of SAdam with hyperparameters. The authors show that the O (√T ) regret bound does not depend on the time horizon. "
11888,SP:9f89501e6319280b4a14b674632a300805aa485c,BlockBERT HYPONYM-OF BERT model. BERT model USED-FOR modeling long - distance dependencies. BlockBERT USED-FOR modeling long - distance dependencies. memory consumption CONJUNCTION training time. training time CONJUNCTION memory consumption. attention heads USED-FOR shortor long - range contextual information. sparse block structures PART-OF attention matrix. BERT USED-FOR model. sparse block structures USED-FOR model. paragraph lengths FEATURE-OF benchmark question answering datasets. BlockBERT COMPARE BERT - based model. BERT - based model COMPARE BlockBERT. prediction accuracy EVALUATE-FOR BERT - based model. BlockBERT COMPARE RoBERTa. RoBERTa COMPARE BlockBERT. memory USED-FOR BlockBERT. RoBERTa HYPONYM-OF BERT - based model. training time EVALUATE-FOR BlockBERT. prediction accuracy EVALUATE-FOR BlockBERT. ,"This paper proposes a BERT model called BlockBERT, which is an extension of BERT for modeling long-distance dependencies. The model is based on sparse block structures in the attention matrix, and the attention heads are used to encode shortor long-range contextual information. The paper shows that BlockBERTs have better memory consumption and training time compared to BERT-based model such as RoBERTa, and achieves better prediction accuracy than BERT - based model.  The paper also provides a set of benchmark question answering datasets with paragraph lengths. ","This paper proposes a BERT model, BlockBERT, for modeling long-distance dependencies. The model is based on sparse block structures in the attention matrix. The attention heads are designed to capture shortor long-range contextual information. The paper provides a set of benchmark question answering datasets with paragraph lengths. The authors show that the memory consumption and training time of BlockBERTs are comparable to the BERT-based model, RoBERTa. The prediction accuracy for BlockBERt is better than the prediction accuracy of RoBERT."
11892,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"test accuracy EVALUATE-FOR pruning approaches. test accuracy EVALUATE-FOR pruning. generalization EVALUATE-FOR pruning. pruning CONJUNCTION regularizing. regularizing CONJUNCTION pruning. generalization FEATURE-OF over - parameterized networks. noise USED-FOR regularizing. noise USED-FOR pruning. OtherScientificTerm are Pruning neural network parameters, overfitting, and instability. Method is pruned model. ","This paper studies the problem of pruning neural network parameters. The authors show that the test accuracy of existing pruning approaches is not consistent with the generalization of the pruned model. Pruning can lead to overfitting and instability. To address this issue, the authors propose to use noise to improve the performance of the proposed pruning and regularizing. ","This paper studies the generalization properties of pruning neural network parameters. The authors show that the test accuracy of different pruning approaches is correlated with test accuracy, and that pruning can lead to overfitting and instability. They also show that noise can be used for pruning and regularizing in over-parameterized networks. "
11896,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"framework USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION neural architecture search ( NAS ) algorithm. neural architecture search ( NAS ) algorithm USED-FOR framework. architecture encoders CONJUNCTION decoders. decoders CONJUNCTION architecture encoders. reinforcement learning USED-FOR embedding space. reinforcement learning USED-FOR approach. decoders USED-FOR reinforcement learning. architecture encoders USED-FOR reinforcement learning. decoders USED-FOR approach. architecture encoders USED-FOR approach. NAS approaches USED-FOR image classification task. architecture network COMPARE NAS approaches. NAS approaches COMPARE architecture network. image classification task EVALUATE-FOR architecture network. CIFAR10 USED-FOR image classification task. CIFAR10 EVALUATE-FOR NAS approaches. NASES procedure USED-FOR architecture network. architecture - embedding searching CONJUNCTION pre - training controller. pre - training controller CONJUNCTION architecture - embedding searching. architecture - embedding searching USED-FOR NASES. pre - training controller USED-FOR NASES. parameter sharing HYPONYM-OF NAS tricks. architecture USED-FOR NASES procedure. Method are neural architectures, NAS in embedding space ( NASES ), and NAS with reinforcement learning approaches. OtherScientificTerm are noncontinuous and highdimensional search spaces, and discrete and high - dimensional architecture space. Task is optimization. ",This paper proposes a framework for the automatic discovering process of neural architectures. The framework is based on the neural architecture search (NAS) algorithm and reinforcement learning with reinforcement learning in the embedding space. The approach uses architecture encoders and decoders for reinforcement learning. The NASES procedure is a combination of architecture-embedding searching and a pre-training controller for NASES. Experimental results on CIFAR10 show that the proposed architecture network outperforms other NAS approaches on the image classification task. ,This paper proposes a framework for the automatic discovering process of neural architectures. The framework is based on the neural architecture search (NAS) algorithm and reinforcement learning. The approach uses architecture encoders and decoders for reinforcement learning in the embedding space. The authors show that the architecture network outperforms other NAS approaches on an image classification task on CIFAR10. They also show that NAS with reinforcement learning approaches outperforms NAS with architecture-embedding searching and a pre-training controller for NASES procedure. The paper also shows that NAS in embedding spaces (NASES) outperform other NAS tricks such as parameter sharing. 
11900,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"Homotopy Training Algorithm ( HTA ) USED-FOR optimization problems. neural networks USED-FOR optimization problems. decoupled systems USED-FOR HTA. low dimensional structure FEATURE-OF decoupled systems. HTA USED-FOR continuous homotopy path. continuous homotopy path USED-FOR system. homotopy solution path USED-FOR convex case. HTA USED-FOR non - convex case. CIFAR-10 USED-FOR VGG models. VGG models HYPONYM-OF examples. accuracy EVALUATE-FOR HTA. examples EVALUATE-FOR HTA. HTA USED-FOR neural networks. HTA CONJUNCTION dropout technique. dropout technique CONJUNCTION HTA. dropout technique USED-FOR neural networks. OtherScientificTerm are high dimensional coupled system, and low dimensionality. ","This paper proposes a homotopy training algorithm called Homotopy Training Algorithm (HTA) for optimization problems where the high dimensional coupled system has a low dimensional structure. HTA is based on decoupled systems, where the system is trained with a continuous homotope path. The authors show that HTA can be applied to the convex case and the non-convex case. The paper also shows that the HTA improves the accuracy of neural networks trained with HTA and the dropout technique. The experiments are conducted on CIFAR-10 and VGG models.","This paper proposes a homotopy training algorithm called Homotopy Training Algorithm (HTA) for optimization problems with neural networks. HTA is based on decoupled systems with low dimensional structure. The main idea is to learn a high dimensional coupled system with low dimensionality. The system is then trained using a continuous homotropic path. The convex case is solved by homotope solution path, and the non-convex case by HTA. The authors show that HTA improves accuracy and dropout technique for neural networks on CIFAR-10 and VGG models."
11904,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,attention USED-FOR entity representations. dot - product attention USED-FOR higher - dimensional attention. higher - dimensional attention PART-OF Transformer. tensor products of value vectors USED-FOR entity representations. inductive bias USED-FOR logical reasoning. architecture USED-FOR inductive bias. logical reasoning PART-OF deep reinforcement learning. Method is 2 - simplicial Transformer. ,This paper proposes a 2-simplicial Transformer that uses dot-product attention to learn entity representations from tensor products of value vectors. The key idea is to use higher-dimensional attention in the Transformer instead of just dot-products. The inductive bias of the proposed architecture is used to improve logical reasoning in deep reinforcement learning.,This paper proposes a 2-simplicial Transformer that combines dot-product attention with higher-dimensional attention for entity representations. The Transformer is based on tensor products of value vectors. The authors show that the inductive bias of the architecture can be alleviated by the proposed architecture. The proposed architecture is evaluated on logical reasoning in deep reinforcement learning.
11908,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,classification and regression approaches USED-FOR task. labelled data USED-FOR classification and regression approaches. labelled data USED-FOR pose estimation. circular latent representations USED-FOR 2D rotations. datasets USED-FOR method. labelled images FEATURE-OF datasets. Task is Pose estimation. OtherScientificTerm is fixed frame of reference. Method is Conditional Variational Autoencoders ( CVAEs ). ,"This paper studies the problem of Pose estimation from labelled data. The authors propose a new method called Conditional Variational Autoencoders (CVAEs), which is based on circular latent representations for 2D rotations. The proposed method is evaluated on a variety of datasets with labelled images. The results show the effectiveness of the proposed method. ","This paper proposes a new method for Pose estimation. The proposed method is based on Conditional Variational Autoencoders (CVAEs), which is a family of generative models that can be applied to a fixed frame of reference. The main idea is to use labelled data for pose estimation and regression approaches for the task. The authors propose to use circular latent representations for 2D rotations. The method is evaluated on two datasets with labelled images."
11912,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"Evolutionary Population Curriculum ( EPC ) HYPONYM-OF curriculum learning paradigm. evolutionary approach USED-FOR objective misalignment issue. evolutionary approach USED-FOR EPC. approach COMPARE baselines. baselines COMPARE approach. MARL algorithm EVALUATE-FOR EPC. MADDPG HYPONYM-OF MARL algorithm. Task is multi - agent games. Metric is complexity. OtherScientificTerm are policies, agent population, curriculum, and scaled populations. Method is MultiAgent Reinforcement Learning ( MARL ). ","This paper proposes a curriculum learning paradigm called Evolutionary Population Curriculum (EPC) that uses an evolutionary approach to solve the objective misalignment issue in multi-agent games. The authors show that the proposed approach can achieve better performance than existing baselines in terms of complexity. The proposed MARL algorithm, MADDPG, is an extension of MultiAgent Reinforcement Learning (MARL). The authors also show that EPC can be applied to scaled populations.","This paper proposes a curriculum learning paradigm called Evolutionary Population Curriculum (EPC) that uses an evolutionary approach to address the objective misalignment issue in multi-agent games. The key idea is to learn policies that maximize the diversity of the agent population. The authors propose MultiAgent Reinforcement Learning (MARL), which is a MARL algorithm based on MADDPG. The proposed approach is shown to outperform other baselines. The main contribution of the paper is that the complexity of the algorithm is lower than that of the baselines, and that the curriculum can be applied to scaled populations."
11916,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,MULTIPLEX GRAPH NETWORKS USED-FOR DIAGRAMMATIC REASONING. ,This paper proposes a new MULTIPLEX GRAPH NETWORKS for DIAGRAMMATIC REASONING. The main idea is to learn a DIAGramMATIC for each layer of the network. The authors show that the proposed network is able to learn the DIAGAN with the same number of parameters as the original network. ,This paper proposes a new MULTIPLEX GRAPH NETWORKS for DIAGRAMMATIC. The authors show that the proposed network is able to achieve better performance than the state-of-the-art. 
11920,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"Bayesian inference USED-FOR deep neural networks. agent exploration CONJUNCTION prediction fairness. prediction fairness CONJUNCTION agent exploration. decision making CONJUNCTION agent exploration. agent exploration CONJUNCTION decision making. calibrated measure of uncertainty USED-FOR decision making. It USED-FOR calibrated measure of uncertainty. calibrated measure of uncertainty USED-FOR agent exploration. calibrated measure of uncertainty USED-FOR prediction fairness. It USED-FOR overfitting. decision making CONJUNCTION prediction fairness. prediction fairness CONJUNCTION decision making. Markov Chain Monte Carlo ( MCMC ) methods USED-FOR Bayesian inference. model parameters FEATURE-OF posterior distribution. optimization methods USED-FOR large scale deep learning tasks. sampling methods COMPARE optimization methods. optimization methods COMPARE sampling methods. MCMC CONJUNCTION optimization methods. optimization methods CONJUNCTION MCMC. ATMC HYPONYM-OF adaptive noise MCMC algorithm. momentum CONJUNCTION noise. noise CONJUNCTION momentum. noise USED-FOR parameter update. momentum USED-FOR parameter update. momentum USED-FOR ATMC. noise USED-FOR ATMC. classification accuracy CONJUNCTION test log - likelihood. test log - likelihood CONJUNCTION classification accuracy. Cifar10 benchmark CONJUNCTION large scale ImageNet benchmark. large scale ImageNet benchmark CONJUNCTION Cifar10 benchmark. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. ResNet architecture USED-FOR ATMC. ResNet architecture CONJUNCTION batch normalization. batch normalization CONJUNCTION ResNet architecture. test log - likelihood EVALUATE-FOR optimization baseline. large scale ImageNet benchmark EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR optimization baseline. Cifar10 benchmark EVALUATE-FOR ATMC. test log - likelihood EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR ATMC. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR optimization baseline. ATMC USED-FOR overfitting. ATMC COMPARE ATMC. ATMC COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR ATMC. OtherScientificTerm are hyperparameters, and","This paper studies the problem of Bayesian inference in deep neural networks. The authors propose Markov Chain Monte Carlo (MCMC) methods, which are a generalization of Markov chain Monte Carlo methods. The main contribution of the paper is a new adaptive noise MCMC algorithm called ATMC. ATMC is based on the idea that the posterior distribution of the model parameters depends on the number of hyperparameters. It is then used as a calibrated measure of uncertainty for decision making, agent exploration, prediction fairness, and prediction fairness.  The authors show that ATMC outperforms existing optimization methods and MCMC in large scale deep learning tasks.   ","This paper proposes a new adaptive noise MCMC algorithm, ATMC, which is a variant of Markov Chain Monte Carlo (MCMC) methods for Bayesian inference for deep neural networks. The authors show that the posterior distribution of the model parameters of a deep neural network is a function of the hyperparameters of the weights of the network. It is shown that ATMC is more robust to overfitting than sampling methods, and that it can be used to improve the calibrated measure of uncertainty for decision making, agent exploration, prediction fairness, and prediction fairness. ATMC outperforms other optimization methods for large scale deep learning tasks on Cifar10 benchmark, large scale ImageNet benchmark, and test log-likelihood on CIFAR10 benchmark. It outperforms several optimization methods, including MCMC, ResNet architecture, batch normalization, and noise. It also outperforms the optimization baseline in terms of classification accuracy, and the calibration of the noise and the momentum for parameter update."
11924,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"accuracy EVALUATE-FOR latter. winning tickets USED-FOR dense, randomly initialized networks. small but critical subnetworks HYPONYM-OF winning tickets. early stopping CONJUNCTION low - precision training. low - precision training CONJUNCTION early stopping. large learning rates FEATURE-OF lowcost training schemes. low - precision training HYPONYM-OF lowcost training schemes. early stopping HYPONYM-OF lowcost training schemes. lowcost training schemes USED-FOR winning tickets. connectivity patterns FEATURE-OF neural networks. mask distance metric USED-FOR EB tickets. computational overhead EVALUATE-FOR mask distance metric. mask distance USED-FOR training methods. low - cost schemes USED-FOR EB tickets. method USED-FOR deep network training. mask distance USED-FOR them. deep networks CONJUNCTION datasets. datasets CONJUNCTION deep networks. accuracy EVALUATE-FOR training methods. Method is train - prune - retrain process. OtherScientificTerm is Early - Bird ( EB ) tickets. Metric is energy savings. Generic is state - ofthe - art training methods. ","This paper studies the problem of learning winning tickets for dense, randomly initialized networks with small but critical subnetworks. The authors propose a train-prune-retrain process, where the goal is to learn winning tickets that are more efficient than early-Bird (EB) tickets. They show that low-cost training schemes such as early stopping, low-precision training, and low-pruning with large learning rates can achieve better accuracy than the latter. They also show that the mask distance metric for EB tickets can be used to improve the computational overhead of these training methods. Finally, they propose a method for deep network training that uses them to reduce energy savings. ","This paper proposes a new method for deep network training. The authors propose to use winning tickets to train dense, randomly initialized networks with small but critical subnetworks. The winning tickets are generated by a train-prune-retrain process. The paper proposes two lowcost training schemes: early stopping and low-precision training with large learning rates. The proposed mask distance metric for EB tickets is based on the computational overhead of learning the EB tickets. They show that these two training methods can achieve better accuracy than other state-ofthe-art training methods in terms of computational overhead. They also show that the proposed method can improve the accuracy of neural networks with connectivity patterns."
11928,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"adversarial examples FEATURE-OF deep convolutional neural networks. intrinsic dimension COMPARE pixel space dimension. pixel space dimension COMPARE intrinsic dimension. low - dimensional space USED-FOR classification. intrinsic dimension FEATURE-OF image data. image data COMPARE pixel space dimension. pixel space dimension COMPARE image data. high - dimensional input images USED-FOR classification. high - dimensional input images USED-FOR low - dimensional space. vulnerability EVALUATE-FOR neural networks. robustness EVALUATE-FOR deep neural networks. adversarial robustness EVALUATE-FOR classifier. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR strong adversarial attack methods. strong adversarial attack methods EVALUATE-FOR framework. OtherScientificTerm are input dimension, lowdimensional space, regularization, and embedding regularization. Method is Embedding Regularized Classifier ( ER - Classifier ). ","This paper studies the vulnerability of deep convolutional neural networks against adversarial examples. The authors propose Embedding Regularized Classifier (ER-Classifier), a new classifier that uses low-dimensional space instead of high-dimensional input images for classification. They show that the intrinsic dimension of the image data is better than the pixel space dimension, and that the input dimension can be used to improve the robustness of the classifier. They also show that regularization can improve the performance of the ER-classifier. The proposed framework is evaluated on several benchmark datasets against strong adversarial attack methods.","This paper studies the vulnerability of deep convolutional neural networks to adversarial examples. The authors propose an Embedding Regularized Classifier (ER-Classifier) which is a regularization of the input dimension to the lowdimensional space. The intrinsic dimension of image data is compared to the pixel space dimension of the low-dimensional space for classification, and high-dimensional input images are used for classification. The proposed framework is evaluated on several benchmark datasets and compared to strong adversarial attack methods. The paper shows that the proposed classifier improves the robustness of deep neural networks. "
11932,SP:efd68097f47dbfdd0208573071686a62240d1b12,"Named entity recognition ( NER ) CONJUNCTION relation extraction ( RE ). relation extraction ( RE ) CONJUNCTION Named entity recognition ( NER ). Named entity recognition ( NER ) HYPONYM-OF tasks. relation extraction ( RE ) HYPONYM-OF tasks. dependency parsers HYPONYM-OF external natural language processing ( NLP ) tools. external natural language processing ( NLP ) tools USED-FOR joint models. neural, end - to - end model USED-FOR jointly extracting entities. large, pre - trained language model USED-FOR neural, end - to - end model. recurrence USED-FOR self - attention. OtherScientificTerm is propagation of error. Method are pipeline - based systems, neural, end - to - end models, and external NLP tools. Generic are tools, and model. ","This paper studies the problem of jointly extracting entities from a large, pre-trained language model using a neural, end-to-end model. The authors propose two tasks: Named entity recognition (NER) and relation extraction (RE) which are commonly used in external natural language processing (NLP) tools such as dependency parsers. The main contribution of the paper is the propagation of error in pipeline-based systems, which is an important problem in the literature. The paper proposes to use a neural network to learn joint models that can be used to jointly extract entities using external NLP tools.  The authors show that the self-attention of the neural network can be improved by recurrence in the training of the model. ","This paper presents a set of external natural language processing (NLP) tools for jointly extracting entities and relation extraction (RE) using pipeline-based systems. The main idea is to learn a neural, end-to-end model that can jointly extract entities and relations from a large, pre-trained language model. The authors propose two tools, dependency parsers and self-attention, which is a propagation of error. The proposed tools are evaluated on two tasks: Named entity recognition (NER) and RE. The results show that the proposed external NLP tools outperform the state-of-the-art in terms of performance. "
11936,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"network USED-FOR task. informative representation USED-FOR tasks. network USED-FOR informative representation. image context FEATURE-OF RGB values of images. RGB values of images HYPONYM-OF low - level feature representation. DNNs USED-FOR Euclidean representation. DNNs USED-FOR algorithm. problem PART-OF machine learning. Ordinal Embedding HYPONYM-OF machine learning. approaches USED-FOR problem. approach COMPARE methods. methods COMPARE approach. real - world large datasets EVALUATE-FOR methods. real - world large datasets EVALUATE-FOR approach. Method are supervised / unsupervised DNNs, and neural networks. OtherScientificTerm is triplet comparisons. Task is unsupervised learning problems. ","This paper studies the problem of learning a low-level feature representation from RGB values of images in an image context. The authors propose a new network for this task, called Ordinal Embedding, which can be used to learn an informative representation for different tasks. The proposed algorithm uses DNNs to learn a Euclidean representation of the input image, which is then used to train a neural network to perform the task. The paper shows that the proposed approach performs better than existing methods on real-world large datasets.",This paper proposes a novel approach to learning a low-level feature representation of the RGB values of images in an image context. The proposed approach is based on supervised/unsupervised DNNs. The idea is to use a network to learn an informative representation for tasks where the task is different from the original image. The authors propose an algorithm based on DNN's to learn a Euclidean representation of a set of images. They show that the proposed approach outperforms existing methods on real-world large datasets. They also show that their approach can be applied to unsupervised learning problems.
11940,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"domain adaptation CONJUNCTION corrupted sample discovery. corrupted sample discovery CONJUNCTION domain adaptation. building insights about the learning task CONJUNCTION domain adaptation. domain adaptation CONJUNCTION building insights about the learning task. corrupted sample discovery CONJUNCTION robust learning. robust learning CONJUNCTION corrupted sample discovery. Data Valuation HYPONYM-OF meta learning framework. Reinforcement Learning ( DVRL ) USED-FOR meta learning framework. deep neural network USED-FOR data value estimator. reinforcement signal USED-FOR data value estimator. DVRL COMPARE methods. methods COMPARE DVRL. DVRL USED-FOR data value estimates. data value estimates EVALUATE-FOR methods. domain adaptation CONJUNCTION robust learning. robust learning CONJUNCTION domain adaptation. DVRL COMPARE state - of - the - art. state - of - the - art COMPARE DVRL. DVRL USED-FOR corrupted sample discovery. DVRL USED-FOR robust learning. DVRL USED-FOR domain adaptation. Task are machine learning, and Data valuation. OtherScientificTerm is data values. Method are task predictor model, and predictor model. ","This paper proposes a meta learning framework called Data Valuation, which is based on Reinforcement Learning (DVRL). The main idea is to use a deep neural network to estimate the data value estimator using a reinforcement signal, and then use a task predictor model to predict the data values. Data valuation is an important problem in machine learning, and it is important to understand the relationship between domain adaptation, domain adaptation with corrupted sample discovery, and robust learning. The authors show that DVRL is able to achieve better data value estimates than state-of-the-art methods. ","This paper proposes a meta learning framework called Data Valuation, which is an extension of Reinforcement Learning (DVRL) to the task predictor model. Data valuation is an important problem in machine learning, where the goal is to estimate the value of a set of data values. The paper proposes to use a deep neural network as the data value estimator, and then use a reinforcement signal to guide the training of the model. Experiments show that DVRL outperforms state-of-the-art methods on data value estimates, domain adaptation, and corrupted sample discovery. "
11944,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"network ’s architecture CONJUNCTION initialization parameters. initialization parameters CONJUNCTION network ’s architecture. gradient FEATURE-OF random fully connected ReLU networks. finite sized networks FEATURE-OF per layer Jacobian. initialization parameters PART-OF network. ResNets CONJUNCTION DenseNets. DenseNets CONJUNCTION ResNets. vanilla networks CONJUNCTION ResNets. ResNets CONJUNCTION vanilla networks. vanilla networks HYPONYM-OF architectures. DenseNets HYPONYM-OF architectures. ResNets HYPONYM-OF architectures. arbitrary depths FEATURE-OF norm. architecture CONJUNCTION layer ’s size. layer ’s size CONJUNCTION architecture. Method are neural networks, initialization strategy, and deep networks. Task is gradient steps post - initialization. OtherScientificTerm are Jacobian squared norm, exploding or decaying gradients, per layer Jacobian norm, and layer ’s depth. ","This paper studies the problem of gradient steps post-initialization in neural networks. The authors consider the case of random fully connected ReLU networks with finite sized networks, where the network’s architecture and initialization parameters depend on the initialization parameters of the network. They show that the Jacobian squared norm of the per layer Jacobian of a finite sized network is a function of the number of initialization parameters in the network, and that exploding or decaying gradients have a significant impact on the performance. They then propose two architectures, ResNets and DenseNets, which are based on vanilla networks. They also propose an initialization strategy that is based on deep networks. Finally, they prove that the norm of a norm with arbitrary depths does not depend on any particular architecture or layer ’s size.","This paper proposes a novel initialization strategy for deep neural networks. The authors propose to use gradient steps post-initialization, where the Jacobian squared norm of a random fully connected ReLU networks is defined as the gradient of the per layer Jacobian of finite sized networks. They show that the network’s architecture and initialization parameters can be decomposed into two parts: (1) the architecture and layer ’s size, and (2) the initialization parameters of the network. They also show that under exploding or decaying gradients, the norm can be computed at arbitrary depths. They compare the architectures of vanilla networks, ResNets and DenseNets, and show that deep networks are more robust."
11948,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"hand - optimized libraries CONJUNCTION compilation heuristics. compilation heuristics CONJUNCTION hand - optimized libraries. genetic algorithms CONJUNCTION stochastic methods. stochastic methods CONJUNCTION genetic algorithms. compilation heuristics CONJUNCTION genetic algorithms. genetic algorithms CONJUNCTION compilation heuristics. hand - optimized libraries USED-FOR neural networks. solution USED-FOR code optimization. reinforcement learning USED-FOR CHAMELEON. reinforcement learning USED-FOR solution. domain - knowledge inspired logic USED-FOR adaptive sampling algorithm. CHAMELEON COMPARE AutoTVM. AutoTVM COMPARE CHAMELEON. optimization time EVALUATE-FOR AutoTVM. real hardware EVALUATE-FOR CHAMELEON. inference time EVALUATE-FOR deep networks. optimization time EVALUATE-FOR CHAMELEON. Metric is compilation time. Generic are methods, and them. OtherScientificTerm are hardware measurements, design space, and real hardware measurements. Task is search. ","This paper studies the problem of code optimization with hand-optimized libraries and compilation heuristics in neural networks. The authors propose an adaptive sampling algorithm based on domain-knowledge inspired logic. The proposed solution, called CHAMELEON, combines reinforcement learning with genetic algorithms and stochastic methods. The optimization time of the proposed method is shown to be competitive with AutoTVM on real hardware measurements. ",This paper proposes a novel adaptive sampling algorithm based on domain-knowledge inspired logic. The main idea is to combine hand-optimized libraries with genetic algorithms and compilation heuristics for code optimization. The proposed solution is based on reinforcement learning. Experiments on real hardware measurements show that CHAMELEON outperforms AutoTVM in terms of optimization time and inference time for deep networks. 
11952,SP:df8483206bb88debeb24b04eb31e016368792a84,adversarial perturbations FEATURE-OF classifiers. certified robustnesses FEATURE-OF top-1 predictions. top - k predictions PART-OF real - world applications. certified robustness USED-FOR top - k predictions. randomized smoothing USED-FOR certified robustness. it USED-FOR large - scale neural networks. it USED-FOR classifier. randomized smoothing USED-FOR classifier. ` 2 norm FEATURE-OF topk predictions. tight robustness FEATURE-OF topk predictions. ` 2 norm FEATURE-OF tight robustness. Gaussian noise USED-FOR randomized smoothing. randomized smoothing USED-FOR tight robustness. top - k predictions FEATURE-OF certified robustness. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. method USED-FOR ImageNet classifier. ` 2 - norms FEATURE-OF adversarial perturbations. top-5 accuracy EVALUATE-FOR ImageNet classifier. OtherScientificTerm is noise. ,This paper studies the problem of certified robustness of top-k predictions in real-world applications. The authors propose a novel method to improve the certification performance of classifiers with adversarial perturbations. The method is based on randomized smoothing with Gaussian noise. They show that it improves the certification of large-scale neural networks. The proposed method is evaluated on CIFAR10 and ImageNet classifier with top-5 accuracy.,"This paper studies the robustness of classifiers to adversarial perturbations in real-world applications with certified robustnesses to top-1 predictions. The authors propose to use randomized smoothing to improve the certified robustness for top-k predictions of the classifier, which is based on the `2 norm' of the topk predictions. They show that it can be used for large-scale neural networks with Gaussian noise. The proposed method is evaluated on CIFAR10 and ImageNet with top-5 accuracy. "
11956,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,gradient signal to noise ratio ( GSNR ) USED-FOR DNNs. GSNR EVALUATE-FOR parameter. GSNR CONJUNCTION generalization gap. generalization gap CONJUNCTION GSNR. logistic regression CONJUNCTION support vector machines. support vector machines CONJUNCTION logistic regression. gradient descent optimization dynamics USED-FOR DNNs. gradient descent optimization dynamics USED-FOR GSNR. support vector machines HYPONYM-OF shallow models. logistic regression HYPONYM-OF shallow models. Method is deep neural networks ( DNNs ). OtherScientificTerm is data distribution. ,This paper studies the gradient signal to noise ratio (GSNR) of deep neural networks (DNNs) in the context of gradient descent optimization dynamics for DNNs. The authors show that the parameter of GSNR can be used as a proxy for the generalization gap between the two parameters. The paper also provides a theoretical analysis of the effect of the parameter on the data distribution. The main result is that the GNNR is more sensitive to the gradient of the data than other shallow models such as support vector machines and logistic regression. ,This paper studies the gradient signal to noise ratio (GSNR) for DNNs. The parameter is defined as the difference between the GSNR and the generalization gap between the data distribution and the training data distribution. The authors show that the gradient descent optimization dynamics can be used to optimize GNNs with support vector machines and logistic regression. 
11960,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"approach USED-FOR problem. KG entities PART-OF vector space. conjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION conjunctions. existential quantifiers USED-FOR queries. conjunctions USED-FOR queries. logical disjunctions ( ∨ ) USED-FOR Handling queries. QUERY2BOX HYPONYM-OF embedding - based framework. embedding - based framework USED-FOR massive and incomplete KGs. QUERY2BOX USED-FOR arbitrary logical queries. Disjunctive Normal Form USED-FOR QUERY2BOX. Disjunctive Normal Form FEATURE-OF queries. ∧ FEATURE-OF arbitrary logical queries. QUERY2BOX COMPARE state of the art. state of the art COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. KGs EVALUATE-FOR QUERY2BOX. Task is Answering complex logical queries. OtherScientificTerm are complex query, hyper - rectangles, and disjunctions. ","This paper studies the problem of Answering complex logical queries with KG entities in the vector space. The authors propose a new approach to solve this problem, called QUERY2BOX, which is an embedding-based framework for massive and incomplete KGs. The queries are formulated as logical disjunctions (∨) between conjunctions and existential quantifiers, and the queries are represented as queries with conjunctions. Disjunctive Normal Form is used to represent the queries as queries in the form of a disjunctive normal form, and a complex query can be represented as a set of hyper-rectangles. The paper shows that the performance of the proposed Q2BOX on KGs is comparable to the state of the art in terms of performance on arbitrary logical queries.","This paper proposes a novel approach for solving the problem of Answering complex logical queries. The problem is formulated as a problem of learning KG entities in the vector space. The authors propose an embedding-based framework, called QUERY2BOX, which is able to handle massive and incomplete KGs. The queries are formulated as logical disjunctions (∨) between conjunctions and existential quantifiers, and the queries are represented as hyper-rectangles. The Disjunctive Normal Form is used to represent the queries in the Query2BOX. Experiments show that the proposed method outperforms the state of the art on arbitrary logical queries, and on KGs, and is more robust to hyper- rectangles."
11964,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"approaches USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) HYPONYM-OF approaches. machine learning USED-FOR convex loss functions. machine learning USED-FOR nonconvex deep neural networks. machine learning USED-FOR SGD. unbiased estimator COMPARE full gradient. full gradient COMPARE unbiased estimator. consistent estimators COMPARE unbiased ones. unbiased ones COMPARE consistent estimators. convergence behavior EVALUATE-FOR consistent estimators. training algorithms USED-FOR large - scale graphs. consistent estimators USED-FOR SGD updates. Method are unbiased gradient estimator, empirical risk minimization, and consistent gradient estimator. OtherScientificTerm are graphs, and convex, convex, and nonconvex objectives. Material is synthetic and real - world data. ","This paper studies the problem of stochastic optimization with Stochastic gradient descent (SGD) using machine learning for convex loss functions. The authors propose a new unbiased gradient estimator for SGD, which is based on the idea of empirical risk minimization. They show that the unbiased estimator performs better than the full gradient in terms of convergence behavior than the consistent estimators. They also provide a theoretical analysis of the convergence behavior of consistent estimator on SGD updates. ","This paper proposes two approaches for stochastic optimization, Stochastic gradient descent (SGD) and unbiased gradient estimator, which are both based on machine learning for convex loss functions. The main idea is to use machine learning to train SGD for nonconvex deep neural networks. The authors provide empirical risk minimization and show that the unbiased estimator outperforms the full gradient and the unbiased ones outperform the consistent estimators in terms of convergence behavior. They also provide empirical results on synthetic and real-world data, showing the convergence behavior of the two unbiased estimators on large-scale graphs, and on training algorithms for large-sized graphs. Finally, the authors show that consistent gradient estimators can be used for SGD updates. "
11968,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,resource constraints FEATURE-OF inference. neural architecture search ( NAS ) USED-FOR specialized neural network. neural architecture search ( NAS ) USED-FOR approaches. OFA network USED-FOR specialized sub - network. width CONJUNCTION kernel size. kernel size CONJUNCTION width. depth CONJUNCTION width. width CONJUNCTION depth. kernel size CONJUNCTION resolution. resolution CONJUNCTION kernel size. progressive shrinking algorithm HYPONYM-OF generalized pruning method. generalized pruning method COMPARE pruning. pruning COMPARE generalized pruning method. depth HYPONYM-OF pruning. progressive shrinking algorithm USED-FOR OFA networks. width HYPONYM-OF pruning. kernel size HYPONYM-OF pruning. It USED-FOR subnetworks. hardware platforms CONJUNCTION latency constraints. latency constraints CONJUNCTION hardware platforms. accuracy EVALUATE-FOR It. MobileNetV3 COMPARE EfficientNet. EfficientNet COMPARE MobileNetV3. OFA COMPARE MobileNetV3. MobileNetV3 COMPARE OFA. OFA COMPARE EfficientNet. EfficientNet COMPARE OFA. ImageNet top1 accuracy EVALUATE-FOR MobileNetV3. MobileNetV3 COMPARE MobileNetV3. MobileNetV3 COMPARE MobileNetV3. CO2 emission EVALUATE-FOR OFA. ImageNet top1 accuracy EVALUATE-FOR OFA. accuracy EVALUATE-FOR OFA. SOTA EVALUATE-FOR OFA. DSP classification track CONJUNCTION LPCVC. LPCVC CONJUNCTION DSP classification track. classification track CONJUNCTION detection track. detection track CONJUNCTION classification track. LPCVC CONJUNCTION classification track. classification track CONJUNCTION LPCVC. detection track HYPONYM-OF LPCVC. LPCVC EVALUATE-FOR OFA. DSP classification track EVALUATE-FOR OFA. Code CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Code. Material is edge devices. Generic is it. OtherScientificTerm is diverse architectural settings. Metric is ImageNet top-1 accuracy. ,"This paper studies the problem of resource constraints in inference in diverse architectural settings. The authors propose two approaches: (1) a neural architecture search (NAS) to find a specialized neural network that can be used as an OFA network for a specialized sub-network, and (2) a progressive shrinking algorithm to shrink the OFA networks. Theoretically, the authors show that the proposed generalized pruning method is more efficient than pruning such as width, depth, kernel size, resolution, and depth. It is also shown that it can be applied to different subnetworks. The paper also shows that OFA outperforms EfficientNet and MobileNetV3 in terms of ImageNet top-1 accuracy and DSP classification track. ","This paper proposes a novel approach to learn a specialized sub-network that can be used in a diverse architectural setting. The idea is to use neural architecture search (NAS) to find a specialized neural network that is more robust to resource constraints in inference. The proposed approaches are based on a generalized pruning method, which is based on the progressive shrinking algorithm of OFA networks. The authors show that the proposed pruning is more effective than pruning in terms of width, depth, and kernel size. It is also shown that it can be applied to subnetworks with diverse architectural settings. The experimental results show that OFA outperforms EfficientNet, MobileNetV3, and ImageNet top-1 accuracy on CO2 emission and SOTA. "
11972,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"discrete, symbolic operations USED-FOR they. Neural module networks ( NMNs ) USED-FOR questions. Neural module networks ( NMNs ) USED-FOR executable programs. modules PART-OF executable programs. synthetic visual QA domains EVALUATE-FOR Neural module networks ( NMNs ). models USED-FOR non - synthetic questions. model USED-FOR reasoning. model USED-FOR natural language. open - domain text USED-FOR non - synthetic questions. sorting CONJUNCTION counting. counting CONJUNCTION sorting. arithmetic CONJUNCTION sorting. sorting CONJUNCTION arithmetic. modules USED-FOR symbolic reasoning. probabilistic and differentiable manner USED-FOR symbolic reasoning. counting HYPONYM-OF symbolic reasoning. arithmetic HYPONYM-OF symbolic reasoning. sorting HYPONYM-OF symbolic reasoning. heuristically - obtained question program CONJUNCTION intermediate module output supervision. intermediate module output supervision CONJUNCTION heuristically - obtained question program. inductive bias USED-FOR learning. heuristically - obtained question program USED-FOR learning. intermediate module output supervision USED-FOR inductive bias. heuristically - obtained question program USED-FOR inductive bias. intermediate module output supervision USED-FOR learning. model COMPARE models. models COMPARE model. DROP dataset EVALUATE-FOR models. DROP dataset EVALUATE-FOR model. Task is Answering compositional questions. Method are NMNs, and unsupervised auxiliary loss. ","This paper studies the problem of Answering compositional questions in the synthetic visual QA domains. Neural module networks (NMNs) are trained to generate executable programs with discrete, symbolic operations, and they are trained in a probabilistic and differentiable manner. The authors show that these modules can be used for symbolic reasoning (e.g., counting, sorting, sorting) and reasoning in non-synthetic questions (i.e., open-domain text). The authors also show that NMNs can be trained in an unsupervised auxiliary loss. The model is able to perform reasoning in natural language, and the model is shown to outperform other models on the DROP dataset. ","This paper presents a new model for reasoning in non-synthetic questions in synthetic visual QA domains. The model is based on Neural module networks (NMNs) and they are able to learn discrete, symbolic operations for questions. The authors show that NMNs can learn executable programs that can be represented as modules in a probabilistic and differentiable manner. They also show that the model can learn natural language from open-domain text. The proposed model is evaluated on the DROP dataset and shows that the proposed model outperforms other models in terms of inductive bias, heuristically-referred question program, intermediate module output supervision, and learning. "
11976,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"Network pruning USED-FOR compressing deep neural networks. approach USED-FOR pruning. connection sensitivity HYPONYM-OF saliency criterion. saliency criterion USED-FOR pruning. initialization conditions USED-FOR connection sensitivity measurements. gradient FEATURE-OF connection sensitivity. signal propagation properties FEATURE-OF pruned networks. image classification tasks EVALUATE-FOR network models. signal propagation perspective CONJUNCTION unsupervised pruning. unsupervised pruning CONJUNCTION signal propagation perspective. pruning USED-FOR arbitrarily - designed architectures. supervision USED-FOR pruning. Method are deep neural networks, data - free method, and pruning at initialization method. Generic is model. OtherScientificTerm is redundant parameters. ","This paper proposes a new approach for compressing deep neural networks by pruning at initialization. The main idea is to use a saliency criterion, called connection sensitivity, as the connection sensitivity criterion for pruning under initialization conditions. The connection sensitivity is defined as the gradient of the difference between the signal propagation properties of the pruned networks and the original model. The authors then propose a data-free method to prune the network at initialization method. The paper shows that the proposed pruning can be applied to arbitrary-designed architectures with signal propagation perspective and unsupervised pruning with supervision. The experimental results on image classification tasks demonstrate the effectiveness of the proposed network models.","This paper proposes a novel approach for compressing deep neural networks. The authors propose a data-free method to prune the parameters of the model. The pruning at initialization method is based on the saliency criterion of connection sensitivity, which is the gradient of the connection sensitivity measurements under initialization conditions. The proposed approach is evaluated on several image classification tasks, showing that the pruned networks have better signal propagation properties compared to the unsupervised pruning and the signal propagation perspective. The paper also shows that the proposed pruning can be applied to arbitrarily-designed architectures without supervision. "
11980,SP:d5899cba36329d863513b91c2db57675086abc49,"deep neural networks PART-OF machine learning research. training ‘ a priori ’ sparse networks USED-FOR method. training ‘ a priori ’ sparse networks USED-FOR layers. information bandwidth FEATURE-OF layers. sparse topology USED-FOR networks. data - free heuristic USED-FOR topology. Task is fast training. OtherScientificTerm are dense and convolutional layers, memory, and intra - layer topology. Method is sparse neural network initialization scheme. Generic are topologies, and network. ","This paper proposes a new sparse neural network initialization scheme. The proposed method uses training ‘a priori’ sparse networks to learn the layers of dense and convolutional layers with high information bandwidth. The topologies are learned by a data-free heuristic, and the network is trained with sparse topology. The authors show that the topology learned by the network can be used to improve the performance of fast training. ","This paper proposes a new sparse neural network initialization scheme for fast training. The method is based on training ‘a priori’ sparse networks for the layers of deep neural networks in machine learning research. The idea is to learn dense and convolutional layers that share the same information bandwidth across layers. The topologies of the networks are learned using a data-free heuristic, where the topology is learned by a sparse topology. The authors show that the topologies can be learned in a single network. The paper also shows that the memory of the network is shared across layers, and that intra-layer topology can be learnt."
11984,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"recurrent neural networks ( RNNs ) USED-FOR long sequence tasks. exponential explosion CONJUNCTION vanishing of signals. vanishing of signals CONJUNCTION exponential explosion. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. RNN architectures COMPARE vanilla RNN cells. vanilla RNN cells COMPARE RNN architectures. GRU HYPONYM-OF RNN architectures. LSTM HYPONYM-OF RNN architectures. LSTMs CONJUNCTION GRUs. GRUs CONJUNCTION LSTMs. time scales CONJUNCTION spectral properties. spectral properties CONJUNCTION time scales. spectral properties FEATURE-OF state - to - state Jacobians. mean field theory USED-FOR signal propagation. mean field theory USED-FOR time scales. mean field theory USED-FOR GRUs. time scales USED-FOR signal propagation. LSTMs FEATURE-OF signal propagation. initialization scheme USED-FOR training instabilities. quantities USED-FOR initialization scheme. initialization hyperparameters USED-FOR quantities. it COMPARE initialization. initialization COMPARE it. it EVALUATE-FOR initialization scheme. multiple sequence tasks EVALUATE-FOR initialization scheme. generalization performance EVALUATE-FOR initialization. Generic are network, and they. Method is algorithmic and architectural modifications. OtherScientificTerm is instabilities. ","This paper studies the problem of training recurrent neural networks (RNNs) for long sequence tasks. RNN architectures such as GRU, LSTM, and GRU-LSTM are well-studied in the literature, but they are not well-suited for long sequences. The authors propose a new initialization scheme for training RNNs that is based on mean field theory for signal propagation on time scales and spectral properties of state-to-state Jacobians. The initialization scheme is evaluated on multiple sequence tasks and shows that it improves the generalization performance of initialization compared to vanilla RNN cells. ","This paper proposes a new initialization scheme for training recurrent neural networks (RNNs) for long sequence tasks. The authors propose to use algorithmic and architectural modifications to improve the generalization performance of RNN architectures such as GRU, LSTM, and LSTMs on time scales and spectral properties of state-to-state Jacobians. They also propose a mean field theory for the signal propagation of GRUs, LASTMs, and time scales. The initialization scheme is evaluated on multiple sequence tasks and it outperforms the initialization scheme on training instabilities. "
11988,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"intra - class diversity CONJUNCTION inter - class similarity. inter - class similarity CONJUNCTION intra - class diversity. deep learning algorithms USED-FOR classification tasks. intra - class diversity FEATURE-OF remote sensing scene image data sets. neural network USED-FOR smoothing operation. neural network USED-FOR approaches. neighboring scene images USED-FOR deep features. siamese network USED-FOR discriminative power. siamese network USED-FOR convolutional neural networks. discriminative power FEATURE-OF convolutional neural networks. neighboring scene images USED-FOR convolutional neural networks. siamese network USED-FOR approach. semantic coherence USED-FOR feature vector. It USED-FOR feature vector. semantic coherence USED-FOR It. approach COMPARE methods. methods COMPARE approach. model COMPARE baseline. baseline COMPARE model. mean squared error value EVALUATE-FOR baseline. disease density estimation task EVALUATE-FOR baseline. mean squared error value EVALUATE-FOR model. disease density estimation task EVALUATE-FOR model. prediction accuracy EVALUATE-FOR model. Metric is accuracy. Method are post - classification methods, and cleanup model. OtherScientificTerm is overhead. Generic is task. ",This paper studies the problem of post-classification methods in remote sensing scene image data sets with intra-class diversity and inter-class similarity. The authors propose a new approach based on a siamese network to improve the discriminative power of convolutional neural networks trained on neighboring scene images for deep features. The proposed approach uses a neural network to perform the smoothing operation. It uses semantic coherence to learn the feature vector. The model is evaluated on a disease density estimation task and shows better prediction accuracy than the baseline.,"This paper proposes a novel approach to improve the performance of deep learning algorithms for classification tasks with intra-class diversity and inter-class similarity in remote sensing scene image data sets. The authors propose to use a siamese network to improve discriminative power of convolutional neural networks trained on neighboring scene images to learn deep features. It uses semantic coherence to learn the feature vector and a neural network to perform smoothing operation. The proposed approach is evaluated on the disease density estimation task and the mean squared error value of the proposed model is compared to a baseline, and the proposed approach outperforms other methods on prediction accuracy. "
11992,SP:99c10e038939aa88fc112db10fe801b42360c8dc,"Self - supervised learning USED-FOR monocular depth estimation. geometry USED-FOR supervision. geometry USED-FOR Self - supervised learning. Depth networks USED-FOR representations. representations USED-FOR visual appearance. category - level patterns USED-FOR representations. semantic structure USED-FOR geometric representation learning. fixed pretrained semantic segmentation networks USED-FOR self - supervised representation learning. semantic labels CONJUNCTION proxy losses. proxy losses CONJUNCTION semantic labels. architecture USED-FOR self - supervised representation learning. semantic labels USED-FOR multi - task approach. proxy losses USED-FOR multi - task approach. semantic labels USED-FOR architecture. pixel - adaptive convolutions USED-FOR architecture. fixed pretrained semantic segmentation networks USED-FOR architecture. pixel - adaptive convolutions USED-FOR self - supervised representation learning. two - stage training process USED-FOR common semantic bias. common semantic bias FEATURE-OF dynamic objects. resampling USED-FOR two - stage training process. resampling USED-FOR common semantic bias. method COMPARE state of the art. state of the art COMPARE method. state of the art USED-FOR self - supervised monocular depth prediction. method USED-FOR self - supervised monocular depth prediction. OtherScientificTerm are 3D properties, self - supervised regime, and fine - grained details. ",This paper studies self-supervised learning for monocular depth estimation with geometry. The authors propose a new architecture based on fixed pretrained semantic segmentation networks to learn representations for visual appearance using depth networks. The proposed architecture is based on semantic labels and proxy losses. The paper shows that the proposed method is able to achieve better performance than the state of the art in terms of common semantic bias for dynamic objects. The method is also able to improve the performance of the proposed two-stage training process by using resampling.,This paper proposes a new architecture for self-supervised monocular depth estimation based on geometry. The proposed architecture is based on fixed pretrained semantic segmentation networks. The authors propose a multi-task approach based on semantic labels and proxy losses. They show that the proposed method outperforms the state of the art in terms of common semantic bias of dynamic objects. They also show that resampling improves the performance of the two-stage training process.
11996,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,label - flipping attacks HYPONYM-OF data poisoning attack. deep features USED-FOR linear classifiers. test - time robustness EVALUATE-FOR technique. randomized smoothing USED-FOR approach. Dogfish binary classification task EVALUATE-FOR baseline undefended classifier. ImageNet FEATURE-OF Dogfish binary classification task. certified accuracy EVALUATE-FOR classifier. accuracy EVALUATE-FOR baseline undefended classifier. multi - class classification algorithm USED-FOR label - flipping attacks. Task is label - flipping. Method is classification. Material is multi - class case. ,"This paper proposes a data poisoning attack called label-flipping attacks. The proposed technique is based on randomized smoothing to improve the test-time robustness of the proposed technique. The authors propose a multi-class classification algorithm to mitigate the harmful effects of label-Flipping attacks, and show that the proposed approach achieves better certified accuracy than the baseline undefended classifier on the Dogfish binary classification task on ImageNet. ","This paper proposes a data poisoning attack called label-flipping attacks, which attacks deep features in linear classifiers. The authors propose a multi-class classification algorithm to mitigate label-Flipping attacks. The proposed approach is based on randomized smoothing, which improves test-time robustness. The paper shows that the proposed approach improves the certified accuracy of the baseline undefended classifier on the Dogfish binary classification task on ImageNet. "
12000,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"Outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION Outlier detection. novelty detection USED-FOR anomaly detection. Outlier detection PART-OF anomaly detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. backdoor poisoning attacks USED-FOR machine learning models. Differential privacy USED-FOR aggregated analysis. dataset USED-FOR aggregated analysis. random noise USED-FOR It. differential privacy USED-FOR outlier detection. differential privacy USED-FOR novelty detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. extension USED-FOR poisoning samples. poisoning samples PART-OF backdoor attacks. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. novelty detection CONJUNCTION backdoor attack detection. backdoor attack detection CONJUNCTION novelty detection. differential privacy USED-FOR detection. differential privacy USED-FOR outlier detection. differential privacy USED-FOR backdoor attack detection. differential privacy USED-FOR novelty detection. OtherScientificTerm are distribution, Outliers, novelties, and outliers. Method is aggregation mechanism. ",This paper studies backdoor poisoning attacks against machine learning models. It proposes a new aggregation mechanism based on differential privacy that aggregates outlier detection and novelty detection with random noise. The authors also propose an extension to the poisoning samples in the backdoor attacks that allows for poisoning samples to be aggregated. The experiments show the effectiveness of the proposed aggregation mechanism. ,"This paper proposes a novel aggregation mechanism for outlier detection and novelty detection. It is based on the idea of differential privacy, which allows for the aggregation of outliers in the distribution. Differential privacy is used for aggregated analysis on a dataset with random noise. Outliers are defined as novelties that are not observed in the original distribution. The authors also propose an extension of the backdoor poisoning attacks for machine learning models to include poisoning samples. The paper is well-written and well-motivated. "
12004,SP:a5f0e531afd970144169823971d2d039bff752fb,"applications CONJUNCTION safety - critical ones. safety - critical ones CONJUNCTION applications. calibration of uncertainty prediction USED-FOR regression tasks. real - world systems FEATURE-OF regression tasks. definition USED-FOR regression uncertainty. reliability diagrams USED-FOR classification tasks. histogram - based approach USED-FOR classification tasks. definition CONJUNCTION evaluation method. evaluation method CONJUNCTION definition. reliability diagrams USED-FOR histogram - based approach. histogram - based approach USED-FOR evaluation method. synthetic, controlled problem CONJUNCTION object detection bounding - box regression task. object detection bounding - box regression task CONJUNCTION synthetic, controlled problem. Generic are method, and examples. OtherScientificTerm are uncertainty prediction, and empirical uncertainty. Method is scaling - based calibration. ","This paper proposes a new calibration of uncertainty prediction for regression tasks in real-world systems. The authors propose a new definition for regression uncertainty and a new evaluation method based on reliability diagrams for classification tasks. The proposed method is evaluated on a synthetic, controlled problem and an object detection bounding-box regression task. The results show the effectiveness of the proposed method. ","The paper proposes a new method for calibration of uncertainty prediction for regression tasks in real-world systems. The proposed method is based on scaling-based calibration. The authors provide a definition for regression uncertainty and an evaluation method based on reliability diagrams for classification tasks in the context of real-World systems. Experiments are conducted on a synthetic, controlled problem and an object detection bounding-box regression task. "
12008,SP:c422afd1df1ac98e23235830585dd0d45513064c,BERT HYPONYM-OF bidirectional Transformer language model. Tensor - Product Representations ( TPRs ) CONJUNCTION BERT. BERT CONJUNCTION Tensor - Product Representations ( TPRs ). structured - representational power CONJUNCTION Tensor - Product Representations ( TPRs ). Tensor - Product Representations ( TPRs ) CONJUNCTION structured - representational power. structured - representational power CONJUNCTION BERT. BERT CONJUNCTION structured - representational power. Tensor - Product Representations ( TPRs ) USED-FOR HUBERT1. BERT PART-OF HUBERT1. structured - representational power PART-OF HUBERT1. HUBERT COMPARE BERT. BERT COMPARE HUBERT. GLUE benchmark CONJUNCTION HANS dataset. HANS dataset CONJUNCTION GLUE benchmark. HANS dataset EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR model. general language structure USED-FOR untangling data - specific semantics. Material is NLP datasets. ,"This paper proposes a bidirectional Transformer language model called BERT, which is a variant of BERT. BERT combines Tensor-Product Representations (TPRs) and BERT with the structured-representational power of HUBERT1. The model is evaluated on the GLUE benchmark and the HANS dataset. The authors show that BERT outperforms BERT in terms of general language structure and untangling data-specific semantics.","This paper proposes a bidirectional Transformer language model called BERT, which is a variant of BERT. The model is based on Tensor-Product Representations (TPRs) and BERT with structured-representational power. The proposed model is evaluated on the GLUE benchmark and the HANS dataset. HUBERT1 outperforms BERT in terms of structured-Representational power and TPRs. The authors also show that the model is able to learn untangling data-specific semantics from the general language structure. "
12012,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,Multi - agent reinforcement learning HYPONYM-OF problem. communication CONJUNCTION centralized training. centralized training CONJUNCTION communication. particle - based agents USED-FOR cooperative and competitive environments. communication USED-FOR particle - based agents. centralized training USED-FOR particle - based agents. dynamics CONJUNCTION humanoid navigation strategies. humanoid navigation strategies CONJUNCTION dynamics. interaction CONJUNCTION dynamics. dynamics CONJUNCTION interaction. Multi - Agent Reinforcement Learning CONJUNCTION Hierarchical Reinforcement Learning. Hierarchical Reinforcement Learning CONJUNCTION Multi - Agent Reinforcement Learning. multi - agent models USED-FOR simulated humanoid navigation. decentralized methods USED-FOR learning. structure USED-FOR optimization problem. goal - conditioned policies USED-FOR low - level physical controllers. balance CONJUNCTION walking. walking CONJUNCTION balance. low - level physical controllers USED-FOR balance. low - level physical controllers USED-FOR walking. lower - level controllers CONJUNCTION higher - level policies. higher - level policies CONJUNCTION lower - level controllers. decentralized heterogeneous policies USED-FOR multi - agent goal - directed collision avoidance. goal conditioned policies USED-FOR decentralized heterogeneous policies. RL techniques USED-FOR policies. methods USED-FOR RL techniques. Method is partial parameter sharing approach. OtherScientificTerm is hierarchy. Task is multi - agent problem. Material is multi - agent pursuit environment. ,"This paper studies the problem of multi-agent reinforcement learning, which is a well-studied problem in the literature. The authors propose a partial parameter sharing approach to solve the problem. The main idea is to use communication between particle-based agents in cooperative and competitive environments, where the dynamics and humanoid navigation strategies are shared across agents. Hierarchical Reinforcement Learning and Multi-Agent Reinforcement learning are used to train the models. The learning is done using decentralized methods. The optimization problem is formulated as a structure where the goal conditioned policies for low-level physical controllers and the higher-level policies for the lower-level controllers are shared. The policies are trained using RL techniques.  The authors show that the multi-adversarial pursuit environment can be solved using decentralized heterogeneous policies for multi-Agent goal-directed collision avoidance. ","This paper studies the multi-agent reinforcement learning problem, which is an important problem in reinforcement learning. The authors propose a partial parameter sharing approach to solve the problem. The main idea is to use particle-based agents for cooperative and competitive environments with communication and centralized training. The paper also proposes Hierarchical Reinforcement Learning to learn the hierarchy of the agents. The learning is done using decentralized methods for learning the structure of the optimization problem. Experiments are conducted on simulated humanoid navigation using multi- agent models, where the dynamics and humanoid navigation strategies are combined with dynamics and interaction. The results show that the proposed policies can be learned using RL techniques, and can be combined with low-level physical controllers for balance and walking, and higher-level controllers for goal-directed collision avoidance."
12016,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"spatial convolution layer USED-FOR Graph Neural Networks ( GNNs ). spatial convolution layer USED-FOR feature vector. convolution layer USED-FOR nodes. convolution layer USED-FOR feature vectors. continuous feature space FEATURE-OF feature vectors. GNN USED-FOR graphs. convolution layers USED-FOR local structures. solution USED-FOR GNNs. spatial representation of the graph USED-FOR approach. point - cloud representation of the graph USED-FOR spatial representation. graph embedding method USED-FOR spatial representation. GNN USED-FOR topological structure. local feature extractor USED-FOR GNN. approach USED-FOR local feature extractor. spatial distribution of the locally extracted feature vectors USED-FOR GNN. spatial distribution of the locally extracted feature vectors USED-FOR topological structure. spatial representation USED-FOR graph down - sampling problem. pooling method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE pooling method. Method are neural network, and graph pooling method. OtherScientificTerm is graph. ","This paper proposes a spatial convolution layer for Graph Neural Networks (GNNs) to learn the feature vector of each node in a continuous feature space. The proposed approach is based on the spatial representation of the graph in the form of a point-cloud representation. The spatial representation is learned by a graph embedding method, which is then used to extract the topological structure of a GNN from the spatial distribution of the locally extracted feature vectors of the GNN. The local feature extractor is then applied to the local features extracted by the local GNN in order to generate the final graph. The authors show that the proposed solution can be applied to a wide range of GNNs.","This paper proposes a spatial convolution layer for Graph Neural Networks (GNNs). The proposed approach is based on the spatial representation of the graph, which is a point-cloud representation of a graph. The spatial representation is learned by a graph embedding method. The neural network is trained with a local feature extractor to extract the feature vectors from the continuous feature space. The local structures of the convolution layers are learned by learning the local structures. The GNN is then used to predict the topological structure of the GNN. The proposed graph pooling method is shown to outperform state-of-the-art methods in the graph down-sampling problem."
12020,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,it USED-FOR image processing. shift - equivalent prior of images USED-FOR Convolutional layer. max - pooling CONJUNCTION averagepooling. averagepooling CONJUNCTION max - pooling. averagepooling CONJUNCTION stride convolution. stride convolution CONJUNCTION averagepooling. down sampling methods USED-FOR convolutional neural networks ( CNNs ). max - pooling HYPONYM-OF convolutional neural networks ( CNNs ). stride convolution HYPONYM-OF down sampling methods. max - pooling HYPONYM-OF down sampling methods. averagepooling HYPONYM-OF down sampling methods. down sampling USED-FOR shift - equivalent. image classifications EVALUATE-FOR frequency pooling. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. CNNs EVALUATE-FOR frequency pooling. accuracy EVALUATE-FOR frequency pooling. robustness EVALUATE-FOR frequency pooling. ,"This paper proposes a new method for training convolutional neural networks (CNNs) with shift-equivalent prior of images. The method is based on the idea of down-sampling, which is a popular technique in image processing. The authors show that it can be used to improve the performance of CNNs in terms of accuracy, robustness, and transferability. The paper also shows that the method can be combined with other down sampling methods, such as max-pooling and averagepooling, as well as stride convolution.","This paper proposes a new method for training convolutional neural networks with shift-equivalent prior of images for image processing. The authors propose two down sampling methods, max-pooling and averagepooling, to improve the performance of the Convolutional layer. The main contribution of the paper is the use of down sampling for the shift-approximate prior of the images. The paper shows that frequency pooling on image classifications improves the accuracy and robustness of the CNNs."
12024,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"high - dimensional state spaces USED-FOR they. images HYPONYM-OF high - dimensional state spaces. technique USED-FOR deep RL agents. technique USED-FOR generalization ability. generalization ability EVALUATE-FOR deep RL agents. randomized ( convolutional ) neural network USED-FOR technique. Monte Carlo approximation USED-FOR inference method. 2D CoinRun CONJUNCTION 3D DeepMind Lab exploration. 3D DeepMind Lab exploration CONJUNCTION 2D CoinRun. 3D DeepMind Lab exploration CONJUNCTION 3D robotics control tasks. 3D robotics control tasks CONJUNCTION 3D DeepMind Lab exploration. it COMPARE regularization and data augmentation methods. regularization and data augmentation methods COMPARE it. 3D robotics control tasks EVALUATE-FOR method. 3D DeepMind Lab exploration EVALUATE-FOR method. 2D CoinRun EVALUATE-FOR method. Method is Deep reinforcement learning ( RL ) agents. Generic are agents, and It. OtherScientificTerm are robust features, varied and randomized environments, and randomization. ","This paper proposes a new technique for training deep RL agents in high-dimensional state spaces (e.g., images) with robust features. Deep reinforcement learning (RL) agents are trained to learn to generalize well in the presence of diverse and randomized environments. The technique is based on a randomized (convolutional) neural network. The inference method uses Monte Carlo approximation. The authors show that the technique improves the generalization ability of deep RL agent in terms of the robustness of the agents. It is also shown that it outperforms existing regularization and data augmentation methods on 2D CoinRun and 3D DeepMind Lab exploration. ","This paper proposes a novel technique to improve the generalization ability of deep RL agents in high-dimensional state spaces (e.g., images). The authors propose to use a randomized (convolutional) neural network to train the agents, and then use a Monte Carlo approximation for the inference method. It is shown that the proposed method outperforms existing regularization and data augmentation methods on 2D CoinRun and 3D DeepMind Lab exploration. The authors also show that the method is more robust to robust features in varied and randomized environments. "
12028,SP:31772a9122ec998c7c829bc4813f6147cdc30145,"image classification CONJUNCTION visual question answering. visual question answering CONJUNCTION image classification. models USED-FOR tasks. visual question answering HYPONYM-OF tasks. image classification HYPONYM-OF tasks. explanation approach USED-FOR image similarity models. score USED-FOR model. saliency map USED-FOR explanation method. saliency map USED-FOR image regions. explanations USED-FOR attribute recognition. diverse domains FEATURE-OF datasets. Polyvore Outfits HYPONYM-OF datasets. datasets EVALUATE-FOR approach. Polyvore Outfits HYPONYM-OF diverse domains. Method are deep learning model, and saliency maps. Task is classification. Generic are task, and methods. ",This paper proposes a new explanation approach for image similarity models for two tasks: image classification and visual question answering. The authors propose a deep learning model that uses a saliency map as a score to evaluate the performance of the model. The proposed approach is evaluated on two datasets: Polyvore Outfits and CIFAR-10. The results show that the proposed approach outperforms existing methods on both datasets. ,"This paper proposes an explanation approach for image similarity models. The authors propose a deep learning model that uses saliency maps to predict the attributes of an image. The model is trained on a set of datasets with diverse domains (e.g., Polyvore Outfits, image classification, visual question answering). The proposed explanation method uses a saliency map to map image regions to attribute recognition. The proposed approach is evaluated on three datasets."
12032,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,WaveFlow HYPONYM-OF small - footprint generative flow. auxiliary losses USED-FOR Parallel WaveNet. small - footprint generative flow USED-FOR raw audio. flowbased models USED-FOR raw audio. autoregressive flow CONJUNCTION bipartite flow. bipartite flow CONJUNCTION autoregressive flow. WaveNet HYPONYM-OF autoregressive flow. autoregressive flow PART-OF flowbased models. WaveGlow HYPONYM-OF bipartite flow. test likelihood CONJUNCTION speech fidelity. speech fidelity CONJUNCTION test likelihood. likelihood - based generative models USED-FOR raw waveforms. test likelihood EVALUATE-FOR likelihood - based generative models. speech fidelity EVALUATE-FOR likelihood - based generative models. WaveFlow USED-FOR high - fidelity speech. WaveFlow COMPARE WaveNet. WaveNet COMPARE WaveFlow. likelihood EVALUATE-FOR WaveFlow. small - footprint WaveFlow COMPARE real - time. real - time COMPARE small - footprint WaveFlow. GPU without engineered inference kernels USED-FOR real - time. Method is maximum likelihood. ,"This paper proposes WaveFlow, a small-footprint generative flow for raw audio with auxiliary losses. WaveFlow is a combination of the autoregressive flow and bipartite flow such as WaveNet and WaveGlow. The authors show that WaveFlow outperforms the state-of-the-art in terms of test likelihood, speech fidelity, and high-fidelity speech. They also show that the maximum likelihood of WaveFlow improves over WaveNet in real-time with GPU without engineered inference kernels. ","The paper proposes a small-footprint generative flow, WaveFlow, for raw audio. WaveFlow is a combination of autoregressive flow and bipartite flow (WaveNet, WaveGlow, Parallel WaveNet) with auxiliary losses. The authors show that WaveFlow outperforms other likelihood-based generative models in terms of test likelihood and speech fidelity for raw waveforms. They also show that in real-time with GPU without engineered inference kernels WaveFlow performs better than WaveNet."
12036,SP:963e85369978dddcd9e3130bc11453696066bbf3,"GT - GAN USED-FOR translation mapping. graph translator USED-FOR translation mapping. graph convolution and deconvolution layers USED-FOR translation mapping. graph convolution and deconvolution layers PART-OF graph translator. graph convolution and deconvolution layers PART-OF GT - GAN. graph translator PART-OF GT - GAN. GT - GAN COMPARE baseline methods. baseline methods COMPARE GT - GAN. synthetic and realworld datasets EVALUATE-FOR GT - GAN. synthetic and realworld datasets EVALUATE-FOR baseline methods. scalability EVALUATE-FOR GT - GAN. effectiveness EVALUATE-FOR GT - GAN. scalability EVALUATE-FOR baseline methods. effectiveness EVALUATE-FOR baseline methods. GT - GAN COMPARE GraphRNN. GraphRNN COMPARE GT - GAN. GraphRNN CONJUNCTION RandomVAE. RandomVAE CONJUNCTION GraphRNN. GT - GAN COMPARE RandomVAE. RandomVAE COMPARE GT - GAN. Method are Deep graph generation models, unconditioned generative models, and conditional graph discriminator. OtherScientificTerm is global and local features. ",This paper proposes a graph translator for translation mapping based on graph convolution and deconvolution layers in GT-GAN. The authors propose a conditional graph discriminator to learn the global and local features of each node in the graph. They show that GT-GAN achieves better scalability than baseline methods on both synthetic and realworld datasets.    The authors also show that the performance is comparable to GraphRNN and RandomVAE. ,"This paper proposes a graph translator for translation mapping using graph convolution and deconvolution layers. Deep graph generation models can be seen as unconditioned generative models. The authors propose a conditional graph discriminator, where the global and local features are encoded in a single graph. The proposed GT-GAN is an extension of the graph translator. The paper shows that GT-GAN outperforms the baseline methods on both synthetic and realworld datasets. The effectiveness of the proposed GAN is evaluated on both scalability and accuracy. The experimental results show that the proposed GraphRNN and RandomVAE outperform GT- GAN."
12040,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"METAGROSS ( Meta Gated Recursive Controller ) HYPONYM-OF neural sequence modeling unit. gating mechanisms PART-OF METAGROSS. inductive bias USED-FOR learning. hierarchically - structured sequence data USED-FOR learning. language HYPONYM-OF hierarchically - structured sequence data. code generation CONJUNCTION machine translation. machine translation CONJUNCTION code generation. tree traversal CONJUNCTION logical inference. logical inference CONJUNCTION tree traversal. sequential pixel - by - pixel classification CONJUNCTION semantic parsing. semantic parsing CONJUNCTION sequential pixel - by - pixel classification. sorting CONJUNCTION tree traversal. tree traversal CONJUNCTION sorting. semantic parsing CONJUNCTION code generation. code generation CONJUNCTION semantic parsing. machine translation CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION machine translation. logical inference CONJUNCTION sequential pixel - by - pixel classification. sequential pixel - by - pixel classification CONJUNCTION logical inference. tasks EVALUATE-FOR approach. polyphonic music modeling HYPONYM-OF recursive logic tasks. sequential pixel - by - pixel classification HYPONYM-OF recursive logic tasks. machine translation HYPONYM-OF recursive logic tasks. logical inference HYPONYM-OF recursive logic tasks. code generation HYPONYM-OF recursive logic tasks. semantic parsing HYPONYM-OF recursive logic tasks. sorting HYPONYM-OF recursive logic tasks. tree traversal HYPONYM-OF recursive logic tasks. Generic is unit. OtherScientificTerm are gating functions, and meta - gating. Method is recurrent model. ","This paper proposes METAGROSS (Meta Gated Recursive Controller), a neural sequence modeling unit based on gating mechanisms. The proposed unit is based on the inductive bias in learning from hierarchically-structured sequence data (e.g., language). The authors show that this inductive biases can be used to improve the performance of the proposed unit. The authors also show that meta-gating can improve the generalization ability of the recurrent model. Finally, the proposed approach is evaluated on a variety of tasks including code generation, machine translation, semantic parsing, sequential pixel-by-pixel classification, tree traversal, and logical inference.","This paper introduces METAGROSS (Meta Gated Recursive Controller) which is a neural sequence modeling unit. The proposed unit is based on gating mechanisms that are based on the inductive bias in learning on hierarchically-structured sequence data (e.g., language). The authors show that meta-gating can be applied to any recurrent model. The authors also show that the proposed approach can be used to perform a variety of tasks including code generation, machine translation, semantic parsing, tree traversal, and sequential pixel-by-pixel classification. "
12044,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,local prior matching ( LPM ) HYPONYM-OF self - supervised objective. self - supervised objective USED-FOR speech recognition. language model USED-FOR learning signal. unlabeled speech USED-FOR learning signal. language model USED-FOR LPM objective. unpaired text CONJUNCTION speech. speech CONJUNCTION unpaired text. it USED-FOR LPM. unpaired text USED-FOR it. speech USED-FOR it. unpaired text USED-FOR LPM. speech USED-FOR LPM. language model USED-FOR LPM. model USED-FOR LPM. unlabeled data USED-FOR model. labeled speech USED-FOR model. WER EVALUATE-FOR LPM. clean and noisy test set EVALUATE-FOR LPM. test sets EVALUATE-FOR fully supervised model. WER EVALUATE-FOR fully supervised model. noisy test set EVALUATE-FOR WER. WER EVALUATE-FOR LPM. noisy data USED-FOR LPM. Method is self - supervised approach. ,"This paper proposes a self-supervised objective for speech recognition, called local prior matching (LPM). The LPM objective is based on a language model that predicts the learning signal from unlabeled speech and unpaired text. The model is trained on labeled speech, and it is used to train the LPM. The authors demonstrate the effectiveness of the proposed LPM on a clean and noisy test set, and a fully supervised model on WER.","This paper proposes a self-supervised approach for speech recognition. The main idea is to use local prior matching (LPM) which is an extension of the self supervised objective in speech recognition to the case of unpaired text and speech. The LPM objective is based on a language model that predicts the learning signal from unlabeled speech and speech, and it is then used to train the LPM. The model is trained on labeled speech and unpaired speech. Experiments on WER and a clean and noisy test set show that the fully supervised model can outperform the LMP on both test sets."
12048,SP:e6af249608633f1776b608852a00946a5c09a357,"data poisoning USED-FOR fair and robust model training. FR - GAN USED-FOR fair and robust model training. generative adversarial networks ( GANs ) USED-FOR FR - GAN. generative adversarial networks ( GANs ) USED-FOR fair and robust model training. fairness discriminator CONJUNCTION robustness discriminator. robustness discriminator CONJUNCTION fairness discriminator. robustness discriminator HYPONYM-OF discriminators. fairness discriminator HYPONYM-OF discriminators. disparate impact CONJUNCTION equalized odds. equalized odds CONJUNCTION disparate impact. equalized odds CONJUNCTION equal opportunity. equal opportunity CONJUNCTION equalized odds. fairness measures FEATURE-OF framework. disparate impact HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. equal opportunity HYPONYM-OF fairness measures. FR - GAN USED-FOR fairness. FR - GAN COMPARE fairness methods. fairness methods COMPARE FR - GAN. fairness CONJUNCTION accuracy. accuracy CONJUNCTION fairness. accuracy EVALUATE-FOR FR - GAN. fairness EVALUATE-FOR FR - GAN. accuracy CONJUNCTION fairness. fairness CONJUNCTION accuracy. fairness EVALUATE-FOR FR - GAN. accuracy EVALUATE-FOR FR - GAN. OtherScientificTerm is bias. Method are model fairness techniques, and generator. Material is validation set. ","This paper studies the problem of data poisoning in fair and robust model training. The authors propose a new model fairness techniques, called FR-GAN, which uses generative adversarial networks (GANs) to train a FR-GAN to discriminate between the fairness and robustness of a model. The discriminators are a combination of two discriminators, a fairness discriminator and a robustness discriminator. The proposed framework is evaluated on a variety of fairness measures, including disparate impact, equalized odds, and equal opportunity. The results show that the proposed model fairness methods outperform existing fairness methods in terms of accuracy and fairness.","This paper proposes a new framework for adversarial training of generative adversarial networks (GANs) against data poisoning for fair and robust model training. The framework consists of two discriminators: a fairness discriminator and a robustness discriminator. The discriminators are trained using model fairness techniques. The generator is trained on the validation set. The authors show that the proposed framework is able to improve fairness measures such as disparate impact, equalized odds, and equal opportunity. They also show that FR-GAN outperforms other fairness methods on fairness and accuracy."
12052,SP:6306417f5a300629ec856495781515c6af05a363,"physics - inspired deep learning approach USED-FOR point cloud processing. static background grid CONJUNCTION Lagrangian material space. Lagrangian material space CONJUNCTION static background grid. moving particles USED-FOR Lagrangian material space. Lagrangian material space USED-FOR learning architecture. moving particles USED-FOR learning architecture. static background grid USED-FOR learning architecture. Eulerian - Lagrangian representation USED-FOR particle features. generalized, high - dimensional force field USED-FOR flow velocities. generalized, high - dimensional force field USED-FOR particle features. flow velocities USED-FOR particle features. point cloud classification and segmentation problems EVALUATE-FOR system. geometric machine learning CONJUNCTION physical simulation. physical simulation CONJUNCTION geometric machine learning. PIC / FLIP scheme USED-FOR natural flow. Task is natural flow phenomena in fluid mechanics. OtherScientificTerm are Eulerian world space, and geometric reservoir. ","This paper proposes a physics-inspired deep learning approach for point cloud processing. The learning architecture is based on moving particles in a Lagrangian material space and a static background grid. The particle features are represented by an Eulerian-Lagrangian representation. The flow velocities of the particle features can be represented by a generalized, high-dimensional force field. The system is evaluated on point cloud classification and segmentation problems. The PIC/FLIP scheme is shown to capture natural flow phenomena in fluid mechanics. Experiments are conducted on geometric machine learning and physical simulation.","This paper proposes a physics-inspired deep learning approach for point cloud processing. The authors propose a learning architecture that combines a static background grid and a Lagrangian material space with moving particles. The particle features are represented by an Eulerian-Lagrangian representation, and the flow velocities are modeled by a generalized, high-dimensional force field. The system is evaluated on point cloud classification and segmentation problems, and is shown to outperform geometric machine learning and physical simulation. The paper also proposes a PIC/FLIP scheme to capture natural flow phenomena in fluid mechanics."
12056,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"clipping USED-FOR dynamics of iterates. local minimum FEATURE-OF rate of convergence. clipping COMPARE vanilla gradient descent. vanilla gradient descent COMPARE clipping. clipping USED-FOR noise. lens USED-FOR gradient clipping. robustness HYPONYM-OF lens. label noise FEATURE-OF classification. robustness EVALUATE-FOR gradient clipping. gradient clipping USED-FOR label noise. Method are Gradient clipping, deep networks, and optimisation lens. OtherScientificTerm are loss function, and cross - entropy loss. ","This paper studies the dynamics of iterates in deep networks. Gradient clipping is a popular technique to reduce the noise in the loss function of deep networks, but it is not well-studied in the literature. The authors propose a lens called gradient clipping, which is an extension of the lens in vanilla gradient descent. They show that gradient clipping can reduce the label noise in classification by reducing the cross-entropy loss. They also show that the local minimum of convergence of the rate of convergence is also reduced by gradient clipping. Finally, they provide a theoretical analysis of the robustness of gradient clipping under label noise.",This paper studies the dynamics of iterates in deep networks. Gradient clipping is a technique to reduce the loss function of deep networks to a local minimum of the rate of convergence. The main idea is to use the optimisation lens of the lens to minimize the cross-entropy loss. The authors show that gradient clipping is robust to label noise in classification. They also show that clipping is more robust to noise than vanilla gradient descent.
12060,SP:414b06d86e132357a54eb844036b78a232571301,they USED-FOR imitating actions. imitation learning methods USED-FOR imitating actions. state alignment based imitation learning method USED-FOR imitator. expert demonstrations FEATURE-OF state sequences. them PART-OF reinforcement learning framework. local and global perspectives USED-FOR state alignment. regularized policy update objective USED-FOR them. regularized policy update objective USED-FOR reinforcement learning framework. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings EVALUATE-FOR method. imitation learning settings EVALUATE-FOR method. Task is imitation learning problem. Method is dynamics models. ,"This paper proposes a new state alignment based imitation learning method for imitator. The key idea is to learn a set of state sequences from expert demonstrations, and then use them to train a reinforcement learning framework with a regularized policy update objective. The proposed method is evaluated on a variety of imitation learning settings and demonstrates its effectiveness.","This paper proposes a state alignment based imitation learning method for imitator. The idea is to use imitation learning methods to learn imitating actions from expert demonstrations. They use them in a reinforcement learning framework with a regularized policy update objective to learn them from both local and global perspectives. The method is evaluated on a variety of imitation learning settings, including imitation learning with dynamics models."
12064,SP:91761d68086330ce378507c152e72218ed7b2196,"Stochastic gradient descent ( SGD ) USED-FOR deep neural networks. deep gradient boosting ( DGB ) HYPONYM-OF SGD. pseudo - residual targets USED-FOR gradient boosting problem. chain rule USED-FOR back - propagated gradients. boosting problem USED-FOR weight update. linear base learner USED-FOR boosting problem. normalization procedure USED-FOR weight update formula. architecture COMPARE architecture. architecture COMPARE architecture. image recognition tasks EVALUATE-FOR architecture. input normalization layer ( INN ) USED-FOR architecture. normalization layers PART-OF architecture. image recognition tasks EVALUATE-FOR architecture. batch normalization ( BN ) COMPARE INN. INN COMPARE batch normalization ( BN ). CIFAR10 CONJUNCTION ImageNet classification tasks. ImageNet classification tasks CONJUNCTION CIFAR10. ImageNet classification tasks EVALUATE-FOR it. CIFAR10 EVALUATE-FOR it. Method are neural network, and DGB. OtherScientificTerm are intrinsic generalization properties, and forward pass. ","This paper studies the problem of stochastic gradient descent (SGD) for deep neural networks. The authors propose a new SGD called deep gradient boosting (DGB) which uses pseudo-residual targets to solve the gradient boosting problem with pseudo-regret targets. The boosting problem is formulated as a weight update with a linear base learner, where the weight update formula is computed using a normalization procedure. The key idea is to use a chain rule to predict the back-propagated gradients of the neural network, which is then used as the intrinsic generalization properties of the DGB. The proposed architecture is based on the input normalization layer (INN) and the normalization layers in the architecture. Experiments on CIFAR10 and ImageNet classification tasks show that the proposed architecture performs better than batch normalization (BN) on most of the image recognition tasks.","This paper proposes a new algorithm for stochastic gradient descent (SGD) for deep neural networks. The authors propose deep gradient boosting (DGB) which is a variant of SGD with pseudo-reinforcement targets. The key idea of DGB is to use a linear base learner to solve the boosting problem, where the weight update formula is computed using a normalization procedure. This normalization layer (INN) consists of two normalization layers, one for the intrinsic generalization properties of the neural network, and one for back-propagated gradients of the chain rule. The proposed architecture is evaluated on CIFAR10 and ImageNet classification tasks, showing that it outperforms batch normalization (BN) on most image recognition tasks."
12068,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"Differentiable architecture search ( DARTS ) USED-FOR network architectures. reduced memory cost USED-FOR PC - DARTS. training stability EVALUATE-FOR PC - DARTS. batch size USED-FOR PC - DARTS. GPU - days USED-FOR search. top-1 error rate EVALUATE-FOR search. ImageNet EVALUATE-FOR top-1 error rate. Metric are memory and computing overheads, and error rate. OtherScientificTerm are super - network, edges of super - net, and edge - level parameters. Generic are approach, strategy, method, and code. Method are Partially - Connected DARTS, operation search, edge normalization, and architecture search. Material is CIFAR10. ","This paper proposes a differentiable architecture search (DARTS) for network architectures with memory and computing overheads. The approach is based on Partially-Connected DARTS, where the super-network is connected to the edges of super-net, and the edge-level parameters are learned by operation search. The search is done on GPU-days, with a reduced memory cost. The authors show that the top-1 error rate of the search on ImageNet is better than PC-DARSTS with the same batch size. The proposed method is evaluated on CIFAR10.","The paper proposes a differentiable architecture search (DARTS) for network architectures. The approach is based on Partially-Connected DARTS, where the super-network is a super-net and the edges of super-nets are a set of edge-level parameters. The authors propose a new strategy, called architecture search, which is a combination of operation search, edge normalization, and architecture search on GPU-days. The proposed search is evaluated on CIFAR10 and on ImageNet. The paper shows that PC-DARSTS with reduced memory cost leads to better training stability and better top-1 error rate for search. "
12072,SP:724870046e990376990ba9f73d63d331f61788d7,"control strategies USED-FOR complex control tasks. model - predictive control ( MPC ) CONJUNCTION trajectory optimization. trajectory optimization CONJUNCTION model - predictive control ( MPC ). Model - based control algorithms USED-FOR control tasks. gradients of underlying system dynamics USED-FOR control tasks. gradients of underlying system dynamics USED-FOR Model - based control algorithms. sample efficiency EVALUATE-FOR control tasks. trajectory optimization HYPONYM-OF Model - based control algorithms. model - predictive control ( MPC ) HYPONYM-OF Model - based control algorithms. gradient - based numerical optimization methods COMPARE model - based control methods. model - based control methods COMPARE gradient - based numerical optimization methods. sampling USED-FOR solution space. gradient - based methods CONJUNCTION DRL. DRL CONJUNCTION gradient - based methods. gradient - based methods USED-FOR hybrid method. DRL USED-FOR hybrid method. true gradients USED-FOR convergence rate. convergence rate FEATURE-OF actor. differentiable physical simulator USED-FOR true gradients. convergence rate EVALUATE-FOR modification. differentiable physical simulator USED-FOR modification. true gradients USED-FOR modification. deep deterministic policy gradients ( DDPG ) algorithm USED-FOR algorithm. differentiable half cheetah HYPONYM-OF one. 2D robot control tasks EVALUATE-FOR algorithm. hard contact constraints FEATURE-OF differentiable half cheetah. method USED-FOR DDPG. robustness EVALUATE-FOR method. OtherScientificTerm are initialization, and local minima. Method is Deep reinforcement learning ( DRL ). Metric is computational cost. ",This paper studies the problem of learning control strategies for complex control tasks. Model-based control algorithms such as model-predictive control (MPC) and trajectory optimization are commonly used for control tasks with gradients of underlying system dynamics. The authors propose a hybrid method based on gradient-based methods and DRL. The proposed method is based on deep deterministic policy gradients (DDPG) algorithm. The key idea of the method is to use a differentiable physical simulator to learn the true gradients for modification and the convergence rate of the actor. The method is evaluated on 2D robot control tasks and shows improved robustness.,"This paper proposes a new control strategies for complex control tasks. Model-based control algorithms such as model-predictive control (MPC), trajectory optimization, and trajectory optimization are commonly used for control tasks with gradients of underlying system dynamics. The authors propose a hybrid method that combines gradient-based methods and DRL. The main idea is to use Deep reinforcement learning (DRL) to learn the initialization, and then use sampling to find the solution space in which the local minima are close to the true gradients. The convergence rate of the actor is the convergence rate in the actor. The proposed algorithm is based on the deep deterministic policy gradients (DDPG) algorithm. The method is evaluated on 2D robot control tasks, and the proposed method is shown to outperform DDPG in terms of robustness and computational cost. "
12076,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"task - agnostic world graphs USED-FOR complex environment. nodes PART-OF world graph. hierarchical RL framework USED-FOR exploration. world graph nodes CONJUNCTION edges. edges CONJUNCTION world graph nodes. structural and connectivity knowledge USED-FOR exploration. trajectory data USED-FOR binary recurrent variational autoencoder ( VAE ). world graph USED-FOR structural and connectivity knowledge. learning phases PART-OF framework. structural and connectivity knowledge USED-FOR hierarchical RL framework. hierarchical RL framework HYPONYM-OF learning phases. world graphs USED-FOR RL. reward CONJUNCTION learning. learning CONJUNCTION reward. maze tasks EVALUATE-FOR approach. OtherScientificTerm are complex environments, and agents. Method is reinforcement learning ( RL ) agents. ","This paper proposes a new task-agnostic world graphs for the complex environment. The authors propose a hierarchical RL framework for exploration using structural and connectivity knowledge between nodes in the world graph and edges in the edges of a world graph. They use trajectory data to train a binary recurrent variational autoencoder (VAE) on the trajectory data. The proposed framework consists of two learning phases: (1) learning the structure of the learned world graph, and (2) reinforcement learning (RL) agents. Experiments on maze tasks demonstrate the effectiveness of the proposed approach.","This paper proposes a new task-agnostic world graphs for the complex environment. The authors propose a hierarchical RL framework for exploration in the context of the world graph with nodes in a world graph and edges in a complex environment, where agents are trained in complex environments. The framework consists of two learning phases, one of which is a binary recurrent variational autoencoder (VAE) based on trajectory data, and the other one is a reinforcement learning (RL) agents based on the structural and connectivity knowledge. The proposed approach is evaluated on maze tasks. The results show that the proposed approach outperforms the state-of-the-art in terms of reward and learning."
12080,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,Neural networks USED-FOR real - world tasks. neural network USED-FOR approximation of any type of continuous functions. non - interpretable black box model USED-FOR neural network. white box network HYPONYM-OF function constructing network. network USED-FOR function blocks. discretized layers USED-FOR network. discretization USED-FOR end - to - end PathNet structure. OtherScientificTerm is continuous functions. Task is reverse engineering. Method is neural networks. ,"This paper proposes a new function constructing network, called white box network, which is based on a non-interpretable black box model. The main idea is to use a neural network to approximate the approximation of any type of continuous functions. The network is trained by discretizing the function blocks into a set of discretized layers. The end-to-end PathNet structure is learned using discretization. The proposed network is evaluated on a variety of real-world tasks.","This paper proposes a new function constructing network, called the white box network, which is a non-interpretable black box model for real-world tasks. The authors propose a neural network for approximation of any type of continuous functions. The network consists of two discretized layers, one for each of the continuous functions, and one for the function blocks. The end-to-end PathNet structure is based on discretization, and the authors show that the proposed neural networks can be used for reverse engineering."
12084,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"approach USED-FOR control policies. Imitation learning algorithms USED-FOR control policies. Imitation learning algorithms USED-FOR approach. supervised learning methods USED-FOR control policies. supervised imitation learning USED-FOR policies. imitation learning USED-FOR policies. algorithm USED-FOR behaviors. user - provided reward functions CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION user - provided reward functions. user - provided reward functions USED-FOR algorithm. method USED-FOR goal - reaching policies. approach USED-FOR method. approach USED-FOR goal - reaching policies. approach USED-FOR imitation learning settings. self - supervised imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION self - supervised imitation learning. it COMPARE reinforcement learning methods. reinforcement learning methods COMPARE it. reinforcement learning methods USED-FOR goal reaching problems. goal reaching problems EVALUATE-FOR it. OtherScientificTerm are expert demonstrator, expert demonstrations, and demonstrations. Task is multi - task setting. Method is suboptimal policy. Generic is tasks. ","This paper proposes a new approach to learn control policies using Imitation learning algorithms. The authors propose a method to learn goal-reaching policies using supervised imitation learning. The goal is to learn a suboptimal policy from expert demonstrations, where the expert demonstrator is trained on a set of tasks. The policies are learned by imitation learning using the learned policies from the expert demonstrations. The proposed algorithm is able to learn the behaviors of the policies using imitation learning and reinforcement learning methods. Experiments show that the proposed approach can achieve state-of-the-art performance in several imitation learning settings, including self-supervised imitation learning as well as reinforcement learning. ","This paper proposes a new approach to learn control policies using Imitation learning algorithms. The approach is based on supervised learning methods to learn the control policies. The goal is to learn a suboptimal policy in a multi-task setting, where the expert demonstrator is given a set of expert demonstrations, and the policies are learned using imitation learning. The authors propose an algorithm to learn behaviors using user-provided reward functions and reinforcement learning methods. The proposed method is evaluated on goal-reaching policies and it is shown to outperform other self-supervised imitation learning, reinforcement learning, and other imitation learning settings. "
12088,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"Zeno++ USED-FOR non - convex problems. Zeno++ COMPARE approaches. approaches COMPARE Zeno++. OtherScientificTerm are workerserver communications, Byzantine workers, candidate gradient, optimization progress, and Byzantine failures. ","This paper studies the problem of non-convex problems with workerserver communications. The authors propose a new method, Zeno++, to solve the problem with Byzantine workers. The main idea is to use the candidate gradient of the Byzantine workers as a candidate gradient, and then use this candidate gradient to optimize the optimization progress of the workerserver. The paper shows that the proposed method outperforms existing approaches. ","This paper proposes a new approach to solve non-convex problems with workerserver communications. The main idea of the paper is to use Byzantine workers as the candidate gradient, and then use Zeno++ as the optimization progress. The authors show that the proposed approach outperforms existing approaches on a number of tasks. "
12092,SP:d16ed9bd4193d99774840783347137e938955b87,"deep neural networks ( DNNs ) HYPONYM-OF Machine learning models. defenses USED-FOR adversarial impact. Lp norm USED-FOR adversarial perturbations. unrestricted ” perturbations USED-FOR image - based visual descriptors. feature squeezing CONJUNCTION adversarially trained model. adversarially trained model CONJUNCTION feature squeezing. JPEG compression CONJUNCTION feature squeezing. feature squeezing CONJUNCTION JPEG compression. semantically aware perturbations USED-FOR JPEG compression. semantically aware perturbations USED-FOR adversarially trained model. semantically aware perturbations USED-FOR feature squeezing. ImageNet CONJUNCTION MSCOCO. MSCOCO CONJUNCTION ImageNet. image classification CONJUNCTION image captioning tasks. image captioning tasks CONJUNCTION image classification. methods USED-FOR image captioning tasks. methods USED-FOR image classification. complex datasets USED-FOR methods. complex datasets USED-FOR image captioning tasks. MSCOCO HYPONYM-OF complex datasets. ImageNet HYPONYM-OF complex datasets. OtherScientificTerm are adversarial examples, and large magnitude perturbations. Method is user studies. Material is semantic adversarial examples. Generic is attacks. ","This paper studies the problem of adversarial impact on deep neural networks (DNNs) in the context of user studies. Machine learning models have been shown to be vulnerable to adversarial perturbations, and defenses against these attacks have been proposed in the literature. In this paper, the authors focus on the case of semantic adversarial examples, where the target image-based visual descriptors are “unrestricted”. The authors show that the Lp norm of the target images can be used as a proxy to measure the robustness of the model against the adversarial attacks. The paper also shows that the “large magnitude perturbation” of the source image can also be used to improve the performance of the adversarially trained model, feature squeezing, and JPEG compression. The proposed methods are evaluated on several complex datasets, including ImageNet, MSCOCO, and image captioning tasks. ","This paper studies the adversarial impact of deep neural networks (DNNs) on image-based visual descriptors. The authors propose two defenses against adversarial impacts. First, the authors introduce a Lp norm for adversarial perturbations. Second, they introduce a “unrestricted” perturbation to the “generalization” of adversarial examples. The paper also presents user studies to demonstrate the effectiveness of the proposed attacks. The proposed methods are evaluated on image classification and image captioning tasks on complex datasets such as ImageNet, MSCOCO, and JPEG compression. "
12096,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,large datasets USED-FOR latent representations. neural networks USED-FOR latent representations. large datasets USED-FOR neural networks. events CONJUNCTION locations. locations CONJUNCTION events. natural language understanding ( NLU ) system USED-FOR emerging entities. RL trainable controller USED-FOR representation learning. representation learning PART-OF neural encoder. neural encoder CONJUNCTION memory management role. memory management role CONJUNCTION neural encoder. controller USED-FOR read and write operations. external memory USED-FOR read and write operations. named Learning to Control ( LTC ) USED-FOR approach. memory plasticity USED-FOR few - shot learning. system USED-FOR few - shot learning of entity recognition. Stanford Task - Oriented Dialogue dataset USED-FOR few - shot learning of entity recognition. Metric is loss function. Generic is solution. ,"This paper proposes a novel method for few-shot learning of entity recognition on the Stanford Task-Oriented Dialogue dataset. The proposed method is based on a natural language understanding (NLU) system, where the goal is to learn a set of latent representations from large datasets. The key idea is to use a RL trainable controller to guide the representation learning of the neural encoder and the memory management role. The controller is trained to predict the read and write operations from external memory. The authors show that the proposed approach, named Learning to Control (LTC), is able to achieve state-of-the-art performance on the dataset. ",This paper proposes a novel method for learning latent representations from large datasets. The key idea is to use natural language understanding (NLU) system to identify emerging entities. The authors propose a novel RL trainable controller for representation learning. The proposed approach is based on named Learning to Control (LTC) and uses external memory to perform read and write operations. The neural encoder and the memory management role of the controller are modeled as a loss function. The system is evaluated on the Stanford Task-Oriented Dialogue dataset for few-shot learning of entity recognition with memory plasticity.
12105,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,approach USED-FOR recomposable motor primitives. large - scale and diverse manipulation demonstrations FEATURE-OF recomposable motor primitives. approaches USED-FOR primitives. manually defined primitives USED-FOR approaches. manually defined primitives USED-FOR primitives. approaches USED-FOR primitive discovery. complexity EVALUATE-FOR primitive. latent representation USED-FOR primitives. motor primitives CONJUNCTION latent representation. latent representation CONJUNCTION motor primitives. hierarchical reinforcement learning setup USED-FOR robotic manipulation tasks. primitives USED-FOR robotic manipulation tasks. primitives PART-OF hierarchical reinforcement learning setup. ,"This paper proposes a new approach for learning recomposable motor primitives from large-scale and diverse manipulation demonstrations. The authors propose two approaches to learn primitives using manually defined primitives, which are then used for primitive discovery. The primitives are learned using a hierarchical reinforcement learning setup where the latent representation of the primitives is used to learn the latent representations of the motor primitive. The main contribution of the paper is to show that the complexity of a primitive can be reduced by using the learned primitives.","This paper presents a novel approach for learning recomposable motor primitives for large-scale and diverse manipulation demonstrations. The authors propose two approaches for learning primitives from manually defined primitives. The primitives are learned in a hierarchical reinforcement learning setup, where the motor primitive is learned from a latent representation of the primitives, and the latent representation is used for primitive discovery. The complexity of the primitive is measured by the number of steps needed to learn the primitive. Experiments are conducted on a variety of robotic manipulation tasks."
12114,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"Transfer CONJUNCTION adaptation. adaptation CONJUNCTION Transfer. adaptation USED-FOR unknown environmental dynamics. Transfer PART-OF reinforcement learning ( RL ). adaptation PART-OF reinforcement learning ( RL ). Transfer USED-FOR unknown environmental dynamics. methods USED-FOR adaptation. experience rollouts USED-FOR adaptation. experience rollouts USED-FOR methods. general algorithm USED-FOR probe. general algorithm USED-FOR inference model. general algorithm USED-FOR latent variables of test dynamics. general algorithm USED-FOR single episode transfer. algorithms USED-FOR RL. algorithms USED-FOR variational inference. variational inference CONJUNCTION RL. RL CONJUNCTION variational inference. algorithms USED-FOR modular approach. method COMPARE adaptive approaches. adaptive approaches COMPARE method. baselines USED-FOR robust transfer. method COMPARE baselines. baselines COMPARE method. method USED-FOR robust transfer. single episode test constraint EVALUATE-FOR method. OtherScientificTerm are dense rewards, and rewards. Method is universal control policy. Generic are approach, and it. ","This paper studies the problem of transfer in reinforcement learning (RL) and adaptation in the context of unknown environmental dynamics. The authors propose two methods for adaptation based on experience rollouts: Transfer and adaptation. Transfer is based on a universal control policy, where each episode is sampled from a set of episodes, and the goal is to transfer the data to the next episode. The general algorithm for the probe is a general algorithm to learn the latent variables of test dynamics, which are then used to train the inference model. The algorithms are applied to variational inference and RL. The proposed modular approach is evaluated on a single episode test constraint and shows that the proposed method outperforms existing adaptive approaches in terms of robust transfer.","This paper proposes a new approach to learn a universal control policy for reinforcement learning (RL). The approach is based on the idea of Transfer and adaptation to unknown environmental dynamics. The authors propose two methods for adaptation: experience rollouts for learning the adaptation and a general algorithm for the probe of the inference model. The general algorithm is used to learn the latent variables of test dynamics, which are then used to train a single episode transfer. Experiments on variational inference and RL show that the proposed modular approach outperforms the baselines for robust transfer under the single episode test constraint. "
12123,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"AlphaZero COMPARE AlphaGo Zero. AlphaGo Zero COMPARE AlphaZero. generalization EVALUATE-FOR neural net. three - head network architecture USED-FOR action - value head. action - value head USED-FOR Monte Carlo tree search ( MCTS ). search efficiency EVALUATE-FOR action - value head. three - head network USED-FOR AlphaZero style learning paradigm. threehead network architecture USED-FOR AlpahZero learning. game of Hex EVALUATE-FOR threehead network architecture. architecture USED-FOR zero - style iterative learning. neural network models COMPARE those. those COMPARE neural network models. two - head counterpart PART-OF MCTS. MCTS EVALUATE-FOR neural network models. two - head counterpart USED-FOR those. two - head counterpart USED-FOR neural network models. Method are search - based reinforcement learning algorithm AlphaZero, two - head network architecture, and two - head net. Material is chess. OtherScientificTerm is policy. ","This paper proposes a new search-based reinforcement learning algorithm AlphaZero, which uses a three-head network architecture to learn an action-value head for Monte Carlo tree search (MCTS). The authors show that AlphaZero improves the search efficiency and generalization of a neural net by a factor of 2.5. AlphaZero outperforms AlphaGo Zero in terms of generalization. The authors also show that AlpahZero learning with a threehead network can outperform AlphaZero in the game of Hex. ","The paper proposes a new search-based reinforcement learning algorithm AlphaZero, which is an extension of AlphaGo Zero. AlphaZero is a generalization of the neural net. The key idea is to use a three-head network architecture for the action-value head in Monte Carlo tree search (MCTS) to improve the search efficiency of the AlphaZero style learning paradigm. The paper shows that AlphaZero outperforms AlphaZero in terms of generalization on the game of Hex, and outperforms those trained with a two-head counterpart in MCTS. The architecture is also applied to zero-style iterative learning in the context of chess, where the goal is to learn a policy that maximizes the generalization ability of the policy. Experiments show that the proposed architecture outperforms neural network models trained with the two-headed counterpart, and that it outperforms the neural network model trained with only a single head."
12132,SP:89d6d55107b6180109affe7522265c751640ad96,policy transfer PART-OF reinforcement learning. warm initialization CONJUNCTION imitation. imitation CONJUNCTION warm initialization. Policy transfer USED-FOR Reinforcement Learning ( RL ) tasks. imitation USED-FOR Policy transfer. warm initialization USED-FOR Policy transfer. biological world FEATURE-OF behavior transfer. adaptation reward CONJUNCTION environmental reward. environmental reward CONJUNCTION adaptation reward. method USED-FOR policies. sample complexity EVALUATE-FOR policies. sample complexity EVALUATE-FOR method. Material is randomized instances. Task is transfer of policies. Generic is mechanism. OtherScientificTerm is transition differences. ,"This paper studies the problem of policy transfer in reinforcement learning. Policy transfer is an important problem in Reinforcement Learning (RL) tasks, where the goal is to learn a set of policies that are transferable across different environments. The authors propose a mechanism to learn policies that can be transferred across environments by warm initialization and imitation. They show that the proposed method can learn policies with high sample complexity with low transition differences. They also show that their method is able to transfer policies to environments with high adaptation reward and low environmental reward.","This paper proposes a method for policy transfer in reinforcement learning. Policy transfer is one of the most important Reinforcement Learning (RL) tasks. The authors propose a novel mechanism for the transfer of policies from one environment to another. The main idea is to use warm initialization and imitation for the Policy transfer. The method is evaluated on randomized instances, and the authors show that the proposed method improves the sample complexity of policies with respect to the adaptation reward and the environmental reward in the biological world. "
12141,SP:626021101836a635ad2d896bd66951aff31aa846,"scale changes FEATURE-OF tasks. steerable filters USED-FOR scale - equivariant convolutional networks. numerical stability EVALUATE-FOR method. computational efficiency EVALUATE-FOR method. computational efficiency CONJUNCTION numerical stability. numerical stability CONJUNCTION computational efficiency. scale equivariance CONJUNCTION local scale invariance. local scale invariance CONJUNCTION scale equivariance. models COMPARE methods. methods COMPARE models. methods USED-FOR scale equivariance. methods USED-FOR local scale invariance. models USED-FOR scale equivariance. models USED-FOR local scale invariance. MNIST - scale dataset CONJUNCTION STL-10 dataset. STL-10 dataset CONJUNCTION MNIST - scale dataset. supervised learning setting FEATURE-OF STL-10 dataset. Method are Convolutional Neural Networks ( CNNs ), CNNs, and scale - convolution. OtherScientificTerm are translation equivariance, and scale - equivariant. Generic is transformations. ","This paper studies the problem of scale-equivariant convolutional networks with steerable filters. The authors propose a new method that combines numerical stability, computational efficiency, and local scale invariance. They show that the proposed method can achieve state-of-the-art performance on the MNIST-scale dataset and the STL-10 dataset in the supervised learning setting. ",This paper proposes a new method for learning scale-equivariant convolutional networks with steerable filters. The authors show that the proposed method achieves better computational efficiency and numerical stability compared to existing methods for learning the scale equivariance and local scale invariance. The proposed method is evaluated on the MNIST-scale dataset and the STL-10 dataset in a supervised learning setting. 
12150,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,deep learning setups USED-FOR scan completion. paired training data FEATURE-OF supervision. supervision USED-FOR methods. synthetic data EVALUATE-FOR methods. real scans USED-FOR scan completion. point clouds USED-FOR approach. Matterport3D CONJUNCTION KITTI. KITTI CONJUNCTION Matterport3D. ScanNet CONJUNCTION Matterport3D. Matterport3D CONJUNCTION ScanNet. incompleteness USED-FOR realistic completions. ScanNet HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. 3D - EPN shape completion dataset EVALUATE-FOR approach. KITTI HYPONYM-OF real - world datasets. Matterport3D HYPONYM-OF real - world datasets. Task is 3D scanning solutions. Material is raw scans. OtherScientificTerm is partial scans. Generic is approaches. ,"This paper studies the problem of 3D scanning solutions in deep learning setups. The authors propose a new approach based on point clouds. The proposed approach is based on the observation that partial scans are more likely to be incomplete than raw scans. The paper then proposes to use paired training data to improve the performance of the proposed methods on synthetic data and real scans for the task of scan completion. The approach is evaluated on three real-world datasets: Matterport3D, KITTI, and ScanNet. The experimental results show that the proposed approach performs well on the 3D-EPN shape completion dataset. ","This paper proposes a novel approach to learning 3D scanning solutions from raw scans. The idea is to use deep learning setups for scan completion. The proposed approach is based on point clouds. The authors show that the proposed methods outperform existing methods on synthetic data and paired training data. They also show that their approach can achieve realistic completions with incompleteness. The approach is evaluated on three real-world datasets (Matterport3D, ScanNet, and KITTI) and on a 3D-EPN shape completion dataset. "
12159,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"models USED-FOR systems. authentication CONJUNCTION anomaly detection. anomaly detection CONJUNCTION authentication. sensor data USED-FOR anomaly detection. sensor data USED-FOR authentication. systems USED-FOR authentication. systems USED-FOR anomaly detection. sensor data USED-FOR systems. learning systems USED-FOR private data. authentication system USED-FOR generative impersonation attacks. optimal strategies USED-FOR attacker. optimal strategies USED-FOR Gaussian source distributions. maximin game USED-FOR problem. they USED-FOR models. real - world data USED-FOR models. Method are Generative neural models, and practical learning approaches. Generic is it. Task are learning, and information theory. Material is nominally - looking artificial data. OtherScientificTerm is optimal strategy. ","This paper studies the problem of generating fake data from real-world data. Generative neural models can be used to generate fake data, but they can also be used as a proxy for real data. The authors propose a method for generating fake samples from real data, which is based on the maximin game. They show that the proposed method can generate fake samples with high probability, and that it can be applied to a variety of tasks, including authentication and anomaly detection.","This paper proposes a novel approach to attack generative adversarial networks. Generative neural models can be seen as an extension of practical learning approaches. The main idea is to use models trained on sensor data for systems for authentication and anomaly detection, and then use these systems to generate private data from the sensor data. The authors show that these learning systems can be used for generating private data, and that they can be applied to real-world data to improve the performance of the models. They also show that the attacker can use optimal strategies for generating Gaussian source distributions, and the optimal strategy is a maximin game. The paper is well-written and well-motivated. However, it is not clear to me why this is an important topic for learning, and information theory is not well-explored."
12168,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,robustness CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robustness. natural accuracy CONJUNCTION robustness. robustness CONJUNCTION natural accuracy. limited data samples USED-FOR high dimensional distribution. sensible adversary USED-FOR defense model. Bayes rule HYPONYM-OF multi - class classifier. 0 - 1 loss FEATURE-OF Bayes rule. 0 - 1 loss FEATURE-OF multi - class classifier. sensible adversarial learning USED-FOR Bayes rule. algorithm USED-FOR robust model. sensible adversarial examples USED-FOR robust model. robust accuracy EVALUATE-FOR PGD attacks. model USED-FOR attacks. CIFAR10 EVALUATE-FOR model. perturbations USED-FOR attacks. Generic is problem. ,"This paper studies the problem of defending against PGD attacks in a multi-class classifier with a 0-1 loss. The authors propose a defense model with a sensible adversary that is trained using sensible adversarial learning. The defense model is based on the Bayes rule, and is trained with limited data samples from a high dimensional distribution. The proposed algorithm is shown to improve the robustness and the standard accuracy of the model. The model is evaluated on CIFAR10 and shows that the proposed model can defend against attacks with perturbations.","This paper proposes a defense model against PGD attacks that uses a sensible adversary. The proposed defense model is based on the Bayes rule, a multi-class classifier with a 0-1 loss. The authors show that the robustness and standard accuracy of the proposed model can be improved by perturbations to the high dimensional distribution of the data samples. The paper also shows that the proposed algorithm can improve the robust model by using sensible adversarial learning. "
12177,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,image intensity CONJUNCTION spatial correlation. spatial correlation CONJUNCTION image intensity. spatial correlation FEATURE-OF Feature maps. image intensity FEATURE-OF Feature maps. class probabilities USED-FOR online knowledge distillation methods. adversarial training framework USED-FOR online knowledge distillation method. discriminators USED-FOR feature map distributions. discriminators USED-FOR networks. discriminator USED-FOR feature map. discriminator PART-OF network. network USED-FOR feature map distribution. network USED-FOR discriminator. Discriminators CONJUNCTION networks. networks CONJUNCTION Discriminators. Discriminators PART-OF minimax twoplayer game. minimax twoplayer game USED-FOR networks. cyclic learning scheme USED-FOR networks. method USED-FOR network architectures. method USED-FOR classification task. classification task EVALUATE-FOR network architectures. Generic is small network. ,"This paper proposes a new adversarial training framework for online knowledge distillation method based on class probabilities. Feature maps with image intensity and spatial correlation are used to train the discriminators for the feature map distributions. The discriminator is trained on a small network, and the network is trained to predict the final feature map distribution using the discriminator. Discriminators are used in a minimax twoplayer game, and networks are trained using a cyclic learning scheme. The proposed method is shown to perform well on a classification task.",This paper proposes an adversarial training framework for online knowledge distillation method based on class probabilities. Feature maps based on image intensity and spatial correlation are used to train a small network. The proposed method is evaluated on a classification task and shows that the proposed method outperforms state-of-the-art network architectures on the classification task. Discriminators and networks trained on a minimax twoplayer game are used as discriminators for the feature map distributions. The authors also propose a cyclic learning scheme to train networks.
12186,SP:e43fc8747f823be6497224696adb92d45150b02d,"natural language processing tasks EVALUATE-FOR word embedding models. word embedding models USED-FOR rich semantic meanings. maximum likelihood estimation CONJUNCTION Bayesian estimation. Bayesian estimation CONJUNCTION maximum likelihood estimation. maximum likelihood estimation USED-FOR model. Bayesian estimation USED-FOR model. model COMPARE baseline methods. baseline methods COMPARE model. baseline methods USED-FOR sentiment analysis. low - frequency words FEATURE-OF sentiment analysis. sentiment analysis EVALUATE-FOR model. it USED-FOR semantic and sentiment analysis tasks. OtherScientificTerm is sentiment information. Method are sentiment word embedding model, and parameter estimating method. Task is semantic and sentiment embeddings. ",This paper proposes a new sentiment word embedding model for natural language processing tasks. The proposed model is based on maximum likelihood estimation and Bayesian estimation. The authors show that the proposed model performs better than baseline methods for sentiment analysis on low-frequency words. They also show that it performs well on semantic and sentiment analysis tasks. ,This paper proposes a new sentiment word embedding model for natural language processing tasks. The model is based on maximum likelihood estimation and Bayesian estimation. The authors show that the model outperforms the baseline methods for sentiment analysis on low-frequency words. They also show that it can be applied to semantic and sentiment analysis tasks. 
12195,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"Noisy labels PART-OF real - world training data. overfitting FEATURE-OF noisy labels. maximal safe set USED-FOR early stopped network. two - phase training method USED-FOR noise - free training. Prestopping HYPONYM-OF two - phase training method. image benchmark data sets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. image benchmark data sets EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR method. Method is deep neural network. OtherScientificTerm are label noise, and real - world noise. ","This paper proposes a two-phase training method for noise-free training, called Prestopping. The main idea is to train a deep neural network with a maximal safe set for the early stopped network. The authors show that the proposed method achieves better test error than state-of-the-art methods on several image benchmark data sets. ","This paper proposes a two-phase training method for noise-free training, called Prestopping. The main idea is to train a deep neural network with a maximal safe set for the early stopped network. The authors show that the proposed method outperforms state-of-the-art methods on image benchmark data sets. The paper also shows that label noise is not the cause of overfitting in real-world training data."
12204,SP:8316872d8b388587bf25f724c80155b25b6cb68e,framework USED-FOR generalization. reinforcement learning USED-FOR actions. regularization metrics USED-FOR generalization. generalization USED-FOR policy. action representations USED-FOR reinforcement learning architecture. policy USED-FOR zero - shot generalization. representation learning method CONJUNCTION policy. policy CONJUNCTION representation learning method. representation learning method USED-FOR zero - shot generalization. sequential decision - making environments USED-FOR zero - shot generalization. Task is intelligence. OtherScientificTerm is action ’s functionality. Method is unsupervised representation learning. ,This paper proposes a framework for generalization in reinforcement learning for actions. The generalization is based on the regularization metrics of the policy and the action’s functionality. The authors propose a new reinforcement learning architecture based on action representations. The representation learning method and the policy are combined with the policy to achieve zero-shot generalization on sequential decision-making environments. Experiments show that the proposed unsupervised representation learning is able to achieve state-of-the-art performance.,This paper proposes a framework for generalization of actions learned by reinforcement learning. The generalization is based on regularization metrics that measure the generalization ability of an action’s functionality. The authors propose a reinforcement learning architecture based on action representations. The proposed representation learning method and policy are used for zero-shot generalization in sequential decision-making environments. Experiments show that the proposed unsupervised representation learning outperforms the state-of-the-art.
12213,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"word2vec CONJUNCTION GloVe. GloVe CONJUNCTION word2vec. vector word embeddings USED-FOR Deep learning natural language processing models. word2vec HYPONYM-OF vector word embeddings. GloVe HYPONYM-OF vector word embeddings. continuous vectors USED-FOR it. embedding vectors USED-FOR dictionary. word embedding matrix USED-FOR inference. word2ket CONJUNCTION word2ketXS. word2ketXS CONJUNCTION word2ket. word embedding matrix USED-FOR training. quantum computing USED-FOR approaches. accuracy EVALUATE-FOR natural language processing tasks. natural language processing tasks EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. OtherScientificTerm are discrete sequence of words, and GPU memory. Material is text corpus. Generic is embeddings. ","This paper proposes a new vector word embeddings, word2vec and GloVe, for Deep learning natural language processing models. The proposed approach is based on continuous vectors, and it uses continuous vectors to encode the discrete sequence of words. The embedding vectors are then used to construct a dictionary, and the word embedding matrix is used for inference and training. The authors show that the proposed approach improves the accuracy of the proposed approaches on a variety of natural languageprocessing tasks. ","This paper proposes a novel approach to learning vector word embeddings for Deep learning natural language processing models. The key idea is to use word2vec, GloVe, and word2ket as vectors to encode the discrete sequence of words in the text corpus. The authors show that it is possible to learn the dictionary using continuous vectors. The paper also shows that the embedding vectors can be used for inference and training. The proposed approaches are based on quantum computing, and are evaluated on a number of tasks. The results show that the proposed approach improves the accuracy of the model on a variety of tasks, and is competitive with state-of-the-art approaches. "
12222,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"machine learning approach USED-FOR policy. Imitation Learning ( IL ) HYPONYM-OF machine learning approach. Imitation Learning ( IL ) USED-FOR policy. human players PART-OF video games. IL USED-FOR reinforcement learning ( RL ). they USED-FOR average ” policy. dataset USED-FOR average ” policy. behavioral descriptions USED-FOR state - action pairs. approach USED-FOR neural network policy. behavior description USED-FOR neural network policy. approach USED-FOR policy. human demonstrations USED-FOR build - order planning task. StarCraft II FEATURE-OF build - order planning task. StarCraft II FEATURE-OF human demonstrations. human demonstrations USED-FOR policy. Dimensionality reduction techniques USED-FOR lowdimensional behavioral space. high - dimensional army unit composition USED-FOR lowdimensional behavioral space. UCB1 algorithm USED-FOR policy. policy COMPARE IL baseline approach. IL baseline approach COMPARE policy. Generic is it. Method are IL approaches, and Behavioral Repertoire Imitation Learning ( BRIL ). OtherScientificTerm is repertoire of behaviors. ","This paper proposes a new machine learning approach for learning a policy from Imitation Learning (IL) using human players in video games. The approach uses a neural network policy to learn a behavior description for each state-action pairs from behavioral descriptions, and then uses IL for reinforcement learning (RL). The authors show that they can learn a “average” policy from this dataset, and that it can be combined with other IL approaches such as Behavioral Repertoire Imitation learning (BRIL). The approach is evaluated on a build-order planning task in StarCraft II with human demonstrations from StarCraft II. Dimensionality reduction techniques are used to reduce the lowdimensional behavioral space to a high-dimensional army unit composition, and the policy is trained using UCB1 algorithm. The proposed policy outperforms the IL baseline approach.","This paper proposes a new machine learning approach for learning a policy from a set of human players in video games. The approach is based on Imitation Learning (IL) which learns a neural network policy based on a behavior description of the state-action pairs. IL approaches are used in reinforcement learning (RL) and they can be used to learn a “average” policy on the dataset. The authors propose Behavioral Repertoire Imitation learning (BRIL), which is a variant of IL where the policy is learned from human demonstrations of the build-order planning task in StarCraft II. The policy is trained using the UCB1 algorithm. The IL baseline approach is compared to the policy learned from the human demonstrations. Dimensionality reduction techniques are used to reduce the lowdimensional behavioral space to a high-dimensional army unit composition. "
12231,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"stochastic gradient descent optimization USED-FOR lottery ticket. weight magnitude FEATURE-OF model. gradual pruning COMPARE one - shot pruning. one - shot pruning COMPARE gradual pruning. accuracy EVALUATE-FOR one - shot pruning. accuracy EVALUATE-FOR gradual pruning. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR ResNet architectures. CIFAR10 EVALUATE-FOR ResNet architectures. ImageNet FEATURE-OF ResNet50. Task is lottery ticket hypothesis. Method are small, sparsified neural networks, sparsified model, iterative pruning, and memorization capacity analysis. Generic is network. OtherScientificTerm are rewinding, transferability of the winning lottery tickets, winning ticket, structure of winning lottery tickets, lottery tickets, Pruning, complex patterns, and pruning rate. Metric is Top-1 accuracy. ","This paper studies the lottery ticket hypothesis in the context of small, sparsified neural networks. The authors propose a stochastic gradient descent optimization for lottery ticket, where the model is trained with weight magnitude proportional to the weight magnitude of the winning ticket, and the goal is to maximize the transferability of the lottery tickets to the next lottery ticket. Pruning, in particular, is done by iterative pruning, where each iteration of the network is rewinding. Theoretically, the authors show that gradual pruning is better than one-shot pruning in terms of accuracy compared to the Top-1 accuracy, and that the performance of ResNet architectures such as CIFAR10 and ImageNet on ResNet50 with complex patterns can be improved by increasing the pruning rate. ","This paper studies the lottery ticket hypothesis in the context of small, sparsified neural networks. The authors propose a stochastic gradient descent optimization for lottery ticket. The model is based on a sparsification model with weight magnitude proportional to the weight magnitude of the winning ticket. Pruning is done by rewinding the winning lottery tickets, and the authors show that the transferability of the lottery tickets can be improved by iterative pruning. The paper also provides a memorization capacity analysis of the network. The main contribution of the paper is to show that in the case of lottery tickets with complex patterns, the pruning rate can be reduced to the Top-1 accuracy. Experiments on CIFAR10 and ImageNet show that ResNet architectures such as ResNet50 outperform one-shot pruning and gradual pruning in terms of accuracy."
12240,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"low - confidence detections USED-FOR unknowns. CNNs USED-FOR product operation. UDN USED-FOR product operation. CNNs USED-FOR UDN. convolutional layers USED-FOR features. product operations USED-FOR UDN. UDN USED-FOR detecting unknowns. learning process USED-FOR UDN. information - theoretic regularization strategy USED-FOR learning process. information - theoretic regularization strategy USED-FOR UDN. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. SVHN HYPONYM-OF benchmark image datasets. MNIST HYPONYM-OF benchmark image datasets. CIFAR-100 HYPONYM-OF benchmark image datasets. CIFAR-10 HYPONYM-OF benchmark image datasets. UDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE UDN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR UDN. classification accuracy EVALUATE-FOR UDN. Method is image classification systems. Generic are they, and problem. ","This paper studies the problem of low-confidence detections for unknowns in image classification systems. The authors propose to use CNNs to learn the product operation of UDN with convolutional layers. The learning process is based on an information-theoretic regularization strategy, and the authors show that UDN can be used for detecting unknowns. They also show that they can achieve better classification accuracy than state-of-the-art methods. Finally, the authors conduct experiments on three benchmark image datasets: MNIST, CIFAR-10, and SVHN.","This paper proposes a novel method for detecting unknowns using low-confidence detections for image classification systems. The key idea is to use CNNs for the product operation of UDN with convolutional layers. The learning process for UDN is based on an information-theoretic regularization strategy. The authors show that they are able to detect unknowns with high confidence. They also show that UDN outperforms state-of-the-art methods on several benchmark image datasets (MNIST, CIFAR-10, SVHN) and improves classification accuracy. "
12249,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"approach USED-FOR approximate Bayesian inference. Variational inference ( VI ) USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR highly parameterized models. Variational inference ( VI ) HYPONYM-OF approach. deep neural networks HYPONYM-OF highly parameterized models. method USED-FOR highly flexible variational distributions. coarse approximation USED-FOR method. benchmark tasks EVALUATE-FOR method. log - likelihood CONJUNCTION ELBO. ELBO CONJUNCTION log - likelihood. variational inference methods USED-FOR deep learning. method COMPARE variational inference methods. variational inference methods COMPARE method. method USED-FOR deep learning. log - likelihood EVALUATE-FOR variational inference methods. ELBO EVALUATE-FOR variational inference methods. log - likelihood EVALUATE-FOR method. ELBO EVALUATE-FOR method. CIFAR10 EVALUATE-FOR residual networks. Task is variational inference. Generic are distribution, and it. OtherScientificTerm are variational families, and Evidence Lower BOund. Method are larger scale models, and VI. ",This paper proposes a new approach for approximate Bayesian inference based on Variational inference (VI) for highly parameterized models such as deep neural networks. The proposed method is based on a coarse approximation of the distribution of the variational families. The authors show that the proposed method can learn highly flexible variational distributions with larger scale models. The method is evaluated on benchmark tasks on CIFAR10 and ELBO and compared with other variational inference methods for deep learning. ,"This paper proposes a new approach to approximate Bayesian inference for highly parameterized models, i.e., deep neural networks. Variational inference (VI) is a popular approach for variational inference. The authors propose a method for learning highly flexible variational distributions with a coarse approximation. The method is evaluated on several benchmark tasks, including CIFAR10 and ELBO. The proposed method is shown to outperform variational in terms of log-likelihood, ELBO, and other popular deep learning methods. "
12258,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,user - defined metrics USED-FOR reinforcement learning. reinforcement learning USED-FOR Sequence generation models. correlated Monte Carlo ( MC ) rollouts USED-FOR variance control. correlated Monte Carlo ( MC ) rollouts USED-FOR policy gradient estimator. policy gradient estimator USED-FOR contextual generation of categorical sequences. rollouts USED-FOR model uncertainty. correlated MC rollouts USED-FOR binary - tree softmax models. large vocabulary scenarios FEATURE-OF high generation cost. binary actions FEATURE-OF categorical action. neural program synthesis CONJUNCTION image captioning. image captioning CONJUNCTION neural program synthesis. image captioning EVALUATE-FOR methods. neural program synthesis EVALUATE-FOR methods. gradient variance EVALUATE-FOR methods. Generic is method. OtherScientificTerm is correlation. ,"This paper proposes a new metric for evaluating the performance of reinforcement learning models based on user-defined metrics. Sequence generation models are a popular topic of interest in reinforcement learning, and this paper focuses on contextual generation of categorical sequences using correlated Monte Carlo (MC) rollouts for variance control. The authors propose a new policy gradient estimator based on correlated MC rollouts to improve the model uncertainty in binary-tree softmax models. The proposed method is based on the observation that the high generation cost in large vocabulary scenarios is due to the lack of correlation between the binary actions and the categorical action. The paper shows that the proposed methods improve the gradient variance in neural program synthesis and image captioning.","The paper proposes a new metric for evaluating the performance of reinforcement learning models on user-defined metrics. Sequence generation models have been used for many years to evaluate the performance on reinforcement learning. The paper proposes to use correlated Monte Carlo (MC) rollouts for variance control and a policy gradient estimator for contextual generation of categorical sequences. The proposed method is based on the correlation between categorical action and binary actions. The authors show that correlated MC rollouts can be used for binary-tree softmax models with large vocabulary scenarios, and that the high generation cost is due to the lack of correlation. The methods are evaluated on neural program synthesis and image captioning, and show that the proposed methods are more robust to gradient variance."
12267,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"Goal recognition USED-FOR applications. goal recognition design USED-FOR online goal recognition process. goal recognition control HYPONYM-OF stages. deceptive opponent modeling HYPONYM-OF stages. proactively static interdiction USED-FOR goal recognition control. worst case distinctiveness ( wcd ) USED-FOR nondistinctive path. approach USED-FOR goal recognition process. opponent ’s deceptive behavior USED-FOR approach. opponent ’s deceptive behavior USED-FOR goal recognition process. OtherScientificTerm are hard action removal, and wcd. Method is S - GRC. ","This paper proposes a new goal recognition design for online goal recognition process. The proposed stages are goal recognition control with proactively static interdiction, and deceptive opponent modeling with hard action removal. The main idea is to use worst case distinctiveness (wcd) for the nondistinctive path, and then use S-GRC for the wcd. The approach is shown to improve the performance of goal recognition through the use of opponent’s deceptive behavior.","This paper proposes a novel goal recognition design for online goal recognition process. The proposed stages are: goal recognition control, deceptive opponent modeling, and hard action removal. The main idea is to use proactively static interdiction to improve the performance of the goal recognition. The approach is based on the best case distinctiveness (wcd) for the nondistinctive path. The authors show that wcd improves the performance on S-GRC. "
12276,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,large mini - batch sizes USED-FOR Generative Adversarial Networks ( GANs ). Coreset - selection USED-FOR active learning. Coreset - selection USED-FOR method. Coreset - selection USED-FOR batch. training time CONJUNCTION memory usage. memory usage CONJUNCTION training time. it USED-FOR GANs. GANs USED-FOR anomaly detection. training time EVALUATE-FOR GAN variants. it USED-FOR dropped modes. technique USED-FOR GAN variants. memory usage EVALUATE-FOR GAN variants. it USED-FOR anomaly detection. dropped modes PART-OF synthetic dataset. memory usage EVALUATE-FOR technique. training time EVALUATE-FOR technique. synthetic dataset EVALUATE-FOR it. Method is GAN. Material is real ’ images. Generic is them. ,"This paper proposes a new method for large mini-batch sizes for Generative Adversarial Networks (GANs). Coreset-selection is a popular method for active learning, and this paper proposes to use Coreset -selection to select the batch for each GAN. The authors show that it can improve the training time and memory usage of GANs for anomaly detection, and it can also improve the performance of dropped modes in a synthetic dataset. ",This paper proposes a new method for training Generative Adversarial Networks (GANs) with large mini-batch sizes. Coreset-selection is used for active learning. The authors show that it can be used to train GANs for anomaly detection and improve the training time and memory usage. The technique is evaluated on a synthetic dataset with dropped modes and on real ‘’ images. 
12285,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,artificial neurons PART-OF recurrent neural networks ( RNNs ). maximum likelihood USED-FOR recurrent neural networks ( RNNs ). restorative Brownian motion CONJUNCTION hand - drawn sketch dataset. hand - drawn sketch dataset CONJUNCTION restorative Brownian motion. information plane FEATURE-OF RNNs. hand - drawn sketch dataset HYPONYM-OF datasets. restorative Brownian motion HYPONYM-OF datasets. RNNs USED-FOR predictive information. maximum likelihood CONJUNCTION contrastive loss training. contrastive loss training CONJUNCTION maximum likelihood. noise USED-FOR hidden state. predictive information USED-FOR maximum likelihood. RNNs USED-FOR contrastive loss training. predictive information USED-FOR contrastive loss training. noise USED-FOR past information. Method is biological neurons. ,"This paper studies the maximum likelihood of recurrent neural networks (RNNs) with artificial neurons in the information plane. The authors propose two datasets: restorative Brownian motion and a hand-drawn sketch dataset. Theoretically, they show that RNNs with predictive information can be trained with maximum likelihood and contrastive loss training with noise. Empirically, the authors show that the noise can be used to learn the hidden state of the RNN. ",This paper proposes a novel approach to learning the maximum likelihood of recurrent neural networks (RNNs) with artificial neurons. The authors show that RNNs with biological neurons can be learned in the information plane. They provide two datasets: restorative Brownian motion and a hand-drawn sketch dataset. They show that maximum likelihood and contrastive loss training can be combined with predictive information for maximum likelihood. They also show that noise can be used to learn the hidden state.
12294,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"techniques USED-FOR delusion. methods USED-FOR delusional bias. Q - approximators USED-FOR methods. penalization scheme USED-FOR Q - labels. search framework USED-FOR premature ( implicit ) policy commitments. search framework USED-FOR Q - approximators. methods USED-FOR Q - learning. Atari games EVALUATE-FOR Q - learning. Atari games EVALUATE-FOR methods. Task is Delusional bias. Method are approximate Q - learning, and tabular value estimates. OtherScientificTerm are greedy policy class, and expressible policy class. ",This paper studies the problem of approximate Q-learning in the context of Delusional bias. The authors propose two techniques to mitigate the delusion caused by Q-approximators. The first is a penalization scheme that penalizes Q-labels for the greedy policy class. The second is a search framework for premature (implicit) policy commitments. The proposed methods are evaluated on a variety of Atari games and show that the proposed methods can reduce the delusional bias.,This paper proposes a novel method to address the problem of Delusional bias in approximate Q-learning. The authors propose two techniques to mitigate the delusion. The first is a penalization scheme to penalize Q-labels for the greedy policy class. The second is a search framework that penalizes premature (implicit) policy commitments. The proposed methods are evaluated on a variety of Atari games and show that the proposed methods can reduce the delusional bias. 
12303,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"approaches USED-FOR unsupervised object - oriented scene representation learning. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. scene - mixture approaches USED-FOR approaches. spatial - attention USED-FOR approaches. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. generative latent variable model USED-FOR unified probabilistic modeling framework. scene - mixture approaches USED-FOR unified probabilistic modeling framework. spatial - attention USED-FOR unified probabilistic modeling framework. SPACE HYPONYM-OF generative latent variable model. factorized object representations USED-FOR foreground objects. SPACE USED-FOR factorized object representations. SPACE USED-FOR methods. SPACE USED-FOR scalability problems. parallel spatial - attention USED-FOR methods. IODINE CONJUNCTION GENESIS. GENESIS CONJUNCTION IODINE. SPACE COMPARE SPAIR. SPAIR COMPARE SPACE. Atari CONJUNCTION 3D - Rooms. 3D - Rooms CONJUNCTION Atari. SPACE COMPARE IODINE. IODINE COMPARE SPACE. SPAIR CONJUNCTION IODINE. IODINE CONJUNCTION SPAIR. SPACE COMPARE GENESIS. GENESIS COMPARE SPACE. OtherScientificTerm are complex multi - object scenes, higher - level cognition, and complex morphology. Task is modeling real - world scenes. Generic is models. ","This paper proposes a new approach for unsupervised object-oriented scene representation learning. The proposed approach, called SPACE, combines spatial-attention with scene-mixture approaches and scene-mixer approaches to improve the performance of existing approaches. The authors propose a unified probabilistic modeling framework based on a generative latent variable model called SPACE HYPONYM, which is able to model complex multi-object scenes with higher-level cognition. SPACE uses factorized object representations for foreground objects and foreground objects for background objects. SPACE is also able to handle scalability problems. Experimental results show that SPACE outperforms IODINE, GENESIS, and SPAIR.","This paper presents a set of approaches for unsupervised object-oriented scene representation learning. The approaches are based on spatial-attention, scene-mixture approaches, and scene-mixing approaches. The authors propose a unified probabilistic modeling framework based on a generative latent variable model called SPACE, which is based on SPACE. The main idea is to model complex multi-object scenes with higher-level cognition, where the foreground objects have complex morphology, and the background objects have more complex morphology. The proposed models are evaluated on Atari, 3D-Rooms, and SPAIR, and compared to IODINE, GENESIS, and SPACE in terms of scalability problems. "
12312,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,classification tasks EVALUATE-FOR Convolutional Neural Networks ( CNN ). depthwise convolution CONJUNCTION pointwise convolution. pointwise convolution CONJUNCTION depthwise convolution. convolution CONJUNCTION depthwise convolution. depthwise convolution CONJUNCTION convolution. convolution PART-OF depthwise separable convolution. accuracies EVALUATE-FOR convolution. method USED-FOR compressing CNN. FALCON HYPONYM-OF method. FALCON USED-FOR compressing CNN. mathematical formulation USED-FOR convolution kernel. FALCON USED-FOR convolution methods. depthwise separable convolution USED-FOR convolution methods. EHP USED-FOR convolution methods. compression CONJUNCTION computation reduction rates. computation reduction rates CONJUNCTION compression. computation reduction rates EVALUATE-FOR generalized version rank - k FALCON. accuracy EVALUATE-FOR generalized version rank - k FALCON. compression EVALUATE-FOR generalized version rank - k FALCON. FALCON PART-OF convolution unit ShuffleUnitV2. FALCON USED-FOR FALCON - branch. FALCON - branch COMPARE methods. methods COMPARE FALCON - branch. FALCON COMPARE methods. methods COMPARE FALCON. FALCON CONJUNCTION FALCON - branch. FALCON - branch CONJUNCTION FALCON. FALCON - branch COMPARE CNN models. CNN models COMPARE FALCON - branch. methods COMPARE CNN models. CNN models COMPARE methods. depthwise separable convolution USED-FOR methods. compression EVALUATE-FOR CNN models. accuracy EVALUATE-FOR CNN models. parameters CONJUNCTION floating - point operations. floating - point operations CONJUNCTION parameters. rank - k FALCON COMPARE convolution. convolution COMPARE rank - k FALCON. accuracy EVALUATE-FOR convolution. floating - point operations USED-FOR rank - k FALCON. parameters USED-FOR rank - k FALCON. accuracy EVALUATE-FOR rank - k FALCON. Generic is they. Method is heuristic approaches. ,"This paper studies the problem of classification tasks with convolutional Neural Networks (CNN). The authors propose a method called FALCON, which is a combination of depthwise convolution and pointwise Convolution. The authors show that the convolution kernel can be decomposed into a mathematical formulation, and that the depthwise separable convolution can be used to compress convolution methods such as EHP.  The authors also show that their method can be applied to compressing CNN. Finally, they show that they can be combined with other heuristic approaches to improve the accuracy of convolution.  ","This paper studies the problem of classification tasks for Convolutional Neural Networks (CNN). The authors propose a method called FALCON, which is an extension of EHP to convolution methods such as depthwise convolution and depthwise separable convolution. The main contribution of the paper is the mathematical formulation of the convolution kernel. The authors show that they are able to achieve better accuracies for convolution compared to existing heuristic approaches. They also show that the proposed method can be used for compressing CNN. Finally, the authors evaluate the proposed convolution unit ShuffleUnitV2. "
12321,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"Batch Normalization HYPONYM-OF normalization layers. normalization algorithm USED-FOR small batch sizes. Ghost Batch Normalization USED-FOR small and medium batch sizes. scaling and shifting parameters FEATURE-OF weight decay regularization. Batch and Group Normalization USED-FOR normalization algorithm. SVHN CONJUNCTION Caltech-256. Caltech-256 CONJUNCTION SVHN. CUB-2011 CONJUNCTION ImageNet. ImageNet CONJUNCTION CUB-2011. Oxford Flowers-102 CONJUNCTION CUB-2011. CUB-2011 CONJUNCTION Oxford Flowers-102. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Caltech-256 CONJUNCTION Oxford Flowers-102. Oxford Flowers-102 CONJUNCTION Caltech-256. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. CUB-2011 HYPONYM-OF datasets. Caltech-256 HYPONYM-OF datasets. Oxford Flowers-102 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. Method are neural network architectures, and deep architectures. Generic are they, and method. Task is inference normalization statistics. OtherScientificTerm is training vs. inference discrepancy. ","This paper proposes a new normalization algorithm for small batch sizes, called Ghost Batch Normalization, for training neural network architectures. Batch and Group Normalization are two different normalization layers, and they are used to normalize the weight decay regularization with scaling and shifting parameters. The authors show that the proposed method is able to reduce the inference normalization statistics in training vs. inference discrepancy in deep architectures. The experiments on three datasets, CUB-2011, CIFAR-100, SVHN and Caltech-256, demonstrate the effectiveness of the method.","This paper proposes a new normalization algorithm for small batch sizes, called Ghost Batch Normalization, for training neural network architectures. Batch and Group Normalization are two normalization layers, and they can be applied to both small and medium batch sizes. The key idea is to use scaling and shifting parameters of the weight decay regularization to reduce the inference discrepancy between the training vs. inference normalization statistics. The method is evaluated on three datasets: CIFAR-100, SVHN, and Caltech-256. "
12330,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"outliers CONJUNCTION misclassifications. misclassifications CONJUNCTION outliers. manual data inspection USED-FOR privacy - sensitive datasets. metrics CONJUNCTION model parameters. model parameters CONJUNCTION metrics. model parameters HYPONYM-OF aggregated outputs. metrics HYPONYM-OF aggregated outputs. federated methods CONJUNCTION formal differential privacy guarantees. formal differential privacy guarantees CONJUNCTION federated methods. formal differential privacy guarantees FEATURE-OF generative models. federated methods USED-FOR generative models. algorithm USED-FOR differentially private federated GANs. text with differentially private federated RNNs CONJUNCTION images. images CONJUNCTION text with differentially private federated RNNs. algorithm USED-FOR images. Task are machine learning, Manual inspection of raw data, and federated learning. Generic are models, two, and methods. OtherScientificTerm are modeling hypotheses, and human - provided labels. ","This paper studies the problem of privacy-sensitive datasets with manual data inspection in machine learning. In particular, the authors focus on the setting where models are trained on a large amount of data, and the goal is to identify outliers and misclassifications. The authors propose a new algorithm for differentially private federated GANs, where the model parameters and model parameters are aggregated outputs. They show that the proposed algorithm is able to identify the outliers, misclassification, and differential privacy guarantees of the generative models, federated methods, and formal differential privacy guarantees. They also show that their algorithm can be applied to images and text with differentiallyprivate federated RNNs. ","This paper proposes a new approach to improve the privacy of machine learning models. The authors propose a new method for training GANs with differentially private labels. The main idea is to train a GAN on a subset of the training data, and then train the model on the rest of the data. The proposed method is evaluated on two datasets, where the model is trained on a set of datasets with different privacy-sensitive labels. Results show that the proposed method outperforms the baselines."
12339,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"Learning diverse and natural behaviors USED-FOR intelligent characters. method USED-FOR generating long range diverse and distinctive behaviors. method USED-FOR motion of human. non - parametric techniques CONJUNCTION parametric ones. parametric ones CONJUNCTION non - parametric techniques. non - parametric techniques USED-FOR method. parametric ones USED-FOR method. memory bank USED-FOR motion references. deep network USED-FOR synthesis. skeleton datasets EVALUATE-FOR method. method COMPARE parametric and non - parametric baselines. parametric and non - parametric baselines COMPARE method. skeleton datasets EVALUATE-FOR parametric and non - parametric baselines. OtherScientificTerm are long range diverse and distinctive behaviors, and starting and ending state. Material is animated world. ",This paper proposes a novel method for generating long range diverse and distinctive behaviors for intelligent characters. The method uses non-parametric techniques and parametric ones to learn the motion of human. The key idea is to use a memory bank to store the motion references from a deep network and then use the synthesis to generate the starting and ending state. The proposed method is evaluated on a variety of skeleton datasets and shows that the proposed method outperforms parametric and non- parametric baselines. ,This paper proposes a novel method for generating long range diverse and distinctive behaviors for intelligent characters. The method is based on non-parametric techniques and parametric ones for the motion of human. The proposed method is evaluated on two skeleton datasets and compared to parametric and non-Parametric baselines. The synthesis is performed on a deep network. The memory bank is used to store motion references from the starting and ending state. Experiments are conducted on an animated world.
12348,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,neural networks USED-FOR natural language processing tasks. model USED-FOR semantic compositions. hierarchies FEATURE-OF non - additivity and context independent importance attributions. inconsistent explanation quality EVALUATE-FOR models. contextual decomposition HYPONYM-OF hierarchical explanations. Sampling and Contextual Decomposition ( SCD ) algorithm CONJUNCTION Sampling and Occlusion ( SOC ) algorithm. Sampling and Occlusion ( SOC ) algorithm CONJUNCTION Sampling and Contextual Decomposition ( SCD ) algorithm. algorithms COMPARE hierarchical explanation algorithms. hierarchical explanation algorithms COMPARE algorithms. LSTM models CONJUNCTION BERT Transformer models. BERT Transformer models CONJUNCTION LSTM models. Human and metrics evaluation EVALUATE-FOR BERT Transformer models. Human and metrics evaluation EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR LSTM models. BERT Transformer models EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR hierarchical explanation algorithms. algorithms USED-FOR semantic composition. algorithms USED-FOR classification rules. models USED-FOR semantic composition. Task is hierarchical explanation of neural network predictions. OtherScientificTerm is word and phrase compositions. ,"This paper studies the problem of hierarchical explanation of neural network predictions in the context of natural language processing tasks. The authors consider hierarchical explanations such as contextual decomposition, Sampling and Contextual Decomposition (SCD) algorithm, and BERT Transformer models. They show that these models have inconsistent explanation quality due to the lack of hierarchical hierarchies in non-additivity and context independent importance attributions. They then propose two algorithms to improve the performance of these models for semantic composition and classification rules. Human and metrics evaluation shows that the proposed algorithms outperform the LSTM models and the BERT Transformers models.","This paper proposes a hierarchical explanation of neural network predictions. The hierarchical explanations are based on contextual decomposition, which is an extension of the Sampling and Contextual Decomposition (SCD) algorithm. The authors show that hierarchical explanation algorithms are more robust to non-additivity and context independent importance attributions in the hierarchies. They also show that models with inconsistent explanation quality are more sensitive to semantic compositions in the context of word and phrase compositions. The proposed algorithms are evaluated on LSTM models, BERT Transformer models, and Human and metrics evaluation on classification rules."
12357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"AI saliency map method USED-FOR deep convolutional neural networks ( CNN ). AI saliency map method COMPARE gradient methods. gradient methods COMPARE AI saliency map method. deep convolutional neural networks ( CNN ) COMPARE gradient methods. gradient methods COMPARE deep convolutional neural networks ( CNN ). accuracy EVALUATE-FOR It. Saliency Map Order Equivalence USED-FOR saliency measures. Layer Ordered Visualization of Information USED-FOR scale / layer contributions. scale information contributions PART-OF network. it COMPARE Guided Backprop. Guided Backprop COMPARE it. layers PART-OF network. forward pass USED-FOR layers. forward pass USED-FOR method. Grad - CAM++ CONJUNCTION Smooth Grad - CAM++. Smooth Grad - CAM++ CONJUNCTION Grad - CAM++. Grad - CAM CONJUNCTION Grad - CAM++. Grad - CAM++ CONJUNCTION Grad - CAM. method COMPARE Guided Backprop. Guided Backprop COMPARE method. method COMPARE class activation methods. class activation methods COMPARE method. Guided Backprop COMPARE class activation methods. class activation methods COMPARE Guided Backprop. Smooth Grad - CAM++ HYPONYM-OF class activation methods. Grad - CAM HYPONYM-OF class activation methods. Grad - CAM++ HYPONYM-OF class activation methods. cell phones CONJUNCTION low cost industrial devices. low cost industrial devices CONJUNCTION cell phones. robots CONJUNCTION cell phones. cell phones CONJUNCTION robots. resource limited platforms USED-FOR methods. low cost industrial devices HYPONYM-OF resource limited platforms. robots HYPONYM-OF resource limited platforms. cell phones HYPONYM-OF resource limited platforms. method USED-FOR CNNs 1. Generic is technique. OtherScientificTerm are network scale, and memory footprint. Method are saliency map, and saliency map methods. Task is satellite image processing. ",This paper proposes a new method to improve the accuracy of deep convolutional neural networks (CNNs) by using a saliency map. The proposed method uses a forward pass on the layers of the network to estimate the scale information contributions of each layer in the network. The authors show that the proposed method improves the performance of CNNs 1.5x over Guided Backprop and 3x over Smooth Grad-CAM++. ,"This paper proposes a new approach to improve the accuracy of deep convolutional neural networks (CNN) by learning a saliency map. The proposed approach is based on the Saliency Map Order Equivalence (SME) method. The authors show that the proposed approach can improve the performance of CNNs 1.0 and 2.0 on satellite image processing. It is shown that it is more accurate than gradient methods. The paper also shows that it outperforms Guided Backprop and Smooth Grad-CAM++. The method is evaluated on several resource limited platforms, including low cost industrial devices, robots, and cell phones. "
12366,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"Non - autoregressive models USED-FOR text generation tasks. position modeling PART-OF non - autoregressive text generation. PNAT USED-FOR text generative process. positions USED-FOR PNAT. PNAT COMPARE baselines. baselines COMPARE PNAT. machine translation CONJUNCTION paraphrase generation tasks. paraphrase generation tasks CONJUNCTION machine translation. paraphrase generation tasks EVALUATE-FOR PNAT. machine translation EVALUATE-FOR PNAT. OtherScientificTerm are positions of generated words, and latent variable. ","This paper studies the problem of position modeling in non-autoregressive text generation tasks. The authors propose to use PNAT to learn the positions of generated words in the text generative process. They show that PNAT can achieve better performance than baselines on machine translation, paraphrase generation tasks, and other tasks. ","This paper proposes a new method for training non-autoregressive models for text generation tasks. The key idea is to use position modeling in the text generative process. The positions of generated words are modeled as a latent variable. The authors show that PNAT is able to learn the positions of the generated words. They also show that the PNAT outperforms other baselines on machine translation and paraphrase generation tasks, which is a significant contribution. "
12375,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,GANs USED-FOR generative model analysis. Random Path Generative Adversarial Network ( RPGAN ) HYPONYM-OF GANs. random paths PART-OF generator network. latent space FEATURE-OF GAN. latent space PART-OF RPGAN. random paths PART-OF latent space. design USED-FOR factors of variation. natural interpretability FEATURE-OF factors of variation. generator layers USED-FOR factors of variation. layers USED-FOR image generation process. RPGAN USED-FOR image generation process. RPGAN model USED-FOR incremental learning. interpretability EVALUATE-FOR RPGAN model. generation quality EVALUATE-FOR RPGAN model. OtherScientificTerm is Gaussian distribution. ,"This paper studies the problem of generative model analysis in GANs. The authors propose a Random Path Generative Adversarial Network (RPGAN), a GAN with random paths in the latent space of the generator network. The design of RPGAN is based on the design of the factors of variation in the generator layers, which is a natural interpretability of the image generation process. The RPGAN model can be used for incremental learning, and is shown to improve the interpretability and the generation quality of the generated images. ",This paper proposes a novel generative model analysis method for generating adversarial examples. The method is based on the Random Path Generative Adversarial Network (RPGAN) which is a variant of GANs. The key idea of RPGAN is to use random paths in the latent space of the generator network to learn the factors of variation in the design. The generator layers are then used in the image generation process. The authors show that the RPGAN model improves interpretability and generation quality for incremental learning. 
12384,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"convolution USED-FOR spherical neural network. efficiency CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION efficiency. DeepSphere HYPONYM-OF method. graph representation of the sampled sphere USED-FOR method. graph USED-FOR equivariance. Generic is formulation. Method are anisotropic filters, and deepsphere. ","This paper proposes a new convolution for spherical neural network. The formulation is based on anisotropic filters, and the authors propose a method called DeepSphere. The proposed method uses a graph representation of the sampled sphere as a basis for the equivariance and efficiency. The authors show that the proposed deepsphere is able to achieve better performance than existing methods.","This paper proposes a new convolution for spherical neural network. The formulation is based on anisotropic filters. The proposed method is called DeepSphere, which is a graph representation of the sampled sphere. The authors show that the proposed method can achieve better efficiency and rotation equivariance. The main idea is to use the graph as a measure of equivariantness. The paper also shows that deepsphere is more efficient than the original DeepSphere."
12393,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"Learning Invariant Representations USED-FOR deep classifiers. invariance USED-FOR compression of representations. weighting representations USED-FOR representation distributions. adaptability EVALUATE-FOR weighting representations. compression CONJUNCTION invariance of learned representations. invariance of learned representations CONJUNCTION compression. adaptability EVALUATE-FOR representation. compression risk EVALUATE-FOR representation. learning weighted representations USED-FOR constraint of invariance. Metric are minimal combined domain error, and risk of compression. OtherScientificTerm are representation invariance, and constraint. Generic is bound. Material is domain adaptation benchmark. Method is adaptation methods. ","This paper studies the problem of learning Invariant Representations for deep classifiers. The authors propose a new bound on the compression of representations with invariance to the weighting representations of the representation distributions. The bound is based on the minimal combined domain error, which is the difference between the compression and invariance of learned representations. They show that the adaptability of a weighting representation with respect to the compression can be improved by learning weighted representations, and that the compression risk of a representation can be reduced by the constraint of invariance. They also provide a domain adaptation benchmark to evaluate the performance of different adaptation methods.","This paper studies the problem of learning Invariant Representations for deep classifiers. The main idea is to use weighting representations to learn representation distributions that are invariant to compression of representations. The authors propose a new bound on the risk of compression, which is based on minimizing the minimal combined domain error. They show that this bound is tighter than the standard domain adaptation benchmark. They also show that learning weighted representations can be used to enforce the constraint of invariance. Finally, they show that the adaptation methods are more robust to compression risk. "
12402,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"learning USED-FOR user interface attributes. it USED-FOR synthetic training dataset. colors CONJUNCTION border radius. border radius CONJUNCTION colors. border radius CONJUNCTION shadow or text properties. shadow or text properties CONJUNCTION border radius. shadow or text properties HYPONYM-OF attributes. border radius HYPONYM-OF attributes. colors HYPONYM-OF attributes. imitation learning USED-FOR neural policy. approach USED-FOR inferring Android Button attribute values. accuracy EVALUATE-FOR dataset. real - world Google Play Store applications FEATURE-OF dataset. dataset EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Task is user interface implementation. Method are black box rendering engine, and neural models. Metric is pixel - level accuracy. OtherScientificTerm are attribute space, and Android Button attribute values. ","This paper proposes a method for learning user interface attributes in a black box rendering engine. The key idea is to learn attributes such as colors, shadow or text properties, border radius, and border radius in the attribute space, and then use it to train a synthetic training dataset. The authors show that the proposed approach improves the accuracy of inferring Android Button attribute values on the dataset from real-world Google Play Store applications. They also show that their approach can be applied to inferring neural policy using imitation learning. ","This paper proposes a new approach to inferring Android Button attribute values for user interface implementation. The key idea is to use imitation learning to learn user interface attributes. The authors propose a black box rendering engine to generate attributes such as colors, shadow or text properties, border radius, etc. Then it is used to train a synthetic training dataset. The paper shows that the proposed approach improves the accuracy of the dataset on real-world Google Play Store applications. "
12411,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"Transfer learning USED-FOR neural network classifiers. data scarcity CONJUNCTION computational limitations. computational limitations CONJUNCTION data scarcity. robust feature extractors PART-OF robust networks. feature extractors USED-FOR classifiers. strategies USED-FOR models. Generic are network, and model. Method are full - scale training, robust transfer learning, lifelong learning strategies, and adversarial training. Metric are robustness, accuracy, and generalization of adversarially trained models. ","This paper studies the problem of robust transfer learning for neural network classifiers. The authors consider the case of full-scale training with robust feature extractors in robust networks. They show that the robustness of the network depends on the data scarcity and computational limitations. They then propose two strategies to improve the performance of the models. The first is lifelong learning strategies, where the model is trained with adversarial training. The second is generalization of adversarially trained models.","This paper studies the problem of full-scale training for neural network classifiers. The authors propose to use robust transfer learning to improve the robustness of classifiers with robust feature extractors in the training of robust networks. The main idea is to learn a network that is robust to adversarial perturbations, and then apply lifelong learning strategies to train the models. The paper shows that robustness improves the accuracy and generalization of adversarially trained models. "
12420,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"natural language USED-FOR complex concepts. compositionality USED-FOR complex concepts. natural language USED-FOR compositionality. neural agents USED-FOR language games. compositionality FEATURE-OF language. neural agents USED-FOR communication protocols. neural iterated learning ( NIL ) algorithm USED-FOR structured type of language. neural iterated learning ( NIL ) algorithm USED-FOR interacting neural agents. OtherScientificTerm are limited vocabulary, and compositional language. Generic is languages. Method are NIL, probabilistic model of NIL, and neural agent communication. ","This paper studies the problem of learning a structured type of language using a neural iterated learning (NIL) algorithm. The authors propose a probabilistic model of NIL, which can be used to learn a compositional language with compositionality in natural language for complex concepts. They show that the compositionality of the language can be learned using neural agents in language games. They also show that NIL can be applied to communication protocols for interacting neural agents. ","This paper proposes a novel approach to learning compositional languages. The authors propose a probabilistic model of NIL, where the compositional language is composed of a natural language and a limited vocabulary. The compositionality of the natural language is used to learn complex concepts, and the compositionality is used for learning complex concepts in a structured type of language. The paper also proposes a neural iterated learning (NIL) algorithm for interacting neural agents for interacting with language games. The proposed neural agents are trained to learn communication protocols, and they are evaluated on a variety of languages."
12429,SP:add48154b31c13f48aef740e665f23694fa83681,inference CONJUNCTION learning. learning CONJUNCTION inference. black - box algorithm USED-FOR inference. black - box algorithm USED-FOR learning. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR inference. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR learning. learning USED-FOR Markov random field ( MRF ). Adversarial Variational Inference and Learning ( AdVIL ) HYPONYM-OF black - box algorithm. variational distributions USED-FOR latent variables. AdVIL USED-FOR partition function. variational distributions USED-FOR partition function. partition function FEATURE-OF MRF. variational distributions USED-FOR AdVIL. variational distributions USED-FOR negative log - likelihood. negative log - likelihood FEATURE-OF MRF. stochastic gradient descent USED-FOR minimax optimization problem. minimax optimization problem USED-FOR negative log - likelihood. contrastive divergence COMPARE AdVIL. AdVIL COMPARE contrastive divergence. AdVIL USED-FOR MRFs. black - box methods COMPARE AdVIL. AdVIL COMPARE black - box methods. AdVIL USED-FOR log partition function. OtherScientificTerm is model structure. ,"This paper proposes Adversarial Variational Inference and Learning (AdVIL), a black-box algorithm for inference and learning in Markov random field (MRF). AdVIL uses variational distributions to learn the latent variables and the partition function of the MRF using stochastic gradient descent. The authors show that AdVil can learn a log partition function with negative log-likelihood in MRF with contrastive divergence, which is better than existing black -box methods for MRFs. ",This paper proposes Adversarial Variational Inference and Learning (AdVIL) for learning in Markov random field (MRF). AdVIL is a black-box algorithm for inference and learning. The authors show that the partition function of MRF with variational distributions for the latent variables is the same as the negative log-likelihood of the MRF under stochastic gradient descent. They also show that contrastive divergence can be used to improve the performance of MRFs compared to other black -box methods.
12438,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"low - dimensional state of the environment USED-FOR robot manipulation tasks. state estimator USED-FOR reward function. reinforcement learning USED-FOR policy. high - dimensional observations USED-FOR reward function. indicator reward function USED-FOR goal - conditioned reinforcement learning. continuous state spaces FEATURE-OF indicator reward function. reward balancing CONJUNCTION reward filtering. reward filtering CONJUNCTION reward balancing. methods USED-FOR convergence. indicator rewards USED-FOR methods. reward filtering HYPONYM-OF methods. indicator rewards USED-FOR convergence. reward balancing HYPONYM-OF methods. method USED-FOR tasks. continuous state spaces USED-FOR method. continuous state spaces FEATURE-OF tasks. RGB - D images HYPONYM-OF continuous state spaces. RGB - D images USED-FOR rope manipulation. rope manipulation HYPONYM-OF continuous state spaces. OtherScientificTerm are deformable objects, high - dimensional sensor inputs, positive reward, positive rewards, ground - truth state, and rewards. Method is end - to - end policy. ","This paper studies the problem of robot manipulation tasks in the low-dimensional state of the environment. The goal-conditioned reinforcement learning is to learn a policy from reinforcement learning using reinforcement learning with high-dimensional observations. The paper proposes a state estimator for the reward function that is based on the indicator reward function in the continuous state spaces of the goal- conditioned reinforcement learning. The reward function is learned by using high-dense observations from the environment, where deformable objects can be represented as high-dimensions of the ground-truth state. The authors propose two methods for convergence: reward balancing and reward filtering based on indicator rewards. The proposed method is evaluated on a variety of tasks in continuous state space (e.g., rope manipulation and RGB-D images) and shows that the proposed method achieves better performance than existing methods. ","The paper proposes a method for learning a low-dimensional state of the environment for robot manipulation tasks. The goal-conditioned reinforcement learning uses reinforcement learning to learn a policy based on high-dimensional observations of deformable objects. The reward function is based on the state estimator of the reward function, which is a function of high-dimensionality of the state. The paper proposes to use a positive reward to encourage the positive rewards to be close to the ground-truth state, while the negative rewards are used to guide the end-to-end policy. The proposed method is evaluated on three tasks in continuous state spaces (e.g., rope manipulation, RGB-D images, reward balancing) and two methods for convergence (reward balancing and reward filtering)."
12447,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"Robustness verification USED-FOR prediction behavior. Robustness verification USED-FOR safety guarantees. Robustness verification USED-FOR neural networks. architectures USED-FOR neural networks. robustness verification problem USED-FOR Transformers. cross - nonlinearity CONJUNCTION cross - position dependency. cross - position dependency CONJUNCTION cross - nonlinearity. self - attention layers FEATURE-OF Transformers. robustness verification algorithm USED-FOR Transformers. method COMPARE naive Interval Bound Propagation. naive Interval Bound Propagation COMPARE method. method COMPARE those. those COMPARE method. certified robustness bounds COMPARE those. those COMPARE certified robustness bounds. naive Interval Bound Propagation USED-FOR those. naive Interval Bound Propagation USED-FOR certified robustness bounds. method USED-FOR certified robustness bounds. bounds USED-FOR Transformers. OtherScientificTerm is model behavior. Task are verification, and sentiment analysis. Generic is they. ","This paper studies the problem of robustness verification for prediction behavior in neural networks. Robustness verification is an important problem for safety guarantees for neural networks, as it can be used to evaluate the model behavior. The authors consider the robustness of neural networks with self-attention layers in Transformers, and propose a method to verify the performance of these architectures. The proposed method is based on naive Interval Bound Propagation, and it is shown that the proposed method achieves better certified robustness bounds for Transformers than those obtained by the naive interval boundpropagation. ",This paper proposes a robustness verification algorithm for predicting the prediction behavior of neural networks. Robustness verification is important for safety guarantees. The authors propose two architectures for training neural networks: Transformers with self-attention layers and Transformers with cross-nonlinearity and cross-position dependency. The proposed method is compared to naive Interval Bound Propagation and those with certified robustness bounds. The results show that the proposed bounds are more robust to model behavior than those with no sentiment analysis.
12456,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"self - supervised learning USED-FOR natural language processing ( NLP ) tasks. pretrained language models USED-FOR self - supervised learning. syntactic and semantic NLP tasks EVALUATE-FOR pretrained models. real - world knowledge FEATURE-OF tasks. tasks EVALUATE-FOR pretrained models. zero - shot fact completion task USED-FOR pretrained models. BERT HYPONYM-OF pretrained models. weakly supervised pretraining objective USED-FOR model. objective USED-FOR Models. fact completion task EVALUATE-FOR objective. fact completion task EVALUATE-FOR Models. TriviaQA CONJUNCTION SearchQA. SearchQA CONJUNCTION TriviaQA. SearchQA CONJUNCTION Quasar - T. Quasar - T CONJUNCTION SearchQA. WebQuestions CONJUNCTION TriviaQA. TriviaQA CONJUNCTION WebQuestions. model COMPARE BERT. BERT COMPARE model. model USED-FOR downstream tasks. FIGER HYPONYM-OF fine - grained entity typing dataset. fine - grained entity typing dataset EVALUATE-FOR model. entity - related question answering datasets EVALUATE-FOR BERT. entity - related question answering datasets EVALUATE-FOR model. WebQuestions HYPONYM-OF entity - related question answering datasets. Quasar - T HYPONYM-OF entity - related question answering datasets. TriviaQA HYPONYM-OF entity - related question answering datasets. SearchQA HYPONYM-OF entity - related question answering datasets. Method is large - scale language modeling. OtherScientificTerm are knowledge, and real - world entities. ","This paper studies the problem of self-supervised learning for natural language processing (NLP) tasks. The authors propose a weakly supervised pretraining objective to train the model. Models are trained on a zero-shot fact completion task, where the goal is to learn the knowledge of the real-world entities. Models trained on this objective are evaluated on both syntactic and semantic NLP tasks. They show that pretrained models trained on these tasks perform better than BERT on both tasks.  The authors also show that BERT performs better on entity-related question answering datasets such as TriviaQA, SearchQA and Quasar-T, as well as on a fine-grained entity typing dataset called FIGER. The model is also shown to perform better on downstream tasks.","This paper presents a new approach to self-supervised learning for natural language processing (NLP) tasks. The authors propose a weakly supervised pretraining objective for the model, which can be applied to both syntactic and semantic NLP tasks. They show that pretrained models on these tasks are able to generalize well to real-world knowledge. They also show that the proposed objective can be used to improve the performance of Models on the zero-shot fact completion task. The model is evaluated on a fine-grained entity typing dataset (FigER) and two entity-related question answering datasets (Quasar-T and TriviaQA). The model outperforms BERT on both entity-wise and entity-level question answering tasks. "
12465,SP:4395d6f3e197df478eee84e092539dc370babd97,"image collection USED-FOR discovering novel classes. setting COMPARE semi - supervised learning. semi - supervised learning COMPARE setting. self - supervised learning USED-FOR representation. supervised classification of the labelled data CONJUNCTION clustering of the unlabelled data. clustering of the unlabelled data CONJUNCTION supervised classification of the labelled data. rank statistics USED-FOR model. rank statistics USED-FOR clustering the unlabelled images. model USED-FOR clustering the unlabelled images. labelled and unlabelled subsets of the data USED-FOR joint objective function. labeled data USED-FOR image representation. joint objective function USED-FOR data representation. classification benchmarks EVALUATE-FOR methods. approach COMPARE methods. methods COMPARE approach. methods USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR novel category discovery. approach USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR approach. OtherScientificTerm is labelled images. Method is general - purpose clustering model. Generic is latter. Material are unlabelled data, and labelled and unlabelled data. ","This paper proposes a general-purpose clustering model for the task of discovering novel classes from unlabelled data. The proposed method is based on self-supervised learning, where the goal is to learn a representation from the labeled data, rather than the unlabeled data. In this setting, the authors show that the proposed approach outperforms existing methods on several classification benchmarks for novel category discovery. The authors also show that their model can be used for clustering the unlabelling images using rank statistics.","This paper proposes a novel approach for discovering novel classes from image collection. The proposed setting is different from semi-supervised learning, where the representation is learned by self-supervision. The authors propose a general-purpose clustering model that learns a joint objective function between labelled and unlabelled subsets of the data. The model uses rank statistics for the classification of the labelled data and the clustering of the unlabelling images. The latter is done using the latter. The paper shows that the proposed approach outperforms existing methods on several classification benchmarks for novel category discovery."
12474,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"selfsupervised robot interaction USED-FOR images. images HYPONYM-OF dynamical system. VP algorithms USED-FOR robotic manipulation and navigation domains. data - driven perception and planning USED-FOR VP algorithms. approach USED-FOR VP. nodes PART-OF graph. connectivity PART-OF graph. image samples PART-OF graph. semiparametric topological memory ( SPTM ) method HYPONYM-OF approach. deep image classification USED-FOR connectivity. topological connectivity FEATURE-OF graph. graph search methods USED-FOR planning. loss function USED-FOR connectivity classifier. manual tuning USED-FOR loss function. loss function USED-FOR SPTM. discriminative classifier USED-FOR energy function. contrastive predictive coding USED-FOR discriminative classifier. contrastive predictive coding USED-FOR energy function. hallucinated samples USED-FOR connectivity graph. connectivity graph USED-FOR zero - shot generalization. domain changes FEATURE-OF zero - shot generalization. simulated domains EVALUATE-FOR HTM. SPTM CONJUNCTION visual foresight methods. visual foresight methods CONJUNCTION SPTM. simulated domains EVALUATE-FOR visual foresight methods. HTM COMPARE SPTM. SPTM COMPARE HTM. HTM COMPARE visual foresight methods. visual foresight methods COMPARE HTM. simulated domains EVALUATE-FOR SPTM. visual foresight methods USED-FOR long - horizon planning. long - horizon planning EVALUATE-FOR HTM. plan quality EVALUATE-FOR visual foresight methods. plan quality EVALUATE-FOR HTM. Task is visual planning ( VP ). Method are Hallucinative Topological Memory ( HTM ), and conditional VAE model. OtherScientificTerm is context image of the domain. ","This paper proposes a new approach for visual planning (VP) based on the semiparametric topological memory (SPTM) method. The idea is to use selfsupervised robot interaction to generate images from a dynamical system (e.g., images of a robot interacting with a robot) and then use these images to train VP algorithms for robotic manipulation and navigation domains. The proposed approach is based on data-driven perception and planning, where the goal is to learn the connectivity between nodes in the graph. The connectivity graph is learned by deep image classification. The planning is done by graph search methods. The loss function for the connectivity classifier is learned via manual tuning. The energy function is learned using contrastive predictive coding and the discriminative classifier. The conditional VAE model is trained using the hallucinated samples from the connectivity graph. Experiments on simulated domains show that HTM outperforms SPTM and visual foresight methods in terms of plan quality and long-horizon planning.","This paper proposes a novel approach to learning a graph with topological connectivity in a dynamical system (e.g. images). The proposed approach is based on the semiparametric topological memory (SPTM) method. The authors propose a conditional VAE model, called Hallucinative Topological Memory (HTM), to learn a connectivity graph with hallucinated samples. The connectivity graph consists of nodes in a graph, and a set of image samples in the context image of the domain. The graph search methods are used for planning in robotic manipulation and navigation domains. The proposed VP algorithms are based on data-driven perception and planning. The loss function of the connectivity classifier is learned by manual tuning. The discriminative classifier uses contrastive predictive coding to learn the energy function. The paper shows that HTM outperforms SPTM on simulated domains and visual foresight methods for long-horizon planning. "
12483,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"Large - scale benchmark datasets USED-FOR AI. benchmarks USED-FOR realistic problem distributions. spurious biases FEATURE-OF them. similar ( thus non - tail ) problems FEATURE-OF benchmarks. long - tail problems PART-OF real world problems. Adversarial Filters USED-FOR model - based reduction of dataset biases. AFLITE HYPONYM-OF iterative greedy algorithm. AFLITE USED-FOR reduced dataset. realistic problem distributions CONJUNCTION spurious biases. spurious biases CONJUNCTION realistic problem distributions. spurious biases FEATURE-OF reduced dataset. realistic problem distributions FEATURE-OF reduced dataset. AFOPTIMUM USED-FOR optimum bias reduction. AFLITE USED-FOR task. MNLI CONJUNCTION QNLI. QNLI CONJUNCTION MNLI. SNLI CONJUNCTION MNLI. MNLI CONJUNCTION SNLI. ImageNet CONJUNCTION SNLI. SNLI CONJUNCTION ImageNet. it USED-FOR benchmarks. SNLI CONJUNCTION QNLI. QNLI CONJUNCTION SNLI. ImageNet HYPONYM-OF benchmarks. QNLI HYPONYM-OF benchmarks. SNLI HYPONYM-OF benchmarks. MNLI HYPONYM-OF benchmarks. AFLITE USED-FOR measurable dataset biases. measurable dataset biases FEATURE-OF synthetic and real datasets. synthetic and real datasets EVALUATE-FOR AFLITE. K - nearest - neighbors USED-FOR dataset biases. Generic are filtered counterparts, and model. Metric is human performance. Task is bias reduction. ","This paper proposes a new algorithm AFLITE, which is an iterative greedy algorithm based on Adversarial Filters to reduce the model-based reduction of dataset biases. The authors show that AFLITE achieves the best performance on both synthetic and real datasets with measurable dataset biases on both realistic problem distributions and spurious biases on them. They also show that AFOPTIMUM can achieve the optimum bias reduction on this task. ","This paper presents a new algorithm AFLITE, an iterative greedy algorithm for learning realistic problem distributions from large-scale benchmark datasets. The proposed algorithm is based on Adversarial Filters, which is a model-based reduction of dataset biases. The authors show that AFLITE is able to learn a reduced dataset with spurious biases, which are similar (though non-tail) problems in the benchmarks, and achieve better performance than their filtered counterparts. They also show that the proposed algorithm AFOPTIMUM can achieve optimum bias reduction with K-nearest-neighbors, and that it outperforms other benchmarks such as MNLI, SNLI, QNLI, and ImageNet. "
12492,SP:82777947d2377efa897c6905261f5375b29a4c19,"decision metric USED-FOR models. matching networks CONJUNCTION prototypical networks. prototypical networks CONJUNCTION matching networks. prototypical networks PART-OF few - shot models. matching networks PART-OF few - shot models. softmax USED-FOR relative distance. batch normalization USED-FOR centering. Gaussian layer USED-FOR distance calculation. Gaussian layer USED-FOR prototypical network. support examples ’ distribution COMPARE centroid. centroid COMPARE support examples ’ distribution. distance calculation PART-OF prototypical network. support examples ’ distribution USED-FOR Gaussian layer. Method are Few - shot models, and prototypical few - shot models. Generic are They, and extension. OtherScientificTerm are class belongings, and null class ”. Material are Omniglot data set, matched test set, unmatched MNIST data, and MiniImageNet data set. Metric are classification accuracy, and test accuracy. ",This paper proposes a new decision metric for few-shot models. The main idea is to use the softmax of the relative distance between the support examples’ distribution and the prototypical networks in few-shoot models (e.g. matching networks and prototypical neural networks). The authors show that the distance calculation in a prototypical network with a Gaussian layer can be approximated by the distance between support examples and the support network’s distribution. They also show that batch normalization can be used to improve the centering. The authors also propose an extension of the Omniglot data set to the matched test set. They show that this extension can improve the classification accuracy. ,"This paper proposes a new decision metric for models. Few-shot models can be seen as prototypical networks with matching networks. They are trained on the Omniglot data set, matched test set, and unmatched MNIST data. The authors propose a new extension to the class belongings, called “null class”. They use softmax to measure the relative distance between the support examples’ distribution and the centroid of the prototypical network with a Gaussian layer with batch normalization for centering. They evaluate the proposed classification accuracy on the MiniImageNet data set. "
12501,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"Graph embedding techniques USED-FOR applications. learning on non - Euclidean data USED-FOR applications. node attribute information PART-OF graph embedding models. computational complexity CONJUNCTION memory usage. memory usage CONJUNCTION computational complexity. graph fusion USED-FOR graph. topology FEATURE-OF graph. topology CONJUNCTION node attribute information. node attribute information CONJUNCTION topology. GraphZoom HYPONYM-OF multi - level framework. graph CONJUNCTION node attribute information. node attribute information CONJUNCTION graph. graph fusion USED-FOR GraphZoom. it USED-FOR embeddings. embedding methods USED-FOR coarsened graph. GraphZoom USED-FOR embedding methods. graph datasets USED-FOR transductive and inductive tasks. transductive and inductive tasks EVALUATE-FOR approach. graph datasets EVALUATE-FOR approach. GraphZoom COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE GraphZoom. GraphZoom USED-FOR graph embedding process. graph embedding process COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE graph embedding process. classification accuracy EVALUATE-FOR GraphZoom. OtherScientificTerm is node attribute noise. Metric are accuracy, and scalability. Generic is them. ","This paper proposes GraphZoom, a multi-level framework for learning graph embeddings on non-Euclidean data. The authors propose to use graph fusion to learn the topology of a graph and the node attribute information of the graph, which is then used to train a graph embedding model. The proposed method is evaluated on a variety of tasks, including classification and inductive learning. The results show that the proposed method achieves state-of-the-art performance in terms of classification accuracy.","This paper proposes GraphZoom, a multi-level framework for learning on non-Euclidean data. Graph embedding techniques are commonly used in applications for applications where the topology of the graph and node attribute information of graph embedding models are not available. The authors propose to use graph fusion to map the graph to a coarsened graph by graph fusion, and then use it to compute the embeddings. They show that graph fusion can reduce the computational complexity and memory usage. They also show that it can reduce node attribute noise, and improve the accuracy. They evaluate the proposed approach on several graph datasets for both transductive and inductive tasks, and compare it to other unsupervised embedding methods. They find that graphZoom outperforms other graph encoding methods in terms of classification accuracy, and also improves the scalability."
12510,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,predictive coding USED-FOR image compression models. neural autoregressive image generation models USED-FOR absolute pixel intensities. prediction COMPARE absolute counterpart. absolute counterpart COMPARE prediction. model USED-FOR sharp transitions. model USED-FOR smooth transitions. absolute predictor USED-FOR model. relative predictor USED-FOR generating smooth transitions. mechanism COMPARE absolute prediction counterparts. absolute prediction counterparts COMPARE mechanism. unconditional image generation CONJUNCTION image colorization. image colorization CONJUNCTION unconditional image generation. image colorization CONJUNCTION super - resolution. super - resolution CONJUNCTION image colorization. benchmarks EVALUATE-FOR mechanism. benchmarks EVALUATE-FOR super - resolution. likelihood EVALUATE-FOR absolute prediction counterparts. benchmarks EVALUATE-FOR unconditional image generation. benchmarks EVALUATE-FOR absolute prediction counterparts. image colorization HYPONYM-OF benchmarks. likelihood EVALUATE-FOR mechanism. Material is natural images. Method is unified probabilistic model. ,"This paper proposes a unified probabilistic model for image compression models. The model is based on a relative predictor that predicts the absolute pixel intensities of the input image. The relative predictor is then used to train the model to generate smooth transitions. The authors show that the proposed model is able to generate sharp transitions with a lower likelihood than the absolute counterpart. The proposed mechanism is evaluated on three benchmarks: unconditional image generation, image colorization, and super-resolution.","This paper proposes a new predictive coding for image compression models. The authors propose to use neural autoregressive image generation models to predict absolute pixel intensities of natural images. The proposed unified probabilistic model is based on a relative predictor. The model is trained to predict sharp transitions and smooth transitions, and the authors show that the proposed mechanism outperforms the absolute prediction counterparts in terms of likelihood, unconditional image generation, image colorization, and super-resolution."
12519,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"reinforcement learning CONJUNCTION Monte Carlo methods. Monte Carlo methods CONJUNCTION reinforcement learning. stationary distribution FEATURE-OF Markov chain. stationary distribution USED-FOR estimating quantities. Markov chain USED-FOR estimating quantities. estimation USED-FOR applications. variational divergence minimization USED-FOR constraint reformulations. off - line PageRank CONJUNCTION off - policy policy evaluation. off - policy policy evaluation CONJUNCTION off - line PageRank. off - policy policy evaluation HYPONYM-OF benchmark problems. off - line PageRank HYPONYM-OF benchmark problems. Task are real - world applications, and consistent estimation. OtherScientificTerm are transition operator, and stationary and empirical distributions. Generic are approach, and algorithm. Method are GenDICE, and error analysis. ","This paper proposes a new approach for consistent estimation in real-world applications where the transition operator is stationary and the stationary distribution of the Markov chain is not known. The authors propose a new algorithm called GenDICE, which is based on variational divergence minimization for constraint reformulations. The proposed algorithm is evaluated on two benchmark problems: off-line PageRank and off-policy policy evaluation. The results show that the proposed algorithm achieves better performance than existing methods. ","The paper proposes a new approach to improve the robustness of reinforcement learning and Monte Carlo methods in real-world applications. The key idea is to use the stationary distribution of the Markov chain of the transition operator for estimating quantities. The stationary distribution is then used for the estimation of applications such as off-line PageRank, off-policy policy evaluation, and constraint reformulations. The proposed algorithm is based on GenDICE. The paper is well-written and easy to follow. However, the error analysis is not very clear. "
12528,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"machine learning systems USED-FOR spurious patterns. statistical frameworks USED-FOR term. models USED-FOR spurious patterns. sentiment analysis CONJUNCTION natural language inference tasks. natural language inference tasks CONJUNCTION sentiment analysis. classifiers USED-FOR natural language inference tasks. classifiers USED-FOR sentiment analysis. classifiers COMPARE models. models COMPARE classifiers. classifiers USED-FOR spurious features. original or manipulated data USED-FOR classifiers. OtherScientificTerm are spurious associations, confounding, direct or indirect causal effects, and internal coherence. Task is natural language processing. Method is Classifiers. ","This paper studies the problem of spurious associations in machine learning systems. The authors propose a new term, “spurious associations”, which is defined as spurious patterns that arise from confounding, direct or indirect causal effects. The term is defined in the context of natural language processing, and the authors propose two statistical frameworks to define the term. The proposed classifiers can be used for sentiment analysis and natural language inference tasks. The classifiers are trained on original or manipulated data, and are able to identify spurious features in the original data. Classifiers are shown to be more robust to spurious features than existing models. ","This paper proposes a new term to describe spurious patterns in machine learning systems. The term is based on statistical frameworks, and is motivated by the fact that spurious associations can be caused by direct or indirect causal effects. The authors show that classifiers trained on original or manipulated data are more likely to produce spurious features than models trained on natural language processing. They also show that the classifiers can be used for sentiment analysis and natural language inference tasks. Classifiers can also be used to improve internal coherence."
12537,SP:b720eb5b6e44473a9392cc572af89270019d4c42,super - resolution CONJUNCTION image restoration. image restoration CONJUNCTION super - resolution. CNN based image quality assessment CONJUNCTION super - resolution. super - resolution CONJUNCTION CNN based image quality assessment. full - reference perceptual quality features USED-FOR CNN based image quality assessment. image restoration CONJUNCTION imageto - image translation problems. imageto - image translation problems CONJUNCTION image restoration. CNN based image quality assessment CONJUNCTION image restoration. image restoration CONJUNCTION CNN based image quality assessment. frequency and orientation tuning of channels PART-OF trained image classification deep CNNs. spatial frequencies FEATURE-OF grating stimuli. VGG-16 HYPONYM-OF trained image classification deep CNNs. CNN channels USED-FOR human visual perception models. CNN channels USED-FOR spatial frequency and orientation selective filters. deep CNN representations USED-FOR perceptual quality features. contrast masking thresholds FEATURE-OF human visual perception. orientation selectivity FEATURE-OF deep CNN channels. perceptual quality features FEATURE-OF deep CNN channels. contrast masking thresholds FEATURE-OF spatial frequencies. ,"This paper studies the problem of image restoration and image restoration with CNN based image quality assessment with full-reference perceptual quality features. The authors propose to use frequency and orientation tuning of channels in trained image classification deep CNNs such as VGG-16, which are trained on spatial frequencies of grating stimuli. The proposed CNN channels are trained to be spatial frequency- and orientation-selective filters and are trained with contrast masking thresholds for human visual perception models. They show that deep CNN representations are able to capture the perceptual quality of deep CNN channels with respect to spatial frequencies and orientation selectivity.","This paper proposes a novel approach to improve the performance of CNN based image quality assessment, image restoration, and image restoration with full-reference perceptual quality features. The key idea is to use frequency and orientation tuning of channels in trained image classification deep CNNs such as VGG-16, where spatial frequencies of grating stimuli are sampled from a set of spatial frequencies and orientation selective filters are used. The authors show that deep CNN channels can be used to improve human visual perception models with contrast masking thresholds for the spatial frequencies as well as for the orientation selectivity of deep CNN representations."
12546,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,drug development CONJUNCTION medical practice. medical practice CONJUNCTION drug development. graph neural networks USED-FOR task. nodes CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION nodes. drugs CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION drugs. drugs CONJUNCTION nodes. nodes CONJUNCTION drugs. graph neural networks USED-FOR DDI predictions. link prediction problems USED-FOR DDI predictions. graph energy neural network ( GENN ) USED-FOR link type correlations. structure prediction problem USED-FOR DDI prediction task. graph neural networks USED-FOR energy function. real world DDI datasets EVALUATE-FOR GENN. GENN COMPARE baselines. baselines COMPARE GENN. real world DDI datasets EVALUATE-FOR baselines. PR - AUC EVALUATE-FOR datasets. datasets EVALUATE-FOR GENN. PR - AUC EVALUATE-FOR GENN. GENN USED-FOR DDI correlations. GENN COMPARE baseline models. baseline models COMPARE GENN. Task is drug - drug interactions ( DDIs ). OtherScientificTerm is DDI types. Method is energy - based model. ,"This paper studies the problem of drug-drug interactions (DDIs) in the context of drug development and medical practice. In this task, graph neural networks are used to perform DDI predictions using link prediction problems. The authors propose a graph energy neural network (GENN) to predict link type correlations between nodes, drugs, and other DDI types. The energy function of the graph neural network is then used to predict the energy function for each DDI prediction task using a structure prediction problem. Experiments on real world DDI datasets show that GENN outperforms baselines on PR-AUC and other datasets. ","This paper proposes a new model for predicting drug-drug interactions (DDIs) based on graph neural networks. The authors propose a graph energy neural network (GENN) to predict link type correlations between two DDI types. This is an energy-based model, which can be applied to both drug development and medical practice. The DDI prediction task is based on a structure prediction problem, where the energy function of the graph network is used to predict the DDI predictions. Experiments on two real world DDI datasets show that GENN outperforms the baselines on PR-AUC, and outperforms baseline models on DDI correlations. "
12555,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,vq - wav2vec USED-FOR discrete representations of audio segments. online k - means clustering USED-FOR dense representations. Gumbel - Softmax CONJUNCTION online k - means clustering. online k - means clustering CONJUNCTION Gumbel - Softmax. algorithm USED-FOR dense representations. online k - means clustering USED-FOR algorithm. Gumbel - Softmax USED-FOR algorithm. Discretization USED-FOR algorithms. discrete inputs USED-FOR algorithms. TIMIT phoneme classification EVALUATE-FOR BERT pre - training. ,This paper proposes a new algorithm for learning discrete representations of audio segments using vq-wav2vec. The algorithm is based on Gumbel-Softmax and online k-means clustering to learn dense representations. Discretization is used to improve the performance of the algorithms. The experimental results show that the proposed algorithms perform well on TIMIT phoneme classification and BERT pre-training.,This paper proposes a new algorithm for learning discrete representations of audio segments using vq-wav2vec. The algorithm is based on Gumbel-Softmax and online k-means clustering to learn dense representations. Discretization is used to improve the performance of the proposed algorithms. The algorithms are evaluated on TIMIT phoneme classification and BERT pre-training. 
12564,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,methods USED-FOR collaborative filtering models. methods USED-FOR ranking - based objective functions. actor - critic reinforcement learning USED-FOR methods. critic network USED-FOR ranking - based metrics. actor network USED-FOR metrics. critic - based method USED-FOR approximate ) ranking scores. critic - based method USED-FOR scoring process. learning - to - rank methods COMPARE critic - based method. critic - based method COMPARE learning - to - rank methods. neural network USED-FOR critic - based method. optimization procedure USED-FOR learning - to - rank methods. neural network USED-FOR scoring process. actor - critic COMPARE baselines. baselines COMPARE actor - critic. prediction models COMPARE baselines. baselines COMPARE prediction models. actor - critic USED-FOR prediction models. large - scale datasets EVALUATE-FOR actor - critic. large - scale datasets EVALUATE-FOR baselines. ,"This paper proposes a new method for collaborative filtering models. The proposed methods are based on actor-critic reinforcement learning, where the critic network is trained to generate ranking-based objective functions. The scoring process is based on a critic-based method, which uses a neural network to generate (approximate) ranking scores. The authors show that the proposed method outperforms existing learning-to-rank methods in terms of the optimization procedure, and outperforms baselines on large-scale datasets. ","This paper proposes two methods for collaborative filtering models. The methods are based on actor-critic reinforcement learning, where the goal is to learn ranking-based objective functions. The key idea is to use a critic network to compute the ranking -based metrics and then use an actor network to evaluate the metrics. The scoring process is performed using a neural network, and the critic-based method is used to compute (or approximate) ranking scores. The authors show that the proposed learning-to-rank methods outperform learning-based methods in terms of optimization procedure, and outperform baselines on large-scale datasets. "
12573,SP:2444a83ae08181b125a325d893789f074d6db8ee,"off - policy reinforcement learning methods USED-FOR robot control. data - efficiency EVALUATE-FOR Deep Q - learning. multi - step TD - learning USED-FOR data - efficiency. Truncated Q - functions HYPONYM-OF TemporalDifference formulations. Shifted Q - functions USED-FOR farsighted return. Model - based Value Expansion CONJUNCTION TD3(∆ ). TD3(∆ ) CONJUNCTION Model - based Value Expansion. TD3 CONJUNCTION Model - based Value Expansion. Model - based Value Expansion CONJUNCTION TD3. approach COMPARE TD3. TD3 COMPARE approach. TD3(∆ ) HYPONYM-OF off - policy variant of TD(∆ ). approach COMPARE Model - based Value Expansion. Model - based Value Expansion COMPARE approach. approach USED-FOR function - approximation setting. function - approximation setting CONJUNCTION TD3. TD3 CONJUNCTION function - approximation setting. tabular case FEATURE-OF Composite Q - learning. simulated robot tasks EVALUATE-FOR Composite TD3. Composite TD3 COMPARE TD3. TD3 COMPARE Composite TD3. Composite TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE Composite TD3. simulated robot tasks EVALUATE-FOR off - policy multi - step approaches. simulated robot tasks EVALUATE-FOR TD3. TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE TD3. data - efficiency EVALUATE-FOR TD3. data - efficiency EVALUATE-FOR off - policy multi - step approaches. data - efficiency EVALUATE-FOR Composite TD3. Task is realworld applications. OtherScientificTerm are off - policy, target - policy rollout, truncated rollout, and shortand long - term predictions. Method is Composite Q - learning algorithm. ","This paper studies the problem of off-policy reinforcement learning methods for robot control. The authors propose a new off policy variant of TD(∆) called Composite TD3, which is a combination of Model-based Value Expansion and TD3. The main idea is to use multi-step TD-learning to improve the data-efficiency of Deep Q-learning by using truncated Q-functions in TemporalDifference formulations. The proposed approach is evaluated on simulated robot tasks and in a function-approximation setting. The results show that the proposed approach performs better than TD3 in terms of data-efficacy in the function-augmentation setting and better than the off-probability setting. ","This paper proposes a novel off-policy reinforcement learning methods for robot control. The main idea is to use multi-step TD-learning to improve the data-efficiency of Deep Q-learning in order to reduce the number of steps needed to learn a target-policy rollout. The proposed approach is a combination of Model-based Value Expansion and TD3(∆), a variant of TD(∁) which is a truncated version of TemporalDifference formulations. Shifted Q-functions are used for farsighted return. The authors show that the proposed approach outperforms TD3 in the function-approximation setting and in the tabular case. Experiments are conducted on simulated robot tasks and simulated robot environments. The results show the effectiveness of the proposed Composite Q- learning algorithm. "
12582,SP:64564b09bd68e7af17845019193825794f08e99b,"reinforcement learning USED-FOR real world robotics. dexterous manipulation USED-FOR system. manually designed resets USED-FOR learning. on - board perception USED-FOR manually designed resets. on - board perception USED-FOR learning. dexterous robotic manipulation tasks EVALUATE-FOR system. system USED-FOR vision - based skills. real - world three - fingered hand FEATURE-OF vision - based skills. Material is instrumented laboratory scenarios. Method are continuous learning, and robotic learning system. OtherScientificTerm are hand - engineered reward functions, and human intervention. Generic are solutions, and learning paradigm. ","This paper studies the problem of reinforcement learning in real world robotics. The authors propose a system based on dexterous manipulation, where the goal is to learn a set of hand-engineered reward functions. The system is trained using on-board perception and manually designed resets to improve the performance of the learning. The proposed system is evaluated on a variety of dexterous robotic manipulation tasks and shows that the system can learn vision-based skills in a real-world three-fingered hand. ","This paper proposes a new continuous learning framework for real world robotics. The system is based on dexterous manipulation, where the goal is to learn a system that can be applied to real world robotic manipulation tasks. The main idea is to use hand-engineered reward functions to guide the learning of a robotic learning system. The authors propose to use on-board perception to guide learning and manually designed resets for learning. The proposed solutions are evaluated on a variety of dexterous robotic manipulation environments, including instrumented laboratory scenarios and real-world three-fingered hand. The results show that the proposed system is able to learn vision-based skills in a real world three-fingered hand, and that the system is more robust to human intervention. "
12591,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"adversarial examples USED-FOR Neural network robustness. labeled data USED-FOR adversarially robust generalization. perturbed test data EVALUATE-FOR networks. unlabeled data USED-FOR model. adversarially robust generalization FEATURE-OF model. classification accuracy EVALUATE-FOR accuracy part. accuracy part HYPONYM-OF parts. unlabeled data USED-FOR part. adversarially robust generalization COMPARE generalization. generalization COMPARE adversarially robust generalization. generalization PART-OF supervised learning. adversarially robust generalization USED-FOR Gaussian mixture problem. adversarial training algorithm USED-FOR adversarial robust generalization. MNIST CONJUNCTION Cifar-10. Cifar-10 CONJUNCTION MNIST. MNIST USED-FOR adversarial robust generalization. Cifar-10 USED-FOR adversarial robust generalization. unlabeled data USED-FOR adversarial training algorithm. Method is risk decomposition theorem. OtherScientificTerm are expected robust risk, prediction stability, perturbations, and label information. Metric is stability part. ",This paper studies the problem of adversarial examples in Neural network robustness. The authors consider the case where the model is trained on unlabeled data and the label information is not available. They show that adversarially robust generalization of the model can be achieved with perturbed test data. They also provide a risk decomposition theorem that shows that the expected robust risk is bounded by the number of perturbations. ,"This paper studies adversarial examples for improving Neural network robustness to perturbations. The authors show that adversarially robust generalization of networks on perturbed test data improves the classification accuracy of these networks. The main contribution of the paper is the risk decomposition theorem, which shows that the expected robust risk does not depend on the prediction stability, but rather on the label information of the model. The paper also provides a generalization result for adversarial training algorithm that is based on adversarial robust generalisation of the Gaussian mixture problem. "
12600,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"estimation of advantage USED-FOR reinforcement learning algorithms. promotion focus CONJUNCTION prevention focus. prevention focus CONJUNCTION promotion focus. order statistics FEATURE-OF path ensemble. promotion focus FEATURE-OF learning process. prevention focus FEATURE-OF learning process. promotion focus COMPARE prevention focus. prevention focus COMPARE promotion focus. Terrain locomotion CONJUNCTION Atari games. Atari games CONJUNCTION Terrain locomotion. Atari games CONJUNCTION sparse - reward environments. sparse - reward environments CONJUNCTION Atari games. MuJoCo continuous control CONJUNCTION Terrain locomotion. Terrain locomotion CONJUNCTION MuJoCo continuous control. benchmarks EVALUATE-FOR schemes. schemes COMPARE mainstream methods. mainstream methods COMPARE schemes. benchmarks EVALUATE-FOR mainstream methods. MuJoCo continuous control HYPONYM-OF benchmarks. sparse - reward environments HYPONYM-OF benchmarks. Atari games HYPONYM-OF benchmarks. Terrain locomotion HYPONYM-OF benchmarks. Generic are it, and formulation. OtherScientificTerm are regulatory focuses, regulatory focus, and sparse rewards. ","This paper studies the estimation of advantage for reinforcement learning algorithms with regulatory focuses. The authors propose a new formulation, called regulatory focus, and show that it can be used to improve the performance of the learning process with promotion focus and prevention focus in a learning process where the order statistics of the path ensemble are not available. The paper also shows that the proposed schemes outperform mainstream methods on several benchmarks, including MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments.","This paper studies the estimation of advantage for reinforcement learning algorithms. The authors propose a new formulation of it, which is based on regulatory focuses. The key idea is to use order statistics of the path ensemble as a guide to guide the learning process, where the promotion focus and the prevention focus are combined. The paper shows that the proposed schemes outperform other mainstream methods on three benchmarks: MuJoCo continuous control, Terrain locomotion, and sparse-reward environments. "
12609,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"precision gating ( PG ) USED-FOR deep neural networks. approach USED-FOR DNN architectures. computational cost EVALUATE-FOR DNN execution. PG USED-FOR CNNs. PG USED-FOR statically compressed mobile - friendly networks. statically compressed mobile - friendly networks HYPONYM-OF CNNs. ShuffleNet HYPONYM-OF statically compressed mobile - friendly networks. prediction - based quantization schemes COMPARE PG. PG COMPARE prediction - based quantization schemes. compute EVALUATE-FOR PG. ImageNet EVALUATE-FOR PG. accuracy EVALUATE-FOR PG. PG USED-FOR RNNs. 8 - bit uniform quantization COMPARE PG. PG COMPARE 8 - bit uniform quantization. computational cost reduction EVALUATE-FOR LSTM. Penn Tree Bank dataset EVALUATE-FOR LSTM. computational cost reduction EVALUATE-FOR PG. perplexity EVALUATE-FOR PG. Metric are precision, and accuracy loss. ",This paper proposes precision gating (PG) for deep neural networks. The authors propose a new approach to improve the performance of DNN architectures by reducing the computational cost of the DNN execution. The proposed PG can be applied to both CNNs such as ShuffleNet and static compressed mobile-friendly networks. They show that PG improves the accuracy of RNNs by a large margin compared to 8-bit uniform quantization and prediction-based quantization schemes. The computational cost reduction of PG is demonstrated on ImageNet and LSTM on the Penn Tree Bank dataset. ,"This paper proposes precision gating (PG) for deep neural networks. PG is a novel approach to improve the computational cost of DNN architectures. PG can be applied to CNNs such as static compressed mobile-friendly networks, such as ShuffleNet, or CNNs with fewer parameters. The authors show that PG can reduce the computation cost for DNN execution by a factor of 1/\sqrt{n}^n. They also show that the accuracy loss of PG is lower than that of prediction-based quantization schemes, and that PG outperforms 8-bit uniform quantization in terms of computational cost reduction on the Penn Tree Bank dataset. The paper also shows that PG improves the accuracy of RNNs on ImageNet."
12618,SP:0c2c9b80c087389168acdd42af15877fb499449b,"clean labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION clean labeled data. unlabeled data PART-OF TD. classifiers PART-OF unsupervised domain adaptation ( UDA ). unlabeled data USED-FOR classifiers. clean labeled data USED-FOR classifiers. noisy labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION noisy labeled data. unlabeled data PART-OF TD. unlabeled data USED-FOR classifiers. noisy labeled data USED-FOR classifiers. Butterfly framework USED-FOR WUDA. solution USED-FOR WUDA. label noise FEATURE-OF SD. Butterfly framework HYPONYM-OF solution. WUDA COMPARE UDA methods. UDA methods COMPARE WUDA. classification USED-FOR TD. models PART-OF Butterfly. deep networks HYPONYM-OF models. Butterfly USED-FOR WUDA. Butterfly COMPARE baseline methods. baseline methods COMPARE Butterfly. WUDA EVALUATE-FOR Butterfly. WUDA EVALUATE-FOR baseline methods. OtherScientificTerm are noisy - to - clean, and SD - to - TD - distributional. ","This paper proposes a new unsupervised domain adaptation (UDA) method based on the Butterfly framework. The main idea is to combine clean labeled data and unlabeled data in the TD with the classifiers in the unlabelled data in order to improve the performance of classifiers trained on the clean labelled data. The proposed solution, called WUDA, is based on Butterfly framework and is able to achieve better performance than UDA methods in terms of classification. The authors also show that the proposed method can be used to train models in deep networks. ","The paper proposes a new unsupervised domain adaptation (UDA) framework, called Butterfly, that combines clean labeled data and unlabeled data in TD with classifiers in order to improve the performance of classifiers. The proposed solution is a variant of WUDA, which is based on the Butterfly framework. The key idea is to use noisy-to-clean, and SD to-TD-distributional, to reduce the label noise in SD. The authors show that the proposed method outperforms other UDA methods in terms of classification for TD. The paper also shows that the models in Butterfly are more robust to label noise than other models in deep networks. The experimental results show that Butterfly outperforms the baseline methods on the WAN dataset."
12627,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"agent USED-FOR control tasks. high - dimensional images USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR control tasks. high - dimensional images USED-FOR control tasks. control policy USED-FOR task. latent representation CONJUNCTION control policy. control policy CONJUNCTION latent representation. latent representation USED-FOR task. scarce reward signal USED-FOR high - capacity encoder. relevant features USED-FOR task. representation learning USED-FOR image - based RL. image reconstruction loss USED-FOR representation learning. auxiliary decoder USED-FOR end - to - end. auxiliary decoder USED-FOR off - policy actor - critic algorithm. control tasks EVALUATE-FOR model - free and model - based algorithms. OtherScientificTerm are suboptimal convergence, and latent features. Metric is sample efficiency. Method is off - policy algorithms. Task is image - based RL1. ","This paper proposes a novel off-policy actor-critic algorithm for image-based RL1. The agent is trained using high-dimensional images to learn control tasks using model-free reinforcement learning (RL) with high-dimensionality of the data. The task is to learn a latent representation and a control policy for a given task using the latent representation, and the control policy is trained with a high-capacity encoder with a scarce reward signal. The auxiliary decoder is used to guide the end-to-end. The authors show that the proposed algorithm achieves suboptimal convergence in terms of sample efficiency. The paper also shows that the representation learning of the agent trained with image -based RL can be improved by using the image reconstruction loss. ","This paper proposes a new model-free reinforcement learning (RL) algorithm for learning control tasks from high-dimensional images. The agent is trained on a set of control tasks that are composed of a latent representation, a control policy, and a high-capacity encoder with a scarce reward signal. The key idea is to learn the latent features of the task and the relevant features for the task. The authors propose an off-policy actor-critic algorithm that uses an auxiliary decoder to predict the end-to-end. The proposed algorithm achieves suboptimal convergence with respect to the sample efficiency. Experiments are conducted on image-based RL1, showing that the proposed representation learning can be used in image -based RL with an image reconstruction loss. "
12636,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"it USED-FOR tasks. Dropout HYPONYM-OF regularization technique. it USED-FOR DNNs. regularization technique USED-FOR generalization. regularization technique USED-FOR deep neural networks ( DNNs ). DNNs USED-FOR tasks. dropout USED-FOR training. dropout technique USED-FOR accelerating training. accelerating training CONJUNCTION generalization. generalization CONJUNCTION accelerating training. dropout technique USED-FOR generalization. multi - sample dropout HYPONYM-OF dropout technique. dropout USED-FOR generalization. multi - sample dropout USED-FOR dropout samples. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION ILSVRC 2012 ( ImageNet ). multi - sample dropout USED-FOR training. CIFAR-100 CONJUNCTION SVHN datasets. SVHN datasets CONJUNCTION CIFAR-100. multi - sample dropout USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION ILSVRC 2012 ( ImageNet ). CIFAR-10 USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) USED-FOR image classification tasks. SVHN datasets USED-FOR image classification tasks. convolution layers CONJUNCTION dropout layer. dropout layer CONJUNCTION convolution layers. computation time FEATURE-OF convolution layers. training set CONJUNCTION validation set. validation set CONJUNCTION training set. error rates CONJUNCTION losses. losses CONJUNCTION error rates. validation set EVALUATE-FOR networks. losses EVALUATE-FOR networks. training set EVALUATE-FOR networks. error rates EVALUATE-FOR networks. multi - sample dropout USED-FOR networks. OtherScientificTerm are overfitting, dropout sample, sample losses, and fully connected layers. Metric is loss. Generic are technique, operator, and network. Method","This paper proposes a new regularization technique for deep neural networks (DNNs) called Dropout. Dropout is a dropout technique for training DNNs and it is used to improve generalization and accelerating training. The dropout is based on the multi-sample dropout, where dropout samples are sampled from the dropout sample. The authors show that dropout improves the performance of the training set, the validation set, and the networks on CIFAR-10, ILSVRC 2012 (ImageNet) and SVHN datasets for image classification tasks.  The authors also show that using dropout can improve the generalization of the trained network. ","This paper proposes a regularization technique for deep neural networks (DNNs) called Dropout. Dropout is a regularisation technique for DNNs that can be applied to a variety of tasks. The authors show that dropout can be used for accelerating training and generalization. They also show that multi-sample dropout is used to sample dropout samples from the dropout sample. They show that the proposed dropout layer is able to reduce the computation time of convolution layers and the number of fully connected layers. Finally, they evaluate the proposed loss on CIFAR-10, ILSVRC 2012 (ImageNet), and SVHN datasets. The proposed loss is shown to improve the performance of the trained networks on the validation set and on the training set. "
12645,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"filters CONJUNCTION features. features CONJUNCTION filters. filter ambiguity FEATURE-OF CNNs. interpretability EVALUATE-FOR models. strategy USED-FOR interpretable CNNs. model USED-FOR disentangled filters. Label Sensitive Gate ( LSG ) structure USED-FOR model. supervised manner USED-FOR model. sparsity regularization USED-FOR LSG. LSG USED-FOR redundant filters. training strategy COMPARE model. model COMPARE training strategy. redundancy CONJUNCTION interpretability. interpretability CONJUNCTION redundancy. redundancy EVALUATE-FOR model. interpretability EVALUATE-FOR training strategy. interpretability EVALUATE-FOR model. Method is Convolutional neural networks ( CNNs ). Generic are pre - trained model, and method. OtherScientificTerm are redundant channels, and periodical shutdown. ","This paper studies the problem of interpretability of convolutional neural networks (CNNs) in the presence of filter ambiguity in CNNs. The authors propose a strategy to train interpretable CNNs in a supervised manner. The model is trained with a Label Sensitive Gate (LSG) structure, which allows the model to learn disentangled filters and features without redundant channels. The proposed method is based on sparsity regularization of the LSG to remove redundant filters. The paper shows that the proposed training strategy is able to achieve better interpretability than the existing model in terms of redundancy and interpretability. ","This paper proposes a new training strategy for learning interpretable CNNs with filter ambiguity. The authors propose a pre-trained model with Label Sensitive Gate (LSG) structure. The model learns disentangled filters with filters and features. The proposed method is based on sparsity regularization, where the model is trained in a supervised manner. The paper shows that the proposed training strategy outperforms the original model in terms of redundancy and interpretability. "
12654,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"scalability CONJUNCTION non - stationarity. non - stationarity CONJUNCTION scalability. value function factorization learning USED-FOR collaborative multiagent systems. communication USED-FOR tasks. framework USED-FOR nearly decomposable Q - functions ( NDQ ). communication minimization USED-FOR framework. value function factorization learning CONJUNCTION communication learning. communication learning CONJUNCTION value function factorization learning. value function factorization learning USED-FOR framework. communication learning USED-FOR framework. information - theoretic regularizers USED-FOR framework. mutual information EVALUATE-FOR regularizers. regularizers COMPARE value function factorization methods. value function factorization methods COMPARE regularizers. QMIX HYPONYM-OF value function factorization methods. StarCraft unit micromanagement benchmark EVALUATE-FOR framework. framework COMPARE baseline methods. baseline methods COMPARE framework. StarCraft unit micromanagement benchmark EVALUATE-FOR baseline methods. Method is Reinforcement learning. Task is multi - agent settings. OtherScientificTerm are decentralized value functions, coordination, action selection, and communication messages. ",This paper proposes a new framework for nearly decomposable Q-functions (NDQ) for collaborative multiagent systems. The framework is based on value function factorization learning and communication learning to improve scalability and non-stationarity in multi-agent settings. The authors use information-theoretic regularizers for the framework and show that the mutual information between agents can be improved by regularizers. The proposed framework is evaluated on the StarCraft unit micromanagement benchmark and shows that the proposed framework outperforms baseline methods such as QMIX.,This paper proposes a new framework for nearly decomposable Q-functions (NDQ) for collaborative multiagent systems. The framework is based on value function factorization learning and communication learning. The key idea is to use information-theoretic regularizers to improve the scalability and non-stationarity of the decentralized value functions in multi-agent settings. The proposed framework is evaluated on the StarCraft unit micromanagement benchmark and shows that the proposed framework outperforms the baseline methods in terms of mutual information and scalability. Reinforcement learning is also used to improve coordination and action selection. 
12663,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"AI USED-FOR complex programs. programming puzzles USED-FOR computers programming. programming puzzle HYPONYM-OF Boolean function. input - output pairs CONJUNCTION English problem descriptions. English problem descriptions CONJUNCTION input - output pairs. GAN - like algorithm USED-FOR puzzles. Troublemaker USED-FOR puzzles. Troublemaker HYPONYM-OF GAN - like algorithm. GAN - like algorithm USED-FOR automatic puzzle generation. Generic are problems, and it. OtherScientificTerm is Puzzles. Task is program synthesis. Method are puzzle - solver, and puzzle - solving techniques. ","This paper studies the problem of program synthesis in the context of programming puzzles. The authors propose a GAN-like algorithm, called Troublemaker, to solve the puzzles in the form of Boolean function, which is a programming puzzle with input-output pairs and English problem descriptions. The main contribution of the paper is to propose a puzzle-solver that is able to solve complex programs. The paper shows that it is possible to solve these problems without any additional knowledge of the problems. ","The paper proposes a new algorithm for solving programming puzzles for computers programming. The algorithm is based on a GAN-like algorithm, called Troublemaker, which solves puzzles with input-output pairs and English problem descriptions. Puzzles are solved by solving a Boolean function, called programming puzzle, which is a programming puzzle. The paper shows that the algorithm is able to solve complex programs with complex programs, and that it can be used for program synthesis. The authors also propose a puzzle-solving solver, and show that it is more efficient than other puzzle-solving techniques."
12672,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"structured decomposition of their behavior USED-FOR Reinforcement learning agents. lower - level primitives CONJUNCTION higher - level meta - policy. higher - level meta - policy CONJUNCTION lower - level primitives. lower - level primitives PART-OF policy. primitives PART-OF hierarchical reinforcement learning. primitives PART-OF policy design. information - theoretic mechanism USED-FOR decentralized decision. natural competition CONJUNCTION specialization. specialization CONJUNCTION natural competition. policy architecture COMPARE flat and hierarchical policies. flat and hierarchical policies COMPARE policy architecture. generalization EVALUATE-FOR policy architecture. Method is meta - policy. OtherScientificTerm are high - level meta - policy, and primitive. ","This paper proposes a new meta-policy architecture for Reinforcement learning agents based on structured decomposition of their behavior. The policy design consists of two primitives in hierarchical reinforcement learning: lower-level primitives and higher-level meta-policies. The lower primitives are used to learn the high-level policy, and the meta-meta-policy is used to train the low-level one. The high level policy is learned by using an information-theoretic mechanism to learn a decentralized decision. The paper shows that the proposed policy architecture achieves better generalization than flat and hierarchical policies. The authors also show that natural competition and specialization can be used to improve the performance.","This paper proposes a new meta-policy architecture for Reinforcement learning agents that is based on the structured decomposition of their behavior. The paper proposes two primitives in hierarchical reinforcement learning: lower-level primitives and higher-level meta-policies. The first one is a high-level policy, while the second one is the low-level one. The authors propose an information-theoretic mechanism for decentralized decision making. They show that their policy architecture outperforms flat and hierarchical policies in terms of generalization, natural competition and specialization."
12681,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"models USED-FOR highdimensional state spaces. Model - based reinforcement learning methods USED-FOR models. rewards USED-FOR latent dynamics model. model - based planning framework USED-FOR latent reward prediction model. multi - step reward prediction USED-FOR planning. multi - step reward prediction USED-FOR latent representation. concise model - free representation USED-FOR framework. multi - pendulum and multi - cheetah environments FEATURE-OF framework. concise latent representation USED-FOR environments. method USED-FOR latent reward prediction model. method COMPARE model - based methods. model - based methods COMPARE method. sample efficiency EVALUATE-FOR model - free and model - based baselines. Planning COMPARE model - free and model - based baselines. model - free and model - based baselines COMPARE Planning. latent state - space FEATURE-OF Planning. sample efficiency EVALUATE-FOR Planning. Method are model - free reinforcement learning, and model - based algorithms. OtherScientificTerm are pendulums, and irrelevant information. Generic is them. ","This paper proposes a model-free reinforcement learning method for high-dimensional state spaces. The proposed method is based on multi-step reward prediction for planning, where the goal is to learn a latent dynamics model that predicts the rewards for each state in the latent space. The framework uses a concise model-based representation to learn the latent representation for the environments. The authors show that the proposed method improves the sample efficiency of the latent reward prediction model in a variety of environments, including multi-pendulum and multi-cheetah environments. ","This paper proposes a model-free reinforcement learning method for learning highdimensional state spaces. Model-based reinforcement learning methods are commonly used to train models to predict highdimensional states spaces. The authors propose a new model-based planning framework for learning a latent reward prediction model that predicts rewards for a latent dynamics model. The framework is based on multi-step reward prediction for planning in multi-pendulum and multi-cheetah environments. The proposed method is able to learn a concise latent representation for these environments, which can be used as a surrogate for the latent state-space. Planning is shown to achieve better sample efficiency than other model -based methods, and the proposed method outperforms both model -free and model-by-based baselines in terms of sample efficiency. "
12690,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"GPU / TPU memory limitations CONJUNCTION training times. training times CONJUNCTION GPU / TPU memory limitations. parameterreduction techniques USED-FOR BERT. parameterreduction techniques USED-FOR memory consumption. training speed EVALUATE-FOR BERT. parameterreduction techniques USED-FOR training speed. methods USED-FOR models. models COMPARE BERT. BERT COMPARE models. it USED-FOR downstream tasks. self - supervised loss USED-FOR inter - sentence coherence. multi - sentence inputs USED-FOR it. multi - sentence inputs USED-FOR downstream tasks. model COMPARE BERT - large. BERT - large COMPARE model. GLUE, RACE, and SQuAD benchmarks EVALUATE-FOR model. OtherScientificTerm are model size, and parameters. Method are natural language representations, and pretrained models. ","This paper studies the problem of training BERT with parameterreduction techniques to reduce the memory consumption and training times. The authors propose a self-supervised loss to improve the inter-sentence coherence between natural language representations and pretrained models. They show that the proposed methods can improve the training speed of BERT by reducing the model size and reducing the number of parameters. They also show that it can be used to learn multi-sentences inputs for downstream tasks. The proposed model outperforms BERT-large on GLUE, RACE, and SQuAD benchmarks.","This paper studies the problem of learning natural language representations. The authors propose two methods to improve the performance of models by reducing the model size. The first method, BERT, is based on parameterreduction techniques to reduce memory consumption and training times. The second method is a self-supervised loss to improve inter-sentence coherence. Experiments on GLUE, RACE, and SQuAD benchmarks show that the proposed model outperforms BERT-large in terms of training speed, and it can be used for downstream tasks with multi-sentences inputs. "
12699,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"Transformer HYPONYM-OF neural network architecture. neural network architecture USED-FOR language understanding. Transformer USED-FOR language understanding. text CONJUNCTION videos. videos CONJUNCTION text. image CONJUNCTION text. text CONJUNCTION image. modalities FEATURE-OF tasks. image HYPONYM-OF modalities. text HYPONYM-OF modalities. videos HYPONYM-OF modalities. temporal input sequence FEATURE-OF hidden states. model USED-FOR tasks. architecture USED-FOR model. model USED-FOR asynchronous multi - task learning. multiple input modalities CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION multiple input modalities. model USED-FOR multiple input modalities. architecture USED-FOR tasks. tasks CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION tasks. multiple input modalities FEATURE-OF tasks. image captioning CONJUNCTION visual question answering. visual question answering CONJUNCTION image captioning. visual question answering CONJUNCTION video activity recognition. video activity recognition CONJUNCTION visual question answering. OmniNet USED-FOR tasks. part - of - speech tagging CONJUNCTION image captioning. image captioning CONJUNCTION part - of - speech tagging. OmniNet USED-FOR part - of - speech tagging. part - of - speech tagging CONJUNCTION visual question answering. visual question answering CONJUNCTION part - of - speech tagging. video activity recognition HYPONYM-OF tasks. part - of - speech tagging HYPONYM-OF tasks. visual question answering HYPONYM-OF tasks. image captioning HYPONYM-OF tasks. compressed model EVALUATE-FOR tasks. video captioning CONJUNCTION video question answering. video question answering CONJUNCTION video captioning. neural network USED-FOR tasks. modalities USED-FOR neural network. video question answering HYPONYM-OF tasks. video captioning HYPONYM-OF tasks. spatio - temporal cache PART-OF OmniNet. spatio - temporal cache USED-FOR self - attention mechanism. Method is spatio - temporal cache mechanism. Generic are it, and them. ","This paper proposes a new neural network architecture called Transformer for language understanding. The proposed model is able to handle multiple input modalities, including images, videos, text, and videos. The model can be used to perform tasks such as asynchronous multi-task learning, multiple inputmodalities, and video activity recognition. The authors also propose a spatio-temporal cache mechanism for self-attention mechanism.  The authors show that the compressed model performs well on a variety of tasks, including video captioning, part-of-speech tagging, and visual question answering. ","This paper proposes a Transformer, a neural network architecture for language understanding. The Transformer is an extension of the Transformer to the context of language understanding, and the authors propose a new architecture for training the model. The proposed architecture is based on the spatio-temporal cache mechanism, which is a self-attention mechanism. The authors show that the temporal input sequence of hidden states can be decomposed into hidden states, and that the model can be used for multiple tasks with different modalities (e.g., image, text, videos, videos), multiple input modalities, and asynchronous multi-task learning. The paper also shows that the compressed model performs better than the original compressed model on a number of tasks. "
12708,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"tasks EVALUATE-FOR Deep learning models. predictive accuracy EVALUATE-FOR Deep learning models. methods USED-FOR uncertainty quantification. Bayesian neural networks USED-FOR methods. discriminative accuracy EVALUATE-FOR approximate posterior inference. frequentist approach USED-FOR uncertainty quantification. formal inference procedure USED-FOR predictive confidence intervals. discriminative jackknife ( DJ ) HYPONYM-OF formal inference procedure. formal inference procedure USED-FOR regression models. higher - order influence functions ( HOIFs ) FEATURE-OF model parameters. higher - order influence functions ( HOIFs ) USED-FOR DJ procedure. loss gradients CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION loss gradients. oracle access USED-FOR loss gradients. DJ USED-FOR HOIFs. oracle access CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION oracle access. oracle access USED-FOR recursive formula. model accuracy EVALUATE-FOR it. recursive formula USED-FOR DJ. recursive formula USED-FOR HOIFs. DJ COMPARE Bayesian and non - Bayesian baselines. Bayesian and non - Bayesian baselines COMPARE DJ. OtherScientificTerm are predictive uncertainty, and Bayesian credible intervals. Material is highand low - confidence prediction instances. Method is Bayesian methods. Metric is frequentist coverage. Task is model training. ",This paper studies the problem of uncertainty quantification in deep learning models. The authors propose a frequentist approach to approximate posterior inference based on a formal inference procedure based on the discriminative jackknife (DJ). The proposed DJ procedure is based on higher-order influence functions (HOIFs) that can be used to estimate the predictive confidence intervals of the model parameters. The theoretical results show that the proposed DJ improves the model accuracy in terms of the number of high and low-confidence prediction instances. The empirical results also show that DJ outperforms Bayesian and non-Bayesian baselines. ,"This paper proposes a new approach to improve the predictive accuracy of Deep learning models. The authors propose a new formal inference procedure called discriminative jackknife (DJ) for estimating predictive confidence intervals for regression models with higher-order influence functions (HOIFs) of the model parameters. The proposed methods are based on Bayesian neural networks, and the authors show that the proposed methods can achieve better predictive quantification than existing methods for estimating uncertainty quantification. They also show that their approach can be used for approximate posterior inference with respect to the predictive uncertainty, and that it can improve the model accuracy in terms of the number of highand low-confidence prediction instances. Finally, the authors provide a recursive formula for estimating HOIFs with oracle access and Hessian-vector products. Experiments show that DJ outperforms Bayesian and non-Bayesian baselines. "
12717,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,Generative Adversarial Networks USED-FOR video samples. complexity CONJUNCTION fidelity. fidelity CONJUNCTION complexity. complex Kinetics-600 dataset EVALUATE-FOR Generative Adversarial Networks. decomposition of its discriminator USED-FOR model. video synthesis CONJUNCTION video prediction. video prediction CONJUNCTION video synthesis. baseline USED-FOR synthesis. Fréchet Inception Distance USED-FOR prediction. Inception Score USED-FOR synthesis. Fréchet Inception Distance CONJUNCTION Inception Score. Inception Score CONJUNCTION Fréchet Inception Distance. prediction USED-FOR Kinetics600. UCF-101 dataset EVALUATE-FOR Inception Score. UCF-101 dataset EVALUATE-FOR synthesis. Kinetics-600 USED-FOR baseline. Kinetics-600 USED-FOR synthesis. Method is Generative models of natural images. Task is video modeling. ,This paper studies the problem of video modeling with generative adversarial networks (GANs). The authors propose a novel method to learn a discriminator that is able to discriminate between the input and the output of a GAN. The discriminator is trained on a set of synthetic and real-world datasets. The authors show that the discriminator can be used to train GANs that are robust to adversarial attacks. ,"This paper presents a novel approach to learning Generative Adversarial Networks for video samples. Generative models of natural images can be used for video modeling. The authors propose a decomposition of its discriminator to make the model more robust to adversarial perturbations. They show that the decomposition can improve the complexity and fidelity of the generated video samples, and show that this decomposition improves the Inception Score and the Fréchet Inception Distance for video synthesis and video prediction on the complex Kinetics-600 dataset. They also provide a baseline for synthesis on the UCF-101 dataset. "
12726,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"few - shot classification benchmarks EVALUATE-FOR representation. base class data USED-FOR representation. stages USED-FOR representation. spatial attention map USED-FOR background clutter. pre - trained classifier USED-FOR spatial attention map. Method are Few - shot learning, pre - trained network, and meta - learning. OtherScientificTerm is prior knowledge. Material is large - scale dataset. Generic is network. ","This paper studies the problem of few-shot learning, where the goal is to learn a representation from base class data without prior knowledge. The authors propose a pre-trained network that learns a spatial attention map over the background clutter, which is then used to train a pre -trained classifier. The representation is trained in two stages: meta-learning where the network is trained on a large-scale dataset, and meta-training where the representation is learned on a small subset of the training data. The results show that the proposed representation outperforms the state-of-the-art on a number of benchmark datasets.","This paper proposes a new method for Few-Shot learning. Few-shot learning is an extension of meta-learning, where a pre-trained network is trained on a large-scale dataset, and the goal is to learn a representation of the base class data. The representation is learned in stages, where the first stage learns the prior knowledge, and then the second stage learns a spatial attention map over the background clutter, which is then used to train the network. Experiments on several few-shot classification benchmarks show that the proposed representation outperforms the baselines. "
12735,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"molecules CONJUNCTION game maps. game maps CONJUNCTION molecules. generative adversarial networks ( GANs ) USED-FOR structured objects. molecules HYPONYM-OF structured objects. game maps HYPONYM-OF structured objects. structural requirements FEATURE-OF objects. molecules HYPONYM-OF structural requirements. constraints PART-OF model. approach USED-FOR logical constraints. knowledge compilation techniques USED-FOR expected disagreement. knowledge compilation techniques USED-FOR approach. setup USED-FOR hybrid logical - neural constraints. hybrid logical - neural constraints USED-FOR complex requirements. setup USED-FOR complex requirements. graph reachability HYPONYM-OF complex requirements. constrained images CONJUNCTION molecules. molecules CONJUNCTION constrained images. molecules CONJUNCTION video game levels. video game levels CONJUNCTION molecules. Method are constrained adversarial networks ( CANs ), generator, unconstrained GANs, and CANs. ","This paper proposes a novel approach to learn the logical constraints in a generative adversarial network (GAN). The proposed approach is based on knowledge compilation techniques, which can be applied to any GAN. The authors show that the learned constraints can be used to train a GAN to generate a set of objects with structural requirements, such as molecules, game maps, and constrained images. They also show that this setup can be combined with hybrid logical-neural constraints to learn complex requirements such as graph reachability. The paper also shows that the resulting generator can be trained with unconstrained GANs.","This paper proposes a new approach to learn logical constraints for structured objects such as molecules, game maps, and constrained images. The proposed approach is based on knowledge compilation techniques to reduce the expected disagreement between the generator and the generator. The authors propose to use generative adversarial networks (GANs) to learn the structural requirements of structured objects (e.g., molecules and game maps) and unconstrained GANs to learn complex requirements such as graph reachability. Experiments on constrained images and molecules show that the proposed approach can achieve better hybrid logical-neural constraints for complex requirements."
12744,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"Deep neural networks COMPARE deep neural networks. deep neural networks COMPARE Deep neural networks. fixed activation functions USED-FOR deep neural networks. learnable activation functions USED-FOR Deep neural networks. adaptability of learnable activation functions USED-FOR model. expressive power FEATURE-OF model. expressive power FEATURE-OF adaptability of learnable activation functions. Adaptive Piecewise Linear units ( APL ) USED-FOR learnable activation function. stages PART-OF activation functions. gradient information PART-OF activation functions. Symmetric - APL activations USED-FOR deep neural networks. robustness EVALUATE-FOR deep neural networks. deep neural networks USED-FOR adversarial attacks. Network - in - Network CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION Network - in - Network. Lenet CONJUNCTION Network - in - Network. Network - in - Network CONJUNCTION Lenet. activation functions COMPARE ReLUs. ReLUs COMPARE activation functions. activation functions USED-FOR architectures. ResNet-18 HYPONYM-OF architectures. activation functions USED-FOR adversarial fooling. Lenet HYPONYM-OF architectures. Network - in - Network HYPONYM-OF architectures. OtherScientificTerm are positive and negative halves, zero - centered continuous non - linearity, and SymmetricAPL function. Task is ablation studies. ","This paper studies the problem of robustness of deep neural networks against adversarial attacks using fixed activation functions. The authors propose Adaptive Piecewise Linear units (APL) as a learnable activation function that can be used to adaptively adjust the expressive power of the model. The main contribution of the paper is to show that Symmetric-APL activations can improve the robustness performance of the deep neural network in the presence of zero-centered continuous non-linearity. The paper also provides a theoretical analysis of the properties of the activation functions in terms of the stages in the training process and the gradient information in the activation function. Finally, the authors show that activation functions trained with these activation functions are more robust to adversarial fooling than ReLUs and other architectures such as Network-in-Network and ResNet-18. ",This paper studies the adaptability of learnable activation functions in Deep neural networks to adversarial attacks. The authors propose Adaptive Piecewise Linear units (APL) which is a learnable activator with positive and negative halves. The main contribution of the paper is to study the expressive power of the model in terms of the number of stages in the activation functions and the gradient information of the activations. They show that the Symmetric-APL activations can be used in deep neural networks with high robustness. They also provide ablation studies to show that ReLUs are more robust than activation functions for different architectures such as Network-in-Network and ResNet-18.
12753,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"models CONJUNCTION proprietary data. proprietary data CONJUNCTION models. third party components CONJUNCTION models. models CONJUNCTION third party components. application programming interfaces ( APIs ) USED-FOR pre - trained encapsulated models. wrapping deep learning model USED-FOR classification black - box. wrapping deep learning model USED-FOR measure of uncertainty. Dirichlet layer USED-FOR fusion layer. black - box classifier USED-FOR probabilistic neural network. Dirichlet layer USED-FOR probabilistic neural network. Dirichlet layer USED-FOR distribution. Dirichlet layer USED-FOR estimation of aleatoric uncertainty. multinomial output parameters USED-FOR distribution. uncertainty measure USED-FOR rejection system. NLP CONJUNCTION computer vision. computer vision CONJUNCTION NLP. technique CONJUNCTION methodology. methodology CONJUNCTION technique. wrapper USED-FOR uncertainty. Method are machine learning models, classifier, and simulated API based. Generic are external components, and components. Metric are auditability, and trustability. OtherScientificTerm are uncontrolled potential risks, black - box, aleatoric uncertainty, and misclassifications. ","This paper studies the problem of auditability of pre-trained encapsulated models with application programming interfaces (APIs). The authors propose a new measure of uncertainty based on a wrapping deep learning model for classification black-box, which is a probabilistic neural network trained with a Dirichlet layer and a fusion layer. The authors show that the Dirichlett layer can be used for the estimation of aleatoric uncertainty for the distribution with multinomial output parameters. The paper also shows that this uncertainty measure can be applied to the rejection system of a rejection system trained with NLP, computer vision, and NLP-based models.   ","This paper proposes a method to improve the auditability of machine learning models. The authors propose to use application programming interfaces (APIs) to train pre-trained encapsulated models with third party components. The main idea is to use a Dirichlet layer as a fusion layer between a black-box classifier and a probabilistic neural network. The black-Box classifier is trained with a set of external components, and the distribution is learned using multinomial output parameters. The distribution is then used for the estimation of aleatoric uncertainty, which is a measure of uncertainty. The rejection system is trained using the uncertainty measure as a rejection system. The method is evaluated on NLP, computer vision, and computer vision with simulated API based. "
12762,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"Stochastic methods USED-FOR deep neural networks. RMSprop CONJUNCTION Adam. Adam CONJUNCTION RMSprop. coordinate - wise adaptive stepsize FEATURE-OF Stochastic methods. Adam HYPONYM-OF coordinate - wise adaptive stepsize. RMSprop HYPONYM-OF coordinate - wise adaptive stepsize. Adam HYPONYM-OF Stochastic methods. RMSprop HYPONYM-OF Stochastic methods. they COMPARE stochastic gradient descent. stochastic gradient descent COMPARE they. blockwise adaptivity COMPARE adaptivity. adaptivity COMPARE blockwise adaptivity. adaptivity CONJUNCTION generalization. generalization CONJUNCTION adaptivity. blockwise adaptive gradient descent USED-FOR optimizing nonconvex objective. convergence rate FEATURE-OF optimizing nonconvex objective. blockwise adaptive gradient descent USED-FOR online convex learning. regret CONJUNCTION convergence rate. convergence rate CONJUNCTION regret. convergence rate EVALUATE-FOR blockwise adaptive gradient descent. regret EVALUATE-FOR blockwise adaptive gradient descent. blockwise adaptivity COMPARE coordinate - wise adaptivity. coordinate - wise adaptivity COMPARE blockwise adaptivity. generalization error EVALUATE-FOR coordinate - wise adaptivity. generalization error EVALUATE-FOR blockwise adaptivity. Nesterov ’s accelerated gradient CONJUNCTION Adam. Adam CONJUNCTION Nesterov ’s accelerated gradient. blockwise adaptive gradient descent COMPARE Adam. Adam COMPARE blockwise adaptive gradient descent. blockwise adaptive gradient descent COMPARE Nesterov ’s accelerated gradient. Nesterov ’s accelerated gradient COMPARE blockwise adaptive gradient descent. Method is Adagrad. OtherScientificTerm are network parameters, blockwise adaptive stepsize, and nonconvex objective. Metric is uniform stability. ","This paper studies the problem of online convex learning with blockwise adaptive gradient descent. The authors propose a new algorithm called Adagrad, which is based on blockwise adaptivity. The main idea is to learn a nonconvex objective by optimizing a coordinate-wise adaptive stepsize of the network parameters. They show that the proposed algorithm achieves better regret and convergence rate than the state-of-the-art Stochastic methods such as Adam and RMSprop. ","This paper proposes a new algorithm for online convex learning, called Adagrad. The main idea is to learn a nonconvex objective with blockwise adaptive stepsize, where the network parameters are uniformly stable. The authors show that they outperform stochastic gradient descent in terms of regret, convergence rate, and generalization. They also show that blockwise adaptivity outperforms adaptivity and adaptivity improves generalization, and that the proposed algorithm is more stable than blockwise adaptation. "
12771,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"large - scale supervised datasets USED-FOR deep features. datasets CONJUNCTION spatiotemporal models. spatiotemporal models CONJUNCTION datasets. scene and object structure FEATURE-OF implicit biases. implicit biases FEATURE-OF video datasets. observable and controllable object and scene bias FEATURE-OF video dataset. spatiotemporal understanding USED-FOR video dataset. 3D objects USED-FOR dataset. diagnostic tools USED-FOR spatiotemporal video architectures. CATER USED-FOR diagnostic tools. CATER HYPONYM-OF dataset. CATER USED-FOR deep video architectures. Task are Computer vision, static image analysis, and video understanding. Method are frame - by - frame classification methods, and long - term reasoning. OtherScientificTerm is temporal structure. ","This paper studies the problem of video classification with large-scale supervised datasets for deep features. The authors propose a new video dataset with observable and controllable object and scene bias based on implicit biases on scene and object structure. The dataset is composed of 3D objects and 2D objects. The proposed dataset is trained on CATER, a set of datasets and spatiotemporal models.  The authors show that the proposed CATER can be used to train deep video architectures using diagnostic tools. They also show that CATER improves the performance of frame-by-frame classification methods. ","This paper proposes a new dataset, CATER, to study the implicit biases of video datasets and spatiotemporal models on scene and object structure. The dataset consists of a video dataset with observable and controllable object and scene bias, and a set of large-scale supervised datasets for learning deep features. The authors propose to use frame-by-frame classification methods to learn the temporal structure of the video dataset. They show that CATER is able to learn deep video architectures on CATER and other diagnostic tools. They also show that long-term reasoning can be used for video understanding. "
12780,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"distribution USED-FOR synthetic data. Generative Adversarial Networks ( GANs ) HYPONYM-OF models. distribution USED-FOR models. image data USED-FOR visual applications. model compatibility problem HYPONYM-OF task. generating near - boundary data USED-FOR classifiers. generating ‘ easier ’ synthetic data USED-FOR GANs. pre - trained classifiers USED-FOR boundary information. Boundary - Calibration GANs ( BCGANs ) USED-FOR GAN. boundary information USED-FOR Boundary - Calibration GANs ( BCGANs ). pre - trained classifiers USED-FOR Boundary - Calibration GANs ( BCGANs ). GAN variants USED-FOR model compatibility. BC - loss CONJUNCTION GAN variants. GAN variants CONJUNCTION BC - loss. BC - loss USED-FOR model compatibility. BCGANs COMPARE GANs. GANs COMPARE BCGANs. model compatibility EVALUATE-FOR GANs. BCGANs USED-FOR realistic images. BCGANs COMPARE GANs. GANs COMPARE BCGANs. GANs USED-FOR realistic images. model compatibility EVALUATE-FOR BCGANs. Material is near - boundary data. Method is generator of GAN. OtherScientificTerm are posterior distributions, and boundaries of the pre - trained classifiers. ","This paper studies the model compatibility problem of GANs. The authors consider the problem of generating near-boundary data for classifiers from a distribution of synthetic data generated by the generator of a GAN. The distribution is used to train the models from the distribution of the synthetic data, and then the posterior distributions of the models are used to predict the boundary information of the pre-trained classifiers.  The authors propose Boundary-Calibration GGANs (BCGANs), a new GAN based on boundary information, which can be used to improve model compatibility. The BC-loss and GAN variants are shown to be effective in improving model compatibility, and BCGANs are also able to generate realistic images from the generated data. ","This paper studies the model compatibility problem in the context of generative adversarial networks (GANs). The authors consider the problem of generating near-boundary data for classifiers. The authors propose a new distribution for generating synthetic data for GANs, which is a ‘easier’ synthetic data, which can be used to train models on the distribution of the posterior distributions of the generator of GAN. They show that the proposed Boundary-Calibration GAN (BCGANs) can learn the boundary information of a GAN using pre-trained classifiers, and that BC-loss and other GAN variants can improve model compatibility. They also show that BCGANs are able to generate realistic images with better model compatibility than GAN, and show that they can be trained on image data for visual applications. "
12789,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,approach USED-FOR robust neural networks. Adversarial training USED-FOR robust neural networks. Adversarial training USED-FOR approach. minimax robust optimization problem USED-FOR adversarial training. outer minimization USED-FOR robust classifier. inner maximization USED-FOR adversarial samples. outer minimization CONJUNCTION inner maximization. inner maximization CONJUNCTION outer minimization. hand - designed algorithms USED-FOR inner problem. convolutional neural network USED-FOR optimizer. robust classifier USED-FOR adversarial attack. optimizer USED-FOR adversarial attack. L2L COMPARE adversarial training methods. adversarial training methods COMPARE L2L. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR L2L. classification accuracy CONJUNCTION computational efficiency. computational efficiency CONJUNCTION classification accuracy. classification accuracy EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR L2L. classification accuracy EVALUATE-FOR L2L. L2L framework USED-FOR generative adversarial imitation learning. Task is minimax problem. OtherScientificTerm is convex - concave structure. Method is adversarial training method. ,"This paper proposes a new approach for training robust neural networks with adversarial training. Adversarial training is a minimax robust optimization problem, and the authors propose an approach to solve this problem by using the convex-concave structure of the minimax problem. The main idea is to use outer minimization and inner maximization to train a robust classifier to generate adversarial samples. The inner problem is solved by hand-designed algorithms. The optimizer is trained using a convolutional neural network. The authors show that L2L outperforms the state-of-the-art in terms of classification accuracy, computational efficiency, and classification accuracy on CIFAR-10 and CifAR-100 datasets. ","This paper proposes a novel approach for training robust neural networks. Adversarial training is a minimax robust optimization problem for adversarial training, and the proposed approach is based on the idea of convex-concave structure. The main idea is to use hand-designed algorithms to solve the inner problem and inner maximization for the adversarial samples. The optimizer is trained with a convolutional neural network. The authors show that the proposed L2L outperforms the state-of-the-art in terms of classification accuracy and computational efficiency on the CIFAR-10 and CifAR-100 datasets. They also show that adversarial attack on the robust classifier can be mitigated by the proposed optimizer. Finally, the authors propose a new adversarial learning method that is based upon the L-2L framework for generative adversarial imitation learning."
12798,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"approaches USED-FOR Inverse Reinforcement Learning ( IRL ). reward function USED-FOR expert agent ’s policy. reward function USED-FOR control task. reward CONJUNCTION hard constraints. hard constraints CONJUNCTION reward. reward USED-FOR behavior. Markov Decision Processes ( MDPs ) USED-FOR IRL. Maximum Entropy IRL framework USED-FOR approach. algorithm USED-FOR Maximum Likelihood Constraint. algorithm USED-FOR observed behavior. Maximum Likelihood Constraint USED-FOR observed behavior. OtherScientificTerm are cumulative rewards, nominal reward function, and MDP. Generic is method. Material is simulated behavior. ","This paper proposes a new approach for Inverse Reinforcement Learning (IRL) based on Markov Decision Processes (MDPs). In IRL, the reward function of an expert agent’s policy is a function of cumulative rewards, and the goal is to maximize the cumulative rewards. The authors propose a new reward function for the control task, which is a nominal reward function. The proposed method is based on the Maximum Entropy IRL framework, where the reward and hard constraints are modeled as a reward and a hard constraint, respectively. They show that the proposed algorithm achieves the Maximum Likelihood Constraint for observed behavior, and that the observed behavior is better than the simulated behavior. ","This paper presents a novel approach to Inverse Reinforcement Learning (IRL) that uses Markov Decision Processes (MDPs) to learn the reward function for an expert agent’s policy. The authors propose a new approach based on the Maximum Entropy IRL framework. The key idea is to use cumulative rewards as a nominal reward function, and then use the reward to control the behavior of the expert agent. The proposed method is evaluated on simulated behavior. The algorithm is based on Maximum Likelihood Constraint, and the proposed algorithm is shown to improve observed behavior."
12807,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,deep recurrent neural network architecture USED-FOR visual cortical circuits. architecture USED-FOR contour detection tasks. architecture COMPARE feedforward networks. feedforward networks COMPARE architecture. sample efficiency EVALUATE-FOR feedforward networks. γ - Net HYPONYM-OF architecture. sample efficiency EVALUATE-FOR architecture. orientation - tilt illusion HYPONYM-OF perceptual illusion. lowlevel edges COMPARE high - level object boundary contours. high - level object boundary contours COMPARE lowlevel edges. it USED-FOR lowlevel edges. γ - Net contour detection accuracy EVALUATE-FOR illusion. neural circuits USED-FOR biological visual systems. circuits PART-OF artificial neural networks. circuits USED-FOR computer vision. neural circuits USED-FOR contour detection. artificial neural networks USED-FOR computer vision. biological visual systems USED-FOR contour detection. neural circuits USED-FOR orientation - tilt illusion. ,"This paper proposes a deep recurrent neural network architecture for visual cortical circuits. The proposed architecture, called γ-Net, is shown to achieve better sample efficiency than feedforward networks on a variety of contour detection tasks. In particular, it achieves better lowlevel edges than high-level object boundary contours. The authors also demonstrate that the resulting optical illusion, called orientation-tilter illusion, can be used to improve the performance of neural circuits for contour discovery on biological visual systems.","This paper proposes a deep recurrent neural network architecture for visual cortical circuits. The proposed architecture is compared to feedforward networks on several contour detection tasks. The authors show that the proposed architecture, called γ-Net, achieves better sample efficiency than high-level object boundary contours, and that it is able to detect lowlevel edges more accurately. The paper also studies the orientation-tilt illusion, which is a perceptual illusion that can be observed in biological visual systems.  The authors also show that neural circuits can be used for computer vision, and show that it can detect the optical properties of the circuits. "
12816,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,object detection methods USED-FOR detecting and classifying objects. deep convolutional neural networks ( CNNs ) USED-FOR object detection methods. semantics context constraints FEATURE-OF CNN - based object detector. conditional random field ( CRF ) model USED-FOR CNN. mean - field inference method USED-FOR CRF. context - aware module USED-FOR mean - field inference method. context - aware module PART-OF conCNN. stack of common CNN operations USED-FOR mean - field inference method. stack of common CNN operations USED-FOR conCNN. It PART-OF region - based object detection paradigm. average precision ( AP ) EVALUATE-FOR object detection. COCO datasets EVALUATE-FOR conCNN. conCNN USED-FOR object detection. average precision ( AP ) EVALUATE-FOR conCNN. Generic is methods. OtherScientificTerm is semantic context. ,"This paper studies the problem of object detection methods for detecting and classifying objects in deep convolutional neural networks (CNNs). The authors propose a new CNN-based object detector with semantics context constraints. The proposed CNN is based on a conditional random field (CRF) model. The CRF is trained with a mean-field inference method based on the stack of common CNN operations, and a context-aware module is added to the CRF. The authors show that the proposed conCNN achieves better average precision (AP) for object detection on COCO datasets. It is also shown that the region-based Object detection paradigm is also effective. ",This paper proposes a new method for object detection methods for detecting and classifying objects with deep convolutional neural networks (CNNs). The main idea is to use a conditional random field (CRF) model to train a CNN with semantics context constraints. The CRF is trained with a mean-field inference method based on a stack of common CNN operations and a context-aware module. It is an extension of the region-based object detection paradigm. Experiments on COCO datasets show that the proposed conCNN is able to achieve better average precision (AP) in object detection compared to other methods. 
12825,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"Batch Normalization ( BatchNorm ) USED-FOR deep neural networks. adversarial perturbations FEATURE-OF it. adversarial vulnerability FEATURE-OF BatchNorm. training CONJUNCTION inference. inference CONJUNCTION training. normalization statistics USED-FOR training. normalization statistics USED-FOR inference. normalization statistics USED-FOR adversarial vulnerability. adversarial vulnerability FEATURE-OF BatchNorm layer. mini - batch statistics HYPONYM-OF normalization statistics. Method are neural network architectures, and Robust Normalization ( RobustNorm ). OtherScientificTerm is adversarial perturbation. ","This paper studies the adversarial vulnerability of Batch Normalization (BatchNorm) in deep neural networks. BatchNorm has been shown to be vulnerable to adversarial perturbations, and it is shown that it is also vulnerable to the perturbation induced by the normalization statistics used in training and inference. The authors propose a new normalization statistic called Robust Normalisation (RobustNorm), which is based on the mini-batch statistics. RobustNorm is shown to improve the robustness of the Batch Norm layer against adversarial attacks. ",This paper studies the adversarial vulnerability of Batch Normalization (BatchNorm) in deep neural networks. The authors show that BatchNorm is vulnerable to adversarial perturbations due to it. They show that adversarial robust normalization (RobustNorm) can be used to improve the robustness of neural network architectures. They propose two normalization statistics for training and inference: mini-batch statistics and adversarial regularization statistics. 
12834,SP:f16d3e61eda162dfee39396abbd594425f47f625,"Over - parameterized deep neural networks USED-FOR labeling of data. first - order methods USED-FOR Over - parameterized deep neural networks. over - fitting ability USED-FOR generalization. clean test data EVALUATE-FOR regularization methods. early - stopping HYPONYM-OF regularization methods. trainable auxiliary variable USED-FOR network output. regularization HYPONYM-OF regularization methods. generalization guarantee FEATURE-OF clean data distribution. gradient descent training USED-FOR generalization guarantee. methods USED-FOR gradient descent training. wide neural network CONJUNCTION neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) CONJUNCTION wide neural network. noisily labeled datasets EVALUATE-FOR methods. OtherScientificTerm are network parameters, noisy labels, and label noise. Metric is generalization bound. ","This paper studies the generalization of Over-parameterized deep neural networks for labeling of data. The authors propose two first-order methods for training Over-Parameterized Deep Neural networks: (1) early-stopping and (2) regularization with trainable auxiliary variable. They show that the over-fitting ability of these regularization methods improves generalization on clean test data. They also provide a generalization bound for the clean data distribution. The generalization guarantee is based on gradient descent training, and the authors show that these methods can be used for gradient descent with a wide neural network and a neural tangent kernel (NTK). The authors also show that their methods perform well on noisily labeled datasets.","This paper proposes a new regularization method for training deep neural networks with noisy labels. The main idea is to use first-order methods to improve the generalization of Over-parameterized neural networks for labeling of data. The authors show that the over-fitting ability of the network parameters improves generalization in the presence of noisy labels, and that the network output with a trainable auxiliary variable is more robust to label noise. They also show that their regularization methods, such as early-stopping and regularization with early-starting, are robust to noise in the clean test data. They provide a generalization guarantee for the clean data distribution under gradient descent training, and show that these methods can be used for gradient descent learning. They evaluate their methods on a wide neural network and a neural tangent kernel (NTK) on noisily labeled datasets."
12843,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"convolutional neural networks ( CNNs ) USED-FOR regression. Convolutional Neural Tangent Kernel ( CNTK ) USED-FOR regression. algorithm USED-FOR CNTK. classification accuracy EVALUATE-FOR CNTK. CNTK COMPARE CNN architecture. CNN architecture COMPARE CNTK. classification accuracy EVALUATE-FOR CNN architecture. CIFAR-10 EVALUATE-FOR CNTK. Local Average Pooling ( LAP ) HYPONYM-OF operation. pixel shifts USED-FOR data augmentation. operation USED-FOR kernel. quadratic training cost FEATURE-OF kernel regression. CNN - GP CONJUNCTION CNTK. CNTK CONJUNCTION CNN - GP. full translation data augmentation USED-FOR CNTK. single convolutional layer USED-FOR pre - processing technique. random image patches PART-OF single convolutional layer. CIFAR-10 EVALUATE-FOR kernel. classifier COMPARE trained neural network. trained neural network COMPARE classifier. AlexNet COMPARE classifier. classifier COMPARE AlexNet. CNN - GP HYPONYM-OF kernel. horizontal flip data augmentation USED-FOR kernel. horizontal flip data augmentation USED-FOR CNN - GP. accuracy EVALUATE-FOR kernel. OtherScientificTerm are ` 2 loss, convolutional layers, and fixed kernel. Generic are layer, and kernels. Method are naive data augmentation, Global Average Pooling ( GAP ), and Fashion - MNIST. ","This paper studies the problem of training convolutional neural networks (CNNs) for regression using Convolutional Neural Tangent Kernel (CNTK) as a pre-processing technique. The authors propose a new algorithm for CNTK, which is based on the previous work, Global Average Pooling (GAP), which uses pixel shifts to improve the performance of the kernel in kernel regression with quadratic training cost. The main contribution of the paper is to propose a single neural layer with random image patches, which can be used as the `2 loss' between the two layers of the network. The proposed method is evaluated on CIFAR-10 and CNN-GP, and compared to a trained neural network and a classifier. ","This paper proposes a novel algorithm for regression, Convolutional Neural Tangent Kernel (CNTK) for regression with convolutional neural networks (CNNs). The authors propose a pre-processing technique, called Global Average Pooling (GAP), which is an operation based on Local Average Pooled (LAP) with pixel shifts. The main idea of GAP is to add random image patches to the top layers of the layer, and then apply a `2 loss on the convolutionals layers. The authors show that the proposed operation can improve the performance of kernel regression with quadratic training cost. They also show that CNTK outperforms the CNN architecture on classification accuracy on CIFAR-10 and CNN-GP with full translation data augmentation. "
12852,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"approach USED-FOR model - free Q - learning. MCTS USED-FOR state - action values. real experience USED-FOR prior. Q - estimates USED-FOR prior. Q - estimates CONJUNCTION real experience. real experience CONJUNCTION Q - estimates. model - free learning CONJUNCTION model - based search. model - based search CONJUNCTION model - free learning. MCTS USED-FOR value computation. physical reasoning tasks CONJUNCTION Atari. Atari CONJUNCTION physical reasoning tasks. agents USED-FOR physical reasoning tasks. agents USED-FOR Atari. it PART-OF agents. model USED-FOR Q - learning agent. Q - learning agent USED-FOR SAVE. SAVE COMPARE model - based search approaches. model - based search approaches COMPARE SAVE. training steps USED-FOR SAVE. real experience USED-FOR SAVE. model - free learning CONJUNCTION planning. planning CONJUNCTION model - free learning. Method is Search with Amortized Value Estimates. OtherScientificTerm are search budgets, and search. ","This paper proposes a new approach for model-free Q-learning based on Search with Amortized Value Estimates (SAVE). The main idea is to use MCTS to estimate the state-action values of the Q-learned agent, and then use Q-estimates from real experience to train the prior. The authors show that SAVE outperforms model-based search and model-trained search approaches in terms of performance on physical reasoning tasks and Atari. They also show that it can be used to train agents for Atari and other agents for physical reasoning and planning. ","This paper proposes a new approach for model-free Q-learning based on Search with Amortized Value Estimates (SAVE). The main idea is to use MCTS to compute state-action values, which are then used for value computation. The authors show that SAVE outperforms model-based search, model-Free learning, and real experience in terms of Q-estimates and prior. They also show that it outperforms agents trained on physical reasoning tasks and on Atari. The main contribution of the paper is the use of a model to learn the Q-learning agent, which is then used to train a Q-Learning agent that can be used for SAVE. The paper also shows that the search budgets of SAVE are much smaller than those of other models, and that the training steps are much faster. "
12861,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"deep learning USED-FOR low - level vision tasks. it USED-FOR 3D visualization of the 2D input scenery. network architecture USED-FOR stereoscopic view synthesis. network architecture USED-FOR Deep 3D Pan. globally and locally adaptive dilations USED-FOR t - shaped ” adaptive kernels. local 3D geometries USED-FOR synthesis of naturally looking 3D panned views. globally and locally adaptive dilation FEATURE-OF t - shaped adaptive kernel. t - shaped adaptive kernel USED-FOR monster - net. t - shaped adaptive kernel USED-FOR network architecture. monster - net HYPONYM-OF network architecture. 2 - D input image USED-FOR synthesis of naturally looking 3D panned views. VICLAB STEREO indoors dataset EVALUATE-FOR method. PSNR CONJUNCTION SSIM. SSIM CONJUNCTION PSNR. RMSE CONJUNCTION PSNR. PSNR CONJUNCTION RMSE. PSNR EVALUATE-FOR monster - net. SSIM EVALUATE-FOR monster - net. RMSE EVALUATE-FOR monster - net. monster - net USED-FOR image structures. synthesized images FEATURE-OF image structures. coherent geometry FEATURE-OF synthesized images. coherent geometry FEATURE-OF image structures. SOTA USED-FOR unsupervised monocular depth estimation task. that USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE that. that COMPARE disparity information. disparity information USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE SOTA. SOTA COMPARE disparity information. t - shaped ” kernel COMPARE SOTA. SOTA COMPARE t - shaped ” kernel. SOTA USED-FOR that. t - shaped ” kernel USED-FOR disparity information. Task is single - image - based view synthesis. OtherScientificTerm are parallel camera views, arbitrary camera positions, X - axis, and global camera shift. Material is KITTI. ","This paper proposes a new network architecture for stereoscopic view synthesis, called Deep 3D Pan. The proposed network architecture combines globally and locally adaptive dilations in the t-shaped “adaptive kernels”. The authors show that the proposed method outperforms SOTA on the VICLAB STEREO indoors dataset, PSNR, and SSIM. They also demonstrate that the “t-shaped” kernel is better than SOTA in the unsupervised monocular depth estimation task. Finally, the authors show how the proposed monster-net can reconstruct image structures from synthesized images with coherent geometry.","This paper proposes a new network architecture for stereoscopic view synthesis, called Deep 3D Pan. The proposed network architecture is based on a t-shaped adaptive kernel that combines globally and locally adaptive dilation with the t-shaped “t-shaped” adaptive kernels. The authors show that the proposed method outperforms SOTA and RMSE on the VICLAB STEREO indoors dataset, and that it can be used for 3D visualization of the 2D input scenery. They also show that their network architecture outperforms PSNR, SSIM, RMSE, and PSNR on a single-image-based view synthesis task. "
12870,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"representation CONJUNCTION generative model. generative model CONJUNCTION representation. Variational AutoEncoder ( VAE ) USED-FOR generative model. Variational AutoEncoder ( VAE ) USED-FOR representation. tasks PART-OF VAE. capacity FEATURE-OF variational network. information properties FEATURE-OF network. information properties USED-FOR tasks. network HYPONYM-OF information properties. generative model HYPONYM-OF information properties. theoretical objective USED-FOR maximal informative generative model. one HYPONYM-OF generative model. Capacity - Constrained InfoMax ( CCIM ) HYPONYM-OF generative model. one USED-FOR clustered and robust representation. one USED-FOR sharper samples. variational lower bound USED-FOR sharper samples. variational lower bound USED-FOR CCIM. variational lower bound USED-FOR one. one HYPONYM-OF VAE. OtherScientificTerm are informative one, and network capacity. Method is Variational InfoMax AutoEncoder ( VIMAE ). ","This paper proposes a variational AutoEncoder (VAE) for learning a representation and a generative model. The VAE consists of two tasks: (1) learning an informative one, and (2) learning a more robust one. The authors show that the capacity of the variational network is a function of the information properties of the network, i.e., the number of parameters and the number and size of parameters.  The authors then propose a theoretical objective to learn a maximal informative generative models, which they call Capacity-Constrained InfoMax (CCIM). The authors propose two variants of the VAE: one for learning the clustered and robust representation, and the other one for generating sharper samples. The variational lower bound for the CCIM is derived from the one proposed by the authors. ","This paper proposes a variational AutoEncoder (VAE) for learning a representation and a generative model. The VAE consists of two tasks. The first task is to learn a network with information properties such as network capacity, and the second task is learning a variative network with capacity. The authors show that the variational autoencoder is able to learn both the informative one and the generative one. The paper also proposes a theoretical objective for learning the maximal informative generative models. The variational autoencoder is a variant of the Variational InfoMax AutoEnoder (VIMAE). The authors also propose Capacity-Constrained InfoMax (CCIM) which is a variated version of the VAE. CCIM is based on variational lower bound for sharper samples, and one can learn the one for clustered and robust representation. "
12879,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"approach COMPARE Skip - gram. Skip - gram COMPARE approach. disconnected networks USED-FOR latent feature identification. latent feature identification HYPONYM-OF applications. embeddings USED-FOR matrices of node - feature pointwise mutual information. algorithms COMPARE models. models COMPARE algorithms. social, web and citation network datasets EVALUATE-FOR models. social, web and citation network datasets EVALUATE-FOR algorithms. Method are network embedding algorithms, random walks, and multiscale approach ( MUSAE ). OtherScientificTerm are local distribution, and attribute - neighborhood relationships. ","This paper proposes a new approach for network embedding algorithms. The main idea is to use disconnected networks for latent feature identification, which is a popular application in many applications such as latent feature classification. The authors propose a multiscale approach (MUSAE) to learn embeddings for matrices of node-feature pointwise mutual information. The key idea is that the local distribution of the nodes is the same for all nodes in the network, so that the embedding of each node can be used to learn a local distribution for each node. This allows the authors to use random walks to learn the attribute-neighborhood relationships. The proposed algorithms are evaluated on social, web and citation network datasets and show that the proposed algorithms perform better than existing models.","This paper proposes a new approach to learn network embedding algorithms that can be applied to a wide range of applications such as latent feature identification and disconnected networks. The authors propose a multiscale approach (MUSAE) to learn the embeddings for matrices of node-feature pointwise mutual information. The key idea is to learn a local distribution over the local distribution, and then use random walks to learn attribute-neighborhood relationships. Experiments on social, web and citation network datasets show that the proposed algorithms outperform existing models."
12888,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"multiple phase transitions FEATURE-OF IB objective. definition USED-FOR IB phase transitions. formula USED-FOR IB phase transitions. secondorder calculus of variations USED-FOR formula. Fisher information matrix USED-FOR parameterized models. canonical - correlation analysis ( CCA ) USED-FOR linear settings. maximum ( nonlinear ) correlation USED-FOR IB phase transition. theory USED-FOR algorithm. theory USED-FOR phase transitions. algorithm USED-FOR phase transitions. algorithm USED-FOR prominent phase transitions. theory CONJUNCTION algorithm. algorithm CONJUNCTION theory. theory USED-FOR prominent phase transitions. algorithm USED-FOR class difficulty. class difficulty FEATURE-OF MNIST. categorical datasets EVALUATE-FOR theory. categorical datasets USED-FOR phase transitions. CIFAR10 USED-FOR prominent phase transitions. Task is Information Bottleneck ( IB ). Generic are terms, dataset, and transitions. OtherScientificTerm are encoding distribution, β, and IB loss landscape. Metric is prediction accuracy. ","This paper proposes a new definition for the Information Bottleneck (IB) objective in the context of multiple phase transitions in the IB objective. The definition is based on the Fisher information matrix in parameterized models. The authors propose a formula for defining IB phase transitions based on a secondorder calculus of variations, which is then used to define the encoding distribution of the IB. The paper also proposes a canonical-correlation analysis (CCA) for linear settings, which shows that the maximum (nonlinear) correlation between the IB phase transition and the input data is the most important factor in predicting the IB loss landscape. Finally, the authors propose an algorithm that uses the theory and the proposed algorithm to predict prominent phase transitions on categorical datasets. The algorithm is evaluated on MNIST and CIFAR10.","This paper proposes a new definition of the Information Bottleneck (IB) objective, which is a special case of the IB objective with multiple phase transitions. The definition of IB phase transitions is based on the secondorder calculus of variations of the encoding distribution. The authors propose a new formula for the IB phase transition based on a Fisher information matrix for parameterized models. They show that the maximum (nonlinear) correlation between the input and the output of the model is the same in linear settings and nonlinear settings with canonical-correlation analysis (CCA) for linear settings. They also provide a new algorithm for the phase transitions based on CIFAR10. The proposed algorithm is evaluated on MNIST and on categorical datasets. The algorithm is shown to improve the class difficulty of MNIST with respect to the prediction accuracy. "
12897,SP:fecfd5e98540e2d146a726f94802d96472455111,"advantage function estimation methods USED-FOR reinforcement learning. independence property USED-FOR importance sampling advantage estimator. close - to - zero variance FEATURE-OF importance sampling advantage estimator. it COMPARE Monte - Carlo estimator. Monte - Carlo estimator COMPARE it. reward decomposition model USED-FOR Monte - Carlo estimator. method COMPARE advantage estimation methods. advantage estimation methods COMPARE method. sample efficiency EVALUATE-FOR advantage estimation methods. advantage estimation methods USED-FOR complex environments. complex environments EVALUATE-FOR method. sample efficiency EVALUATE-FOR method. OtherScientificTerm are time horizon, Monte - Carlo return signal, and estimation variance. Method is advantage estimation. Generic is estimator. ",This paper proposes a new advantage function estimation methods for reinforcement learning. The main idea is to use the independence property of importance sampling advantage estimator with close-to-zero variance as the objective. The authors propose a reward decomposition model for the Monte-Carlo estimator and show that it performs better than the standard estimator in terms of sample efficiency and sample efficiency in complex environments. They also show that their estimator is more efficient than other advantage estimation methods.,"This paper proposes a new advantage function estimation methods for reinforcement learning. The main idea is to use the independence property of importance sampling advantage estimator with close-to-zero variance. The authors propose a Monte-Carlo estimator based on a reward decomposition model, where the time horizon of the estimator is the sum of the value of the mean and covariance matrix of the gradient of the loss function. This estimator can be seen as an extension of advantage estimation. The paper shows that it is more robust to the uncertainty in the estimators than the original estimator. The proposed method is shown to improve sample efficiency over other advantage estimation methods in complex environments."
12906,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"approach USED-FOR infinite horizon off - policy evaluation. value function USED-FOR accuracy. value function USED-FOR bias - reduced augmentation of their method. density ratio CONJUNCTION value function estimation. value function estimation CONJUNCTION density ratio. method COMPARE methods. methods COMPARE method. Task is Infinite horizon off - policy policy evaluation. OtherScientificTerm are stationary density ratio, biases, and bias. Method is density ratio estimation. Generic is them. ","This paper proposes a new approach for infinite horizon off-policy evaluation. The proposed method is based on density ratio estimation and value function estimation. The authors propose a bias-reduced augmentation of their method, where the value function is used to improve the accuracy and the stationary density ratio is used as the bias. They show that the proposed method performs better than existing methods. ",This paper proposes a novel approach for infinite horizon off-policy evaluation. The authors propose a bias-reduced augmentation of their method by using a value function to improve the accuracy and the stationary density ratio. They show that the proposed method outperforms existing methods in terms of density ratio estimation and value function estimation. They also show that their method is more robust to bias than existing methods. 
12915,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"large - scale base classes USED-FOR generalization of prior knowledge. base data USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR discriminative feature. Metric - Softmax loss COMPARE episodic training. episodic training COMPARE Metric - Softmax loss. episodic training USED-FOR discriminative feature. task - adaptive transformation USED-FOR classifier. classifier USED-FOR few - shot setting. task - adaptive transformation USED-FOR few - shot setting. approach COMPARE state - of - the - arts. state - of - the - arts COMPARE approach. mini - ImageNet CONJUNCTION CUB-200 - 2011 benchmarks. CUB-200 - 2011 benchmarks CONJUNCTION mini - ImageNet. CUB-200 - 2011 benchmarks EVALUATE-FOR state - of - the - arts. mini - ImageNet EVALUATE-FOR state - of - the - arts. CUB-200 - 2011 benchmarks EVALUATE-FOR approach. mini - ImageNet EVALUATE-FOR approach. Task is Few - shot classification. Generic are task, and two - stage framework. Method are Metric - Softmax classifier, and fine - tuning scheme. ","This paper proposes a new method for few-shot classification based on the Metric-Softmax loss. The main idea is to use large-scale base classes for generalization of prior knowledge to new tasks. The task-agnostic feature is learned from the base data, and the discriminative feature is trained with the new method. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed approach performs better than state-of-the-arts on the two-stage framework. ",This paper proposes a two-stage framework for Few-shot classification. The first stage is to train a Metric-Softmax classifier on a set of large-scale base classes for generalization of prior knowledge. The second stage is a fine-tuning scheme where the classifier learns a task-agnostic feature from the base data. The proposed approach is evaluated on mini-ImageNet and CUB-200-2011 benchmarks and outperforms state-of-the-arts. The discriminative feature is learned using Metric - Softmax loss compared to episodic training. 
12924,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,fluid flow USED-FOR simulations of complex flow phenomena. fluid flow HYPONYM-OF problems. data - driven approach USED-FOR numerical solvers. accuracy EVALUATE-FOR numerical solvers. fine simulation resolution FEATURE-OF numerical scheme. numerical scheme USED-FOR method. neural network USED-FOR correction. fully supervised learning methods CONJUNCTION unsupervised learning method. unsupervised learning method CONJUNCTION fully supervised learning methods. naive and an optimized data acquisition USED-FOR fully supervised learning methods. differentiable Navier - Stokes solver USED-FOR unsupervised learning method. fully supervised learning methods HYPONYM-OF learning approaches. learning approaches USED-FOR targeted learning problem. unsupervised learning method HYPONYM-OF learning approaches. approach USED-FOR arbitrary partial differential equation models. accuracy EVALUATE-FOR fluid flow simulations. Method is numerical methods. Task is nonlinear simulation problems. ,"This paper proposes a data-driven approach to train numerical solvers with fluid flow for simulations of complex flow phenomena, such as fluid flow. The numerical scheme is based on fine simulation resolution, and the proposed method uses a neural network to perform the correction. The authors show that the proposed numerical methods achieve better accuracy than fully supervised learning methods and an unsupervised learning method based on a differentiable Navier-Stokes solver. They also show that their approach can be used to train arbitrary partial differential equation models. ","This paper proposes a data-driven approach to learn numerical solvers for nonlinear simulation problems such as fluid flow. The numerical scheme is based on fine simulation resolution, and the correction is performed by a neural network. The proposed learning approaches are compared to fully supervised learning methods and an unsupervised learning method based on a differentiable Navier-Stokes solver. The approach is applied to arbitrary partial differential equation models, and is shown to improve the accuracy of fluid flow simulations. The paper also proposes a targeted learning problem, where the goal is to find the optimal solution to the target learning problem."
12933,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"problem USED-FOR downstream online continual learning tasks. resource constrained data collection USED-FOR learning methods. discrete autoencoders PART-OF Quantization Modules ( SQM ). Quantization Modules ( SQM ) PART-OF architecture. discrete autoencoders PART-OF architecture. module USED-FOR latent space. latent space FEATURE-OF module. module USED-FOR module. methods COMPARE approach. approach COMPARE methods. method USED-FOR applications. episodic memory USED-FOR Experience Replay. episodic memory USED-FOR SQM. fixed memory budget USED-FOR continual learning benchmarks. method USED-FOR online compression of larger images. it USED-FOR modalities. Imagenet USED-FOR online compression of larger images. LiDAR data HYPONYM-OF modalities. Task is Online Continual Compression. Generic are learned representation, and modularity. OtherScientificTerm are moderate compressions, and pretraining. ","This paper studies the problem of online continual learning with resource constrained data collection. The authors propose a new architecture called Quantization Modules (SQM), which consists of discrete autoencoders and a module that maps the learned representation to the latent space. The proposed method is able to perform online compression of larger images using Imagenet. Experiments show that the proposed approach outperforms existing methods on a variety of applications.","This paper tackles the problem of downstream online continual learning tasks with resource constrained data collection. The authors propose a new architecture called Quantization Modules (SQM), which consists of discrete autoencoders in the latent space, and a module that compresses the learned representation into a single module. The proposed approach is evaluated on a variety of continual learning benchmarks with a fixed memory budget, and compared to existing methods for learning methods with moderate compressions. Experiments show that the proposed approach outperforms existing methods in several applications, including online compression of larger images (e.g., LiDAR data) and using episodic memory for Experience Replay. The paper also shows that it can be applied to other modalities such as Imagenet. "
12942,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"framework USED-FOR multi - label problem. Metric Learning USED-FOR k nearest neighbor algorithms. k nearest neighbor algorithms USED-FOR multi - label problem. k nearest neighbor algorithms HYPONYM-OF framework. deep representation approach USED-FOR metric learning. neural networks USED-FOR feature data. raw image data USED-FOR neural networks. neural networks USED-FOR deep representation approach. deep convolutional networks USED-FOR image data. Bidirectional Representation learning USED-FOR network architecture. deep neural networks USED-FOR metric space. metric space FEATURE-OF testing data. deep neural networks USED-FOR model. approach COMPARE methods. methods COMPARE approach. multi - labels tasks EVALUATE-FOR approach. multi - labels tasks EVALUATE-FOR methods. systematic metric EVALUATE-FOR approach. systematic metric EVALUATE-FOR methods. Task is Multi - Label Learning task. Method are multiple - label metric learning, and multi - label metric learning. OtherScientificTerm are application restriction, and label dependency. Material is multi - label data set. ","This paper proposes a new framework for multi-label metric learning, called Metric Learning for k nearest neighbor algorithms. The proposed framework is based on the idea of Bidirectional Representation learning (Bidirectional) in which the network architecture is trained using deep convolutional networks on raw image data. The model is trained with deep neural networks to learn the metric space of the test data, which is then used to train the model. The approach is evaluated on a variety of multi-labels tasks and shows that the proposed approach outperforms existing methods in terms of systematic metric.","This paper proposes a framework for the multi-label problem, called Metric Learning for k nearest neighbor algorithms. The proposed framework is based on the idea of multiple-label metric learning. The authors propose a deep representation approach for metric learning on raw image data, where neural networks are trained on feature data. The model is trained using deep convolutional networks on image data. Bidirectional Representation learning is used to train the network architecture. The paper shows that the proposed approach outperforms other methods on multi-labels tasks and a systematic metric on testing data in the metric space. "
12951,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,large batch sizes CONJUNCTION data parallelism. data parallelism CONJUNCTION large batch sizes. large batch sizes USED-FOR neural networks training. convergence rate EVALUATE-FOR asynchronous methods. dynamical stability USED-FOR asynchronous training. asynchronous stochastic gradient descent algorithm USED-FOR minima. closed - form rules USED-FOR learning rate. momentum PART-OF analysis. momentum USED-FOR training stability. OtherScientificTerm is delay. Task is training. Metric is generalization. Generic is algorithm. ,This paper studies the problem of training with large batch sizes and data parallelism in neural networks training with dynamical stability. The authors propose an asynchronous stochastic gradient descent algorithm for learning minima. They show that the learning rate of the algorithm is bounded by closed-form rules. They also provide a convergence rate for asynchronous methods. The main contribution of the paper is a theoretical analysis of the effect of momentum on the training stability and generalization.,This paper proposes an asynchronous stochastic gradient descent algorithm for learning minima with large batch sizes and data parallelism. The main idea is to use dynamical stability in asynchronous training to improve the convergence rate of asynchronous methods. The authors propose to use closed-form rules to reduce the learning rate. They also propose a new algorithm for generalization. The analysis is based on momentum in the training stability.
12960,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"Value estimation PART-OF reinforcement learning ( RL ) paradigm. rich transition structure USED-FOR Model learning. weak scalar signal USED-FOR model - free methods. approach USED-FOR representation learning. representation learning USED-FOR RL. OtherScientificTerm are predictors, reward function, features, and value function. Generic are approaches, and task. Task are value prediction, and policy evaluation settings. Material is Atari 2600 games. ","This paper proposes a new value estimation method for reinforcement learning (RL) paradigm. The key idea is to use a rich transition structure in Model learning with a weak scalar signal as the reward function. The authors show that this approach can be used for representation learning in RL, where the predictors can be trained to predict the value function. They also show that the proposed approaches can be combined with existing model-free methods that rely on a weak Scalar signal. Finally, they show that their method can be applied to value prediction in both policy evaluation settings. ","This paper proposes a novel value estimation in reinforcement learning (RL) paradigm. Model learning with a rich transition structure is an important part of Model learning. The authors propose a new approach for representation learning in RL, where the predictors are the reward function and the features are the value function. The proposed approach is based on a weak scalar signal, which has been used in model-free methods. The paper shows that the proposed approaches outperform existing approaches in both value prediction, and policy evaluation settings. The experiments are conducted on Atari 2600 games. "
12969,SP:6388fb91f2eaac02d9406672760a237f78735452,node classification CONJUNCTION graph classification. graph classification CONJUNCTION node classification. Graph Neural Networks ( GNNs ) USED-FOR graph related tasks. graph classification HYPONYM-OF graph related tasks. node classification HYPONYM-OF graph related tasks. graph neural networks USED-FOR adversarial attacks. graph rewiring operation USED-FOR graph. graph rewiring operation COMPARE operators. operators COMPARE graph rewiring operation. reinforcement learning USED-FOR attack strategy. rewiring operation USED-FOR reinforcement learning. rewiring operation USED-FOR attack strategy. real world graphs EVALUATE-FOR framework. perturbation FEATURE-OF graph structure. OtherScientificTerm is edges. ,This paper studies the problem of adversarial attacks on graph neural networks (GNNs) in graph related tasks such as node classification and graph classification. The authors propose a novel attack strategy based on a graph rewiring operation that rewires the graph by perturbations to the edges of the graph. The proposed framework is evaluated on real world graphs and shows that the proposed graph reweighing operation outperforms existing operators.,"This paper proposes a novel attack strategy for adversarial attacks on graph neural networks. The authors propose a graph rewiring operation that rewires the graph in order to make it more robust to edges. The proposed attack strategy is based on reinforcement learning, where the graph is rewired in the presence of perturbation to the graph structure. Experiments on real world graphs show the effectiveness of the proposed framework. "
12978,SP:233b12d422d0ac40026efdf7aab9973181902d70,"selected data USED-FOR neural networks. Stein ’s unbiased risk estimator ( SURE ) USED-FOR denoising problems. divergence term PART-OF SURE. neural network framework USED-FOR divergence term. close form expression USED-FOR unbiased estimator. unbiased estimator USED-FOR prediction error. close form expression USED-FOR prediction error. piecewise linear representation USED-FOR encoderdecoder CNN. bootstrap and aggregation scheme USED-FOR neural network. close form representation USED-FOR bootstrap and aggregation scheme. neural network USED-FOR identity mapping. inverse problems EVALUATE-FOR algorithm. Generic are architectures, and it. ",This paper proposes a new unbiased risk estimator (SURE) for denoising problems. SURE is based on the divergence term in the neural network framework. The authors propose a close form expression for the unbiased estimator to reduce the prediction error. The encoderdecoder CNN is trained using a piecewise linear representation. The bootstrap and aggregation scheme is used to train the neural net. The proposed algorithm is evaluated on inverse problems and shows good performance. ,This paper proposes a Stein’s unbiased risk estimator (SURE) for denoising problems. The divergence term in SURE is defined as the sum of the divergence term between the input and the output of the neural network framework. The unbiased estimator is based on a close form expression. The authors propose two architectures to compute it. The first is a piecewise linear representation for the encoderdecoder CNN. The second is a bootstrap and aggregation scheme for training a neural network for identity mapping. The proposed algorithm is evaluated on inverse problems.
12987,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"meta - learning approaches USED-FOR few - shot classification. meta - knowledge CONJUNCTION task - specific learning. task - specific learning CONJUNCTION meta - knowledge. Bayesian inference framework USED-FOR objective. variational inference USED-FOR objective. variational inference USED-FOR it. it COMPARE meta - learning approaches. meta - learning approaches COMPARE it. balancing component CONJUNCTION Bayesian learning framework. Bayesian learning framework CONJUNCTION balancing component. OtherScientificTerm are distributional difference, task relatedness, and balancing variables. Method are meta - learning model, and meta - learning. Material is multiple realistic taskand class - imbalanced datasets. ",This paper proposes a new meta-learning model for few-shot classification. The objective is based on a Bayesian inference framework with a balancing component and a variational inference framework for the distributional difference between the meta-knowledge and the task-specific learning. The authors show that it performs better than existing meta-learned approaches for the task and class-imbalanced datasets. They also show that the balancing variables can be used to improve the performance. ,This paper proposes a meta-learning model for few-shot classification. The objective is based on variational inference and a Bayesian inference framework. The meta-knowledge and task-specific learning are combined to improve the distributional difference between the two tasks. The balancing component of the Bayesian learning framework is used to balance the task relatedness between the meta-learned and the task-unlearned tasks. Experiments are conducted on multiple realistic taskand class-imbalanced datasets. The paper shows that it outperforms other meta learning approaches.
12996,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"high - dimensional, continuous observations CONJUNCTION unknown dynamics. unknown dynamics CONJUNCTION high - dimensional, continuous observations. distribution shift USED-FOR Supervised learning methods. behavioral cloning ( BC ) USED-FOR Supervised learning methods. inverse RL CONJUNCTION generative adversarial imitation learning ( GAIL ). generative adversarial imitation learning ( GAIL ) CONJUNCTION inverse RL. generative adversarial imitation learning ( GAIL ) HYPONYM-OF methods. inverse RL HYPONYM-OF methods. reinforcement learning ( RL ) USED-FOR methods. generative adversarial imitation learning ( GAIL ) HYPONYM-OF reinforcement learning ( RL ). inverse RL HYPONYM-OF reinforcement learning ( RL ). methods USED-FOR reward function. reward function USED-FOR task. adversarial training USED-FOR complex and brittle approximation techniques. complex and brittle approximation techniques USED-FOR reward function. complex and brittle approximation techniques USED-FOR methods. RL USED-FOR alternative. sparsity prior USED-FOR long - horizon imitation. regularized variant of BC USED-FOR SQIL. sparsity prior USED-FOR regularized variant of BC. SQIL COMPARE GAIL. GAIL COMPARE SQIL. Atari CONJUNCTION MuJoCo. MuJoCo CONJUNCTION Atari. Box2D CONJUNCTION Atari. Atari CONJUNCTION Box2D. SQIL COMPARE BC. BC COMPARE SQIL. Atari FEATURE-OF image - based and low - dimensional tasks. Box2D FEATURE-OF image - based and low - dimensional tasks. MuJoCo FEATURE-OF image - based and low - dimensional tasks. image - based and low - dimensional tasks EVALUATE-FOR SQIL. image - based and low - dimensional tasks EVALUATE-FOR GAIL. imitation method COMPARE methods. methods COMPARE imitation method. constant rewards FEATURE-OF RL. constant rewards USED-FOR imitation method. RL USED-FOR imitation method. learned rewards USED-FOR methods. OtherScientificTerm are expert behavior, error accumulation, out - of - distribution states, and constant reward. Method are RL agent, and soft Q imitation learning ( SQIL ). Generic is method. ","This paper studies the problem of behavioral cloning (BC) in the context of Supervised learning methods with distribution shift. The authors propose two methods: inverse RL and generative adversarial imitation learning (GAIL). Both methods are based on reinforcement learning (RL) to learn a reward function for each task. The main difference between the two methods is that the RL agent learns the reward function by adversarial training rather than using complex and brittle approximation techniques.  The authors also propose a regularized variant of BC called SQIL, which uses sparsity prior for long-horizon imitation. The proposed method is evaluated on image-based and low-dimensional tasks such as Box2D, MuJoCo, and Atari. The results show that SQIL outperforms BC and other methods with learned rewards. ","This paper proposes a new method for behavioral cloning (BC) for Supervised learning methods with distribution shift. The authors propose two methods: generative adversarial imitation learning (GAIL) and inverse RL, both based on reinforcement learning (RL) to learn the reward function for a task. The main difference between GAIL and BC is that GAIL is based on adversarial training, while BC uses sparsity prior for long-horizon imitation. The proposed method is evaluated on image-based and low-dimensional tasks, including MuJoCo, Atari, and Box2D. SQIL is a regularized variant of BC, where the RL agent is trained on the expert behavior, and the out-of-distribution states are learned by soft Q imitation learning. The results show that the proposed method outperforms other methods with learned rewards. "
13005,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"Point clouds HYPONYM-OF Lagrangian representation. deep - learning method USED-FOR stable and temporally coherent feature spaces. stable and temporally coherent feature spaces FEATURE-OF points clouds. temporal loss function USED-FOR mingling. higher time derivatives USED-FOR temporal loss function. super - resolution method CONJUNCTION truncation approach. truncation approach CONJUNCTION super - resolution method. techniques CONJUNCTION truncation approach. truncation approach CONJUNCTION techniques. techniques PART-OF super - resolution method. method USED-FOR large, deforming point sets. Generic are approaches, and approach. OtherScientificTerm are time dimension, flickering, undesirable local minima, halo structures, and halos. ","This paper proposes a deep-learning method for learning stable and temporally coherent feature spaces of points clouds in Lagrangian representation. The key idea is to learn a temporal loss function for mingling between two points clouds with higher time derivatives, where the time dimension depends on the number of halo structures. The authors propose two approaches to achieve this: a super-resolution method and a truncation approach. The proposed method can be applied to large, deforming point sets. The experimental results show the effectiveness of the proposed approach.","This paper proposes a deep-learning method for learning stable and temporally coherent feature spaces of points clouds, which is a Lagrangian representation. The key idea is to learn a temporal loss function for mingling between points clouds with higher time derivatives, where the time dimension depends on the number of points in the point clouds. The authors propose two approaches to achieve this: a super-resolution method and a truncation approach. The proposed approach is evaluated on large, deforming point sets, and the results show that the proposed method is able to achieve good performance on large, deforming points sets. "
13014,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"hand - crafted or Euclidean cost USED-FOR cost functions. side information USED-FOR cost function. side information USED-FOR subset correspondence. annotated cell type PART-OF single - cell data. side information USED-FOR cost function. marriage - matching CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION marriage - matching. images CONJUNCTION marriage - matching. marriage - matching CONJUNCTION images. images EVALUATE-FOR method. single - cell RNA - seq EVALUATE-FOR method. method COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE method. marriage - matching EVALUATE-FOR method. images CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION images. Generic is it. OtherScientificTerm is confounding. Method are Optimal transport ( OT ), OT, transport cost function, and Sinkhorn algorithm. ","This paper proposes Optimal Transport (OT), a new transport cost function based on the hand-crafted or Euclidean cost. The cost function is based on side information from the subset correspondence between the annotated cell type and the single-cell data. The proposed method is evaluated on images, single- cell RNA-seq, and marriage-matching. The results show that the proposed method outperforms state-of-the-art benchmarks.","The paper proposes Optimal transport (OT), which is a variant of Optimal Transport (OT) where the transport cost function is a hand-crafted or Euclidean cost. The paper proposes to use side information to learn the cost function for subset correspondence between the annotated cell type and the single-cell data. The main contribution of the paper is to propose a novel Sinkhorn algorithm. The method is evaluated on images, single- cell RNA-seq, and marriage-matching. The proposed method outperforms state-of-the-art benchmarks."
13023,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,large datasets USED-FOR method. algorithm USED-FOR representation underlying normal data. clustering technique USED-FOR high dimensional data. clustering CONJUNCTION representation learning. representation learning CONJUNCTION clustering. clustering USED-FOR hypothesizing normal candidate subset. normal data subset USED-FOR autoencoder. representation learning USED-FOR hypothesizing normal candidate subset. reconstruction error USED-FOR scoring function. autoencoder USED-FOR reconstruction error. public benchmark datasets EVALUATE-FOR method. method COMPARE semi - supervised techniques. semi - supervised techniques COMPARE method. method COMPARE unsupervised techniques. unsupervised techniques COMPARE method. public benchmark datasets EVALUATE-FOR unsupervised techniques. unsupervised techniques COMPARE semi - supervised techniques. semi - supervised techniques COMPARE unsupervised techniques. Task is normal data selection. ,"This paper proposes a clustering technique for high dimensional data. The proposed method uses large datasets to learn a representation underlying normal data, which is then used to train an autoencoder. The authors show that the reconstruction error of the scoring function can be computed using the reconstruction of the normal data subset. The method is evaluated on public benchmark datasets and shows that the proposed method outperforms semi-supervised techniques and representation learning. ","This paper proposes a new algorithm for learning the representation underlying normal data. The authors propose a clustering technique for high dimensional data, which can be applied to large datasets. The method is evaluated on two public benchmark datasets and compared to semi-supervised techniques. The main idea is to use clustering and representation learning to learn the hypothesizing normal candidate subset of the autoencoder. The reconstruction error of the scoring function is computed by the autencoder and the normal data subset is used for normal data selection."
13032,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"learning control policies USED-FOR reward function. local reward improvement update USED-FOR first step. L norm CONJUNCTION Kullback - Leibler divergence. Kullback - Leibler divergence CONJUNCTION L norm. convergence FEATURE-OF PCPO. metrics EVALUATE-FOR PCPO. L norm HYPONYM-OF metrics. Kullback - Leibler divergence HYPONYM-OF metrics. metrics USED-FOR convergence. control tasks EVALUATE-FOR PCPO. constraint violation CONJUNCTION reward. reward CONJUNCTION constraint violation. reward EVALUATE-FOR PCPO. OtherScientificTerm are constraints, fairness, policy, constraint set, and policy update. Generic are algorithm, and second step. Method is iterative method. Metric is reward improvement. ","This paper studies the problem of learning control policies for a reward function. The authors propose a new iterative method, PCPO, where the first step is a local reward improvement update, and the second step is an algorithm that updates the reward function based on a set of constraints. The goal of the algorithm is to maximize the fairness between the policy and the constraint set. The main contribution of the paper is to show that PCPO achieves the convergence of the L norm, Kullback-Leibler divergence, and reward on a variety of control tasks. ","The paper proposes an iterative method for learning control policies for a reward function. The first step is a local reward improvement update, where the goal is to improve the fairness of the reward function, and the second step is to update the policy. The algorithm is based on the idea that the first step improves the fairness, and then the policy is updated to match the reward set. The paper shows convergence of PCPO on two metrics, L norm and Kullback-Leibler divergence, and shows that PCPO converges to convergence on control tasks. "
13041,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"analogy structure FEATURE-OF embedding space. low rank transformation USED-FOR embedding. embedding transformation USED-FOR relative distances. α USED-FOR word embedding. Method is word embedding methods. Generic are inner mechanism, and method. OtherScientificTerm is word - context co - occurrence space. Material is real datasets. ","This paper studies the problem of word embedding methods, where the embedding space has analogy structure with respect to the word-context co-occurrence space. The authors propose a low rank transformation for embedding, which is based on the idea that the relative distances between two words can be computed by embedding transformation. The inner mechanism of the proposed method can be viewed as an extension of α, which can be seen as a special case of the idea of embedding with α. Experiments on real datasets demonstrate the effectiveness of the method.","This paper proposes a new method for learning word embedding methods. The idea is to use low rank transformation for embedding the embedding space based on analogy structure. The inner mechanism is based on the idea of α, and the relative distances are computed by embedding transformation. The proposed method is evaluated on real datasets. The authors show that the proposed method can learn word-context co-occurrence space."
13050,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"Similarity measurement USED-FOR data mining and machine learning tasks. approximate Random Projection Trees PART-OF X - Forest. RP Trees USED-FOR similarity measurement. layers PART-OF tree. randomness USED-FOR partition. real - world datasets EVALUATE-FOR model. Euclidean distance - based similarity metrics USED-FOR clustering tasks. model COMPARE RP Trees. RP Trees COMPARE model. X - Forest COMPARE RP Trees. RP Trees COMPARE X - Forest. X - Forest HYPONYM-OF model. efficiency EVALUATE-FOR model. accuracy EVALUATE-FOR RP Trees. Method is similarity measurement solution. Metric are speed, and exalted speed. OtherScientificTerm are prior knowledge, and projection vectors. Task is similarity measurements. ","This paper studies the problem of similarity measurement for data mining and machine learning tasks. The authors propose a new similarity measurement solution based on approximate Random Projection Trees (RP Trees), which is an extension of the X-Forest. The key idea is to partition the tree into layers based on the randomness of the prior knowledge, and then use these layers to construct a partition of the data. The proposed model is evaluated on two real-world datasets and compared to the state-of-the-art RP Trees. The efficiency of the model is shown to be better than the state of the art in terms of both accuracy and speed. ","This paper proposes a novel similarity measurement solution for data mining and machine learning tasks. The similarity measurements are based on approximate Random Projection Trees (RP Trees), a family of X-Forest that consists of two layers, one for prior knowledge and one for learning the partition of the data. The authors propose to use Euclidean distance-based similarity metrics for clustering tasks. They show that the proposed model outperforms existing RP Trees in terms of efficiency and accuracy on real-world datasets. "
13059,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"Statistical inference methods USED-FOR machine learning. Markov chain Monte Carlo ( MCMC ) CONJUNCTION variational inference ( VI ). variational inference ( VI ) CONJUNCTION Markov chain Monte Carlo ( MCMC ). Markov chain Monte Carlo ( MCMC ) USED-FOR inference algorithms. variational inference ( VI ) USED-FOR inference algorithms. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. simulation bias FEATURE-OF finite - length MCMC chains. hybrid method USED-FOR VI. gradient - based optimisation USED-FOR hybrid method. gradient - based optimisation USED-FOR simulation bias. method USED-FOR low - biased samples. approximation bias CONJUNCTION computational efficiency. computational efficiency CONJUNCTION approximation bias. method USED-FOR MCMC hyper - parameters. method COMPARE hybrid methods. hybrid methods COMPARE method. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. hybrid methods USED-FOR MCMC. hybrid methods USED-FOR VI. Generic is methods. Method are MCMC methods, and VI methods. OtherScientificTerm is MCMC simulation. ","This paper studies statistical inference methods for machine learning with Markov chain Monte Carlo (MCMC) and variational inference (VI) in the context of inference algorithms such as MCMC and VI. The authors propose a hybrid method based on gradient-based optimisation to improve the simulation bias of finite-length MCMC chains with simulation bias due to simulation bias in MCMC simulation. The proposed method is able to recover low-biased samples from MCMC methods. The method also improves the approximation bias and the computational efficiency of MCMC hyper-parameters. The experimental results show the effectiveness of the proposed method compared to MCMC, VI, and other hybrid methods.",This paper proposes a new method to reduce the simulation bias of Markov chain Monte Carlo (MCMC) and variational inference (VI) for machine learning. The authors propose a hybrid method based on gradient-based optimisation to improve the performance of MCMC and VI. They show that the proposed method improves the approximation bias and the computational efficiency of the MCMC hyper-parameters. They also show that their method can be applied to low-biased samples. 
13068,SP:64f2744e938bd62cd47c1066dc404a42134953da,"treatment USED-FOR Inferring causal effects. observational data USED-FOR Inferring causal effects. state - of - the - art methods USED-FOR causal inference. adapted unconfoundedness hypothesis USED-FOR they. variational autoencoders USED-FOR missing values. variational autoencoders USED-FOR latent confounders. them PART-OF multiple imputation strategy. methodology COMPARE competitors. competitors COMPARE methodology. methodology USED-FOR non - linear models. non - linear models COMPARE competitors. competitors COMPARE non - linear models. OtherScientificTerm are covariates, and Missing data. Task are real - world analyses, and causal inference procedures. Generic is They. ",This paper proposes a treatment for Inferring causal effects from observational data using state-of-the-art methods for causal inference. The authors propose an adapted unconfoundedness hypothesis for the covariates and propose to use them in a multiple imputation strategy. They use variational autoencoders to capture the missing values in the latent confounders. They show that their methodology outperforms competitors on a number of real-world analyses. ,This paper proposes a new treatment for Inferring causal effects from observational data. The authors propose a state-of-the-art methods for causal inference based on the adapted unconfoundedness hypothesis. They use variational autoencoders to capture missing values and use them in a multiple imputation strategy. They show that the proposed methodology outperforms competitors on non-linear models. They also show that covariates can be learned from Missing data. 
13077,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"neural architecture search algorithm USED-FOR compact reinforcement learning ( RL ) policies. ENAS CONJUNCTION ES. ES CONJUNCTION ENAS. combinatorial search space FEATURE-OF NAS. edge - partitionings USED-FOR compact architectures. vanilla policies COMPARE compact policies. compact policies COMPARE vanilla policies. compression EVALUATE-FOR vanilla policies. compression EVALUATE-FOR compact policies. colorings USED-FOR policies. colorings USED-FOR RL tasks. Toeplitz matrices USED-FOR compact policies. structured neural network architectures USED-FOR RL problems. approach USED-FOR structured neural network architectures. mobile robotics FEATURE-OF RL problems. limited storage and computational resources FEATURE-OF mobile robotics. OtherScientificTerm are edge - partitionings ( colorings ), same - weight classes, and weight parameters. ","This paper proposes a novel neural architecture search algorithm for compact reinforcement learning (RL) policies. The proposed approach is based on edge-partitionings (colorings), which can be used in the combinatorial search space of NAS. The authors show that edge- partitionings can be applied to compact architectures such as ENAS and ES. Toeplitz matrices are used to compute the compact policies, and the compression of the vanilla policies is shown to be better than that of compact policies with colorings. The paper also shows that the proposed approach can be extended to other structured neural network architectures for RL problems such as mobile robotics with limited storage and computational resources. ","This paper proposes a novel neural architecture search algorithm for compact reinforcement learning (RL) policies. The authors propose to use edge-partitionings (colorings) in the combinatorial search space of NAS, which is an extension of ENAS and ES. The key idea is to use the same-weight classes of the two architectures in the same search space. The paper shows that the proposed approach can be applied to structured neural network architectures for RL problems such as mobile robotics with limited storage and computational resources. Toeplitz matrices are used to compute the compact policies, and the authors show that the compression of the proposed compact policies outperforms vanilla policies. "
13086,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"time - series USED-FOR representation learning. Group Transform approach USED-FOR representation learning. framework USED-FOR time - frequency transformations. Wavelet Transform HYPONYM-OF time - frequency transformations. approach USED-FOR non - linear transformations. affine transformations of a mother filter USED-FOR Wavelet Transform filter - bank. transformations USED-FOR signal representations. maps USED-FOR signal representations. maps USED-FOR transformations. parameterization USED-FOR non - linear map. Deep Neural Network USED-FOR Group Transform. time - series datasets EVALUATE-FOR framework. Generic is representation. OtherScientificTerm are invertible maps, and strictly increasing and continuous functions. ","This paper proposes a new framework for time-frequency transformations, Wavelet Transform, for representation learning on time-series. The key idea is to use affine transformations of a mother filter as a Wavelet transform filter-bank, and then use these transformations to learn signal representations from these maps. The proposed Group Transform approach is based on a Deep Neural Network, and is able to learn non-linear transformations with parameterization. The authors show that the proposed representation can be used to learn invertible maps, and that it can be applied to both strictly increasing and continuous functions. Empirical results show the effectiveness of the proposed framework on a variety of time-sparse datasets.","This paper proposes a novel framework for time-frequency transformations, Wavelet Transform, for representation learning in time-series. The proposed approach is based on affine transformations of a mother filter. The authors show that invertible maps can be represented as a set of transformations, which can be used to generate signal representations that are non-linear in the form of maps. They also show that the representation can be learned by a Deep Neural Network. "
13095,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"inductive biases USED-FOR real - world data properties. scale - free HYPONYM-OF real - world data properties. hyperbolic or spherical HYPONYM-OF non - Euclidean spaces. Euclidean geometry CONJUNCTION vector space operations. vector space operations CONJUNCTION Euclidean geometry. Euclidean geometry USED-FOR graph neural networks. vector space operations USED-FOR graph neural networks. graph convolutional networks ( GCN ) USED-FOR ( products of ) constant curvature spaces. gyro - barycentric coordinates USED-FOR Euclidean concept of the center of mass. models USED-FOR Euclidean counterparts. node classification CONJUNCTION distortion minimization. distortion minimization CONJUNCTION node classification. Euclidean GCNs USED-FOR node classification. Euclidean GCNs USED-FOR distortion minimization. distortion minimization USED-FOR symbolic data. non - Euclidean behavior FEATURE-OF symbolic data. Method is unified formalism. OtherScientificTerm are geometries of constant curvature, curvature, and discrete curvature. ","This paper proposes a unified formalism for learning the geometries of constant curvature in graph neural networks using Euclidean geometry and vector space operations. The main idea is to use graph convolutional networks (GCN) to learn the (products of) constants curvature spaces (e.g., hyperbolic or spherical) of non-Euclidean spaces such as scale-free HYPONYM-of real-world data properties. The authors use gyro-barycentric coordinates to represent the Euclidea concept of the center of mass, and then use these models to train Euclideans counterparts. The curvature is defined as the sum of the distance between the center and the radius of the curvature of a point in the space, and the distance from the center to the center is defined by a discrete curvature.  The authors show that the proposed model can be used for node classification, distortion minimization, and other symbolic data with non-euclideans behavior. ","This paper proposes a unified formalism for learning the geometries of constant curvature in graph neural networks with Euclidean geometry and vector space operations. The authors propose to use graph convolutional networks (GCN) to learn the (products of) constants curvature spaces in non-Euclidean spaces (e.g., hyperbolic or spherical) with scale-free data properties. The main idea is to use gyro-barycentric coordinates of the center of mass to represent the curvature, and the authors show that models trained with these models are more robust to distortion minimization and node classification compared to Euclideans. "
